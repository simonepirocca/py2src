{"total_count": 288, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/pytorch/text/issues/923", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/923/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/923/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/923/events", "html_url": "https://github.com/pytorch/text/issues/923", "id": 678040788, "node_id": "MDU6SXNzdWU2NzgwNDA3ODg=", "number": 923, "title": "Feature request for torchtext.experimental.vocab", "user": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "Nayef211", "id": 22487263, "node_id": "MDQ6VXNlcjIyNDg3MjYz", "avatar_url": "https://avatars1.githubusercontent.com/u/22487263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nayef211", "html_url": "https://github.com/Nayef211", "followers_url": "https://api.github.com/users/Nayef211/followers", "following_url": "https://api.github.com/users/Nayef211/following{/other_user}", "gists_url": "https://api.github.com/users/Nayef211/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nayef211/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nayef211/subscriptions", "organizations_url": "https://api.github.com/users/Nayef211/orgs", "repos_url": "https://api.github.com/users/Nayef211/repos", "events_url": "https://api.github.com/users/Nayef211/events{/privacy}", "received_events_url": "https://api.github.com/users/Nayef211/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Nayef211", "id": 22487263, "node_id": "MDQ6VXNlcjIyNDg3MjYz", "avatar_url": "https://avatars1.githubusercontent.com/u/22487263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nayef211", "html_url": "https://github.com/Nayef211", "followers_url": "https://api.github.com/users/Nayef211/followers", "following_url": "https://api.github.com/users/Nayef211/following{/other_user}", "gists_url": "https://api.github.com/users/Nayef211/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nayef211/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nayef211/subscriptions", "organizations_url": "https://api.github.com/users/Nayef211/orgs", "repos_url": "https://api.github.com/users/Nayef211/repos", "events_url": "https://api.github.com/users/Nayef211/events{/privacy}", "received_events_url": "https://api.github.com/users/Nayef211/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-08-12T23:34:20Z", "updated_at": "2020-08-13T16:07:31Z", "closed_at": "2020-08-13T16:07:31Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "For the record, we need some update for `torchtext.experimental.vocab`:\r\n\r\n- `__call__` func for the behavior of `lookup_tokens`\r\n- attach `<unk>` to position 0 if not provided in `OrderedDict`.\r\n\r\nSame proposal for `torchtext.experimental.vectors`.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/913", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/913/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/913/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/913/events", "html_url": "https://github.com/pytorch/text/issues/913", "id": 675602890, "node_id": "MDU6SXNzdWU2NzU2MDI4OTA=", "number": 913, "title": "TypeError: embedding(): argument 'indices' (position 2) must be Tensor, not list", "user": {"login": "EmreTokyuez", "id": 52999998, "node_id": "MDQ6VXNlcjUyOTk5OTk4", "avatar_url": "https://avatars1.githubusercontent.com/u/52999998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EmreTokyuez", "html_url": "https://github.com/EmreTokyuez", "followers_url": "https://api.github.com/users/EmreTokyuez/followers", "following_url": "https://api.github.com/users/EmreTokyuez/following{/other_user}", "gists_url": "https://api.github.com/users/EmreTokyuez/gists{/gist_id}", "starred_url": "https://api.github.com/users/EmreTokyuez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EmreTokyuez/subscriptions", "organizations_url": "https://api.github.com/users/EmreTokyuez/orgs", "repos_url": "https://api.github.com/users/EmreTokyuez/repos", "events_url": "https://api.github.com/users/EmreTokyuez/events{/privacy}", "received_events_url": "https://api.github.com/users/EmreTokyuez/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-08T23:16:17Z", "updated_at": "2020-08-10T11:01:02Z", "closed_at": "2020-08-10T10:15:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\nI am following [this](https://github.com/bentrevett/pytorch-sentiment-analysis) tutorial for Sentiment Analysis and I am stuck on the [first tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb)\r\n\r\nAs I am combining the train and the test set for a valid comparison of different classifiers as mentioned in issue #912 , I was \"forced\" to use the new experimental dataset for the IMDB Review Dataset. Sadly, the tutorial I am trying to follow is using the legacy tools and thus, can't be used to combine the train and the test sets (as far as I am aware).\r\n\r\nUsing this code and building the same model as the tutorial, I get the error from the title:\r\n\r\n```\r\nfrom torchtext.experimental.datasets import IMDB\r\nfrom torchtext.data.utils import get_tokenizer\r\n\r\ntokenizer = get_tokenizer(\"spacy\")\r\n\r\ntrain_data, test_data = IMDB(tokenizer = tokenizer)\r\ntest_data, valid_data = torch.utils.data.random_split(test_data, [17500,7500], generator=torch.Generator().manual_seed(SEED))\r\nvocab = train_data.get_vocab()\r\nfrom torch.utils.data import ConcatDataset\r\ntrain_dataset = ConcatDataset([train_data, test_data])\r\n\r\n# Generate 8x8 batches (copied from github issue #651 \r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\ndef generate_rows(data):\r\n\tlabel_list = []\r\n\ttxt_list = []\r\n\tfor label, txt in data:\r\n\t\tlabel_list.append(label)\r\n\t\ttxt_list.append(txt)\r\n\treturn txt_list, label_list\r\ndataloader = DataLoader(train_dataset, batch_size=64, num_workers=0,    collate_fn=generate_rows)\r\n```\r\n```\r\nimport  torch.nn as nn\r\n\r\nclass RNN(nn.Module):\r\n    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\r\n\r\n        super().__init__()\r\n        \r\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\r\n\r\n        self.rnn = nn.RNN(embedding_dim, hidden_dim)\r\n\r\n        self.fc = nn.Linear(hidden_dim, output_dim)\r\n\r\n    def forward(self,txt):\r\n\r\n        #txt = [sent len, batch size]\r\n\r\n        embedded = self.embedding(txt)\r\n\r\n        #embedded = [sent len, batch size, emb dim]\r\n\r\n        output, hidden = self.rnn(embedded)\r\n\r\n        #output = [sent len, batch size, hid dim]\r\n        #hidden = [1, batch size, hid dim]\r\n        \r\n        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\r\n        \r\n        return self.fc(hidden.squeeze(0))\r\n\r\nINPUT_DIM = len(vocab)\r\nEMBEDDING_DIM = 250\r\nHIDDEN_DIM = 256\r\nOUTPUT_DIM = 1\r\n\r\nmodel = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\r\n\r\nimport torch.optim as optim \r\n\r\noptimizer = optim.SGD(model.parameters(), lr=1e-3)\r\n\r\ncriterion = nn.BCEWithLogitsLoss()\r\n\r\n\r\n\r\nmodel = model.to('cuda:0')\r\ncriterion = criterion.to('cuda:0')\r\n\r\ndef  train(model, dataloader, optimizer, criterion):\r\n\r\n    epoch_loss = 0\r\n    epoch_acc = 0\r\n\r\n    model.train()\r\n\r\n    \r\n    for txt, label in dataloader:\r\n        optimizer.zero_grad()\r\n                \r\n        predictions = model(txt).squeeze(1)\r\n        \r\n        loss = criterion(predictions,label)\r\n        \r\n        acc = binary_accuracy(predictions,label)\r\n        \r\n        loss.backward()\r\n        \r\n        optimizer.step()\r\n        \r\n        epoch_loss += loss.item()\r\n        epoch_acc += acc.item()\r\n\r\n    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\r\n\r\ndef evaluate(model, dataloader, criterion):\r\n    \r\n    epoch_loss = 0\r\n    epoch_acc = 0\r\n    \r\n    model.eval()\r\n    \r\n    with torch.no_grad():\r\n    \r\n        for txt, label in dataloader:\r\n\r\n\r\n            predictions = model(txt).squeeze(1)\r\n            \r\n            loss = criterion(predictions, label)\r\n            \r\n            acc = binary_accuracy(predictions, label)\r\n\r\n            epoch_loss += loss.item()\r\n            epoch_acc += acc.item()\r\n        \r\n    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\r\n\r\nimport time\r\n\r\ndef epoch_time(start_time, end_time):\r\n    elapsed_time = end_time - start_time\r\n    elapsed_mins = int(elapsed_time / 60)\r\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n    return elapsed_mins, elapsed_secs\r\n\r\ndataloader_valid = DataLoader(valid_data, batch_size=64, num_workers=0,    collate_fn=generate_rows)\r\n\r\nN_EPOCHS = 5\r\n\r\nbest_valid_loss = float('inf')\r\n\r\nfor epoch in range(N_EPOCHS):\r\n\r\n    start_time = time.time()\r\n    \r\n    train_loss, train_acc = train(model, dataloader, optimizer, criterion)\r\n    valid_loss, valid_acc = evaluate(model, dataloader_valid, criterion)\r\n    \r\n    end_time = time.time()\r\n\r\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n    \r\n    if valid_loss < best_valid_loss:\r\n        best_valid_loss = valid_loss\r\n        torch.save(model.state_dict(), 'tut1-model.pt')\r\n```\r\n\r\nUsing this code (sorry for the amount, I am a newbie in pytorch and want to make sure everything that could cause problems is in the post) I get the following error:\r\n\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n in \r\n      7     start_time = time.time()\r\n      8 \r\n----> 9     train_loss, train_acc = train(model, dataloader, optimizer, criterion)\r\n     10     valid_loss, valid_acc = evaluate(model, dataloader_valid, criterion)\r\n     11 \r\n\r\n in train(model, dataloader, optimizer, criterion)\r\n     10         optimizer.zero_grad()\r\n     11 \r\n---> 12         predictions = model(txt).squeeze(1)\r\n     13 \r\n     14         loss = criterion(predictions,label)\r\n\r\n~\\.conda\\envs\\matura-ml\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\r\n    720             result = self._slow_forward(*input, **kwargs)\r\n    721         else:\r\n--> 722             result = self.forward(*input, **kwargs)\r\n    723         for hook in itertools.chain(\r\n    724                 _global_forward_hooks.values(),\r\n\r\n in forward(self, txt)\r\n     16         #txt = [sent len, batch size]\r\n     17 \r\n---> 18         embedded = self.embedding(txt)\r\n     19 \r\n     20         #embedded = [sent len, batch size, emb dim]\r\n\r\n~\\.conda\\envs\\matura-ml\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs)\r\n    720             result = self._slow_forward(*input, **kwargs)\r\n    721         else:\r\n--> 722             result = self.forward(*input, **kwargs)\r\n    723         for hook in itertools.chain(\r\n    724                 _global_forward_hooks.values(),\r\n\r\n~\\.conda\\envs\\matura-ml\\lib\\site-packages\\torch\\nn\\modules\\sparse.py in forward(self, input)\r\n    124         return F.embedding(\r\n    125             input, self.weight, self.padding_idx, self.max_norm,\r\n--> 126             self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n    127 \r\n    128     def extra_repr(self) -> str:\r\n\r\n~\\.conda\\envs\\matura-ml\\lib\\site-packages\\torch\\nn\\functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\r\n   1812         # remove once script supports set_grad_enabled\r\n   1813         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\n-> 1814     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n   1815 \r\n   1816 \r\n\r\nTypeError: embedding(): argument 'indices' (position 2) must be Tensor, not list\r\n```\r\n\r\n\r\nHow can I fix this? And are there any good \"templates\" or guides for the new dataset approach? I couldn't find anything and it's pretty hard as a newbie to convert tutorials with old code into the new code.\r\n\r\nAlso as a question, does the combination of the train and test set make sense? I trained a naive bayes and a logistic regression classifier using the whole dataset and tested it on reviews I sourced from IMDB and MAL so I thought I could increase the examples I have in my ML model combining the two.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/912", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/912/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/912/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/912/events", "html_url": "https://github.com/pytorch/text/issues/912", "id": 674943997, "node_id": "MDU6SXNzdWU2NzQ5NDM5OTc=", "number": 912, "title": "How to combine train and test set for IMDB Dataset in Torchtext / Pytorch", "user": {"login": "EmreTokyuez", "id": 52999998, "node_id": "MDQ6VXNlcjUyOTk5OTk4", "avatar_url": "https://avatars1.githubusercontent.com/u/52999998?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EmreTokyuez", "html_url": "https://github.com/EmreTokyuez", "followers_url": "https://api.github.com/users/EmreTokyuez/followers", "following_url": "https://api.github.com/users/EmreTokyuez/following{/other_user}", "gists_url": "https://api.github.com/users/EmreTokyuez/gists{/gist_id}", "starred_url": "https://api.github.com/users/EmreTokyuez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EmreTokyuez/subscriptions", "organizations_url": "https://api.github.com/users/EmreTokyuez/orgs", "repos_url": "https://api.github.com/users/EmreTokyuez/repos", "events_url": "https://api.github.com/users/EmreTokyuez/events{/privacy}", "received_events_url": "https://api.github.com/users/EmreTokyuez/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1959214834, "node_id": "MDU6TGFiZWwxOTU5MjE0ODM0", "url": "https://api.github.com/repos/pytorch/text/labels/legacy", "name": "legacy", "color": "bfd4f2", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-07T11:10:51Z", "updated_at": "2020-08-08T22:34:43Z", "closed_at": "2020-08-08T22:34:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n\r\n\r\nI want to use the examples in the test set of the IMDB Sentiment Analysis Dataset for training, as I have built my own benchmark with which I will compare the performance of various Models (my Matura Thesis)\r\n\r\nSo after trying, I got the appending working and also managed ot split it, so that I have a validation set as well. The code is the following:\r\n\r\n```\r\n from torchtext import datasets\r\n    \r\n    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\r\n    \r\n    import random\r\n    train_data, valid_data = train_data.split(random_state = random.seed(SEED))\r\n    from torch.utils.data import ConcatDataset\r\n    data_list = list()\r\n    data_list.append(train_data)\r\n    data_list.append(test_data)\r\n    train_data = ConcatDataset(data_list)\r\n    print(f'Number of validation examples: {len(valid_data)}')\r\n    print(f'Number of training examples: {len(train_data)}')\r\n```\r\n\r\nAnd I get the following split (which is my goal):\r\n\r\n```\r\nNumber of validation examples: 7500\r\nNumber of training examples: 42500\r\n```\r\n\r\nNow when I want to built the vocab with the following code, I get this error:\r\n\r\n```\r\nMAX_VOCAB_SIZE = 25_000 \r\nLABEL.build_vocab(train_data)\r\n\r\nTEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\r\n~\\.conda\\envs\\matura-ml\\lib\\collections\\__init__.py in update(*args, **kwds)\r\n    654             else:\r\n--> 655                 _count_elements(self, iterable)\r\n    656         if kwds:\r\n\r\nTypeError: 'Example' object is not iterable\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n in \r\n      1 MAX_VOCAB_SIZE = 25_000\r\n      2 \r\n----> 3 TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\r\n      4 LABEL.build_vocab(train_data)\r\n\r\n~\\.conda\\envs\\matura-ml\\lib\\site-packages\\torchtext\\data\\field.py in build_vocab(self, *args, **kwargs)\r\n    299                     counter.update(x)\r\n    300                 except TypeError:\r\n--> 301                     counter.update(chain.from_iterable(x))\r\n    302         specials = list(OrderedDict.fromkeys(\r\n    303             tok for tok in [self.unk_token, self.pad_token, self.init_token,\r\n\r\nTypeError: 'Example' object is not iterable\r\n```\r\n\r\nHow can I combine the train and test split in the correct way?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/903", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/903/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/903/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/903/events", "html_url": "https://github.com/pytorch/text/issues/903", "id": 667431465, "node_id": "MDU6SXNzdWU2Njc0MzE0NjU=", "number": 903, "title": "torchtext.experimental.vectors.FastText crashes", "user": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "Nayef211", "id": 22487263, "node_id": "MDQ6VXNlcjIyNDg3MjYz", "avatar_url": "https://avatars1.githubusercontent.com/u/22487263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nayef211", "html_url": "https://github.com/Nayef211", "followers_url": "https://api.github.com/users/Nayef211/followers", "following_url": "https://api.github.com/users/Nayef211/following{/other_user}", "gists_url": "https://api.github.com/users/Nayef211/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nayef211/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nayef211/subscriptions", "organizations_url": "https://api.github.com/users/Nayef211/orgs", "repos_url": "https://api.github.com/users/Nayef211/repos", "events_url": "https://api.github.com/users/Nayef211/events{/privacy}", "received_events_url": "https://api.github.com/users/Nayef211/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Nayef211", "id": 22487263, "node_id": "MDQ6VXNlcjIyNDg3MjYz", "avatar_url": "https://avatars1.githubusercontent.com/u/22487263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nayef211", "html_url": "https://github.com/Nayef211", "followers_url": "https://api.github.com/users/Nayef211/followers", "following_url": "https://api.github.com/users/Nayef211/following{/other_user}", "gists_url": "https://api.github.com/users/Nayef211/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nayef211/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nayef211/subscriptions", "organizations_url": "https://api.github.com/users/Nayef211/orgs", "repos_url": "https://api.github.com/users/Nayef211/repos", "events_url": "https://api.github.com/users/Nayef211/events{/privacy}", "received_events_url": "https://api.github.com/users/Nayef211/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-28T22:52:31Z", "updated_at": "2020-07-30T15:41:32Z", "closed_at": "2020-07-30T15:41:31Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\ntorchtext.experimental.vectors.FasText crashes during the construction period.\r\n\r\n**To Reproduce**\r\n```\r\nfrom torchtext.experimental.vectors import FastText; \r\nv1 = FastText()\r\n```\r\n\r\n**Expected behavior**\r\nThe above code works", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/896", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/896/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/896/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/896/events", "html_url": "https://github.com/pytorch/text/issues/896", "id": 663730481, "node_id": "MDU6SXNzdWU2NjM3MzA0ODE=", "number": 896, "title": "Cannot use the batch with TypeError", "user": {"login": "kewellyoung", "id": 34187152, "node_id": "MDQ6VXNlcjM0MTg3MTUy", "avatar_url": "https://avatars2.githubusercontent.com/u/34187152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kewellyoung", "html_url": "https://github.com/kewellyoung", "followers_url": "https://api.github.com/users/kewellyoung/followers", "following_url": "https://api.github.com/users/kewellyoung/following{/other_user}", "gists_url": "https://api.github.com/users/kewellyoung/gists{/gist_id}", "starred_url": "https://api.github.com/users/kewellyoung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kewellyoung/subscriptions", "organizations_url": "https://api.github.com/users/kewellyoung/orgs", "repos_url": "https://api.github.com/users/kewellyoung/repos", "events_url": "https://api.github.com/users/kewellyoung/events{/privacy}", "received_events_url": "https://api.github.com/users/kewellyoung/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-22T12:39:25Z", "updated_at": "2020-07-22T15:09:54Z", "closed_at": "2020-07-22T13:29:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\nI've load a dataset and try to generate  an iterator with torchtext. When I get the iterator and want to use it, there is TypeError as below\r\n\r\n**Description**\r\nHere is my code:\r\n\r\n\r\n`from torchtext import data, vocab`\r\n\r\n`class DataLoader():\r\n\r\n    def __init__(self): \r\n        self.CONTENT = data.Field(sequential=True, \r\n                                  tokenize=lambda text:list(jieba.cut(text, cut_all=False)), \r\n                                  fix_length=150)\r\n        self.LABEL = data.Field(sequential=False, use_vocab=False)\r\n        self.field=[( 'text',self.CONTENT), ('label',self.LABEL)]        \r\n        self.train_data, self.valid_data = data.TabularDataset.splits(path='',\r\n                                        train = '%s_t_data.csv'%task,\r\n                                        validation = '%s_v_data.csv'%task,\r\n                                        format = 'csv',\r\n                                        fields = self.field)        \r\n        self.CONTENT.build_vocab(self.train_data)\r\n    def load_data(self):\r\n        batchsize = self.batchsize\r\n        train_iterator = data.BucketIterator(self.train_data, batch_size=batchsize,\r\n                                device='cuda',\r\n                                sort_within_batch=False, shuffle=True,\r\n                                 sort_key = lambda x:len(x.text))\r\n        valid_iterator = data.Iterator(self.valid_data, batch_size=batchsize,\r\n                        device='cuda', train=False,\r\n                        shuffle=False, sort=False, sort_within_batch=False)\r\n        return train_iterator, valid_iterator`\r\n\r\nobj = DataLoader()\r\nt,e = obj.load_data()\r\nfor batch in e: break\r\n\r\nThen I got:\r\n\r\n> ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-24-d67fe0c6e293> in <module>\r\n----> 1 for batch in e:\r\n      2     break\r\n\r\n/usr/local/miniconda3/envs/dl/lib/python3.6/site-packages/torchtext/data/iterator.py in __iter__(self)\r\n    154                     else:\r\n    155                         minibatch.sort(key=self.sort_key, reverse=True)\r\n--> 156                 yield Batch(minibatch, self.dataset, self.device)\r\n    157             if not self.repeat:\r\n    158                 return\r\n\r\n/usr/local/miniconda3/envs/dl/lib/python3.6/site-packages/torchtext/data/batch.py in __init__(self, data, dataset, device)\r\n     32                 if field is not None:\r\n     33                     batch = [getattr(x, name) for x in data]\r\n---> 34                     setattr(self, name, field.process(batch, device=device))\r\n     35 \r\n     36     @classmethod\r\n\r\n/usr/local/miniconda3/envs/dl/lib/python3.6/site-packages/torchtext/data/field.py in process(self, batch, device)\r\n    234             and custom postprocessing Pipeline.\r\n    235         \"\"\"\r\n--> 236         padded = self.pad(batch)\r\n    237         tensor = self.numericalize(padded, device=device)\r\n    238         return tensor\r\n\r\n/usr/local/miniconda3/envs/dl/lib/python3.6/site-packages/torchtext/data/field.py in pad(self, minibatch)\r\n    269                     + list(x[-max_len:] if self.truncate_first else x[:max_len])\r\n    270                     + ([] if self.eos_token is None else [self.eos_token])\r\n--> 271                     + [self.pad_token] * max(0, max_len - len(x)))\r\n    272             lengths.append(len(padded[-1]) - max(0, max_len - len(x)))\r\n    273         if self.include_lengths:\r\n\r\nTypeError: 'generator' object is not subscriptable\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/892", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/892/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/892/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/892/events", "html_url": "https://github.com/pytorch/text/issues/892", "id": 660192805, "node_id": "MDU6SXNzdWU2NjAxOTI4MDU=", "number": 892, "title": "Request for configurable delimiter in Tabular Dataset", "user": {"login": "harshit-py", "id": 28984166, "node_id": "MDQ6VXNlcjI4OTg0MTY2", "avatar_url": "https://avatars3.githubusercontent.com/u/28984166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshit-py", "html_url": "https://github.com/harshit-py", "followers_url": "https://api.github.com/users/harshit-py/followers", "following_url": "https://api.github.com/users/harshit-py/following{/other_user}", "gists_url": "https://api.github.com/users/harshit-py/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshit-py/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshit-py/subscriptions", "organizations_url": "https://api.github.com/users/harshit-py/orgs", "repos_url": "https://api.github.com/users/harshit-py/repos", "events_url": "https://api.github.com/users/harshit-py/events{/privacy}", "received_events_url": "https://api.github.com/users/harshit-py/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-18T14:51:18Z", "updated_at": "2020-08-01T15:49:10Z", "closed_at": "2020-08-01T15:49:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nHi\r\n\r\nWould like to have additional delimiters while reading flat files as Tabular Dataset. Current supported delimiters are ',' (default) and '\\t'\r\n\r\n**Motivation**\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nA lot of raw text data when preprocessed requires special delimiters like '@' or '^' while dumping them so as to not have ',' or even '\\t' misinterpreted as a delimiter contained in the text.\r\n\r\n**Pitch**\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nI see TabularDataset constructor accepts 'CSV', 'TSV' or 'JSON' for 'format'. This could be generalised as 'format' only accepting 'CSV' or 'JSON' and having additional kwarg 'delimiter' when 'format' == 'CSV'. This would then be passed to the csv.reader via utils.unicode_csv_reader function.\r\n\r\n**Alternatives**\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\nCurrently only way around this is to manually change the delimiter in the raw file which might not always work.\r\n\r\n**Additional context**\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/890", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/890/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/890/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/890/events", "html_url": "https://github.com/pytorch/text/issues/890", "id": 658726458, "node_id": "MDU6SXNzdWU2NTg3MjY0NTg=", "number": 890, "title": "special tokens are not allowed in ordered_dict for torchtext.experimental.Vocab", "user": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-17T01:20:06Z", "updated_at": "2020-07-17T20:05:38Z", "closed_at": "2020-07-17T20:05:38Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug/Question\r\n**Describe the bug**\r\nThe constructor of `torchtext.experimental.Vocab` checks if special tokens exist in `ordered_dict`. If so, it will crash. However, it's common that special tokens are defined in `ordered_dict`. For example, in Hugging Face's library, several special tokens, like `UNK`, `PAD`, etc, are provided in the [vocab.txt](https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-small-generator/vocab.txt) file, which will be loaded to `ordered_dict`.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```\r\nfrom torchtext.experimental.vocab import Vocab\r\nfrom collections import OrderedDict\r\ntokens = ['<unk>', 'e', 'd', 'c', 'b', 'a']\r\nv2 = Vocab(OrderedDict([(token, 1) for token in tokens]))\r\n```\r\n\r\n**Proposed solutions**\r\nTo fix this issue, I propose to remove `special_tokens` argument. Instead, when constructing the vocab, we only need to check that `unk_token` exists in `ordered_dict`. `special_tokens` is around and attached to the beginning or end of the token lists. IMO, that's the only reason to have `special_tokens`. Since we support `Vocab.insert_token` and `Vocab.append_token`, we can insert those special tokens later on if necessary.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/879", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/879/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/879/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/879/events", "html_url": "https://github.com/pytorch/text/issues/879", "id": 655256840, "node_id": "MDU6SXNzdWU2NTUyNTY4NDA=", "number": 879, "title": "Is it possible to filter examples in torchtext.datasets, instead of using all of them?", "user": {"login": "joelowj", "id": 13001361, "node_id": "MDQ6VXNlcjEzMDAxMzYx", "avatar_url": "https://avatars0.githubusercontent.com/u/13001361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joelowj", "html_url": "https://github.com/joelowj", "followers_url": "https://api.github.com/users/joelowj/followers", "following_url": "https://api.github.com/users/joelowj/following{/other_user}", "gists_url": "https://api.github.com/users/joelowj/gists{/gist_id}", "starred_url": "https://api.github.com/users/joelowj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joelowj/subscriptions", "organizations_url": "https://api.github.com/users/joelowj/orgs", "repos_url": "https://api.github.com/users/joelowj/repos", "events_url": "https://api.github.com/users/joelowj/events{/privacy}", "received_events_url": "https://api.github.com/users/joelowj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1959214834, "node_id": "MDU6TGFiZWwxOTU5MjE0ODM0", "url": "https://api.github.com/repos/pytorch/text/labels/legacy", "name": "legacy", "color": "bfd4f2", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-11T18:28:35Z", "updated_at": "2020-07-13T13:26:51Z", "closed_at": "2020-07-13T10:02:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\nIs there a way to leverage on torchtext.datasets and setting a length filter on the sentences? For example, when using the IMDB dataset I might not when to use the whole dataset but rather only sentences with length less than 15. \r\n\r\nIf not, can we consider this to be a feature? Often times, researcher wants to iterate on their models quickly on their own machine. This also means that they might not want to commit to writing their own data loader. However, loading the whole dataset also means that we might not have the hardware to support the memory needs. Happy to hear the thoughts of the community.\r\n\r\nThanks and I appreciate any help provided.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/878", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/878/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/878/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/878/events", "html_url": "https://github.com/pytorch/text/issues/878", "id": 655015333, "node_id": "MDU6SXNzdWU2NTUwMTUzMzM=", "number": 878, "title": "[do not delete] testing probot", "user": {"login": "astaff", "id": 1032174, "node_id": "MDQ6VXNlcjEwMzIxNzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1032174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/astaff", "html_url": "https://github.com/astaff", "followers_url": "https://api.github.com/users/astaff/followers", "following_url": "https://api.github.com/users/astaff/following{/other_user}", "gists_url": "https://api.github.com/users/astaff/gists{/gist_id}", "starred_url": "https://api.github.com/users/astaff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/astaff/subscriptions", "organizations_url": "https://api.github.com/users/astaff/orgs", "repos_url": "https://api.github.com/users/astaff/repos", "events_url": "https://api.github.com/users/astaff/events{/privacy}", "received_events_url": "https://api.github.com/users/astaff/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2141426289, "node_id": "MDU6TGFiZWwyMTQxNDI2Mjg5", "url": "https://api.github.com/repos/pytorch/text/labels/Windows", "name": "Windows", "color": "7082ea", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-10T20:21:02Z", "updated_at": "2020-07-13T22:05:32Z", "closed_at": "2020-07-13T22:05:32Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "\n\ncc @astaff", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/868", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/868/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/868/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/868/events", "html_url": "https://github.com/pytorch/text/issues/868", "id": 651214666, "node_id": "MDU6SXNzdWU2NTEyMTQ2NjY=", "number": 868, "title": "DLL load failure on Windows", "user": {"login": "peterjc123", "id": 9998726, "node_id": "MDQ6VXNlcjk5OTg3MjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9998726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterjc123", "html_url": "https://github.com/peterjc123", "followers_url": "https://api.github.com/users/peterjc123/followers", "following_url": "https://api.github.com/users/peterjc123/following{/other_user}", "gists_url": "https://api.github.com/users/peterjc123/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterjc123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterjc123/subscriptions", "organizations_url": "https://api.github.com/users/peterjc123/orgs", "repos_url": "https://api.github.com/users/peterjc123/repos", "events_url": "https://api.github.com/users/peterjc123/events{/privacy}", "received_events_url": "https://api.github.com/users/peterjc123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-06T03:05:23Z", "updated_at": "2020-07-06T04:55:04Z", "closed_at": "2020-07-06T04:54:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n```\r\n(base) circleci@PACKER-5EFB90C1 C:\\Users\\circleci\\project>pip install --pre torch torchvision torchaudio torchtext -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html \r\nLooking in links: https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\r\nCollecting torch\r\n  Downloading https://download.pytorch.org/whl/nightly/cu102/torch-1.7.0.dev20200702-cp37-cp37m-win_amd64.whl (1078.9 MB)\r\nCollecting torchvision\r\n  Downloading https://download.pytorch.org/whl/nightly/cu102/torchvision-0.8.0.dev20200701-cp37-cp37m-win_amd64.whl (1.1 MB)\r\nCollecting torchaudio\r\n  Downloading https://download.pytorch.org/whl/nightly/torchaudio-0.7.0.dev20200701-cp37-none-win_amd64.whl (83 kB)\r\nCollecting torchtext\r\n  Downloading https://download.pytorch.org/whl/nightly/torchtext-0.8.0.dev20200701-cp37-cp37m-win_amd64.whl (1.4 MB)\r\nCollecting numpy\r\n  Downloading numpy-1.19.0-cp37-cp37m-win_amd64.whl (13.0 MB)\r\nCollecting future\r\n  Downloading future-0.18.2.tar.gz (829 kB)\r\nCollecting pillow>=4.1.1\r\n  Downloading Pillow-7.2.0-cp37-cp37m-win_amd64.whl (2.1 MB)\r\nCollecting sentencepiece\r\n  Downloading sentencepiece-0.1.91-cp37-cp37m-win_amd64.whl (1.2 MB)\r\nRequirement already satisfied: tqdm in c:\\users\\circleci\\project\\conda\\lib\\site-packages (from torchtext) (4.46.0)\r\nRequirement already satisfied: requests in c:\\users\\circleci\\project\\conda\\lib\\site-packages (from torchtext) (2.23.0)\r\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\circleci\\project\\conda\\lib\\site-packages (from requests->torchtext) (2.9)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\circleci\\project\\conda\\lib\\site-packages (from requests->torchtext) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\circleci\\project\\conda\\lib\\site-packages (from requests->torchtext) (1.25.8)\r\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\circleci\\project\\conda\\lib\\site-packages (from requests->torchtext) (2020.4.5.1)\r\nBuilding wheels for collected packages: future\r\n  Building wheel for future (setup.py): started\r\n  Building wheel for future (setup.py): finished with status 'done'\r\n  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491062 sha256=8fc90b1a20bac73666027a32891d6135a0eb367649a2d7bff47e0b1b5adbb888\r\n  Stored in directory: c:\\users\\circleci\\appdata\\local\\pip\\cache\\wheels\\56\\b0\\fe\\4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\r\nSuccessfully built future\r\nERROR: torchvision 0.8.0.dev20200701 has requirement torch==1.7.0.dev20200701, but you'll have torch 1.7.0.dev20200702 which is incompatible.\r\nERROR: torchaudio 0.7.0.dev20200701 has requirement torch==1.7.0.dev20200701+cu92, but you'll have torch 1.7.0.dev20200702 which is incompatible.\r\nInstalling collected packages: numpy, future, torch, pillow, torchvision, torchaudio, sentencepiece, torchtext\r\nSuccessfully installed future-0.18.2 numpy-1.19.0 pillow-7.2.0 sentencepiece-0.1.91 torch-1.7.0.dev20200702 torchaudio-0.7.0.dev20200701 torchtext-0.8.0.dev20200701 torchvision-0.8.0.dev20200701\r\n\r\n(base) circleci@PACKER-5EFB90C1 C:\\Users\\circleci\\project\\testcases>python -c \"import torch\" \r\n\r\n(base) circleci@PACKER-5EFB90C1 C:\\Users\\circleci\\project\\testcases>python -c \"import torchtext\" \r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\circleci\\project\\conda\\lib\\site-packages\\torchtext\\__init__.py\", line 42, in <module>\r\n    _init_extension()\r\n  File \"C:\\Users\\circleci\\project\\conda\\lib\\site-packages\\torchtext\\__init__.py\", line 38, in _init_extension\r\n    torch.ops.load_library(ext_specs.origin)\r\n  File \"C:\\Users\\circleci\\project\\conda\\lib\\site-packages\\torch\\_ops.py\", line 105, in load_library\r\n    ctypes.CDLL(path)\r\n  File \"C:\\Users\\circleci\\project\\conda\\lib\\ctypes\\__init__.py\", line 364, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 127] The specified procedure could not be found\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. pip install --pre torch torchtext -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html\r\n2. python -c \"import torchtext\"\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\nImporting the library doesn't fail.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/861", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/861/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/861/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/861/events", "html_url": "https://github.com/pytorch/text/issues/861", "id": 647719925, "node_id": "MDU6SXNzdWU2NDc3MTk5MjU=", "number": 861, "title": "Why does torchtext.data.Batch.to() not exist?", "user": {"login": "awaelchli", "id": 5495193, "node_id": "MDQ6VXNlcjU0OTUxOTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5495193?v=4", "gravatar_id": "", "url": "https://api.github.com/users/awaelchli", "html_url": "https://github.com/awaelchli", "followers_url": "https://api.github.com/users/awaelchli/followers", "following_url": "https://api.github.com/users/awaelchli/following{/other_user}", "gists_url": "https://api.github.com/users/awaelchli/gists{/gist_id}", "starred_url": "https://api.github.com/users/awaelchli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/awaelchli/subscriptions", "organizations_url": "https://api.github.com/users/awaelchli/orgs", "repos_url": "https://api.github.com/users/awaelchli/repos", "events_url": "https://api.github.com/users/awaelchli/events{/privacy}", "received_events_url": "https://api.github.com/users/awaelchli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1959214834, "node_id": "MDU6TGFiZWwxOTU5MjE0ODM0", "url": "https://api.github.com/repos/pytorch/text/labels/legacy", "name": "legacy", "color": "bfd4f2", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-29T22:47:30Z", "updated_at": "2020-07-04T23:38:39Z", "closed_at": "2020-07-04T23:38:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\nI am wondering why the Batch class does not support moving the data to the device with a \".to()\" call like it is possible for tensors and nn modules. Since the attribute of this Batch object are tensors, wouldn't it be straightforward to implement this convenience method?\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/858", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/858/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/858/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/858/events", "html_url": "https://github.com/pytorch/text/issues/858", "id": 647154891, "node_id": "MDU6SXNzdWU2NDcxNTQ4OTE=", "number": 858, "title": "Customize torchtext.data.Dataset takes much time to generate dataset ", "user": {"login": "xdwang0726", "id": 16963017, "node_id": "MDQ6VXNlcjE2OTYzMDE3", "avatar_url": "https://avatars2.githubusercontent.com/u/16963017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xdwang0726", "html_url": "https://github.com/xdwang0726", "followers_url": "https://api.github.com/users/xdwang0726/followers", "following_url": "https://api.github.com/users/xdwang0726/following{/other_user}", "gists_url": "https://api.github.com/users/xdwang0726/gists{/gist_id}", "starred_url": "https://api.github.com/users/xdwang0726/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xdwang0726/subscriptions", "organizations_url": "https://api.github.com/users/xdwang0726/orgs", "repos_url": "https://api.github.com/users/xdwang0726/repos", "events_url": "https://api.github.com/users/xdwang0726/events{/privacy}", "received_events_url": "https://api.github.com/users/xdwang0726/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2169782914, "node_id": "MDU6TGFiZWwyMTY5NzgyOTE0", "url": "https://api.github.com/repos/pytorch/text/labels/new%20datasets%20and%20building%20blocks", "name": "new datasets and building blocks", "color": "0a7f5c", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2020-06-29T06:33:39Z", "updated_at": "2020-07-16T17:01:16Z", "closed_at": "2020-07-08T18:13:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\nI wrote a customized data.Dataset for multilabel classification. When I processed the data, I found that it is very slow to generate train and test using the customized dataset (it takes about 1.5s per example). I am wondering is it normal or it's something wrong with my customized dataset. \r\n\r\nCustomized data.Dataset for mulilabel classification is as follows:\r\n```\r\nclass TextMultiLabelDataset(data.Dataset):\r\n    def __init__(self, text, text_field, label_field, lbls=None, **kwargs):\r\n        # torchtext Field objects\r\n        fields = [('text', text_field), ('label', label_field)]\r\n        # for l in lbl_cols:\r\n        # fields.append((l, label_field))\r\n\r\n        is_test = True if lbls is None else False\r\n        if is_test:\r\n            pass\r\n        else:\r\n            n_labels = len(lbls)\r\n\r\n        examples = []\r\n        for i, txt in enumerate(tqdm(text)):\r\n            if not is_test:\r\n                l = lbls[i]\r\n            else:\r\n                l = [0.0] * n_labels\r\n\r\n            examples.append(data.Example.fromlist([txt, l], fields))\r\n\r\n        super(TextMultiLabelDataset, self).__init__(examples, fields, **kwargs)\r\n```\r\n```\r\nwhere text is a list of list strings that in the documents, and lbls is a list of list labels in binary. (Total number of labels ~ 20000)\r\n```\r\nexamples of text:\r\n```\r\n[[\"There are few factors more important to the mechanisms of evolution than stress. The stress response has formed as a result of natural selection...\"], [\"A 46-year-old female patient presenting with unspecific lower back pain, diffuse abdominal pain, and slightly elevated body temperature\"], ...]\r\n```\r\nexamples of lbls:\r\n```\r\n[[1 1 1 1 0 0 0 1 0 ...], [1 0 1 0 1 1 1 1 ...], ...]\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/857", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/857/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/857/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/857/events", "html_url": "https://github.com/pytorch/text/issues/857", "id": 647065732, "node_id": "MDU6SXNzdWU2NDcwNjU3MzI=", "number": 857, "title": "Add loading pre-trained word2vec in bin file to vocab.Vectors", "user": {"login": "xdwang0726", "id": 16963017, "node_id": "MDQ6VXNlcjE2OTYzMDE3", "avatar_url": "https://avatars2.githubusercontent.com/u/16963017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xdwang0726", "html_url": "https://github.com/xdwang0726", "followers_url": "https://api.github.com/users/xdwang0726/followers", "following_url": "https://api.github.com/users/xdwang0726/following{/other_user}", "gists_url": "https://api.github.com/users/xdwang0726/gists{/gist_id}", "starred_url": "https://api.github.com/users/xdwang0726/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xdwang0726/subscriptions", "organizations_url": "https://api.github.com/users/xdwang0726/orgs", "repos_url": "https://api.github.com/users/xdwang0726/repos", "events_url": "https://api.github.com/users/xdwang0726/events{/privacy}", "received_events_url": "https://api.github.com/users/xdwang0726/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-29T02:26:59Z", "updated_at": "2020-06-29T14:07:05Z", "closed_at": "2020-06-29T03:44:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\ude80 Feature\r\nRight now, vocab.Vectors could only take .w2v(actually .txt) format pre-trained word vectors. Would it be possible to add loading pre-trained vectors in .bin format as well since .bin is the most commonly used for pre-trained format (i.e. google's pre-trained word2vec is in bin file). If the pre-trained .w2v file is very big, loading from .txt will take much longer than loading from .bin file (for my case, the same pre-trained word2vec in .bin file is ~12G and in .w2v file is 24G). Thank you!   \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/855", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/855/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/855/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/855/events", "html_url": "https://github.com/pytorch/text/issues/855", "id": 646816962, "node_id": "MDU6SXNzdWU2NDY4MTY5NjI=", "number": 855, "title": "ModuleNotFoundError: No module named 'torchtext.data.metrics'", "user": {"login": "D-i-l-r-u-k-s-h-i", "id": 47185867, "node_id": "MDQ6VXNlcjQ3MTg1ODY3", "avatar_url": "https://avatars0.githubusercontent.com/u/47185867?v=4", "gravatar_id": "", "url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i", "html_url": "https://github.com/D-i-l-r-u-k-s-h-i", "followers_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/followers", "following_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/following{/other_user}", "gists_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/gists{/gist_id}", "starred_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/subscriptions", "organizations_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/orgs", "repos_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/repos", "events_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/events{/privacy}", "received_events_url": "https://api.github.com/users/D-i-l-r-u-k-s-h-i/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-06-28T02:19:45Z", "updated_at": "2020-06-29T15:35:14Z", "closed_at": "2020-06-29T15:35:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\nI was testing bleu_score in Google Colab note book and these were the precise steps taken;\r\n\r\n`!pip install torchtext`\r\n```\r\nRequirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\r\nRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.5.1+cu101)\r\nRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\r\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.9)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\r\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\r\n```\r\n\r\n`import torch`\r\n`from torchtext.data.metrics import bleu_score`\r\n```\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-10-ee5ca0897ab0> in <module>()\r\n      1 import torch\r\n----> 2 from torchtext.data.metrics import bleu_score\r\n\r\nModuleNotFoundError: No module named 'torchtext.data.metrics'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------\r\nOPEN EXAMPLESSEARCH STACK OVERFLOW\r\n```\r\n\r\nEven after installing  torchtext alone, module not found error is still there. Hoping to find a solution soon.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/843", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/843/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/843/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/843/events", "html_url": "https://github.com/pytorch/text/issues/843", "id": 644218638, "node_id": "MDU6SXNzdWU2NDQyMTg2Mzg=", "number": 843, "title": "Cannot select embedding size for experimental 6B and twitter.27B GloVe vectors", "user": {"login": "bentrevett", "id": 8006479, "node_id": "MDQ6VXNlcjgwMDY0Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/8006479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bentrevett", "html_url": "https://github.com/bentrevett", "followers_url": "https://api.github.com/users/bentrevett/followers", "following_url": "https://api.github.com/users/bentrevett/following{/other_user}", "gists_url": "https://api.github.com/users/bentrevett/gists{/gist_id}", "starred_url": "https://api.github.com/users/bentrevett/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bentrevett/subscriptions", "organizations_url": "https://api.github.com/users/bentrevett/orgs", "repos_url": "https://api.github.com/users/bentrevett/repos", "events_url": "https://api.github.com/users/bentrevett/events{/privacy}", "received_events_url": "https://api.github.com/users/bentrevett/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-23T23:51:47Z", "updated_at": "2020-06-24T18:17:33Z", "closed_at": "2020-06-24T18:17:33Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "GloVe embeddings for 6B and twitter.27B are available in multiple dimension sizes, however due to [this](https://github.com/pytorch/text/blob/master/torchtext/experimental/vectors.py#L139) line only the first file (vectors) in each of the extracted archive is loaded.\r\n\r\nThis is fine for the 420B and 840B GloVe vectors as they only contain one size of embeddings. However, for the 6B and twitter.27B  vectors there are four sizes of embeddings available, but only the first in each archive (the smallest size) is obtained from the new experimental vector class. \r\n\r\nIt would be nice to be able to get the other 6B and twitter.27B vectors. Potentially a `dim` argument can be added to the `GloVe` function?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/806", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/806/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/806/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/806/events", "html_url": "https://github.com/pytorch/text/issues/806", "id": 630459041, "node_id": "MDU6SXNzdWU2MzA0NTkwNDE=", "number": 806, "title": "torchtext and training of a Transformer", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 498907456, "node_id": "MDU6TGFiZWw0OTg5MDc0NTY=", "url": "https://api.github.com/repos/pytorch/text/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-04T02:22:17Z", "updated_at": "2020-06-04T14:14:12Z", "closed_at": "2020-06-04T03:54:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello\r\n\r\nI am a bit confused about training my pytorch Transformer.\r\nI am using the code below to pre-process the Penn Treebank corpus before I analyze it with my (non pre-trained) Transformer:\r\n\r\n```python\r\n# define the English text field\r\nTEXT_ch2 = Field(init_token = '<sos>',\r\n                 eos_token = '<eos>',\r\n                 unk_token = '<unk>',\r\n                 pad_token = '<pad>',\r\n                 fix_length = bptt,\r\n                 lower = True)\r\n\r\n# split the PennTreeBank corpus into a train, val, and test set.\r\ntrain_penn, val_penn, test_penn = torchtext.datasets.PennTreebank.splits(TEXT_ch2)\r\n\r\n# build vocabulary based on the field that we just definTVD.\r\n# (building vocabulary over all language datasets)\r\nTEXT_ch2.build_vocab(train_penn, val_penn, test_penn,\r\n                     specials=['<sos>','<eos>','<unk>','<pad>'])\r\n\r\n# BPTTIterator\r\ntrain_penn_iter, val_penn_iter, test_penn_iter = BPTTIterator.splits(\r\n            (train_penn, val_penn, test_penn),\r\n            batch_size = batch_size,\r\n            bptt_len= bptt,\r\n            sort_key=lambda x: len(x.text),\r\n            sort_within_batch = True,\r\n            shuffle = False,\r\n            device= device,\r\n            repeat=False)\r\n```\r\n\r\nMy question is, since my Penn Treebank corpus are separated into train, validation, and test sets, when I train my Transformer on the Penn Treebank corpus, I would only be training the model on the Penn Treebank train and validation sets (`train_penn_iter`), am I right?\r\n\r\nBut then if I train my Transformer only on the train and validation portions of the corpus, wouldn't this mean that my Transformer will not be trained to properly handle those tokens that are contained only in the test set? so does this mean that I need to train my Transformer on the entire Penn Treebank corpus, instead of just on the training  and validation sets? But if this is the case, what then is the point of having the `split` function to separate the corpus into training, validation, and test sets? To me, this contradicts how the test sets are normally used in machine learning.\r\n\r\nDoes it make sense to \"test\" a Transformer on the sequences that contain the tokens which it was not trained on?\r\n\r\nThank you,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/793", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/793/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/793/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/793/events", "html_url": "https://github.com/pytorch/text/issues/793", "id": 625127026, "node_id": "MDU6SXNzdWU2MjUxMjcwMjY=", "number": 793, "title": "Setting Up Nightly Build", "user": {"login": "mthrok", "id": 855818, "node_id": "MDQ6VXNlcjg1NTgxOA==", "avatar_url": "https://avatars3.githubusercontent.com/u/855818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mthrok", "html_url": "https://github.com/mthrok", "followers_url": "https://api.github.com/users/mthrok/followers", "following_url": "https://api.github.com/users/mthrok/following{/other_user}", "gists_url": "https://api.github.com/users/mthrok/gists{/gist_id}", "starred_url": "https://api.github.com/users/mthrok/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mthrok/subscriptions", "organizations_url": "https://api.github.com/users/mthrok/orgs", "repos_url": "https://api.github.com/users/mthrok/repos", "events_url": "https://api.github.com/users/mthrok/events{/privacy}", "received_events_url": "https://api.github.com/users/mthrok/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mthrok", "id": 855818, "node_id": "MDQ6VXNlcjg1NTgxOA==", "avatar_url": "https://avatars3.githubusercontent.com/u/855818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mthrok", "html_url": "https://github.com/mthrok", "followers_url": "https://api.github.com/users/mthrok/followers", "following_url": "https://api.github.com/users/mthrok/following{/other_user}", "gists_url": "https://api.github.com/users/mthrok/gists{/gist_id}", "starred_url": "https://api.github.com/users/mthrok/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mthrok/subscriptions", "organizations_url": "https://api.github.com/users/mthrok/orgs", "repos_url": "https://api.github.com/users/mthrok/repos", "events_url": "https://api.github.com/users/mthrok/events{/privacy}", "received_events_url": "https://api.github.com/users/mthrok/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mthrok", "id": 855818, "node_id": "MDQ6VXNlcjg1NTgxOA==", "avatar_url": "https://avatars3.githubusercontent.com/u/855818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mthrok", "html_url": "https://github.com/mthrok", "followers_url": "https://api.github.com/users/mthrok/followers", "following_url": "https://api.github.com/users/mthrok/following{/other_user}", "gists_url": "https://api.github.com/users/mthrok/gists{/gist_id}", "starred_url": "https://api.github.com/users/mthrok/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mthrok/subscriptions", "organizations_url": "https://api.github.com/users/mthrok/orgs", "repos_url": "https://api.github.com/users/mthrok/repos", "events_url": "https://api.github.com/users/mthrok/events{/privacy}", "received_events_url": "https://api.github.com/users/mthrok/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-05-26T19:16:42Z", "updated_at": "2020-05-29T01:10:02Z", "closed_at": "2020-05-29T01:10:01Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Remaining TODOs\r\n- [x] Set up cron job [link](https://www.internalfb.com/intern/chronos/job/?jobname=torchtext%20nightly%20trigger&smc=chronos_gp_admin_client)\r\n- [x] Create PR #792 \r\n- [x] Set up Docker hub repo -> https://hub.docker.com/repository/docker/pytorch/torchtext_smoke_base/tags\r\n- [x] Update CI config https://github.com/pytorch/text/pull/794\r\n- [x] Manual push and test\r\n- Fixes\r\n  - [x] https://github.com/pytorch/text/pull/800\r\n  - [x] https://github.com/pytorch/text/pull/801\r\n  - [x] https://github.com/pytorch/text/pull/803", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/791", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/791/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/791/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/791/events", "html_url": "https://github.com/pytorch/text/issues/791", "id": 625038364, "node_id": "MDU6SXNzdWU2MjUwMzgzNjQ=", "number": 791, "title": "Regular Expression not JIT-able", "user": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-26T17:00:26Z", "updated_at": "2020-06-05T04:45:31Z", "closed_at": "2020-06-05T04:45:31Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "## \ud83d\ude80 Feature\r\nThe `re` library in python is not JIT-able, which is used by the tokenizers in torchtext. \r\n\r\n**Motivation**\r\n\r\nWith a JIT-able `re` library, the `basic_english` tokenizer will have JIT support.\r\n\r\n```\r\nimport torch\r\nimport torchtext\r\nfrom typing import List\r\nimport re\r\n\r\ndef regex_tokenizer(line: str) -> List[str]:\r\n\tline = line.lower()\t\r\n\tline = re.compile(r'\\\"').sub('', line)\r\n\tline = re.compile(r'\\(').sub(' ( ', line)\r\n\treturn line.split()\r\n\t\r\njit_tokenizer = torch.jit.script(regex_tokenizer)\r\n```\r\n\r\nError:\r\n```\r\nUnsupportedNodeError: try blocks aren't supported:\r\n  File \"/private/home/zhangguanheng/anaconda3/lib/python3.7/re.py\", line 275\r\n    if isinstance(flags, RegexFlag):\r\n        flags = flags.value\r\n    try:\r\n    ~~~ <--- HERE\r\n        return _cache[type(pattern), pattern, flags]\r\n    except KeyError:\r\n'compile' is being compiled since it was called from 'regex_tokenizer'\r\n  File \"<ipython-input-1-e544d1c55cf5>\", line 8\r\ndef regex_tokenizer(line: str) -> List[str]:\r\n        line = line.lower()\r\n        line = re.compile(r'\\\"').sub('', line)\r\n ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n        line = re.compile(r'\\(').sub(' ( ', line)\r\n        return line.split()\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/770", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/770/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/770/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/770/events", "html_url": "https://github.com/pytorch/text/issues/770", "id": 617858394, "node_id": "MDU6SXNzdWU2MTc4NTgzOTQ=", "number": 770, "title": "Fastest way to load pre-trained vectors?", "user": {"login": "thvasilo", "id": 9048995, "node_id": "MDQ6VXNlcjkwNDg5OTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/9048995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thvasilo", "html_url": "https://github.com/thvasilo", "followers_url": "https://api.github.com/users/thvasilo/followers", "following_url": "https://api.github.com/users/thvasilo/following{/other_user}", "gists_url": "https://api.github.com/users/thvasilo/gists{/gist_id}", "starred_url": "https://api.github.com/users/thvasilo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thvasilo/subscriptions", "organizations_url": "https://api.github.com/users/thvasilo/orgs", "repos_url": "https://api.github.com/users/thvasilo/repos", "events_url": "https://api.github.com/users/thvasilo/events{/privacy}", "received_events_url": "https://api.github.com/users/thvasilo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-14T01:32:47Z", "updated_at": "2020-05-20T18:12:38Z", "closed_at": "2020-05-20T18:12:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using Docker to create a container that will run a task using torchtext.\r\n\r\nFor one of those I'm planning to use one specific set of vectors, say glove-6b-100. What I'd like to do is make those available quickly and fast to the container, by perhaps copying the actual file, vs. relying on the mechanism that downloads and unzips the original vectors as that is taking too long.\r\n\r\nIs there an easy way assuming that I have access to the embedding text file to load the vectors into my `Vocab` object?\r\n\r\nMy current assumption is to do something like\r\n\r\n```\r\n# Prepare sensitive data\r\nTEXT = Field(sequential=True, tokenize=\"basic_english\", include_lengths=True)\r\n\r\nLABEL = data.LabelField(dtype=torch.float)\r\n\r\nfields = [('text', TEXT),\r\n              ('label', LABEL)]\r\n\r\ntext_data = TabularDataset(\r\n        path='path-to-file.csv',\r\n        fields=fields,\r\n        skip_header=False)\r\n\r\nTEXT.build_vocab(text_data)\r\n\r\nmy_vectors = Vectors(name=\"my-embedding-file.txt\")\r\n\r\nTEXT.vocab.load_vectors(my_vectors)\r\n```\r\n\r\nWould that work? I think that TEXT.build_vocab would require me to provide a vectors object?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/760", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/760/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/760/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/760/events", "html_url": "https://github.com/pytorch/text/issues/760", "id": 614998072, "node_id": "MDU6SXNzdWU2MTQ5OTgwNzI=", "number": 760, "title": "Is the `.data` directory supposed to be auto-created when using the experimental datasets?", "user": {"login": "thvasilo", "id": 9048995, "node_id": "MDQ6VXNlcjkwNDg5OTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/9048995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thvasilo", "html_url": "https://github.com/thvasilo", "followers_url": "https://api.github.com/users/thvasilo/followers", "following_url": "https://api.github.com/users/thvasilo/following{/other_user}", "gists_url": "https://api.github.com/users/thvasilo/gists{/gist_id}", "starred_url": "https://api.github.com/users/thvasilo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thvasilo/subscriptions", "organizations_url": "https://api.github.com/users/thvasilo/orgs", "repos_url": "https://api.github.com/users/thvasilo/repos", "events_url": "https://api.github.com/users/thvasilo/events{/privacy}", "received_events_url": "https://api.github.com/users/thvasilo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2048817896, "node_id": "MDU6TGFiZWwyMDQ4ODE3ODk2", "url": "https://api.github.com/repos/pytorch/text/labels/minor", "name": "minor", "color": "5df4d3", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-08T21:31:31Z", "updated_at": "2020-05-31T18:15:17Z", "closed_at": "2020-05-31T18:15:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I just tried running the following on a fresh instance:\r\n\r\n```python\r\nimport torch\r\nimport torchtext\r\n\r\n# import datasets\r\nfrom torchtext.experimental.datasets import IMDB\r\n\r\nSEED = 11\r\n\r\ntorch.manual_seed(SEED)\r\ntorch.backends.cudnn.deterministic = True\r\n\r\n# use the default tokenizer\r\ntrain_dataset, test_dataset = IMDB()\r\nvocab = train_dataset.get_vocab()\r\n```\r\n\r\nand got the error `RuntimeError: Download directory .data does not exist. Did you create it?\r\n`\r\n\r\nI expected that the library would create the directory on its own, like it does for the `.vector_cache` directory. Is that not the case?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/758", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/758/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/758/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/758/events", "html_url": "https://github.com/pytorch/text/issues/758", "id": 613680371, "node_id": "MDU6SXNzdWU2MTM2ODAzNzE=", "number": 758, "title": "How to apply Torchtext  convenience classes to prepare data for a Transformer", "user": {"login": "Ceceu", "id": 11181748, "node_id": "MDQ6VXNlcjExMTgxNzQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/11181748?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ceceu", "html_url": "https://github.com/Ceceu", "followers_url": "https://api.github.com/users/Ceceu/followers", "following_url": "https://api.github.com/users/Ceceu/following{/other_user}", "gists_url": "https://api.github.com/users/Ceceu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ceceu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ceceu/subscriptions", "organizations_url": "https://api.github.com/users/Ceceu/orgs", "repos_url": "https://api.github.com/users/Ceceu/repos", "events_url": "https://api.github.com/users/Ceceu/events{/privacy}", "received_events_url": "https://api.github.com/users/Ceceu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-06T23:47:51Z", "updated_at": "2020-05-07T13:54:37Z", "closed_at": "2020-05-07T13:54:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nReading the tutorial [Language Translation with torchText](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html) I wondered how someone could use those convenience classes (`Field, BucketIterator`) to train/fine-tune a `Transformer` such as those available at [Huggingface](https://github.com/huggingface/transformers).\r\n\r\nFor instance, I'm currently working with a large dataset distributed in jsonl files which looks like:\r\n```python\r\n{ \"query\": \"this is a query 1\", \"doc\": \"relevant document regarding query 1\" },\r\n{ \"query\": \"this is a query 2\", \"doc\": \"relevant document regarding query 2\" },\r\n               ...\r\n \r\n```\r\n\r\nNow, to forward this data into a transformer like Bert, it is necessary to convert this dataset into the format:\r\n\r\n```python3\r\n(\r\n#queries\r\n {\r\n    'input_ids': tensor([\r\n        [  101,  2023,  2003,  1037, 23032,  1015,   102,     0],\r\n        [  101,  2023,  2003,  1037, 23032,  1016,   102,     0]]), \r\n    'attention_mask': tensor([\r\n        [1, 1, 1, 1, 1, 1, 1, 0],\r\n        [1, 1, 1, 1, 1, 1, 1, 0]])\r\n }, \r\n\r\n #docs\r\n {\r\n    'input_ids': tensor([\r\n        [ 101, 2023, 2003, 2028, 7882, 6254, 4953,  102],\r\n        [ 101, 2023, 2003, 2028, 7882, 6254, 4953,  102]]), \r\n    'attention_mask': 'input_ids': tensor([\r\n        [1, 1, 1, 1, 1, 1, 1, 1],\r\n        [1, 1, 1, 1, 1, 1, 1, 1]])\r\n}\r\n```\r\nSo, what would be a clear and efficient approach to apply those convenience classes to tokenize a text dataset to fit it in the required format of a transformer?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/754", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/754/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/754/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/754/events", "html_url": "https://github.com/pytorch/text/issues/754", "id": 612634011, "node_id": "MDU6SXNzdWU2MTI2MzQwMTE=", "number": 754, "title": "HTTP 301 when calling FastText()", "user": {"login": "TalitaAnthonio", "id": 25078987, "node_id": "MDQ6VXNlcjI1MDc4OTg3", "avatar_url": "https://avatars3.githubusercontent.com/u/25078987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TalitaAnthonio", "html_url": "https://github.com/TalitaAnthonio", "followers_url": "https://api.github.com/users/TalitaAnthonio/followers", "following_url": "https://api.github.com/users/TalitaAnthonio/following{/other_user}", "gists_url": "https://api.github.com/users/TalitaAnthonio/gists{/gist_id}", "starred_url": "https://api.github.com/users/TalitaAnthonio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TalitaAnthonio/subscriptions", "organizations_url": "https://api.github.com/users/TalitaAnthonio/orgs", "repos_url": "https://api.github.com/users/TalitaAnthonio/repos", "events_url": "https://api.github.com/users/TalitaAnthonio/events{/privacy}", "received_events_url": "https://api.github.com/users/TalitaAnthonio/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-05T14:10:59Z", "updated_at": "2020-07-21T15:10:02Z", "closed_at": "2020-05-05T14:27:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\n\r\nCalling FastText() results to HTTP Error 301: Moved Permanently \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nfrom torchtext.vocab import FastText\r\n\r\nembeddings = FastText()\r\n```\r\n\r\n**Expected behavior**\r\nMaking a call to FastText() should download the FastText embeddings. \r\n\r\n**Screenshots**\r\n<img width=\"939\" alt=\"Schermafbeelding 2020-05-05 om 16 01 14\" src=\"https://user-images.githubusercontent.com/25078987/81074749-bdbc9b80-8ee9-11ea-9d6a-98aca69e41a0.png\">\r\n\r\n**Environment**\r\n\r\nPyTorch version: 1.5.0+cu101\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.3\r\n[pip3] torch==1.5.0+cu101\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.6.0+cu101\r\n[conda] Could not collect\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/727", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/727/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/727/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/727/events", "html_url": "https://github.com/pytorch/text/issues/727", "id": 596075984, "node_id": "MDU6SXNzdWU1OTYwNzU5ODQ=", "number": 727, "title": "BLEU Score example does not work", "user": {"login": "aigagror", "id": 19999715, "node_id": "MDQ6VXNlcjE5OTk5NzE1", "avatar_url": "https://avatars3.githubusercontent.com/u/19999715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aigagror", "html_url": "https://github.com/aigagror", "followers_url": "https://api.github.com/users/aigagror/followers", "following_url": "https://api.github.com/users/aigagror/following{/other_user}", "gists_url": "https://api.github.com/users/aigagror/gists{/gist_id}", "starred_url": "https://api.github.com/users/aigagror/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aigagror/subscriptions", "organizations_url": "https://api.github.com/users/aigagror/orgs", "repos_url": "https://api.github.com/users/aigagror/repos", "events_url": "https://api.github.com/users/aigagror/events{/privacy}", "received_events_url": "https://api.github.com/users/aigagror/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-07T18:45:09Z", "updated_at": "2020-04-08T14:46:29Z", "closed_at": "2020-04-08T14:46:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "For the `torchtext.data.metrics.bleu_score` [documentation](https://pytorch.org/text/data_metrics.html)\r\nThe example does not work.\r\nIf you run their example:\r\n```\r\nfrom torchtext.data.metrics import bleu_score\r\ncandidate_corpus = [['I', 'ate', 'the', 'apple'], ['I', 'did']]\r\nreferences_corpus = [[['I', 'ate', 'it'], ['I', 'ate', 'apples']],\r\n        [['I', 'did']]]\r\nbleu_score(candidate_corpus, references_corpus)\r\n```\r\n\r\nYou get `0` instead of `0.7598356856515925`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/725", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/725/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/725/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/725/events", "html_url": "https://github.com/pytorch/text/issues/725", "id": 595450249, "node_id": "MDU6SXNzdWU1OTU0NTAyNDk=", "number": 725, "title": "[HELP WANTED] Clean up python 2 coding", "user": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 498907454, "node_id": "MDU6TGFiZWw0OTg5MDc0NTQ=", "url": "https://api.github.com/repos/pytorch/text/labels/help%20wanted", "name": "help wanted", "color": "128A0C", "default": true, "description": null}, {"id": 1383683717, "node_id": "MDU6TGFiZWwxMzgzNjgzNzE3", "url": "https://api.github.com/repos/pytorch/text/labels/high%20priority", "name": "high priority", "color": "b1f74f", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-06T21:48:34Z", "updated_at": "2020-04-24T13:49:10Z", "closed_at": "2020-04-24T13:49:10Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "## \ud83d\ude80 Feature\r\n\r\nSince we are dropping the python2 support, we should clean up python 2 coding style. For example, the dependency on six should be removed. PRs from OSS are welcome and will be merged.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/724", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/724/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/724/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/724/events", "html_url": "https://github.com/pytorch/text/issues/724", "id": 595182336, "node_id": "MDU6SXNzdWU1OTUxODIzMzY=", "number": 724, "title": "FileNotFoundError", "user": {"login": "anweshpanda", "id": 60091167, "node_id": "MDQ6VXNlcjYwMDkxMTY3", "avatar_url": "https://avatars0.githubusercontent.com/u/60091167?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anweshpanda", "html_url": "https://github.com/anweshpanda", "followers_url": "https://api.github.com/users/anweshpanda/followers", "following_url": "https://api.github.com/users/anweshpanda/following{/other_user}", "gists_url": "https://api.github.com/users/anweshpanda/gists{/gist_id}", "starred_url": "https://api.github.com/users/anweshpanda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anweshpanda/subscriptions", "organizations_url": "https://api.github.com/users/anweshpanda/orgs", "repos_url": "https://api.github.com/users/anweshpanda/repos", "events_url": "https://api.github.com/users/anweshpanda/events{/privacy}", "received_events_url": "https://api.github.com/users/anweshpanda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1959214834, "node_id": "MDU6TGFiZWwxOTU5MjE0ODM0", "url": "https://api.github.com/repos/pytorch/text/labels/legacy", "name": "legacy", "color": "bfd4f2", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-06T14:51:14Z", "updated_at": "2020-04-07T05:48:52Z", "closed_at": "2020-04-07T05:48:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to execute the following code in google colab:::\r\n.................................................\r\nTEXT = data.Field(tokenize = 'spacy', lower = True)\r\nLABEL = data.LabelField()\r\ntrain_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)\r\n....................................................\r\nHowever, I am getting the following error:::\r\n....................................................\r\nFileNotFoundError Traceback (most recent call last)\r\nin ()\r\n----> 1 train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/torchtext/data/dataset.py in init(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\r\n249 'tsv': Example.fromCSV, 'csv': Example.fromCSV}[format]\r\n250\r\n--> 251 with io.open(os.path.expanduser(path), encoding=\"utf8\") as f:\r\n252 if format == 'csv':\r\n253 reader = unicode_csv_reader(f, **csv_reader_params)\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '.data/snli/snli_1.0/snli_1.0_train.jsonl'\r\n.....................................................\r\n\r\nCan anyone please help?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/717", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/717/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/717/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/717/events", "html_url": "https://github.com/pytorch/text/issues/717", "id": 591676082, "node_id": "MDU6SXNzdWU1OTE2NzYwODI=", "number": 717, "title": "[Dependency Bug]`sentencepiece` is not installed as a dependency in conda", "user": {"login": "z-a-f", "id": 4216323, "node_id": "MDQ6VXNlcjQyMTYzMjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/4216323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/z-a-f", "html_url": "https://github.com/z-a-f", "followers_url": "https://api.github.com/users/z-a-f/followers", "following_url": "https://api.github.com/users/z-a-f/following{/other_user}", "gists_url": "https://api.github.com/users/z-a-f/gists{/gist_id}", "starred_url": "https://api.github.com/users/z-a-f/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/z-a-f/subscriptions", "organizations_url": "https://api.github.com/users/z-a-f/orgs", "repos_url": "https://api.github.com/users/z-a-f/repos", "events_url": "https://api.github.com/users/z-a-f/events{/privacy}", "received_events_url": "https://api.github.com/users/z-a-f/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-01T07:00:03Z", "updated_at": "2020-04-10T20:27:50Z", "closed_at": "2020-04-10T20:27:49Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\nAfter installing the `torchtext`, and trying the code below (in the repro section) an error pops up: `ModuleNotFoundError: No module named 'sentencepiece'`. I think the package should be installed automatically as a dependency.\r\n\r\nThe problem is that the depndency on `sentencepiece` is specified for pip, but not for conda (see attachments)\r\n\r\n**To Reproduce**\r\n\r\nAfter installing using\r\n\r\n```console\r\nconda install -c pytorch torchtext\r\n```\r\n\r\nrun the following:\r\n\r\n```python\r\nfrom torchtext import data\r\nfrom torchtext import datasets\r\n\r\nimdb_set = datasets.IMDB(DATA_PATH,\r\n                         text_field=data.Field(),\r\n                         label_field=data.LabelField())\r\n```\r\n\r\n```console\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-15-019e470b76d6> in <module>\r\n----> 1 from torchtext import data\r\n      2 from torchtext import datasets\r\n      3 \r\n      4 imdb_set = datasets.IMDB(DATA_PATH,\r\n      5                          text_field=data.Field(),\r\n\r\n~/miniconda3/envs/py37-ml/lib/python3.7/site-packages/torchtext/__init__.py in <module>\r\n----> 1 from . import data\r\n      2 from . import datasets\r\n      3 from . import utils\r\n      4 from . import vocab\r\n      5 from . import experimental\r\n\r\n~/miniconda3/envs/py37-ml/lib/python3.7/site-packages/torchtext/data/__init__.py in <module>\r\n      8 from .pipeline import Pipeline\r\n      9 from .utils import get_tokenizer, interleave_keys\r\n---> 10 from .functional import generate_sp_model, \\\r\n     11     load_sp_model, \\\r\n     12     sentencepiece_numericalizer, \\\r\n\r\n~/miniconda3/envs/py37-ml/lib/python3.7/site-packages/torchtext/data/functional.py in <module>\r\n----> 1 import sentencepiece as spm\r\n      2 import re\r\n      3 \r\n      4 __all__ = [\r\n      5     \"generate_sp_model\", \"load_sp_model\",\r\n\r\nModuleNotFoundError: No module named 'sentencepiece'\r\n\r\n```\r\n\r\n**Expected behavior**\r\n\r\nIf some core part of the `torchtext` package is using a third-party package, it should be added as a dependency.\r\n\r\n**Screenshots**\r\n\r\n`conda search --info torchtext` doesn't list `sentencepiece` as a dependency.\r\n\r\n![image](https://user-images.githubusercontent.com/4216323/78107971-4c9a3d80-73ab-11ea-9c87-cff345ae03ad.png)\r\n\r\nHowever, `pip show torchtext` does:\r\n\r\n![image](https://user-images.githubusercontent.com/4216323/78108058-7bb0af00-73ab-11ea-82e0-cde4af77219b.png)\r\n\r\n**Environment**\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 8.4.0-1ubuntu1~18.04) 8.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: \r\nGPU 0: GeForce GT 730\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 440.44\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.6.5\r\n/usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.6.5\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.4.0\r\n[pip] torchaudio==0.4.0a0+719bcc7\r\n[pip] torchsummary==1.5.1\r\n[pip] torchtext==0.5.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2020.0                      166  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchaudio                0.4.0                      py37    pytorch\r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchtext                 0.5.0                      py_1    pytorch\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n\r\n**Additional context**\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/716", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/716/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/716/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/716/events", "html_url": "https://github.com/pytorch/text/issues/716", "id": 588358792, "node_id": "MDU6SXNzdWU1ODgzNTg3OTI=", "number": 716, "title": "Tokenizer of target not being called", "user": {"login": "joanPlepi", "id": 28218228, "node_id": "MDQ6VXNlcjI4MjE4MjI4", "avatar_url": "https://avatars2.githubusercontent.com/u/28218228?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joanPlepi", "html_url": "https://github.com/joanPlepi", "followers_url": "https://api.github.com/users/joanPlepi/followers", "following_url": "https://api.github.com/users/joanPlepi/following{/other_user}", "gists_url": "https://api.github.com/users/joanPlepi/gists{/gist_id}", "starred_url": "https://api.github.com/users/joanPlepi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joanPlepi/subscriptions", "organizations_url": "https://api.github.com/users/joanPlepi/orgs", "repos_url": "https://api.github.com/users/joanPlepi/repos", "events_url": "https://api.github.com/users/joanPlepi/events{/privacy}", "received_events_url": "https://api.github.com/users/joanPlepi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-26T11:44:56Z", "updated_at": "2020-03-26T15:37:57Z", "closed_at": "2020-03-26T15:37:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Tokenizer is not being called from target field**\r\nMy training data as input consists into a sentence as a question, and the answer formatted into a list with actions\r\n\r\nFor that I declare two tokenizers and fields as below:\r\n\r\n```python\r\nTOKENIZE_QUESTION = lambda  x: x.replace(\"?\", \" ?\").\\\r\n                                          replace(\".\", \" .\").\\\r\n                                          replace(\",\", \" ,\").\\\r\n                                          replace(\"'\", \" '\").\\\r\n                                          split()\r\nTOKENIZE_LOGICAL_FORM = lambda  x: [a[1] if a[0] == 'action' else a[0] for a in x]\r\n\r\nsrc_field = Field(tokenize=TOKENIZE_QUESTION,\r\n                               init_token=SOS_TOKEN,\r\n                               eos_token=EOS_TOKEN,\r\n                               lower=True,\r\n                               include_lengths=True,\r\n                               batch_first=True)\r\n\r\ntrg_field = Field(tokenize=TOKENIZE_LOGICAL_FORM,\r\n                               init_token=SOS_TOKEN,\r\n                               eos_token=EOS_TOKEN,\r\n                               batch_first=True, \r\n                               unk_token=None)\r\n\r\nfields_tuple = [(SRC_NAME, src_field), (TRG_NAME, trg_field)]\r\n```\r\n\r\nIf I try to explicitly call the tokenizers seperaty in one example of my dataset, my output is the one I expect. \r\n\r\n```python\r\nval_data = [' <SEP>  <SEP> Which administrative territory is Thomas Arents a civilian of ?', [['action', 'filter_type'], ['action', 'find'], ['entity', 'Q18638789'], ['predicate', 'P27'], ['type', 'Q15617994']], [['action', 'filter_type'], ['action', 'find'], ['entity', 'Q18638789'], ['predicate', 'P27'], ['type', 'Q15617994']]]\r\nprint(src_field.tokenize(val_data[0][0]))\r\nprint(trg_field.tokenize(val_data[0][1])\r\n```\r\nOutputs:\r\n> ['<SEP>', '<SEP>', 'Which', 'administrative', 'territory', 'is', 'Thomas', 'Arents', 'a', 'civilian', 'of', '?']\r\n\r\n> ['filter_type', 'find', 'entity', 'predicate', 'type']\r\n\r\n\r\nHowever, when I try to make an Example and create the dataset from it, the tokenizer of the target field is not called at all.\r\n\r\n```python\r\nd = Dataset([Example.fromlist(val_data[0], fields_tuple)], fields_tuple)\r\nprint(d.examples[0].src)\r\nprint(d.examples[0].trg)\r\n```\r\n\r\nOutputs:\r\n\r\n> ['<sep>', '<sep>', 'which', 'administrative', 'territory', 'is', 'thomas', 'arents', 'a', 'civilian', 'of', '?']\r\n\r\n> [['action', 'filter_type'], ['action', 'find'], ['entity', 'Q18638789'], ['predicate', 'P27'], ['type', 'Q15617994']]\r\n\r\nI am not sure why the tokenizer of the target field is not called. I have no exception thrown also. \r\nAny help or suggestion in the issue is appreciated. \r\n\r\n        ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/709", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/709/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/709/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/709/events", "html_url": "https://github.com/pytorch/text/issues/709", "id": 587019617, "node_id": "MDU6SXNzdWU1ODcwMTk2MTc=", "number": 709, "title": "KeyError in `torchtext.utils.download_from_url`", "user": {"login": "ehoelzl", "id": 17425020, "node_id": "MDQ6VXNlcjE3NDI1MDIw", "avatar_url": "https://avatars3.githubusercontent.com/u/17425020?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ehoelzl", "html_url": "https://github.com/ehoelzl", "followers_url": "https://api.github.com/users/ehoelzl/followers", "following_url": "https://api.github.com/users/ehoelzl/following{/other_user}", "gists_url": "https://api.github.com/users/ehoelzl/gists{/gist_id}", "starred_url": "https://api.github.com/users/ehoelzl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ehoelzl/subscriptions", "organizations_url": "https://api.github.com/users/ehoelzl/orgs", "repos_url": "https://api.github.com/users/ehoelzl/repos", "events_url": "https://api.github.com/users/ehoelzl/events{/privacy}", "received_events_url": "https://api.github.com/users/ehoelzl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-24T14:53:28Z", "updated_at": "2020-03-24T15:08:36Z", "closed_at": "2020-03-24T15:08:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\nWhen downloading the dataset gzip form a google-drive (e.g. WMT14 dataset), the following error pops up: \r\n`KeyError: 'content-disposition'` at `torchtext.utils:54`\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```\r\nfrom torchtext.utils import download_from_url\r\npath = \"../../Data/wmt16_en_de.tar.gz\"\r\nurl = \"https://drive.google.com/uc?export=download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8\"\r\ndownload_from_url(url, path=path) <-- KeyError: 'content-disposition'\r\n```\r\n\r\n\r\n\r\n**Expected behavior**\r\nDownload and extract dataset\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment**\r\n\r\n - PyTorch Version (e.g., 1.0):1.0.0\r\n - OS (e.g., Linux): Linux Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): \r\n - Python version: 3.6.6\r\n - CUDA/cuDNN version: 9.0, 7.3.0\r\n - GPU models and configuration: None\r\n - Any other relevant information: None\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/708", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/708/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/708/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/708/events", "html_url": "https://github.com/pytorch/text/issues/708", "id": 581746700, "node_id": "MDU6SXNzdWU1ODE3NDY3MDA=", "number": 708, "title": "feat(BucketIterator): add num_workers", "user": {"login": "AmitMY", "id": 5757359, "node_id": "MDQ6VXNlcjU3NTczNTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5757359?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AmitMY", "html_url": "https://github.com/AmitMY", "followers_url": "https://api.github.com/users/AmitMY/followers", "following_url": "https://api.github.com/users/AmitMY/following{/other_user}", "gists_url": "https://api.github.com/users/AmitMY/gists{/gist_id}", "starred_url": "https://api.github.com/users/AmitMY/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AmitMY/subscriptions", "organizations_url": "https://api.github.com/users/AmitMY/orgs", "repos_url": "https://api.github.com/users/AmitMY/repos", "events_url": "https://api.github.com/users/AmitMY/events{/privacy}", "received_events_url": "https://api.github.com/users/AmitMY/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-15T16:58:18Z", "updated_at": "2020-04-13T20:24:16Z", "closed_at": "2020-04-13T20:24:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\ude80 Feature\r\nAdd `num_workers` to `BucketIterator`\r\n\r\n**Motivation**\r\n\r\nWhile `torchtext` is designed for test, it is also the best thing to use for sequence data.\r\nI have sequence data in the form of sign language poses, which falls under the category of language, and I want to batch it the same way I would batch text - sorted by length.\r\n\r\nFor sign language, the dataset needs to handle data augmentation ([my specific use case](https://youtu.be/78eeBoxTr-w?t=323)) and current data augmentation libraries like `imgaug` and `albumnations` are slow (see issues https://github.com/aleju/imgaug/issues/635 and https://github.com/albumentations-team/albumentations/issues/554).\r\nTherefore, using `num_workers` to be able to augment a batch or many batches from the iterator will be a great help (Instead of waiting 10 minutes, I would wait 15 seconds, with 40 CPUs)\r\n\r\n**Pitch**\r\n\r\nAdd `num_workers` to `BucketIterator`. \r\nUse `num_workers` to distribute the `Dataset.__getitem__` calls on all workers when iterating the `BucketIterator`.\r\n\r\n**Alternatives**\r\n\r\nWriting my own implementation of a bucket iterator extending `DataLoader`.\r\n\r\nOR\r\n\r\nUse a `DataLoader` with batch size of 1\r\n\r\n\r\n**Other Info**\r\n\r\nConfirmation that BucketIterator doesn't support `num_workers` https://github.com/pytorch/text/issues/437\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/705", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/705/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/705/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/705/events", "html_url": "https://github.com/pytorch/text/issues/705", "id": 578464641, "node_id": "MDU6SXNzdWU1Nzg0NjQ2NDE=", "number": 705, "title": "for _, batch in enumerate(train_iter): KeyError: None", "user": {"login": "gumowangfei", "id": 25955151, "node_id": "MDQ6VXNlcjI1OTU1MTUx", "avatar_url": "https://avatars3.githubusercontent.com/u/25955151?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gumowangfei", "html_url": "https://github.com/gumowangfei", "followers_url": "https://api.github.com/users/gumowangfei/followers", "following_url": "https://api.github.com/users/gumowangfei/following{/other_user}", "gists_url": "https://api.github.com/users/gumowangfei/gists{/gist_id}", "starred_url": "https://api.github.com/users/gumowangfei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gumowangfei/subscriptions", "organizations_url": "https://api.github.com/users/gumowangfei/orgs", "repos_url": "https://api.github.com/users/gumowangfei/repos", "events_url": "https://api.github.com/users/gumowangfei/events{/privacy}", "received_events_url": "https://api.github.com/users/gumowangfei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-10T10:04:04Z", "updated_at": "2020-03-13T01:39:10Z", "closed_at": "2020-03-11T23:43:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "TEXT = data.Field(batch_first=True, lower=True, include_lengths=True)\r\nSLOT_LABEL = data.Field(batch_first=True, is_target=True, unk_token=None, pad_token=None)\r\nINTENT_LABEL = data.Field(batch_first=True, is_target=True, unk_token=None, pad_token=None)\r\n\r\nfields = [('text', TEXT), ('slot', SLOT_LABEL), ('intent', INTENT_LABEL)]\r\n\r\ntrain, val, test = data.TabularDataset.splits(\r\n    path=path,\r\n    format='csv',\r\n    train=train_path,\r\n    validation=dev_path,\r\n    test=test_path,\r\n    skip_header=True,\r\n    fields=fields\r\n)\r\n\r\nTEXT.build_vocab(train)\r\nSLOT_LABEL.build_vocab(train)\r\nINTENT_LABEL.build_vocab(train)\r\n\r\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\r\n    (train, val, test),\r\n    batch_sizes=(32, 32, 1),\r\n    device=device,\r\n    sort_key= lambda x: len(x.text),\r\n    sort_within_batch=True,\r\n    repeat=False\r\n)\r\n\r\nfor _, batch in enumerate(train_iter):\r\n    print('---')\r\n\r\n\r\n---\r\n---\r\n---\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-92-b8ca9a142d50> in <module>\r\n      1 ngram = 4\r\n      2 # edges_itos = ['<unk>']\r\n----> 3 for _, batch in enumerate(train_iter):\r\n      4     print('---')\r\n      5 #     text_padded_batch, text_included_length_batch = batch.text\r\n\r\n/opt/conda/lib/python3.6/site-packages/torchtext/data/iterator.py in __iter__(self)\r\n    154                     else:\r\n    155                         minibatch.sort(key=self.sort_key, reverse=True)\r\n--> 156                 yield Batch(minibatch, self.dataset, self.device)\r\n    157             if not self.repeat:\r\n    158                 return\r\n\r\n/opt/conda/lib/python3.6/site-packages/torchtext/data/batch.py in __init__(self, data, dataset, device)\r\n     32                 if field is not None:\r\n     33                     batch = [getattr(x, name) for x in data]\r\n---> 34                     setattr(self, name, field.process(batch, device=device))\r\n     35 \r\n     36     @classmethod\r\n\r\n/opt/conda/lib/python3.6/site-packages/torchtext/data/field.py in process(self, batch, device)\r\n    235         \"\"\"\r\n    236         padded = self.pad(batch)\r\n--> 237         tensor = self.numericalize(padded, device=device)\r\n    238         return tensor\r\n    239 \r\n\r\n/opt/conda/lib/python3.6/site-packages/torchtext/data/field.py in numericalize(self, arr, device)\r\n    334         if self.use_vocab:\r\n    335             if self.sequential:\r\n--> 336                 arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\n    337             else:\r\n    338                 arr = [self.vocab.stoi[x] for x in arr]\r\n\r\n/opt/conda/lib/python3.6/site-packages/torchtext/data/field.py in <listcomp>(.0)\r\n    334         if self.use_vocab:\r\n    335             if self.sequential:\r\n--> 336                 arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\n    337             else:\r\n    338                 arr = [self.vocab.stoi[x] for x in arr]\r\n\r\n/opt/conda/lib/python3.6/site-packages/torchtext/data/field.py in <listcomp>(.0)\r\n    334         if self.use_vocab:\r\n    335             if self.sequential:\r\n--> 336                 arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\n    337             else:\r\n    338                 arr = [self.vocab.stoi[x] for x in arr]\r\n\r\nKeyError: None", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/703", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/703/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/703/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/703/events", "html_url": "https://github.com/pytorch/text/issues/703", "id": 573920668, "node_id": "MDU6SXNzdWU1NzM5MjA2Njg=", "number": 703, "title": "Windows and Sentence Piece", "user": {"login": "achapkowski", "id": 5131271, "node_id": "MDQ6VXNlcjUxMzEyNzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5131271?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achapkowski", "html_url": "https://github.com/achapkowski", "followers_url": "https://api.github.com/users/achapkowski/followers", "following_url": "https://api.github.com/users/achapkowski/following{/other_user}", "gists_url": "https://api.github.com/users/achapkowski/gists{/gist_id}", "starred_url": "https://api.github.com/users/achapkowski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achapkowski/subscriptions", "organizations_url": "https://api.github.com/users/achapkowski/orgs", "repos_url": "https://api.github.com/users/achapkowski/repos", "events_url": "https://api.github.com/users/achapkowski/events{/privacy}", "received_events_url": "https://api.github.com/users/achapkowski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2020-03-02T11:48:08Z", "updated_at": "2020-05-19T14:45:17Z", "closed_at": "2020-05-19T14:45:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\n\r\nIt appears one of the requirements sentencepiece is not built for windows machines.\r\n\r\nThis causes an import error:\r\n\r\n```python\r\n     1 import torch\r\n----> 2 import torchtext\r\n      3 import numpy as np\r\n      4 import pandas as pd\r\n\r\nC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-dev\\lib\\site-packages\\torchtext\\__init__.py in <module>\r\n----> 1 from . import data\r\n      2 from . import datasets\r\n      3 from . import utils\r\n      4 from . import vocab\r\n      5 from . import experimental\r\n\r\nC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-dev\\lib\\site-packages\\torchtext\\data\\__init__.py in <module>\r\n      8 from .pipeline import Pipeline\r\n      9 from .utils import get_tokenizer, interleave_keys\r\n---> 10 from .functional import generate_sp_model, \\\r\n     11     load_sp_model, \\\r\n     12     sentencepiece_numericalizer, \\\r\n\r\nC:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-dev\\lib\\site-packages\\torchtext\\data\\functional.py in <module>\r\n----> 1 import sentencepiece as spm\r\n      2 import re\r\n      3 \r\n      4 __all__ = [\r\n      5     \"generate_sp_model\", \"load_sp_model\",\r\n\r\nModuleNotFoundError: No module named 'sentencepiece'\r\n```\r\n\r\n\r\n\r\n**Expected behavior**\r\nmodule imports without issue.\r\n\r\n\r\n**Environment**\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/text/master/torchtext/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\npython -c \"import torchtext; print(\\\"torchtext version is \\\", torchtext.__version__)\"\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): latest\r\n - OS (e.g., Linux): Windows\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Build command you used (if compiling from source):N/A\r\n - Python version:3.6\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information: Install torchtext from conda.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/698", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/698/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/698/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/698/events", "html_url": "https://github.com/pytorch/text/issues/698", "id": 569509759, "node_id": "MDU6SXNzdWU1Njk1MDk3NTk=", "number": 698, "title": "Custom dataset: It seems tokenization is always being done at a character level", "user": {"login": "davidsbatista", "id": 7937824, "node_id": "MDQ6VXNlcjc5Mzc4MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/7937824?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidsbatista", "html_url": "https://github.com/davidsbatista", "followers_url": "https://api.github.com/users/davidsbatista/followers", "following_url": "https://api.github.com/users/davidsbatista/following{/other_user}", "gists_url": "https://api.github.com/users/davidsbatista/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidsbatista/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidsbatista/subscriptions", "organizations_url": "https://api.github.com/users/davidsbatista/orgs", "repos_url": "https://api.github.com/users/davidsbatista/repos", "events_url": "https://api.github.com/users/davidsbatista/events{/privacy}", "received_events_url": "https://api.github.com/users/davidsbatista/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-23T15:13:18Z", "updated_at": "2020-02-24T18:49:19Z", "closed_at": "2020-02-24T18:49:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\nIt seems tokenization is being done at a character level\r\n\r\n```\r\n    import torchtext\r\n\r\n    class Example(object):\r\n        def __init__(self, news_text, category):\r\n            self.text = news_text\r\n            self.category = category\r\n\r\n    sample_1 = Example('a simple sentence', 'label_1')\r\n    sample_2 = Example('any other thing', 'label_2')\r\n    texts = [sample_1, sample_2]\r\n\r\n    TEXT = torchtext.data.Field(sequential=True)\r\n    LABEL = torchtext.data.Field(sequential=False)\r\n\r\n    dataset = torchtext.data.Dataset(texts, fields=[('text', TEXT), ('category', LABEL)])\r\n    TEXT.build_vocab(dataset)\r\n    \r\n    print(TEXT.vocab.freqs)\r\n```\r\n\r\nI get this\r\n\r\n   `Counter({'e': 5, ' ': 4, 'n': 4, 't': 3, 'a': 2, 's': 2, 'i': 2, 'h': 2, 'm': 1, 'p': 1, 'l': 1, 'c': 1, 'y': 1, 'o': 1, 'r': 1, 'g': 1})`\r\n\r\nEven if I try to pass a custom tokenizer, simple one based on `split()`, which I I think is also the default one, the same result happens.\r\n\r\n     import torchtext\r\n\r\n     class Example(object):\r\n         def __init__(self, news_text, category):\r\n             self.text = news_text\r\n             self.category = category\r\n\r\n     sample_1 = Example('a simple sentence', 'label_1')\r\n     sample_2 = Example('any other thing', 'label_2')\r\n     texts = [sample_1, sample_2]\r\n\r\n     TEXT = torchtext.data.Field(sequential=True, tokenize=lambda x: x.split())\r\n     LABEL = torchtext.data.Field(sequential=False)\r\n\r\n     dataset = torchtext.data.Dataset(texts, fields=[('text', TEXT), ('category', LABEL)])\r\n\r\n     TEXT.build_vocab(dataset)\r\n\r\n     print(TEXT.vocab.freqs)\r\n\r\nThe same thing:\r\n\r\n   `Counter({'e': 5, ' ': 4, 'n': 4, 't': 3, 'a': 2, 's': 2, 'i': 2, 'h': 2, 'm': 1, 'p': 1, 'l': 1, 'c': 1, 'y': 1, 'o': 1, 'r': 1, 'g': 1})`\r\n\r\nI think I'm missing something simple in setting up the custom dataset but can't figure it out what it is. I also looked through the `build_vocab()` and it seems to me that the tokenization is never triggered there, and I can't understand where tokenization is called. \r\n\r\nAnyone might know how to solve this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/695", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/695/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/695/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/695/events", "html_url": "https://github.com/pytorch/text/issues/695", "id": 565080943, "node_id": "MDU6SXNzdWU1NjUwODA5NDM=", "number": 695, "title": "Where am i wrong\uff1f", "user": {"login": "LiangTiger", "id": 18245952, "node_id": "MDQ6VXNlcjE4MjQ1OTUy", "avatar_url": "https://avatars3.githubusercontent.com/u/18245952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LiangTiger", "html_url": "https://github.com/LiangTiger", "followers_url": "https://api.github.com/users/LiangTiger/followers", "following_url": "https://api.github.com/users/LiangTiger/following{/other_user}", "gists_url": "https://api.github.com/users/LiangTiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/LiangTiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LiangTiger/subscriptions", "organizations_url": "https://api.github.com/users/LiangTiger/orgs", "repos_url": "https://api.github.com/users/LiangTiger/repos", "events_url": "https://api.github.com/users/LiangTiger/events{/privacy}", "received_events_url": "https://api.github.com/users/LiangTiger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-14T03:28:44Z", "updated_at": "2020-02-14T05:02:14Z", "closed_at": "2020-02-14T05:02:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\n\r\nimport numpy as np\r\nimport random\r\nimport pandas as pd\r\n\r\nUSE_CUDA=torch.cuda.is_available()\r\nBATCH_SIZE=32\r\nEMBEDDING_SIZE=100\r\nMAX_VOCAB_SIZE=50000\r\ndevice=torch.device('cuda')\r\n\r\n\r\nTEXT=torchtext.data.Field(lower=True)\r\nLABEL=torchtext.data.LabelField(dtype=torch.long)\r\ntrain=torchtext.data.TabularDataset(path='train.csv',format='csv',skip_header=True,fields=[('id',None),('keyword',None),('location',None),('text',TEXT),('target',LABEL)])\r\nTEXT.build_vocab(train,max_size=MAX_VOCAB_SIZE)\r\nVOCAB_SIZE=len(TEXT.vocab)\r\ntrain_iter=torchtext.data.BPTTIterator(dataset=train,batch_size=BATCH_SIZE,device=device,bptt_len=32,repeat=False,shuffle=True)\r\nnext(iter(train_iter))\r\n**errors:\r\nTraceback (most recent call last):\r\n  Python Shell, prompt 2, line 1\r\nbuiltins.StopIteration:**\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/694", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/694/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/694/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/694/events", "html_url": "https://github.com/pytorch/text/issues/694", "id": 564693340, "node_id": "MDU6SXNzdWU1NjQ2OTMzNDA=", "number": 694, "title": "builtins.PermissionError: [Errno 13] Permission denied", "user": {"login": "LiangTiger", "id": 18245952, "node_id": "MDQ6VXNlcjE4MjQ1OTUy", "avatar_url": "https://avatars3.githubusercontent.com/u/18245952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LiangTiger", "html_url": "https://github.com/LiangTiger", "followers_url": "https://api.github.com/users/LiangTiger/followers", "following_url": "https://api.github.com/users/LiangTiger/following{/other_user}", "gists_url": "https://api.github.com/users/LiangTiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/LiangTiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LiangTiger/subscriptions", "organizations_url": "https://api.github.com/users/LiangTiger/orgs", "repos_url": "https://api.github.com/users/LiangTiger/repos", "events_url": "https://api.github.com/users/LiangTiger/events{/privacy}", "received_events_url": "https://api.github.com/users/LiangTiger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-02-13T13:45:56Z", "updated_at": "2020-08-21T13:44:54Z", "closed_at": "2020-02-14T04:36:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\n\r\nTEXT=torchtext.data.Field(lower=True)\r\n\r\ntrain,test=torchtext.data.TabularDataset(path='data',train='train.csv',test='test.csv',format='csv',skip_header=True,fields=[('id',None),('keyword',None),('location',None),('text',TEXT),('target',None)])\r\n\r\nerror:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:/TF and Torch/kaggle2/kaggle2.py\", line 16, in <module>\r\n    train,test=torchtext.data.TabularDataset(path='data',train='train.csv',test='test.csv',format='csv',skip_header=True,fields=[('id',None),('keyword',None),('location',None),('text',TEXT),('target',None)])\r\n  File \"C:\\Users\\Tiger\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\torchtext\\data\\dataset.py\", line 251, in __init__\r\n    with io.open(os.path.expanduser(path), encoding=\"utf8\") as f:\r\n**builtins.PermissionError: [Errno 13] Permission denied: 'data'**", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/690", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/690/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/690/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/690/events", "html_url": "https://github.com/pytorch/text/issues/690", "id": 560477367, "node_id": "MDU6SXNzdWU1NjA0NzczNjc=", "number": 690, "title": "Adding validation splits to (experimental) text_classification datasets that do not have vocabulary built over them", "user": {"login": "bentrevett", "id": 8006479, "node_id": "MDQ6VXNlcjgwMDY0Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/8006479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bentrevett", "html_url": "https://github.com/bentrevett", "followers_url": "https://api.github.com/users/bentrevett/followers", "following_url": "https://api.github.com/users/bentrevett/following{/other_user}", "gists_url": "https://api.github.com/users/bentrevett/gists{/gist_id}", "starred_url": "https://api.github.com/users/bentrevett/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bentrevett/subscriptions", "organizations_url": "https://api.github.com/users/bentrevett/orgs", "repos_url": "https://api.github.com/users/bentrevett/repos", "events_url": "https://api.github.com/users/bentrevett/events{/privacy}", "received_events_url": "https://api.github.com/users/bentrevett/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-02-05T16:15:10Z", "updated_at": "2020-04-21T20:03:35Z", "closed_at": "2020-04-21T20:03:35Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\ude80 Feature\r\nThe experimental text_classification datasets should have a way to build a validation set from them, without the vocabulary being built over the validation set. \r\n\r\n**Motivation**\r\n\r\nIn ML, you should always have a test, validation and training set. In NLP, your vocabulary should be built from the training set only, and not from the test/validation. \r\n\r\nThe current experimental text classification (IMDB) dataset does not have a validation set and automatically builds the vocabulary whilst loading the train/test sets. After loading the train and test sets, we would need to construct a validation set with `torch.utils.data.random_split`. The issue here is that our vocabulary has already been built over the validation set we are about to create. There is currently no way to solve this issue.\r\n\r\n**Pitch**\r\n\r\n`'valid'` should be accepted as a `data_select` argument and should create a validation set before the vocabulary has been created over the training set. As the IMDB dataset does not have a standardized validation split, we can do something like taking the last 20% of the training set. \r\n\r\nI am proposing something like the following after the `iters_group` is created [here](https://github.com/pytorch/text/blob/master/torchtext/experimental/datasets/text_classification.py#L63):\r\n\r\n```python\r\nfrom itertools import islice, tee\r\n\r\nif 'valid' in iters_group.keys():\r\n    train_iter_a, train_iter_b, train_iter_c = tee(iters_group['train'], 3)\r\n    len_train = int(sum(1 for _ in train_iter_a) * 0.8)\r\n    iters_group['valid'] = islice(train_iter_b, len_train, None)\r\n    iters_group['train'] = islice(train_iter_c, 0, len_train)\r\n    iters_group['vocab'] = islice(iters_group['vocab'] 0, len_train)\r\n```\r\n\r\n`tee` duplicates generators and `islice` slices into generators. We need to duplicate the training data iterator as we will be using it three times. We use the first iterator to get the length of the training set so we know what size the validation set will be (the last 20% of the examples from the training set). We then use `islice` to get the last 20% of the training examples to form the validation set, the first 80% of the training examples to use as the new training set, and the first 80% examples of the \"vocab\" set as this needs to match the training set as it is what we want to build our vocab from.\r\n\r\nWe can now correctly load a train, valid and test set with vocabulary built only over the training set:\r\n```python\r\nfrom torchtext.experimental import datasets\r\n\r\ntrain_data, valid_data, test_data = datasets.IMDB(data_select=('train', 'valid', 'test'))\r\n```\r\n\r\nCan also load a custom vocabulary built from the original vocabulary like so (note that 'valid' needs to be in the `data_select` when building the original vocabulary):\r\n```python\r\nfrom torchtext import vocab\r\nfrom torchtext.experimental import datasets\r\n\r\ndef get_IMDB(root, tokenizer, vocab_max_size, vocab_min_freq):\r\n    \r\n    os.makedirs(root, exist_ok = True)\r\n    \r\n    train_data, _ = datasets.IMDB(tokenizer = tokenizer, \r\n                                 data_select = ('train', 'valid'))\r\n    \r\n    old_vocab = train_data.get_vocab()\r\n    \r\n    new_vocab = vocab.Vocab(old_vocab.freqs, \r\n                            max_size = vocab_max_size, \r\n                            min_freq = vocab_min_freq)\r\n    \r\n    train_data, valid_data, test_data = datasets.IMDB(tokenizer = tokenizer, \r\n                                                      vocab = new_vocab,\r\n                                                      data_select=('train', 'valid', 'test'))\r\n    \r\n    return train_data, valid_data, test_data\r\n```\r\n\r\nHappy to make the PR if this is given the go-ahead.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/689", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/689/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/689/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/689/events", "html_url": "https://github.com/pytorch/text/issues/689", "id": 559261995, "node_id": "MDU6SXNzdWU1NTkyNjE5OTU=", "number": 689, "title": "Unsure of the dimension of the nested field", "user": {"login": "thak123", "id": 3891859, "node_id": "MDQ6VXNlcjM4OTE4NTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3891859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thak123", "html_url": "https://github.com/thak123", "followers_url": "https://api.github.com/users/thak123/followers", "following_url": "https://api.github.com/users/thak123/following{/other_user}", "gists_url": "https://api.github.com/users/thak123/gists{/gist_id}", "starred_url": "https://api.github.com/users/thak123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thak123/subscriptions", "organizations_url": "https://api.github.com/users/thak123/orgs", "repos_url": "https://api.github.com/users/thak123/repos", "events_url": "https://api.github.com/users/thak123/events{/privacy}", "received_events_url": "https://api.github.com/users/thak123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-02-03T18:41:34Z", "updated_at": "2020-02-04T11:01:08Z", "closed_at": "2020-02-04T11:01:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to implement character embeddings on the multi30k dataset using the following code but the dimension of the objects that I get is not making sense to me\r\nfor ex if source words is of .Size([8, 13])\r\nthen character Size([8, 12, 19]) is displayed in this fashion.\r\n\r\nis everything fine with my code ?\r\n\r\n\r\nimport spacy\r\n\r\nspacy_en = spacy.load('en_core_web_sm') \r\nimport de_core_news_sm\r\nspacy_de = de_core_news_sm.load()\r\n# spacy_de = spacy.load('de_core_news_sm')\r\nfrom torchtext.data import Field,BucketIterator\r\nfrom torchtext import data\r\nfrom torchtext.datasets import TranslationDataset, Multi30k\r\ndef tokenize_de(text):\r\n    \"\"\"\r\n    Tokenizes German text from a string into a list of strings\r\n    \"\"\"\r\n    return [tok.text for tok in spacy_de.tokenizer(text)]\r\n\r\n\r\ndef tokenize_en(text):\r\n    \"\"\"\r\n    Tokenizes English text from a string into a list of strings\r\n    \"\"\"\r\n    return [tok.text for tok in spacy_en.tokenizer(text)]\r\n\r\n\r\nSRC = Field(tokenize=tokenize_de,\r\n            init_token='<sos>',\r\n            eos_token='<eos>',\r\n            lower=True,\r\n            batch_first=True,include_lengths=False)\r\n\r\nTRG = Field(tokenize=tokenize_en,\r\n            init_token='<sos>',\r\n            eos_token='<eos>',\r\n            batch_first=True,\r\n            lower=True,include_lengths=False)\r\n\r\nCHAR_NESTING = data.Field(tokenize=list,batch_first=True)\r\n\r\nCHAR = data.NestedField(CHAR_NESTING, init_token=\"<sos>\", eos_token=\"<eos>\")\r\n\r\ntrain_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\r\n                                                    fields=((('src', 'char_src'), (SRC, CHAR)),\r\n                                                            (('trg', 'char_trg'), (TRG, CHAR))))\r\n\r\nSRC.build_vocab(train_data.src,min_freq=2)\r\nCHAR.build_vocab(train_data.char_src)\r\n\r\nTRG.vocab = SRC.vocab\r\n\r\nBATCH_SIZE = 8\r\n\r\n\r\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n    (train_data, valid_data, test_data),\r\n    batch_size=BATCH_SIZE,\r\n    sort_key=lambda x: len(x.src),\r\n    sort_within_batch=True)\r\nfor i, batch in enumerate(train_iterator):\r\n  src, trg,  =  batch.src, batch.trg\r\n  char_src, char_trg = batch.char_src, batch.char_trg\r\n\r\n\r\ntorch.Size([8, 13]) torch.Size([8, 12, 19]) torch.Size([8, 17]) torch.Size([8, 16, 9])\r\n  print([SRC.vocab.itos[j]  for i in src for j in i])\r\n  print([CHAR.vocab.itos[j]  for i in char_src[0] for j in i])\r\n\r\n  print(src.shape,char_src.shape,trg.shape, char_trg.shape)\r\n\r\n  break\r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/686", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/686/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/686/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/686/events", "html_url": "https://github.com/pytorch/text/issues/686", "id": 557567765, "node_id": "MDU6SXNzdWU1NTc1Njc3NjU=", "number": 686, "title": "Hard negatives for Translation ranking task", "user": {"login": "thak123", "id": 3891859, "node_id": "MDQ6VXNlcjM4OTE4NTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3891859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thak123", "html_url": "https://github.com/thak123", "followers_url": "https://api.github.com/users/thak123/followers", "following_url": "https://api.github.com/users/thak123/following{/other_user}", "gists_url": "https://api.github.com/users/thak123/gists{/gist_id}", "starred_url": "https://api.github.com/users/thak123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thak123/subscriptions", "organizations_url": "https://api.github.com/users/thak123/orgs", "repos_url": "https://api.github.com/users/thak123/repos", "events_url": "https://api.github.com/users/thak123/events{/privacy}", "received_events_url": "https://api.github.com/users/thak123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-01-30T15:25:50Z", "updated_at": "2020-02-05T18:28:54Z", "closed_at": "2020-02-05T18:28:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to implement Guo et al 2018 bi-text retrieval setup for the translation ranking task.\r\n\r\nI want to sample hard negatives for the translation pairs.\r\n\r\nI was wondering if this is easily achievable using torchtext.\r\n\r\nAny pointers will be highly helpful .", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/681", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/681/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/681/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/681/events", "html_url": "https://github.com/pytorch/text/issues/681", "id": 553004362, "node_id": "MDU6SXNzdWU1NTMwMDQzNjI=", "number": 681, "title": "Error when using provided vocab for experimental IMDB dataset", "user": {"login": "bentrevett", "id": 8006479, "node_id": "MDQ6VXNlcjgwMDY0Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/8006479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bentrevett", "html_url": "https://github.com/bentrevett", "followers_url": "https://api.github.com/users/bentrevett/followers", "following_url": "https://api.github.com/users/bentrevett/following{/other_user}", "gists_url": "https://api.github.com/users/bentrevett/gists{/gist_id}", "starred_url": "https://api.github.com/users/bentrevett/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bentrevett/subscriptions", "organizations_url": "https://api.github.com/users/bentrevett/orgs", "repos_url": "https://api.github.com/users/bentrevett/repos", "events_url": "https://api.github.com/users/bentrevett/events{/privacy}", "received_events_url": "https://api.github.com/users/bentrevett/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-01-21T16:55:15Z", "updated_at": "2020-01-21T22:52:52Z", "closed_at": "2020-01-21T22:52:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\nWhen you try and provide a vocabulary to the new experimental IMDB dataset you get the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-83dc4fb02510> in <module>\r\n      5 vocab = train_data.get_vocab()\r\n      6 \r\n----> 7 train_data, test_data = IMDB(vocab = vocab)\r\n\r\n~/.conda/envs/pytorch14/lib/python3.7/site-packages/torchtext/experimental/datasets/text_classification.py in IMDB(*args, **kwargs)\r\n    129     \"\"\"\r\n    130 \r\n--> 131     return _setup_datasets(*((\"IMDB\",) + args), **kwargs)\r\n    132 \r\n    133 \r\n\r\n~/.conda/envs/pytorch14/lib/python3.7/site-packages/torchtext/experimental/datasets/text_classification.py in _setup_datasets(dataset_name, root, ngrams, vocab, removed_tokens, tokenizer, data_select)\r\n     81         logging.info('Creating {} data'.format(item))\r\n     82         data_iter = _create_data_from_iterator(vocab, iters_group[item], removed_tokens)\r\n---> 83         for cls, tokens in data_iter:\r\n     84             data[item]['data'].append((torch.tensor(cls),\r\n     85                                        torch.tensor([token_id for token_id in tokens])))\r\n\r\n~/.conda/envs/pytorch14/lib/python3.7/site-packages/torchtext/experimental/datasets/text_classification.py in _create_data_from_iterator(vocab, iterator, removed_tokens)\r\n     16 \r\n     17 def _create_data_from_iterator(vocab, iterator, removed_tokens):\r\n---> 18     for cls, tokens in iterator:\r\n     19         yield cls, iter(map(lambda x: vocab[x],\r\n     20                         filter(lambda x: x not in removed_tokens, tokens)))\r\n\r\nValueError: too many values to unpack (expected 2)\r\n```\r\n\r\nThis even happens when you try to use the vocabulary created by the dataset itself.\r\n\r\n**To Reproduce**\r\nUsing edited vocabulary:\r\n```python\r\nfrom torchtext.experimental.datasets import IMDB\r\nfrom torchtext.vocab import Vocab\r\n\r\ntrain_data, test_data = IMDB()\r\n\r\nold_vocab = train_data.get_vocab()\r\n\r\nnew_vocab = Vocab(counter = old_vocab.freqs,\r\n                  max_size = 25_000)\r\n\r\ntrain_data, test_data = IMDB(vocab = new_vocab)\r\n```\r\n\r\nUsing un-edited vocabulary:\r\n```python\r\nfrom torchtext.experimental.datasets import IMDB\r\n\r\ntrain_data, test_data = IMDB()\r\n\r\nvocab = train_data.get_vocab()\r\n\r\ntrain_data, test_data = IMDB(vocab = vocab)\r\n```\r\n\r\n**Expected behavior**\r\nTorchText to create the new datasets with the provided vocabulary, such as with the current `text_classification` datasets, e.g.\r\n```python\r\nimport torchtext\r\n\r\ntrain_data, _ = torchtext.datasets.text_classification.AG_NEWS()\r\n\r\nold_vocab = train_data.get_vocab()\r\n    \r\nnew_vocab = torchtext.vocab.Vocab(counter = old_vocab.freqs, \r\n                                  max_size = 25_000)\r\n\r\ntrain_data, test_data = torchtext.datasets.text_classification.AG_NEWS(vocab = new_vocab)\r\n```\r\n\r\n**Environment**\r\nPython 3.7, PyTorch 1.4 and TorchText 0.5", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/678", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/678/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/678/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/678/events", "html_url": "https://github.com/pytorch/text/issues/678", "id": 550858064, "node_id": "MDU6SXNzdWU1NTA4NTgwNjQ=", "number": 678, "title": "How do I share the vocab between the source and target language for machine translation", "user": {"login": "thak123", "id": 3891859, "node_id": "MDQ6VXNlcjM4OTE4NTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3891859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thak123", "html_url": "https://github.com/thak123", "followers_url": "https://api.github.com/users/thak123/followers", "following_url": "https://api.github.com/users/thak123/following{/other_user}", "gists_url": "https://api.github.com/users/thak123/gists{/gist_id}", "starred_url": "https://api.github.com/users/thak123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thak123/subscriptions", "organizations_url": "https://api.github.com/users/thak123/orgs", "repos_url": "https://api.github.com/users/thak123/repos", "events_url": "https://api.github.com/users/thak123/events{/privacy}", "received_events_url": "https://api.github.com/users/thak123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-01-16T14:53:06Z", "updated_at": "2020-02-02T18:22:16Z", "closed_at": "2020-02-02T18:22:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi I do I create combined vocabulary from the source and target fields from the multi30k dataset\r\n. I am interested in having a shared encoder which can represent source as well the target words.\r\n\r\n```\r\nSRC = Field(tokenize=tokenize_de,\r\n            init_token='<sos>',\r\n            eos_token='<eos>',\r\n            lower=True,\r\n            batch_first=True)\r\n\r\nTRG = Field(tokenize=tokenize_en,\r\n            init_token='<sos>',\r\n            eos_token='<eos>',\r\n            lower=True,\r\n            batch_first=True)\r\n\r\ntrain_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\r\n                                                    fields=(SRC, TRG))\r\n\r\nSRC.build_vocab(train_data, min_freq=2)\r\nTRG.build_vocab(train_data, min_freq=2)\r\n\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/675", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/675/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/675/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/675/events", "html_url": "https://github.com/pytorch/text/issues/675", "id": 545809632, "node_id": "MDU6SXNzdWU1NDU4MDk2MzI=", "number": 675, "title": "Getting the erorr: AttributeError: 'Field' object has no attribute 'vocab' when setting `use_vocab` to `False` in a `Field`", "user": {"login": "abdullah-alnahas", "id": 5329543, "node_id": "MDQ6VXNlcjUzMjk1NDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5329543?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abdullah-alnahas", "html_url": "https://github.com/abdullah-alnahas", "followers_url": "https://api.github.com/users/abdullah-alnahas/followers", "following_url": "https://api.github.com/users/abdullah-alnahas/following{/other_user}", "gists_url": "https://api.github.com/users/abdullah-alnahas/gists{/gist_id}", "starred_url": "https://api.github.com/users/abdullah-alnahas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abdullah-alnahas/subscriptions", "organizations_url": "https://api.github.com/users/abdullah-alnahas/orgs", "repos_url": "https://api.github.com/users/abdullah-alnahas/repos", "events_url": "https://api.github.com/users/abdullah-alnahas/events{/privacy}", "received_events_url": "https://api.github.com/users/abdullah-alnahas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1959214834, "node_id": "MDU6TGFiZWwxOTU5MjE0ODM0", "url": "https://api.github.com/repos/pytorch/text/labels/legacy", "name": "legacy", "color": "bfd4f2", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-06T16:14:13Z", "updated_at": "2020-07-27T18:10:31Z", "closed_at": "2020-01-06T16:25:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\nHow to set `use_vocab` to `False` in a `Field` without getting the error: `AttributeError: 'Field' object has no attribute 'vocab'`?\r\n**Description**\r\nI am trying to use my own tokenization function in `torchtext.data.Field`. I am setting the `tokenize` argument to a custom function, and `use_vocab` to `False`. Then, after creating a dataset and an iterator I try to iterate through the dataset, however, I get the following error: \r\n`AttributeError: 'Field' object has no attribute 'vocab'`.\r\nHow to get over that?\r\nI get this error on my own machine as well as on a [colab notebook](https://colab.research.google.com/drive/1e008bRC1hhFpV0p9NbRfhwe0dus5ZnaG). \r\nIs it a bug? or me missing something?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/667", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/667/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/667/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/667/events", "html_url": "https://github.com/pytorch/text/issues/667", "id": 539109852, "node_id": "MDU6SXNzdWU1MzkxMDk4NTI=", "number": 667, "title": "In IMDB dataset , Number of training examples: 0 [ solved by myself]", "user": {"login": "YKX-A", "id": 38134733, "node_id": "MDQ6VXNlcjM4MTM0NzMz", "avatar_url": "https://avatars0.githubusercontent.com/u/38134733?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YKX-A", "html_url": "https://github.com/YKX-A", "followers_url": "https://api.github.com/users/YKX-A/followers", "following_url": "https://api.github.com/users/YKX-A/following{/other_user}", "gists_url": "https://api.github.com/users/YKX-A/gists{/gist_id}", "starred_url": "https://api.github.com/users/YKX-A/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YKX-A/subscriptions", "organizations_url": "https://api.github.com/users/YKX-A/orgs", "repos_url": "https://api.github.com/users/YKX-A/repos", "events_url": "https://api.github.com/users/YKX-A/events{/privacy}", "received_events_url": "https://api.github.com/users/YKX-A/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-17T14:41:30Z", "updated_at": "2019-12-17T16:03:15Z", "closed_at": "2019-12-17T16:03:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n```\r\nfrom torchtext import datasets\r\n\r\ntrain_data, test_data = datasets.IMDB.splits(TEXT, LABEL,root='.data')\r\ndatasets.IMDB.splits()\r\n\r\nprint(f'Number of training examples: {len(train_data)}')\r\nprint(f'Number of testing examples: {len(test_data)}')\r\n```\r\noutput \r\n```\r\nNumber of training examples: 0\r\nNumber of testing examples: 0\r\n```\r\nThere are \"./.data/imdb\" file in my current folder downloaded by  ''datasets.IMDB.splits()''. \r\n\r\nThe problem is the output should be : \r\n```\r\nNumber of training examples: 25000\r\nNumber of testing examples: 25000\r\n```\r\nfrom  https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\r\n\r\n<------------------------------------------------------->\r\n\ud83d\ude02, After I typed these word above just now.\r\nI return back to delete all the \".data\" folder and rerun `train_data, test_data = datasets.IMDB.splits(TEXT, LABEL,root='.data')`. All results seem to be right.\r\nI guess the reason is that I force-stop the jupyter notebook when torchtext download IMDB datasets.\r\nI do this because I want to use  proxy, and then I run the code in shell. Maybe my operations results some error.\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/659", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/659/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/659/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/659/events", "html_url": "https://github.com/pytorch/text/issues/659", "id": 530112848, "node_id": "MDU6SXNzdWU1MzAxMTI4NDg=", "number": 659, "title": "Pytorch Transformers: should I re-initialize my optimizer and my scheduler before I try to fine tune my neural network on the different dataset?", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-11-29T01:07:45Z", "updated_at": "2019-11-29T01:59:18Z", "closed_at": "2019-11-29T01:59:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nHello,\r\n\r\nI am doing NLP, and I have this block of Transformer body that was already trained on dataset A.\r\nNow I am interested in fine tuning this same Transformer on a new dataset B.\r\n\r\nIn my Python code, should I re-initialize my optimizer and my scheduler before I try to fine tune my neural network on the different dataset? or is this not needed?\r\n\r\nThank you,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/657", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/657/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/657/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/657/events", "html_url": "https://github.com/pytorch/text/issues/657", "id": 529463029, "node_id": "MDU6SXNzdWU1Mjk0NjMwMjk=", "number": 657, "title": "text_classification dataset default ngrams argument does not match comments", "user": {"login": "bentrevett", "id": 8006479, "node_id": "MDQ6VXNlcjgwMDY0Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/8006479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bentrevett", "html_url": "https://github.com/bentrevett", "followers_url": "https://api.github.com/users/bentrevett/followers", "following_url": "https://api.github.com/users/bentrevett/following{/other_user}", "gists_url": "https://api.github.com/users/bentrevett/gists{/gist_id}", "starred_url": "https://api.github.com/users/bentrevett/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bentrevett/subscriptions", "organizations_url": "https://api.github.com/users/bentrevett/orgs", "repos_url": "https://api.github.com/users/bentrevett/repos", "events_url": "https://api.github.com/users/bentrevett/events{/privacy}", "received_events_url": "https://api.github.com/users/bentrevett/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-27T16:45:08Z", "updated_at": "2019-12-05T19:57:36Z", "closed_at": "2019-12-05T19:57:36Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n\r\n`_setup_datasets` for the `text_classification` datasets has the `ngrams` argument default to 2, [see](https://github.com/pytorch/text/blob/master/torchtext/datasets/text_classification.py#L116)\r\n\r\nHowever the comments for each of the `text_classification` datasets says that the default `ngrams` argument is 1, [see](https://github.com/pytorch/text/blob/master/torchtext/datasets/text_classification.py#L159).\r\n\r\nSo the default in `_setup_datasets` should be changed.\r\n\r\nI would submit a pull request but it looks like this can be incorporated as part of adding the IMDB dataset: https://github.com/pytorch/text/pull/651\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/653", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/653/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/653/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/653/events", "html_url": "https://github.com/pytorch/text/issues/653", "id": 528726358, "node_id": "MDU6SXNzdWU1Mjg3MjYzNTg=", "number": 653, "title": "LanguageModelingDataset is not recognizing special tokens", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-26T13:31:21Z", "updated_at": "2019-11-26T13:39:47Z", "closed_at": "2019-11-26T13:39:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nWhen I define my Fields and perform split like below, under `train_Wiki2.examples[0].text`, `val_Wiki2.examples[0].text`, `test_Wiki2.examples[0].text`, I see that the end-of-sentence token `<eos>`,etc. are correctly recognized as a single token as a whole, but for some reason the unknown-token `<unk>` is not recognized as a single token as a whole:\r\n\r\n```js\r\n# define the English text field\r\nTEXT_Wiki2 = Field(tokenize = \"spacy\",\r\n             init_token = '<sos>',\r\n             eos_token = '<eos>',\r\n             unk_token = '<unk>',\r\n             pad_token = '<pad>',\r\n             tokenizer_language = 'en',\r\n             lower = True)\r\n\r\n# load WikiText2 dataset and split it into train and test set\r\ntrain_Wiki2, val_Wiki2, test_Wiki2 = torchtext.datasets.WikiText2.splits(TEXT_Wiki2)\r\n\r\ntrain_Wiki2.examples[0].text\r\n [ ...\r\n# <unk> NOT recognized as a single token\r\n '<',\r\n 'unk',\r\n '>',\r\n ...\r\n# <eos> recognized as a single token\r\n '<eos>',\r\n...]\r\n```\r\nHow can I make LanguageModelingDataset to recognize all of the special tokens such as `<sos>`,`<eos>`,`<unk>`,`<pad>`, and so on?\r\n\r\nThanks,\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/652", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/652/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/652/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/652/events", "html_url": "https://github.com/pytorch/text/issues/652", "id": 528704109, "node_id": "MDU6SXNzdWU1Mjg3MDQxMDk=", "number": 652, "title": "How to add special token in torch text.Data.Field( )?", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-26T12:50:00Z", "updated_at": "2019-11-26T13:40:24Z", "closed_at": "2019-11-26T13:40:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI defined my text Field as below:\r\n```js\r\nTEXT_openbookQA = Field(tokenize = \"spacy\",\r\n             init_token = '<sos>',\r\n             eos_token = '<eos>',\r\n             unk_token = '<unk>',\r\n             pad_token = '<pad>',\r\n             tokenizer_language = 'en',\r\n             lower = True)\r\n```\r\nHowever, in the text `openbookQA`, there is a special token named `<mcoption>`. How can I make the text Field to recognize this special token?\r\n\r\nThank you,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/650", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/650/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/650/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/650/events", "html_url": "https://github.com/pytorch/text/issues/650", "id": 528349392, "node_id": "MDU6SXNzdWU1MjgzNDkzOTI=", "number": 650, "title": "combining TEXT.build_vocab with flair embeddings", "user": {"login": "antgr", "id": 2175768, "node_id": "MDQ6VXNlcjIxNzU3Njg=", "avatar_url": "https://avatars3.githubusercontent.com/u/2175768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/antgr", "html_url": "https://github.com/antgr", "followers_url": "https://api.github.com/users/antgr/followers", "following_url": "https://api.github.com/users/antgr/following{/other_user}", "gists_url": "https://api.github.com/users/antgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/antgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/antgr/subscriptions", "organizations_url": "https://api.github.com/users/antgr/orgs", "repos_url": "https://api.github.com/users/antgr/repos", "events_url": "https://api.github.com/users/antgr/events{/privacy}", "received_events_url": "https://api.github.com/users/antgr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-11-25T21:33:12Z", "updated_at": "2020-01-31T14:17:07Z", "closed_at": "2019-11-27T12:39:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\nHi, we can use glove embedding  when building vocab, using \r\nsomething like:\r\n```\r\nMIN_FREQ = 2\r\n\r\nTEXT.build_vocab(train_data, \r\n                 min_freq = MIN_FREQ,\r\n                 vectors = \"glove.6B.300d\",\r\n                 unk_init = torch.Tensor.normal_)\r\n```\r\nWe also can create embeddings using flair library, using for example:\r\n``` \r\nembedding_types: List[TokenEmbeddings] = [\r\n \r\n    WordEmbeddings('glove'),\r\n \r\n    # comment in this line to use character embeddings\r\n    #CharacterEmbeddings(),\r\n \r\n    # comment in these lines to use flair embeddings\r\n    FlairEmbeddings('news-forward'),\r\n    FlairEmbeddings('news-backward'),\r\n    ELMoEmbeddings(),\r\n    BertEmbeddings('bert-base-uncased'),\r\n]\r\n \r\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\r\n```\r\nCould I use the above embeddings instead of glove in the above code?\r\nIs anything similar to this supported?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/649", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/649/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/649/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/649/events", "html_url": "https://github.com/pytorch/text/issues/649", "id": 527178937, "node_id": "MDU6SXNzdWU1MjcxNzg5Mzc=", "number": 649, "title": "How to perform common sense reasoning task with GPT-2?", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-22T12:52:44Z", "updated_at": "2019-11-23T14:38:47Z", "closed_at": "2019-11-23T14:38:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI am new to NLP so I have lots of questions.\r\nI am interested in carrying out common sense reasoning task with GPT-2, for example, with Winograd Schema Challenge dataset.\r\n\r\nQ1. How should I tokenize the Winograd Schema Challenge dataset to process it with GPT-2 (with the double heads model, for instance)? Can someone please give me an example?\r\n\r\nQ2. Can GPT2DoubleHeadsModel be used to conduct common sense reasoning task with Winograd Schema Challenge dataset?\r\n\r\nThank you,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/647", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/647/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/647/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/647/events", "html_url": "https://github.com/pytorch/text/issues/647", "id": 525847859, "node_id": "MDU6SXNzdWU1MjU4NDc4NTk=", "number": 647, "title": "Length of a dataset", "user": {"login": "antgr", "id": 2175768, "node_id": "MDQ6VXNlcjIxNzU3Njg=", "avatar_url": "https://avatars3.githubusercontent.com/u/2175768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/antgr", "html_url": "https://github.com/antgr", "followers_url": "https://api.github.com/users/antgr/followers", "following_url": "https://api.github.com/users/antgr/following{/other_user}", "gists_url": "https://api.github.com/users/antgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/antgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/antgr/subscriptions", "organizations_url": "https://api.github.com/users/antgr/orgs", "repos_url": "https://api.github.com/users/antgr/repos", "events_url": "https://api.github.com/users/antgr/events{/privacy}", "received_events_url": "https://api.github.com/users/antgr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-11-20T14:43:43Z", "updated_at": "2019-11-20T15:30:06Z", "closed_at": "2019-11-20T15:04:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\nHello, how could I calculate the length of a default dataset like imdb etc,  With length I mean the number of its examples.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/644", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/644/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/644/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/644/events", "html_url": "https://github.com/pytorch/text/issues/644", "id": 524728873, "node_id": "MDU6SXNzdWU1MjQ3Mjg4NzM=", "number": 644, "title": "Using Google 1-billion benchmark data on PyTorch", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2019-11-19T02:12:10Z", "updated_at": "2019-11-20T01:03:20Z", "closed_at": "2019-11-20T01:03:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI am new to NLP and I have some questions.\r\n\r\nI downloaded the Google 1-billion benchmark dataset, and I am trying to use the dataset on PyTorch:\r\n\r\n```js\r\n               \r\n# Import packages \r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom sklearn.model_selection import train_test_split\r\nfrom torchtext.data import Field, BucketIterator, TabularDataset\r\nfrom transformers import OpenAIGPTConfig, OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\r\nfrom transformers import AdamW, WarmupLinearSchedule\r\nfrom scipy.spatial import distance\r\nimport spacy\r\nimport torchtext\r\nfrom torchtext.data.utils import get_tokenizer\r\nfrom torchtext.data import Field, BPTTIterator\r\nimport tensorflow as tf\r\n#import lineflow as lf\r\n#import lineflow.datasets as lfds\r\nimport math\r\nimport random\r\nimport numpy as np\r\nimport pandas as pd \r\nimport time\r\n\r\n# set hyperparameters for this experiment\r\nbptt = 30\r\nbatch_size = 64\r\nlr = 0.01 # learning rate\r\n#criterion = nn.CrossEntropyLoss() # loss criterion\r\nlog_interval = 200\r\nnlayer = 6\r\n\r\n# define tokenizer\r\nen = spacy.load('en')\r\n\r\ndef Sp_Tokenizer(text): \r\n    return [tok.text for tok in en.tokenizer(text)]\r\n\r\n# define the English text field\r\nTEXT = Field(tokenize = Sp_Tokenizer,\r\n             init_token = '<sos>',\r\n             eos_token = '<eos>',\r\n             unk_token = '<unk>',\r\n             pad_token = '<pad>',\r\n             tokenizer_language = 'en',\r\n             lower = True)\r\n\r\n# loading Google 1 Billion Benchmark dataset\r\nbillion_google = open('/Users/dev/billion_google', encoding='utf-8').read()\r\nbillion_google_dict = {'English' : [line for line in billion_google]}\r\n# convert billion_google into a pandas dataframe\r\nbillion_google_df = pd.DataFrame(billion_google_dict, columns=[\"English\"])\r\n\r\n# remove very long sentences\r\nbillion_google_df['eng_len'] = billion_google_df['English'].str.count(' ')\r\nbillion_google_df = billion_google_df.query('eng_len < 1025')\r\n\r\n# create train and test set \r\ntrain_billion_google, test_billion_google = train_test_split(billion_google_df, test_size=0.2)\r\ntrain_billion_google.to_csv(\"train_billion_google.csv\", index=False)\r\ntest_billion_google.to_csv(\"test_billion_google.csv\", index=False)\r\n\r\ndata_fields = [('English', TEXT)]\r\ntrain_billion_google, test_billion_google = TabularDataset.splits(path='./', \r\n                                                                  train='train_billion_google.csv',\r\n                                                                  validation='test_billion_google.csv', \r\n                                                                  format='csv', \r\n                                                                  fields=data_fields)\r\n```\r\n\r\nI also want to make a use of WikiText2 that is built in PyTorch:\r\n\r\n```json\r\ntrain_Wiki2, val_Wiki2, test_Wiki2 = torchtext.datasets.WikiText2.splits(TEXT)\r\n```\r\n...and I want my `train_billion_google` to have the same structure as my `train_Wiki2`, more specifically, I want my `train_billion_google` to store a list of individual tokens under `train_billion_google.examples[0].text`.\r\n\r\nHow can I do this?\r\n\r\nThank you,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/643", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/643/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/643/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/643/events", "html_url": "https://github.com/pytorch/text/issues/643", "id": 523787103, "node_id": "MDU6SXNzdWU1MjM3ODcxMDM=", "number": 643, "title": "How to skip last batch that has a different batch size?", "user": {"login": "Hans0124SG", "id": 18539093, "node_id": "MDQ6VXNlcjE4NTM5MDkz", "avatar_url": "https://avatars0.githubusercontent.com/u/18539093?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hans0124SG", "html_url": "https://github.com/Hans0124SG", "followers_url": "https://api.github.com/users/Hans0124SG/followers", "following_url": "https://api.github.com/users/Hans0124SG/following{/other_user}", "gists_url": "https://api.github.com/users/Hans0124SG/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hans0124SG/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hans0124SG/subscriptions", "organizations_url": "https://api.github.com/users/Hans0124SG/orgs", "repos_url": "https://api.github.com/users/Hans0124SG/repos", "events_url": "https://api.github.com/users/Hans0124SG/events{/privacy}", "received_events_url": "https://api.github.com/users/Hans0124SG/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-16T04:08:41Z", "updated_at": "2019-11-18T15:54:07Z", "closed_at": "2019-11-18T15:54:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\n\r\nSorry if this is a newbie question.\r\nIn `torch.nn.utils.data.dataloader` we can drop the last batch by specifying `drop_last=True`.\r\nDo we have something equivalent for our `Iterator`? Currently I continue the training loop if I see the current `batch_size` is different from my preset `batch_size`. Is there something built-in?\r\n\r\nThank you very much!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/635", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/635/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/635/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/635/events", "html_url": "https://github.com/pytorch/text/issues/635", "id": 516824737, "node_id": "MDU6SXNzdWU1MTY4MjQ3Mzc=", "number": 635, "title": "Can I get embedding vectors from my pretrained vectors?", "user": {"login": "lsh23", "id": 34297340, "node_id": "MDQ6VXNlcjM0Mjk3MzQw", "avatar_url": "https://avatars2.githubusercontent.com/u/34297340?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lsh23", "html_url": "https://github.com/lsh23", "followers_url": "https://api.github.com/users/lsh23/followers", "following_url": "https://api.github.com/users/lsh23/following{/other_user}", "gists_url": "https://api.github.com/users/lsh23/gists{/gist_id}", "starred_url": "https://api.github.com/users/lsh23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lsh23/subscriptions", "organizations_url": "https://api.github.com/users/lsh23/orgs", "repos_url": "https://api.github.com/users/lsh23/repos", "events_url": "https://api.github.com/users/lsh23/events{/privacy}", "received_events_url": "https://api.github.com/users/lsh23/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-03T13:01:35Z", "updated_at": "2019-11-05T08:03:03Z", "closed_at": "2019-11-05T08:03:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n```\r\n    tokenize = lambda x: x.split()\r\n    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\r\n    LABEL = data.LabelField()\r\n\r\n    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\r\n    train_data\r\n\r\n\r\n    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\r\n    LABEL.build_vocab(train_data)\r\n\r\n    word_embeddings = TEXT.vocab.vectors\r\n    TEXT.vocab\r\n```\r\n\r\nI want to get TEXT.vocab.vectors from not the vectors torchtext had but my pretrained vector.\r\n\r\nfor example \r\n```\r\nTEXT.build_vocab(train_data, vectors=my_pretrained_vectors)\r\n```\r\nand then I will get the torch text embedding vectors from my_pretrained_vectors.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/633", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/633/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/633/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/633/events", "html_url": "https://github.com/pytorch/text/issues/633", "id": 515924498, "node_id": "MDU6SXNzdWU1MTU5MjQ0OTg=", "number": 633, "title": "Custom Dataset class in Torchtext with variable number of Fields ", "user": {"login": "LeenaShekhar", "id": 12227436, "node_id": "MDQ6VXNlcjEyMjI3NDM2", "avatar_url": "https://avatars1.githubusercontent.com/u/12227436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LeenaShekhar", "html_url": "https://github.com/LeenaShekhar", "followers_url": "https://api.github.com/users/LeenaShekhar/followers", "following_url": "https://api.github.com/users/LeenaShekhar/following{/other_user}", "gists_url": "https://api.github.com/users/LeenaShekhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/LeenaShekhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LeenaShekhar/subscriptions", "organizations_url": "https://api.github.com/users/LeenaShekhar/orgs", "repos_url": "https://api.github.com/users/LeenaShekhar/repos", "events_url": "https://api.github.com/users/LeenaShekhar/events{/privacy}", "received_events_url": "https://api.github.com/users/LeenaShekhar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-11-01T05:44:29Z", "updated_at": "2019-11-04T10:34:07Z", "closed_at": "2019-11-04T10:34:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\nA custom dataset in Torchtext where each data is (text1, text2, and label) can be written something like below:\r\n\r\n\r\n```\r\nclass CustomDataset(ttdata.Dataset):\r\n    'Reads a JSON file. Each line has text1, text2, and the associated label.'\r\n\r\n    def __init__(self, path, max_seq_length=100, min_seq_length=3, add_eos=True):\r\n\r\n        text1_field = Field(lower=True, tokenize='spacy')\r\n        text2_field = Field(lower=True, tokenize='spacy')\r\n        label_field = ttdata.Field(sequential=False, use_vocab=False)\r\n        fields = [('text1', text1_field), ('text2', text2_field), ('label', label_field)]\r\n\r\n        examples = []\r\n\r\n        with open(path, 'r') as f:\r\n            for item in f:\r\n                item = json.loads(item)\r\n                text1, text2, label = item['text1'], item['text2'], item['label']\r\n                examples.append(ttdata.Example.fromlist([text1, text2, label], fields))\r\n\r\n\r\n        super(CustomDataset, self).__init__(examples, fields)\r\n```\r\n\r\nBut in my case rather than text1 and text2, I have 2 lists of strings of variable length (text1 = [\"some text\", \"some text\", ...] and text2 = [\"yet some text\", \"yet some text\", ...])and a label for the 2 lists. \r\n\r\nHow can I write a dataset class in this case? I think the main problem is now I cannot define fixed number of Field objects. Can NestedField be used in some way?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/629", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/629/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/629/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/629/events", "html_url": "https://github.com/pytorch/text/issues/629", "id": 512939667, "node_id": "MDU6SXNzdWU1MTI5Mzk2Njc=", "number": 629, "title": "How to use custom-built Torchtext vocabulary with the HuggingFace TransfoXLLMHeadModel?", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-10-27T09:02:13Z", "updated_at": "2019-11-01T15:21:23Z", "closed_at": "2019-11-01T15:21:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI am trying to use my custom built vocabulary which I defined using Torchtext functions with the HuggingFace TransfoXLLMHeadModel, and I am having some troubles with it.\r\nI defined my text field as below:\r\n```js\r\n\r\n# Import packages \r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom transformers import TransfoXLConfig, TransfoXLTokenizer, TransfoXLLMHeadModel\r\nfrom transformers import AdamW, WarmupLinearSchedule\r\nimport spacy\r\nimport torchtext\r\nfrom torchtext.data.utils import get_tokenizer\r\nfrom torchtext.data import Field, BPTTIterator, TabularDataset \r\nimport tensorflow as tf\r\n#import lineflow as lf\r\n#import lineflow.datasets as lfds\r\nimport math\r\nimport random\r\nimport numpy as np\r\nimport pandas as pd \r\nimport time\r\n\r\n# define tokenizer\r\nen = spacy.load('en')\r\n\r\ndef Sp_Tokenizer(text): \r\n    return [tok.text for tok in en.tokenizer(text)]\r\n\r\n# define the English text field\r\nTEXT = Field(tokenize = Sp_Tokenizer,\r\n             init_token='< sos >',\r\n             eos_token='< eos >',\r\n             unk_token='< unk >',\r\n             tokenizer_language='en',\r\n             lower=True)\r\n\r\n# load WikiText-2 dataset and split it into train and test set\r\ntrain_Wiki2, val_Wiki2, test_Wiki2 = torchtext.datasets.WikiText2.splits(TEXT)\r\ntrain_Wiki103, val_Wiki103, test_Wiki103 = torchtext.datasets.WikiText103.splits(TEXT)\r\ntrain_Penn, val_Penn, test_Penn = torchtext.datasets.PennTreebank.splits(TEXT)\r\n\r\n# build custom vocabulary based on the field that we just defined.\r\nTEXT.build_vocab(train_Wiki2, val_Wiki2, test_Wiki2, \r\n                 train_Wiki103, val_Wiki103, test_Wiki103,\r\n                 train_Penn, val_Penn, test_Penn)\r\n```\r\nand then I defined the HuggingFace transformer's configuration as below:\r\n```js\r\n\r\n# set hyperparameter ntokens\r\nntokens = len(TEXT.vocab.stoi)\r\n\r\n# define transformer-XL configuration.\r\ntransfoXLconfig = TransfoXLConfig(vocab_size_or_config_json_file = ntokens,\r\n                                  cutoffs = [20000, 40000, 200000], \r\n                                  d_model = 64, \r\n                                  d_embed = 64, \r\n                                  n_head = 16, \r\n                                  d_head = 64,\r\n                                  n_layer = 5,\r\n                                  attn_type = 0,\r\n                                  dropout = 0.1, \r\n                                  output_hidden_states = True,\r\n                                  output_attentions = True)\r\n\r\n# define the transformer-XL model based on the specified configuration.\r\nmodel = TransfoXLLMHeadModel(transfoXLconfig)\r\n\r\n# add new tokens to the embeddings of our model\r\nmodel.resize_token_embeddings(ntokens)\r\n```\r\nand then I want to somehow specify that I want to use my `TEXT.vocab` that I defined earlier via Torchtext for my vocabulary along with the TransfoXLLMHeadModel, but I am not sure how to do this. Can someone help me on this? Thank you!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/628", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/628/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/628/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/628/events", "html_url": "https://github.com/pytorch/text/issues/628", "id": 512776826, "node_id": "MDU6SXNzdWU1MTI3NzY4MjY=", "number": 628, "title": "can't download dataset from drive.google.com", "user": {"login": "yonghangzhou", "id": 24536813, "node_id": "MDQ6VXNlcjI0NTM2ODEz", "avatar_url": "https://avatars1.githubusercontent.com/u/24536813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yonghangzhou", "html_url": "https://github.com/yonghangzhou", "followers_url": "https://api.github.com/users/yonghangzhou/followers", "following_url": "https://api.github.com/users/yonghangzhou/following{/other_user}", "gists_url": "https://api.github.com/users/yonghangzhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/yonghangzhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yonghangzhou/subscriptions", "organizations_url": "https://api.github.com/users/yonghangzhou/orgs", "repos_url": "https://api.github.com/users/yonghangzhou/repos", "events_url": "https://api.github.com/users/yonghangzhou/events{/privacy}", "received_events_url": "https://api.github.com/users/yonghangzhou/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-26T01:58:09Z", "updated_at": "2019-10-27T15:12:44Z", "closed_at": "2019-10-27T15:12:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \ud83d\udc1b Bug\r\n**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nwhen i run train_dataset,test_dataset = text_classification.DATASETS['AG_NEWS'](root = './data/text_classcification',ngrams=NGRAMS,vocab=None)\r\n\r\nit will be error \uff1a\r\nConnectionError: HTTPSConnectionPool(host='drive.google.com', port=443): Max retries exceeded with url: /uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f475c8403c8>: Failed to establish a new connection: [Errno 110] Connection timed out',))", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/625", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/625/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/625/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/625/events", "html_url": "https://github.com/pytorch/text/issues/625", "id": 511589997, "node_id": "MDU6SXNzdWU1MTE1ODk5OTc=", "number": 625, "title": "id field in the example", "user": {"login": "imran3180", "id": 6565456, "node_id": "MDQ6VXNlcjY1NjU0NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6565456?v=4", "gravatar_id": "", "url": "https://api.github.com/users/imran3180", "html_url": "https://github.com/imran3180", "followers_url": "https://api.github.com/users/imran3180/followers", "following_url": "https://api.github.com/users/imran3180/following{/other_user}", "gists_url": "https://api.github.com/users/imran3180/gists{/gist_id}", "starred_url": "https://api.github.com/users/imran3180/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/imran3180/subscriptions", "organizations_url": "https://api.github.com/users/imran3180/orgs", "repos_url": "https://api.github.com/users/imran3180/repos", "events_url": "https://api.github.com/users/imran3180/events{/privacy}", "received_events_url": "https://api.github.com/users/imran3180/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-23T21:49:06Z", "updated_at": "2019-12-03T23:18:18Z", "closed_at": "2019-12-03T23:18:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\nHow can I find the id of the particular example while processing the batch?\r\n\r\nI need it because I have to maintain a particular statistics for each example while training. Since Iterators shuffle the example while batching, so the order is getting lost. \r\n\r\nOr Is there any workaround by which I can identify the reverse mapping of examples in batch to the original datasets.? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/623", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/623/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/623/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/623/events", "html_url": "https://github.com/pytorch/text/issues/623", "id": 511276219, "node_id": "MDU6SXNzdWU1MTEyNzYyMTk=", "number": 623, "title": "Is vocab embedding shared across fields\uff1f", "user": {"login": "lwgkzl", "id": 30727306, "node_id": "MDQ6VXNlcjMwNzI3MzA2", "avatar_url": "https://avatars3.githubusercontent.com/u/30727306?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lwgkzl", "html_url": "https://github.com/lwgkzl", "followers_url": "https://api.github.com/users/lwgkzl/followers", "following_url": "https://api.github.com/users/lwgkzl/following{/other_user}", "gists_url": "https://api.github.com/users/lwgkzl/gists{/gist_id}", "starred_url": "https://api.github.com/users/lwgkzl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lwgkzl/subscriptions", "organizations_url": "https://api.github.com/users/lwgkzl/orgs", "repos_url": "https://api.github.com/users/lwgkzl/repos", "events_url": "https://api.github.com/users/lwgkzl/events{/privacy}", "received_events_url": "https://api.github.com/users/lwgkzl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-23T12:02:10Z", "updated_at": "2019-10-23T18:29:25Z", "closed_at": "2019-10-23T18:29:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\nWhen i train to generat a dialog response by a question in different field. Are questions and answers shared their embedding?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/619", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/619/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/619/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/619/events", "html_url": "https://github.com/pytorch/text/issues/619", "id": 507292271, "node_id": "MDU6SXNzdWU1MDcyOTIyNzE=", "number": 619, "title": "How to use torchtext for sequence labelling with wordpiece tokeniers", "user": {"login": "JohnGiorgi", "id": 8917831, "node_id": "MDQ6VXNlcjg5MTc4MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/8917831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JohnGiorgi", "html_url": "https://github.com/JohnGiorgi", "followers_url": "https://api.github.com/users/JohnGiorgi/followers", "following_url": "https://api.github.com/users/JohnGiorgi/following{/other_user}", "gists_url": "https://api.github.com/users/JohnGiorgi/gists{/gist_id}", "starred_url": "https://api.github.com/users/JohnGiorgi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JohnGiorgi/subscriptions", "organizations_url": "https://api.github.com/users/JohnGiorgi/orgs", "repos_url": "https://api.github.com/users/JohnGiorgi/repos", "events_url": "https://api.github.com/users/JohnGiorgi/events{/privacy}", "received_events_url": "https://api.github.com/users/JohnGiorgi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-10-15T14:42:09Z", "updated_at": "2020-02-22T03:22:23Z", "closed_at": "2020-02-22T03:22:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\n\r\nHi,\r\n\r\nIn a previous issue (#609), I asked how to use the tokenizer from the [Transformers](https://github.com/huggingface/transformers) library with torch text.\r\n\r\nI now would like to be able to use this tokenizer and torchtext to load sequence labelling datasets. The issue I am facing is that the tokenizer introduces wordpiece tokens, which ends up breaking the alignment between tokens and labels.\r\n\r\nIgnoring labels, I am able to load a sequence labelling dataset with a Transformer tokenizer like so,\r\n\r\n```python\r\nfrom torchtext import data\r\nfrom torchtext import datasets\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\r\n\r\ndef preprocessor(batch):\r\n    return tokenizer.encode(batch, add_special_tokens=True)\r\n\r\nTEXT = data.Field(\r\n    use_vocab=False,\r\n    batch_first=True,\r\n    pad_token=tokenizer.pad_token_id,\r\n    preprocessing=preprocessor\r\n)\r\n# LABEL = data.LabelField()\r\n\r\nfields = [('text', TEXT), ('unused_col_1', None), ('unused_col_2', None), ('label', None)]\r\n\r\ntrain, valid, test = datasets.SequenceTaggingDataset.splits(\r\n    path='/Users/johngiorgi/Downloads/bert_data/BC5CDR/chem',\r\n    train='train.tsv',\r\n    validation='devel.tsv',\r\n    test='test.tsv',\r\n    fields=fields\r\n)\r\n\r\ntrain_iter, valid_iter, test_iter = data.BucketIterator.splits(\r\n    (train, valid, test), batch_sizes=(16, 256, 256)\r\n)\r\n\r\n# LABEL.build_vocab(train)\r\n``` \r\n\r\nThe data comes from [here](https://github.com/ncbi-nlp/BLUE_Benchmark/releases/download/0.1/bert_data.zip), and is a tab-seperated file with four columns. The first column contains words, the last labels and each sentence is sperated by a newline, e.g.\r\n\r\n```\r\nNaloxone\t227508\t0\tB\r\nreverses\t-\t9\tO\r\nthe\t-\t18\tO\r\nantihypertensive\t-\t22\tO\r\neffect\t-\t39\tO\r\nof\t-\t46\tO\r\nclonidine\t-\t49\tB\r\n.\t-\t58\tO\r\n\r\nIn\t227508\t60\tO\r\n.\r\n.\r\n.\r\n```\r\n\r\nBut when I try to load the labels, e.g.\r\n\r\n```python\r\nfrom torchtext import data\r\nfrom torchtext import datasets\r\nfrom transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\r\n\r\ndef preprocessor(batch):\r\n    return tokenizer.encode(batch, add_special_tokens=True)\r\n\r\nTEXT = data.Field(\r\n    use_vocab=False,\r\n    batch_first=True,\r\n    pad_token=tokenizer.pad_token_id,\r\n    preprocessing=preprocessor\r\n)\r\nLABEL = data.LabelField()\r\n\r\nfields = [('text', TEXT), ('unused_col_1', None), ('unused_col_2', None), ('label', LABEL)]\r\n\r\ntrain, valid, test = datasets.SequenceTaggingDataset.splits(\r\n    path='/Users/johngiorgi/Downloads/bert_data/BC5CDR/chem',\r\n    train='train.tsv',\r\n    validation='devel.tsv',\r\n    test='test.tsv',\r\n    fields=fields\r\n)\r\n\r\ntrain_iter, valid_iter, test_iter = data.BucketIterator.splits(\r\n    (train, valid, test), batch_sizes=(16, 256, 256)\r\n)\r\n\r\nLABEL.build_vocab(train)\r\n``` \r\n\r\nI get issues when trying to access the batch\r\n\r\n```python\r\nbatch = next(iter(train_iter))\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-39-9919119fad82> in <module>\r\n----> 1 batch = next(iter(train_iter))\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/iterator.py in __iter__(self)\r\n    154                     else:\r\n    155                         minibatch.sort(key=self.sort_key, reverse=True)\r\n--> 156                 yield Batch(minibatch, self.dataset, self.device)\r\n    157             if not self.repeat:\r\n    158                 return\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/batch.py in __init__(self, data, dataset, device)\r\n     32                 if field is not None:\r\n     33                     batch = [getattr(x, name) for x in data]\r\n---> 34                     setattr(self, name, field.process(batch, device=device))\r\n     35 \r\n     36     @classmethod\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/field.py in process(self, batch, device)\r\n    235         \"\"\"\r\n    236         padded = self.pad(batch)\r\n--> 237         tensor = self.numericalize(padded, device=device)\r\n    238         return tensor\r\n    239 \r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/field.py in numericalize(self, arr, device)\r\n    336                 arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\n    337             else:\r\n--> 338                 arr = [self.vocab.stoi[x] for x in arr]\r\n    339 \r\n    340             if self.postprocessing is not None:\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/field.py in <listcomp>(.0)\r\n    336                 arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\n    337             else:\r\n--> 338                 arr = [self.vocab.stoi[x] for x in arr]\r\n    339 \r\n    340             if self.postprocessing is not None:\r\n\r\nTypeError: unhashable type: 'list'\r\n```\r\n\r\nWhich I am guessing arise because the number of items in the text and label fields are no longer the same.\r\n\r\nHas anyone come across this issue and been able to solve it? I know how to write a function to re-align the labels with the wordpiece tokenized text. Where might I insert that function in the loading process?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/617", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/617/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/617/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/617/events", "html_url": "https://github.com/pytorch/text/issues/617", "id": 505795182, "node_id": "MDU6SXNzdWU1MDU3OTUxODI=", "number": 617, "title": "Field information unavailable for validation set using BucketIterator.splits", "user": {"login": "ZeerakW", "id": 207574, "node_id": "MDQ6VXNlcjIwNzU3NA==", "avatar_url": "https://avatars0.githubusercontent.com/u/207574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZeerakW", "html_url": "https://github.com/ZeerakW", "followers_url": "https://api.github.com/users/ZeerakW/followers", "following_url": "https://api.github.com/users/ZeerakW/following{/other_user}", "gists_url": "https://api.github.com/users/ZeerakW/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZeerakW/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZeerakW/subscriptions", "organizations_url": "https://api.github.com/users/ZeerakW/orgs", "repos_url": "https://api.github.com/users/ZeerakW/repos", "events_url": "https://api.github.com/users/ZeerakW/events{/privacy}", "received_events_url": "https://api.github.com/users/ZeerakW/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-10-11T11:24:46Z", "updated_at": "2019-10-31T16:33:41Z", "closed_at": "2019-10-31T16:33:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "When generating data splits using BucketIterator.splits the fields are only applicable on the train data, meaning that it's impossible to iterate through dev or test splits.\r\n\r\nFollowing [example 5](https://pytorch.org/text/examples.html) in the docs the issue occurs when iterating over the dev or test splits.\r\n\r\nRunning the code below \r\n\r\n```python\r\nfrom torchtext.data import BucketIterator, TabularDataset, Field\r\n\r\ntext = Field(sequential = True,\r\n                  include_lengths=True,\r\n                  use_vocab=True)\r\nlabel = Field(sequential = False,\r\n              include_lengths = False,\r\n              use_vocab = True,\r\n              pad_token = None,\r\n              unk_token = None)\r\n\r\nfields = [('id', None),\r\n          ('bad_word', text),\r\n          ('question', text),\r\n          ('question_sentiment_gold', label),\r\n          ('answer', text),\r\n          ('answer_sentiment_gold', label),\r\n          ('username', text)]\r\n\r\ntrain, dev, test = TabularDataset.splits(path = '~/Documents/project/data/',\r\n                                         format = 'csv',\r\n                                         fields = fields,\r\n                                         train = 'train.csv', validation = 'dev.csv', test = 'test.csv',\r\n                                         skip_header = True)\r\n\r\ntrain_iter, dev_iter, test_iter = BucketIterator.splits((train, dev, test),\r\n                                                        batch_sizes = (32, 32, 32),\r\n                                                        sort_key = lambda x: len(x.question),\r\n                                                        device = 'cpu')\r\n\r\nprint(train, dev, test)\r\n\r\ntext.build_vocab(train)\r\nlabel.build_vocab(train)\r\n\r\nprint(\"Train iter\", next(iter(train_iter)))\r\nprint(\"Test iter\", next(iter(test_iter)))\r\nprint(\"dev iter\", next(iter(dev_iter)))\r\n```\r\nproduces the correct output:\r\n\r\n```python\r\n<torchtext.data.dataset.TabularDataset object at 0x10f6974d0> <torchtext.data.dataset.TabularDataset object at 0x1182c8b50> <torchtext.data.dataset.TabularDataset object at 0x118290ad0>\r\nTrain iter\r\n[torchtext.data.batch.Batch of size 32]\r\n\t[.bad_word]:('[torch.LongTensor of size 1x32]', '[torch.LongTensor of size 32]')\r\n\t[.question]:('[torch.LongTensor of size 57x32]', '[torch.LongTensor of size 32]')\r\n\t[.question_sentiment_gold]:[torch.LongTensor of size 32]\r\n\t[.answer]:('[torch.LongTensor of size 47x32]', '[torch.LongTensor of size 32]')\r\n\t[.answer_sentiment_gold]:[torch.LongTensor of size 32]\r\n\t[.username]:('[torch.LongTensor of size 1x32]', '[torch.LongTensor of size 32]')\r\nTest iter\r\n[torchtext.data.batch.Batch of size 32]\r\n\t[.bad_word]:('[torch.LongTensor of size 1x32]', '[torch.LongTensor of size 32]')\r\n\t[.question]:('[torch.LongTensor of size 4x32]', '[torch.LongTensor of size 32]')\r\n\t[.question_sentiment_gold]:[torch.LongTensor of size 32]\r\n\t[.answer]:('[torch.LongTensor of size 126x32]', '[torch.LongTensor of size 32]')\r\n\t[.answer_sentiment_gold]:[torch.LongTensor of size 32]\r\n\t[.username]:('[torch.LongTensor of size 1x32]', '[torch.LongTensor of size 32]')\r\n```\r\n\r\nand this traceback\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"tester.py\", line 93, in <module>\r\n    print(\"dev iter\", next(iter(dev_iter)))\r\n  File \"/Users/zee/.virtualenvs/generalisable/lib/python3.7/site-packages/torchtext/data/iterator.py\", line 141, in __iter__\r\n    self.init_epoch()\r\n  File \"/Users/zee/.virtualenvs/generalisable/lib/python3.7/site-packages/torchtext/data/iterator.py\", line 117, in init_epoch\r\n    self.create_batches()\r\n  File \"/Users/zee/.virtualenvs/generalisable/lib/python3.7/site-packages/torchtext/data/iterator.py\", line 245, in create_batches\r\n    self.batches = batch(self.data(), self.batch_size,\r\n  File \"/Users/zee/.virtualenvs/generalisable/lib/python3.7/site-packages/torchtext/data/iterator.py\", line 102, in data\r\n    xs = sorted(self.dataset, key=self.sort_key)\r\n  File \"tester.py\", line 83, in <lambda>\r\n    sort_key = lambda x: len(x.question),\r\nAttributeError: 'Example' object has no attribute 'question'\r\n```\r\n\r\n**Environment**\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0\r\n - OS (e.g., Linux): OSX 10.14.5\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.7.4", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/615", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/615/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/615/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/615/events", "html_url": "https://github.com/pytorch/text/issues/615", "id": 505119946, "node_id": "MDU6SXNzdWU1MDUxMTk5NDY=", "number": 615, "title": "Why can not find some models and weights(such as bert ) in this repository", "user": {"login": "daydayfun", "id": 39835967, "node_id": "MDQ6VXNlcjM5ODM1OTY3", "avatar_url": "https://avatars2.githubusercontent.com/u/39835967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daydayfun", "html_url": "https://github.com/daydayfun", "followers_url": "https://api.github.com/users/daydayfun/followers", "following_url": "https://api.github.com/users/daydayfun/following{/other_user}", "gists_url": "https://api.github.com/users/daydayfun/gists{/gist_id}", "starred_url": "https://api.github.com/users/daydayfun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daydayfun/subscriptions", "organizations_url": "https://api.github.com/users/daydayfun/orgs", "repos_url": "https://api.github.com/users/daydayfun/repos", "events_url": "https://api.github.com/users/daydayfun/events{/privacy}", "received_events_url": "https://api.github.com/users/daydayfun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-10T08:34:32Z", "updated_at": "2019-10-12T06:14:47Z", "closed_at": "2019-10-12T06:14:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\nIf I want to add Bert for pytorch community, can I push models and weights of BERT in **pytorch/text**", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/611", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/611/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/611/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/611/events", "html_url": "https://github.com/pytorch/text/issues/611", "id": 502801392, "node_id": "MDU6SXNzdWU1MDI4MDEzOTI=", "number": 611, "title": "Getting started", "user": {"login": "Hravan", "id": 31450069, "node_id": "MDQ6VXNlcjMxNDUwMDY5", "avatar_url": "https://avatars1.githubusercontent.com/u/31450069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hravan", "html_url": "https://github.com/Hravan", "followers_url": "https://api.github.com/users/Hravan/followers", "following_url": "https://api.github.com/users/Hravan/following{/other_user}", "gists_url": "https://api.github.com/users/Hravan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hravan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hravan/subscriptions", "organizations_url": "https://api.github.com/users/Hravan/orgs", "repos_url": "https://api.github.com/users/Hravan/repos", "events_url": "https://api.github.com/users/Hravan/events{/privacy}", "received_events_url": "https://api.github.com/users/Hravan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-04T19:16:01Z", "updated_at": "2019-10-04T19:26:43Z", "closed_at": "2019-10-04T19:26:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\nI would like to contribute to torchtext and therefore I decided to learn more about the internals of the library. Is there any guide that introduces into torchtext? I mean something that presents the core assumptions, classes, functionalities, etc., so that a person can learn how to use torchtext incrementally. If not and the only way to learn more about torchtext is to read the source code, maybe making such a document is a good first issue.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/609", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/609/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/609/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/609/events", "html_url": "https://github.com/pytorch/text/issues/609", "id": 501457901, "node_id": "MDU6SXNzdWU1MDE0NTc5MDE=", "number": 609, "title": "How might I use the tokenizers from the HuggingFace Transformers library", "user": {"login": "JohnGiorgi", "id": 8917831, "node_id": "MDQ6VXNlcjg5MTc4MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/8917831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JohnGiorgi", "html_url": "https://github.com/JohnGiorgi", "followers_url": "https://api.github.com/users/JohnGiorgi/followers", "following_url": "https://api.github.com/users/JohnGiorgi/following{/other_user}", "gists_url": "https://api.github.com/users/JohnGiorgi/gists{/gist_id}", "starred_url": "https://api.github.com/users/JohnGiorgi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JohnGiorgi/subscriptions", "organizations_url": "https://api.github.com/users/JohnGiorgi/orgs", "repos_url": "https://api.github.com/users/JohnGiorgi/repos", "events_url": "https://api.github.com/users/JohnGiorgi/events{/privacy}", "received_events_url": "https://api.github.com/users/JohnGiorgi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1959214834, "node_id": "MDU6TGFiZWwxOTU5MjE0ODM0", "url": "https://api.github.com/repos/pytorch/text/labels/legacy", "name": "legacy", "color": "bfd4f2", "default": false, "description": ""}, {"id": 2169782914, "node_id": "MDU6TGFiZWwyMTY5NzgyOTE0", "url": "https://api.github.com/repos/pytorch/text/labels/new%20datasets%20and%20building%20blocks", "name": "new datasets and building blocks", "color": "0a7f5c", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2019-10-02T12:27:39Z", "updated_at": "2020-07-22T19:25:10Z", "closed_at": "2019-10-06T23:45:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\n\r\n__TL;DR__: Has anyone been able to successfully integrate the [transformers library](https://github.com/huggingface/transformers) tokenizer with torchtext?\r\n\r\nI wanted to use the torchtext library to process/load data for use with the [transformers library](https://github.com/huggingface/transformers). I was able to set their tokenizer in a Field object, and build a vocabulary without issue\r\n\r\n```python\r\nfrom torchtext import data\r\nfrom torchtext import datasets\r\nfrom transformers import AutoTokenizer\r\n\r\npath = 'path/to/med_nli/'\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n\r\nTEXT = data.Field(use_vocab=True, tokenize=tokenizer.tokenize)\r\nLABEL = data.LabelField()\r\n\r\nfields = {'sentence1': ('premise', TEXT),\r\n          'sentence2': ('hypothesis', TEXT),\r\n          'gold_label': ('label', LABEL)}\r\n\r\ntrain, valid, test = data.TabularDataset.splits(\r\n    path=path, \r\n    train='mli_train_v1.jsonl',\r\n    validation='mli_dev_v1.jsonl',\r\n    test='mli_test_v1.jsonl',\r\n    format='json', \r\n    fields=fields\r\n)\r\n\r\ntrain_iter, valid_iter, test_iter = data.BucketIterator.splits(\r\n    (train, valid, test), batch_sizes=(16, 256, 256)\r\n)\r\n\r\nTEXT.build_vocab(train)\r\nLABEL.build_vocab(train)\r\n```\r\n\r\n> Note, I am using the MedNLI dataset but it appears to be formatted according to the SNLI dataset.\r\n\r\nBut I am stuck on how to numericalize _according to their tokenizers vocab_.  So I tried to numericalize in the field with their tokenizers `encode` method and  set `vocab=False`.\r\n\r\n```python\r\nfrom torchtext import data\r\nfrom torchtext import datasets\r\nfrom transformers import AutoTokenizer\r\n\r\npath = 'path/to/med_nli/'\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\r\n\r\nTEXT = data.Field(use_vocab=False, tokenize=tokenizer.encode)\r\nLABEL = data.LabelField()\r\n\r\nfields = {'sentence1': ('premise', TEXT),\r\n          'sentence2': ('hypothesis', TEXT),\r\n          'gold_label': ('label', LABEL)}\r\n\r\ntrain, valid, test = data.TabularDataset.splits(\r\n    path=path, \r\n    train='mli_train_v1.jsonl',\r\n    validation='mli_dev_v1.jsonl',\r\n    test='mli_test_v1.jsonl',\r\n    format='json', \r\n    fields=fields\r\n)\r\n\r\ntrain_iter, valid_iter, test_iter = data.BucketIterator.splits(\r\n    (train, valid, test), batch_sizes=(16, 256, 256)\r\n)\r\n\r\n# TEXT.build_vocab(train)\r\nLABEL.build_vocab(train)\r\n```\r\n\r\nBut then I get strange issues when trying to access the batch,\r\n\r\n```\r\nbatch = next(iter(train_iter))\r\nprint(\"Numericalize premises:\\n\", batch.premise)\r\nprint(\"Numericalize hypotheses:\\n\", batch.hypothesis)\r\nprint(\"Entailment labels:\\n\", batch.label)\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-55-9919119fad82> in <module>\r\n----> 1 batch = next(iter(train_iter))\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/iterator.py in __iter__(self)\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/batch.py in __init__(self, data, dataset, device)\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/field.py in process(self, batch, device)\r\n\r\n~/miniconda3/envs/ml4h/lib/python3.7/site-packages/torchtext/data/field.py in numericalize(self, arr, device)\r\n\r\nValueError: too many dimensions 'str'\r\n```\r\n\r\nAny suggestions on how to go about this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/608", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/608/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/608/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/608/events", "html_url": "https://github.com/pytorch/text/issues/608", "id": 499714283, "node_id": "MDU6SXNzdWU0OTk3MTQyODM=", "number": 608, "title": "Different design on Text Classification Dataset", "user": {"login": "akurniawan", "id": 4723643, "node_id": "MDQ6VXNlcjQ3MjM2NDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/4723643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akurniawan", "html_url": "https://github.com/akurniawan", "followers_url": "https://api.github.com/users/akurniawan/followers", "following_url": "https://api.github.com/users/akurniawan/following{/other_user}", "gists_url": "https://api.github.com/users/akurniawan/gists{/gist_id}", "starred_url": "https://api.github.com/users/akurniawan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akurniawan/subscriptions", "organizations_url": "https://api.github.com/users/akurniawan/orgs", "repos_url": "https://api.github.com/users/akurniawan/repos", "events_url": "https://api.github.com/users/akurniawan/events{/privacy}", "received_events_url": "https://api.github.com/users/akurniawan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-28T02:13:42Z", "updated_at": "2019-10-02T09:54:09Z", "closed_at": "2019-10-02T09:54:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\nHi, I notice there is a noticeably different approach for text classification dataset. In test folder, for text classification, we are no longer using data.Fields nor data.Iterator anymore and instead build our own iterator for that dataset. Is there any change of direction in terms of the design from this moment forward? Or it's just for this particular dataset?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/606", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/606/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/606/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/606/events", "html_url": "https://github.com/pytorch/text/issues/606", "id": 498193802, "node_id": "MDU6SXNzdWU0OTgxOTM4MDI=", "number": 606, "title": "Using cache when loading vocab.Vectors on Windows gives OSError Invalid Argument error", "user": {"login": "tomgun132", "id": 13296565, "node_id": "MDQ6VXNlcjEzMjk2NTY1", "avatar_url": "https://avatars1.githubusercontent.com/u/13296565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomgun132", "html_url": "https://github.com/tomgun132", "followers_url": "https://api.github.com/users/tomgun132/followers", "following_url": "https://api.github.com/users/tomgun132/following{/other_user}", "gists_url": "https://api.github.com/users/tomgun132/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomgun132/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomgun132/subscriptions", "organizations_url": "https://api.github.com/users/tomgun132/orgs", "repos_url": "https://api.github.com/users/tomgun132/repos", "events_url": "https://api.github.com/users/tomgun132/events{/privacy}", "received_events_url": "https://api.github.com/users/tomgun132/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-09-25T10:25:38Z", "updated_at": "2020-06-16T18:39:42Z", "closed_at": "2020-06-16T18:39:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was trying to load fasttext common-crawl vector (.vec.gz) file into torchtext by using torchtext.vocab.Vectors\r\n`emb = vocab.Vectors(name=fasttext_name, url=URL, cache=fasttext_cache)`\r\nFor the first time without the .pt file, loading the vectors works fine and it creates<filename> .pt file on the cache folder. However, whenever I load the vector again when the .pt file exists in the folder, it gives this kind of error:\r\n```\r\n File \"load_emb.py\", line 94, in get_fasttext_embs\r\n    name=fasttext_name, url=URL, cache=fasttext_cache\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\torchtext\\vocab.py\", line 323, in __init__\r\n    self.cache(name, cache, url=url, max_vectors=max_vectors)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\torchtext\\vocab.py\", line 432, in cache\r\n    self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 386, in load\r\n    return _load(f, map_location, pickle_module, **pickle_load_args)\r\n  File \"C:\\Users\\user\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 580, in _load\r\n    deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)\r\nOSError: [Errno 22] Invalid argument\r\n```\r\n\r\nI don't have this error when I run the same code inside a linux machine. I wonder what can I do to fix this on Windows machine since I need to delete the .pt file and starts loading fasttext vector from the start again every time I need to run the code.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/593", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/593/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/593/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/593/events", "html_url": "https://github.com/pytorch/text/issues/593", "id": 483758247, "node_id": "MDU6SXNzdWU0ODM3NTgyNDc=", "number": 593, "title": "Dataset.split() generate wrong ratio", "user": {"login": "gitfourteen", "id": 22335780, "node_id": "MDQ6VXNlcjIyMzM1Nzgw", "avatar_url": "https://avatars3.githubusercontent.com/u/22335780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gitfourteen", "html_url": "https://github.com/gitfourteen", "followers_url": "https://api.github.com/users/gitfourteen/followers", "following_url": "https://api.github.com/users/gitfourteen/following{/other_user}", "gists_url": "https://api.github.com/users/gitfourteen/gists{/gist_id}", "starred_url": "https://api.github.com/users/gitfourteen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gitfourteen/subscriptions", "organizations_url": "https://api.github.com/users/gitfourteen/orgs", "repos_url": "https://api.github.com/users/gitfourteen/repos", "events_url": "https://api.github.com/users/gitfourteen/events{/privacy}", "received_events_url": "https://api.github.com/users/gitfourteen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-22T04:07:51Z", "updated_at": "2019-08-22T14:07:09Z", "closed_at": "2019-08-22T14:07:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## \u2753 Questions and Help\r\n\r\n**Description**\r\n<!-- Please send questions or ask for help here. -->\r\n`torchtext.__version__` '0.3.1'\r\n\r\n- code snippet\r\n\r\n```\r\ndataset = TabularDataset('data-jigsaw-toxic/train.csv', 'csv', datafields, skip_header=True)\r\ndata_train, data_test, data_valid = dataset.split(split_ratio=[0.1, 0.1, 0.8], random_state=None)\r\n````\r\n\r\n`len(data_train), len(data_test), len(data_valid)`\r\n(15957, 127657, 15957)\r\n\r\nIf I reset the **split_ratio** as` [0.1, 0.8, 0.1]`, will get (15957, 15957, 127657) instead. \r\n\r\nAccording to **[check_split_ratio](https://github.com/pytorch/text/blob/master/torchtext/data/dataset.py#L295)** and **[rationed_split](https://github.com/pytorch/text/blob/master/torchtext/data/dataset.py#L337)** which define the split_ratio,  this order is wrong.\r\n\r\nIs this a BUG?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/592", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/592/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/592/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/592/events", "html_url": "https://github.com/pytorch/text/issues/592", "id": 483418734, "node_id": "MDU6SXNzdWU0ODM0MTg3MzQ=", "number": 592, "title": "arr = [[self.vocab.stoi[x] for x in ex] for ex in arr] KeyError: None", "user": {"login": "wpfnlp", "id": 47095734, "node_id": "MDQ6VXNlcjQ3MDk1NzM0", "avatar_url": "https://avatars3.githubusercontent.com/u/47095734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wpfnlp", "html_url": "https://github.com/wpfnlp", "followers_url": "https://api.github.com/users/wpfnlp/followers", "following_url": "https://api.github.com/users/wpfnlp/following{/other_user}", "gists_url": "https://api.github.com/users/wpfnlp/gists{/gist_id}", "starred_url": "https://api.github.com/users/wpfnlp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wpfnlp/subscriptions", "organizations_url": "https://api.github.com/users/wpfnlp/orgs", "repos_url": "https://api.github.com/users/wpfnlp/repos", "events_url": "https://api.github.com/users/wpfnlp/events{/privacy}", "received_events_url": "https://api.github.com/users/wpfnlp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 498907451, "node_id": "MDU6TGFiZWw0OTg5MDc0NTE=", "url": "https://api.github.com/repos/pytorch/text/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-08-21T13:23:44Z", "updated_at": "2020-06-02T21:42:45Z", "closed_at": "2019-09-03T14:27:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "torchtext=0.4.0 BUG:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/weipengfei/workspaces/FastNLPProjects/research01/Intent+SlotFilling01.py\", line 112, in <module>\r\n    for i, batch in enumerate(train_iter):\r\n  File \"/miniconda3/lib/python3.7/site-packages/torchtext/data/iterator.py\", line 156, in __iter__\r\n    yield Batch(minibatch, self.dataset, self.device)\r\n  File \"/miniconda3/lib/python3.7/site-packages/torchtext/data/batch.py\", line 34, in __init__\r\n    setattr(self, name, field.process(batch, device=device))\r\n  File \"/miniconda3/lib/python3.7/site-packages/torchtext/data/field.py\", line 237, in process\r\n    tensor = self.numericalize(padded, device=device)\r\n  File \"/miniconda3/lib/python3.7/site-packages/torchtext/data/field.py\", line 336, in numericalize\r\n    arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\n  File \"/miniconda3/lib/python3.7/site-packages/torchtext/data/field.py\", line 336, in <listcomp>\r\n    arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\n  File \"/miniconda3/lib/python3.7/site-packages/torchtext/data/field.py\", line 336, in <listcomp>\r\n    arr = [[self.vocab.stoi[x] for x in ex] for ex in arr]\r\nKeyError: None\r\n\r\n**The same code torchtext=0.3.1 No problem, please tell me what caused it, thank you.**", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/589", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/589/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/589/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/589/events", "html_url": "https://github.com/pytorch/text/issues/589", "id": 482484635, "node_id": "MDU6SXNzdWU0ODI0ODQ2MzU=", "number": 589, "title": "Add SentencePiece binding to torchtext", "user": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2019-08-19T19:24:32Z", "updated_at": "2019-09-27T21:04:49Z", "closed_at": "2019-09-27T21:04:49Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "SentencePiece is an unsupervised text tokenizer that is mainly applicable for text-based systems. The size of vocab is predetermined prior to train the tokenizer. The subword unit implemented in SentencePiece allows for tokenizing raw sentences without language-specific pre/postprocessing.\r\n\r\nIn addition to text-based system, the SentencePiece may be also applicable for torchaudio script.\r\n\r\n@cpuhrsch @vincentqb @soumith ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/587", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/587/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/587/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/587/events", "html_url": "https://github.com/pytorch/text/issues/587", "id": 480921242, "node_id": "MDU6SXNzdWU0ODA5MjEyNDI=", "number": 587, "title": "Penn-treebank dataset does not download automatically ", "user": {"login": "yiulau", "id": 17055021, "node_id": "MDQ6VXNlcjE3MDU1MDIx", "avatar_url": "https://avatars0.githubusercontent.com/u/17055021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yiulau", "html_url": "https://github.com/yiulau", "followers_url": "https://api.github.com/users/yiulau/followers", "following_url": "https://api.github.com/users/yiulau/following{/other_user}", "gists_url": "https://api.github.com/users/yiulau/gists{/gist_id}", "starred_url": "https://api.github.com/users/yiulau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yiulau/subscriptions", "organizations_url": "https://api.github.com/users/yiulau/orgs", "repos_url": "https://api.github.com/users/yiulau/repos", "events_url": "https://api.github.com/users/yiulau/events{/privacy}", "received_events_url": "https://api.github.com/users/yiulau/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2019-08-14T22:45:42Z", "updated_at": "2019-09-03T14:28:11Z", "closed_at": "2019-09-03T14:28:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Probably similar to this issue : [Translation datasets not automatically downloading](https://github.com/pytorch/text/issues/312)\r\n```\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-9-35c4f74f3d61> in <module>\r\n     15 from torchtext import datasets\r\n     16 \r\n---> 17 train_data, test_data = datasets.PennTreebank.splits(TEXT)\r\n\r\n~/.conda/envs/tf1torchpy36/lib/python3.6/site-packages/torchtext/datasets/language_modeling.py in splits(cls, text_field, root, train, validation, test, **kwargs)\r\n    186         return super(PennTreebank, cls).splits(\r\n    187             root=root, train=train, validation=validation, test=test,\r\n--> 188             text_field=text_field, **kwargs)\r\n    189 \r\n    190     @classmethod\r\n\r\n~/.conda/envs/tf1torchpy36/lib/python3.6/site-packages/torchtext/data/dataset.py in splits(cls, path, root, train, validation, test, **kwargs)\r\n     76             path = cls.download(root)\r\n     77         train_data = None if train is None else cls(\r\n---> 78             os.path.join(path, train), **kwargs)\r\n     79         val_data = None if validation is None else cls(\r\n     80             os.path.join(path, validation), **kwargs)\r\n\r\n~/.conda/envs/tf1torchpy36/lib/python3.6/site-packages/torchtext/datasets/language_modeling.py in __init__(self, path, text_field, newline_eos, encoding, **kwargs)\r\n     20         fields = [('text', text_field)]\r\n     21         text = []\r\n---> 22         with io.open(path, encoding=encoding) as f:\r\n     23             for line in f:\r\n     24                 text += text_field.preprocess(line)\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '.data/penn-treebank/ptb.train.txt'\r\n\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/581", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/581/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/581/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/581/events", "html_url": "https://github.com/pytorch/text/issues/581", "id": 476407845, "node_id": "MDU6SXNzdWU0NzY0MDc4NDU=", "number": 581, "title": "why IMDB could not work? ", "user": {"login": "meshiguge", "id": 3081494, "node_id": "MDQ6VXNlcjMwODE0OTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/3081494?v=4", "gravatar_id": "", "url": "https://api.github.com/users/meshiguge", "html_url": "https://github.com/meshiguge", "followers_url": "https://api.github.com/users/meshiguge/followers", "following_url": "https://api.github.com/users/meshiguge/following{/other_user}", "gists_url": "https://api.github.com/users/meshiguge/gists{/gist_id}", "starred_url": "https://api.github.com/users/meshiguge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/meshiguge/subscriptions", "organizations_url": "https://api.github.com/users/meshiguge/orgs", "repos_url": "https://api.github.com/users/meshiguge/repos", "events_url": "https://api.github.com/users/meshiguge/events{/privacy}", "received_events_url": "https://api.github.com/users/meshiguge/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-03T02:41:42Z", "updated_at": "2019-08-04T18:39:19Z", "closed_at": "2019-08-04T18:39:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\nfrom torchtext import data\r\nfrom torchtext import datasets\r\nTEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\r\nLABEL = data.Field(sequential=False)\r\n\r\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\r\nprint ( len ( train ))\r\n```\r\nThis returns a length of **0** .   \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/572", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/572/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/572/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/572/events", "html_url": "https://github.com/pytorch/text/issues/572", "id": 474191810, "node_id": "MDU6SXNzdWU0NzQxOTE4MTA=", "number": 572, "title": "How to set the max length for batches?", "user": {"login": "XinDongol", "id": 15049780, "node_id": "MDQ6VXNlcjE1MDQ5Nzgw", "avatar_url": "https://avatars1.githubusercontent.com/u/15049780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XinDongol", "html_url": "https://github.com/XinDongol", "followers_url": "https://api.github.com/users/XinDongol/followers", "following_url": "https://api.github.com/users/XinDongol/following{/other_user}", "gists_url": "https://api.github.com/users/XinDongol/gists{/gist_id}", "starred_url": "https://api.github.com/users/XinDongol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XinDongol/subscriptions", "organizations_url": "https://api.github.com/users/XinDongol/orgs", "repos_url": "https://api.github.com/users/XinDongol/repos", "events_url": "https://api.github.com/users/XinDongol/events{/privacy}", "received_events_url": "https://api.github.com/users/XinDongol/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-07-29T18:25:25Z", "updated_at": "2019-09-16T15:04:40Z", "closed_at": "2019-08-09T18:19:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there a way to use something like `max_len` argument: all text entries in a dataset longer than a certain length can be thrown out.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/559", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/559/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/559/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/559/events", "html_url": "https://github.com/pytorch/text/issues/559", "id": 467885425, "node_id": "MDU6SXNzdWU0Njc4ODU0MjU=", "number": 559, "title": "Implementing C++ Side", "user": {"login": "ShahriarSS", "id": 24262836, "node_id": "MDQ6VXNlcjI0MjYyODM2", "avatar_url": "https://avatars2.githubusercontent.com/u/24262836?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ShahriarSS", "html_url": "https://github.com/ShahriarSS", "followers_url": "https://api.github.com/users/ShahriarSS/followers", "following_url": "https://api.github.com/users/ShahriarSS/following{/other_user}", "gists_url": "https://api.github.com/users/ShahriarSS/gists{/gist_id}", "starred_url": "https://api.github.com/users/ShahriarSS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ShahriarSS/subscriptions", "organizations_url": "https://api.github.com/users/ShahriarSS/orgs", "repos_url": "https://api.github.com/users/ShahriarSS/repos", "events_url": "https://api.github.com/users/ShahriarSS/events{/privacy}", "received_events_url": "https://api.github.com/users/ShahriarSS/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-14T21:14:31Z", "updated_at": "2019-08-09T16:59:42Z", "closed_at": "2019-08-09T16:59:42Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi @soumith I want to work on torchtext and implement its C++ API. I'm less familiar with torchtext so where can I can start? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/549", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/549/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/549/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/549/events", "html_url": "https://github.com/pytorch/text/issues/549", "id": 451302229, "node_id": "MDU6SXNzdWU0NTEzMDIyMjk=", "number": 549, "title": "field larger than field limit (131072)", "user": {"login": "lipingbj", "id": 4567321, "node_id": "MDQ6VXNlcjQ1NjczMjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4567321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lipingbj", "html_url": "https://github.com/lipingbj", "followers_url": "https://api.github.com/users/lipingbj/followers", "following_url": "https://api.github.com/users/lipingbj/following{/other_user}", "gists_url": "https://api.github.com/users/lipingbj/gists{/gist_id}", "starred_url": "https://api.github.com/users/lipingbj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lipingbj/subscriptions", "organizations_url": "https://api.github.com/users/lipingbj/orgs", "repos_url": "https://api.github.com/users/lipingbj/repos", "events_url": "https://api.github.com/users/lipingbj/events{/privacy}", "received_events_url": "https://api.github.com/users/lipingbj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 498907456, "node_id": "MDU6TGFiZWw0OTg5MDc0NTY=", "url": "https://api.github.com/repos/pytorch/text/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-06-03T05:46:57Z", "updated_at": "2019-09-03T14:28:34Z", "closed_at": "2019-09-03T14:28:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I use the data.TabularDataset ,there occurs an problem(field larger than field limit (131072)).\r\n\r\ntrain_data = data.TabularDataset(path='train.txt.labeled2', format='tsv',\r\nfields=[('sentence', TEXT),('label', LABEL)], csv_reader_params={\"lineterminator\": \"\\n\"})\r\n\r\nThe lines of the train.txt.labeled2 is larger than 1000,0000. I don't know how to set the attribute of the csv.field_size_limit. And i found there is no place to set this parameter.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/548", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/548/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/548/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/548/events", "html_url": "https://github.com/pytorch/text/issues/548", "id": 451198980, "node_id": "MDU6SXNzdWU0NTExOTg5ODA=", "number": 548, "title": "ValueError from FastText", "user": {"login": "justice-suri", "id": 44358135, "node_id": "MDQ6VXNlcjQ0MzU4MTM1", "avatar_url": "https://avatars2.githubusercontent.com/u/44358135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justice-suri", "html_url": "https://github.com/justice-suri", "followers_url": "https://api.github.com/users/justice-suri/followers", "following_url": "https://api.github.com/users/justice-suri/following{/other_user}", "gists_url": "https://api.github.com/users/justice-suri/gists{/gist_id}", "starred_url": "https://api.github.com/users/justice-suri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justice-suri/subscriptions", "organizations_url": "https://api.github.com/users/justice-suri/orgs", "repos_url": "https://api.github.com/users/justice-suri/repos", "events_url": "https://api.github.com/users/justice-suri/events{/privacy}", "received_events_url": "https://api.github.com/users/justice-suri/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 498907456, "node_id": "MDU6TGFiZWw0OTg5MDc0NTY=", "url": "https://api.github.com/repos/pytorch/text/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2019-06-02T14:39:18Z", "updated_at": "2019-09-03T14:28:53Z", "closed_at": "2019-09-03T14:28:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello.\r\nI recently installed Torchtext 0.3.1 and tqdm 4.32.1 and tried to use FastText.\r\nHowever, the following error occurred:\r\n\r\n> torchtext.vocab.FastText()\r\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ros-slam/.local/lib/python3.5/site-packages/torchtext/vocab.py\", line 411, in __init__\r\n    super(FastText, self).__init__(name, url=url, **kwargs)\r\n  File \"/home/ros-slam/.local/lib/python3.5/site-packages/torchtext/vocab.py\", line 280, in __init__\r\n    self.cache(name, cache, url=url, max_vectors=max_vectors)\r\n  File \"/home/ros-slam/.local/lib/python3.5/site-packages/torchtext/vocab.py\", line 370, in cache\r\n    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\r\n  File \"/home/ros-slam/.local/lib/python3.5/site-packages/torchtext/vocab.py\", line 370, in <listcomp>\r\n    vectors[vectors_loaded] = torch.tensor([float(x) for x in entries])\r\nValueError: could not convert string to float: b'version=\"1.0\"'\r\n\r\n\r\nTo find the cause of ValueError, I checked the vocab.py\r\n\r\n```\r\nclass FastText(Vectors):\r\n\r\n    url_base = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.{}.vec'\r\n\r\n    def __init__(self, language=\"en\", **kwargs):\r\n        url = self.url_base.format(language)\r\n        name = os.path.basename(url)\r\n        super(FastText, self).__init__(name, url=url, **kwargs)\r\n```\r\n\r\nIt is assumed that the cause of this error occurred while accessing that website 'url_base'.\r\nSo, how can we solve this error?\r\nI've looked here for a case of a similar problem to mine, but unfortunately it doesn't appear to be an unusual case.\r\nI've also tried the code in  0.2.3 versions, but the same thing is happening.\r\n\r\nThank you in advance for your reply.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/535", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/535/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/535/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/535/events", "html_url": "https://github.com/pytorch/text/issues/535", "id": 442837245, "node_id": "MDU6SXNzdWU0NDI4MzcyNDU=", "number": 535, "title": "Numericalize torch.FloatTensor and torch.DoubleTensor", "user": {"login": "shafiul", "id": 289639, "node_id": "MDQ6VXNlcjI4OTYzOQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/289639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shafiul", "html_url": "https://github.com/shafiul", "followers_url": "https://api.github.com/users/shafiul/followers", "following_url": "https://api.github.com/users/shafiul/following{/other_user}", "gists_url": "https://api.github.com/users/shafiul/gists{/gist_id}", "starred_url": "https://api.github.com/users/shafiul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shafiul/subscriptions", "organizations_url": "https://api.github.com/users/shafiul/orgs", "repos_url": "https://api.github.com/users/shafiul/repos", "events_url": "https://api.github.com/users/shafiul/events{/privacy}", "received_events_url": "https://api.github.com/users/shafiul/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-05-10T18:12:11Z", "updated_at": "2019-06-01T15:52:53Z", "closed_at": "2019-05-30T09:38:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI was unable to use floating-point labels using the following field declaration:\r\n\r\n    LABEL = data.Field(sequential=False, use_vocab=False, is_target=True, dtype=torch.DoubleTensor)\r\n\r\n\r\nGot this error: \"Specified Field dtype <class 'torch.DoubleTensor'> can not be used with use_vocab=False because we do not know how to numericalize it\"\r\n\r\nWould it be straight-forward to numericalize such fields?\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/520", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/520/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/520/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/520/events", "html_url": "https://github.com/pytorch/text/issues/520", "id": 426735809, "node_id": "MDU6SXNzdWU0MjY3MzU4MDk=", "number": 520, "title": "Unable to download wiki.simple.vec", "user": {"login": "rjknight", "id": 8375909, "node_id": "MDQ6VXNlcjgzNzU5MDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/8375909?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rjknight", "html_url": "https://github.com/rjknight", "followers_url": "https://api.github.com/users/rjknight/followers", "following_url": "https://api.github.com/users/rjknight/following{/other_user}", "gists_url": "https://api.github.com/users/rjknight/gists{/gist_id}", "starred_url": "https://api.github.com/users/rjknight/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rjknight/subscriptions", "organizations_url": "https://api.github.com/users/rjknight/orgs", "repos_url": "https://api.github.com/users/rjknight/repos", "events_url": "https://api.github.com/users/rjknight/events{/privacy}", "received_events_url": "https://api.github.com/users/rjknight/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-03-28T22:31:47Z", "updated_at": "2019-05-07T14:40:20Z", "closed_at": "2019-05-07T14:40:20Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I was running the unit tests for pytorch/text and hit a failure as follows, up until a few days ago it had been working fine\r\n\r\n```\r\nCopying tests to temporary directory /tmp/tmp.i1t1PY3pSQ/torchtext.. \r\n======================================================================== test session starts ========================================================================\r\nplatform linux2 -- Python 2.7.16, pytest-4.1.1, py-1.8.0, pluggy-0.9.0 -- /opt/anaconda/envs/my-py2-env/bin/python\r\ncachedir: .pytest_cache\r\nrootdir: /tmp/tmp.i1t1PY3pSQ/torchtext, inifile: pytest.ini\r\nplugins: hypothesis-3.59.1\r\ncollected 60 items                                                                                                                                                  \r\n\r\ntest/test_vocab.py::TestVocab::test_errors PASSED                                                                                                             [  1%]\r\ntest/test_vocab.py::TestVocab::test_serialization PASSED                                                                                                      [  3%]\r\ntest/test_vocab.py::TestVocab::test_vocab_basic PASSED                                                                                                        [  5%]\r\ntest/test_vocab.py::TestVocab::test_vocab_download_charngram_vectors SKIPPED                                                                                  [  6%]\r\ntest/test_vocab.py::TestVocab::test_vocab_download_custom_vectors FAILED                                                                                      [  8%]\r\n\r\n============================================================================= FAILURES ==============================================================================\r\n___________________________________________________________ TestVocab.test_vocab_download_custom_vectors ____________________________________________________________\r\n\r\nself = <torchtext.test.test_vocab.TestVocab testMethod=test_vocab_download_custom_vectors>\r\n\r\n    def test_vocab_download_custom_vectors(self):\r\n        c = Counter({'hello': 4, 'world': 3, '\u144c\u144eI\u1455O\u15ea\u156e_T\u156e\u166dT': 5, 'freq_too_low': 2})\r\n        # Build a vocab and get vectors twice to test caching.\r\n        for i in range(2):\r\n            v = vocab.Vocab(c, min_freq=3, specials=['<unk>', '<pad>', '<bos>'],\r\n                            vectors=Vectors('wiki.simple.vec',\r\n>                                           url=FastText.url_base.format('simple')))\r\n\r\n/opt/anaconda/envs/my-py2-env/lib/python2.7/site-packages/torchtext/test/test_vocab.py:139: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nvocab.py:280: in __init__\r\n    self.cache(name, cache, url=url, max_vectors=max_vectors)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <torchtext.vocab.Vectors object at 0x7fff514cdd50>, name = 'wiki.simple.vec', cache = '.vector_cache'\r\nurl = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec', max_vectors = 0\r\n```\r\ndiging in a little I see the URL is not accessable anymore\r\n\r\n```\r\nwget https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec \r\n--2019-03-28 19:08:03--  https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec\r\nResolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 54.231.236.25\r\nConnecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|54.231.236.25|:443... connected.\r\nHTTP request sent, awaiting response... 403 Forbidden\r\n2019-03-28 19:08:03 ERROR 403: Forbidden.\r\n```\r\nmaking this change \r\n```\r\nSubject: [PATCH] Update fasttext vector url\r\n\r\n-Update urls for downloading wiki-simple.vec and wiki-en.vec\r\n---\r\n test/sst.py        | 3 ++-\r\n torchtext/vocab.py | 2 +-\r\n 2 files changed, 3 insertions(+), 2 deletions(-)\r\n\r\ndiff --git a/test/sst.py b/test/sst.py\r\nindex 943ceb6..17c1a0d 100644\r\n--- a/test/sst.py\r\n+++ b/test/sst.py\r\n@@ -19,7 +19,8 @@ print('len(train)', len(train))\r\n print('vars(train[0])', vars(train[0]))\r\n \r\n # build the vocabulary\r\n-url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\r\n+url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.simple.vec'\r\n+\r\n TEXT.build_vocab(train, vectors=Vectors('wiki.simple.vec', url=url))\r\n LABEL.build_vocab(train)\r\n \r\ndiff --git a/torchtext/vocab.py b/torchtext/vocab.py\r\nindex c6088a2..937f185 100644\r\n--- a/torchtext/vocab.py\r\n+++ b/torchtext/vocab.py\r\n@@ -407,7 +407,7 @@ class GloVe(Vectors):\r\n \r\n class FastText(Vectors):\r\n \r\n-    url_base = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.{}.vec'\r\n+    url_base = 'https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.vec'\r\n \r\n     def __init__(self, language=\"en\", **kwargs):\r\n         url = self.url_base.format(language)\r\n-- \r\n1.8.3.1\r\n```\r\nseems to resolve the issue", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/517", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/517/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/517/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/517/events", "html_url": "https://github.com/pytorch/text/issues/517", "id": 425574250, "node_id": "MDU6SXNzdWU0MjU1NzQyNTA=", "number": 517, "title": "torchtext 0.4.0 not available", "user": {"login": "Mingyu-academic", "id": 35502736, "node_id": "MDQ6VXNlcjM1NTAyNzM2", "avatar_url": "https://avatars3.githubusercontent.com/u/35502736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mingyu-academic", "html_url": "https://github.com/Mingyu-academic", "followers_url": "https://api.github.com/users/Mingyu-academic/followers", "following_url": "https://api.github.com/users/Mingyu-academic/following{/other_user}", "gists_url": "https://api.github.com/users/Mingyu-academic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mingyu-academic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mingyu-academic/subscriptions", "organizations_url": "https://api.github.com/users/Mingyu-academic/orgs", "repos_url": "https://api.github.com/users/Mingyu-academic/repos", "events_url": "https://api.github.com/users/Mingyu-academic/events{/privacy}", "received_events_url": "https://api.github.com/users/Mingyu-academic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-26T18:13:12Z", "updated_at": "2019-05-07T14:42:47Z", "closed_at": "2019-05-07T14:42:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "$ pip install torchtext==0.4.0\r\nCollecting torchtext==0.4.0\r\n  Could not find a version that satisfies the requirement torchtext==0.4.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1)\r\nNo matching distribution found for torchtext==0.4.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/516", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/516/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/516/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/516/events", "html_url": "https://github.com/pytorch/text/issues/516", "id": 425552381, "node_id": "MDU6SXNzdWU0MjU1NTIzODE=", "number": 516, "title": "Cheeseshop 0.4.0?", "user": {"login": "drozzy", "id": 140710, "node_id": "MDQ6VXNlcjE0MDcxMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/140710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drozzy", "html_url": "https://github.com/drozzy", "followers_url": "https://api.github.com/users/drozzy/followers", "following_url": "https://api.github.com/users/drozzy/following{/other_user}", "gists_url": "https://api.github.com/users/drozzy/gists{/gist_id}", "starred_url": "https://api.github.com/users/drozzy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drozzy/subscriptions", "organizations_url": "https://api.github.com/users/drozzy/orgs", "repos_url": "https://api.github.com/users/drozzy/repos", "events_url": "https://api.github.com/users/drozzy/events{/privacy}", "received_events_url": "https://api.github.com/users/drozzy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-26T17:27:28Z", "updated_at": "2019-05-07T14:42:30Z", "closed_at": "2019-05-07T14:42:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Can you guys push 0.4.0 to pypi?\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/512", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/512/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/512/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/512/events", "html_url": "https://github.com/pytorch/text/issues/512", "id": 419250221, "node_id": "MDU6SXNzdWU0MTkyNTAyMjE=", "number": 512, "title": "TabularDataset splits when created", "user": {"login": "drozzy", "id": 140710, "node_id": "MDQ6VXNlcjE0MDcxMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/140710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drozzy", "html_url": "https://github.com/drozzy", "followers_url": "https://api.github.com/users/drozzy/followers", "following_url": "https://api.github.com/users/drozzy/following{/other_user}", "gists_url": "https://api.github.com/users/drozzy/gists{/gist_id}", "starred_url": "https://api.github.com/users/drozzy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drozzy/subscriptions", "organizations_url": "https://api.github.com/users/drozzy/orgs", "repos_url": "https://api.github.com/users/drozzy/repos", "events_url": "https://api.github.com/users/drozzy/events{/privacy}", "received_events_url": "https://api.github.com/users/drozzy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-11T00:01:44Z", "updated_at": "2019-03-11T00:07:38Z", "closed_at": "2019-03-11T00:07:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "If I try to load a file with 12000 entries with TabularDataset, e.g.:\r\n\r\n```\r\nd = TabularDataset(path, fields=fields, format='tsv')\r\n```\r\n\r\nfor some reason it only sees 10149 of them!\r\n```\r\n>>> print(len(d))\r\n10149\r\n```\r\n\r\nI suspect it has something to do with an extra `split` call somewhere in the code, because this is similar to a default split one would get if they called \"split\" on the dataset.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/505", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/505/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/505/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/505/events", "html_url": "https://github.com/pytorch/text/issues/505", "id": 411692397, "node_id": "MDU6SXNzdWU0MTE2OTIzOTc=", "number": 505, "title": "Is indexing broken?", "user": {"login": "sarahwie", "id": 8027676, "node_id": "MDQ6VXNlcjgwMjc2NzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8027676?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sarahwie", "html_url": "https://github.com/sarahwie", "followers_url": "https://api.github.com/users/sarahwie/followers", "following_url": "https://api.github.com/users/sarahwie/following{/other_user}", "gists_url": "https://api.github.com/users/sarahwie/gists{/gist_id}", "starred_url": "https://api.github.com/users/sarahwie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sarahwie/subscriptions", "organizations_url": "https://api.github.com/users/sarahwie/orgs", "repos_url": "https://api.github.com/users/sarahwie/repos", "events_url": "https://api.github.com/users/sarahwie/events{/privacy}", "received_events_url": "https://api.github.com/users/sarahwie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-02-19T00:17:34Z", "updated_at": "2019-03-03T23:51:46Z", "closed_at": "2019-02-19T00:20:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Specs: python 3.6.0, on Mac, no CUDA install. \r\ntorch version ==1.0.1.post2, installed via `pip3 install torch`.\r\n\r\nMinimal example:\r\n```\r\nimport torch\r\nstore = torch.randn((12,100,6,50))\r\nstore[:, :, 0, :]\r\n```\r\nproduces \r\n```\r\ndyld: lazy symbol binding failed: Symbol not found: _PySlice_Unpack\r\n  Referenced from: /Users/SWiegreffe/virtualenv/lib/python3.6/site-packages/torch/lib/libtorch_python.dylib\r\n  Expected in: flat namespace\r\n\r\ndyld: Symbol not found: _PySlice_Unpack\r\n  Referenced from: /Users/SWiegreffe/virtualenv/lib/python3.6/site-packages/torch/lib/libtorch_python.dylib\r\n  Expected in: flat namespace\r\n\r\nAbort trap: 6\r\n```\r\n\r\nI can't replicate in python 2.7.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/503", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/503/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/503/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/503/events", "html_url": "https://github.com/pytorch/text/issues/503", "id": 411164510, "node_id": "MDU6SXNzdWU0MTExNjQ1MTA=", "number": 503, "title": "Positive and negative example sampler for Iterator", "user": {"login": "AveryLiu", "id": 5574795, "node_id": "MDQ6VXNlcjU1NzQ3OTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/5574795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AveryLiu", "html_url": "https://github.com/AveryLiu", "followers_url": "https://api.github.com/users/AveryLiu/followers", "following_url": "https://api.github.com/users/AveryLiu/following{/other_user}", "gists_url": "https://api.github.com/users/AveryLiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/AveryLiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AveryLiu/subscriptions", "organizations_url": "https://api.github.com/users/AveryLiu/orgs", "repos_url": "https://api.github.com/users/AveryLiu/repos", "events_url": "https://api.github.com/users/AveryLiu/events{/privacy}", "received_events_url": "https://api.github.com/users/AveryLiu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-02-17T08:53:34Z", "updated_at": "2019-02-23T12:14:53Z", "closed_at": "2019-02-23T12:14:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have an unbalanced classification problem at hand and it requires me to manually balance the ratio of positive and negative samples in each batch to reach better performance. The problem is similar to this [related issue](https://github.com/pytorch/text/issues/283).   \r\n\r\nThe default Iterator accepts train, validation and test datasets. What I would like to do is loading data from train_pos, train_neg, validation, test and automatically adjust the ratio of positive and negative samples in the training batch. Is there a correct way to achieve this in torchtext?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/502", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/502/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/502/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/502/events", "html_url": "https://github.com/pytorch/text/issues/502", "id": 409576502, "node_id": "MDU6SXNzdWU0MDk1NzY1MDI=", "number": 502, "title": "Feature Request: Loading multiple datasets simultaneously", "user": {"login": "akurniawan", "id": 4723643, "node_id": "MDQ6VXNlcjQ3MjM2NDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/4723643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akurniawan", "html_url": "https://github.com/akurniawan", "followers_url": "https://api.github.com/users/akurniawan/followers", "following_url": "https://api.github.com/users/akurniawan/following{/other_user}", "gists_url": "https://api.github.com/users/akurniawan/gists{/gist_id}", "starred_url": "https://api.github.com/users/akurniawan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akurniawan/subscriptions", "organizations_url": "https://api.github.com/users/akurniawan/orgs", "repos_url": "https://api.github.com/users/akurniawan/repos", "events_url": "https://api.github.com/users/akurniawan/events{/privacy}", "received_events_url": "https://api.github.com/users/akurniawan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-02-13T01:11:33Z", "updated_at": "2019-02-23T04:58:21Z", "closed_at": "2019-02-23T04:58:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm thinking of using pytorch's [ConcatDataset](https://github.com/pytorch/pytorch/blob/f435fb8290aeb00fbfa0191d5c42c59c5a772623/torch/utils/data/dataset.py#L46) to achieve of doing so since all of torchtext dataset classes are also inherited from pytorch's Dataset. However, in torchtext dataset, it is required to provide `Field` whenever we instantiate any dataset class. Is there a way to just use one `Field` for all of the datasets? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/500", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/500/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/500/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/500/events", "html_url": "https://github.com/pytorch/text/issues/500", "id": 405554219, "node_id": "MDU6SXNzdWU0MDU1NTQyMTk=", "number": 500, "title": "add stopwords to Field", "user": {"login": "ReactiveCJ", "id": 3960787, "node_id": "MDQ6VXNlcjM5NjA3ODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3960787?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ReactiveCJ", "html_url": "https://github.com/ReactiveCJ", "followers_url": "https://api.github.com/users/ReactiveCJ/followers", "following_url": "https://api.github.com/users/ReactiveCJ/following{/other_user}", "gists_url": "https://api.github.com/users/ReactiveCJ/gists{/gist_id}", "starred_url": "https://api.github.com/users/ReactiveCJ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ReactiveCJ/subscriptions", "organizations_url": "https://api.github.com/users/ReactiveCJ/orgs", "repos_url": "https://api.github.com/users/ReactiveCJ/repos", "events_url": "https://api.github.com/users/ReactiveCJ/events{/privacy}", "received_events_url": "https://api.github.com/users/ReactiveCJ/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-02-01T05:08:19Z", "updated_at": "2019-05-30T09:38:13Z", "closed_at": "2019-05-30T09:38:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "At the __init__ function of class Field,\r\n    if stop_words is not None:\r\n            try:\r\n                self.stop_words = set(stop_words)\r\n            except TypeError:\r\n                raise ValueError(\"Stop words must be convertible to a set\")\r\n        else:\r\n            self.stop_words = stop_words\r\n        self.stop_words = stop_words\r\nwe try to convert stop_words to set, but at the end, we duplicately assign stop_words to self.stop_words, therefore the upper code doesn't work.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/497", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/497/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/497/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/497/events", "html_url": "https://github.com/pytorch/text/issues/497", "id": 404675920, "node_id": "MDU6SXNzdWU0MDQ2NzU5MjA=", "number": 497, "title": "initializing the vectors at vocab.py is random", "user": {"login": "ReactiveCJ", "id": 3960787, "node_id": "MDQ6VXNlcjM5NjA3ODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3960787?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ReactiveCJ", "html_url": "https://github.com/ReactiveCJ", "followers_url": "https://api.github.com/users/ReactiveCJ/followers", "following_url": "https://api.github.com/users/ReactiveCJ/following{/other_user}", "gists_url": "https://api.github.com/users/ReactiveCJ/gists{/gist_id}", "starred_url": "https://api.github.com/users/ReactiveCJ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ReactiveCJ/subscriptions", "organizations_url": "https://api.github.com/users/ReactiveCJ/orgs", "repos_url": "https://api.github.com/users/ReactiveCJ/repos", "events_url": "https://api.github.com/users/ReactiveCJ/events{/privacy}", "received_events_url": "https://api.github.com/users/ReactiveCJ/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-30T09:25:37Z", "updated_at": "2019-01-31T20:14:39Z", "closed_at": "2019-01-31T20:14:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "At the vocab.py,   \r\nfor this line self.vectors = torch.Tensor(len(self), tot_dim) \r\nwe want to initialize the element of  self.vectors by zero, but when the len(self) or tot_dim is small, this initialization results is random.\r\nCorrect \r\nself.vectors = torch.Tensor(len(self), tot_dim)  \r\nto\r\n self.vectors = torch.zeros(len(self), tot_dim)  \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/496", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/496/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/496/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/496/events", "html_url": "https://github.com/pytorch/text/issues/496", "id": 404499095, "node_id": "MDU6SXNzdWU0MDQ0OTkwOTU=", "number": 496, "title": "Project status?", "user": {"login": "aatkinson", "id": 22773542, "node_id": "MDQ6VXNlcjIyNzczNTQy", "avatar_url": "https://avatars0.githubusercontent.com/u/22773542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aatkinson", "html_url": "https://github.com/aatkinson", "followers_url": "https://api.github.com/users/aatkinson/followers", "following_url": "https://api.github.com/users/aatkinson/following{/other_user}", "gists_url": "https://api.github.com/users/aatkinson/gists{/gist_id}", "starred_url": "https://api.github.com/users/aatkinson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aatkinson/subscriptions", "organizations_url": "https://api.github.com/users/aatkinson/orgs", "repos_url": "https://api.github.com/users/aatkinson/repos", "events_url": "https://api.github.com/users/aatkinson/events{/privacy}", "received_events_url": "https://api.github.com/users/aatkinson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-01-29T21:40:59Z", "updated_at": "2019-01-30T17:42:10Z", "closed_at": "2019-01-30T15:14:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've noticed a long period of inactivity on this project and I'm curious if it's still being actively developed?\r\n\r\nAlso is this library going to be merged with PyText - or vice-versa - since it is a dependency? (See https://github.com/facebookresearch/pytext)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/494", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/494/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/494/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/494/events", "html_url": "https://github.com/pytorch/text/issues/494", "id": 403694826, "node_id": "MDU6SXNzdWU0MDM2OTQ4MjY=", "number": 494, "title": "does pytorch team stop developing torchtext?", "user": {"login": "speedcell4", "id": 3585459, "node_id": "MDQ6VXNlcjM1ODU0NTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3585459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/speedcell4", "html_url": "https://github.com/speedcell4", "followers_url": "https://api.github.com/users/speedcell4/followers", "following_url": "https://api.github.com/users/speedcell4/following{/other_user}", "gists_url": "https://api.github.com/users/speedcell4/gists{/gist_id}", "starred_url": "https://api.github.com/users/speedcell4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/speedcell4/subscriptions", "organizations_url": "https://api.github.com/users/speedcell4/orgs", "repos_url": "https://api.github.com/users/speedcell4/repos", "events_url": "https://api.github.com/users/speedcell4/events{/privacy}", "received_events_url": "https://api.github.com/users/speedcell4/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-28T08:18:51Z", "updated_at": "2019-01-28T11:42:10Z", "closed_at": "2019-01-28T11:42:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I notice the latest commit is in Nov 2018. \r\ndoes that mean pytorch team have already stopped developing torchtext?\r\nif yes, then official suggestion is migrating to pytext instead?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/493", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/493/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/493/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/493/events", "html_url": "https://github.com/pytorch/text/issues/493", "id": 403571785, "node_id": "MDU6SXNzdWU0MDM1NzE3ODU=", "number": 493, "title": "AttributeError: 'TextMultiField' object has no attribute 'is_target'", "user": {"login": "tangxiangru", "id": 22478336, "node_id": "MDQ6VXNlcjIyNDc4MzM2", "avatar_url": "https://avatars3.githubusercontent.com/u/22478336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tangxiangru", "html_url": "https://github.com/tangxiangru", "followers_url": "https://api.github.com/users/tangxiangru/followers", "following_url": "https://api.github.com/users/tangxiangru/following{/other_user}", "gists_url": "https://api.github.com/users/tangxiangru/gists{/gist_id}", "starred_url": "https://api.github.com/users/tangxiangru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tangxiangru/subscriptions", "organizations_url": "https://api.github.com/users/tangxiangru/orgs", "repos_url": "https://api.github.com/users/tangxiangru/repos", "events_url": "https://api.github.com/users/tangxiangru/events{/privacy}", "received_events_url": "https://api.github.com/users/tangxiangru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-27T16:59:02Z", "updated_at": "2019-03-12T11:57:17Z", "closed_at": "2019-01-31T21:30:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Traceback (most recent call last):\r\n  File \"translate.py\", line 42, in module\r\n    main(opt)\r\n  File \"translate.py\", line 27, in main\r\n    attn_debug=opt.attn_debug\r\n  File \"/home/txr/OpenNMT-py-ende/onmt/translate/translator.py\", line 208, in translate\r\n    for batch in data_iter:\r\n  File \"/home/txr/anaconda3/envs/python3/lib/python3.5/site-packages/torchtext/data/iterator.py\", line 157, in __iter__\r\n    yield Batch(minibatch, self.dataset, self.device)\r\n  File \"/home/txr/anaconda3/envs/python3/lib/python3.5/site-packages/torchtext/data/batch.py\", line 26, in __init__\r\n    self.input_fields = [k for k, v in dataset.fields.items() if\r\n  File \"/home/txr/anaconda3/envs/python3/lib/python3.5/site-packages/torchtext/data/batch.py\", line 27, in <listcomp>\r\n    v is not None and not v.is_target]\r\nAttributeError: 'TextMultiField' object has no attribute 'is_target'\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/489", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/489/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/489/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/489/events", "html_url": "https://github.com/pytorch/text/issues/489", "id": 398602113, "node_id": "MDU6SXNzdWUzOTg2MDIxMTM=", "number": 489, "title": "Load JSON data into NestedField", "user": {"login": "larry0123du", "id": 11513232, "node_id": "MDQ6VXNlcjExNTEzMjMy", "avatar_url": "https://avatars0.githubusercontent.com/u/11513232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/larry0123du", "html_url": "https://github.com/larry0123du", "followers_url": "https://api.github.com/users/larry0123du/followers", "following_url": "https://api.github.com/users/larry0123du/following{/other_user}", "gists_url": "https://api.github.com/users/larry0123du/gists{/gist_id}", "starred_url": "https://api.github.com/users/larry0123du/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/larry0123du/subscriptions", "organizations_url": "https://api.github.com/users/larry0123du/orgs", "repos_url": "https://api.github.com/users/larry0123du/repos", "events_url": "https://api.github.com/users/larry0123du/events{/privacy}", "received_events_url": "https://api.github.com/users/larry0123du/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-12T23:24:14Z", "updated_at": "2019-01-14T22:23:12Z", "closed_at": "2019-01-14T22:23:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "If I have a train.json and a test.json that contain JSON data of the below format:\r\n`{\"review\": [[\"it\", \"is\", \"good\"], [\"I\", \"like\", \"it\"]], \"label\": 1}`\r\nWhen I was trying to use the following code to load it:\r\n```python\r\nimport torch\r\nfrom torchtext import data, datasets\r\n\r\nREVIEW_NEST = data.Field()\r\nREVIEW = data.NestedField(REVIEW_NEST)\r\nLABEL = data.LabelField(dtype=torch.float)\r\n\r\nfields = {'review': ('r', REVIEW), 'label': ('l', LABEL)}\r\n\r\ntrain_data, test_data = data.TabularDataset.splits(\r\n                            path = 'popcorn_data',\r\n                            train = 'train.json',\r\n                            test = 'test.json',\r\n                            format = 'json',\r\n                            fields = fields\r\n)\r\n\r\nREVIEW.build_vocab(train_data.review, max_size=25000)\r\nprint(REVIEW.vocab.freqs.most_common(20))\r\n```\r\nI got an empty list. Is there anything I did wrong here? Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/487", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/487/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/487/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/487/events", "html_url": "https://github.com/pytorch/text/issues/487", "id": 397368115, "node_id": "MDU6SXNzdWUzOTczNjgxMTU=", "number": 487, "title": "reverse source and target for each batch", "user": {"login": "raheelqader", "id": 3978693, "node_id": "MDQ6VXNlcjM5Nzg2OTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3978693?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raheelqader", "html_url": "https://github.com/raheelqader", "followers_url": "https://api.github.com/users/raheelqader/followers", "following_url": "https://api.github.com/users/raheelqader/following{/other_user}", "gists_url": "https://api.github.com/users/raheelqader/gists{/gist_id}", "starred_url": "https://api.github.com/users/raheelqader/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raheelqader/subscriptions", "organizations_url": "https://api.github.com/users/raheelqader/orgs", "repos_url": "https://api.github.com/users/raheelqader/repos", "events_url": "https://api.github.com/users/raheelqader/events{/privacy}", "received_events_url": "https://api.github.com/users/raheelqader/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-09T13:20:48Z", "updated_at": "2019-05-30T14:28:17Z", "closed_at": "2019-05-30T14:28:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "I need to reverse the my source and target fields for each batch and then re-sort the batch again such that it can work with pack_padded_sequence(). Is there a way to do this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/484", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/484/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/484/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/484/events", "html_url": "https://github.com/pytorch/text/issues/484", "id": 389000001, "node_id": "MDU6SXNzdWUzODkwMDAwMDE=", "number": 484, "title": "get BucketIterator with arbitrary batch size", "user": {"login": "livc", "id": 11692045, "node_id": "MDQ6VXNlcjExNjkyMDQ1", "avatar_url": "https://avatars0.githubusercontent.com/u/11692045?v=4", "gravatar_id": "", "url": "https://api.github.com/users/livc", "html_url": "https://github.com/livc", "followers_url": "https://api.github.com/users/livc/followers", "following_url": "https://api.github.com/users/livc/following{/other_user}", "gists_url": "https://api.github.com/users/livc/gists{/gist_id}", "starred_url": "https://api.github.com/users/livc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/livc/subscriptions", "organizations_url": "https://api.github.com/users/livc/orgs", "repos_url": "https://api.github.com/users/livc/repos", "events_url": "https://api.github.com/users/livc/events{/privacy}", "received_events_url": "https://api.github.com/users/livc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-12-09T08:38:50Z", "updated_at": "2018-12-12T13:03:03Z", "closed_at": "2018-12-12T13:03:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nNow we can create BucketIterator with batch_size:\r\n\r\n```python\r\n>>> train_iter = data.BucketIterator(\r\n...     dataset=mt_train, batch_size=32,\r\n...     sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\r\n>>> next(iter(train_iter))\r\n```\r\n\r\nHow to get the Iterator without specifying batch_size?\r\nFor example, it may be:\r\n```python\r\n>>> train_iter = data.BucketIterator(\r\n...     dataset=mt_train,\r\n...     sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\r\n>>> next(iter(train_iter, batch=32))\r\n>>> next(iter(train_iter, batch=16))\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/483", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/483/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/483/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/483/events", "html_url": "https://github.com/pytorch/text/issues/483", "id": 388956500, "node_id": "MDU6SXNzdWUzODg5NTY1MDA=", "number": 483, "title": "How do I get label using torchtext?", "user": {"login": "oya163", "id": 7055478, "node_id": "MDQ6VXNlcjcwNTU0Nzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/7055478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oya163", "html_url": "https://github.com/oya163", "followers_url": "https://api.github.com/users/oya163/followers", "following_url": "https://api.github.com/users/oya163/following{/other_user}", "gists_url": "https://api.github.com/users/oya163/gists{/gist_id}", "starred_url": "https://api.github.com/users/oya163/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oya163/subscriptions", "organizations_url": "https://api.github.com/users/oya163/orgs", "repos_url": "https://api.github.com/users/oya163/repos", "events_url": "https://api.github.com/users/oya163/events{/privacy}", "received_events_url": "https://api.github.com/users/oya163/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-12-08T20:28:38Z", "updated_at": "2020-02-25T20:57:11Z", "closed_at": "2019-05-30T14:43:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "For example, if I have training set for binary classification with labels/classes [0, 1]. Is there anyway that I can get these classes directly using torchtext?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/479", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/479/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/479/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/479/events", "html_url": "https://github.com/pytorch/text/issues/479", "id": 381928478, "node_id": "MDU6SXNzdWUzODE5Mjg0Nzg=", "number": 479, "title": "Why Iterator doesn't have collate_fn to change anyone in batch?", "user": {"login": "illcat", "id": 40688514, "node_id": "MDQ6VXNlcjQwNjg4NTE0", "avatar_url": "https://avatars3.githubusercontent.com/u/40688514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/illcat", "html_url": "https://github.com/illcat", "followers_url": "https://api.github.com/users/illcat/followers", "following_url": "https://api.github.com/users/illcat/following{/other_user}", "gists_url": "https://api.github.com/users/illcat/gists{/gist_id}", "starred_url": "https://api.github.com/users/illcat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/illcat/subscriptions", "organizations_url": "https://api.github.com/users/illcat/orgs", "repos_url": "https://api.github.com/users/illcat/repos", "events_url": "https://api.github.com/users/illcat/events{/privacy}", "received_events_url": "https://api.github.com/users/illcat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-18T04:48:26Z", "updated_at": "2019-01-30T10:02:59Z", "closed_at": "2019-01-30T10:02:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "For example:\r\ntorch.utils.data.DataLoader(dataset=training_set,batch_size=batch_size,collate_fn=my_fn_align_sent)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/478", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/478/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/478/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/478/events", "html_url": "https://github.com/pytorch/text/issues/478", "id": 381479544, "node_id": "MDU6SXNzdWUzODE0Nzk1NDQ=", "number": 478, "title": "Can't deal with text label (invalid literal for int() with base 10: 'my_label1')", "user": {"login": "tnlin", "id": 5557403, "node_id": "MDQ6VXNlcjU1NTc0MDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5557403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tnlin", "html_url": "https://github.com/tnlin", "followers_url": "https://api.github.com/users/tnlin/followers", "following_url": "https://api.github.com/users/tnlin/following{/other_user}", "gists_url": "https://api.github.com/users/tnlin/gists{/gist_id}", "starred_url": "https://api.github.com/users/tnlin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tnlin/subscriptions", "organizations_url": "https://api.github.com/users/tnlin/orgs", "repos_url": "https://api.github.com/users/tnlin/repos", "events_url": "https://api.github.com/users/tnlin/events{/privacy}", "received_events_url": "https://api.github.com/users/tnlin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-11-16T07:32:58Z", "updated_at": "2019-05-30T14:49:26Z", "closed_at": "2019-05-30T14:49:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "I wanna do a multi-class classification task\r\nHere is my sample data (updated, thanks to @bentrevett )\r\n```\r\ntext, label\r\nMy string 1, my_label1\r\nMy string 2, my_label2\r\nMy string 3, my_label3\r\nMy string 4, my_label3\r\nMy string 5, my_label4\r\n...\r\n```\r\n\r\nSample code\r\n```\r\nTEXT = data.Field(sequential=True, tokenize=word_tokenize, lower=True, fix_length=None)\r\nLABEL = data.Field(sequential=False, use_vocab=False, unk_token=None)\r\n\r\ntrain, valid, test = data.TabularDataset.splits(\r\n    path=path , train='train.csv', validation='valid.csv', test='test.csv',\r\n    skip_header=True, format='csv',\r\n    fields=[('text', TEXT), ('label', LABEL)])\r\n\r\n# Building vocabulary\r\nTEXT.build_vocab(train, valid, test, max_size=10000, \r\n                 vectors='glove.6B.300d',  \r\n                 unk_init=torch.nn.init.xavier_uniform_)\r\nLABEL.build_vocab(train, valid, test)\r\nvocab = TEXT.vocab\r\n\r\niter_train, iter_valid = data.BucketIterator.splits((train, valid), batch_size=64, device=device, sort_key=lambda x: len(x.text), sort_within_batch=False, repeat=False)\r\niter_test = data.Iterator(test, batch_size=64, train=False, device=device, sort=False,  sort_within_batch=False, repeat=False)\r\n```\r\n\r\nWhen I call \r\n```\r\nbatch = next(iter(iter_valid))\r\nbatch.text\r\n```\r\n\r\nIt will pop out error  `invalid literal for int() with base 10: 'my_label1'`\r\nCan anyone give me any hint?  \r\nGoogle for whole day but still can't solve it...\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/473", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/473/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/473/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/473/events", "html_url": "https://github.com/pytorch/text/issues/473", "id": 379610914, "node_id": "MDU6SXNzdWUzNzk2MTA5MTQ=", "number": 473, "title": "How to make TabularDataset loading csv faster", "user": {"login": "whbzju", "id": 1175651, "node_id": "MDQ6VXNlcjExNzU2NTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1175651?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whbzju", "html_url": "https://github.com/whbzju", "followers_url": "https://api.github.com/users/whbzju/followers", "following_url": "https://api.github.com/users/whbzju/following{/other_user}", "gists_url": "https://api.github.com/users/whbzju/gists{/gist_id}", "starred_url": "https://api.github.com/users/whbzju/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whbzju/subscriptions", "organizations_url": "https://api.github.com/users/whbzju/orgs", "repos_url": "https://api.github.com/users/whbzju/repos", "events_url": "https://api.github.com/users/whbzju/events{/privacy}", "received_events_url": "https://api.github.com/users/whbzju/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-11-12T03:54:11Z", "updated_at": "2020-05-29T09:43:30Z", "closed_at": "2019-01-31T20:00:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "I try torchtext with data from https://www.kaggle.com/c/quora-question-pairs/data, which size is 200MB.\r\n\r\ntrain.csv: 404302 lines\r\ntest.csv:3563490 lines\r\n\r\nI use TabularDataset to load them, it's too slow. For train.csv, it cost 5mins, and for test.csv, it can't finish in 40mins. But it only need 3sec when use pandas to load train.csv, and 20 sec for keras's texts_to_sequences to process.\r\n\r\nhere is my code, does anything wrong:\r\n`print(\"Preparing CSV files...\")\r\n\r\n    QUESTION = data.Field(\r\n        sequential=True,\r\n        fix_length=fix_length,\r\n        tokenize=tokenizer,\r\n        init_token='SOS',\r\n        eos_token='EOS',\r\n        lower=lower\r\n    )\r\n\r\n    LABEL = data.Field(\r\n        sequential=False,\r\n    )\r\n\r\n    print(\"Reading train csv file...\")\r\n    train = data.TabularDataset(\r\n        path= mypath + '/train.csv', format='csv', skip_header=True,\r\n        fields=[\r\n            ('id', None),\r\n            ('qid1', None),\r\n            ('qid2', None),\r\n            ('question1', QUESTION),\r\n            ('question2', QUESTION),\r\n            ('label', LABEL),\r\n        ])\r\n\r\n    print(vars(train[0]))` ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/469", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/469/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/469/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/469/events", "html_url": "https://github.com/pytorch/text/issues/469", "id": 377813931, "node_id": "MDU6SXNzdWUzNzc4MTM5MzE=", "number": 469, "title": "How does torchtext define the example that makes up the dataset?", "user": {"login": "cswangjiawei", "id": 33107884, "node_id": "MDQ6VXNlcjMzMTA3ODg0", "avatar_url": "https://avatars0.githubusercontent.com/u/33107884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cswangjiawei", "html_url": "https://github.com/cswangjiawei", "followers_url": "https://api.github.com/users/cswangjiawei/followers", "following_url": "https://api.github.com/users/cswangjiawei/following{/other_user}", "gists_url": "https://api.github.com/users/cswangjiawei/gists{/gist_id}", "starred_url": "https://api.github.com/users/cswangjiawei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cswangjiawei/subscriptions", "organizations_url": "https://api.github.com/users/cswangjiawei/orgs", "repos_url": "https://api.github.com/users/cswangjiawei/repos", "events_url": "https://api.github.com/users/cswangjiawei/events{/privacy}", "received_events_url": "https://api.github.com/users/cswangjiawei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-06T11:54:58Z", "updated_at": "2018-11-15T07:50:05Z", "closed_at": "2018-11-15T07:50:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "How does torchtext define the example that makes up the dataset ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/468", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/468/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/468/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/468/events", "html_url": "https://github.com/pytorch/text/issues/468", "id": 377742171, "node_id": "MDU6SXNzdWUzNzc3NDIxNzE=", "number": 468, "title": "field", "user": {"login": "cswangjiawei", "id": 33107884, "node_id": "MDQ6VXNlcjMzMTA3ODg0", "avatar_url": "https://avatars0.githubusercontent.com/u/33107884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cswangjiawei", "html_url": "https://github.com/cswangjiawei", "followers_url": "https://api.github.com/users/cswangjiawei/followers", "following_url": "https://api.github.com/users/cswangjiawei/following{/other_user}", "gists_url": "https://api.github.com/users/cswangjiawei/gists{/gist_id}", "starred_url": "https://api.github.com/users/cswangjiawei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cswangjiawei/subscriptions", "organizations_url": "https://api.github.com/users/cswangjiawei/orgs", "repos_url": "https://api.github.com/users/cswangjiawei/repos", "events_url": "https://api.github.com/users/cswangjiawei/events{/privacy}", "received_events_url": "https://api.github.com/users/cswangjiawei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-06T08:40:22Z", "updated_at": "2018-11-15T07:49:45Z", "closed_at": "2018-11-15T07:49:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a TSV file training_abstracts.tsv when I use the function:\r\n\u2018def test1():\r\n    with open('F:\\\\small paper\\\\ner\\\\data_file\\\\training_abstracts.tsv', 'r', encoding='utf-8') as f1:\r\n        line = f1.readline()\r\n        while line:\r\n            list1 = line.split('\\t')\r\n            text = list1[1]\r\n            bio = list1[2]\r\n            if len(tokenizer(text)) != len(bio.split()):\r\n                print('len(tokenizer(text)):', len(tokenizer(text)))\r\n                print('len(bio.split()):', len(bio.split()))\r\n                print(line)\r\n            line = f1.readline()\u2019\r\n\r\nWhen testing it, the length of the text column is the same as the length of the tag column, but when I define\uff1a\r\n\u2018text = data.Field(batch_first=True, include_lengths=True, tokenize=tokenizer, lower=True)\r\n tag = data.Field(batch_first=True, include_lengths=True)\u2019\uff0c\r\nduring the experiment, there is a situation where the length of the text column and the length of the tag column are inconsistent. Why?\r\n\r\nOther related code:\r\n\u2018train_data, val_data, test_data = data.TabularDataset.splits(path='data_file/', train='training_abstracts.tsv',\r\n                                                                 validation='development_abstracts.tsv',\r\n                                                                 test='evaluation_abstracts.tsv',\r\n                                                                 fields=[('pmid', None), ('text', text), ('tag', tag)], format='tsv')\u2019\r\n\r\n\u2018train_loader, val_loader, test_loader = data.BucketIterator.splits((train_data, val_data, test_data), \r\n                                                                        batch_sizes=batch_sizes,\r\n                                                                       sort_key=lambda x: len(x.text),\r\n                                                                       sort_within_batch=True, repeat=False)\u2019\r\n\r\n\u8fd9\u662f'training_abstracts.tsv'\u6587\u4ef6\uff1a\r\n\r\n\r\n[training_abstracts.zip](https://github.com/pytorch/text/files/2552220/training_abstracts.zip)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/467", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/467/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/467/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/467/events", "html_url": "https://github.com/pytorch/text/issues/467", "id": 376941754, "node_id": "MDU6SXNzdWUzNzY5NDE3NTQ=", "number": 467, "title": "Copy Construct warning", "user": {"login": "vince62s", "id": 15141326, "node_id": "MDQ6VXNlcjE1MTQxMzI2", "avatar_url": "https://avatars3.githubusercontent.com/u/15141326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vince62s", "html_url": "https://github.com/vince62s", "followers_url": "https://api.github.com/users/vince62s/followers", "following_url": "https://api.github.com/users/vince62s/following{/other_user}", "gists_url": "https://api.github.com/users/vince62s/gists{/gist_id}", "starred_url": "https://api.github.com/users/vince62s/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vince62s/subscriptions", "organizations_url": "https://api.github.com/users/vince62s/orgs", "repos_url": "https://api.github.com/users/vince62s/repos", "events_url": "https://api.github.com/users/vince62s/events{/privacy}", "received_events_url": "https://api.github.com/users/vince62s/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-11-02T19:35:03Z", "updated_at": "2020-01-18T21:42:44Z", "closed_at": "2019-05-30T15:51:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nwas this ever fixed ?\r\n\r\n/usr/local/lib/python3.5/dist-packages/torchtext/data/field.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/465", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/465/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/465/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/465/events", "html_url": "https://github.com/pytorch/text/issues/465", "id": 376404055, "node_id": "MDU6SXNzdWUzNzY0MDQwNTU=", "number": 465, "title": "Package six is missing from requirements", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-01T13:59:28Z", "updated_at": "2019-01-31T16:31:14Z", "closed_at": "2019-01-31T16:31:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I just installed a fresh version of torchtext from pip and tried to load a simple TabularDataset. \r\nIt crashed due to the dependency to `six`, which is used in [example.py](https://github.com/pytorch/text/blob/9187177364570e0625c8bfe409f05e494412b4d6/torchtext/data/example.py#L3).\r\n\r\n**Problem**: The [setup.py](https://github.com/pytorch/text/blob/9187177364570e0625c8bfe409f05e494412b4d6/setup.py#L40) script does not contain `six` under `install_requires`. \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/464", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/464/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/464/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/464/events", "html_url": "https://github.com/pytorch/text/issues/464", "id": 376328025, "node_id": "MDU6SXNzdWUzNzYzMjgwMjU=", "number": 464, "title": "How should I use torchtext to partition the data?", "user": {"login": "lmatz", "id": 5791930, "node_id": "MDQ6VXNlcjU3OTE5MzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5791930?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmatz", "html_url": "https://github.com/lmatz", "followers_url": "https://api.github.com/users/lmatz/followers", "following_url": "https://api.github.com/users/lmatz/following{/other_user}", "gists_url": "https://api.github.com/users/lmatz/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmatz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmatz/subscriptions", "organizations_url": "https://api.github.com/users/lmatz/orgs", "repos_url": "https://api.github.com/users/lmatz/repos", "events_url": "https://api.github.com/users/lmatz/events{/privacy}", "received_events_url": "https://api.github.com/users/lmatz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-01T10:05:12Z", "updated_at": "2018-11-02T01:23:56Z", "closed_at": "2018-11-02T01:23:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there a way that I can ask torchtext to let workers load mutually exclusive partitions of a single data file? Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/463", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/463/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/463/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/463/events", "html_url": "https://github.com/pytorch/text/issues/463", "id": 374903792, "node_id": "MDU6SXNzdWUzNzQ5MDM3OTI=", "number": 463, "title": "Multiple Fields Share One Column in CSV or TSV", "user": {"login": "shocho3858", "id": 36754839, "node_id": "MDQ6VXNlcjM2NzU0ODM5", "avatar_url": "https://avatars3.githubusercontent.com/u/36754839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shocho3858", "html_url": "https://github.com/shocho3858", "followers_url": "https://api.github.com/users/shocho3858/followers", "following_url": "https://api.github.com/users/shocho3858/following{/other_user}", "gists_url": "https://api.github.com/users/shocho3858/gists{/gist_id}", "starred_url": "https://api.github.com/users/shocho3858/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shocho3858/subscriptions", "organizations_url": "https://api.github.com/users/shocho3858/orgs", "repos_url": "https://api.github.com/users/shocho3858/repos", "events_url": "https://api.github.com/users/shocho3858/events{/privacy}", "received_events_url": "https://api.github.com/users/shocho3858/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-29T08:22:09Z", "updated_at": "2019-05-30T15:53:19Z", "closed_at": "2019-05-30T15:53:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "However I can duplicate the column in the input file, I wonder if this functionality exists in raw torchtext?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/460", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/460/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/460/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/460/events", "html_url": "https://github.com/pytorch/text/issues/460", "id": 373321556, "node_id": "MDU6SXNzdWUzNzMzMjE1NTY=", "number": 460, "title": "RawField incompatibility inside of Batch", "user": {"login": "mfuntowicz", "id": 2241520, "node_id": "MDQ6VXNlcjIyNDE1MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2241520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfuntowicz", "html_url": "https://github.com/mfuntowicz", "followers_url": "https://api.github.com/users/mfuntowicz/followers", "following_url": "https://api.github.com/users/mfuntowicz/following{/other_user}", "gists_url": "https://api.github.com/users/mfuntowicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfuntowicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfuntowicz/subscriptions", "organizations_url": "https://api.github.com/users/mfuntowicz/orgs", "repos_url": "https://api.github.com/users/mfuntowicz/repos", "events_url": "https://api.github.com/users/mfuntowicz/events{/privacy}", "received_events_url": "https://api.github.com/users/mfuntowicz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-24T06:17:20Z", "updated_at": "2018-10-24T16:05:31Z", "closed_at": "2018-10-24T16:05:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've updated torchtext to its latest master, and it seems that the setup for Squad I was using before is not working anymore.\r\n\r\nMore precisely, I was using 2 RawField which allow me to get back from the predicted tokens span to the initial text : \r\n\r\n\r\n```python\r\nfields = [\r\n                ('id', RawField()),\r\n                (('content_w', 'content_c'), (text_f, char_f)),\r\n                (('question_w', 'question_c'), (text_f, char_f)),\r\n                ('context', RawField()),\r\n                ('answer', RawField()),\r\n                ('answer_start', LabelField(use_vocab=False)),\r\n                ('answer_end', LabelField(use_vocab=False))\r\n            ]\r\n```\r\nNow, the following set of Field makes the Batch creation crash as : \r\n\r\n```python\r\n  File \"/data/Workspace/miniconda3/envs/pytorch/lib/python3.7/site-packages/torchtext/data/iterator.py\", line 156, in __iter__\r\n    yield Batch(minibatch, self.dataset, self.device)\r\n  File \"/data/Workspace/miniconda3/envs/pytorch/lib/python3.7/site-packages/torchtext/data/batch.py\", line 26, in __init__\r\n    self.input_fields = [k for k, v in dataset.fields.items() if\r\n  File \"/data/Workspace/miniconda3/envs/pytorch/lib/python3.7/site-packages/torchtext/data/batch.py\", line 27, in <listcomp>\r\n    v is not None and not v.is_target]\r\nAttributeError: 'RawField' object has no attribute 'is_target'\r\n```\r\n\r\nIs there a new way of handling this ? Or is it possible to make Batch compatible again with RawField ?\r\n\r\n_One possible way would be to introduce the property is_target at the RawField level, so that every children of RawField would have this field available._\r\n\r\nThanks for your feedback,\r\nMorgan", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/pytorch/text/issues/457", "repository_url": "https://api.github.com/repos/pytorch/text", "labels_url": "https://api.github.com/repos/pytorch/text/issues/457/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/text/issues/457/comments", "events_url": "https://api.github.com/repos/pytorch/text/issues/457/events", "html_url": "https://github.com/pytorch/text/issues/457", "id": 372574829, "node_id": "MDU6SXNzdWUzNzI1NzQ4Mjk=", "number": 457, "title": "Field.stop_words is overwritten and is no longer a set", "user": {"login": "artemisart", "id": 9201969, "node_id": "MDQ6VXNlcjkyMDE5Njk=", "avatar_url": "https://avatars2.githubusercontent.com/u/9201969?v=4", "gravatar_id": "", "url": "https://api.github.com/users/artemisart", "html_url": "https://github.com/artemisart", "followers_url": "https://api.github.com/users/artemisart/followers", "following_url": "https://api.github.com/users/artemisart/following{/other_user}", "gists_url": "https://api.github.com/users/artemisart/gists{/gist_id}", "starred_url": "https://api.github.com/users/artemisart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/artemisart/subscriptions", "organizations_url": "https://api.github.com/users/artemisart/orgs", "repos_url": "https://api.github.com/users/artemisart/repos", "events_url": "https://api.github.com/users/artemisart/events{/privacy}", "received_events_url": "https://api.github.com/users/artemisart/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-22T15:41:22Z", "updated_at": "2018-10-23T08:31:16Z", "closed_at": "2018-10-23T08:31:16Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/pytorch/text/blob/758d356fd76ee6102de06a4cb5e414a3164c9e42/torchtext/data/field.py#L166-L173\r\n\r\nI think this is an error and self.stop_words should not be overwritten after the condition.", "performed_via_github_app": null, "score": 1.0}]}