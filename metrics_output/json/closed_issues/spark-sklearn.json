{"total_count": 35, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/110", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/110/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/110/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/110/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/110", "id": 434523181, "node_id": "MDU6SXNzdWU0MzQ1MjMxODE=", "number": 110, "title": "spark-sklearn on windows- not working on local", "user": {"login": "mohkar123", "id": 10978313, "node_id": "MDQ6VXNlcjEwOTc4MzEz", "avatar_url": "https://avatars1.githubusercontent.com/u/10978313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mohkar123", "html_url": "https://github.com/mohkar123", "followers_url": "https://api.github.com/users/mohkar123/followers", "following_url": "https://api.github.com/users/mohkar123/following{/other_user}", "gists_url": "https://api.github.com/users/mohkar123/gists{/gist_id}", "starred_url": "https://api.github.com/users/mohkar123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mohkar123/subscriptions", "organizations_url": "https://api.github.com/users/mohkar123/orgs", "repos_url": "https://api.github.com/users/mohkar123/repos", "events_url": "https://api.github.com/users/mohkar123/events{/privacy}", "received_events_url": "https://api.github.com/users/mohkar123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-04-17T23:10:01Z", "updated_at": "2019-04-18T12:58:42Z", "closed_at": "2019-04-18T07:10:14Z", "author_association": "NONE", "active_lock_reason": null, "body": " am trying to execute this code from the spark-sklearn API documentation. I'm running on Windows 7 and on the latest spark-sklearn version. I'm executing inside the pyspark shell. Running spark version 2.3.2 (tried 2.4.0, same error)\r\n```\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.cluster import KMeans\r\nfrom pyspark.ml.linalg import Vectors, Matrices, MatrixUDT\r\nfrom pyspark.sql.functions import udf\r\nfrom pyspark.sql import SparkSession\r\nfrom spark_sklearn.util import createLocalSparkSession\r\nfrom spark_sklearn.keyed_models import KeyedEstimator\r\nspark = createLocalSparkSession()\r\ndf = spark.createDataFrame([(user,\r\n                             Vectors.dense([i, i ** 2, i ** 3]),\r\n                             0.0 + user + i + 2 * i ** 2 + 3 * i ** 3)\r\n                            for user in range(3) for i in range(5)])\r\ndf = df.toDF(\"key\", \"features\", \"y\")\r\ndf.where(\"5 < y and y < 10\").sort(\"key\", \"y\").show()\r\n\r\nkm = KeyedEstimator(sklearnEstimator=LinearRegression(), yCol=\"y\").fit(df)\r\ndef printFloat(x):\r\n    rounded = round(x, 2)\r\n    return \"{:.2f}\".format(0 if rounded == 0 else rounded)\r\n\r\ndef printModel(model):\r\n    coef = \"[\" + \", \".join(map(printFloat, model.coef_)) + \"]\"\r\n    intercept = printFloat(model.intercept_)\r\n    return \"intercept: {} coef: {}\".format(intercept, coef)\r\n\r\nkm.keyedModels.columns\r\n\r\nprintedModels = udf(printModel)(\"estimator\").alias(\"linear fit\")\r\nkm.keyedModels.select(\"key\", printedModels).sort(\"key\").show(truncate=False)`\r\n```\r\n\r\nOn running this I get the following error on the show at `km.keyedModels.select(\"key\", printedModels).sort(\"key\").show(truncate=False)`\r\n\r\n![image](https://user-images.githubusercontent.com/10978313/56326492-dec61900-613b-11e9-93cc-3f041ec6d70c.png)\r\n I cannot seem to find any solution online. Since the show before this is working, clearly my spark seems to be working. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/109", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/109/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/109/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/109/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/109", "id": 430046873, "node_id": "MDU6SXNzdWU0MzAwNDY4NzM=", "number": 109, "title": "It appears that you are attempting to reference SparkContext from a broadcast", "user": {"login": "fissehab", "id": 10905263, "node_id": "MDQ6VXNlcjEwOTA1MjYz", "avatar_url": "https://avatars1.githubusercontent.com/u/10905263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fissehab", "html_url": "https://github.com/fissehab", "followers_url": "https://api.github.com/users/fissehab/followers", "following_url": "https://api.github.com/users/fissehab/following{/other_user}", "gists_url": "https://api.github.com/users/fissehab/gists{/gist_id}", "starred_url": "https://api.github.com/users/fissehab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fissehab/subscriptions", "organizations_url": "https://api.github.com/users/fissehab/orgs", "repos_url": "https://api.github.com/users/fissehab/repos", "events_url": "https://api.github.com/users/fissehab/events{/privacy}", "received_events_url": "https://api.github.com/users/fissehab/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-04-06T15:48:32Z", "updated_at": "2019-04-06T16:26:56Z", "closed_at": "2019-04-06T16:26:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/108", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/108/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/108/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/108/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/108", "id": 429368150, "node_id": "MDU6SXNzdWU0MjkzNjgxNTA=", "number": 108, "title": "toSpark() must be called with Converter instance as first argument ", "user": {"login": "fissehab", "id": 10905263, "node_id": "MDQ6VXNlcjEwOTA1MjYz", "avatar_url": "https://avatars1.githubusercontent.com/u/10905263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fissehab", "html_url": "https://github.com/fissehab", "followers_url": "https://api.github.com/users/fissehab/followers", "following_url": "https://api.github.com/users/fissehab/following{/other_user}", "gists_url": "https://api.github.com/users/fissehab/gists{/gist_id}", "starred_url": "https://api.github.com/users/fissehab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fissehab/subscriptions", "organizations_url": "https://api.github.com/users/fissehab/orgs", "repos_url": "https://api.github.com/users/fissehab/repos", "events_url": "https://api.github.com/users/fissehab/events{/privacy}", "received_events_url": "https://api.github.com/users/fissehab/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-04-04T16:16:21Z", "updated_at": "2019-04-04T16:49:51Z", "closed_at": "2019-04-04T16:20:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want to convert a sklearn logistic regression model to Spark but it is not working for me:\r\n\r\n`from spark_sklearn import Converter\r\nlogreg_spark_ml = Converter.toSpark(logreg_cv)\r\n`\r\n\r\n> TypeError: unbound method toSpark() must be called with Converter instance as first argument (got GridSearchCV instance instead)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/107", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/107/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/107/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/107/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/107", "id": 411037803, "node_id": "MDU6SXNzdWU0MTEwMzc4MDM=", "number": 107, "title": "Need for an example", "user": {"login": "frbattid", "id": 3865056, "node_id": "MDQ6VXNlcjM4NjUwNTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/3865056?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frbattid", "html_url": "https://github.com/frbattid", "followers_url": "https://api.github.com/users/frbattid/followers", "following_url": "https://api.github.com/users/frbattid/following{/other_user}", "gists_url": "https://api.github.com/users/frbattid/gists{/gist_id}", "starred_url": "https://api.github.com/users/frbattid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frbattid/subscriptions", "organizations_url": "https://api.github.com/users/frbattid/orgs", "repos_url": "https://api.github.com/users/frbattid/repos", "events_url": "https://api.github.com/users/frbattid/events{/privacy}", "received_events_url": "https://api.github.com/users/frbattid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-02-16T06:45:28Z", "updated_at": "2019-02-16T15:36:30Z", "closed_at": "2019-02-16T15:36:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "More than an issue, I need an example regarding this statement in the main `README`:\r\n\r\n> convert Spark's Dataframes seamlessly into numpy ndarray or sparse matrices\r\n\r\nHow can I do this with `spark-sklearn`? Can you provide an example?\r\n\r\nMany thanks in advance.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/101", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/101/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/101/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/101/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/101", "id": 402019396, "node_id": "MDU6SXNzdWU0MDIwMTkzOTY=", "number": 101, "title": "Spark Broadcast exceeding executor memory with large training data set", "user": {"login": "shaunswanson", "id": 1555731, "node_id": "MDQ6VXNlcjE1NTU3MzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/1555731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shaunswanson", "html_url": "https://github.com/shaunswanson", "followers_url": "https://api.github.com/users/shaunswanson/followers", "following_url": "https://api.github.com/users/shaunswanson/following{/other_user}", "gists_url": "https://api.github.com/users/shaunswanson/gists{/gist_id}", "starred_url": "https://api.github.com/users/shaunswanson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shaunswanson/subscriptions", "organizations_url": "https://api.github.com/users/shaunswanson/orgs", "repos_url": "https://api.github.com/users/shaunswanson/repos", "events_url": "https://api.github.com/users/shaunswanson/events{/privacy}", "received_events_url": "https://api.github.com/users/shaunswanson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-01-23T00:47:09Z", "updated_at": "2019-03-20T06:22:33Z", "closed_at": "2019-01-28T17:00:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "My X variable is between 1-4 GB. The pre_dispatch arg when initializing GridSearchCV doesn't appear to be used or have any effect, preventing me from parallelizing a decently-sized param_grid in my Spark cluster.\r\n\r\nThe X variable is being broadcast to all combinations of param_grid (48 in our case), causing memory to fill up.\r\n\r\nhttps://github.com/databricks/spark-sklearn/blob/705401005de4c951604645e737ea303a1788709f/python/spark_sklearn/grid_search.py#L307\r\n\r\nIs there a way that the code above could pull the broadcast variables, rather than pushing them eagerly to every task?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/87", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/87/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/87/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/87/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/87", "id": 373862729, "node_id": "MDU6SXNzdWUzNzM4NjI3Mjk=", "number": 87, "title": "ImportError: No module named", "user": {"login": "PoeteMaudit", "id": 32899037, "node_id": "MDQ6VXNlcjMyODk5MDM3", "avatar_url": "https://avatars3.githubusercontent.com/u/32899037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PoeteMaudit", "html_url": "https://github.com/PoeteMaudit", "followers_url": "https://api.github.com/users/PoeteMaudit/followers", "following_url": "https://api.github.com/users/PoeteMaudit/following{/other_user}", "gists_url": "https://api.github.com/users/PoeteMaudit/gists{/gist_id}", "starred_url": "https://api.github.com/users/PoeteMaudit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PoeteMaudit/subscriptions", "organizations_url": "https://api.github.com/users/PoeteMaudit/orgs", "repos_url": "https://api.github.com/users/PoeteMaudit/repos", "events_url": "https://api.github.com/users/PoeteMaudit/events{/privacy}", "received_events_url": "https://api.github.com/users/PoeteMaudit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-25T09:47:57Z", "updated_at": "2018-12-08T19:58:11Z", "closed_at": "2018-12-08T19:58:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Please have a look at my post on StackOverflow: https://stackoverflow.com/questions/52985968/gridsearchcv-on-spark-cluster-importerror-no-module-named.\r\n\r\nI am not entirely sure that the error which I am getting has to do directly with `spark-sklearn` but this is a nice opportunity to clarify this.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/86", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/86/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/86/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/86/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/86", "id": 365582864, "node_id": "MDU6SXNzdWUzNjU1ODI4NjQ=", "number": 86, "title": "\"TypeError: Can't instantiate abstract class GridSearchCV with abstract methods _run_search\"", "user": {"login": "cmglaze", "id": 43760238, "node_id": "MDQ6VXNlcjQzNzYwMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/43760238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cmglaze", "html_url": "https://github.com/cmglaze", "followers_url": "https://api.github.com/users/cmglaze/followers", "following_url": "https://api.github.com/users/cmglaze/following{/other_user}", "gists_url": "https://api.github.com/users/cmglaze/gists{/gist_id}", "starred_url": "https://api.github.com/users/cmglaze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cmglaze/subscriptions", "organizations_url": "https://api.github.com/users/cmglaze/orgs", "repos_url": "https://api.github.com/users/cmglaze/repos", "events_url": "https://api.github.com/users/cmglaze/events{/privacy}", "received_events_url": "https://api.github.com/users/cmglaze/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-01T18:20:01Z", "updated_at": "2018-12-09T21:51:32Z", "closed_at": "2018-12-09T21:51:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Getting this error after a very basic call to GridSearchCV on Databricks:\r\n \r\n![capture](https://user-images.githubusercontent.com/43760238/46307495-121cce00-c585-11e8-90ed-443baddd658e.PNG)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/82", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/82/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/82/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/82/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/82", "id": 341366819, "node_id": "MDU6SXNzdWUzNDEzNjY4MTk=", "number": 82, "title": "Spark 2.3 compatible?", "user": {"login": "Tagar", "id": 3013418, "node_id": "MDQ6VXNlcjMwMTM0MTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/3013418?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tagar", "html_url": "https://github.com/Tagar", "followers_url": "https://api.github.com/users/Tagar/followers", "following_url": "https://api.github.com/users/Tagar/following{/other_user}", "gists_url": "https://api.github.com/users/Tagar/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tagar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tagar/subscriptions", "organizations_url": "https://api.github.com/users/Tagar/orgs", "repos_url": "https://api.github.com/users/Tagar/repos", "events_url": "https://api.github.com/users/Tagar/events{/privacy}", "received_events_url": "https://api.github.com/users/Tagar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-16T00:54:55Z", "updated_at": "2018-12-08T12:17:09Z", "closed_at": "2018-12-08T12:17:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Seems to be such a great idea / use case to run `sklearn` distributely through Spark!\r\nHaven't seen `databricks/spark-sklearn` updates for almost a year.. is this an active project? \r\nIs this known to work with Spark 2.3? \r\nThank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/81", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/81/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/81/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/81/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/81", "id": 326526561, "node_id": "MDU6SXNzdWUzMjY1MjY1NjE=", "number": 81, "title": "ImportError: Module not found with Azure Spark Cluster", "user": {"login": "Nimi42", "id": 25004433, "node_id": "MDQ6VXNlcjI1MDA0NDMz", "avatar_url": "https://avatars2.githubusercontent.com/u/25004433?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nimi42", "html_url": "https://github.com/Nimi42", "followers_url": "https://api.github.com/users/Nimi42/followers", "following_url": "https://api.github.com/users/Nimi42/following{/other_user}", "gists_url": "https://api.github.com/users/Nimi42/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nimi42/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nimi42/subscriptions", "organizations_url": "https://api.github.com/users/Nimi42/orgs", "repos_url": "https://api.github.com/users/Nimi42/repos", "events_url": "https://api.github.com/users/Nimi42/events{/privacy}", "received_events_url": "https://api.github.com/users/Nimi42/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-25T13:34:37Z", "updated_at": "2018-12-08T19:59:41Z", "closed_at": "2018-12-08T19:59:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to run spark-sklearns GridSearch on an HDInsight Cluster from Azure. Here is a Code Snippet:\r\n\r\n```\r\n        model = KerasRegressor(build_fn=build_model, verbose=0)\r\n\r\n        kf = KFold(n_splits=self.cv_split, shuffle=True)  # Cross validation with k=5\r\n\r\n        sc = SparkContext.getOrCreate()\r\n        grid = GridSearchCV(sc=sc, estimator=model, param_grid=self.params, \r\n                            cv=kf, return_train_score=True, verbose=2,\r\n                            fit_params={'epochs': nb_epoch, 'batch_size': 32})\r\n\r\n        hist = grid.fit(x_train, y_train)\r\n```\r\n\r\nIt works fine until I call the grid.fit method, which returns the following exception:\r\n\r\n```\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000001/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\r\n    format(target_id, \".\", name), value)\r\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 13, wn0-bt-nsu.kkatsjzvwzuephdjshji40kxae.ax.internal.cloudapp.net, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/worker.py\", line 166, in main\r\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/worker.py\", line 55, in read_command\r\n    command = serializer._read_with_length(file)\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\r\n    return self.loads(obj)\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/serializers.py\", line 455, in loads\r\n    return pickle.loads(obj, encoding=encoding)\r\nImportError: No module named 'ml'\r\n\r\n...\r\n\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/worker.py\", line 166, in main\r\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/worker.py\", line 55, in read_command\r\n    command = serializer._read_with_length(file)\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\r\n    return self.loads(obj)\r\n  File \"/mnt/resource/hadoop/yarn/local/usercache/nsusshuser/appcache/application_1526397916826_0020/container_e01_1526397916826_0020_01_000002/pyspark.zip/pyspark/serializers.py\", line 455, in loads\r\n    return pickle.loads(obj, encoding=encoding)\r\nImportError: No module named 'ml'\r\n```\r\n\r\nThe ml module is part of our project. I checked sys.modules and it is in there. Don't really understand the error message. Can somebody help me out?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/80", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/80/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/80/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/80/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/80", "id": 323616877, "node_id": "MDU6SXNzdWUzMjM2MTY4Nzc=", "number": 80, "title": "pip install spark-sklearn-(version-no) doesn't work", "user": {"login": "falconic", "id": 8172181, "node_id": "MDQ6VXNlcjgxNzIxODE=", "avatar_url": "https://avatars1.githubusercontent.com/u/8172181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/falconic", "html_url": "https://github.com/falconic", "followers_url": "https://api.github.com/users/falconic/followers", "following_url": "https://api.github.com/users/falconic/following{/other_user}", "gists_url": "https://api.github.com/users/falconic/gists{/gist_id}", "starred_url": "https://api.github.com/users/falconic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/falconic/subscriptions", "organizations_url": "https://api.github.com/users/falconic/orgs", "repos_url": "https://api.github.com/users/falconic/repos", "events_url": "https://api.github.com/users/falconic/events{/privacy}", "received_events_url": "https://api.github.com/users/falconic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-05-16T13:18:21Z", "updated_at": "2018-12-08T20:04:57Z", "closed_at": "2018-12-08T20:04:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi\r\nI am looking to use best_score_ parameter from GridSearchCV function, but it looks like that is not present in the latest version of the library. When I'm trying to uninstall the latest version and reinstall and older version with the command \r\npip install spark-sklearn-(version-no)\r\nIt does not work. Please rectify this or refer me to a some documentation to see how I can install older versions in my cluster environments. \r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/77", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/77/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/77/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/77/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/77", "id": 317726886, "node_id": "MDU6SXNzdWUzMTc3MjY4ODY=", "number": 77, "title": "Implement parallelized RandomizedSearchCV", "user": {"login": "munro", "id": 500774, "node_id": "MDQ6VXNlcjUwMDc3NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/500774?v=4", "gravatar_id": "", "url": "https://api.github.com/users/munro", "html_url": "https://github.com/munro", "followers_url": "https://api.github.com/users/munro/followers", "following_url": "https://api.github.com/users/munro/following{/other_user}", "gists_url": "https://api.github.com/users/munro/gists{/gist_id}", "starred_url": "https://api.github.com/users/munro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/munro/subscriptions", "organizations_url": "https://api.github.com/users/munro/orgs", "repos_url": "https://api.github.com/users/munro/repos", "events_url": "https://api.github.com/users/munro/events{/privacy}", "received_events_url": "https://api.github.com/users/munro/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-25T17:40:18Z", "updated_at": "2018-12-09T23:09:24Z", "closed_at": "2018-12-09T23:09:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is a very useful tool in sklearn for me, I could make my case but there's a lot of places online that explains it much better than I can.\r\n\r\n\r\nhttps://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881\r\n\r\nhttps://stats.stackexchange.com/questions/160479/practical-hyperparameter-optimization-random-vs-grid-search\r\n\r\nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/63", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/63/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/63/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/63/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/63", "id": 257615962, "node_id": "MDU6SXNzdWUyNTc2MTU5NjI=", "number": 63, "title": "TypeError: fit() argument after ** must be a mapping, not NoneType", "user": {"login": "skepticleo", "id": 2614728, "node_id": "MDQ6VXNlcjI2MTQ3Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/2614728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skepticleo", "html_url": "https://github.com/skepticleo", "followers_url": "https://api.github.com/users/skepticleo/followers", "following_url": "https://api.github.com/users/skepticleo/following{/other_user}", "gists_url": "https://api.github.com/users/skepticleo/gists{/gist_id}", "starred_url": "https://api.github.com/users/skepticleo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skepticleo/subscriptions", "organizations_url": "https://api.github.com/users/skepticleo/orgs", "repos_url": "https://api.github.com/users/skepticleo/repos", "events_url": "https://api.github.com/users/skepticleo/events{/privacy}", "received_events_url": "https://api.github.com/users/skepticleo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-14T07:05:00Z", "updated_at": "2018-12-09T21:52:20Z", "closed_at": "2018-12-09T21:52:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to run the sample code with iris dataset:\r\n\r\n```\r\nfrom sklearn import svm, datasets\r\nfrom pyspark import SparkContext\r\nfrom spark_sklearn import GridSearchCV\r\niris = datasets.load_iris()\r\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\r\nsvr = svm.SVC()\r\nsc = SparkContext()\r\nclf = GridSearchCV(sc, svr, parameters)\r\nclf.fit(iris.data, iris.target)\r\n```\r\n\r\nRunning this throws a TypeError, \r\n\r\n```\r\nTraceback (most recent call last):\r\n\"<stdin>\", line 1, in <module>\r\n\"spark_sklearn/grid_search.py\", line 272, in fit\r\nreturn self._fit(X, y, groups, ParameterGrid(self.param_grid))\r\n\"spark_sklearn/grid_search.py\", line 400, in _fit\r\nbest_estimator.fit(X, y, **fit_params)\r\nTypeError: fit() argument after ** must be a mapping, not NoneType\r\n```\r\n\r\n```\r\nEnvironment Info:\r\npython 3.5.2\r\nspark 2.2.0\r\nspark_sklearn 0.2.0\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/59", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/59/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/59/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/59/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/59", "id": 251975999, "node_id": "MDU6SXNzdWUyNTE5NzU5OTk=", "number": 59, "title": "AttributeError: 'function' object has no attribute '_input_kwargs'", "user": {"login": "sounakban", "id": 11195849, "node_id": "MDQ6VXNlcjExMTk1ODQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/11195849?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sounakban", "html_url": "https://github.com/sounakban", "followers_url": "https://api.github.com/users/sounakban/followers", "following_url": "https://api.github.com/users/sounakban/following{/other_user}", "gists_url": "https://api.github.com/users/sounakban/gists{/gist_id}", "starred_url": "https://api.github.com/users/sounakban/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sounakban/subscriptions", "organizations_url": "https://api.github.com/users/sounakban/orgs", "repos_url": "https://api.github.com/users/sounakban/repos", "events_url": "https://api.github.com/users/sounakban/events{/privacy}", "received_events_url": "https://api.github.com/users/sounakban/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-22T14:26:54Z", "updated_at": "2018-12-09T21:56:41Z", "closed_at": "2018-12-09T21:56:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using python 2.7, spark-2.2.0 with hadoop2.7 and sklearn 0.19.\r\nI get the following error:\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 19, in <module>\r\n    km = KeyedEstimator(sklearnEstimator=LinearRegression(), yCol=\"y\").fit(df)\r\n  File \"C:\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\\\_\\_init\\_\\_.py\", line 104, in wrapper\r\n    return func(self, **kwargs)\r\n  File \"C:\\Python27\\lib\\site-packages\\spark_sklearn\\keyed_models.py\", line 323, in \\_\\_init\\_\\_\r\n    kwargs = KeyedEstimator._inferredParams(sklearnEstimator, self.\\_\\_init\\_\\_._input_kwargs)\r\nAttributeError: 'function' object has no attribute '_input_kwargs'\r\n\r\nwhen I try to run the code:\r\nkm = KeyedEstimator(sklearnEstimator=LinearRegression(), yCol=\"y\").fit(df)\r\nFrom the origian example code available in the welcome page.\r\nI also tried the Kmeans clustering, but it caused the same error.\r\n\r\nI downloaded the source code and checked line 323 in keyed_models.py, which was:\r\nkwargs = KeyedEstimator._inferredParams(sklearnEstimator, self._input_kwargs)\r\nPlease correct me if I'm wrong but the two, do not seem to match", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/58", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/58/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/58/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/58/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/58", "id": 244073289, "node_id": "MDU6SXNzdWUyNDQwNzMyODk=", "number": 58, "title": "Convert IndexError: tuple index out of range", "user": {"login": "nlathia", "id": 1625337, "node_id": "MDQ6VXNlcjE2MjUzMzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1625337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nlathia", "html_url": "https://github.com/nlathia", "followers_url": "https://api.github.com/users/nlathia/followers", "following_url": "https://api.github.com/users/nlathia/following{/other_user}", "gists_url": "https://api.github.com/users/nlathia/gists{/gist_id}", "starred_url": "https://api.github.com/users/nlathia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nlathia/subscriptions", "organizations_url": "https://api.github.com/users/nlathia/orgs", "repos_url": "https://api.github.com/users/nlathia/repos", "events_url": "https://api.github.com/users/nlathia/events{/privacy}", "received_events_url": "https://api.github.com/users/nlathia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 255692414, "node_id": "MDU6TGFiZWwyNTU2OTI0MTQ=", "url": "https://api.github.com/repos/databricks/spark-sklearn/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": true, "assignee": {"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/databricks/spark-sklearn/milestones/1", "html_url": "https://github.com/databricks/spark-sklearn/milestone/1", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/milestones/1/labels", "id": 3882332, "node_id": "MDk6TWlsZXN0b25lMzg4MjMzMg==", "number": 1, "title": "0.3.0", "description": "", "creator": {"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 11, "state": "closed", "created_at": "2018-12-08T15:31:49Z", "updated_at": "2019-01-30T19:23:14Z", "due_on": null, "closed_at": "2019-01-29T20:36:29Z"}, "comments": 4, "created_at": "2017-07-19T15:05:07Z", "updated_at": "2018-12-10T22:00:11Z", "closed_at": "2018-12-10T16:48:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using Python 3.4.5, and spark-2.1.0-bin-hadoop2.7, and sklearn 0.18.2. When I use the `Convert.toSKLearn()` and then try to make predictions, I get a `IndexError: tuple index out of range` error.\r\n\r\nFull example below:\r\n```\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.ml.linalg import Vectors\r\nfrom pyspark.sql import Row\r\nfrom pyspark.ml.classification import LogisticRegression\r\nfrom spark_sklearn import Converter\r\nimport numpy as np\r\n\r\nspark = SparkSession.builder.getOrCreate()\r\n\r\n# Create fake data\r\nfeatures = [\r\n    [1, 0, 0],\r\n    [1, 0, 0],\r\n    [0, 0, 1]\r\n]\r\nlabels = [1, 1, 0]\r\n\r\ndata = []\r\nfor i in range(len(features)):\r\n    data.append(\r\n        Row(label=labels[i], features=Vectors.dense(features[i]))\r\n    )\r\n\r\ndf = spark.sparkContext.parallelize(data).toDF()\r\ndf.show()\r\n\r\n# Train a model\r\nmodel = LogisticRegression()\r\nmodel = model.fit(df)\r\n\r\n# Convert to sklearn\r\nconverter = Converter(spark.sparkContext)\r\nsk_model = converter.toSKLearn(model)\r\n\r\n# Make some predictions\r\npredictions = sk_model.predict_proba(np.vstack(features))\r\nprint(predictions)\r\n```\r\nWhich raises this:\r\n```\r\nTraceback (most recent call last):\r\n[...]\r\n  File \"<path>/.venv/lib/python3.4/site-packages/sklearn/linear_model/logistic.py\", line 1286, in predict_proba\r\n    return super(LogisticRegression, self)._predict_proba_lr(X)\r\n  File \"<path>/.venv/lib/python3.4/site-packages/sklearn/linear_model/base.py\", line 350, in _predict_proba_lr\r\n    prob = self.decision_function(X)\r\n  File \"<path>/.venv/lib/python3.4/site-packages/sklearn/linear_model/base.py\", line 314, in decision_function\r\n    n_features = self.coef_.shape[1]\r\nIndexError: tuple index out of range\r\n```\r\n\r\nIt seems that this can be solved by reshaping the coefficients:\r\n```\r\n# Convert to sklearn\r\nconverter = Converter(spark.sparkContext)\r\nsk_model = converter.toSKLearn(model)\r\n\r\nsk_model.coef_ = sk_model.coef_.reshape(1, -1)  # This is needed!\r\n\r\npredictions = sk_model.predict_proba(np.vstack(features))\r\nprint(predictions)\r\n```\r\n\r\nIs this the right thing to do, or have I missed something?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/54", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/54/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/54/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/54/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/54", "id": 235976099, "node_id": "MDU6SXNzdWUyMzU5NzYwOTk=", "number": 54, "title": "Update to Spark 2.1", "user": {"login": "jkbradley", "id": 5084283, "node_id": "MDQ6VXNlcjUwODQyODM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5084283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jkbradley", "html_url": "https://github.com/jkbradley", "followers_url": "https://api.github.com/users/jkbradley/followers", "following_url": "https://api.github.com/users/jkbradley/following{/other_user}", "gists_url": "https://api.github.com/users/jkbradley/gists{/gist_id}", "starred_url": "https://api.github.com/users/jkbradley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jkbradley/subscriptions", "organizations_url": "https://api.github.com/users/jkbradley/orgs", "repos_url": "https://api.github.com/users/jkbradley/repos", "events_url": "https://api.github.com/users/jkbradley/events{/privacy}", "received_events_url": "https://api.github.com/users/jkbradley/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-14T18:52:59Z", "updated_at": "2017-09-05T20:12:45Z", "closed_at": "2017-09-05T20:12:45Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "* Add 2.1 to Travis build\r\n* Fix any issues which come up", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/52", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/52/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/52/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/52/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/52", "id": 206175791, "node_id": "MDU6SXNzdWUyMDYxNzU3OTE=", "number": 52, "title": "Crashing for larger data set", "user": {"login": "manjush3v", "id": 4996710, "node_id": "MDQ6VXNlcjQ5OTY3MTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/4996710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manjush3v", "html_url": "https://github.com/manjush3v", "followers_url": "https://api.github.com/users/manjush3v/followers", "following_url": "https://api.github.com/users/manjush3v/following{/other_user}", "gists_url": "https://api.github.com/users/manjush3v/gists{/gist_id}", "starred_url": "https://api.github.com/users/manjush3v/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manjush3v/subscriptions", "organizations_url": "https://api.github.com/users/manjush3v/orgs", "repos_url": "https://api.github.com/users/manjush3v/repos", "events_url": "https://api.github.com/users/manjush3v/events{/privacy}", "received_events_url": "https://api.github.com/users/manjush3v/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-08T11:48:23Z", "updated_at": "2018-12-08T20:03:13Z", "closed_at": "2018-12-08T20:03:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am running spark context with specification -\r\n```\r\nfrom pyspark import SparkConf, SparkContext\r\nconf = (SparkConf()\r\n         .setMaster(\"spark-master-url\")\r\n         .setAppName(\"PySparkShell\")\r\n         .set(\"spark.executor.memory\", \"6800M\"))\r\nsc = SparkContext(conf = conf)\r\n```\r\nThe program is working fine when X_train length is 5000 but fails when the size is increased to 12000.\r\n\r\nspark keeps crashing with following errors -\r\n\r\n```\r\n Lost task 13.0 in stage 1.0 (TID 109, 172.31.8.203, executor 1): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:230)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n        at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:99)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\r\n        ... 11 more\r\n```\r\n\r\n\r\nMore details [here](https://stackoverflow.com/questions/42110358/the-maximum-recommended-task-size-is-100-kb-pyspark-error)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/50", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/50/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/50/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/50/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/50", "id": 198402779, "node_id": "MDU6SXNzdWUxOTg0MDI3Nzk=", "number": 50, "title": "readme typo", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-01-03T02:52:53Z", "updated_at": "2018-12-08T12:16:40Z", "closed_at": "2018-12-08T12:16:40Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "\"the official Spark target is Spark 0.2\"", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/49", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/49/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/49/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/49/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/49", "id": 195941572, "node_id": "MDU6SXNzdWUxOTU5NDE1NzI=", "number": 49, "title": "Error When Calling toPandas()", "user": {"login": "sdjksdafji", "id": 2344068, "node_id": "MDQ6VXNlcjIzNDQwNjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/2344068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sdjksdafji", "html_url": "https://github.com/sdjksdafji", "followers_url": "https://api.github.com/users/sdjksdafji/followers", "following_url": "https://api.github.com/users/sdjksdafji/following{/other_user}", "gists_url": "https://api.github.com/users/sdjksdafji/gists{/gist_id}", "starred_url": "https://api.github.com/users/sdjksdafji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sdjksdafji/subscriptions", "organizations_url": "https://api.github.com/users/sdjksdafji/orgs", "repos_url": "https://api.github.com/users/sdjksdafji/repos", "events_url": "https://api.github.com/users/sdjksdafji/events{/privacy}", "received_events_url": "https://api.github.com/users/sdjksdafji/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-12-15T23:14:08Z", "updated_at": "2019-04-24T16:19:53Z", "closed_at": "2018-12-08T20:00:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nMy scripts are throwing errors when running on clusters:\r\n\r\nConverting training data to pandas dataframe\r\n                                                                                \r\nTraceback (most recent call last):\r\n  File \"/tmp/49a99d28-350b-4942-ab63-f7efa0d2f0ec/random-forest-sklearn.py\", line 244, in <module>\r\n    training_data = sk_converter.toPandas(trainingSet)\r\n  File \"/usr/local/lib/python3.4/dist-packages/spark_sklearn/converter.py\", line 159, in toPandas\r\n    return df.select(*cols).toPandas()\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 1442, in toPandas\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 311, in collect\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 142, in _load_from_socket\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in load_stream\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 156, in _read_with_length\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 543, in read_int\r\n  File \"/usr/lib/python3.4/socket.py\", line 371, in readinto\r\n    return self._sock.recv_into(b)\r\nsocket.timeout: timed out\r\n16/12/15 22:49:04 ERROR org.apache.spark.api.python.PythonRDD: Error while sending iterator\r\njava.net.SocketException: Broken pipe (Write failed)\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:109)\r\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:153)\r\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:492)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:504)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:504)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:112)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)\r\n\tat org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply$mcV$sp(PythonRDD.scala:700)\r\n\tat org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply(PythonRDD.scala:700)\r\n\tat org.apache.spark.api.python.PythonRDD$$anon$2$$anonfun$run$1.apply(PythonRDD.scala:700)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310)\r\n\tat org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:701)\r\n16/12/15 22:49:04 INFO org.spark_project.jetty.server.ServerConnector: Stopped ServerConnector@54284c76{HTTP/1.1}{0.0.0.0:4040}\r\n\r\nHowever, the scripts works totally fine when running on small dataset locally. My memory should be fine. I got roughly 200G memory each machine (both executors and driver) for my cluster. My data is only 16G. Anyone know the possible issue here ? I appreciate your help in advance.\r\n\r\n@thunterdb \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/47", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/47/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/47/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/47/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/47", "id": 189650597, "node_id": "MDU6SXNzdWUxODk2NTA1OTc=", "number": 47, "title": "Question about further support", "user": {"login": "praveend", "id": 194470, "node_id": "MDQ6VXNlcjE5NDQ3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/194470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/praveend", "html_url": "https://github.com/praveend", "followers_url": "https://api.github.com/users/praveend/followers", "following_url": "https://api.github.com/users/praveend/following{/other_user}", "gists_url": "https://api.github.com/users/praveend/gists{/gist_id}", "starred_url": "https://api.github.com/users/praveend/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/praveend/subscriptions", "organizations_url": "https://api.github.com/users/praveend/orgs", "repos_url": "https://api.github.com/users/praveend/repos", "events_url": "https://api.github.com/users/praveend/events{/privacy}", "received_events_url": "https://api.github.com/users/praveend/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-11-16T09:56:53Z", "updated_at": "2018-12-08T12:18:52Z", "closed_at": "2018-12-08T12:18:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI am looking for road map information on this project. In specific I am looking for, if there is any work going on/plans for support of using the model generated from scikit in Spark pipelineModel and also any plans on support of other ML Algorithms like Decision Tress, K-means etc.\r\n\r\nCould anyone shed some light on this.\r\n\r\nThanks\r\nPraveen", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/46", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/46/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/46/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/46/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/46", "id": 187403759, "node_id": "MDU6SXNzdWUxODc0MDM3NTk=", "number": 46, "title": "Namespace issue with pyspark.ml and pyspark.mllib", "user": {"login": "ovlaere", "id": 2300860, "node_id": "MDQ6VXNlcjIzMDA4NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/2300860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ovlaere", "html_url": "https://github.com/ovlaere", "followers_url": "https://api.github.com/users/ovlaere/followers", "following_url": "https://api.github.com/users/ovlaere/following{/other_user}", "gists_url": "https://api.github.com/users/ovlaere/gists{/gist_id}", "starred_url": "https://api.github.com/users/ovlaere/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ovlaere/subscriptions", "organizations_url": "https://api.github.com/users/ovlaere/orgs", "repos_url": "https://api.github.com/users/ovlaere/repos", "events_url": "https://api.github.com/users/ovlaere/events{/privacy}", "received_events_url": "https://api.github.com/users/ovlaere/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-11-04T18:16:21Z", "updated_at": "2018-12-08T12:24:07Z", "closed_at": "2018-12-08T12:24:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to run the default example on the README page\r\n```\r\nfrom sklearn import svm, grid_search, datasets\r\nfrom spark_sklearn import GridSearchCV\r\niris = datasets.load_iris()\r\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\r\nsvr = svm.SVC()\r\nclf = GridSearchCV(sc, svr, parameters)\r\nclf.fit(iris.data, iris.target)\r\n```\r\non Spark, but got the following error:\r\n```\r\nImportError: No module named linalg\r\n```\r\nThe code that causes this is the import of `pyspark.ml.linalg` [on this line in converter.py in spark_sklearn](https://github.com/databricks/spark-sklearn/blob/master/python/spark_sklearn/converter.py#L16)\r\n\r\nWe are running Spark 1.6, and according to [the documentation](http://spark.apache.org/docs/1.6.0/api/python/index.html), in 1.6 and above, `linalg` is under `pyspark.mllib.linalg` instead of `pyspark.ml.linalg`.\r\n\r\nI'm trying to figure out if it's an issue with my versions or what else exactly, given that the README mentions Spark 2.0 compatibility, but if this indeed an issue with spark_sklearn, it looks like this should be broken then since at least 1.6.0? Can someone confirm?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/45", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/45/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/45/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/45/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/45", "id": 184668342, "node_id": "MDU6SXNzdWUxODQ2NjgzNDI=", "number": 45, "title": "gridsearch with StratifiedShuffleSplit error", "user": {"login": "ginberg", "id": 7089667, "node_id": "MDQ6VXNlcjcwODk2Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7089667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ginberg", "html_url": "https://github.com/ginberg", "followers_url": "https://api.github.com/users/ginberg/followers", "following_url": "https://api.github.com/users/ginberg/following{/other_user}", "gists_url": "https://api.github.com/users/ginberg/gists{/gist_id}", "starred_url": "https://api.github.com/users/ginberg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ginberg/subscriptions", "organizations_url": "https://api.github.com/users/ginberg/orgs", "repos_url": "https://api.github.com/users/ginberg/repos", "events_url": "https://api.github.com/users/ginberg/events{/privacy}", "received_events_url": "https://api.github.com/users/ginberg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-10-23T03:03:20Z", "updated_at": "2019-01-28T17:01:21Z", "closed_at": "2019-01-28T17:01:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am getting this error when running the code below\n'StratifiedShuffleSplit' object is not iterable\n\nfrom sklearn import svm, grid_search, datasets\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom spark_sklearn import GridSearchCV\niris = datasets.load_iris()\n\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n\nsvr = svm.SVC()\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.5)\nclf = GridSearchCV(sc, svr, parameters, cv=sss)\nclf.fit(iris.data, iris.target)\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/39", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/39/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/39/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/39/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/39", "id": 162576651, "node_id": "MDU6SXNzdWUxNjI1NzY2NTE=", "number": 39, "title": "unskip spark_sklearn.tests.test_keyed_models:KeyedModelTests.test_surprise_key", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-06-28T00:19:22Z", "updated_at": "2016-08-16T22:07:31Z", "closed_at": "2016-08-16T22:07:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "test_surprise_key (spark_sklearn.tests.test_keyed_models.KeyedModelTests) ... SKIP: python vector nulls are unsupported for now, see SPARK-16175\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/38", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/38/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/38/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/38/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/38", "id": 162576494, "node_id": "MDU6SXNzdWUxNjI1NzY0OTQ=", "number": 38, "title": "Grid search tests generate warnings", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-06-28T00:18:01Z", "updated_at": "2018-12-08T12:21:29Z", "closed_at": "2018-12-08T12:21:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "These warnings need to be fixed or muted with justification:\n\n[warning 1](https://gist.github.com/vlad17/140d736b71355f04aad6eb0c7b3f7686)\n\n[warning 2](https://gist.github.com/vlad17/512d322f990f13c6fbd93b2c1810ecee)\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/37", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/37/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/37/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/37/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/37", "id": 162576290, "node_id": "MDU6SXNzdWUxNjI1NzYyOTA=", "number": 37, "title": "GridSearchCV documentation generation warnings", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 255692414, "node_id": "MDU6TGFiZWwyNTU2OTI0MTQ=", "url": "https://api.github.com/repos/databricks/spark-sklearn/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": true, "assignee": {"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/databricks/spark-sklearn/milestones/1", "html_url": "https://github.com/databricks/spark-sklearn/milestone/1", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/milestones/1/labels", "id": 3882332, "node_id": "MDk6TWlsZXN0b25lMzg4MjMzMg==", "number": 1, "title": "0.3.0", "description": "", "creator": {"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 11, "state": "closed", "created_at": "2018-12-08T15:31:49Z", "updated_at": "2019-01-30T19:23:14Z", "due_on": null, "closed_at": "2019-01-29T20:36:29Z"}, "comments": 2, "created_at": "2016-06-28T00:15:57Z", "updated_at": "2018-12-09T21:00:23Z", "closed_at": "2018-12-09T21:00:22Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Here's the warnings from a generation run, which need to be fixed: https://gist.github.com/vlad17/5ed3d53c28474e67ccafc5e4d48a245f\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/36", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/36/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/36/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/36/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/36", "id": 162576046, "node_id": "MDU6SXNzdWUxNjI1NzYwNDY=", "number": 36, "title": "_call_java in util.py is unused", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-06-28T00:13:57Z", "updated_at": "2018-12-08T12:23:34Z", "closed_at": "2018-12-08T12:23:33Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Should be removed\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/35", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/35/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/35/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/35/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/35", "id": 162575309, "node_id": "MDU6SXNzdWUxNjI1NzUzMDk=", "number": 35, "title": "Lint script", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-06-28T00:06:34Z", "updated_at": "2018-12-08T12:17:51Z", "closed_at": "2018-12-08T12:17:51Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Repo requires a lint script for style checks, which should be active in the travis CI script.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/34", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/34/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/34/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/34/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/34", "id": 162575265, "node_id": "MDU6SXNzdWUxNjI1NzUyNjU=", "number": 34, "title": "2.0.0 TODOs", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-06-28T00:06:05Z", "updated_at": "2016-08-16T22:08:04Z", "closed_at": "2016-08-16T22:08:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Various TODOs are left around the code regarding CI scripts and a 2.0.0 spark dependency, which need to be resolved once the release is available.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/32", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/32/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/32/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/32/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/32", "id": 162574801, "node_id": "MDU6SXNzdWUxNjI1NzQ4MDE=", "number": 32, "title": "Link pyspark docs in generated docs", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-06-28T00:01:38Z", "updated_at": "2018-12-08T20:01:33Z", "closed_at": "2018-12-08T20:01:33Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Need to configure intersphinx\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/31", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/31/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/31/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/31/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/31", "id": 162574502, "node_id": "MDU6SXNzdWUxNjI1NzQ1MDI=", "number": 31, "title": "unskip spark_sklearn.tests.test_gapply:GapplyTests.test_gapply_empty_schema", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-06-27T23:59:10Z", "updated_at": "2016-08-16T22:07:03Z", "closed_at": "2016-08-16T22:07:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "test_gapply_empty_schema (spark_sklearn.tests.test_gapply.GapplyTests) ... SKIP: Generating an empty dataframe after exploding fails, see SPARK-16179\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/30", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/30/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/30/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/30/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/30", "id": 162573852, "node_id": "MDU6SXNzdWUxNjI1NzM4NTI=", "number": 30, "title": "Native linalg libs not getting picked up", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-06-27T23:53:32Z", "updated_at": "2017-02-23T21:49:40Z", "closed_at": "2017-02-23T21:49:40Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When I run unit tests, the following is printed to stderr as they run:\n\n```\ntest_LinearRegression_spark2skl (spark_sklearn.converter_test.ConverterTests) ... 16/06/27 16:49:19 WARN WeightedLeastSquares: regParam is zero, which might cause numerical instability and overfitting.\n16/06/27 16:49:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n16/06/27 16:49:19 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n16/06/27 16:49:19 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n16/06/27 16:49:19 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n```\n\nAnd yet I have the appropriate packages:\n\n```\nvlad@vlad-databricks:~$ dpkg -s libgfortran3 libgfortran-4.9-dev libgfortran-5-dev liblapack-dev libblas-dev >/dev/null ; echo $?\n0\n```\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/24", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/24/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/24/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/24/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/24", "id": 160939071, "node_id": "MDU6SXNzdWUxNjA5MzkwNzE=", "number": 24, "title": "CSR Matrix Support for Spark 2.0", "user": {"login": "vlad17", "id": 5834964, "node_id": "MDQ6VXNlcjU4MzQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5834964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vlad17", "html_url": "https://github.com/vlad17", "followers_url": "https://api.github.com/users/vlad17/followers", "following_url": "https://api.github.com/users/vlad17/following{/other_user}", "gists_url": "https://api.github.com/users/vlad17/gists{/gist_id}", "starred_url": "https://api.github.com/users/vlad17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vlad17/subscriptions", "organizations_url": "https://api.github.com/users/vlad17/orgs", "repos_url": "https://api.github.com/users/vlad17/repos", "events_url": "https://api.github.com/users/vlad17/events{/privacy}", "received_events_url": "https://api.github.com/users/vlad17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-06-17T17:38:59Z", "updated_at": "2018-12-09T20:58:43Z", "closed_at": "2018-12-09T20:58:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "There's no support for CSR matrices in Spark 2.0.\n\nTo replicate, run `python/run-tests.sh --nologcapture spark_sklearn.converter_test`\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/20", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/20/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/20/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/20/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/20", "id": 148208339, "node_id": "MDU6SXNzdWUxNDgyMDgzMzk=", "number": 20, "title": "error message: sbt.ResolveException: unresolved dependency: org.apache.spark#spark-mllib_2.10;1.6.0-SNAPSHOT: not found", "user": {"login": "jxiablox", "id": 15805366, "node_id": "MDQ6VXNlcjE1ODA1MzY2", "avatar_url": "https://avatars1.githubusercontent.com/u/15805366?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jxiablox", "html_url": "https://github.com/jxiablox", "followers_url": "https://api.github.com/users/jxiablox/followers", "following_url": "https://api.github.com/users/jxiablox/following{/other_user}", "gists_url": "https://api.github.com/users/jxiablox/gists{/gist_id}", "starred_url": "https://api.github.com/users/jxiablox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jxiablox/subscriptions", "organizations_url": "https://api.github.com/users/jxiablox/orgs", "repos_url": "https://api.github.com/users/jxiablox/repos", "events_url": "https://api.github.com/users/jxiablox/events{/privacy}", "received_events_url": "https://api.github.com/users/jxiablox/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-04-13T23:12:18Z", "updated_at": "2018-09-23T05:44:46Z", "closed_at": "2016-04-13T23:27:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I clone the repo, and install the following packages, \n1. pip install spark-sklearn\n2. pip install nose\n\nalso I go to the folder spark-sklearn/python run, \n1. python setup.py build\n2. python setup.py install\n3. ./run run-tests.sh\n\nEverything looks fine. \n\nHowever, when I go to spark-sklearn/ folder and run sbt package, it gave me an error, Please help me what's wrong on my environment. \n\nThanks,\nJianhong\n\n[info] Loading project definition from /jianhong/GitHub/spark-sklearn/spark-sklearn/project\n[info] Set current project to spark-sklearn (in build file:/jianhong/GitHub/spark-sklearn/spark-sklearn/)\n[info] Updating {file:/jianhong/GitHub/spark-sklearn/spark-sklearn/}spark-sklearn...\n[info] Resolving org.apache.spark#spark-mllib_2.10;1.6.0-SNAPSHOT ...\n[warn]  module not found: org.apache.spark#spark-mllib_2.10;1.6.0-SNAPSHOT\n[warn] ==== local: tried\n[warn]   /home/jxia/.ivy2/local/org.apache.spark/spark-mllib_2.10/1.6.0-SNAPSHOT/ivys/ivy.xml\n[warn] ==== public: tried\n[warn]   https://repo1.maven.org/maven2/org/apache/spark/spark-mllib_2.10/1.6.0-SNAPSHOT/spark-mllib_2.10-1.6.0-SNAPSHOT.pom\n[warn] ==== Spark Packages Repo: tried\n[warn]   https://dl.bintray.com/spark-packages/maven/org/apache/spark/spark-mllib_2.10/1.6.0-SNAPSHOT/spark-mllib_2.10-1.6.0-SNAPSHOT.pom\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: org.apache.spark#spark-mllib_2.10;1.6.0-SNAPSHOT: not found\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]\n[warn]  Note: Unresolved dependencies path:\n[warn]          org.apache.spark:spark-mllib_2.10:1.6.0-SNAPSHOT ((sbtsparkpackage.SparkPackagePlugin) SparkPackagePlugin.scala#L241)\n[warn]            +- default:spark-sklearn_2.10:0.0.1-SNAPSHOT\nsbt.ResolveException: unresolved dependency: org.apache.spark#spark-mllib_2.10;1.6.0-SNAPSHOT: not found\n        at sbt.IvyActions$.sbt$IvyActions$$resolve(IvyActions.scala:243)\n        at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:158)\n        at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:156)\n        at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:147)\n        at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:147)\n        at sbt.IvySbt$$anonfun$withIvy$1.apply(Ivy.scala:124)\n        at sbt.IvySbt.sbt$IvySbt$$action$1(Ivy.scala:56)\n        at sbt.IvySbt$$anon$3.call(Ivy.scala:64)\n        at xsbt.boot.Locks$GlobalLock.withChannel$1(Locks.scala:93)\n        at xsbt.boot.Locks$GlobalLock.xsbt$boot$Locks$GlobalLock$$withChannelRetries$1(Locks.scala:78)\n        at xsbt.boot.Locks$GlobalLock$$anonfun$withFileLock$1.apply(Locks.scala:97)\n        at xsbt.boot.Using$.withResource(Using.scala:10)\n        at xsbt.boot.Using$.apply(Using.scala:9)\n        at xsbt.boot.Locks$GlobalLock.ignoringDeadlockAvoided(Locks.scala:58)\n        at xsbt.boot.Locks$GlobalLock.withLock(Locks.scala:48)\n        at xsbt.boot.Locks$.apply0(Locks.scala:31)\n        at xsbt.boot.Locks$.apply(Locks.scala:28)\n        at sbt.IvySbt.withDefaultLogger(Ivy.scala:64)\n        at sbt.IvySbt.withIvy(Ivy.scala:119)\n        at sbt.IvySbt.withIvy(Ivy.scala:116)\n        at sbt.IvySbt$Module.withModule(Ivy.scala:147)\n        at sbt.IvyActions$.updateEither(IvyActions.scala:156)\n        at sbt.Classpaths$$anonfun$sbt$Classpaths$$work$1$1.apply(Defaults.scala:1282)\n        at sbt.Classpaths$$anonfun$sbt$Classpaths$$work$1$1.apply(Defaults.scala:1279)\n        at sbt.Classpaths$$anonfun$doWork$1$1$$anonfun$84.apply(Defaults.scala:1309)\n        at sbt.Classpaths$$anonfun$doWork$1$1$$anonfun$84.apply(Defaults.scala:1307)\n        at sbt.Tracked$$anonfun$lastOutput$1.apply(Tracked.scala:35)\n        at sbt.Classpaths$$anonfun$doWork$1$1.apply(Defaults.scala:1312)\n        at sbt.Classpaths$$anonfun$doWork$1$1.apply(Defaults.scala:1306)\n        at sbt.Tracked$$anonfun$inputChanged$1.apply(Tracked.scala:45)\n        at sbt.Classpaths$.cachedUpdate(Defaults.scala:1324)\n        at sbt.Classpaths$$anonfun$updateTask$1.apply(Defaults.scala:1264)\n        at sbt.Classpaths$$anonfun$updateTask$1.apply(Defaults.scala:1242)\n        at scala.Function1$$anonfun$compose$1.apply(Function1.scala:47)\n        at sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:40)\n        at sbt.std.Transform$$anon$4.work(System.scala:63)\n        at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)\n        at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:226)\n        at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:17)\n        at sbt.Execute.work(Execute.scala:235)\n        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)\n        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:226)\n        at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:159)\n        at sbt.CompletionService$$anon$2.call(CompletionService.scala:28)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[error](*:update) sbt.ResolveException: unresolved dependency: org.apache.spark#spark-mllib_2.10;1.6.0-SNAPSHOT: not found\n[error] Total time: 1 s, completed Apr 13, 2016 4:11:28 PM\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/16", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/16/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/16/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/16/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/16", "id": 136420428, "node_id": "MDU6SXNzdWUxMzY0MjA0Mjg=", "number": 16, "title": "ImportError: No module named 'converter'", "user": {"login": "rhiever", "id": 1719223, "node_id": "MDQ6VXNlcjE3MTkyMjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1719223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rhiever", "html_url": "https://github.com/rhiever", "followers_url": "https://api.github.com/users/rhiever/followers", "following_url": "https://api.github.com/users/rhiever/following{/other_user}", "gists_url": "https://api.github.com/users/rhiever/gists{/gist_id}", "starred_url": "https://api.github.com/users/rhiever/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rhiever/subscriptions", "organizations_url": "https://api.github.com/users/rhiever/orgs", "repos_url": "https://api.github.com/users/rhiever/repos", "events_url": "https://api.github.com/users/rhiever/events{/privacy}", "received_events_url": "https://api.github.com/users/rhiever/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 255692416, "node_id": "MDU6TGFiZWwyNTU2OTI0MTY=", "url": "https://api.github.com/repos/databricks/spark-sklearn/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-02-25T15:28:05Z", "updated_at": "2016-07-20T22:39:43Z", "closed_at": "2016-03-17T20:17:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\n\nI am trying to use the example code from the README for spark-sklearn. I have installed the latest version of Anaconda, and ran `pip install spark_sklearn` to install the package. The following code:\n\n``` python\nfrom sklearn import svm, grid_search, datasets\nfrom spark_sklearn import GridSearchCV\niris = datasets.load_iris()\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvr = svm.SVC()\nclf = GridSearchCV(sc, svr, parameters)\nclf.fit(iris.data, iris.target)\n```\n\nresults in the following traceback:\n\n```\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n<ipython-input-1-d38860ad054d> in <module>()\n      1 from sklearn import svm, grid_search, datasets\n----> 2 from spark_sklearn import GridSearchCV\n      3 iris = datasets.load_iris()\n      4 parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n      5 svr = svm.SVC()\n\n/Users/randal_olson/anaconda/lib/python3.5/site-packages/spark_sklearn-0.1.1-py3.5.egg/spark_sklearn/__init__.py in <module>()\n      1 from scipy.sparse import csr_matrix\n      2 \n----> 3 from converter import Converter\n      4 from grid_search import GridSearchCV\n      5 from udt import CSRVectorUDT\n\nImportError: No module named 'converter'\n```\n\nI have tried to install the latest version of spark-sklearn from this repository to no avail. Please advise me how to proceed.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/15", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/15/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/15/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/15/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/15", "id": 134844602, "node_id": "MDU6SXNzdWUxMzQ4NDQ2MDI=", "number": 15, "title": "Randomized Search", "user": {"login": "sixers", "id": 1457102, "node_id": "MDQ6VXNlcjE0NTcxMDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/1457102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sixers", "html_url": "https://github.com/sixers", "followers_url": "https://api.github.com/users/sixers/followers", "following_url": "https://api.github.com/users/sixers/following{/other_user}", "gists_url": "https://api.github.com/users/sixers/gists{/gist_id}", "starred_url": "https://api.github.com/users/sixers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sixers/subscriptions", "organizations_url": "https://api.github.com/users/sixers/orgs", "repos_url": "https://api.github.com/users/sixers/repos", "events_url": "https://api.github.com/users/sixers/events{/privacy}", "received_events_url": "https://api.github.com/users/sixers/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 255692416, "node_id": "MDU6TGFiZWwyNTU2OTI0MTY=", "url": "https://api.github.com/repos/databricks/spark-sklearn/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/databricks/spark-sklearn/milestones/1", "html_url": "https://github.com/databricks/spark-sklearn/milestone/1", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/milestones/1/labels", "id": 3882332, "node_id": "MDk6TWlsZXN0b25lMzg4MjMzMg==", "number": 1, "title": "0.3.0", "description": "", "creator": {"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 11, "state": "closed", "created_at": "2018-12-08T15:31:49Z", "updated_at": "2019-01-30T19:23:14Z", "due_on": null, "closed_at": "2019-01-29T20:36:29Z"}, "comments": 6, "created_at": "2016-02-19T11:39:15Z", "updated_at": "2019-01-28T17:01:42Z", "closed_at": "2019-01-28T17:01:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey!\n\nWould it be possible to implement a distributed version of http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html ?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/spark-sklearn/issues/12", "repository_url": "https://api.github.com/repos/databricks/spark-sklearn", "labels_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/12/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/12/comments", "events_url": "https://api.github.com/repos/databricks/spark-sklearn/issues/12/events", "html_url": "https://github.com/databricks/spark-sklearn/issues/12", "id": 132688240, "node_id": "MDU6SXNzdWUxMzI2ODgyNDA=", "number": 12, "title": "Training large number of models", "user": {"login": "danielnee", "id": 1815441, "node_id": "MDQ6VXNlcjE4MTU0NDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1815441?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielnee", "html_url": "https://github.com/danielnee", "followers_url": "https://api.github.com/users/danielnee/followers", "following_url": "https://api.github.com/users/danielnee/following{/other_user}", "gists_url": "https://api.github.com/users/danielnee/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielnee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielnee/subscriptions", "organizations_url": "https://api.github.com/users/danielnee/orgs", "repos_url": "https://api.github.com/users/danielnee/repos", "events_url": "https://api.github.com/users/danielnee/events{/privacy}", "received_events_url": "https://api.github.com/users/danielnee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-02-10T12:35:06Z", "updated_at": "2016-03-17T19:02:28Z", "closed_at": "2016-03-17T19:02:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\n\nThis looks fantastic and looks to almost solve one of my problems.\n\nEssentially I have a model to predict the amount of sales of a particular product for each day given various features (historical sales volumes, type of store, day of week, time of year, etc.). The training set fits easily in memory,\n\nI want to train this model for 1000's of different products (we need a model per product)\n\nThe features and model used are the same. The exact values of the features will change per product.\n\nIn pseudo code this looks like:\n\n```\nfor (product in products) { # We can parallelise this loop on a single machine, but could we parallelise over spark?\n\n   # Extract train dataset from hive over ODBC\n   # Train model\n   # Output predictions\n}\n```\n\nAny thoughts if spark-sklearn could be used to support this?\n", "performed_via_github_app": null, "score": 1.0}]}