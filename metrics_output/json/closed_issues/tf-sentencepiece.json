{"total_count": 307, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/google/sentencepiece/issues/525", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/525/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/525/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/525/events", "html_url": "https://github.com/google/sentencepiece/issues/525", "id": 667747596, "node_id": "MDU6SXNzdWU2Njc3NDc1OTY=", "number": 525, "title": "Oh, i think i have figure out this problem.", "user": {"login": "Yunlongs", "id": 32975935, "node_id": "MDQ6VXNlcjMyOTc1OTM1", "avatar_url": "https://avatars3.githubusercontent.com/u/32975935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yunlongs", "html_url": "https://github.com/Yunlongs", "followers_url": "https://api.github.com/users/Yunlongs/followers", "following_url": "https://api.github.com/users/Yunlongs/following{/other_user}", "gists_url": "https://api.github.com/users/Yunlongs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yunlongs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yunlongs/subscriptions", "organizations_url": "https://api.github.com/users/Yunlongs/orgs", "repos_url": "https://api.github.com/users/Yunlongs/repos", "events_url": "https://api.github.com/users/Yunlongs/events{/privacy}", "received_events_url": "https://api.github.com/users/Yunlongs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-29T10:52:21Z", "updated_at": "2020-07-29T10:52:37Z", "closed_at": "2020-07-29T10:52:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Oh, i think i have figure out this problem.\r\nWe can split the training courpus each line a word, and enable `--add_dummy_prefix=false` options.\r\n\r\nThen we will get the only one form about words in the vocabulary.\r\n\r\n_Originally posted by @Yunlongs in https://github.com/google/sentencepiece/issues/524#issuecomment-665514365_", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/521", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/521/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/521/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/521/events", "html_url": "https://github.com/google/sentencepiece/issues/521", "id": 663560003, "node_id": "MDU6SXNzdWU2NjM1NjAwMDM=", "number": 521, "title": "How to SampleEncode with a specific random state?", "user": {"login": "CZWin32768", "id": 12969670, "node_id": "MDQ6VXNlcjEyOTY5Njcw", "avatar_url": "https://avatars3.githubusercontent.com/u/12969670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CZWin32768", "html_url": "https://github.com/CZWin32768", "followers_url": "https://api.github.com/users/CZWin32768/followers", "following_url": "https://api.github.com/users/CZWin32768/following{/other_user}", "gists_url": "https://api.github.com/users/CZWin32768/gists{/gist_id}", "starred_url": "https://api.github.com/users/CZWin32768/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CZWin32768/subscriptions", "organizations_url": "https://api.github.com/users/CZWin32768/orgs", "repos_url": "https://api.github.com/users/CZWin32768/repos", "events_url": "https://api.github.com/users/CZWin32768/events{/privacy}", "received_events_url": "https://api.github.com/users/CZWin32768/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-22T08:02:24Z", "updated_at": "2020-07-23T02:21:33Z", "closed_at": "2020-07-23T02:21:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Taku,\r\n\r\nI am currently trying to incorporate subword regularization into my data generation pipeline. \r\n\r\nDue to the limited time and storage, I want to apply subword regularization online. I am wondering whether the spm model always generates the same sequences in different runs, because I want to compare different models with the same input sequences that are generated by spm.\r\n\r\nIt seems that providing a random state to each encode operation can ensure that the generated sequences only depends on the random state.\r\n\r\nCould you please provide me some suggestions on how to achieve this?\r\n\r\nThanks,\r\nZewen", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/520", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/520/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/520/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/520/events", "html_url": "https://github.com/google/sentencepiece/issues/520", "id": 663465869, "node_id": "MDU6SXNzdWU2NjM0NjU4Njk=", "number": 520, "title": "Is there any way to perform non-destructive tokenization?", "user": {"login": "Ryou0634", "id": 17979572, "node_id": "MDQ6VXNlcjE3OTc5NTcy", "avatar_url": "https://avatars3.githubusercontent.com/u/17979572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ryou0634", "html_url": "https://github.com/Ryou0634", "followers_url": "https://api.github.com/users/Ryou0634/followers", "following_url": "https://api.github.com/users/Ryou0634/following{/other_user}", "gists_url": "https://api.github.com/users/Ryou0634/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ryou0634/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ryou0634/subscriptions", "organizations_url": "https://api.github.com/users/Ryou0634/orgs", "repos_url": "https://api.github.com/users/Ryou0634/repos", "events_url": "https://api.github.com/users/Ryou0634/events{/privacy}", "received_events_url": "https://api.github.com/users/Ryou0634/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-22T04:33:41Z", "updated_at": "2020-07-23T02:33:42Z", "closed_at": "2020-07-23T02:28:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using the python wrapper of sentencepiece and want to preserve all the special characters such as newline characters, consecutive white spaces. However, the tokenizer automatically perform normalization for them and I cannot find an option to disable it.\r\n\r\n**Additional Context**\r\nI want to use sentencepiece for the task of question answering (https://github.com/google-research-datasets/tydiqa). For each example in the dataset, the answer span is specified by the unicode offset from the beginning of the paragraph. The automatic normalization of the special characters makes it unable to recover the original position information of each token.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/519", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/519/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/519/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/519/events", "html_url": "https://github.com/google/sentencepiece/issues/519", "id": 653192038, "node_id": "MDU6SXNzdWU2NTMxOTIwMzg=", "number": 519, "title": "Training Creates Float Number Vocab", "user": {"login": "Yanmarka", "id": 8938141, "node_id": "MDQ6VXNlcjg5MzgxNDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/8938141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yanmarka", "html_url": "https://github.com/Yanmarka", "followers_url": "https://api.github.com/users/Yanmarka/followers", "following_url": "https://api.github.com/users/Yanmarka/following{/other_user}", "gists_url": "https://api.github.com/users/Yanmarka/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yanmarka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yanmarka/subscriptions", "organizations_url": "https://api.github.com/users/Yanmarka/orgs", "repos_url": "https://api.github.com/users/Yanmarka/repos", "events_url": "https://api.github.com/users/Yanmarka/events{/privacy}", "received_events_url": "https://api.github.com/users/Yanmarka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-08T10:48:04Z", "updated_at": "2020-07-09T04:11:50Z", "closed_at": "2020-07-09T04:11:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "When training on the data/botchan.txt fle, my vocab file looks like this:\r\n\r\n\\<unk>\t0\r\n\\<s>\t0\r\n\\</s>\t0\r\n,\t-3.24863\r\n.\t-3.38172\r\n\u2581the\t-3.40852\r\n\u2581I\t-3.69947\r\n\u2581to\t-3.91285\r\ns\t-3.92535\r\n\u2581a\t-4.04742\r\n\u2581and\t-4.11791\r\n\u2581of\t-4.12551\r\n\u2581\t-4.18102\r\ned\t-4.26836\r\ning\t-4.29098\r\n\u2581in\t-4.56303\r\n\u2581was\t-4.62928\r\n\u2581\"\t-4.65978\r\nt\t-4.75102\r\n\u2581it\t-4.80293\r\ne\t-4.87948\r\n\u2581be\t-4.9757\r\n\\-\t-5.01115\r\n\u2581that\t-5.01613\r\n\r\nand so on.\r\nCommand run: spm_train --input=botchan.txt --model_prefix=test --vocab_size=2000 --character_coverage=1.0 --model_type=unigram\r\nThis happens with other input files as well.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/518", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/518/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/518/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/518/events", "html_url": "https://github.com/google/sentencepiece/issues/518", "id": 653166767, "node_id": "MDU6SXNzdWU2NTMxNjY3Njc=", "number": 518, "title": "Allow regex for user-defined symbols", "user": {"login": "villmow", "id": 2743060, "node_id": "MDQ6VXNlcjI3NDMwNjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2743060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/villmow", "html_url": "https://github.com/villmow", "followers_url": "https://api.github.com/users/villmow/followers", "following_url": "https://api.github.com/users/villmow/following{/other_user}", "gists_url": "https://api.github.com/users/villmow/gists{/gist_id}", "starred_url": "https://api.github.com/users/villmow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/villmow/subscriptions", "organizations_url": "https://api.github.com/users/villmow/orgs", "repos_url": "https://api.github.com/users/villmow/repos", "events_url": "https://api.github.com/users/villmow/events{/privacy}", "received_events_url": "https://api.github.com/users/villmow/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-08T10:07:21Z", "updated_at": "2020-07-09T09:44:15Z", "closed_at": "2020-07-09T04:21:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nthanks for this nice library. Is it possible to provide regular expressions for tokens that should remain single tokens? For example: \r\n`\"<\\w>\"`\r\n\r\nThis is supported by subword-nmt (called glossaries), but I didn't find something like this in sentencepiece.\r\n\r\nThanks for your help", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/511", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/511/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/511/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/511/events", "html_url": "https://github.com/google/sentencepiece/issues/511", "id": 644446297, "node_id": "MDU6SXNzdWU2NDQ0NDYyOTc=", "number": 511, "title": "SentenceIterator and trainer_spec.input() must be exclusive.", "user": {"login": "Srj", "id": 44947896, "node_id": "MDQ6VXNlcjQ0OTQ3ODk2", "avatar_url": "https://avatars1.githubusercontent.com/u/44947896?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Srj", "html_url": "https://github.com/Srj", "followers_url": "https://api.github.com/users/Srj/followers", "following_url": "https://api.github.com/users/Srj/following{/other_user}", "gists_url": "https://api.github.com/users/Srj/gists{/gist_id}", "starred_url": "https://api.github.com/users/Srj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Srj/subscriptions", "organizations_url": "https://api.github.com/users/Srj/orgs", "repos_url": "https://api.github.com/users/Srj/repos", "events_url": "https://api.github.com/users/Srj/events{/privacy}", "received_events_url": "https://api.github.com/users/Srj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-24T09:02:45Z", "updated_at": "2020-06-25T10:08:02Z", "closed_at": "2020-06-25T10:08:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\nimport sentencepiece as spm\r\nparams = ('--input=./bn.txt ','--model_prefix=spm','--vocab_size=30000')\r\nspm.SentencePieceTrainer.Train(params)\r\n```\r\nthe size of bn.txt is 10.8 G and i am trying to train it on colab. It throws following error.\r\n`\r\nRuntimeError: Internal: /sentencepiece/src/trainer_interface.cc(311) [(sentence_iterator_ != nullptr && trainer_spec_.input().empty()) || (sentence_iterator_ == nullptr && !trainer_spec_.input().empty())] SentenceIterator and trainer_spec.input() must be exclusive.`\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/510", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/510/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/510/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/510/events", "html_url": "https://github.com/google/sentencepiece/issues/510", "id": 642965042, "node_id": "MDU6SXNzdWU2NDI5NjUwNDI=", "number": 510, "title": "unk_surface always", "user": {"login": "codekali", "id": 32861240, "node_id": "MDQ6VXNlcjMyODYxMjQw", "avatar_url": "https://avatars2.githubusercontent.com/u/32861240?v=4", "gravatar_id": "", "url": "https://api.github.com/users/codekali", "html_url": "https://github.com/codekali", "followers_url": "https://api.github.com/users/codekali/followers", "following_url": "https://api.github.com/users/codekali/following{/other_user}", "gists_url": "https://api.github.com/users/codekali/gists{/gist_id}", "starred_url": "https://api.github.com/users/codekali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/codekali/subscriptions", "organizations_url": "https://api.github.com/users/codekali/orgs", "repos_url": "https://api.github.com/users/codekali/repos", "events_url": "https://api.github.com/users/codekali/events{/privacy}", "received_events_url": "https://api.github.com/users/codekali/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-22T10:50:19Z", "updated_at": "2020-06-22T16:12:58Z", "closed_at": "2020-06-22T16:12:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nI'm tuning a T5-transformer model for my source code summarization. The ```T5Tokenizer``` provided by ```huggingface``` is good but sometimes it produces an irrelevant result. I'm trying to make my own tokenizer using sentencepiece. But an issue happens whenever I use my custom spiece model, when I decode the output generated by the transformer model, it always sticks a unk_surface ```\u2047```at the beginning, the rest of the string is ok ok.\r\n\r\n```\r\ncustom  = T5Tokenizer.from_pretrained('spiece.model')\r\ninput = custom.encode(\"move the cursor down and return its position\", return_tensors=\"pt\")\r\nout = model.generate(input)\r\ncustom.generate(out[0])\r\n```\r\n\r\nThis code produces:\r\n```\r\n' \u2047  the cursor down the cursor down the cursor down the cursor down the cursor down the cursor down the'\r\n```\r\n\r\nI've created the sentencepiece model using this code snippet:\r\n```\r\nimport sentencepiece as spm\r\n\r\nparams = (\"--input=vocab.txt --model_prefix=spiece --vocab_size=35000 --hard_vocab_limit=false\")\r\n\r\nspm.SentencePieceTrainer.Train(params)\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/508", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/508/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/508/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/508/events", "html_url": "https://github.com/google/sentencepiece/issues/508", "id": 642223398, "node_id": "MDU6SXNzdWU2NDIyMjMzOTg=", "number": 508, "title": "spm_encode an entire file using Python API", "user": {"login": "jerrybai1995", "id": 11433746, "node_id": "MDQ6VXNlcjExNDMzNzQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/11433746?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jerrybai1995", "html_url": "https://github.com/jerrybai1995", "followers_url": "https://api.github.com/users/jerrybai1995/followers", "following_url": "https://api.github.com/users/jerrybai1995/following{/other_user}", "gists_url": "https://api.github.com/users/jerrybai1995/gists{/gist_id}", "starred_url": "https://api.github.com/users/jerrybai1995/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jerrybai1995/subscriptions", "organizations_url": "https://api.github.com/users/jerrybai1995/orgs", "repos_url": "https://api.github.com/users/jerrybai1995/repos", "events_url": "https://api.github.com/users/jerrybai1995/events{/privacy}", "received_events_url": "https://api.github.com/users/jerrybai1995/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-19T21:20:34Z", "updated_at": "2020-06-22T16:09:13Z", "closed_at": "2020-06-22T16:09:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI'm looking for a way to replicate the exact same effect of \r\n```\r\nspm_encode --model=<model_file> --output_format=piece < input > output\r\n```\r\n(which encodes the entire `input` file to produce `output` file) in the Python wrapper. However, `sp.encode` seems to support only string input only. Is there anyway that I can do this without using a for loop to go through every line in `input`?\r\n\r\nThanks for the help!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/507", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/507/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/507/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/507/events", "html_url": "https://github.com/google/sentencepiece/issues/507", "id": 636174202, "node_id": "MDU6SXNzdWU2MzYxNzQyMDI=", "number": 507, "title": "y", "user": {"login": "drless", "id": 63640913, "node_id": "MDQ6VXNlcjYzNjQwOTEz", "avatar_url": "https://avatars0.githubusercontent.com/u/63640913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drless", "html_url": "https://github.com/drless", "followers_url": "https://api.github.com/users/drless/followers", "following_url": "https://api.github.com/users/drless/following{/other_user}", "gists_url": "https://api.github.com/users/drless/gists{/gist_id}", "starred_url": "https://api.github.com/users/drless/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drless/subscriptions", "organizations_url": "https://api.github.com/users/drless/orgs", "repos_url": "https://api.github.com/users/drless/repos", "events_url": "https://api.github.com/users/drless/events{/privacy}", "received_events_url": "https://api.github.com/users/drless/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-10T11:36:31Z", "updated_at": "2020-06-16T00:44:23Z", "closed_at": "2020-06-16T00:44:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/506", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/506/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/506/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/506/events", "html_url": "https://github.com/google/sentencepiece/issues/506", "id": 635217544, "node_id": "MDU6SXNzdWU2MzUyMTc1NDQ=", "number": 506, "title": "Question about Lattice::PopulateMarginal() in unigram_model.cc", "user": {"login": "hymzoque", "id": 34158908, "node_id": "MDQ6VXNlcjM0MTU4OTA4", "avatar_url": "https://avatars0.githubusercontent.com/u/34158908?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hymzoque", "html_url": "https://github.com/hymzoque", "followers_url": "https://api.github.com/users/hymzoque/followers", "following_url": "https://api.github.com/users/hymzoque/following{/other_user}", "gists_url": "https://api.github.com/users/hymzoque/gists{/gist_id}", "starred_url": "https://api.github.com/users/hymzoque/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hymzoque/subscriptions", "organizations_url": "https://api.github.com/users/hymzoque/orgs", "repos_url": "https://api.github.com/users/hymzoque/repos", "events_url": "https://api.github.com/users/hymzoque/events{/privacy}", "received_events_url": "https://api.github.com/users/hymzoque/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-09T08:24:42Z", "updated_at": "2020-06-10T05:01:15Z", "closed_at": "2020-06-10T05:01:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "```c++\r\nalpha[rnode->node_id] = LogSumExp(alpha[rnode->node_id],\r\n                                  lnode->score + alpha[lnode->node_id],\r\n                                  lnode == end_nodes_[pos][0]);\r\n```\r\nbecause the alpha[i] is initialized by 0.0, the first time updating of alpha[i] will makes alpha[i] = log(e^0 + ...)\r\ni'm not sure whether the e^0 here is unexpected", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/503", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/503/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/503/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/503/events", "html_url": "https://github.com/google/sentencepiece/issues/503", "id": 632104092, "node_id": "MDU6SXNzdWU2MzIxMDQwOTI=", "number": 503, "title": "Support custom tokens, symbols", "user": {"login": "zhangguanheng66", "id": 6156351, "node_id": "MDQ6VXNlcjYxNTYzNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6156351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangguanheng66", "html_url": "https://github.com/zhangguanheng66", "followers_url": "https://api.github.com/users/zhangguanheng66/followers", "following_url": "https://api.github.com/users/zhangguanheng66/following{/other_user}", "gists_url": "https://api.github.com/users/zhangguanheng66/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangguanheng66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangguanheng66/subscriptions", "organizations_url": "https://api.github.com/users/zhangguanheng66/orgs", "repos_url": "https://api.github.com/users/zhangguanheng66/repos", "events_url": "https://api.github.com/users/zhangguanheng66/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangguanheng66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-06T00:57:01Z", "updated_at": "2020-06-06T06:37:41Z", "closed_at": "2020-06-06T06:37:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "For the vocab in sentencepiece, is there a way to insert some custom tokens and symbols (for example `<pad>`, `<sep`>) in the vocab? And to make spm work for some pretrained models, we want to use the old vocab as well.\r\n\r\nFor our usage case, we use spm to convert strings into a list of ids as a single step.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/502", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/502/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/502/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/502/events", "html_url": "https://github.com/google/sentencepiece/issues/502", "id": 630350655, "node_id": "MDU6SXNzdWU2MzAzNTA2NTU=", "number": 502, "title": "Line length limit when using Python module", "user": {"login": "johntmyers", "id": 9696606, "node_id": "MDQ6VXNlcjk2OTY2MDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9696606?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johntmyers", "html_url": "https://github.com/johntmyers", "followers_url": "https://api.github.com/users/johntmyers/followers", "following_url": "https://api.github.com/users/johntmyers/following{/other_user}", "gists_url": "https://api.github.com/users/johntmyers/gists{/gist_id}", "starred_url": "https://api.github.com/users/johntmyers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johntmyers/subscriptions", "organizations_url": "https://api.github.com/users/johntmyers/orgs", "repos_url": "https://api.github.com/users/johntmyers/repos", "events_url": "https://api.github.com/users/johntmyers/events{/privacy}", "received_events_url": "https://api.github.com/users/johntmyers/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-03T21:39:18Z", "updated_at": "2020-06-05T15:51:14Z", "closed_at": "2020-06-05T15:51:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using the Python module, when a line is too long, this error is logged out and the process aborts:\r\n\r\n`RuntimeError: Internal: /Users/travis/build/google/sentencepiece/src/trainer_interface.cc(336) [!sentences_.empty()]`\r\n\r\nWhen I adjust the max length using `max_sentence_length` things work. Why do we receive the above error instead of the other warnings?\r\n\r\nI've attached sample input data.\r\n\r\nThis is running on v 0.1.91\r\n\r\n[wide.csv.zip](https://github.com/google/sentencepiece/files/4726336/wide.csv.zip)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/501", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/501/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/501/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/501/events", "html_url": "https://github.com/google/sentencepiece/issues/501", "id": 627800767, "node_id": "MDU6SXNzdWU2Mjc4MDA3Njc=", "number": 501, "title": "Unknown Field Name Error", "user": {"login": "Uncertain-Quark", "id": 39159599, "node_id": "MDQ6VXNlcjM5MTU5NTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/39159599?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Uncertain-Quark", "html_url": "https://github.com/Uncertain-Quark", "followers_url": "https://api.github.com/users/Uncertain-Quark/followers", "following_url": "https://api.github.com/users/Uncertain-Quark/following{/other_user}", "gists_url": "https://api.github.com/users/Uncertain-Quark/gists{/gist_id}", "starred_url": "https://api.github.com/users/Uncertain-Quark/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Uncertain-Quark/subscriptions", "organizations_url": "https://api.github.com/users/Uncertain-Quark/orgs", "repos_url": "https://api.github.com/users/Uncertain-Quark/repos", "events_url": "https://api.github.com/users/Uncertain-Quark/events{/privacy}", "received_events_url": "https://api.github.com/users/Uncertain-Quark/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-30T19:20:06Z", "updated_at": "2020-06-08T09:04:14Z", "closed_at": "2020-06-08T09:04:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/500", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/500/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/500/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/500/events", "html_url": "https://github.com/google/sentencepiece/issues/500", "id": 627267738, "node_id": "MDU6SXNzdWU2MjcyNjc3Mzg=", "number": 500, "title": "TypeError: __init__() got an unexpected keyword argument 'model_file'", "user": {"login": "rossbrown9879", "id": 61983534, "node_id": "MDQ6VXNlcjYxOTgzNTM0", "avatar_url": "https://avatars0.githubusercontent.com/u/61983534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rossbrown9879", "html_url": "https://github.com/rossbrown9879", "followers_url": "https://api.github.com/users/rossbrown9879/followers", "following_url": "https://api.github.com/users/rossbrown9879/following{/other_user}", "gists_url": "https://api.github.com/users/rossbrown9879/gists{/gist_id}", "starred_url": "https://api.github.com/users/rossbrown9879/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rossbrown9879/subscriptions", "organizations_url": "https://api.github.com/users/rossbrown9879/orgs", "repos_url": "https://api.github.com/users/rossbrown9879/repos", "events_url": "https://api.github.com/users/rossbrown9879/events{/privacy}", "received_events_url": "https://api.github.com/users/rossbrown9879/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-29T13:26:34Z", "updated_at": "2020-05-29T16:04:33Z", "closed_at": "2020-05-29T16:04:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I run following script, I'm getting this TypeError !\r\n\r\n```\r\nimport sentencepiece as spm\r\nsp = spm.SentencePieceProcessor(model_file='test/test_model.model')\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-6fe7b084c0bf> in <module>\r\n      1 import sentencepiece as spm\r\n----> 2 sp = spm.SentencePieceProcessor(model_file='test/test_model.model')\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'model_file'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/499", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/499/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/499/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/499/events", "html_url": "https://github.com/google/sentencepiece/issues/499", "id": 627085216, "node_id": "MDU6SXNzdWU2MjcwODUyMTY=", "number": 499, "title": "Sentencepiece input file format", "user": {"login": "working12", "id": 12879472, "node_id": "MDQ6VXNlcjEyODc5NDcy", "avatar_url": "https://avatars3.githubusercontent.com/u/12879472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/working12", "html_url": "https://github.com/working12", "followers_url": "https://api.github.com/users/working12/followers", "following_url": "https://api.github.com/users/working12/following{/other_user}", "gists_url": "https://api.github.com/users/working12/gists{/gist_id}", "starred_url": "https://api.github.com/users/working12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/working12/subscriptions", "organizations_url": "https://api.github.com/users/working12/orgs", "repos_url": "https://api.github.com/users/working12/repos", "events_url": "https://api.github.com/users/working12/events{/privacy}", "received_events_url": "https://api.github.com/users/working12/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-29T08:11:25Z", "updated_at": "2020-05-29T16:09:10Z", "closed_at": "2020-05-29T16:09:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am training sentencepiece model from a text file. The file consists of line lines separated by `\\n`. And it contains multiple logical sentences. Now if the portion separated by `\\n` is lengthy, then will it be a problem? How to check if my input file is good enough for training or whether the training was successful or not?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/498", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/498/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/498/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/498/events", "html_url": "https://github.com/google/sentencepiece/issues/498", "id": 625879041, "node_id": "MDU6SXNzdWU2MjU4NzkwNDE=", "number": 498, "title": "How to make several characters as a whole 'character' (not token or piece) ?", "user": {"login": "liuyaox", "id": 7260977, "node_id": "MDQ6VXNlcjcyNjA5Nzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7260977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liuyaox", "html_url": "https://github.com/liuyaox", "followers_url": "https://api.github.com/users/liuyaox/followers", "following_url": "https://api.github.com/users/liuyaox/following{/other_user}", "gists_url": "https://api.github.com/users/liuyaox/gists{/gist_id}", "starred_url": "https://api.github.com/users/liuyaox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liuyaox/subscriptions", "organizations_url": "https://api.github.com/users/liuyaox/orgs", "repos_url": "https://api.github.com/users/liuyaox/repos", "events_url": "https://api.github.com/users/liuyaox/events{/privacy}", "received_events_url": "https://api.github.com/users/liuyaox/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-27T17:18:06Z", "updated_at": "2020-05-28T03:15:29Z", "closed_at": "2020-05-28T03:15:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there any way that I can make several characters as a whole 'character' (not token or piece in user-defined symbols).  \r\n\r\nI mean, the several characters can always be dealt with as a whole, and they can be either a token or a part of other token, but they will never be split, just like a character. For example, \r\n\r\nThe several characters is \"it's\"\r\n\r\nInput text is \"it's okay and...\"\r\n\r\nThe result is \"\u2581it ' s\u2581okay \u2581and...\"  (Note that there are whitespaces after \"it\" and before \"s\")\r\n\r\nExpected result is that \"it's\" is not split into 3 parts and \"it's\" can be a token or a part of a token, for example, \"\u2581it's\u2581okay \u2581and...\"(there is only one whitespace after \"okay\") or \"\u2581it's \u2581okay \u2581and...\"(there are 2 whitespaces after \"it's\" and after \"okay\")\r\n\r\nThanks !", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/497", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/497/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/497/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/497/events", "html_url": "https://github.com/google/sentencepiece/issues/497", "id": 625363908, "node_id": "MDU6SXNzdWU2MjUzNjM5MDg=", "number": 497, "title": "[Minor Bug] CLI flagging nbest_size = -1", "user": {"login": "mingruimingrui", "id": 18568364, "node_id": "MDQ6VXNlcjE4NTY4MzY0", "avatar_url": "https://avatars1.githubusercontent.com/u/18568364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingruimingrui", "html_url": "https://github.com/mingruimingrui", "followers_url": "https://api.github.com/users/mingruimingrui/followers", "following_url": "https://api.github.com/users/mingruimingrui/following{/other_user}", "gists_url": "https://api.github.com/users/mingruimingrui/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingruimingrui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingruimingrui/subscriptions", "organizations_url": "https://api.github.com/users/mingruimingrui/orgs", "repos_url": "https://api.github.com/users/mingruimingrui/repos", "events_url": "https://api.github.com/users/mingruimingrui/events{/privacy}", "received_events_url": "https://api.github.com/users/mingruimingrui/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-27T04:19:24Z", "updated_at": "2020-05-28T15:47:17Z", "closed_at": "2020-05-28T15:47:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "`spm_encode --nbest_size -1` is invalid.\r\n\r\n`spm_encode --nbest_size=-1` is valid.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/495", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/495/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/495/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/495/events", "html_url": "https://github.com/google/sentencepiece/issues/495", "id": 624492771, "node_id": "MDU6SXNzdWU2MjQ0OTI3NzE=", "number": 495, "title": "TypeError: SentencePieceTrainer_train() takes no keyword arguments", "user": {"login": "cifkao", "id": 8046580, "node_id": "MDQ6VXNlcjgwNDY1ODA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8046580?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cifkao", "html_url": "https://github.com/cifkao", "followers_url": "https://api.github.com/users/cifkao/followers", "following_url": "https://api.github.com/users/cifkao/following{/other_user}", "gists_url": "https://api.github.com/users/cifkao/gists{/gist_id}", "starred_url": "https://api.github.com/users/cifkao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cifkao/subscriptions", "organizations_url": "https://api.github.com/users/cifkao/orgs", "repos_url": "https://api.github.com/users/cifkao/repos", "events_url": "https://api.github.com/users/cifkao/events{/privacy}", "received_events_url": "https://api.github.com/users/cifkao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-25T21:32:25Z", "updated_at": "2020-05-27T07:50:56Z", "closed_at": "2020-05-26T00:42:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "I installed the Python package with `pip`. When I try to call `SentencePieceTrainer.train()`, I get the following error: `TypeError: SentencePieceTrainer_train() takes no keyword arguments`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/493", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/493/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/493/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/493/events", "html_url": "https://github.com/google/sentencepiece/issues/493", "id": 623457396, "node_id": "MDU6SXNzdWU2MjM0NTczOTY=", "number": 493, "title": "Force tokenizer to ignore special characters", "user": {"login": "zaidalyafeai", "id": 15667714, "node_id": "MDQ6VXNlcjE1NjY3NzE0", "avatar_url": "https://avatars2.githubusercontent.com/u/15667714?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zaidalyafeai", "html_url": "https://github.com/zaidalyafeai", "followers_url": "https://api.github.com/users/zaidalyafeai/followers", "following_url": "https://api.github.com/users/zaidalyafeai/following{/other_user}", "gists_url": "https://api.github.com/users/zaidalyafeai/gists{/gist_id}", "starred_url": "https://api.github.com/users/zaidalyafeai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zaidalyafeai/subscriptions", "organizations_url": "https://api.github.com/users/zaidalyafeai/orgs", "repos_url": "https://api.github.com/users/zaidalyafeai/repos", "events_url": "https://api.github.com/users/zaidalyafeai/events{/privacy}", "received_events_url": "https://api.github.com/users/zaidalyafeai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-22T20:19:15Z", "updated_at": "2020-05-27T07:29:25Z", "closed_at": "2020-05-23T15:36:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want to train `sentencepiece` but the text has some special characters that should be considered as part of words i.e the tokenizer shouldn't add space/prerpocess them. \r\n\r\nFor instance, if I want to tokenize the statement \r\n`stmt = I +am +good`\r\nThe plus sign shouldn't be considered as a punctuation i.e the output might be \r\n`tokenized = [\"I\", \"+am\", \"+good\"] `\r\n\r\nAny idea how to do that ? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/491", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/491/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/491/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/491/events", "html_url": "https://github.com/google/sentencepiece/issues/491", "id": 622544892, "node_id": "MDU6SXNzdWU2MjI1NDQ4OTI=", "number": 491, "title": "What does `--accept_language` actually do?", "user": {"login": "mingruimingrui", "id": 18568364, "node_id": "MDQ6VXNlcjE4NTY4MzY0", "avatar_url": "https://avatars1.githubusercontent.com/u/18568364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingruimingrui", "html_url": "https://github.com/mingruimingrui", "followers_url": "https://api.github.com/users/mingruimingrui/followers", "following_url": "https://api.github.com/users/mingruimingrui/following{/other_user}", "gists_url": "https://api.github.com/users/mingruimingrui/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingruimingrui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingruimingrui/subscriptions", "organizations_url": "https://api.github.com/users/mingruimingrui/orgs", "repos_url": "https://api.github.com/users/mingruimingrui/repos", "events_url": "https://api.github.com/users/mingruimingrui/events{/privacy}", "received_events_url": "https://api.github.com/users/mingruimingrui/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-21T14:39:48Z", "updated_at": "2020-05-22T00:54:21Z", "closed_at": "2020-05-22T00:54:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I've been wondering what this variable does.\r\nFrom what I could tell, the effects are mostly decorative.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/490", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/490/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/490/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/490/events", "html_url": "https://github.com/google/sentencepiece/issues/490", "id": 622325274, "node_id": "MDU6SXNzdWU2MjIzMjUyNzQ=", "number": 490, "title": "I want to use sentencepiece command (spm_encode) in google colaboratory", "user": {"login": "pdc-kaminaga", "id": 9833365, "node_id": "MDQ6VXNlcjk4MzMzNjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/9833365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pdc-kaminaga", "html_url": "https://github.com/pdc-kaminaga", "followers_url": "https://api.github.com/users/pdc-kaminaga/followers", "following_url": "https://api.github.com/users/pdc-kaminaga/following{/other_user}", "gists_url": "https://api.github.com/users/pdc-kaminaga/gists{/gist_id}", "starred_url": "https://api.github.com/users/pdc-kaminaga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pdc-kaminaga/subscriptions", "organizations_url": "https://api.github.com/users/pdc-kaminaga/orgs", "repos_url": "https://api.github.com/users/pdc-kaminaga/repos", "events_url": "https://api.github.com/users/pdc-kaminaga/events{/privacy}", "received_events_url": "https://api.github.com/users/pdc-kaminaga/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-21T08:02:49Z", "updated_at": "2020-05-21T09:40:00Z", "closed_at": "2020-05-21T09:39:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI want to use sentencepiece command (spm_encode) in google colaboratory.\r\nI ran make and got 'CMake Error: cmake_symlink_library: System Error: Operation not supported'.\r\n!make -j $(nproc)\r\n[  8%] Built target sentencepiece_train-static\r\n[  9%] Linking CXX shared library libsentencepiece.so\r\n[ 44%] Built target sentencepiece-static\r\nCMake Error: cmake_symlink_library: System Error: Operation not supported\r\nCMake Error: cmake_symlink_library: System Error: Operation not supported\r\nsrc/CMakeFiles/sentencepiece.dir/build.make:638: recipe for target 'src/libsentencepiece.so.0.0.0' failed\r\nmake[2]: *** [src/libsentencepiece.so.0.0.0] Error 1\r\nmake[2]: *** Deleting file 'src/libsentencepiece.so.0.0.0'\r\nCMakeFiles/Makefile2:106: recipe for target 'src/CMakeFiles/sentencepiece.dir/all' failed\r\nmake[1]: *** [src/CMakeFiles/sentencepiece.dir/all] Error 2\r\nMakefile:151: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\nHow do I install sentencepiece in google colaboratory ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/489", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/489/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/489/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/489/events", "html_url": "https://github.com/google/sentencepiece/issues/489", "id": 621762359, "node_id": "MDU6SXNzdWU2MjE3NjIzNTk=", "number": 489, "title": "Working on folders (Training)", "user": {"login": "mainakmanna", "id": 28564860, "node_id": "MDQ6VXNlcjI4NTY0ODYw", "avatar_url": "https://avatars0.githubusercontent.com/u/28564860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mainakmanna", "html_url": "https://github.com/mainakmanna", "followers_url": "https://api.github.com/users/mainakmanna/followers", "following_url": "https://api.github.com/users/mainakmanna/following{/other_user}", "gists_url": "https://api.github.com/users/mainakmanna/gists{/gist_id}", "starred_url": "https://api.github.com/users/mainakmanna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mainakmanna/subscriptions", "organizations_url": "https://api.github.com/users/mainakmanna/orgs", "repos_url": "https://api.github.com/users/mainakmanna/repos", "events_url": "https://api.github.com/users/mainakmanna/events{/privacy}", "received_events_url": "https://api.github.com/users/mainakmanna/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-20T13:27:16Z", "updated_at": "2020-05-20T15:41:22Z", "closed_at": "2020-05-20T15:41:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "From the documentation, it is clear that the way to train over multiple files is to pass comma separated file names to the `input` (In case of python API) parameter. Is there any way we can process just from the folders instead or folder of folders? \r\n\r\n```\r\n+----------+ f1\r\n|\r\n+-----+f2\r\n|\r\n+--> A.txt\r\n|\r\n+--> B.txt\r\n|\r\n+-----+f3\r\n|\r\n+--> C.txt\r\n```\r\n\r\nFor this kind of situation we have to pass `input=./f1/f2/A.txt,./f1/f2/B.txt,./f1/f3/C.txt`. More specifically \r\n```\r\nimport sentencepiece as spm\r\nspm.SentencePieceTrainer.Train('--input=./f1/f2/A.txt,./f1/f2/B.txt,./f1/f3/C.txt --model_prefix=m --vocab_size=64000')\r\n```\r\n1. Is there any better way of handling this?\r\n2. Is there any plan to include this as a feature later?\r\n ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/488", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/488/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/488/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/488/events", "html_url": "https://github.com/google/sentencepiece/issues/488", "id": 620877434, "node_id": "MDU6SXNzdWU2MjA4Nzc0MzQ=", "number": 488, "title": "How to remove \"_\" from the vocab terms of the trained model?", "user": {"login": "rossbrown9879", "id": 61983534, "node_id": "MDQ6VXNlcjYxOTgzNTM0", "avatar_url": "https://avatars0.githubusercontent.com/u/61983534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rossbrown9879", "html_url": "https://github.com/rossbrown9879", "followers_url": "https://api.github.com/users/rossbrown9879/followers", "following_url": "https://api.github.com/users/rossbrown9879/following{/other_user}", "gists_url": "https://api.github.com/users/rossbrown9879/gists{/gist_id}", "starred_url": "https://api.github.com/users/rossbrown9879/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rossbrown9879/subscriptions", "organizations_url": "https://api.github.com/users/rossbrown9879/orgs", "repos_url": "https://api.github.com/users/rossbrown9879/repos", "events_url": "https://api.github.com/users/rossbrown9879/events{/privacy}", "received_events_url": "https://api.github.com/users/rossbrown9879/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-19T10:51:37Z", "updated_at": "2020-05-20T15:54:41Z", "closed_at": "2020-05-19T11:56:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "I do not want \"\\_\" in the vocabulary during training. I'm training the model just by providing the tsv file of tokens as input. How to get rid of \"\\_\"?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/487", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/487/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/487/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/487/events", "html_url": "https://github.com/google/sentencepiece/issues/487", "id": 620665746, "node_id": "MDU6SXNzdWU2MjA2NjU3NDY=", "number": 487, "title": "[sentencepiece-0.1.90] libicu related build error", "user": {"login": "mingruimingrui", "id": 18568364, "node_id": "MDQ6VXNlcjE4NTY4MzY0", "avatar_url": "https://avatars1.githubusercontent.com/u/18568364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingruimingrui", "html_url": "https://github.com/mingruimingrui", "followers_url": "https://api.github.com/users/mingruimingrui/followers", "following_url": "https://api.github.com/users/mingruimingrui/following{/other_user}", "gists_url": "https://api.github.com/users/mingruimingrui/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingruimingrui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingruimingrui/subscriptions", "organizations_url": "https://api.github.com/users/mingruimingrui/orgs", "repos_url": "https://api.github.com/users/mingruimingrui/repos", "events_url": "https://api.github.com/users/mingruimingrui/events{/privacy}", "received_events_url": "https://api.github.com/users/mingruimingrui/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-19T04:21:52Z", "updated_at": "2020-05-20T09:29:45Z", "closed_at": "2020-05-20T09:29:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When building locally on my mac, I'm getting this error.\r\n\r\n```shell\r\nScanning dependencies of target sentencepiece_train-static\r\n[ 70%] Building CXX object src/CMakeFiles/sentencepiece_train-static.dir/builder.cc.o\r\n/Users/mingrui.wang/Downloads/sentencepiece-0.1.90/src/builder.cc:62:3: error: use of undeclared identifier\r\n      'u_strFromUTF8Lenient_66'\r\n  u_strFromUTF8Lenient(utf16, ustr.getCapacity(), &utf16_length, utf8.data(),\r\n  ^\r\n/usr/local/opt/icu4c/include/unicode/urename.h:360:30: note: expanded from macro 'u_strFromUTF8Lenient'\r\n#define u_strFromUTF8Lenient U_ICU_ENTRY_POINT_RENAME(u_strFromUTF8Lenient)\r\n                             ^\r\n/usr/local/opt/icu4c/include/unicode/uvernum.h:130:47: note: expanded from macro 'U_ICU_ENTRY_POINT_RENAME'\r\n#       define U_ICU_ENTRY_POINT_RENAME(x)    U_DEF2_ICU_ENTRY_POINT_RENAME(x,U_ICU_VERSION_SUFFIX)\r\n                                              ^\r\n/usr/local/opt/icu4c/include/unicode/uvernum.h:129:51: note: expanded from macro\r\n      'U_DEF2_ICU_ENTRY_POINT_RENAME'\r\n#       define U_DEF2_ICU_ENTRY_POINT_RENAME(x,y) U_DEF_ICU_ENTRY_POINT_RENAME(x,y)\r\n                                                  ^\r\n/usr/local/opt/icu4c/include/unicode/uvernum.h:128:50: note: expanded from macro 'U_DEF_ICU_ENTRY_POINT_RENAME'\r\n#       define U_DEF_ICU_ENTRY_POINT_RENAME(x,y) x ## y\r\n                                                 ^\r\n<scratch space>:230:1: note: expanded from here\r\nu_strFromUTF8Lenient_66\r\n^\r\n1 error generated.\r\nmake[3]: *** [src/CMakeFiles/sentencepiece_train-static.dir/builder.cc.o] Error 1\r\nmake[2]: *** [src/CMakeFiles/sentencepiece_train-static.dir/all] Error 2\r\nmake[1]: *** [all] Error 2\r\nmake: *** [build] Error 2\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/485", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/485/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/485/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/485/events", "html_url": "https://github.com/google/sentencepiece/issues/485", "id": 612012584, "node_id": "MDU6SXNzdWU2MTIwMTI1ODQ=", "number": 485, "title": "How should I change the value of a SentencePieceProcessor?", "user": {"login": "gyunggyung", "id": 26733242, "node_id": "MDQ6VXNlcjI2NzMzMjQy", "avatar_url": "https://avatars0.githubusercontent.com/u/26733242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gyunggyung", "html_url": "https://github.com/gyunggyung", "followers_url": "https://api.github.com/users/gyunggyung/followers", "following_url": "https://api.github.com/users/gyunggyung/following{/other_user}", "gists_url": "https://api.github.com/users/gyunggyung/gists{/gist_id}", "starred_url": "https://api.github.com/users/gyunggyung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gyunggyung/subscriptions", "organizations_url": "https://api.github.com/users/gyunggyung/orgs", "repos_url": "https://api.github.com/users/gyunggyung/repos", "events_url": "https://api.github.com/users/gyunggyung/events{/privacy}", "received_events_url": "https://api.github.com/users/gyunggyung/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-04T16:39:04Z", "updated_at": "2020-05-16T04:03:32Z", "closed_at": "2020-05-16T03:24:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I want to change the value. \r\n\r\nLoad a dictionary.\r\n```\r\nimport sentencepiece as spm\r\n\r\nsp = spm.SentencePieceProcessor()\r\nsp.Load(tok_path)\r\n```\r\n\r\nChange from dictionary to desired.\r\n```\r\ntest = {}\r\nfor id in range(sp.get_piece_size()):\r\n    test[sp.id_to_piece(id)] = id\r\n\r\ntest[\"\\n\"] = test[\"<unused0>\"]\r\ndel test[\"<unused0>\"]\r\ntest[\"<|endoftext|>\"] = test[\"<unused1>\"]\r\ndel test[\"<unused1>\"]\r\n```\r\n\r\nBut I don't know what to do next. I'd like to set it up, but I don't know how. Help me.\r\n\r\n![image](https://user-images.githubusercontent.com/26733242/80990160-ea809c80-8e70-11ea-9b63-caa95bb4aff1.png)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/484", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/484/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/484/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/484/events", "html_url": "https://github.com/google/sentencepiece/issues/484", "id": 611733551, "node_id": "MDU6SXNzdWU2MTE3MzM1NTE=", "number": 484, "title": "Getting error: libc++abi.dylib: terminating with uncaught exception of type std::length_error: vector ", "user": {"login": "danyaljj", "id": 2441454, "node_id": "MDQ6VXNlcjI0NDE0NTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2441454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danyaljj", "html_url": "https://github.com/danyaljj", "followers_url": "https://api.github.com/users/danyaljj/followers", "following_url": "https://api.github.com/users/danyaljj/following{/other_user}", "gists_url": "https://api.github.com/users/danyaljj/gists{/gist_id}", "starred_url": "https://api.github.com/users/danyaljj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danyaljj/subscriptions", "organizations_url": "https://api.github.com/users/danyaljj/orgs", "repos_url": "https://api.github.com/users/danyaljj/repos", "events_url": "https://api.github.com/users/danyaljj/events{/privacy}", "received_events_url": "https://api.github.com/users/danyaljj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-04T09:33:00Z", "updated_at": "2020-05-08T02:49:57Z", "closed_at": "2020-05-08T02:49:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am running the standard training code: \r\n\r\n```\r\nimport sentencepiece as spm\r\n\r\nSPM_COMMAND = ('--input={} --model_prefix={} '\r\n               '--vocab_size={} --input_sentence_size={} '\r\n               '--shuffle_input_sentence=true '\r\n               '--bos_id=-1 --eos_id=-1 --hard_vocab_limit=false').format(\r\n    PRC_DATA_FPATH, MODEL_PREFIX,\r\n    VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\r\nspm.SentencePieceTrainer.Train(SPM_COMMAND)\r\n```\r\n\r\nHere is the output log: \r\n```\r\nsentencepiece_trainer.cc(116) LOG(INFO) Running command: --input=../text_files/all_text_merged_cleaned.txt --model_prefix=tokenizer --vocab_size=31744 --input_sentence_size=12800000 --shuffle_input_sentence=true --bos_id=-1 --eos_id=-1 --hard_vocab_limit=false\r\nsentencepiece_trainer.cc(49) LOG(INFO) Starts training with : \r\nTrainerSpec {\r\n  input: ../text_files/all_text_merged_cleaned.txt\r\n  input_format: \r\n  model_prefix: tokenizer\r\n  model_type: UNIGRAM\r\n  vocab_size: 31744\r\n  self_test_sample_size: 0\r\n  character_coverage: 0.9995\r\n  input_sentence_size: 12800000\r\n  shuffle_input_sentence: 1\r\n  seed_sentencepiece_size: 1000000\r\n  shrinking_factor: 0.75\r\n  max_sentence_length: 4192\r\n  num_threads: 16\r\n  num_sub_iterations: 2\r\n  max_sentencepiece_length: 16\r\n  split_by_unicode_script: 1\r\n  split_by_number: 1\r\n  split_by_whitespace: 1\r\n  treat_whitespace_as_suffix: 0\r\n  hard_vocab_limit: 0\r\n  use_all_vocab: 0\r\n  unk_id: 0\r\n  bos_id: -1\r\n  eos_id: -1\r\n  pad_id: -1\r\n  unk_piece: <unk>\r\n  bos_piece: <s>\r\n  eos_piece: </s>\r\n  pad_piece: <pad>\r\n  unk_surface:  \u2047 \r\n}\r\nNormalizerSpec {\r\n  name: nmt_nfkc\r\n  add_dummy_prefix: 1\r\n  remove_extra_whitespaces: 1\r\n  escape_whitespaces: 1\r\n  normalization_rule_tsv: \r\n}\r\n\r\ntrainer_interface.cc(267) LOG(INFO) Loading corpus: ../text_files/all_text_merged_cleaned.txt\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 1000000 lines\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 2000000 lines\r\ntrainer_interface.cc(287) LOG(WARNING) Found too long line (10604 > 4192).\r\ntrainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\r\ntrainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 3000000 lines\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 4000000 lines\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 5000000 lines\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 6000000 lines\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 7000000 lines\r\ntrainer_interface.cc(139) LOG(INFO) Loaded 8000000 lines\r\ntrainer_interface.cc(114) LOG(WARNING) Too many sentences are loaded! (8731020), which may slow down training.\r\ntrainer_interface.cc(116) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\r\ntrainer_interface.cc(119) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\r\ntrainer_interface.cc(315) LOG(INFO) Loaded all 8731020 sentences\r\ntrainer_interface.cc(321) LOG(INFO) Skipped 4527068 too long sentences.\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\r\ntrainer_interface.cc(335) LOG(INFO) Normalizing sentences...\r\ntrainer_interface.cc(384) LOG(INFO) all chars count=6587984663\r\ntrainer_interface.cc(392) LOG(INFO) Done: 99.9503% characters are covered.\r\ntrainer_interface.cc(402) LOG(INFO) Alphabet size=153\r\ntrainer_interface.cc(403) LOG(INFO) Final character coverage=0.999503\r\ntrainer_interface.cc(435) LOG(INFO) Done! preprocessed 8731020 sentences.\r\n\r\n\r\n\r\nlibc++abi.dylib: terminating with uncaught exception of type std::length_error: vector\r\nAbort trap: 6\r\n```\r\n\r\nWondering if anyone else has seen this error before. \r\nCan it be because of long lines/paragraphs? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/483", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/483/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/483/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/483/events", "html_url": "https://github.com/google/sentencepiece/issues/483", "id": 610263095, "node_id": "MDU6SXNzdWU2MTAyNjMwOTU=", "number": 483, "title": "SentencePiece for sentence boundary detection?", "user": {"login": "mustaszewski", "id": 13680208, "node_id": "MDQ6VXNlcjEzNjgwMjA4", "avatar_url": "https://avatars1.githubusercontent.com/u/13680208?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mustaszewski", "html_url": "https://github.com/mustaszewski", "followers_url": "https://api.github.com/users/mustaszewski/followers", "following_url": "https://api.github.com/users/mustaszewski/following{/other_user}", "gists_url": "https://api.github.com/users/mustaszewski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mustaszewski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mustaszewski/subscriptions", "organizations_url": "https://api.github.com/users/mustaszewski/orgs", "repos_url": "https://api.github.com/users/mustaszewski/repos", "events_url": "https://api.github.com/users/mustaszewski/events{/privacy}", "received_events_url": "https://api.github.com/users/mustaszewski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-30T17:37:04Z", "updated_at": "2020-05-08T02:47:15Z", "closed_at": "2020-05-08T02:47:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nrather than posting an issue, I'd like to ask whether SentencePiece can be used for the task of sentence segmentation (i.e. splitting raw text input into sentences). For my task in mind, only the sentence boundary detection would be required. I am interested in SentencePiece for this task because it is language agnostic and trainable.\r\nThank you very much for you reply, best regards!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/482", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/482/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/482/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/482/events", "html_url": "https://github.com/google/sentencepiece/issues/482", "id": 607679141, "node_id": "MDU6SXNzdWU2MDc2NzkxNDE=", "number": 482, "title": "Microsoft Visual C++ 14.0 isn't detected in conda environments", "user": {"login": "JGCoelho", "id": 59511387, "node_id": "MDQ6VXNlcjU5NTExMzg3", "avatar_url": "https://avatars0.githubusercontent.com/u/59511387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JGCoelho", "html_url": "https://github.com/JGCoelho", "followers_url": "https://api.github.com/users/JGCoelho/followers", "following_url": "https://api.github.com/users/JGCoelho/following{/other_user}", "gists_url": "https://api.github.com/users/JGCoelho/gists{/gist_id}", "starred_url": "https://api.github.com/users/JGCoelho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JGCoelho/subscriptions", "organizations_url": "https://api.github.com/users/JGCoelho/orgs", "repos_url": "https://api.github.com/users/JGCoelho/repos", "events_url": "https://api.github.com/users/JGCoelho/events{/privacy}", "received_events_url": "https://api.github.com/users/JGCoelho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-27T16:21:16Z", "updated_at": "2020-04-28T04:10:06Z", "closed_at": "2020-04-28T04:10:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Ok, so i am trying to install transformers, and sentencepiece is a required package. I got transformers installed no problem on my default python 3.8, and was trying to install it on my python 3.5 conda environment. The issue is that when i run \"pip install transformers\" on the conda environment i get the following error message:\r\n\r\n```bash\r\nBuilding wheels for collected packages: sentencepiece\r\n  Building wheel for sentencepiece (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: 'c:\\users\\jgc\\anaconda3\\envs\\tensorflow\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\JGC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-bgwb_604\\\\sentencepiece\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\JGC\\\\AppData\\\\Local\\\\Temp\\\\pip-install-bgwb_604\\\\sentencepiece\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\JGC\\AppData\\Local\\Temp\\pip-wheel-bprwrdpd'\r\n       cwd: C:\\Users\\JGC\\AppData\\Local\\Temp\\pip-install-bgwb_604\\sentencepiece\\\r\n  Complete output (9 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build\\lib.win-amd64-3.5\r\n  copying sentencepiece.py -> build\\lib.win-amd64-3.5\r\n  running build_ext\r\n  building '_sentencepiece' extension\r\n  error: Microsoft Visual C++ 14.0 is required. Get it with \"Build Tools for Visual Studio\": https://visualstudio.microsoft.com/downloads\r\n```\r\n\r\nEnvironment: Python 3.5.6 conda env on Windows 10. I have gone to https://visualstudio.microsoft.com/downloads/ and installed the build tools for Visual Studio.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/480", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/480/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/480/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/480/events", "html_url": "https://github.com/google/sentencepiece/issues/480", "id": 606943773, "node_id": "MDU6SXNzdWU2MDY5NDM3NzM=", "number": 480, "title": "How to get the frequency of a subword ?", "user": {"login": "liuyaox", "id": 7260977, "node_id": "MDQ6VXNlcjcyNjA5Nzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7260977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liuyaox", "html_url": "https://github.com/liuyaox", "followers_url": "https://api.github.com/users/liuyaox/followers", "following_url": "https://api.github.com/users/liuyaox/following{/other_user}", "gists_url": "https://api.github.com/users/liuyaox/gists{/gist_id}", "starred_url": "https://api.github.com/users/liuyaox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liuyaox/subscriptions", "organizations_url": "https://api.github.com/users/liuyaox/orgs", "repos_url": "https://api.github.com/users/liuyaox/repos", "events_url": "https://api.github.com/users/liuyaox/events{/privacy}", "received_events_url": "https://api.github.com/users/liuyaox/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-26T07:27:18Z", "updated_at": "2020-04-26T07:58:13Z", "closed_at": "2020-04-26T07:58:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "We know that in BPE algorithm, the frequency of subwords is used to iteratively update the model. After training the model, how can I get the frequency of a subword ?\r\n\r\nI only found a function `sp.get_score(id)`, which obviously is not what I want.\r\n\r\nBTW, anyone who can tell me what `sp.get_score(id)` means ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/479", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/479/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/479/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/479/events", "html_url": "https://github.com/google/sentencepiece/issues/479", "id": 604764445, "node_id": "MDU6SXNzdWU2MDQ3NjQ0NDU=", "number": 479, "title": "No package for macOS for python 3.8", "user": {"login": "Kruszylo", "id": 27746090, "node_id": "MDQ6VXNlcjI3NzQ2MDkw", "avatar_url": "https://avatars2.githubusercontent.com/u/27746090?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kruszylo", "html_url": "https://github.com/Kruszylo", "followers_url": "https://api.github.com/users/Kruszylo/followers", "following_url": "https://api.github.com/users/Kruszylo/following{/other_user}", "gists_url": "https://api.github.com/users/Kruszylo/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kruszylo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kruszylo/subscriptions", "organizations_url": "https://api.github.com/users/Kruszylo/orgs", "repos_url": "https://api.github.com/users/Kruszylo/repos", "events_url": "https://api.github.com/users/Kruszylo/events{/privacy}", "received_events_url": "https://api.github.com/users/Kruszylo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-22T13:37:04Z", "updated_at": "2020-04-23T01:34:19Z", "closed_at": "2020-04-23T01:34:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "**MacOS: 10.14.6\r\nPython 3.8.1**\r\ncurrently there is no package for `python 3.8` for Mac. Workaround is: \r\n\r\n- download source code `tar.gz` ([v0.1.85](https://github.com/google/sentencepiece/releases/tag/v0.1.85))\r\n- unpackage and install:\r\n```\r\ncd /path/to/surce/code/\r\ntar xopf sentencepiece-0.1.85.tar\r\ncd sentencepiece-0.1.85/python/\r\npip install .\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/478", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/478/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/478/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/478/events", "html_url": "https://github.com/google/sentencepiece/issues/478", "id": 603307635, "node_id": "MDU6SXNzdWU2MDMzMDc2MzU=", "number": 478, "title": "Deprecation warning due to invalid escape sequences in Python 3.8", "user": {"login": "tirkarthi", "id": 3972343, "node_id": "MDQ6VXNlcjM5NzIzNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3972343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tirkarthi", "html_url": "https://github.com/tirkarthi", "followers_url": "https://api.github.com/users/tirkarthi/followers", "following_url": "https://api.github.com/users/tirkarthi/following{/other_user}", "gists_url": "https://api.github.com/users/tirkarthi/gists{/gist_id}", "starred_url": "https://api.github.com/users/tirkarthi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tirkarthi/subscriptions", "organizations_url": "https://api.github.com/users/tirkarthi/orgs", "repos_url": "https://api.github.com/users/tirkarthi/repos", "events_url": "https://api.github.com/users/tirkarthi/events{/privacy}", "received_events_url": "https://api.github.com/users/tirkarthi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-20T15:02:45Z", "updated_at": "2020-04-24T13:39:29Z", "closed_at": "2020-04-24T13:39:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Depreaction warning due to invalid escape sequences in Python 3.7 . https://github.com/asottile/pyupgrade/ can be used to resolve the warnings.\r\n\r\n```\r\n find . -iname '*py' | grep -Ev 'setup|rdf4|tool' | xargs -P4 -I{} python3.8 -Wall -m py_compile {}\r\n./tensorflow/tf_sentencepiece/sentencepiece_processor_ops.py:38: DeprecationWarning: invalid escape sequence \\.\r\n  re.search('so.([0-9]+\\.[0-9\\.]+.*)$', os.path.basename(n)).group(0)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/477", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/477/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/477/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/477/events", "html_url": "https://github.com/google/sentencepiece/issues/477", "id": 602516119, "node_id": "MDU6SXNzdWU2MDI1MTYxMTk=", "number": 477, "title": "Segfault upon import", "user": {"login": "adamjstewart", "id": 12021217, "node_id": "MDQ6VXNlcjEyMDIxMjE3", "avatar_url": "https://avatars2.githubusercontent.com/u/12021217?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamjstewart", "html_url": "https://github.com/adamjstewart", "followers_url": "https://api.github.com/users/adamjstewart/followers", "following_url": "https://api.github.com/users/adamjstewart/following{/other_user}", "gists_url": "https://api.github.com/users/adamjstewart/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamjstewart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamjstewart/subscriptions", "organizations_url": "https://api.github.com/users/adamjstewart/orgs", "repos_url": "https://api.github.com/users/adamjstewart/repos", "events_url": "https://api.github.com/users/adamjstewart/events{/privacy}", "received_events_url": "https://api.github.com/users/adamjstewart/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-18T18:00:56Z", "updated_at": "2020-05-03T02:57:46Z", "closed_at": "2020-04-24T14:08:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "I built `sentencepiece` from source using the [Spack](https://spack.io) package manager. When I try to import sentencepiece, I see the following issue:\r\n```console\r\n$ python -X faulthandler -m sentencepiece\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00002aaaac847d80 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 219 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 583 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 670 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 967 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 983 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006 in _gcd_import\r\n  File \"/mnt/b/projects/eot/bbcj/stewart1/spack/var/spack/environments/codeflection/.spack-env/view/lib/python3.7/importlib/__init__.py\", line 127 in import_module\r\n  File \"/mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/py-sentencepiece-0.1.85-nwyazuhjuwovyvq26vjkbtphroyfiylh/lib/python3.7/site-packages/sentencepiece.py\", line 20 in swig_import_helper\r\n  File \"/mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/py-sentencepiece-0.1.85-nwyazuhjuwovyvq26vjkbtphroyfiylh/lib/python3.7/site-packages/sentencepiece.py\", line 21 in <module>\r\n  File \"/mnt/b/projects/eot/bbcj/stewart1/spack/var/spack/environments/codeflection/.spack-env/view/lib/python3.7/runpy.py\", line 85 in _run_code\r\n  File \"/mnt/b/projects/eot/bbcj/stewart1/spack/var/spack/environments/codeflection/.spack-env/view/lib/python3.7/runpy.py\", line 193 in _run_module_as_main\r\nSegmentation fault\r\n```\r\nIf I run `ldd` on the shared object library, I don't see anything wrong:\r\n```console\r\n$ ldd -r _sentencepiece.cpython-37m-x86_64-linux-gnu.so \r\n\tlinux-vdso.so.1 =>  (0x00002aaaaaaab000)\r\n\tlibpython3.7m.so.1.0 => /mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/python-3.7.6-zqbxmao55n4zdmahvn4bernyw2e3ivpa/lib/libpython3.7m.so.1.0 (0x00002aaaaaad0000)\r\n\tlibsentencepiece.so.0 => /mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/sentencepiece-0.1.85-dfoe4enciqhx7gr3o32zfsc2budmtv7j/lib/libsentencepiece.so.0 (0x00002aaaaae32000)\r\n\tlibsentencepiece_train.so.0 => /mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/sentencepiece-0.1.85-dfoe4enciqhx7gr3o32zfsc2budmtv7j/lib/libsentencepiece_train.so.0 (0x00002aaaaaee1000)\r\n\tlibstdc++.so.6 => /opt/gcc/5.3.0/snos/lib64/libstdc++.so.6 (0x00002aaaab040000)\r\n\tlibm.so.6 => /lib64/libm.so.6 (0x00002aaaab468000)\r\n\tlibgcc_s.so.1 => /opt/gcc/5.3.0/snos/lib64/libgcc_s.so.1 (0x00002aaaab6e3000)\r\n\tlibc.so.6 => /lib64/libc.so.6 (0x00002aaaab8fb000)\r\n\tlibcrypt.so.1 => /lib64/libcrypt.so.1 (0x00002aaaabc78000)\r\n\tlibintl.so.8 => /mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/gettext-0.20.1-ahcqhleubgf7dqchj36gnkyyya2cdhxx/lib/libintl.so.8 (0x00002aaaabeb3000)\r\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00002aaaac0be000)\r\n\tlibutil.so.1 => /lib64/libutil.so.1 (0x00002aaaac2c3000)\r\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00002aaaac4c6000)\r\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00002aaaac6cf000)\r\n\tlibtcmalloc_minimal.so.4 => /mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/gperftools-2.7-65tknfddkxuphxbff4ixh3sqxrnt44hi/lib/libtcmalloc_minimal.so.4 (0x00002aaaac8ed000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x0000555555554000)\r\n\tlibiconv.so.2 => /mnt/b/projects/eot/bbcj/stewart1/spack/opt/spack/cray-cnl5-interlagos/gcc-5.3.0/libiconv-1.16-7wxzj3qqnroh7esew7nacrkbsnqhm3hr/lib/libiconv.so.2 (0x00002aaaacac1000)\r\n\tlibgfortran.so.3 => /opt/gcc/5.3.0/snos/lib/../lib64/libgfortran.so.3 (0x00002aaaacdbd000)\r\n\tlibquadmath.so.0 => /opt/gcc/5.3.0/snos/lib/../lib64/libquadmath.so.0 (0x00002aaaad0e0000)\r\n```\r\nAny clue what I'm doing wrong here?\r\n\r\nRelevant info:\r\n* Cray CNL5 cluster, SLES 11\r\n* Python 3.7.6\r\n* Sentencepiece 0.1.85\r\n\r\nSpack contains a separate package for `sentencepiece` C++ and Python. The Python package depends on the C++ package. If there's a different recommended way to install sentencepiece from source, let me know and I can try that.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/475", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/475/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/475/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/475/events", "html_url": "https://github.com/google/sentencepiece/issues/475", "id": 595478033, "node_id": "MDU6SXNzdWU1OTU0NzgwMzM=", "number": 475, "title": "Error when trying to compile _sentencepiece_processor_ops.so against TF 2.2.0rc2", "user": {"login": "wdirons", "id": 22967383, "node_id": "MDQ6VXNlcjIyOTY3Mzgz", "avatar_url": "https://avatars3.githubusercontent.com/u/22967383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wdirons", "html_url": "https://github.com/wdirons", "followers_url": "https://api.github.com/users/wdirons/followers", "following_url": "https://api.github.com/users/wdirons/following{/other_user}", "gists_url": "https://api.github.com/users/wdirons/gists{/gist_id}", "starred_url": "https://api.github.com/users/wdirons/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wdirons/subscriptions", "organizations_url": "https://api.github.com/users/wdirons/orgs", "repos_url": "https://api.github.com/users/wdirons/repos", "events_url": "https://api.github.com/users/wdirons/events{/privacy}", "received_events_url": "https://api.github.com/users/wdirons/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-06T22:56:36Z", "updated_at": "2020-04-24T15:01:39Z", "closed_at": "2020-04-24T13:38:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm running tensorflow/make_py_wheel.sh, using the tensorflow/tensorflow:custom-op-ubuntu16 container and pip3/python3 commands.\r\n\r\nI'm running build_tf_wrapper \"2.2.0rc2\"\r\n\r\nIt is failing to compile because of changes in TF 2.2. Any ideas on how I should update tensorflow/sentencepiece_processor_ops.cc to compile against TF 2.2?\r\n\r\nThis is the compile error:\r\n\r\n```\r\n+ TF_CFLAGS=($(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))'))\r\n++ python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_compile_flags()))'\r\n+ TF_LFLAGS=($(python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))'))\r\n++ python3 -c 'import tensorflow as tf; print(\" \".join(tf.sysconfig.get_link_flags()))'\r\n+ TF_VERSION=($(python3 -c 'import tensorflow as tf; print(tf.__version__)'))\r\n++ python3 -c 'import tensorflow as tf; print(tf.__version__)'\r\n+ echo TF_CFLAGS=-I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\nTF_CFLAGS=-I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0\r\n+ echo TF_LFLAGS=-L/usr/local/lib/python3.6/dist-packages/tensorflow -l:libtensorflow_framework.so.2\r\nTF_LFLAGS=-L/usr/local/lib/python3.6/dist-packages/tensorflow -l:libtensorflow_framework.so.2\r\n+ echo TF_VERSION=2.2.0-rc2\r\nTF_VERSION=2.2.0-rc2\r\n+ g++ -std=c++11 -shared -I../../src -fPIC -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -O2 -Wl,--whole-archive /usr/local/lib/libsentencepiece.a -Wl,--no-whole-archive sentencepiece_processor_ops.cc -o tf_sentencepiece/_sentencepiece_processor_ops.so.2.2.0-rc2 -L/usr/local/lib/python3.6/dist-packages/tensorflow -l:libtensorflow_framework.so.2\r\nIn file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:25:0,\r\n                 from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26,\r\n                 from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:28,\r\n                 from sentencepiece_processor_ops.cc:22:\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h: In instantiation of 'struct tensorflow::DataTypeToEnum<std::basic_string<char> >':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:97:62:   required from 'tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = std::basic_string<char>]'\r\nsentencepiece_processor_ops.cc:575:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:361:3: error: static assertion failed: Specified Data Type not supported\r\n   static_assert(IsValidDataType<T>::value, \"Specified Data Type not supported\");\r\n   ^\r\nIn file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:31:0,\r\n                 from sentencepiece_processor_ops.cc:22:\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h: In instantiation of 'tensorflow::KernelDefBuilder& tensorflow::KernelDefBuilder::TypeConstraint(const char*) [with T = std::basic_string<char>]':\r\nsentencepiece_processor_ops.cc:575:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/kernel_def_builder.h:97:62: error: 'v' is not a member of 'tensorflow::DataTypeToEnum<std::basic_string<char> >'\r\n   return this->TypeConstraint(attr_name, DataTypeToEnum<T>::v());\r\n                                                              ^\r\nIn file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26:0,\r\n                 from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:28,\r\n                 from sentencepiece_processor_ops.cc:22:\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::ConstTensor tensorflow::Tensor::tensor() const [with T = std::basic_string<char>; long unsigned int NDIMS = 2ul; typename tensorflow::TTypes<T, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const std::basic_string<char>, 2, 1, long int>, 16, Eigen::MakePointer>]':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:509:24:   required from 'typename tensorflow::TTypes<T>::ConstMatrix tensorflow::Tensor::matrix() const [with T = std::basic_string<char>; typename tensorflow::TTypes<T>::ConstMatrix = Eigen::TensorMap<Eigen::Tensor<const std::basic_string<char>, 2, 1, long int>, 16, Eigen::MakePointer>]'\r\nsentencepiece_processor_ops.cc:455:59:   required from 'void sentencepiece::SentencePieceDecodeOp<T>::Compute(tensorflow::OpKernelContext*) [with T = std::basic_string<char>]'\r\nsentencepiece_processor_ops.cc:645:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:722:45: error: 'v' is not a member of 'tensorflow::DataTypeToEnum<std::basic_string<char> >'\r\n   CheckTypeAndIsAligned(DataTypeToEnum<T>::v());\r\n                                             ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::tensor() [with T = std::basic_string<char>; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>]':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:386:24:   required from 'typename tensorflow::TTypes<T>::Vec tensorflow::Tensor::vec() [with T = std::basic_string<char>; typename tensorflow::TTypes<T>::Vec = Eigen::TensorMap<Eigen::Tensor<std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>]'\r\nsentencepiece_processor_ops.cc:463:64:   required from 'void sentencepiece::SentencePieceDecodeOp<T>::Compute(tensorflow::OpKernelContext*) [with T = std::basic_string<char>]'\r\nsentencepiece_processor_ops.cc:645:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:715:45: error: 'v' is not a member of 'tensorflow::DataTypeToEnum<std::basic_string<char> >'\r\n   CheckTypeAndIsAligned(DataTypeToEnum<T>::v());\r\n                                             ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::ConstTensor tensorflow::Tensor::tensor() const [with T = std::basic_string<char>; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>]':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:504:24:   required from 'typename tensorflow::TTypes<T>::ConstVec tensorflow::Tensor::vec() const [with T = std::basic_string<char>; typename tensorflow::TTypes<T>::ConstVec = Eigen::TensorMap<Eigen::Tensor<const std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>]'\r\nsentencepiece_processor_ops.cc:258:65:   required from 'void sentencepiece::SentencePieceEncodeOpBase<T>::Compute(tensorflow::OpKernelContext*) [with T = std::basic_string<char>]'\r\nsentencepiece_processor_ops.cc:645:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:722:45: error: 'v' is not a member of 'tensorflow::DataTypeToEnum<std::basic_string<char> >'\r\n   CheckTypeAndIsAligned(DataTypeToEnum<T>::v());\r\n                                             ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = std::basic_string<char>; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>; tensorflow::gtl::ArraySlice<long long int> = absl::Span<const long long int>]':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:446:24:   required from 'typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = std::basic_string<char>; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>]'\r\nsentencepiece_processor_ops.cc:368:56:   required from 'void sentencepiece::SentencePieceEncodeSparseOp<T>::MakeOutputTensor(tensorflow::OpKernelContext*, const std::vector<std::vector<_RealType> >&) [with T = std::basic_string<char>]'\r\nsentencepiece_processor_ops.cc:645:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:812:45: error: 'v' is not a member of 'tensorflow::DataTypeToEnum<std::basic_string<char> >'\r\n   CheckTypeAndIsAligned(DataTypeToEnum<T>::v());\r\n                                             ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::tensor() [with T = std::basic_string<char>; long unsigned int NDIMS = 2ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<std::basic_string<char>, 2, 1, long int>, 16, Eigen::MakePointer>]':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:391:24:   required from 'typename tensorflow::TTypes<T>::Matrix tensorflow::Tensor::matrix() [with T = std::basic_string<char>; typename tensorflow::TTypes<T>::Matrix = Eigen::TensorMap<Eigen::Tensor<std::basic_string<char>, 2, 1, long int>, 16, Eigen::MakePointer>]'\r\nsentencepiece_processor_ops.cc:414:58:   required from 'void sentencepiece::SentencePieceEncodeDenseOp<T>::MakeOutputTensor(tensorflow::OpKernelContext*, const std::vector<std::vector<_RealType> >&) [with T = std::basic_string<char>]'\r\nsentencepiece_processor_ops.cc:645:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:715:45: error: 'v' is not a member of 'tensorflow::DataTypeToEnum<std::basic_string<char> >'\r\n   CheckTypeAndIsAligned(DataTypeToEnum<T>::v());\r\n                                             ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of 'typename tensorflow::TTypes<T, NDIMS>::ConstTensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) const [with T = std::basic_string<char>; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>; tensorflow::gtl::ArraySlice<long long int> = absl::Span<const long long int>]':\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:535:24:   required from 'typename tensorflow::TTypes<T>::ConstFlat tensorflow::Tensor::flat() const [with T = std::basic_string<char>; typename tensorflow::TTypes<T>::ConstFlat = Eigen::TensorMap<Eigen::Tensor<const std::basic_string<char>, 1, 1, long int>, 16, Eigen::MakePointer>]'\r\nsentencepiece_processor_ops.cc:189:68:   required from 'void sentencepiece::SentencePieceConvertPieceOp<S, T>::Compute(tensorflow::OpKernelContext*) [with S = std::basic_string<char>; T = int]'\r\nsentencepiece_processor_ops.cc:645:1:   required from here\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:839:33: error: 'v' is not a member of 'tensorflow::DataTypeToEnum<std::basic_string<char> >'\r\n   CheckType(DataTypeToEnum<T>::v());\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/474", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/474/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/474/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/474/events", "html_url": "https://github.com/google/sentencepiece/issues/474", "id": 592099861, "node_id": "MDU6SXNzdWU1OTIwOTk4NjE=", "number": 474, "title": "Using `set_vocabulary` to modify vocabulary", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-01T17:58:18Z", "updated_at": "2020-05-30T13:49:36Z", "closed_at": "2020-04-23T01:42:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to use `set_vocabulary` to both change special tokens and add some tokens to the end of my vocab file. \r\n\r\nIn the example below, set_vocabulary does not seem to have any effect. (In the real example, `new_vocab` is a much longer list and the behavior is identical).\r\nIs that expected behavior?\r\n\r\nI am open to manipulating `m.pieces` as in #121 , but I'm not clear on how to instantiate a new piece to append.\r\n\r\n\r\n### Example Code:\r\nThe processor does not seem to be affected by `set_vocabulary`:\r\n\r\n```python\r\n\r\n#wget https://s3.amazonaws.com/models.huggingface.co/bert/facebook/mbart-large-en-ro/sentence.bpe.model\r\n\r\nimport sentencepiece as spm\r\nvocab_file = 'sentence.bpe.model'\r\ntest_string = '\u2581de'\r\n\r\nsp = spm.SentencePieceProcessor()\r\nsp.load(vocab_file)\r\nnew_vocab = ['<s>', '\u2581de', '-']\r\nprint('encoded:', sp.encode_as_ids(test_string)) # 7\r\noriginal_vocab = [sp.IdToPiece(id) for id in range(0, sp.GetPieceSize())]\r\nsp.set_vocabulary(new_vocab)\r\nv2 = [sp.IdToPiece(id) for id in range(0, sp.GetPieceSize())]\r\nprint('encoded:', sp.encode_as_ids(test_string)) # 7\r\nassert v2 == original_vocab  # doesnt raise\r\nassert v2 == new_vocab #=> AssertionError\r\nassert v2 != original_vocab#  => AssertionError\r\n```\r\n\r\n\r\nThanks!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/473", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/473/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/473/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/473/events", "html_url": "https://github.com/google/sentencepiece/issues/473", "id": 591019241, "node_id": "MDU6SXNzdWU1OTEwMTkyNDE=", "number": 473, "title": "Transform vocab file to spm model", "user": {"login": "hazardwayne", "id": 45623193, "node_id": "MDQ6VXNlcjQ1NjIzMTkz", "avatar_url": "https://avatars3.githubusercontent.com/u/45623193?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hazardwayne", "html_url": "https://github.com/hazardwayne", "followers_url": "https://api.github.com/users/hazardwayne/followers", "following_url": "https://api.github.com/users/hazardwayne/following{/other_user}", "gists_url": "https://api.github.com/users/hazardwayne/gists{/gist_id}", "starred_url": "https://api.github.com/users/hazardwayne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hazardwayne/subscriptions", "organizations_url": "https://api.github.com/users/hazardwayne/orgs", "repos_url": "https://api.github.com/users/hazardwayne/repos", "events_url": "https://api.github.com/users/hazardwayne/events{/privacy}", "received_events_url": "https://api.github.com/users/hazardwayne/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-31T11:15:45Z", "updated_at": "2020-04-14T01:15:33Z", "closed_at": "2020-04-14T01:15:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "If simply the vocab file  is available, how can i get the spm model rather than to retrain one?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/472", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/472/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/472/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/472/events", "html_url": "https://github.com/google/sentencepiece/issues/472", "id": 588824108, "node_id": "MDU6SXNzdWU1ODg4MjQxMDg=", "number": 472, "title": "No module named '_sentencepiece'", "user": {"login": "ayusharora99", "id": 16875522, "node_id": "MDQ6VXNlcjE2ODc1NTIy", "avatar_url": "https://avatars1.githubusercontent.com/u/16875522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ayusharora99", "html_url": "https://github.com/ayusharora99", "followers_url": "https://api.github.com/users/ayusharora99/followers", "following_url": "https://api.github.com/users/ayusharora99/following{/other_user}", "gists_url": "https://api.github.com/users/ayusharora99/gists{/gist_id}", "starred_url": "https://api.github.com/users/ayusharora99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ayusharora99/subscriptions", "organizations_url": "https://api.github.com/users/ayusharora99/orgs", "repos_url": "https://api.github.com/users/ayusharora99/repos", "events_url": "https://api.github.com/users/ayusharora99/events{/privacy}", "received_events_url": "https://api.github.com/users/ayusharora99/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-27T01:26:41Z", "updated_at": "2020-04-24T14:28:16Z", "closed_at": "2020-04-24T14:28:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to import sentencepiece to my jupyter notebook but I am running into an error on this line:\r\n\r\n`import tokenization` \r\n\r\nbecause of this line in sentencepiece.py:\r\n\r\n`return importlib.import_module('_sentencepiece')`\r\n\r\nThis is the error:\r\n\r\nNo module named '_sentencepiece'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/471", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/471/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/471/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/471/events", "html_url": "https://github.com/google/sentencepiece/issues/471", "id": 588592225, "node_id": "MDU6SXNzdWU1ODg1OTIyMjU=", "number": 471, "title": "No so file is found for [1.10.1] from [1.7.0, 1.13.1, 1.10.0, 1.11.0, 1.12.0, 1.9.0, 1.8.0]   ', '.join(versions)))", "user": {"login": "hazardwayne", "id": 45623193, "node_id": "MDQ6VXNlcjQ1NjIzMTkz", "avatar_url": "https://avatars3.githubusercontent.com/u/45623193?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hazardwayne", "html_url": "https://github.com/hazardwayne", "followers_url": "https://api.github.com/users/hazardwayne/followers", "following_url": "https://api.github.com/users/hazardwayne/following{/other_user}", "gists_url": "https://api.github.com/users/hazardwayne/gists{/gist_id}", "starred_url": "https://api.github.com/users/hazardwayne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hazardwayne/subscriptions", "organizations_url": "https://api.github.com/users/hazardwayne/orgs", "repos_url": "https://api.github.com/users/hazardwayne/repos", "events_url": "https://api.github.com/users/hazardwayne/events{/privacy}", "received_events_url": "https://api.github.com/users/hazardwayne/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-26T17:24:21Z", "updated_at": "2020-04-23T03:28:31Z", "closed_at": "2020-04-23T03:28:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Could you please support tensorflow=1.10.1?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/470", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/470/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/470/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/470/events", "html_url": "https://github.com/google/sentencepiece/issues/470", "id": 587570178, "node_id": "MDU6SXNzdWU1ODc1NzAxNzg=", "number": 470, "title": ": undefined symbol: _ZN10tensorflow12OpDefBuilder5InputENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE", "user": {"login": "fcqfcq", "id": 19226226, "node_id": "MDQ6VXNlcjE5MjI2MjI2", "avatar_url": "https://avatars0.githubusercontent.com/u/19226226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fcqfcq", "html_url": "https://github.com/fcqfcq", "followers_url": "https://api.github.com/users/fcqfcq/followers", "following_url": "https://api.github.com/users/fcqfcq/following{/other_user}", "gists_url": "https://api.github.com/users/fcqfcq/gists{/gist_id}", "starred_url": "https://api.github.com/users/fcqfcq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fcqfcq/subscriptions", "organizations_url": "https://api.github.com/users/fcqfcq/orgs", "repos_url": "https://api.github.com/users/fcqfcq/repos", "events_url": "https://api.github.com/users/fcqfcq/events{/privacy}", "received_events_url": "https://api.github.com/users/fcqfcq/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-25T09:48:23Z", "updated_at": "2020-05-15T18:59:53Z", "closed_at": "2020-04-24T13:44:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "#define EIGEN_USE_THREADS\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n    for (int i = 1; i < N; i++) {\r\n      output_flat(i) = 0;\r\n    }\r\n\r\n    // Preserve the first input value if possible.\r\n    if (N > 0) output_flat(0) = input(0);\r\n  }\r\n};\r\n\r\n\r\n// REGISTER_OP(\"ZeroOut\")\r\n//     .Input(\"to_zero: int32\")\r\n//     .Output(\"zeroed: int32\")\r\n//     .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n//       c->set_output(0, c->input(0));\r\n//       return Status::OK();\r\n//     });\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\");    \r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n\r\n\r\n\r\n\r\n\r\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\r\nTF_LIB=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_lib())')\r\n$TF_INC\r\n$TF_LIB\r\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC -I$TF_INC -I$TF_INC/external/nsync/public -L$TF_LIB -ltensorflow_framework -O2   \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/469", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/469/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/469/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/469/events", "html_url": "https://github.com/google/sentencepiece/issues/469", "id": 586902753, "node_id": "MDU6SXNzdWU1ODY5MDI3NTM=", "number": 469, "title": "Missing Mac OSX binaries for Python 3.8", "user": {"login": "kynan", "id": 346079, "node_id": "MDQ6VXNlcjM0NjA3OQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/346079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kynan", "html_url": "https://github.com/kynan", "followers_url": "https://api.github.com/users/kynan/followers", "following_url": "https://api.github.com/users/kynan/following{/other_user}", "gists_url": "https://api.github.com/users/kynan/gists{/gist_id}", "starred_url": "https://api.github.com/users/kynan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kynan/subscriptions", "organizations_url": "https://api.github.com/users/kynan/orgs", "repos_url": "https://api.github.com/users/kynan/repos", "events_url": "https://api.github.com/users/kynan/events{/privacy}", "received_events_url": "https://api.github.com/users/kynan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-03-24T11:50:41Z", "updated_at": "2020-04-24T13:37:52Z", "closed_at": "2020-04-24T13:37:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "See https://pypi.org/project/sentencepiece/#files\r\n\r\nCould you please add those?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/468", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/468/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/468/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/468/events", "html_url": "https://github.com/google/sentencepiece/issues/468", "id": 585358420, "node_id": "MDU6SXNzdWU1ODUzNTg0MjA=", "number": 468, "title": "Version 0.1.84 not hosted on PyPI", "user": {"login": "JLHasson", "id": 2087380, "node_id": "MDQ6VXNlcjIwODczODA=", "avatar_url": "https://avatars3.githubusercontent.com/u/2087380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JLHasson", "html_url": "https://github.com/JLHasson", "followers_url": "https://api.github.com/users/JLHasson/followers", "following_url": "https://api.github.com/users/JLHasson/following{/other_user}", "gists_url": "https://api.github.com/users/JLHasson/gists{/gist_id}", "starred_url": "https://api.github.com/users/JLHasson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JLHasson/subscriptions", "organizations_url": "https://api.github.com/users/JLHasson/orgs", "repos_url": "https://api.github.com/users/JLHasson/repos", "events_url": "https://api.github.com/users/JLHasson/events{/privacy}", "received_events_url": "https://api.github.com/users/JLHasson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-20T22:27:37Z", "updated_at": "2020-04-24T14:26:18Z", "closed_at": "2020-04-24T14:26:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Version 0.1.84 was released here: https://github.com/google/sentencepiece/releases/tag/v0.1.84\r\n\r\nbut it is not hosted on PyPI: https://pypi.org/project/tf-sentencepiece/#history", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/467", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/467/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/467/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/467/events", "html_url": "https://github.com/google/sentencepiece/issues/467", "id": 584903910, "node_id": "MDU6SXNzdWU1ODQ5MDM5MTA=", "number": 467, "title": "how to add comma ',' to user_defined_symbols", "user": {"login": "gaoxuezhao", "id": 8369701, "node_id": "MDQ6VXNlcjgzNjk3MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/8369701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaoxuezhao", "html_url": "https://github.com/gaoxuezhao", "followers_url": "https://api.github.com/users/gaoxuezhao/followers", "following_url": "https://api.github.com/users/gaoxuezhao/following{/other_user}", "gists_url": "https://api.github.com/users/gaoxuezhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaoxuezhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaoxuezhao/subscriptions", "organizations_url": "https://api.github.com/users/gaoxuezhao/orgs", "repos_url": "https://api.github.com/users/gaoxuezhao/repos", "events_url": "https://api.github.com/users/gaoxuezhao/events{/privacy}", "received_events_url": "https://api.github.com/users/gaoxuezhao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-20T08:05:21Z", "updated_at": "2020-05-13T04:21:50Z", "closed_at": "2020-04-23T01:48:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "how to add comma ',' to user_defined_symbols", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/466", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/466/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/466/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/466/events", "html_url": "https://github.com/google/sentencepiece/issues/466", "id": 583495368, "node_id": "MDU6SXNzdWU1ODM0OTUzNjg=", "number": 466, "title": "How to save and load vocab to index correspondence?", "user": {"login": "yangkevin2", "id": 22579946, "node_id": "MDQ6VXNlcjIyNTc5OTQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/22579946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yangkevin2", "html_url": "https://github.com/yangkevin2", "followers_url": "https://api.github.com/users/yangkevin2/followers", "following_url": "https://api.github.com/users/yangkevin2/following{/other_user}", "gists_url": "https://api.github.com/users/yangkevin2/gists{/gist_id}", "starred_url": "https://api.github.com/users/yangkevin2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yangkevin2/subscriptions", "organizations_url": "https://api.github.com/users/yangkevin2/orgs", "repos_url": "https://api.github.com/users/yangkevin2/repos", "events_url": "https://api.github.com/users/yangkevin2/events{/privacy}", "received_events_url": "https://api.github.com/users/yangkevin2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-18T06:20:53Z", "updated_at": "2020-03-18T06:45:16Z", "closed_at": "2020-03-18T06:44:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Nvm, it was an unrelated issue", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/465", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/465/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/465/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/465/events", "html_url": "https://github.com/google/sentencepiece/issues/465", "id": 579699632, "node_id": "MDU6SXNzdWU1Nzk2OTk2MzI=", "number": 465, "title": "How to tokenize sentence to all characters", "user": {"login": "ynebula", "id": 22788865, "node_id": "MDQ6VXNlcjIyNzg4ODY1", "avatar_url": "https://avatars0.githubusercontent.com/u/22788865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ynebula", "html_url": "https://github.com/ynebula", "followers_url": "https://api.github.com/users/ynebula/followers", "following_url": "https://api.github.com/users/ynebula/following{/other_user}", "gists_url": "https://api.github.com/users/ynebula/gists{/gist_id}", "starred_url": "https://api.github.com/users/ynebula/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ynebula/subscriptions", "organizations_url": "https://api.github.com/users/ynebula/orgs", "repos_url": "https://api.github.com/users/ynebula/repos", "events_url": "https://api.github.com/users/ynebula/events{/privacy}", "received_events_url": "https://api.github.com/users/ynebula/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-12T05:19:39Z", "updated_at": "2020-03-12T07:11:40Z", "closed_at": "2020-03-12T07:11:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am studying machine reading comprehension on xlmroberta.\r\nMy data is korquad.\r\n\r\nI need to tokenize all word to character.\r\ne.g. by english\r\nThis is a dog\r\n-> _T h i s _i s _a _d o g\r\n\r\nplease let me know.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/464", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/464/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/464/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/464/events", "html_url": "https://github.com/google/sentencepiece/issues/464", "id": 579098121, "node_id": "MDU6SXNzdWU1NzkwOTgxMjE=", "number": 464, "title": "module 'sentencepiece' has no attribute 'SentencePieceTrainer'", "user": {"login": "rossbrown9879", "id": 61983534, "node_id": "MDQ6VXNlcjYxOTgzNTM0", "avatar_url": "https://avatars0.githubusercontent.com/u/61983534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rossbrown9879", "html_url": "https://github.com/rossbrown9879", "followers_url": "https://api.github.com/users/rossbrown9879/followers", "following_url": "https://api.github.com/users/rossbrown9879/following{/other_user}", "gists_url": "https://api.github.com/users/rossbrown9879/gists{/gist_id}", "starred_url": "https://api.github.com/users/rossbrown9879/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rossbrown9879/subscriptions", "organizations_url": "https://api.github.com/users/rossbrown9879/orgs", "repos_url": "https://api.github.com/users/rossbrown9879/repos", "events_url": "https://api.github.com/users/rossbrown9879/events{/privacy}", "received_events_url": "https://api.github.com/users/rossbrown9879/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-11T08:46:49Z", "updated_at": "2020-03-12T01:34:31Z", "closed_at": "2020-03-12T01:34:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting following error, when I run sentence piece trainer in python.\r\n\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-8589c63acb87> in <module>\r\n----> 1 spm.SentencePieceTrainer.Train('--input=tx2.txt --model_prefix=m --vocab_size=1000')\r\n\r\nAttributeError: module 'sentencepiece' has no attribute 'SentencePieceTrainer'\r\n```\r\n\r\nI'm running following code:\r\n\r\n```\r\nimport sentencepiece as spm\r\n\r\nspm.SentencePieceTrainer.Train('--input=tx2.txt --model_prefix=m --vocab_size=1000')\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/463", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/463/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/463/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/463/events", "html_url": "https://github.com/google/sentencepiece/issues/463", "id": 578683560, "node_id": "MDU6SXNzdWU1Nzg2ODM1NjA=", "number": 463, "title": "Train model on large MS-MARCO corpus. ", "user": {"login": "rossbrown9879", "id": 61983534, "node_id": "MDQ6VXNlcjYxOTgzNTM0", "avatar_url": "https://avatars0.githubusercontent.com/u/61983534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rossbrown9879", "html_url": "https://github.com/rossbrown9879", "followers_url": "https://api.github.com/users/rossbrown9879/followers", "following_url": "https://api.github.com/users/rossbrown9879/following{/other_user}", "gists_url": "https://api.github.com/users/rossbrown9879/gists{/gist_id}", "starred_url": "https://api.github.com/users/rossbrown9879/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rossbrown9879/subscriptions", "organizations_url": "https://api.github.com/users/rossbrown9879/orgs", "repos_url": "https://api.github.com/users/rossbrown9879/repos", "events_url": "https://api.github.com/users/rossbrown9879/events{/privacy}", "received_events_url": "https://api.github.com/users/rossbrown9879/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-10T15:52:57Z", "updated_at": "2020-03-14T23:29:53Z", "closed_at": "2020-03-12T01:27:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n\r\nI'd like to train a sentencepiece model on MSMARCO corpus. The format of the corpus is `docid <tab> URL <tab> Query <tab> Document`. Now the real sentences in my corpus file are Query and Document lines. Is it possible to iteratively train a sentence piece model on lines (queries/documents) one by one? I tried training my model directly using\r\n\r\n`spm_train --input=msmarco-docs.tsv --model_prefix=ir_model --vocab_size=30000 --character_coverage=1.0`\r\n\r\nMy model is starting to train, but I'm getting following warnings at the beginning.\r\n```\r\ntrainer_interface.cc(267) LOG(INFO) Loading corpus: msmarco-docs.tsv\r\ntrainer_interface.cc(287) LOG(WARNING) Found too long line (6120 > 4192).\r\ntrainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\r\ntrainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\r\n```\r\n\r\nNow as determined in warning, training in this way skips some of the long lines. I guess this method will skip most of my documents, since they are all represented as a single line in MSMARCO dataset. I don't want to let my documents getting skipped. Does anyone have a better solution here? MS-MARCO dataset is a huge dataset of query-document pairs, with size of 22 gigs. Does anyone have idea on how long it should take to train a model on this huge dataset on i5 8th gen intel laptop with 8GB memory? Thanks in advance :)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/462", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/462/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/462/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/462/events", "html_url": "https://github.com/google/sentencepiece/issues/462", "id": 578048112, "node_id": "MDU6SXNzdWU1NzgwNDgxMTI=", "number": 462, "title": "Likelihood in the loss computation", "user": {"login": "Ryou0634", "id": 17979572, "node_id": "MDQ6VXNlcjE3OTc5NTcy", "avatar_url": "https://avatars3.githubusercontent.com/u/17979572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ryou0634", "html_url": "https://github.com/Ryou0634", "followers_url": "https://api.github.com/users/Ryou0634/followers", "following_url": "https://api.github.com/users/Ryou0634/following{/other_user}", "gists_url": "https://api.github.com/users/Ryou0634/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ryou0634/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ryou0634/subscriptions", "organizations_url": "https://api.github.com/users/Ryou0634/orgs", "repos_url": "https://api.github.com/users/Ryou0634/repos", "events_url": "https://api.github.com/users/Ryou0634/events{/privacy}", "received_events_url": "https://api.github.com/users/Ryou0634/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-09T16:52:28Z", "updated_at": "2020-03-12T06:19:13Z", "closed_at": "2020-03-12T01:51:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I'm trying to understand how sentencepiece works in detail, and have a question for the loss computation part.\r\n\r\nhttps://github.com/google/sentencepiece/blob/0f6db9477090688f3f8368fa9598b332b7d3c6dc/src/unigram_model_trainer.cc#L305-L307\r\n\r\nIn this part, it seems that the likelihood is computed by counting the token frequency in the viterbi path. However, I feel that it is more natural to use the expected token frequency over all possible paths, as computed in the EM algorithm or described in your paper. Why does the loss computation deviate from the true likelihood of the unigram LM, or am I misunderstanding something?\r\n\r\nThank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/461", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/461/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/461/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/461/events", "html_url": "https://github.com/google/sentencepiece/issues/461", "id": 578032281, "node_id": "MDU6SXNzdWU1NzgwMzIyODE=", "number": 461, "title": "Query in subword regularization, in readme.", "user": {"login": "rossbrown9879", "id": 61983534, "node_id": "MDQ6VXNlcjYxOTgzNTM0", "avatar_url": "https://avatars0.githubusercontent.com/u/61983534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rossbrown9879", "html_url": "https://github.com/rossbrown9879", "followers_url": "https://api.github.com/users/rossbrown9879/followers", "following_url": "https://api.github.com/users/rossbrown9879/following{/other_user}", "gists_url": "https://api.github.com/users/rossbrown9879/gists{/gist_id}", "starred_url": "https://api.github.com/users/rossbrown9879/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rossbrown9879/subscriptions", "organizations_url": "https://api.github.com/users/rossbrown9879/orgs", "repos_url": "https://api.github.com/users/rossbrown9879/repos", "events_url": "https://api.github.com/users/rossbrown9879/events{/privacy}", "received_events_url": "https://api.github.com/users/rossbrown9879/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-09T16:27:44Z", "updated_at": "2020-03-12T01:29:19Z", "closed_at": "2020-03-12T01:29:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Following line is mentioned at the beginning of Subword regularization in README.md. \r\n\r\n>To enable subword regularization, you would like to integrate SentencePiece library (C++/Python) into the NMT system to sample one segmentation for each parameter update, which is different from the standard off-line data preparations\r\n\r\nWhat do author mean by \"to sample one segmentation for each parameter update, which is different from the standard off-line data preparations\". What parameter update is he talking about? Also I did not get anything about standard off-line data preparations.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/460", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/460/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/460/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/460/events", "html_url": "https://github.com/google/sentencepiece/issues/460", "id": 576701147, "node_id": "MDU6SXNzdWU1NzY3MDExNDc=", "number": 460, "title": "Encounter error message while load the pacakge:", "user": {"login": "saravanansaminathan", "id": 36497548, "node_id": "MDQ6VXNlcjM2NDk3NTQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/36497548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saravanansaminathan", "html_url": "https://github.com/saravanansaminathan", "followers_url": "https://api.github.com/users/saravanansaminathan/followers", "following_url": "https://api.github.com/users/saravanansaminathan/following{/other_user}", "gists_url": "https://api.github.com/users/saravanansaminathan/gists{/gist_id}", "starred_url": "https://api.github.com/users/saravanansaminathan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saravanansaminathan/subscriptions", "organizations_url": "https://api.github.com/users/saravanansaminathan/orgs", "repos_url": "https://api.github.com/users/saravanansaminathan/repos", "events_url": "https://api.github.com/users/saravanansaminathan/events{/privacy}", "received_events_url": "https://api.github.com/users/saravanansaminathan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-06T05:05:21Z", "updated_at": "2020-04-24T13:49:14Z", "closed_at": "2020-04-24T13:49:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "I encounter the same problem with error message:\r\n\r\ndlopen(/Users/...../site-packages/tf_sentencepiece/_sentencepiece_processor_ops.so.so.2.0.0, 6): image not found\r\n\r\nVersion installed within virtualenv:\r\n- TensorFlow: 2.1.0\r\n- tf-sentencepiece 0.1.85\r\n\r\nOS: MacOS 10.15.2\r\n\r\n_Originally posted by @RogerChu8 in https://github.com/google/sentencepiece/issues/309#issuecomment-592318076_", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/459", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/459/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/459/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/459/events", "html_url": "https://github.com/google/sentencepiece/issues/459", "id": 573562073, "node_id": "MDU6SXNzdWU1NzM1NjIwNzM=", "number": 459, "title": "How to deal with space in custom symbols?", "user": {"login": "houwenxin", "id": 22743466, "node_id": "MDQ6VXNlcjIyNzQzNDY2", "avatar_url": "https://avatars2.githubusercontent.com/u/22743466?v=4", "gravatar_id": "", "url": "https://api.github.com/users/houwenxin", "html_url": "https://github.com/houwenxin", "followers_url": "https://api.github.com/users/houwenxin/followers", "following_url": "https://api.github.com/users/houwenxin/following{/other_user}", "gists_url": "https://api.github.com/users/houwenxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/houwenxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/houwenxin/subscriptions", "organizations_url": "https://api.github.com/users/houwenxin/orgs", "repos_url": "https://api.github.com/users/houwenxin/repos", "events_url": "https://api.github.com/users/houwenxin/events{/privacy}", "received_events_url": "https://api.github.com/users/houwenxin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-01T15:05:52Z", "updated_at": "2020-05-13T04:17:54Z", "closed_at": "2020-04-23T01:51:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "spm.SentencePieceTrainer.Train(\" \".join(args)) reports error when there is a space in the middle of custom symbols (like --user_defined_symbols=\\<IS MAN\\>). The function cut the arguments by the space.\r\n\r\nIs there any solution to this problem?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/458", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/458/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/458/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/458/events", "html_url": "https://github.com/google/sentencepiece/issues/458", "id": 573184047, "node_id": "MDU6SXNzdWU1NzMxODQwNDc=", "number": 458, "title": "building '_sentencepiece' extension error: [WinError 2] \u7cfb\u7edf\u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6587\u4ef6", "user": {"login": "Misoknisky", "id": 12208899, "node_id": "MDQ6VXNlcjEyMjA4ODk5", "avatar_url": "https://avatars0.githubusercontent.com/u/12208899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Misoknisky", "html_url": "https://github.com/Misoknisky", "followers_url": "https://api.github.com/users/Misoknisky/followers", "following_url": "https://api.github.com/users/Misoknisky/following{/other_user}", "gists_url": "https://api.github.com/users/Misoknisky/gists{/gist_id}", "starred_url": "https://api.github.com/users/Misoknisky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Misoknisky/subscriptions", "organizations_url": "https://api.github.com/users/Misoknisky/orgs", "repos_url": "https://api.github.com/users/Misoknisky/repos", "events_url": "https://api.github.com/users/Misoknisky/events{/privacy}", "received_events_url": "https://api.github.com/users/Misoknisky/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-29T05:22:07Z", "updated_at": "2020-03-12T01:34:59Z", "closed_at": "2020-03-12T01:34:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/456", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/456/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/456/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/456/events", "html_url": "https://github.com/google/sentencepiece/issues/456", "id": 571567142, "node_id": "MDU6SXNzdWU1NzE1NjcxNDI=", "number": 456, "title": "Library version selection breaks for TF 2.1", "user": {"login": "jjedele", "id": 779877, "node_id": "MDQ6VXNlcjc3OTg3Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/779877?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjedele", "html_url": "https://github.com/jjedele", "followers_url": "https://api.github.com/users/jjedele/followers", "following_url": "https://api.github.com/users/jjedele/following{/other_user}", "gists_url": "https://api.github.com/users/jjedele/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjedele/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjedele/subscriptions", "organizations_url": "https://api.github.com/users/jjedele/orgs", "repos_url": "https://api.github.com/users/jjedele/repos", "events_url": "https://api.github.com/users/jjedele/events{/privacy}", "received_events_url": "https://api.github.com/users/jjedele/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-26T18:15:09Z", "updated_at": "2020-04-24T13:40:16Z", "closed_at": "2020-04-24T13:40:16Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "The regex to looking for the most recent library version if no exact match is found extracts the whole match instead of the version number. This leads to the .so extension being duplicated.\r\n\r\nProduces:\r\nUserWarning: No so file is found for [2.1.0] from [so.1.15.0, so.1.14.0, so.2.0.0, so.1.13.1]\r\nShould be:\r\nUserWarning: No so file is found for [2.1.0] from [1.15.0, 1.14.0, 2.0.0, 1.13.1]\r\n\r\nI'll fix and open a PR right away.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/455", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/455/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/455/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/455/events", "html_url": "https://github.com/google/sentencepiece/issues/455", "id": 569773339, "node_id": "MDU6SXNzdWU1Njk3NzMzMzk=", "number": 455, "title": "Question: BPE & Unicode Points", "user": {"login": "jchwenger", "id": 34098722, "node_id": "MDQ6VXNlcjM0MDk4NzIy", "avatar_url": "https://avatars1.githubusercontent.com/u/34098722?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jchwenger", "html_url": "https://github.com/jchwenger", "followers_url": "https://api.github.com/users/jchwenger/followers", "following_url": "https://api.github.com/users/jchwenger/following{/other_user}", "gists_url": "https://api.github.com/users/jchwenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/jchwenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jchwenger/subscriptions", "organizations_url": "https://api.github.com/users/jchwenger/orgs", "repos_url": "https://api.github.com/users/jchwenger/repos", "events_url": "https://api.github.com/users/jchwenger/events{/privacy}", "received_events_url": "https://api.github.com/users/jchwenger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-24T10:46:07Z", "updated_at": "2020-04-25T11:25:39Z", "closed_at": "2020-04-23T02:01:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nThanks for this great library! I've started experimenting with it and find it really efficient and easy to use.\r\n\r\nI have one technical/theoretical question, and perhaps someone here might be able to give me some pointers.\r\n\r\nI have been looking at OpenAI's GPT-2, and their choices for BPE. Especially this bit:\r\n\r\n> Despite its name, reference BPE implementations often operate on Unicode code points and not byte sequences. These implementations would require including the full space of Unicode symbols in order to model all Unicode strings.  [...] In contrast, a byte-level version of BPE only requires a base vocabulary of size 256. However,directly applying BPE to the byte sequence results in suboptimal merges due to BPE using a greedy frequency based heuristic for building the token vocabulary. [...] This results in a suboptimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens.\r\n\r\nI then found this line in your Readme:\r\n> By default, SentencePiece normalizes the input with Unicode NFKC.\r\n\r\nI'm researching this at the moment, but is that a hint that sentencepiece works in this way, i.e. working at a byte-level with the appropriate caveats for cross-categories merges? \r\n\r\nThanks a lot in advance!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/454", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/454/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/454/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/454/events", "html_url": "https://github.com/google/sentencepiece/issues/454", "id": 569336430, "node_id": "MDU6SXNzdWU1NjkzMzY0MzA=", "number": 454, "title": "what do these two logs mean? Thanks", "user": {"login": "gaoxuezhao", "id": 8369701, "node_id": "MDQ6VXNlcjgzNjk3MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/8369701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaoxuezhao", "html_url": "https://github.com/gaoxuezhao", "followers_url": "https://api.github.com/users/gaoxuezhao/followers", "following_url": "https://api.github.com/users/gaoxuezhao/following{/other_user}", "gists_url": "https://api.github.com/users/gaoxuezhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaoxuezhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaoxuezhao/subscriptions", "organizations_url": "https://api.github.com/users/gaoxuezhao/orgs", "repos_url": "https://api.github.com/users/gaoxuezhao/repos", "events_url": "https://api.github.com/users/gaoxuezhao/events{/privacy}", "received_events_url": "https://api.github.com/users/gaoxuezhao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-22T13:42:51Z", "updated_at": "2020-02-25T03:39:22Z", "closed_at": "2020-02-25T03:39:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "bpe_model_trainer.cc(166) LOG(INFO) Updating active symbols. max_freq=3276 min_freq=42\r\n\r\nbpe_model_trainer.cc(257) LOG(INFO) Added: freq=3276 size=16020 all=20688530 active=1034409 piece=\u571f\u5929\u5929\u98ce\u4e4b\u65c5\u5fcd\u8005\u89d2\u8272\u5206\u6790\u4eca\u5929", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/453", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/453/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/453/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/453/events", "html_url": "https://github.com/google/sentencepiece/issues/453", "id": 566245760, "node_id": "MDU6SXNzdWU1NjYyNDU3NjA=", "number": 453, "title": "DLL Load Failed : Access is denied", "user": {"login": "pyturn", "id": 25935364, "node_id": "MDQ6VXNlcjI1OTM1MzY0", "avatar_url": "https://avatars2.githubusercontent.com/u/25935364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pyturn", "html_url": "https://github.com/pyturn", "followers_url": "https://api.github.com/users/pyturn/followers", "following_url": "https://api.github.com/users/pyturn/following{/other_user}", "gists_url": "https://api.github.com/users/pyturn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pyturn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pyturn/subscriptions", "organizations_url": "https://api.github.com/users/pyturn/orgs", "repos_url": "https://api.github.com/users/pyturn/repos", "events_url": "https://api.github.com/users/pyturn/events{/privacy}", "received_events_url": "https://api.github.com/users/pyturn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-17T11:46:11Z", "updated_at": "2020-02-26T02:59:06Z", "closed_at": "2020-02-26T02:59:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey All, \r\n\r\nI installed sentence piece module. It was successful for me , But when I am trying to import sentencepiece , it is showing me the  error \r\n\r\n`ImportError: DLL load failed: Access is denied.`\r\n\r\nI tried to install it using conda also, but is seems like conda version is available for Linux System only. Can anyone help me to succesful install and import sentencepiece. \r\n\r\nThanks  \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/452", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/452/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/452/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/452/events", "html_url": "https://github.com/google/sentencepiece/issues/452", "id": 565952291, "node_id": "MDU6SXNzdWU1NjU5NTIyOTE=", "number": 452, "title": "Issue in installing.", "user": {"login": "tkhan3", "id": 5717687, "node_id": "MDQ6VXNlcjU3MTc2ODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5717687?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tkhan3", "html_url": "https://github.com/tkhan3", "followers_url": "https://api.github.com/users/tkhan3/followers", "following_url": "https://api.github.com/users/tkhan3/following{/other_user}", "gists_url": "https://api.github.com/users/tkhan3/gists{/gist_id}", "starred_url": "https://api.github.com/users/tkhan3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tkhan3/subscriptions", "organizations_url": "https://api.github.com/users/tkhan3/orgs", "repos_url": "https://api.github.com/users/tkhan3/repos", "events_url": "https://api.github.com/users/tkhan3/events{/privacy}", "received_events_url": "https://api.github.com/users/tkhan3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2020-02-16T19:03:38Z", "updated_at": "2020-07-24T12:24:35Z", "closed_at": "2020-05-24T02:23:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Python 3.7.3\r\nOS: Redhat\r\n\r\nI am getting following error message while installing:\r\n\r\nI already tried installing wheel but getting message:\r\n```\r\n(tanveer) [ai_u@powcbds tanveer]$ pip install sentencepiece-0.1.85-cp38-cp38-manylinux1_i686.whl\r\nERROR: sentencepiece-0.1.85-cp38-cp38-manylinux1_i686.whl is not a supported wheel on this platform.\r\n```\r\n\r\n\r\n```\r\n> Using cached sentencepiece-0.1.83.tar.gz (497 kB)\r\n>   ERROR: Command errored out with exit status 1:\r\n>    command: /power8nfs/home/ai_u/.conda/envs/tanveer/bin/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-6kz16kgn/sentencepiece/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-6kz16kgn/sentencepiece/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-install-6kz16kgn/sentencepiece/pip-egg-info\r\n>        cwd: /tmp/pip-install-6kz16kgn/sentencepiece/\r\n>   Complete output (7 lines):\r\n>   Traceback (most recent call last):\r\n>     File \"<string>\", line 1, in <module>\r\n>     File \"/tmp/pip-install-6kz16kgn/sentencepiece/setup.py\", line 29, in <module>\r\n>       with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:\r\n>     File \"/power8nfs/home/ai_u/.conda/envs/tanveer/lib/python3.7/codecs.py\", line 904, in open\r\n>       file = builtins.open(filename, mode, buffering)\r\n>   FileNotFoundError: [Errno 2] No such file or directory: '../VERSION'\r\n>   ----------------------------------------\r\n> ERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n> \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/451", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/451/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/451/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/451/events", "html_url": "https://github.com/google/sentencepiece/issues/451", "id": 565182935, "node_id": "MDU6SXNzdWU1NjUxODI5MzU=", "number": 451, "title": "VERSIONS", "user": {"login": "AndreV84", "id": 16374873, "node_id": "MDQ6VXNlcjE2Mzc0ODcz", "avatar_url": "https://avatars3.githubusercontent.com/u/16374873?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreV84", "html_url": "https://github.com/AndreV84", "followers_url": "https://api.github.com/users/AndreV84/followers", "following_url": "https://api.github.com/users/AndreV84/following{/other_user}", "gists_url": "https://api.github.com/users/AndreV84/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreV84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreV84/subscriptions", "organizations_url": "https://api.github.com/users/AndreV84/orgs", "repos_url": "https://api.github.com/users/AndreV84/repos", "events_url": "https://api.github.com/users/AndreV84/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreV84/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-14T08:43:41Z", "updated_at": "2020-02-14T10:57:58Z", "closed_at": "2020-02-14T10:57:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "`Collecting sentencepiece\r\n  Using cached sentencepiece-0.1.83.tar.gz (497 kB)\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-3ka6fp8l/sentencepiece/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-3ka6fp8l/sentencepiece/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-install-3ka6fp8l/sentencepiece/pip-egg-info\r\n         cwd: /tmp/pip-install-3ka6fp8l/sentencepiece/\r\n    Complete output (7 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-3ka6fp8l/sentencepiece/setup.py\", line 29, in <module>\r\n        with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:\r\n      File \"/usr/lib/python3.6/codecs.py\", line 897, in open\r\n        file = builtins.open(filename, mode, buffering)\r\n    FileNotFoundError: [Errno 2] No such file or directory: '../VERSION'\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n`\r\nI installed it from sources but how do I exclude it from many executed dependencies to avoid interruption?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/450", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/450/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/450/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/450/events", "html_url": "https://github.com/google/sentencepiece/issues/450", "id": 559047840, "node_id": "MDU6SXNzdWU1NTkwNDc4NDA=", "number": 450, "title": "can't access file on \"My Drive\", despite quotes change", "user": {"login": "j2l", "id": 65325, "node_id": "MDQ6VXNlcjY1MzI1", "avatar_url": "https://avatars2.githubusercontent.com/u/65325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/j2l", "html_url": "https://github.com/j2l", "followers_url": "https://api.github.com/users/j2l/followers", "following_url": "https://api.github.com/users/j2l/following{/other_user}", "gists_url": "https://api.github.com/users/j2l/gists{/gist_id}", "starred_url": "https://api.github.com/users/j2l/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/j2l/subscriptions", "organizations_url": "https://api.github.com/users/j2l/orgs", "repos_url": "https://api.github.com/users/j2l/repos", "events_url": "https://api.github.com/users/j2l/events{/privacy}", "received_events_url": "https://api.github.com/users/j2l/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-02-03T12:19:40Z", "updated_at": "2020-05-13T04:19:18Z", "closed_at": "2020-05-13T04:19:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n`spm.SentencePieceTrainer.Train('--input=/content/drive/My Drive/base.txt --model_prefix=base --vocab_size=3200 --character_coverage=1.0  --model_type=unigram')`\r\nI tried to change quotes to \"\r\nI also tried to inject it as a variable:\r\n```\r\npath = \"/content/drive/My Drive/narniachar.txt\"\r\n'+path+'\r\n```\r\nI also tried to escape it with \\\\\r\nBut it splits at the space in \"My Drive\" and stops.\r\n\r\nHow to use senetenpiece within python with \"My Drive\"?\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/449", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/449/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/449/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/449/events", "html_url": "https://github.com/google/sentencepiece/issues/449", "id": 558843075, "node_id": "MDU6SXNzdWU1NTg4NDMwNzU=", "number": 449, "title": "Error building in Raspberry Pi", "user": {"login": "sharad461", "id": 24848946, "node_id": "MDQ6VXNlcjI0ODQ4OTQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/24848946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sharad461", "html_url": "https://github.com/sharad461", "followers_url": "https://api.github.com/users/sharad461/followers", "following_url": "https://api.github.com/users/sharad461/following{/other_user}", "gists_url": "https://api.github.com/users/sharad461/gists{/gist_id}", "starred_url": "https://api.github.com/users/sharad461/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sharad461/subscriptions", "organizations_url": "https://api.github.com/users/sharad461/orgs", "repos_url": "https://api.github.com/users/sharad461/repos", "events_url": "https://api.github.com/users/sharad461/events{/privacy}", "received_events_url": "https://api.github.com/users/sharad461/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-03T05:00:08Z", "updated_at": "2020-06-11T02:53:53Z", "closed_at": "2020-02-07T06:13:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "![Untitled](https://user-images.githubusercontent.com/24848946/73626595-926bab80-4671-11ea-84af-934046e9dcdb.png)\r\n\r\nWhile making sentencepiece C++ on a Raspberry Pi 4 with 4 GB RAM, this is the error I get. I'm using the instructions in the readme file to build.\r\n\r\n``` uname -a ```:\r\nLinux raspberrypi 4.19.93-v7l+ #1290 SMP Fri Jan 10 16:45:11 GMT 2020 armv7l GNU/Linux\r\n\r\nWhat am I doing wrong? \r\nCan provide more details if necessary.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/448", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/448/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/448/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/448/events", "html_url": "https://github.com/google/sentencepiece/issues/448", "id": 556285945, "node_id": "MDU6SXNzdWU1NTYyODU5NDU=", "number": 448, "title": "Segmentation Fault loading USE Xling Embeddings with TF==1.15.0", "user": {"login": "jasonw247", "id": 12789310, "node_id": "MDQ6VXNlcjEyNzg5MzEw", "avatar_url": "https://avatars0.githubusercontent.com/u/12789310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jasonw247", "html_url": "https://github.com/jasonw247", "followers_url": "https://api.github.com/users/jasonw247/followers", "following_url": "https://api.github.com/users/jasonw247/following{/other_user}", "gists_url": "https://api.github.com/users/jasonw247/gists{/gist_id}", "starred_url": "https://api.github.com/users/jasonw247/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jasonw247/subscriptions", "organizations_url": "https://api.github.com/users/jasonw247/orgs", "repos_url": "https://api.github.com/users/jasonw247/repos", "events_url": "https://api.github.com/users/jasonw247/events{/privacy}", "received_events_url": "https://api.github.com/users/jasonw247/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-28T15:30:00Z", "updated_at": "2020-04-24T13:50:23Z", "closed_at": "2020-04-24T13:50:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am running into a seg fault when loading :\r\n\r\nuniversal-sentence-encoder-multilingual-large/1 or universal-sentence-encoder-xling-many/1\r\n\r\nusing Tensorflow 1.15.0\r\n\r\nEnvironment:\r\n\r\npip freeze | grep -iE \"tensor|sentence\"\r\nbert-tensorflow==1.0.1\r\ntensorboard==1.15.0\r\ntensorflow==1.15.0\r\ntensorflow-estimator==1.15.1\r\ntensorflow-hub==0.7.0\r\ntf-sentencepiece==0.1.85\r\nError:\r\n\r\nPython 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 21:14:29)\r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow_hub\r\n>>> import tf_sentencepiece\r\n>>> tensorflow_hub.Module('https://tfhub.dev/google/universal-sentence-encoder-xling-many/1')\r\nSegmentation fault (core dumped)\r\n\r\nI am able to load the module using tensorflow==1.14.0 and tf-sentencepiece==0.1.85, as specified at: https://github.com/tensorflow/hub/issues/345.\r\n\r\nI have previously opened an issue with the Tensorflow Hub team at: https://github.com/tensorflow/hub/issues/490 where it was specified the underlying issue is an incompatibility between tf-sentencepiece and tf 1.15.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/447", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/447/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/447/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/447/events", "html_url": "https://github.com/google/sentencepiece/issues/447", "id": 554408270, "node_id": "MDU6SXNzdWU1NTQ0MDgyNzA=", "number": 447, "title": "model mixes spanish Accents", "user": {"login": "cri5Castro", "id": 11454650, "node_id": "MDQ6VXNlcjExNDU0NjUw", "avatar_url": "https://avatars1.githubusercontent.com/u/11454650?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cri5Castro", "html_url": "https://github.com/cri5Castro", "followers_url": "https://api.github.com/users/cri5Castro/followers", "following_url": "https://api.github.com/users/cri5Castro/following{/other_user}", "gists_url": "https://api.github.com/users/cri5Castro/gists{/gist_id}", "starred_url": "https://api.github.com/users/cri5Castro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cri5Castro/subscriptions", "organizations_url": "https://api.github.com/users/cri5Castro/orgs", "repos_url": "https://api.github.com/users/cri5Castro/repos", "events_url": "https://api.github.com/users/cri5Castro/events{/privacy}", "received_events_url": "https://api.github.com/users/cri5Castro/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-23T21:09:43Z", "updated_at": "2020-04-23T02:10:32Z", "closed_at": "2020-04-23T02:10:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "hi there I'm training a tokenizer for spanish but the model are mixing the accents with the  \u2581 resulting in things like : \u2581\u0301  and  _ a ar _\u0301o n for instance. \r\n\r\nNotice that my spanish sentences are conformed by  words with accents as a separed char for example: \r\nmuseo arqueol\u00b4ogico de nic\u00b4opolis\r\n\r\n\r\nthe problem is the tokenizer are mixing the accents with the \u2581 instead of treat them as a normal char", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/446", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/446/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/446/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/446/events", "html_url": "https://github.com/google/sentencepiece/issues/446", "id": 552445816, "node_id": "MDU6SXNzdWU1NTI0NDU4MTY=", "number": 446, "title": "Get sentencepiece vocab file from tf model", "user": {"login": "c00k1ez", "id": 16941854, "node_id": "MDQ6VXNlcjE2OTQxODU0", "avatar_url": "https://avatars0.githubusercontent.com/u/16941854?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c00k1ez", "html_url": "https://github.com/c00k1ez", "followers_url": "https://api.github.com/users/c00k1ez/followers", "following_url": "https://api.github.com/users/c00k1ez/following{/other_user}", "gists_url": "https://api.github.com/users/c00k1ez/gists{/gist_id}", "starred_url": "https://api.github.com/users/c00k1ez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c00k1ez/subscriptions", "organizations_url": "https://api.github.com/users/c00k1ez/orgs", "repos_url": "https://api.github.com/users/c00k1ez/repos", "events_url": "https://api.github.com/users/c00k1ez/events{/privacy}", "received_events_url": "https://api.github.com/users/c00k1ez/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-20T18:11:02Z", "updated_at": "2020-03-12T02:01:00Z", "closed_at": "2020-03-12T02:00:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi!\r\nSeveral days ago I had an interesting problem: there are some models in tfhub (e.g. Universal Sentence Encoder) with integrated preprocessing from ft.text and vocab file from sentencepiece.\r\nIs it possible to get this vocab file from a model?\r\n\r\nThank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/445", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/445/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/445/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/445/events", "html_url": "https://github.com/google/sentencepiece/issues/445", "id": 551474933, "node_id": "MDU6SXNzdWU1NTE0NzQ5MzM=", "number": 445, "title": "Integrating with OSS-Fuzz", "user": {"login": "Google-Autofuzz", "id": 27442508, "node_id": "MDQ6VXNlcjI3NDQyNTA4", "avatar_url": "https://avatars1.githubusercontent.com/u/27442508?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Google-Autofuzz", "html_url": "https://github.com/Google-Autofuzz", "followers_url": "https://api.github.com/users/Google-Autofuzz/followers", "following_url": "https://api.github.com/users/Google-Autofuzz/following{/other_user}", "gists_url": "https://api.github.com/users/Google-Autofuzz/gists{/gist_id}", "starred_url": "https://api.github.com/users/Google-Autofuzz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Google-Autofuzz/subscriptions", "organizations_url": "https://api.github.com/users/Google-Autofuzz/orgs", "repos_url": "https://api.github.com/users/Google-Autofuzz/repos", "events_url": "https://api.github.com/users/Google-Autofuzz/events{/privacy}", "received_events_url": "https://api.github.com/users/Google-Autofuzz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-17T15:21:23Z", "updated_at": "2020-06-29T19:59:53Z", "closed_at": "2020-06-29T19:59:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Greetings sentencepiece developers and contributors,\r\n\r\nWe\u2019re reaching out because your project is an important part of the open source ecosystem, and we\u2019d like to invite you to integrate with our [fuzzing](https://www.owasp.org/index.php/Fuzzing) service, [OSS-Fuzz]( https://opensource.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html ). OSS-Fuzz is a free fuzzing infrastructure you can use to identify security vulnerabilities and stability bugs in your project. OSS-Fuzz will:\r\n\r\n- Continuously run at scale all the fuzzers you write.\r\n- Alert you when it finds issues.\r\n- Automatically close issues after they\u2019ve been fixed by a commit.\r\n\r\nMany widely used [open source projects]( https://github.com/google/oss-fuzz/tree/master/projects ) like OpenSSL, FFmpeg, LibreOffice, and ImageMagick are fuzzing via OSS-Fuzz, which helps them find and remediate [critical issues]( https://bugs.chromium.org/p/oss-fuzz/issues/list?can=1&q=status%3AFixed%2CVerified+Type%3ABug%2CBug-Security+-component%3AInfra+ ). \r\n\r\nEven though typical integrations can be done in < 100 LoC, we have a [reward program]( https://www.google.com/about/appsecurity/patch-rewards/ ) in place which aims to recognize folks who are not just contributing to open source, but are also working hard to make it more secure.\r\n\r\nWe want to stress that anyone who meets the eligibility criteria and integrates a project with OSS-Fuzz is eligible for a reward.\r\n\r\nIf you're not interested in integrating with OSS-Fuzz, it would be helpful for us to understand why\u2014lack of interest, lack of time, or something else\u2014so we can better support projects like yours in the future.\r\n\r\nIf we\u2019ve missed your question in our [FAQ]( https://google.github.io/oss-fuzz/faq/ ), feel free to reply or reach out to us at oss-fuzz-outreach@googlegroups.com.\r\n\r\n\r\nThanks!\r\n\r\nTommy\r\nOSS-Fuzz Team\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/444", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/444/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/444/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/444/events", "html_url": "https://github.com/google/sentencepiece/issues/444", "id": 550689048, "node_id": "MDU6SXNzdWU1NTA2ODkwNDg=", "number": 444, "title": "Get vocab and merges file from model file", "user": {"login": "andompesta", "id": 6725612, "node_id": "MDQ6VXNlcjY3MjU2MTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6725612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andompesta", "html_url": "https://github.com/andompesta", "followers_url": "https://api.github.com/users/andompesta/followers", "following_url": "https://api.github.com/users/andompesta/following{/other_user}", "gists_url": "https://api.github.com/users/andompesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/andompesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andompesta/subscriptions", "organizations_url": "https://api.github.com/users/andompesta/orgs", "repos_url": "https://api.github.com/users/andompesta/repos", "events_url": "https://api.github.com/users/andompesta/events{/privacy}", "received_events_url": "https://api.github.com/users/andompesta/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-16T09:44:20Z", "updated_at": "2020-01-16T11:45:45Z", "closed_at": "2020-01-16T11:45:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "There is a way to obtain the vocab file and the merges file from a .model file?\r\n\r\nI'm using the python binding and I have a .model file, but I would like to obtain the original file vocab and merges generated during training.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/441", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/441/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/441/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/441/events", "html_url": "https://github.com/google/sentencepiece/issues/441", "id": 548921171, "node_id": "MDU6SXNzdWU1NDg5MjExNzE=", "number": 441, "title": "Specify the delimiter to segment the sentence at starting of training of BPE?", "user": {"login": "venkateshvayyavuru", "id": 48354711, "node_id": "MDQ6VXNlcjQ4MzU0NzEx", "avatar_url": "https://avatars3.githubusercontent.com/u/48354711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/venkateshvayyavuru", "html_url": "https://github.com/venkateshvayyavuru", "followers_url": "https://api.github.com/users/venkateshvayyavuru/followers", "following_url": "https://api.github.com/users/venkateshvayyavuru/following{/other_user}", "gists_url": "https://api.github.com/users/venkateshvayyavuru/gists{/gist_id}", "starred_url": "https://api.github.com/users/venkateshvayyavuru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/venkateshvayyavuru/subscriptions", "organizations_url": "https://api.github.com/users/venkateshvayyavuru/orgs", "repos_url": "https://api.github.com/users/venkateshvayyavuru/repos", "events_url": "https://api.github.com/users/venkateshvayyavuru/events{/privacy}", "received_events_url": "https://api.github.com/users/venkateshvayyavuru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-13T13:13:52Z", "updated_at": "2020-04-23T01:59:35Z", "closed_at": "2020-04-23T01:59:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI want specify the delimiter for splitting the sentence at starting of BPE training rather than utf-8 character sequence. \r\nEx:\r\n   c1-c2-c3-c4 c5-c6-c7-c8 c9-c10-c11-c12.\r\nIn this case delimiter is \"-\". The pairs are like (c1,c2) (c2,c3) and so on. Is there any option to specify the delimiter while training.    Please help me. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/440", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/440/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/440/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/440/events", "html_url": "https://github.com/google/sentencepiece/issues/440", "id": 547558391, "node_id": "MDU6SXNzdWU1NDc1NTgzOTE=", "number": 440, "title": "Unable to import tf_sentencepiece with tensorflow 1.15.0", "user": {"login": "r-wheeler", "id": 5184858, "node_id": "MDQ6VXNlcjUxODQ4NTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5184858?v=4", "gravatar_id": "", "url": "https://api.github.com/users/r-wheeler", "html_url": "https://github.com/r-wheeler", "followers_url": "https://api.github.com/users/r-wheeler/followers", "following_url": "https://api.github.com/users/r-wheeler/following{/other_user}", "gists_url": "https://api.github.com/users/r-wheeler/gists{/gist_id}", "starred_url": "https://api.github.com/users/r-wheeler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/r-wheeler/subscriptions", "organizations_url": "https://api.github.com/users/r-wheeler/orgs", "repos_url": "https://api.github.com/users/r-wheeler/repos", "events_url": "https://api.github.com/users/r-wheeler/events{/privacy}", "received_events_url": "https://api.github.com/users/r-wheeler/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-01-09T15:43:49Z", "updated_at": "2020-08-22T03:09:26Z", "closed_at": "2020-04-24T13:40:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "After seeing [this issue](https://github.com/google/sentencepiece/issues/414) closed.\r\n\r\nFrom a fresh venv \r\n\r\n```\r\npip install tensorflow==1.15.0\r\npip install tf-sentencepiece==0.1.85\r\n```\r\n\r\n```\r\n((.venv)) \u2771 python\r\nPython 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31)\r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.15.0'\r\n>>> import tf_sentencepiece\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ryan.wheeler/repos/tomo/.venv/lib/python3.6/site-packages/tf_sentencepiece/__init__.py\", line 5, in <module>\r\n    from tf_sentencepiece.sentencepiece_processor_ops import *\r\n  File \"/Users/ryan.wheeler/repos/tomo/.venv/lib/python3.6/site-packages/tf_sentencepiece/sentencepiece_processor_ops.py\", line 47, in <module>\r\n    _gen_sentencepiece_processor_op = tf.load_op_library(so_file)\r\n  File \"/Users/ryan.wheeler/repos/tomo/.venv/lib/python3.6/site-packages/tensorflow_core/python/framework/load_library.py\", line 61, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/ryan.wheeler/repos/tomo/.venv/lib/python3.6/site-packages/tf_sentencepiece/_sentencepiece_processor_ops.so.1.15.0, 6): no suitable image found.  Did find:\r\n\t/Users/ryan.wheeler/repos/tomo/.venv/lib/python3.6/site-packages/tf_sentencepiece/_sentencepiece_processor_ops.so.1.15.0: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x00\r\n\t/Users/ryan.wheeler/repos/tomo/.venv/lib/python3.6/site-packages/tf_sentencepiece/_sentencepiece_processor_ops.so.1.15.0: unknown file type, first eight bytes: 0x7F 0x45 0x4C 0x46 0x02 0x01 0x01 0x00\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/439", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/439/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/439/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/439/events", "html_url": "https://github.com/google/sentencepiece/issues/439", "id": 546607752, "node_id": "MDU6SXNzdWU1NDY2MDc3NTI=", "number": 439, "title": "Is there a way to train on an in-memory corpus in Python?", "user": {"login": "pwsiegel", "id": 3418384, "node_id": "MDQ6VXNlcjM0MTgzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/3418384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pwsiegel", "html_url": "https://github.com/pwsiegel", "followers_url": "https://api.github.com/users/pwsiegel/followers", "following_url": "https://api.github.com/users/pwsiegel/following{/other_user}", "gists_url": "https://api.github.com/users/pwsiegel/gists{/gist_id}", "starred_url": "https://api.github.com/users/pwsiegel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pwsiegel/subscriptions", "organizations_url": "https://api.github.com/users/pwsiegel/orgs", "repos_url": "https://api.github.com/users/pwsiegel/repos", "events_url": "https://api.github.com/users/pwsiegel/events{/privacy}", "received_events_url": "https://api.github.com/users/pwsiegel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-08T02:21:15Z", "updated_at": "2020-05-26T20:03:20Z", "closed_at": "2020-01-08T06:33:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is a great library - thanks for all the hard work!\r\n\r\nFor both the CLI and the Python wrapper, the only documented way to train a new tokenizer is to specify a path to one or more text files containing the training corpus.  I am using the tokenizer as a step in the middle of a longer text processing pipeline (in Python), and so the corpus is in memory rather than in a file somewhere.  My workaround is to temporarily save the data to a file and then pass that filename to sentencepiece, but as I understand it the first thing sentencepiece does is load the file back into memory, so the workaround is a little silly.  Is there a better way?  I don't mind writing a little Cython or something if needed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/438", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/438/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/438/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/438/events", "html_url": "https://github.com/google/sentencepiece/issues/438", "id": 546001466, "node_id": "MDU6SXNzdWU1NDYwMDE0NjY=", "number": 438, "title": "Extra piece preceding user_defined_symbol encoding", "user": {"login": "sarvghotra", "id": 9509853, "node_id": "MDQ6VXNlcjk1MDk4NTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/9509853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sarvghotra", "html_url": "https://github.com/sarvghotra", "followers_url": "https://api.github.com/users/sarvghotra/followers", "following_url": "https://api.github.com/users/sarvghotra/following{/other_user}", "gists_url": "https://api.github.com/users/sarvghotra/gists{/gist_id}", "starred_url": "https://api.github.com/users/sarvghotra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sarvghotra/subscriptions", "organizations_url": "https://api.github.com/users/sarvghotra/orgs", "repos_url": "https://api.github.com/users/sarvghotra/repos", "events_url": "https://api.github.com/users/sarvghotra/events{/privacy}", "received_events_url": "https://api.github.com/users/sarvghotra/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-06T23:55:10Z", "updated_at": "2020-01-08T09:11:44Z", "closed_at": "2020-01-08T06:30:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I trained a sentencepiece model with some `--user_defined_symbols` like **\\<documenttitle\\> \\<main\\> \\<paragraph\\>** etc. When I encode a sentence containing these symbols, there is always a `_` piece that appears as a **separate piece** preceding all user_defined_symbols in the encoding\r\n\r\n**For e.g.**, encoding of **\"\\<documenttitle\\> \\<main\\> \\<paragraph\\> how many points to win\"** is \r\n\r\n**EncodeAsPiece:** [b'\\xe2\\x96\\x81',  b'<documenttitle>',  b'\\xe2\\x96\\x81',  b'<main>',  b'\\xe2\\x96\\x81',  b'<paragraph>',  b'\\xe2\\x96\\x81how',  b'\\xe2\\x96\\x81many',  b'\\xe2\\x96\\x81points',  b'\\xe2\\x96\\x81to',  b'\\xe2\\x96\\x81win']\r\n\r\n**EncodeAsIds:** [19902, 8, 19902, 9, 19902, 10, 235, 460, 2854, 63, 1322]\r\n\r\nMy question is that is it a desired behavior to have `_` as a separate piece?\r\n\r\nThanks in advance.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/437", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/437/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/437/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/437/events", "html_url": "https://github.com/google/sentencepiece/issues/437", "id": 542986158, "node_id": "MDU6SXNzdWU1NDI5ODYxNTg=", "number": 437, "title": "no valid symbol found", "user": {"login": "5118Python", "id": 14151451, "node_id": "MDQ6VXNlcjE0MTUxNDUx", "avatar_url": "https://avatars2.githubusercontent.com/u/14151451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/5118Python", "html_url": "https://github.com/5118Python", "followers_url": "https://api.github.com/users/5118Python/followers", "following_url": "https://api.github.com/users/5118Python/following{/other_user}", "gists_url": "https://api.github.com/users/5118Python/gists{/gist_id}", "starred_url": "https://api.github.com/users/5118Python/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/5118Python/subscriptions", "organizations_url": "https://api.github.com/users/5118Python/orgs", "repos_url": "https://api.github.com/users/5118Python/repos", "events_url": "https://api.github.com/users/5118Python/events{/privacy}", "received_events_url": "https://api.github.com/users/5118Python/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-27T20:48:41Z", "updated_at": "2020-04-23T01:57:32Z", "closed_at": "2020-04-23T01:57:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "![image](https://user-images.githubusercontent.com/14151451/71532211-4bc9b980-292d-11ea-8dda-d84bb11f2747.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/436", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/436/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/436/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/436/events", "html_url": "https://github.com/google/sentencepiece/issues/436", "id": 542250175, "node_id": "MDU6SXNzdWU1NDIyNTAxNzU=", "number": 436, "title": "installation error !", "user": {"login": "nlpofwhat", "id": 38346174, "node_id": "MDQ6VXNlcjM4MzQ2MTc0", "avatar_url": "https://avatars1.githubusercontent.com/u/38346174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nlpofwhat", "html_url": "https://github.com/nlpofwhat", "followers_url": "https://api.github.com/users/nlpofwhat/followers", "following_url": "https://api.github.com/users/nlpofwhat/following{/other_user}", "gists_url": "https://api.github.com/users/nlpofwhat/gists{/gist_id}", "starred_url": "https://api.github.com/users/nlpofwhat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nlpofwhat/subscriptions", "organizations_url": "https://api.github.com/users/nlpofwhat/orgs", "repos_url": "https://api.github.com/users/nlpofwhat/repos", "events_url": "https://api.github.com/users/nlpofwhat/events{/privacy}", "received_events_url": "https://api.github.com/users/nlpofwhat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-25T04:17:58Z", "updated_at": "2019-12-26T05:32:13Z", "closed_at": "2019-12-26T05:18:50Z", "author_association": "NONE", "active_lock_reason": null, "body": " pip install sentencepiece\r\nERROR: Command errored out with exit status 1:\r\n     command: 'd:\\python\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\brave\\\\AppData\\\\Local\\\\Temp\\\\pip-install-gu69yst7\\\\sentencepiece\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\brave\\\\AppData\\\\Local\\\\Temp\\\\pip-install-gu69yst7\\\\sentencepiece\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\brave\\AppData\\Local\\Temp\\pip-install-gu69yst7\\sentencepiece\\pip-egg-info'\r\n         cwd: C:\\Users\\brave\\AppData\\Local\\Temp\\pip-install-gu69yst7\\sentencepiece\\\r\n    Complete output (7 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"C:\\Users\\brave\\AppData\\Local\\Temp\\pip-install-gu69yst7\\sentencepiece\\setup.py\", line 29, in <module>\r\n        with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:\r\n      File \"d:\\python\\lib\\codecs.py\", line 895, in open\r\n        file = builtins.open(filename, mode, buffering)\r\n    FileNotFoundError: [Errno 2] No such file or directory: '..\\\\VERSION'\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n\r\npython 3.5", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/435", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/435/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/435/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/435/events", "html_url": "https://github.com/google/sentencepiece/issues/435", "id": 540767422, "node_id": "MDU6SXNzdWU1NDA3Njc0MjI=", "number": 435, "title": "Would you please support BPE dropout?", "user": {"login": "zodiacR", "id": 2304163, "node_id": "MDQ6VXNlcjIzMDQxNjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2304163?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zodiacR", "html_url": "https://github.com/zodiacR", "followers_url": "https://api.github.com/users/zodiacR/followers", "following_url": "https://api.github.com/users/zodiacR/following{/other_user}", "gists_url": "https://api.github.com/users/zodiacR/gists{/gist_id}", "starred_url": "https://api.github.com/users/zodiacR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zodiacR/subscriptions", "organizations_url": "https://api.github.com/users/zodiacR/orgs", "repos_url": "https://api.github.com/users/zodiacR/repos", "events_url": "https://api.github.com/users/zodiacR/events{/privacy}", "received_events_url": "https://api.github.com/users/zodiacR/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-12-20T05:20:24Z", "updated_at": "2020-05-20T16:00:28Z", "closed_at": "2019-12-21T06:29:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Team,\r\nRecently there is an interesting paper coming out from yandex: BPE dropout (https://arxiv.org/abs/1910.13267). This simple and effective method adopts a dropout technique over BPE, and demonstrates a competitive performance against subword regularization. I'm wondering whether this new technique can be incorporated into this repo.\r\n\r\nBest\r\nZodiac", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/434", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/434/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/434/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/434/events", "html_url": "https://github.com/google/sentencepiece/issues/434", "id": 539391352, "node_id": "MDU6SXNzdWU1MzkzOTEzNTI=", "number": 434, "title": "build subword dictionary MANUALLY", "user": {"login": "hahmyg", "id": 3884429, "node_id": "MDQ6VXNlcjM4ODQ0Mjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/3884429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hahmyg", "html_url": "https://github.com/hahmyg", "followers_url": "https://api.github.com/users/hahmyg/followers", "following_url": "https://api.github.com/users/hahmyg/following{/other_user}", "gists_url": "https://api.github.com/users/hahmyg/gists{/gist_id}", "starred_url": "https://api.github.com/users/hahmyg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hahmyg/subscriptions", "organizations_url": "https://api.github.com/users/hahmyg/orgs", "repos_url": "https://api.github.com/users/hahmyg/repos", "events_url": "https://api.github.com/users/hahmyg/events{/privacy}", "received_events_url": "https://api.github.com/users/hahmyg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-18T00:58:14Z", "updated_at": "2019-12-20T09:45:31Z", "closed_at": "2019-12-20T09:44:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nEven though SentencePiece is based on a unsupervised model,\r\nthere are a way to build pre-defined subword set manually?\r\n\r\nFor example, add a suffix \"-ing\" manually.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/433", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/433/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/433/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/433/events", "html_url": "https://github.com/google/sentencepiece/issues/433", "id": 538471317, "node_id": "MDU6SXNzdWU1Mzg0NzEzMTc=", "number": 433, "title": "PyPI is missing 0.1.85 wheels for Python 3.6 and later", "user": {"login": "willfrey", "id": 13784361, "node_id": "MDQ6VXNlcjEzNzg0MzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/13784361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/willfrey", "html_url": "https://github.com/willfrey", "followers_url": "https://api.github.com/users/willfrey/followers", "following_url": "https://api.github.com/users/willfrey/following{/other_user}", "gists_url": "https://api.github.com/users/willfrey/gists{/gist_id}", "starred_url": "https://api.github.com/users/willfrey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/willfrey/subscriptions", "organizations_url": "https://api.github.com/users/willfrey/orgs", "repos_url": "https://api.github.com/users/willfrey/repos", "events_url": "https://api.github.com/users/willfrey/events{/privacy}", "received_events_url": "https://api.github.com/users/willfrey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-16T15:09:37Z", "updated_at": "2019-12-16T18:05:35Z", "closed_at": "2019-12-16T16:00:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Could you please upload wheels for Python 3.6 and above? It appears that they're missing from the [files listed on PyPI](https://pypi.org/project/sentencepiece/#files) for the 0.1.85 release.\r\n\r\nThank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/432", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/432/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/432/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/432/events", "html_url": "https://github.com/google/sentencepiece/issues/432", "id": 538333795, "node_id": "MDU6SXNzdWU1MzgzMzM3OTU=", "number": 432, "title": "compilation issues", "user": {"login": "jwijffels", "id": 1710810, "node_id": "MDQ6VXNlcjE3MTA4MTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1710810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwijffels", "html_url": "https://github.com/jwijffels", "followers_url": "https://api.github.com/users/jwijffels/followers", "following_url": "https://api.github.com/users/jwijffels/following{/other_user}", "gists_url": "https://api.github.com/users/jwijffels/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwijffels/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwijffels/subscriptions", "organizations_url": "https://api.github.com/users/jwijffels/orgs", "repos_url": "https://api.github.com/users/jwijffels/repos", "events_url": "https://api.github.com/users/jwijffels/events{/privacy}", "received_events_url": "https://api.github.com/users/jwijffels/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-12-16T10:50:04Z", "updated_at": "2020-06-08T18:20:12Z", "closed_at": "2020-06-08T16:25:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI would like to upload the R wrapper (https://github.com/bnosac/sentencepiece) around sentencepiece to CRAN. CRAN is pretty strict on compilation issues. Would it be possible to fix the following issues which occur when checking on Debian, Solaris, ASAN/UBSAN\r\n\r\nOn **Debian Linux, GCC, no long double**, I'm getting:\r\n\r\n```\r\nFound the following significant warnings:\r\n  sentencepiece/src/freelist.h:62:13: warning: \u2018void* memset(void*, int, size_t)\u2019 clearing an object of non-trivial type \u2018struct sentencepiece::unigram::Lattice::Node\u2019; use assignment or value-initialization instead [-Wclass-memaccess]\r\n  sentencepiece/src/freelist.h:39:13: warning: \u2018void* memset(void*, int, size_t)\u2019 clearing an object of non-trivial type \u2018struct sentencepiece::unigram::Lattice::Node\u2019; use assignment or value-initialization instead [-Wclass-memaccess]\r\n```\r\n\r\nOn **Oracle Solaris, compilation fails**: with the following error\r\n\r\n```\r\nRunning `R CMD build`...\r\n* checking for file \u2018/export/home/XUABCjr/Rtemp/RtmpnMayqW/remotes60831f397749/sentencepiece/DESCRIPTION\u2019 ... OK\r\n* preparing \u2018sentencepiece\u2019:\r\n* checking DESCRIPTION meta-information ... OK\r\n* cleaning src\r\n* checking for LF line-endings in source and make files and shell scripts\r\n* checking for empty or unneeded directories\r\n* building \u2018sentencepiece_0.1.0.tar.gz\u2019\r\nInstalling package into \u2018/export/home/XUABCjr/R\u2019\r\n(as \u2018lib\u2019 is unspecified)\r\n* installing *source* package \u2018sentencepiece\u2019 ...\r\n** using staged installation\r\n** libs\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/arena.cc -o third_party/protobuf-lite/arena.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/arenastring.cc -o third_party/protobuf-lite/arenastring.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/bytestream.cc -o third_party/protobuf-lite/bytestream.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/coded_stream.cc -o third_party/protobuf-lite/coded_stream.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/common.cc -o third_party/protobuf-lite/common.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/extension_set.cc -o third_party/protobuf-lite/extension_set.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/generated_message_table_driven_lite.cc -o third_party/protobuf-lite/generated_message_table_driven_lite.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/generated_message_util.cc -o third_party/protobuf-lite/generated_message_util.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/implicit_weak_message.cc -o third_party/protobuf-lite/implicit_weak_message.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/int128.cc -o third_party/protobuf-lite/int128.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/io_win32.cc -o third_party/protobuf-lite/io_win32.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/message_lite.cc -o third_party/protobuf-lite/message_lite.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/repeated_field.cc -o third_party/protobuf-lite/repeated_field.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/status.cc -o third_party/protobuf-lite/status.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/statusor.cc -o third_party/protobuf-lite/statusor.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/stringpiece.cc -o third_party/protobuf-lite/stringpiece.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/stringprintf.cc -o third_party/protobuf-lite/stringprintf.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/structurally_valid.cc -o third_party/protobuf-lite/structurally_valid.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/strutil.cc -o third_party/protobuf-lite/strutil.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/time.cc -o third_party/protobuf-lite/time.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/wire_format_lite.cc -o third_party/protobuf-lite/wire_format_lite.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/zero_copy_stream.cc -o third_party/protobuf-lite/zero_copy_stream.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/protobuf-lite/zero_copy_stream_impl_lite.cc -o third_party/protobuf-lite/zero_copy_stream_impl_lite.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c third_party/absl/strings/string_view.cc -o third_party/absl/strings/string_view.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c sentencepiece/src/builtin_pb/sentencepiece.pb.cc -o sentencepiece/src/builtin_pb/sentencepiece.pb.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c sentencepiece/src/builtin_pb/sentencepiece_model.pb.cc -o sentencepiece/src/builtin_pb/sentencepiece_model.pb.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c sentencepiece/src/bpe_model.cc -o sentencepiece/src/bpe_model.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c sentencepiece/src/bpe_model_trainer.cc -o sentencepiece/src/bpe_model_trainer.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c sentencepiece/src/builder.cc -o sentencepiece/src/builder.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c sentencepiece/src/char_model.cc -o sentencepiece/src/char_model.o\r\n/opt/csw/bin/g++ -std=gnu++11 -I\"/opt/R/R-3.6.0/lib/R/include\" -DNDEBUG -D HAVE_PTHREAD=1 -Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare -pthread -DSTRICT_R_HEADERS -I. -I./sentencepiece -I./sentencepiece/src -I./sentencepiece/src/builtin_pb -I./third_party/absl -I./third_party/darts_clone -I./third_party/esaxx -I./third_party/protobuf-lite  -I\"/export/home/XUABCjr/R/Rcpp/include\" -I/opt/cdw/include -I/usr/local/include  -fPIC  -O2  -c sentencepiece/src/char_model_trainer.cc -o sentencepiece/src/char_model_trainer.o\r\nsentencepiece/src/char_model_trainer.cc: In member function \u2018virtual sentencepiece::util::Status sentencepiece::character::Trainer::Train()\u2019:\r\nsentencepiece/src/char_model_trainer.cc:41:31: error: call of overloaded \u2018log(uint64&)\u2019 is ambiguous\r\n   const float logsum = log(sum);\r\n                               ^\r\nIn file included from /opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/math.h:22:0,\r\n                 from /opt/csw/include/c++/5.5.0/cmath:44,\r\n                 from /opt/csw/include/c++/5.5.0/random:38,\r\n                 from sentencepiece/src/util.h:24,\r\n                 from sentencepiece/src/trainer_interface.h:29,\r\n                 from sentencepiece/src/char_model_trainer.h:19,\r\n                 from sentencepiece/src/char_model_trainer.cc:15:\r\n/opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/iso/math_iso.h:206:21: note: candidate: long double std::log(long double)\r\n  inline long double log(long double __X) { return __logl(__X); }\r\n                     ^\r\n/opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/iso/math_iso.h:167:15: note: candidate: float std::log(float)\r\n  inline float log(float __X) { return __logf(__X); }\r\n               ^\r\n/opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/iso/math_iso.h:67:15: note: candidate: double std::log(double)\r\n extern double log __P((double));\r\n               ^\r\nsentencepiece/src/char_model_trainer.cc:50:45: error: call of overloaded \u2018log(const long long int&)\u2019 is ambiguous\r\n                                log(it.second) - logsum);\r\n                                             ^\r\nIn file included from /opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/math.h:22:0,\r\n                 from /opt/csw/include/c++/5.5.0/cmath:44,\r\n                 from /opt/csw/include/c++/5.5.0/random:38,\r\n                 from sentencepiece/src/util.h:24,\r\n                 from sentencepiece/src/trainer_interface.h:29,\r\n                 from sentencepiece/src/char_model_trainer.h:19,\r\n                 from sentencepiece/src/char_model_trainer.cc:15:\r\n/opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/iso/math_iso.h:206:21: note: candidate: long double std::log(long double)\r\n  inline long double log(long double __X) { return __logl(__X); }\r\n                     ^\r\n/opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/iso/math_iso.h:167:15: note: candidate: float std::log(float)\r\n  inline float log(float __X) { return __logf(__X); }\r\n               ^\r\n/opt/csw/lib/gcc/i386-pc-solaris2.10/5.5.0/include-fixed/iso/math_iso.h:67:15: note: candidate: double std::log(double)\r\n extern double log __P((double));\r\n               ^\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \u2018-Wno-misleading-indentation\u2019\r\ngmake: *** [/opt/R/R-3.6.0/lib/R/etc/Makeconf:174: sentencepiece/src/char_model_trainer.o] Error 1\r\n```\r\n\r\nOn **Debian Linux, GCC, with Address Sanitizers (ASAN/UBSAN)**, I'm getting address sanitizer failures in protobuf-lite. See below:\r\n\r\n```\r\nsentencepiece_trainer.cc(116) LOG(INFO) Running command: --input=traindata.txt --model_prefix=sentencepiece --vocab_size=10 --character_coverage=0.9999 --model_type=char\r\nsentencepiece_trainer.cc(49) LOG(INFO) Starts training with : \r\nTrainerSpec {\r\n  input: traindata.txt\r\n  input_format: \r\n  model_prefix: sentencepiece\r\n  model_type: CHAR\r\n  vocab_size: 10\r\n  self_test_sample_size: 0\r\n  character_coverage: 0.9999\r\n  input_sentence_size: 0\r\n  shuffle_input_sentence: 1\r\n  seed_sentencepiece_size: 1000000\r\n  shrinking_factor: 0.75\r\n  max_sentence_length: 4192\r\n  num_threads: 16\r\n  num_sub_iterations: 2\r\n  max_sentencepiece_length: 16\r\n  split_by_unicode_script: 1\r\n  split_by_number: 1\r\n  split_by_whitespace: 1\r\n  treat_whitespace_as_suffix: 0\r\n  hard_vocab_limit: 1\r\n  use_all_vocab: 0\r\n  unk_id: 0\r\n  bos_id: 1\r\n  eos_id: 2\r\n  pad_id: -1\r\n  unk_piece: <unk>\r\n  bos_piece: <s>\r\n  eos_piece: </s>\r\n  pad_piece: <pad>\r\n  unk_surface:  \u2047 \r\n}\r\nNormalizerSpec {\r\n  name: nmt_nfkc\r\n  add_dummy_prefix: 1\r\n  remove_extra_whitespaces: 1\r\n  escape_whitespaces: 1\r\n  normalization_rule_tsv: \r\n}\r\n\r\ntrainer_interface.cc(267) LOG(INFO) Loading corpus: traindata.txt\r\ntrainer_interface.cc(287) LOG(WARNING) Found too long line (4825 > 4192).\r\ntrainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\r\ntrainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\r\ntrainer_interface.cc(315) LOG(INFO) Loaded all 3992 sentences\r\ntrainer_interface.cc(321) LOG(INFO) Skipped 8 too long sentences.\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: <s>\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: </s>\r\ntrainer_interface.cc(335) LOG(INFO) Normalizing sentences...\r\ntrainer_interface.cc(384) LOG(INFO) all chars count=2593880\r\ntrainer_interface.cc(392) LOG(INFO) Done: 99.9928% characters are covered.\r\ntrainer_interface.cc(402) LOG(INFO) Alphabet size=73\r\ntrainer_interface.cc(403) LOG(INFO) Final character coverage=0.999928\r\ntrainer_interface.cc(435) LOG(INFO) Done! preprocessed 3992 sentences.\r\ntrainer_interface.cc(507) LOG(INFO) Saving model: sentencepiece.model\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1537:35: runtime error: index 1 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1537:35: runtime error: index 3 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/repeated_field.cc:88:39: runtime error: index 4 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 1 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 1 out of bounds for type 'void *[1]'\r\ntrainer_interface.cc(531) LOG(INFO) Saving vocabs: sentencepiece.vocab\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1537:35: runtime error: index 1 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 9 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 1 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 1 out of bounds for type 'void *[1]'\r\n\r\n...\r\n\r\nsentencepiece_trainer.cc(116) LOG(INFO) Running command: --input=traindata.txt --model_prefix=sentencepiece --vocab_size=10 --character_coverage=0.9999 --model_type=bpe\r\nsentencepiece_trainer.cc(49) LOG(INFO) Starts training with : \r\nTrainerSpec {\r\n  input: traindata.txt\r\n  input_format: \r\n  model_prefix: sentencepiece\r\n  model_type: BPE\r\n  vocab_size: 10\r\n  self_test_sample_size: 0\r\n  character_coverage: 0.9999\r\n  input_sentence_size: 0\r\n  shuffle_input_sentence: 1\r\n  seed_sentencepiece_size: 1000000\r\n  shrinking_factor: 0.75\r\n  max_sentence_length: 4192\r\n  num_threads: 16\r\n  num_sub_iterations: 2\r\n  max_sentencepiece_length: 16\r\n  split_by_unicode_script: 1\r\n  split_by_number: 1\r\n  split_by_whitespace: 1\r\n  treat_whitespace_as_suffix: 0\r\n  hard_vocab_limit: 1\r\n  use_all_vocab: 0\r\n  unk_id: 0\r\n  bos_id: 1\r\n  eos_id: 2\r\n  pad_id: -1\r\n  unk_piece: <unk>\r\n  bos_piece: <s>\r\n  eos_piece: </s>\r\n  pad_piece: <pad>\r\n  unk_surface:  \u2047 \r\n}\r\nNormalizerSpec {\r\n  name: nmt_nfkc\r\n  add_dummy_prefix: 1\r\n  remove_extra_whitespaces: 1\r\n  escape_whitespaces: 1\r\n  normalization_rule_tsv: \r\n}\r\n\r\ntrainer_interface.cc(267) LOG(INFO) Loading corpus: traindata.txt\r\ntrainer_interface.cc(287) LOG(WARNING) Found too long line (4825 > 4192).\r\ntrainer_interface.cc(289) LOG(WARNING) Too long lines are skipped in the training.\r\ntrainer_interface.cc(290) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\r\ntrainer_interface.cc(315) LOG(INFO) Loaded all 3992 sentences\r\ntrainer_interface.cc(321) LOG(INFO) Skipped 8 too long sentences.\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: <s>\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: </s>\r\ntrainer_interface.cc(335) LOG(INFO) Normalizing sentences...\r\ntrainer_interface.cc(384) LOG(INFO) all chars count=2593880\r\ntrainer_interface.cc(392) LOG(INFO) Done: 99.9928% characters are covered.\r\ntrainer_interface.cc(402) LOG(INFO) Alphabet size=73\r\ntrainer_interface.cc(403) LOG(INFO) Final character coverage=0.999928\r\n...\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 110 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 110 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 89 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 89 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 269 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 269 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 248 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 248 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1507:28: runtime error: index 248 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1537:35: runtime error: index 1 out of bounds for type 'void *[1]'\r\nthird_party/protobuf-lite/google/protobuf/repeated_field.h:1537:35: runtime error: index 1 out of bounds for type 'void *[1]'\r\n```\r\n\r\n\r\nNext it would be nice if we don't need to compile with `-Wno-pedantic -Wno-misleading-indentation -Wno-sign-compare` as that makes the compiler pretty noisy. Could such warnings also be removed.\r\n\r\nthank you :)!\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/431", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/431/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/431/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/431/events", "html_url": "https://github.com/google/sentencepiece/issues/431", "id": 538207763, "node_id": "MDU6SXNzdWU1MzgyMDc3NjM=", "number": 431, "title": "Internal: trainer_interface.cc(540) output->WriteLine(os.str())", "user": {"login": "Saltychtao", "id": 9932507, "node_id": "MDQ6VXNlcjk5MzI1MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9932507?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Saltychtao", "html_url": "https://github.com/Saltychtao", "followers_url": "https://api.github.com/users/Saltychtao/followers", "following_url": "https://api.github.com/users/Saltychtao/following{/other_user}", "gists_url": "https://api.github.com/users/Saltychtao/gists{/gist_id}", "starred_url": "https://api.github.com/users/Saltychtao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Saltychtao/subscriptions", "organizations_url": "https://api.github.com/users/Saltychtao/orgs", "repos_url": "https://api.github.com/users/Saltychtao/repos", "events_url": "https://api.github.com/users/Saltychtao/events{/privacy}", "received_events_url": "https://api.github.com/users/Saltychtao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-16T06:14:38Z", "updated_at": "2020-01-03T06:52:56Z", "closed_at": "2020-01-03T06:52:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n I am using the bpe algorithm provided by the repository to segment the wikipedia text (about 5GB). The command I used is as follows:\r\n`./spm_train --input=wikiclean.txt --model_prefix=wikiclean --vocab_size=32000 --model_type=bpe\r\n`\r\n\r\nHowever, after a long time run, the program threw an error when saving vocab:\r\n```\r\nbpe_model_trainer.cc(257) LOG(INFO) Added: freq=2725 size=31920 all=2696220 active=135254 piece=\u2581showcasing\r\n\r\nbpe_model_trainer.cc(257) LOG(INFO) Added: freq=2721 size=31940 all=2696413 active=135447 piece=elan\r\n\r\ntrainer_interface.cc(507) LOG(INFO) Saving model: wikiclean.model\r\n\r\ntrainer_interface.cc(531) LOG(INFO) Saving vocabs: wikiclean.vocab\r\n\r\nspm_train_main.cc(178) [_status.ok()] Internal: sentencepiece-master/src/trainer_interface.cc(540) [\r\noutput->WriteLine(os.str())]\r\n\r\nProgram terminated with an unrecoverable error.\r\n```\r\nI tried both the C++ version and python version and both threw the same error. Would you tell me what is the problem?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/429", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/429/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/429/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/429/events", "html_url": "https://github.com/google/sentencepiece/issues/429", "id": 534555384, "node_id": "MDU6SXNzdWU1MzQ1NTUzODQ=", "number": 429, "title": "Python 3.8 wheels for Windows", "user": {"login": "ahtik", "id": 140952, "node_id": "MDQ6VXNlcjE0MDk1Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/140952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahtik", "html_url": "https://github.com/ahtik", "followers_url": "https://api.github.com/users/ahtik/followers", "following_url": "https://api.github.com/users/ahtik/following{/other_user}", "gists_url": "https://api.github.com/users/ahtik/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahtik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahtik/subscriptions", "organizations_url": "https://api.github.com/users/ahtik/orgs", "repos_url": "https://api.github.com/users/ahtik/repos", "events_url": "https://api.github.com/users/ahtik/events{/privacy}", "received_events_url": "https://api.github.com/users/ahtik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-08T15:12:23Z", "updated_at": "2019-12-16T16:02:57Z", "closed_at": "2019-12-16T16:02:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/google/sentencepiece/releases/tag/v0.1.84 has all the Windows wheels for Python 3.5, 3.6, 3.7, but is missing Python 3.8.\r\n\r\nLinux wheels for Python 3.8 do exist, so it's resuling in a bit of inconsistent installation experience between the platforms.\r\n\r\nPlease, would it be possible to include Python 3.8 Windows to the build matrix with the next release or even better, release the 3.8 Windows wheels for 0.1.84?\r\n\r\nHuggingface Transformers depends on sentencepiece and looks like sentencepiece wheel is now the final component before it gets the full Python 3.8 support also under Windows. https://github.com/huggingface/transformers/issues/1802", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/428", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/428/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/428/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/428/events", "html_url": "https://github.com/google/sentencepiece/issues/428", "id": 534161219, "node_id": "MDU6SXNzdWU1MzQxNjEyMTk=", "number": 428, "title": "R package wrapping sentencepiece at https://github.com/bnosac/sentencepiece", "user": {"login": "jwijffels", "id": 1710810, "node_id": "MDQ6VXNlcjE3MTA4MTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1710810?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwijffels", "html_url": "https://github.com/jwijffels", "followers_url": "https://api.github.com/users/jwijffels/followers", "following_url": "https://api.github.com/users/jwijffels/following{/other_user}", "gists_url": "https://api.github.com/users/jwijffels/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwijffels/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwijffels/subscriptions", "organizations_url": "https://api.github.com/users/jwijffels/orgs", "repos_url": "https://api.github.com/users/jwijffels/repos", "events_url": "https://api.github.com/users/jwijffels/events{/privacy}", "received_events_url": "https://api.github.com/users/jwijffels/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-12-06T17:25:01Z", "updated_at": "2019-12-16T10:27:23Z", "closed_at": "2019-12-16T10:27:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "FYI. I've made an R package around the C++ library. It is available at https://github.com/bnosac/sentencepiece", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/427", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/427/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/427/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/427/events", "html_url": "https://github.com/google/sentencepiece/issues/427", "id": 532038359, "node_id": "MDU6SXNzdWU1MzIwMzgzNTk=", "number": 427, "title": "[Question] Is NFKC normalization applied when a tsv rule is used?", "user": {"login": "ZJaume", "id": 11339330, "node_id": "MDQ6VXNlcjExMzM5MzMw", "avatar_url": "https://avatars1.githubusercontent.com/u/11339330?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZJaume", "html_url": "https://github.com/ZJaume", "followers_url": "https://api.github.com/users/ZJaume/followers", "following_url": "https://api.github.com/users/ZJaume/following{/other_user}", "gists_url": "https://api.github.com/users/ZJaume/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZJaume/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZJaume/subscriptions", "organizations_url": "https://api.github.com/users/ZJaume/orgs", "repos_url": "https://api.github.com/users/ZJaume/repos", "events_url": "https://api.github.com/users/ZJaume/events{/privacy}", "received_events_url": "https://api.github.com/users/ZJaume/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-03T14:48:36Z", "updated_at": "2019-12-05T11:23:32Z", "closed_at": "2019-12-05T11:23:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "I noticed that, when using `spm_normalize` with a custom normalization tsv file, the NFKC normalization is not applied. If I use the custom rule on the training parameters, are both normalizations applied?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/426", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/426/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/426/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/426/events", "html_url": "https://github.com/google/sentencepiece/issues/426", "id": 525102941, "node_id": "MDU6SXNzdWU1MjUxMDI5NDE=", "number": 426, "title": "How to extend tokens dictionary?", "user": {"login": "kpe", "id": 2535923, "node_id": "MDQ6VXNlcjI1MzU5MjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2535923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kpe", "html_url": "https://github.com/kpe", "followers_url": "https://api.github.com/users/kpe/followers", "following_url": "https://api.github.com/users/kpe/following{/other_user}", "gists_url": "https://api.github.com/users/kpe/gists{/gist_id}", "starred_url": "https://api.github.com/users/kpe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kpe/subscriptions", "organizations_url": "https://api.github.com/users/kpe/orgs", "repos_url": "https://api.github.com/users/kpe/repos", "events_url": "https://api.github.com/users/kpe/events{/privacy}", "received_events_url": "https://api.github.com/users/kpe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-11-19T16:00:43Z", "updated_at": "2019-12-07T09:22:20Z", "closed_at": "2019-11-20T01:45:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "It would be nice if an existing sentencepiece model could be extended with additional extra special/`[MARKUP]` tokens.\r\n\r\nWhile adding real language tokens might not be needed (or possible), adding additional atomic markup tokens to an existing model, should be possible right?\r\n\r\nAs an usage scenario - imagine you have a plain text pre-trained model, and you want to handle some HTML-like document, i.e. you need to add special markup/atomic tokens like - `[<P>]`, `[<TABLE>]`, etc.\r\n\r\nThis is currently not possible, right?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/425", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/425/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/425/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/425/events", "html_url": "https://github.com/google/sentencepiece/issues/425", "id": 525097364, "node_id": "MDU6SXNzdWU1MjUwOTczNjQ=", "number": 425, "title": "do_lower_case in the sentencepiece model files", "user": {"login": "kpe", "id": 2535923, "node_id": "MDQ6VXNlcjI1MzU5MjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2535923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kpe", "html_url": "https://github.com/kpe", "followers_url": "https://api.github.com/users/kpe/followers", "following_url": "https://api.github.com/users/kpe/following{/other_user}", "gists_url": "https://api.github.com/users/kpe/gists{/gist_id}", "starred_url": "https://api.github.com/users/kpe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kpe/subscriptions", "organizations_url": "https://api.github.com/users/kpe/orgs", "repos_url": "https://api.github.com/users/kpe/repos", "events_url": "https://api.github.com/users/kpe/events{/privacy}", "received_events_url": "https://api.github.com/users/kpe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-19T15:52:01Z", "updated_at": "2019-11-20T09:26:42Z", "closed_at": "2019-11-20T01:51:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm wondering if it is a good idea to include a `do_lower_case` flag in the model files (as kind of minimal pre-processing instruction of how to prepare the input)?\r\n\r\nAs an alternative, one could take care that the `tokenize()` method automatically does `lower()` case for models trained on low case (instead of returning `<unk>` for all upper case characters).\r\n\r\nCurrently the flag of whether lower case pre-processing is needed is not contained in the model, thus those sentencepiece model are not self contained (one of the motivations of this wonderful project, right?).\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/424", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/424/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/424/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/424/events", "html_url": "https://github.com/google/sentencepiece/issues/424", "id": 524742654, "node_id": "MDU6SXNzdWU1MjQ3NDI2NTQ=", "number": 424, "title": "brew install sentencepiece", "user": {"login": "ankane", "id": 220358, "node_id": "MDQ6VXNlcjIyMDM1OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/220358?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankane", "html_url": "https://github.com/ankane", "followers_url": "https://api.github.com/users/ankane/followers", "following_url": "https://api.github.com/users/ankane/following{/other_user}", "gists_url": "https://api.github.com/users/ankane/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankane/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankane/subscriptions", "organizations_url": "https://api.github.com/users/ankane/orgs", "repos_url": "https://api.github.com/users/ankane/repos", "events_url": "https://api.github.com/users/ankane/events{/privacy}", "received_events_url": "https://api.github.com/users/ankane/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-19T03:00:25Z", "updated_at": "2019-11-20T08:18:12Z", "closed_at": "2019-11-20T08:18:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, just wanted to let you know that Mac users can now use Homebrew to install SentencePiece.\r\n\r\n```sh\r\nbrew install sentencepiece\r\n```\r\n\r\nHomebrew precompiles libraries for each supported version of macOS, so installation is pretty much instant after it downloads.\r\n\r\nRef: https://github.com/Homebrew/homebrew-core/pull/46853", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/423", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/423/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/423/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/423/events", "html_url": "https://github.com/google/sentencepiece/issues/423", "id": 523387556, "node_id": "MDU6SXNzdWU1MjMzODc1NTY=", "number": 423, "title": "Whitespace scaping", "user": {"login": "carlosep93", "id": 5550934, "node_id": "MDQ6VXNlcjU1NTA5MzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/5550934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlosep93", "html_url": "https://github.com/carlosep93", "followers_url": "https://api.github.com/users/carlosep93/followers", "following_url": "https://api.github.com/users/carlosep93/following{/other_user}", "gists_url": "https://api.github.com/users/carlosep93/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlosep93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlosep93/subscriptions", "organizations_url": "https://api.github.com/users/carlosep93/orgs", "repos_url": "https://api.github.com/users/carlosep93/repos", "events_url": "https://api.github.com/users/carlosep93/events{/privacy}", "received_events_url": "https://api.github.com/users/carlosep93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-15T10:29:15Z", "updated_at": "2019-11-18T14:28:47Z", "closed_at": "2019-11-18T14:28:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI'm trying to employ sentence piece in a project but I would need subwords to belong to just one original token, like bpe does.\r\nIt is supported by the library?\r\n\r\nThanks in advance,\r\n\r\nCarlos", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/422", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/422/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/422/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/422/events", "html_url": "https://github.com/google/sentencepiece/issues/422", "id": 521916815, "node_id": "MDU6SXNzdWU1MjE5MTY4MTU=", "number": 422, "title": "how to save sentencepiece model (except training time)", "user": {"login": "eagle705", "id": 7252598, "node_id": "MDQ6VXNlcjcyNTI1OTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/7252598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eagle705", "html_url": "https://github.com/eagle705", "followers_url": "https://api.github.com/users/eagle705/followers", "following_url": "https://api.github.com/users/eagle705/following{/other_user}", "gists_url": "https://api.github.com/users/eagle705/gists{/gist_id}", "starred_url": "https://api.github.com/users/eagle705/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eagle705/subscriptions", "organizations_url": "https://api.github.com/users/eagle705/orgs", "repos_url": "https://api.github.com/users/eagle705/repos", "events_url": "https://api.github.com/users/eagle705/events{/privacy}", "received_events_url": "https://api.github.com/users/eagle705/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-13T02:24:11Z", "updated_at": "2019-11-16T07:12:03Z", "closed_at": "2019-11-16T07:12:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "hi, \r\n\r\nAs I know, spm is saved when I train the model.\r\n\r\nbut except that case,  I wonder how to save sentencepiece model. As I know, it doesn't support api.\r\n\r\nthe reason I want to know is I use sentencepiece model trained from gluonnlp wrapper \r\n\r\n```from gluonnlp.data import SentencepieceTokenizer```\r\n\r\nBut when it doesn't work in some case (e.g. low spec cpu).\r\n\r\nSo I want to load it in my laptop and save pure sentencepiece object.\r\n\r\nis there any way to do that?\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/421", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/421/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/421/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/421/events", "html_url": "https://github.com/google/sentencepiece/issues/421", "id": 521501551, "node_id": "MDU6SXNzdWU1MjE1MDE1NTE=", "number": 421, "title": "Not merging accents with alphabets when running on normalized sentences", "user": {"login": "zaemyung", "id": 3746478, "node_id": "MDQ6VXNlcjM3NDY0Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3746478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zaemyung", "html_url": "https://github.com/zaemyung", "followers_url": "https://api.github.com/users/zaemyung/followers", "following_url": "https://api.github.com/users/zaemyung/following{/other_user}", "gists_url": "https://api.github.com/users/zaemyung/gists{/gist_id}", "starred_url": "https://api.github.com/users/zaemyung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zaemyung/subscriptions", "organizations_url": "https://api.github.com/users/zaemyung/orgs", "repos_url": "https://api.github.com/users/zaemyung/repos", "events_url": "https://api.github.com/users/zaemyung/events{/privacy}", "received_events_url": "https://api.github.com/users/zaemyung/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-12T11:46:21Z", "updated_at": "2019-11-18T06:29:21Z", "closed_at": "2019-11-18T06:29:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nThank you for creating and maintaining this awesome library!\r\n\r\nI have been trying to train a BPE model on NFD-normalized sentences.\r\nFor example, the sentence, `Madame la Pr\u00e9sidente, c'est une motion de proc\u00e9dure.` is now decomposed to 54 characters:\r\n```\r\n0 M\r\n1 a\r\n2 d\r\n3 a\r\n4 m\r\n5 e\r\n6\r\n7 l\r\n8 a\r\n9\r\n10 P\r\n11 r\r\n12 e\r\n13 \u0301\r\n14 s\r\n15 i\r\n16 d\r\n17 e\r\n18 n\r\n19 t\r\n20 e\r\n21 ,\r\n22\r\n23 c\r\n24 '\r\n25 e\r\n26 s\r\n27 t\r\n28\r\n29 u\r\n30 n\r\n31 e\r\n32\r\n33 m\r\n34 o\r\n35 t\r\n36 i\r\n37 o\r\n38 n\r\n39\r\n40 d\r\n41 e\r\n42\r\n43 p\r\n44 r\r\n45 o\r\n46 c\r\n47 e\r\n48 \u0301\r\n49 d\r\n50 u\r\n51 r\r\n52 e\r\n53 .\r\n```\r\nAnd because I want those accented characters not to be merged back with the alphabets (to form single characters), I set the `--normalization-rule` to `identity` when training the BPE model.\r\nAfter training, however, I noticed that the accented characters are never merged to form longer BPE tokens, and always exists as single characters:\r\n```\r\n0 \u2581\r\n1 M\r\n2 a\r\n3 d\r\n4 a\r\n5 m\r\n6 e\r\n7\r\n8 \u2581\r\n9 l\r\n10 a\r\n11\r\n12 \u2581\r\n13 P\r\n14 r\r\n15 e\r\n16\r\n17 \u0301\r\n18\r\n19 s\r\n20 i\r\n21 d\r\n22 e\r\n23 n\r\n24 t\r\n25 e\r\n26\r\n27 ,\r\n28\r\n29 \u2581\r\n30 c\r\n31\r\n32 '\r\n33\r\n34 e\r\n35 s\r\n36 t\r\n37\r\n38 \u2581\r\n39 u\r\n40 n\r\n41 e\r\n42\r\n43 \u2581\r\n44 m\r\n45 o\r\n46 t\r\n47 i\r\n48 o\r\n49 n\r\n50\r\n51 \u2581\r\n52 d\r\n53 e\r\n54\r\n55 \u2581\r\n56 p\r\n57 r\r\n58 o\r\n59 c\r\n60 e\r\n61\r\n62 \u0301\r\n63\r\n64 d\r\n65 u\r\n66 r\r\n67 e\r\n68\r\n69 .\r\n```\r\nLooking at the example above, the acute accent at 17th position is a separate single token.\r\nI would expect that the characters from 12th to 25th (or at least to 17th) positions should be merged into one token.\r\nWould there a way to do so?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/419", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/419/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/419/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/419/events", "html_url": "https://github.com/google/sentencepiece/issues/419", "id": 518708614, "node_id": "MDU6SXNzdWU1MTg3MDg2MTQ=", "number": 419, "title": "SentencePieceTrainer.Train does not work with filepaths containing blank spaces", "user": {"login": "tony-lung-centric", "id": 55240369, "node_id": "MDQ6VXNlcjU1MjQwMzY5", "avatar_url": "https://avatars2.githubusercontent.com/u/55240369?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tony-lung-centric", "html_url": "https://github.com/tony-lung-centric", "followers_url": "https://api.github.com/users/tony-lung-centric/followers", "following_url": "https://api.github.com/users/tony-lung-centric/following{/other_user}", "gists_url": "https://api.github.com/users/tony-lung-centric/gists{/gist_id}", "starred_url": "https://api.github.com/users/tony-lung-centric/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tony-lung-centric/subscriptions", "organizations_url": "https://api.github.com/users/tony-lung-centric/orgs", "repos_url": "https://api.github.com/users/tony-lung-centric/repos", "events_url": "https://api.github.com/users/tony-lung-centric/events{/privacy}", "received_events_url": "https://api.github.com/users/tony-lung-centric/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-06T20:14:40Z", "updated_at": "2020-05-13T04:19:57Z", "closed_at": "2020-05-13T04:19:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "On 11/6/2019 I did a pip install sentencepiece from the Anconda distribution 3.7.4. The following line of code (I know it's incomplete, but it's enough to illustrate and replicate the command parsing issue.):\r\n\r\n`sentencepiece.SentencePieceTrainer.Train('--input=MY FILE.TXT')`\r\n\r\nresults in the following error:\r\n> OSError: Not found: unknown field name \"FILE.TXT\" in TrainerSpec.\r\n\r\nI tried embedding quotes around the filepath like this:\r\n`sentencepiece.SentencePieceTrainer.Train('--input=\"MY FILE.TXT\"')`\r\n\r\nresulting in the following error (note the embedded trailing double-quote is treated as part of string):\r\n> OSError: Not found: unknown field name \"FILE.TXT\"\" in TrainerSpec.\r\n\r\nI had similar results using various other attempts to embed escaped and unescaped delimiters around the filename. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/418", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/418/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/418/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/418/events", "html_url": "https://github.com/google/sentencepiece/issues/418", "id": 516778419, "node_id": "MDU6SXNzdWU1MTY3Nzg0MTk=", "number": 418, "title": "0.1.84 PyPI release", "user": {"login": "blaizeberry4", "id": 19916224, "node_id": "MDQ6VXNlcjE5OTE2MjI0", "avatar_url": "https://avatars2.githubusercontent.com/u/19916224?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blaizeberry4", "html_url": "https://github.com/blaizeberry4", "followers_url": "https://api.github.com/users/blaizeberry4/followers", "following_url": "https://api.github.com/users/blaizeberry4/following{/other_user}", "gists_url": "https://api.github.com/users/blaizeberry4/gists{/gist_id}", "starred_url": "https://api.github.com/users/blaizeberry4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blaizeberry4/subscriptions", "organizations_url": "https://api.github.com/users/blaizeberry4/orgs", "repos_url": "https://api.github.com/users/blaizeberry4/repos", "events_url": "https://api.github.com/users/blaizeberry4/events{/privacy}", "received_events_url": "https://api.github.com/users/blaizeberry4/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-03T04:11:42Z", "updated_at": "2019-12-16T16:03:46Z", "closed_at": "2019-12-16T16:03:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Wondering when version 0.1.84 will be released to PyPI as from the [tf-sentencepiece PyPI history](https://pypi.org/project/tf-sentencepiece/#history), it appears the most recent version is still 0.1.83 for both `sentencepiece` and `tf-sentencepiece` on PyPI. From comparing the Github release history and PyPI release history, it seems that the release date has typically been the same, so curious if the lack of accessibility via PyPI is intended. Thanks in advance for the assistance!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/415", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/415/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/415/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/415/events", "html_url": "https://github.com/google/sentencepiece/issues/415", "id": 514454036, "node_id": "MDU6SXNzdWU1MTQ0NTQwMzY=", "number": 415, "title": "Optimal vocabulary size", "user": {"login": "venkatesh-parvathala", "id": 46398879, "node_id": "MDQ6VXNlcjQ2Mzk4ODc5", "avatar_url": "https://avatars3.githubusercontent.com/u/46398879?v=4", "gravatar_id": "", "url": "https://api.github.com/users/venkatesh-parvathala", "html_url": "https://github.com/venkatesh-parvathala", "followers_url": "https://api.github.com/users/venkatesh-parvathala/followers", "following_url": "https://api.github.com/users/venkatesh-parvathala/following{/other_user}", "gists_url": "https://api.github.com/users/venkatesh-parvathala/gists{/gist_id}", "starred_url": "https://api.github.com/users/venkatesh-parvathala/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/venkatesh-parvathala/subscriptions", "organizations_url": "https://api.github.com/users/venkatesh-parvathala/orgs", "repos_url": "https://api.github.com/users/venkatesh-parvathala/repos", "events_url": "https://api.github.com/users/venkatesh-parvathala/events{/privacy}", "received_events_url": "https://api.github.com/users/venkatesh-parvathala/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-10-30T07:31:43Z", "updated_at": "2020-01-24T02:10:33Z", "closed_at": "2019-11-06T06:28:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "what might be the optimal vocabulary size for a raw text file with 50,000 unique words in it", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/414", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/414/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/414/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/414/events", "html_url": "https://github.com/google/sentencepiece/issues/414", "id": 510915115, "node_id": "MDU6SXNzdWU1MTA5MTUxMTU=", "number": 414, "title": "Request:  Tensorflow 1.15.0 support", "user": {"login": "gobrewers14", "id": 6145862, "node_id": "MDQ6VXNlcjYxNDU4NjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6145862?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gobrewers14", "html_url": "https://github.com/gobrewers14", "followers_url": "https://api.github.com/users/gobrewers14/followers", "following_url": "https://api.github.com/users/gobrewers14/following{/other_user}", "gists_url": "https://api.github.com/users/gobrewers14/gists{/gist_id}", "starred_url": "https://api.github.com/users/gobrewers14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gobrewers14/subscriptions", "organizations_url": "https://api.github.com/users/gobrewers14/orgs", "repos_url": "https://api.github.com/users/gobrewers14/repos", "events_url": "https://api.github.com/users/gobrewers14/events{/privacy}", "received_events_url": "https://api.github.com/users/gobrewers14/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-22T21:07:23Z", "updated_at": "2019-12-16T16:03:11Z", "closed_at": "2019-12-16T16:03:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Wondering if we could get a wheel built for tensorflow 1.15.0\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/413", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/413/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/413/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/413/events", "html_url": "https://github.com/google/sentencepiece/issues/413", "id": 509901925, "node_id": "MDU6SXNzdWU1MDk5MDE5MjU=", "number": 413, "title": "ERR on windows   when pip install sentencepiece  ", "user": {"login": "zhengya01", "id": 43601548, "node_id": "MDQ6VXNlcjQzNjAxNTQ4", "avatar_url": "https://avatars3.githubusercontent.com/u/43601548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhengya01", "html_url": "https://github.com/zhengya01", "followers_url": "https://api.github.com/users/zhengya01/followers", "following_url": "https://api.github.com/users/zhengya01/following{/other_user}", "gists_url": "https://api.github.com/users/zhengya01/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhengya01/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhengya01/subscriptions", "organizations_url": "https://api.github.com/users/zhengya01/orgs", "repos_url": "https://api.github.com/users/zhengya01/repos", "events_url": "https://api.github.com/users/zhengya01/events{/privacy}", "received_events_url": "https://api.github.com/users/zhengya01/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-21T11:28:14Z", "updated_at": "2019-12-16T16:05:17Z", "closed_at": "2019-12-16T16:05:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "I got a error when pip install sentencepiece, and my OS is windows, \r\nsentencepiece python API  not support windows ??\r\n\r\n\r\nFile \"setup.py\", line 29, in <module>\r\n  with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8')  as f:\r\nFile \"C:\\Python35\\lib\\codecs.py\" line 895, in open\r\n  file = builtins.open(filename, mode, buffering)\r\nFileNotFoundError: [Error 2] No such file or directory: '..\\\\VERSION'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/412", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/412/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/412/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/412/events", "html_url": "https://github.com/google/sentencepiece/issues/412", "id": 509104814, "node_id": "MDU6SXNzdWU1MDkxMDQ4MTQ=", "number": 412, "title": "Regarding `character_coverage`", "user": {"login": "ArbinTimilsina", "id": 18752223, "node_id": "MDQ6VXNlcjE4NzUyMjIz", "avatar_url": "https://avatars3.githubusercontent.com/u/18752223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArbinTimilsina", "html_url": "https://github.com/ArbinTimilsina", "followers_url": "https://api.github.com/users/ArbinTimilsina/followers", "following_url": "https://api.github.com/users/ArbinTimilsina/following{/other_user}", "gists_url": "https://api.github.com/users/ArbinTimilsina/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArbinTimilsina/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArbinTimilsina/subscriptions", "organizations_url": "https://api.github.com/users/ArbinTimilsina/orgs", "repos_url": "https://api.github.com/users/ArbinTimilsina/repos", "events_url": "https://api.github.com/users/ArbinTimilsina/events{/privacy}", "received_events_url": "https://api.github.com/users/ArbinTimilsina/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-18T14:08:00Z", "updated_at": "2019-10-25T18:00:52Z", "closed_at": "2019-10-25T18:00:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to understand the default value of **0.9995** for `character_coverage`. If I have 100 unique characters, what fraction would be covered by this number? Or in another word, what is the difference between using **0.9995** vs. **0.9998**?\r\n\r\nThanks in advance.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/411", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/411/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/411/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/411/events", "html_url": "https://github.com/google/sentencepiece/issues/411", "id": 507920877, "node_id": "MDU6SXNzdWU1MDc5MjA4Nzc=", "number": 411, "title": "problem installing sentencepiece from source", "user": {"login": "winobes", "id": 3182514, "node_id": "MDQ6VXNlcjMxODI1MTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/3182514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/winobes", "html_url": "https://github.com/winobes", "followers_url": "https://api.github.com/users/winobes/followers", "following_url": "https://api.github.com/users/winobes/following{/other_user}", "gists_url": "https://api.github.com/users/winobes/gists{/gist_id}", "starred_url": "https://api.github.com/users/winobes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/winobes/subscriptions", "organizations_url": "https://api.github.com/users/winobes/orgs", "repos_url": "https://api.github.com/users/winobes/repos", "events_url": "https://api.github.com/users/winobes/events{/privacy}", "received_events_url": "https://api.github.com/users/winobes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-10-16T15:17:49Z", "updated_at": "2020-04-22T13:36:25Z", "closed_at": "2019-12-16T16:04:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI encounter the following error installing sentencepiece from source:\r\n\r\n```\r\n$ pip install sentencepiece --no-binary :all:\r\nCollecting sentencepiece\r\n  Using cached https://files.pythonhosted.org/packages/1b/87/c3c2fa8cbec61fffe031ca9f0da512747520bec9be7f886f748457daac31/sentencepiece-0.1.83.tar.gz\r\n    ERROR: Command errored out with exit status 1:\r\n     command: ~/tmp-env/bin/python3.7 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/cp/kh9797c57j938rxn3wj9kscm0000gp/T/pip-install-04w9v7zw/sentencepiece/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/cp/kh9797c57j938rxn3wj9kscm0000gp/T/pip-install-04w9v7zw/sentencepiece/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /private/var/folders/cp/kh9797c57j938rxn3wj9kscm0000gp/T/pip-install-04w9v7zw/sentencepiece/pip-egg-info\r\n         cwd: /private/var/folders/cp/kh9797c57j938rxn3wj9kscm0000gp/T/pip-install-04w9v7zw/sentencepiece/\r\n    Complete output (7 lines):\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/private/var/folders/cp/kh9797c57j938rxn3wj9kscm0000gp/T/pip-install-04w9v7zw/sentencepiece/setup.py\", line 29, in <module>\r\n        with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:\r\n      File \"~/tmp-env/bin/../lib/python3.7/codecs.py\", line 904, in open\r\n        file = builtins.open(filename, mode, buffering)\r\n    FileNotFoundError: [Errno 2] No such file or directory: '../VERSION'\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n```\r\n\r\nIt seems that the pypi source distribution is missing `VERSION`\r\n\r\nunfortunately, i need to install it from source because my distribution isn't compatible with manylinux1", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/410", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/410/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/410/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/410/events", "html_url": "https://github.com/google/sentencepiece/issues/410", "id": 505457842, "node_id": "MDU6SXNzdWU1MDU0NTc4NDI=", "number": 410, "title": "Fails to load latest version of .so when module installed in user dir", "user": {"login": "mikaelhg", "id": 264506, "node_id": "MDQ6VXNlcjI2NDUwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/264506?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikaelhg", "html_url": "https://github.com/mikaelhg", "followers_url": "https://api.github.com/users/mikaelhg/followers", "following_url": "https://api.github.com/users/mikaelhg/following{/other_user}", "gists_url": "https://api.github.com/users/mikaelhg/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikaelhg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikaelhg/subscriptions", "organizations_url": "https://api.github.com/users/mikaelhg/orgs", "repos_url": "https://api.github.com/users/mikaelhg/repos", "events_url": "https://api.github.com/users/mikaelhg/events{/privacy}", "received_events_url": "https://api.github.com/users/mikaelhg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-10T19:11:26Z", "updated_at": "2019-12-25T00:18:09Z", "closed_at": "2019-12-25T00:18:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "In `sentencepiece_processor_ops.py`, row 47\r\n\r\n```python\r\n_gen_sentencepiece_processor_op = tf.load_op_library(so_file)\r\n```\r\n\r\nattempts to load the latest shared library.\r\n\r\nHowever, the 2.0.0 library hasn't been published yet, so the row tries to load the 2.0.0-beta1 .so.\r\n\r\nWhen the `tf_sentencepiece` package has been installed in the user directory with the command\r\n\r\n```\r\npip3 install --user tf_sentencepiece\r\n```\r\n\r\nthe install path unfortunately has the Python version number in it, so the row 38 of `sentencepiece_processor_ops.py`\r\n\r\n```python\r\n      re.search('[0-9]+\\.[0-9\\.]+.*$', n).group(0)\r\n```\r\n\r\ngrabs something like `$HOME/.local/lib/python3.6/site-packages/tf_sentencepiece/_sentencepiece_processor_ops.so.3.6/site-packages/tf_sentencepiece/_sentencepiece_processor_ops.so.2.0.0-beta1`, which won't resolve to a .so.\r\n\r\nIf you replace that row with a slightly friendlier\r\n\r\n```python\r\n      re.search('so.([0-9]+\\.[0-9\\.]+.*)$', n).group(1)\r\n```\r\n\r\nthings go smoothly once again.\r\n\r\n(Well, the execution ended up in a SIGSEGV, but at least that happened somewhere else, and the .so was loaded.)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/409", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/409/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/409/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/409/events", "html_url": "https://github.com/google/sentencepiece/issues/409", "id": 503951751, "node_id": "MDU6SXNzdWU1MDM5NTE3NTE=", "number": 409, "title": "Pip install does not accept symbol flags", "user": {"login": "Fhrozen", "id": 11988996, "node_id": "MDQ6VXNlcjExOTg4OTk2", "avatar_url": "https://avatars3.githubusercontent.com/u/11988996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Fhrozen", "html_url": "https://github.com/Fhrozen", "followers_url": "https://api.github.com/users/Fhrozen/followers", "following_url": "https://api.github.com/users/Fhrozen/following{/other_user}", "gists_url": "https://api.github.com/users/Fhrozen/gists{/gist_id}", "starred_url": "https://api.github.com/users/Fhrozen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Fhrozen/subscriptions", "organizations_url": "https://api.github.com/users/Fhrozen/orgs", "repos_url": "https://api.github.com/users/Fhrozen/repos", "events_url": "https://api.github.com/users/Fhrozen/events{/privacy}", "received_events_url": "https://api.github.com/users/Fhrozen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-10-08T10:07:45Z", "updated_at": "2019-10-08T10:54:13Z", "closed_at": "2019-10-08T10:54:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, there\r\n\r\nThank you very much for such a nice work.\r\nI am being working with sentencepiece installed from pip and just realized that it does not accept `--user_defined_symbols` or `--control_symbols`. Showing the following error:\r\n```\r\nsentencepiece_trainer.cc(116) LOG(INFO) Running command: --input=data/lang_char/input.txt --vocab_size=2000 --model_type=unigram --model_prefix=data/lang_char/train_si284_unigram2000 '--user_defined_symbols=<*IN*>,<*MR.*>,<NOISE>' --input_sentence_size=100000000\r\nTraceback (most recent call last):\r\n  File \"/export/db/transf/egs/wsj/asr2/../../../utils/spm_train\", line 17, in <module>\r\n    spm.SentencePieceTrainer.Train(\" \".join(map(shlex.quote, sys.argv[1:])))\r\nOSError: Not found: unknown field name \"'--user_defined_symbols\" in TrainerSpec.\r\n```\r\nif I just print the options i got:\r\n```\r\nTrainerSpec {\r\n  input_format: \r\n  model_prefix: \r\n  model_type: UNIGRAM\r\n  vocab_size: 8000\r\n  self_test_sample_size: 0\r\n  character_coverage: 0.9995\r\n  input_sentence_size: 0\r\n  shuffle_input_sentence: 1\r\n  seed_sentencepiece_size: 1000000\r\n  shrinking_factor: 0.75\r\n  max_sentence_length: 4192\r\n  num_threads: 16\r\n  num_sub_iterations: 2\r\n  max_sentencepiece_length: 16\r\n  split_by_unicode_script: 1\r\n  split_by_number: 1\r\n  split_by_whitespace: 1\r\n  treat_whitespace_as_suffix: 0\r\n  hard_vocab_limit: 1\r\n  use_all_vocab: 0\r\n  unk_id: 0\r\n  bos_id: 1\r\n  eos_id: 2\r\n  pad_id: -1\r\n  unk_piece: <unk>\r\n  bos_piece: <s>\r\n  eos_piece: </s>\r\n  pad_piece: <pad>\r\n  unk_surface:  \u2047 \r\n}\r\nNormalizerSpec {\r\n  name: nmt_nfkc\r\n  add_dummy_prefix: 1\r\n  remove_extra_whitespaces: 1\r\n  escape_whitespaces: 1\r\n  normalization_rule_tsv: \r\n}\r\n```\r\n\r\nand does not show the required flags, but if I test `spm_train.py` built from the source it displays the flags:\r\n```\r\n   --use_all_vocab (If set to true, use all tokens as vocab. Valid for word/char models.)  type: bool  default: false\r\n   --user_defined_symbols (comma separated list of user defined symbols)  type: string  default: \r\n```\r\nI am wondering if the .so file probably has not been updated and so the flags are not available. \r\n\r\nBTW, I was trying to make a custom installation for python but it requires to be installed with sudo. I will be nice if there is available and custom installation flag (?).\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/407", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/407/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/407/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/407/events", "html_url": "https://github.com/google/sentencepiece/issues/407", "id": 502667544, "node_id": "MDU6SXNzdWU1MDI2Njc1NDQ=", "number": 407, "title": "Installation fails on a Power system", "user": {"login": "apokayi", "id": 13006191, "node_id": "MDQ6VXNlcjEzMDA2MTkx", "avatar_url": "https://avatars1.githubusercontent.com/u/13006191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apokayi", "html_url": "https://github.com/apokayi", "followers_url": "https://api.github.com/users/apokayi/followers", "following_url": "https://api.github.com/users/apokayi/following{/other_user}", "gists_url": "https://api.github.com/users/apokayi/gists{/gist_id}", "starred_url": "https://api.github.com/users/apokayi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apokayi/subscriptions", "organizations_url": "https://api.github.com/users/apokayi/orgs", "repos_url": "https://api.github.com/users/apokayi/repos", "events_url": "https://api.github.com/users/apokayi/events{/privacy}", "received_events_url": "https://api.github.com/users/apokayi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-04T14:24:39Z", "updated_at": "2019-11-06T06:32:00Z", "closed_at": "2019-11-06T06:32:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to install the package from source since I wasn't able to find a pre-built package for Power.\r\n\r\nI followed the instructions and during make I am getting these errors:\r\n\r\n```\r\n...\r\n[ 86%] Linking CXX executable spm_export_vocab\r\nlibsentencepiece.so.0.0.0: undefined reference to `__atomic_store'\r\nmake[2]: *** [src/spm_export_vocab] Error 1\r\nmake[1]: *** [src/CMakeFiles/spm_export_vocab.dir/all] Error 2\r\nmake[1]: *** Waiting for unfinished jobs....\r\n[ 87%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/unicode_script.cc.o\r\n/gpfs/wscgpfs01/kayi/codes/sentencepiece/src/builder.cc:43:15: warning: unused variable 'kMaxUnicode' [-Wunused-const-variable]\r\nconstexpr int kMaxUnicode = 0x10FFFF;\r\n              ^\r\n/gpfs/wscgpfs01/kayi/codes/sentencepiece/src/builder.cc:45:23: warning: unused variable 'kDefaultNormalizerName' [-Wunused-const-variable]\r\nstatic constexpr char kDefaultNormalizerName[] = \"nfkc\";\r\n                      ^\r\n[ 88%] Linking CXX executable spm_decode\r\nlibsentencepiece.so.0.0.0: undefined reference to `__atomic_store'\r\nmake[2]: *** [src/spm_decode] Error 1\r\nmake[1]: *** [src/CMakeFiles/spm_decode.dir/all] Error 2\r\n[ 89%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_factory.cc.o\r\n2 warnings generated.\r\n[ 90%] Linking CXX executable spm_encode\r\nlibsentencepiece.so.0.0.0: undefined reference to `__atomic_store'\r\nmake[2]: *** [src/spm_encode] Error 1\r\nmake[1]: *** [src/CMakeFiles/spm_encode.dir/all] Error 2\r\n[ 90%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/trainer_interface.cc.o\r\n[ 91%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/unigram_model_trainer.cc.o\r\n[ 92%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/word_model_trainer.cc.o\r\n[ 93%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/char_model_trainer.cc.o\r\n[ 94%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/bpe_model_trainer.cc.o\r\n    1500-036: (I) The NOSTRICT option (default at OPT(3)) has the potential to alter the semantics of a program.  Please refer to documentation on the STRICT/NOSTRICT option for more information.\r\n[ 95%] Building CXX object src/CMakeFiles/sentencepiece_train.dir/sentencepiece_trainer.cc.o\r\n    1500-036: (I) The NOSTRICT option (default at OPT(3)) has the potential to alter the semantics of a program.  Please refer to documentation on the STRICT/NOSTRICT option for more information.\r\n[ 96%] Linking CXX shared library libsentencepiece_train.so\r\n[ 96%] Built target sentencepiece_train\r\nmake: *** [all] Error 2```\r\n\r\nIt might be due to x86 instrinsics. Is Power supported at all?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/406", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/406/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/406/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/406/events", "html_url": "https://github.com/google/sentencepiece/issues/406", "id": 500435435, "node_id": "MDU6SXNzdWU1MDA0MzU0MzU=", "number": 406, "title": "Explanation of encoding method", "user": {"login": "rmrao", "id": 6496605, "node_id": "MDQ6VXNlcjY0OTY2MDU=", "avatar_url": "https://avatars0.githubusercontent.com/u/6496605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmrao", "html_url": "https://github.com/rmrao", "followers_url": "https://api.github.com/users/rmrao/followers", "following_url": "https://api.github.com/users/rmrao/following{/other_user}", "gists_url": "https://api.github.com/users/rmrao/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmrao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmrao/subscriptions", "organizations_url": "https://api.github.com/users/rmrao/orgs", "repos_url": "https://api.github.com/users/rmrao/repos", "events_url": "https://api.github.com/users/rmrao/events{/privacy}", "received_events_url": "https://api.github.com/users/rmrao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-30T18:10:57Z", "updated_at": "2019-10-02T01:35:57Z", "closed_at": "2019-10-02T01:35:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to understand the code in [bpe_model.cc](https://github.com/google/sentencepiece/blob/master/src/bpe_model.cc). From explanations I've found on the internet, BPE encodes a string in a greedy fashion, replacing the longest possible subsequence first. However, your code seems to use a priority queue and recursive resegmentation? Does this implement a fundamentally different approach to encoding, or is this simply for speed? If you could point me to a paper that explains this, that would be great - it doesn't seem to be explained in the SentencePiece paper, unless I'm missing something.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/405", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/405/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/405/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/405/events", "html_url": "https://github.com/google/sentencepiece/issues/405", "id": 500364191, "node_id": "MDU6SXNzdWU1MDAzNjQxOTE=", "number": 405, "title": "terminate called after throwing an instance of 'std::bad_alloc'", "user": {"login": "pstjohn", "id": 2576846, "node_id": "MDQ6VXNlcjI1NzY4NDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/2576846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pstjohn", "html_url": "https://github.com/pstjohn", "followers_url": "https://api.github.com/users/pstjohn/followers", "following_url": "https://api.github.com/users/pstjohn/following{/other_user}", "gists_url": "https://api.github.com/users/pstjohn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pstjohn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pstjohn/subscriptions", "organizations_url": "https://api.github.com/users/pstjohn/orgs", "repos_url": "https://api.github.com/users/pstjohn/repos", "events_url": "https://api.github.com/users/pstjohn/events{/privacy}", "received_events_url": "https://api.github.com/users/pstjohn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2019-09-30T15:48:14Z", "updated_at": "2020-06-29T21:13:45Z", "closed_at": "2020-05-13T04:28:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm running a sentencepiece model and getting an `std::bad_alloc` error when I increase the training size from 5M to 10M sentences. (it works fine for 5M sentences). Here's how I'm calling the function:\r\n```bash\r\nspm_train --input=input.txt --vocab_size=32000 --character_coverage=1.0\r\n    --model_type=unigram --input_sentence_size=10000000 --num_threads=32\r\n```\r\n\r\nhere's the specific error:\r\n```\r\ntrainer_interface.cc(317) LOG(INFO) Sampled 10000000 sentences from 283087079 sentences.\r\ntrainer_interface.cc(321) LOG(INFO) Skipped 209436 too long sentences.\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: <unk>\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: <s>\r\ntrainer_interface.cc(330) LOG(INFO) Adding meta_piece: </s>\r\ntrainer_interface.cc(335) LOG(INFO) Normalizing sentences...\r\ntrainer_interface.cc(384) LOG(INFO) all chars count=3460742236\r\ntrainer_interface.cc(392) LOG(INFO) Done: 100% characters are covered.\r\ntrainer_interface.cc(402) LOG(INFO) Alphabet size=25\r\ntrainer_interface.cc(403) LOG(INFO) Final character coverage=1\r\ntrainer_interface.cc(435) LOG(INFO) Done! preprocessed 10000000 sentences.\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\n```\r\n\r\nI've tried compiling SentencePiece with and without gperftools, and get the same error message. Compiled with `gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)`, in case that matters. (*Edit:* also tried a more recent gcc 8.2.0 with the same results.) I doubt that it's a RAM limitation, I'm running this on a pretty beefy compute node with 768 GB of memory, and watching memory utilization as the program is running (even at 5M input sentences) I never get close to maxing out. Any thoughts why I might be getting this error message?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/404", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/404/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/404/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/404/events", "html_url": "https://github.com/google/sentencepiece/issues/404", "id": 495562597, "node_id": "MDU6SXNzdWU0OTU1NjI1OTc=", "number": 404, "title": "Trying to understanding symbol \"\u2581\" (U+2581) at the start of the Japanese sentences", "user": {"login": "weiyengs", "id": 38935956, "node_id": "MDQ6VXNlcjM4OTM1OTU2", "avatar_url": "https://avatars2.githubusercontent.com/u/38935956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyengs", "html_url": "https://github.com/weiyengs", "followers_url": "https://api.github.com/users/weiyengs/followers", "following_url": "https://api.github.com/users/weiyengs/following{/other_user}", "gists_url": "https://api.github.com/users/weiyengs/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyengs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyengs/subscriptions", "organizations_url": "https://api.github.com/users/weiyengs/orgs", "repos_url": "https://api.github.com/users/weiyengs/repos", "events_url": "https://api.github.com/users/weiyengs/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyengs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-19T04:12:32Z", "updated_at": "2019-10-02T01:37:53Z", "closed_at": "2019-10-02T01:37:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\nI noticed that there are symbol \"\u2581\" (U+2581) at the start of most Japanese sentences. I thought that this might be related to: #15 \r\n\r\nIt seems that the example that was illustrated for cases like:\r\nsp.EncodeAsPieces(\"\u770c\u4ecf\u6559\u4f1a(\u5c90\u961c\u5e02)\u306f16\u65e5\u3001\u6df7\u4e71\u304c\u7d9a\u3044\u3066\u3044\u308b\u4e2d\u56fd\u306e\u30c1\u30d9\u30c3\u30c8\u554f\u984c\u306b\u3064\u3044\u3066\u3001\u300c\u5e73\u548c\u7684\u306a\u89e3\u6c7a\u3092\u9858\u3046\u300d\u3068\u3057\u305f\u5ba3\u8a00\u3092\u63a1\u629e\u3057\u305f\u3068\u767a\u8868\u3057\u305f\u3002\u3002\u540c\u4ecf\u6559\u4f1a\u306f\u770c\u5185\u7d042\u5343\u306e\u5bfa\u9662\u304c\u5b97\u6d3e\u3092\u8d85\u3048\u3066\u52a0\u76df\u3057\u3066\u3044\u3002\")\r\n\r\n>> ['\u2581\u770c', '\u4ecf', '\u6559', '\u4f1a', '(', '\u5c90\u961c\u5e02',..and so on\r\nwhere the symbol is prefix to the character and I can understand that we want to same treatment as the whitespace counterpart.\r\n\r\nBut what I'm see-ing on my end is: \r\nsp.EncodeAsPieces(\"\u5bb9\u8a8d\u306e\u95a3\u8b70\u6c7a\u5b9a\u306e\u64a4\u56de\u3092\u76ee\u6307\u3059\u3053\u3068\u306a\u3069\u3067\u5408\u610f\u3067\u304d\u308c\u3070\u3001\u515a\u672c\u90e8\u306e\u65b9\u91dd\u306b\u57fa\u3065\u304d\u3001\u515a\u306e\u72ec\u81ea\u5019\u88dc\u3092\u53d6\u308a\u4e0b\u3052\u308b\u3002\u3002\u305d\u306e\u4e0a\u3067\u3001\u6c11\u4e3b\u65b0\u9854\u3067\u5143TBS\u5831\u9053\u5c40\u89e3\u8aac\u30fb\u5c02\u9580\u8a18\u8005\u5ba4\u9577\u306e\u6749\u5c3e\u79c0\u54c9\u6c0f(58)\u3092\u91ce\u515a\u7d71\u4e00\u5019\u88dc\u3068\u3057\u3066\u652f\u63f4\u3059\u308b\u3068\u3057\u3066\u3044\u308b\u3002\")\r\n\r\n>> ['\u2581', '\u5bb9\u8a8d', '\u306e', '\u95a3\u8b70\u6c7a\u5b9a', '\u306e', '\u64a4\u56de',..and so on\r\nwhere the symbol lives as a character on its on.\r\n\r\nIs there a reason why the difference in behaviour?\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/403", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/403/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/403/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/403/events", "html_url": "https://github.com/google/sentencepiece/issues/403", "id": 494373683, "node_id": "MDU6SXNzdWU0OTQzNzM2ODM=", "number": 403, "title": "pip install failed on linux cluster", "user": {"login": "wanting0wang", "id": 25274600, "node_id": "MDQ6VXNlcjI1Mjc0NjAw", "avatar_url": "https://avatars1.githubusercontent.com/u/25274600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wanting0wang", "html_url": "https://github.com/wanting0wang", "followers_url": "https://api.github.com/users/wanting0wang/followers", "following_url": "https://api.github.com/users/wanting0wang/following{/other_user}", "gists_url": "https://api.github.com/users/wanting0wang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wanting0wang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wanting0wang/subscriptions", "organizations_url": "https://api.github.com/users/wanting0wang/orgs", "repos_url": "https://api.github.com/users/wanting0wang/repos", "events_url": "https://api.github.com/users/wanting0wang/events{/privacy}", "received_events_url": "https://api.github.com/users/wanting0wang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2007515592, "node_id": "MDU6TGFiZWwyMDA3NTE1NTky", "url": "https://api.github.com/repos/google/sentencepiece/labels/execution%20environment", "name": "execution environment", "color": "ed6f88", "default": false, "description": "Any issues related to execution environment, installation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2019-09-17T02:58:19Z", "updated_at": "2020-08-11T00:31:50Z", "closed_at": "2020-04-24T14:07:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "System Info:\r\nLinux version 4.14.0-115.7.1.el7a.ppc64le (mockbuild@ppc-056.build.eng.bos.redhat.com) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC))\r\n\r\nI tried both installing from PyPI and installing from source file, but neither of them worked.\r\n\r\nWhen installing from PyPI:\r\n```\r\n$ pip install sentencepiece\r\nCollecting sentencepiece\r\n  Using cached https://files.pythonhosted.org/packages/1b/87/c3c2fa8cbec61fffe031ca9f0da512747520bec9be7f886f748457daac31/sentencepiece-0.1.83.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-t33o0yz4/sentencepiece/setup.py\", line 29, in <module>\r\n        with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:\r\n      File \"/opt/anaconda3/lib/python3.6/codecs.py\", line 897, in open\r\n        file = builtins.open(filename, mode, buffering)\r\n    FileNotFoundError: [Errno 2] No such file or directory: '../VERSION'\r\n\r\n    ----------------------------------------\r\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-t33o0yz4/sentencepiece/\r\n```\r\n\r\n\r\nI then manually downloaded the tar.gz source file, uncompressed it, changed the directory to \"./python\", and tried to install directly from the setup.py:\r\n```\r\n$ python setup.py install\r\nPackage sentencepiece was not found in the pkg-config search path.\r\nPerhaps you should add the directory containing `sentencepiece.pc'\r\nto the PKG_CONFIG_PATH environment variable\r\nNo package 'sentencepiece' found\r\nFailed to find sentencepiece pkgconfig\r\n```\r\n\r\nHowever `pip install .` gives a different error message:\r\n```\r\n$ pip install .\r\nProcessing <...>/sentencepiece-0.1.83/python\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-req-build-209jgy5x/setup.py\", line 29, in <module>\r\n        with codecs.open(os.path.join('..', 'VERSION'), 'r', 'utf-8') as f:\r\n      File \"/opt/anaconda3/lib/python3.6/codecs.py\", line 897, in open\r\n        file = builtins.open(filename, mode, buffering)\r\n    FileNotFoundError: [Errno 2] No such file or directory: '../VERSION'\r\n\r\n    ----------------------------------------\r\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-req-build-209jgy5x/\r\n```\r\n\r\n\r\nDoes anyone know what might be wrong and how to fix it?\r\nThank you!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/402", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/402/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/402/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/402/events", "html_url": "https://github.com/google/sentencepiece/issues/402", "id": 494063060, "node_id": "MDU6SXNzdWU0OTQwNjMwNjA=", "number": 402, "title": "How to apply subword regularization(sampling) to training NMT?", "user": {"login": "minstar", "id": 24719775, "node_id": "MDQ6VXNlcjI0NzE5Nzc1", "avatar_url": "https://avatars2.githubusercontent.com/u/24719775?v=4", "gravatar_id": "", "url": "https://api.github.com/users/minstar", "html_url": "https://github.com/minstar", "followers_url": "https://api.github.com/users/minstar/followers", "following_url": "https://api.github.com/users/minstar/following{/other_user}", "gists_url": "https://api.github.com/users/minstar/gists{/gist_id}", "starred_url": "https://api.github.com/users/minstar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/minstar/subscriptions", "organizations_url": "https://api.github.com/users/minstar/orgs", "repos_url": "https://api.github.com/users/minstar/repos", "events_url": "https://api.github.com/users/minstar/events{/privacy}", "received_events_url": "https://api.github.com/users/minstar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-16T13:50:42Z", "updated_at": "2020-04-23T02:23:53Z", "closed_at": "2020-04-23T02:23:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, first of all, thank you for your great library.\r\n\r\nI'm currently trying to train my NMT model (pytorch/fairseq) with subword regularization method.(sampling)\r\n\r\nI'm having hard time applying this method to training NMT model. Is there any example or tutorial link that I can reference? \r\n\r\nAlso, do I need to sample every epoch? (your paper states that you sampled segments every parameter update. is sampling every epoch enough?)\r\n\r\nAgain, thanks for your great work.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/401", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/401/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/401/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/401/events", "html_url": "https://github.com/google/sentencepiece/issues/401", "id": 493062428, "node_id": "MDU6SXNzdWU0OTMwNjI0Mjg=", "number": 401, "title": "handling case sensitivity?", "user": {"login": "hkmztrk", "id": 4271817, "node_id": "MDQ6VXNlcjQyNzE4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4271817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hkmztrk", "html_url": "https://github.com/hkmztrk", "followers_url": "https://api.github.com/users/hkmztrk/followers", "following_url": "https://api.github.com/users/hkmztrk/following{/other_user}", "gists_url": "https://api.github.com/users/hkmztrk/gists{/gist_id}", "starred_url": "https://api.github.com/users/hkmztrk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hkmztrk/subscriptions", "organizations_url": "https://api.github.com/users/hkmztrk/orgs", "repos_url": "https://api.github.com/users/hkmztrk/repos", "events_url": "https://api.github.com/users/hkmztrk/events{/privacy}", "received_events_url": "https://api.github.com/users/hkmztrk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-09-12T22:43:50Z", "updated_at": "2019-09-12T22:56:16Z", "closed_at": "2019-09-12T22:56:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI'm trying to extract segments via BPE from long series of strings. It usually contains upper case characters but sometimes, lower case characters appear right after upper cases. There are also, same lower case character repeating for a while, for instance \"aaaaaa\" happening frequently.\r\n\r\nInterestingly, the vocabulary includes, \"a\", and \"aa\" but does not include \"aaa\", or \"aaaa\" even though they are also frequent. What might be the cause of this?\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google/sentencepiece/issues/399", "repository_url": "https://api.github.com/repos/google/sentencepiece", "labels_url": "https://api.github.com/repos/google/sentencepiece/issues/399/labels{/name}", "comments_url": "https://api.github.com/repos/google/sentencepiece/issues/399/comments", "events_url": "https://api.github.com/repos/google/sentencepiece/issues/399/events", "html_url": "https://github.com/google/sentencepiece/issues/399", "id": 492121017, "node_id": "MDU6SXNzdWU0OTIxMjEwMTc=", "number": 399, "title": "Access to trained models' parameters experimented on NMT experiments.", "user": {"login": "JJumSSu", "id": 39372342, "node_id": "MDQ6VXNlcjM5MzcyMzQy", "avatar_url": "https://avatars1.githubusercontent.com/u/39372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JJumSSu", "html_url": "https://github.com/JJumSSu", "followers_url": "https://api.github.com/users/JJumSSu/followers", "following_url": "https://api.github.com/users/JJumSSu/following{/other_user}", "gists_url": "https://api.github.com/users/JJumSSu/gists{/gist_id}", "starred_url": "https://api.github.com/users/JJumSSu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JJumSSu/subscriptions", "organizations_url": "https://api.github.com/users/JJumSSu/orgs", "repos_url": "https://api.github.com/users/JJumSSu/repos", "events_url": "https://api.github.com/users/JJumSSu/events{/privacy}", "received_events_url": "https://api.github.com/users/JJumSSu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-11T09:28:40Z", "updated_at": "2020-04-23T02:31:55Z", "closed_at": "2020-04-23T02:31:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, First of all, thank you for your great work and nice library. \r\nI was inspired by your work which tries to inform the NMT model \"the word composition\".\r\n\r\nI'm currently doing my research on the effects of subword regularization method(unigram language model) on NMT models.\r\n\r\nhttps://github.com/google/sentencepiece/blob/master/doc/experiments.md \r\n\r\nIn this link, there were experimental results for various segmentation methodologies.\r\n\r\nI was hoping that I could have access to the trained models' parameters(e.g. .ckpt files)\r\n\r\nIf possible, can I have access to the trained models' files or quick explanation for how to train NMT model with \"sentencepiece\" segmentation method? \r\n\r\nThank you", "performed_via_github_app": null, "score": 1.0}]}