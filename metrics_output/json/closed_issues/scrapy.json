{"total_count": 1793, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/scrapy/scrapy/issues/4744", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4744/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4744/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4744/events", "html_url": "https://github.com/scrapy/scrapy/issues/4744", "id": 682799832, "node_id": "MDU6SXNzdWU2ODI3OTk4MzI=", "number": 4744, "title": "raise ValueError('Missing scheme in request url: %s' % self._url) ValueError: Missing scheme in request url: javascript:void(0);", "user": {"login": "VishvajeetRamanuj", "id": 24875310, "node_id": "MDQ6VXNlcjI0ODc1MzEw", "avatar_url": "https://avatars0.githubusercontent.com/u/24875310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VishvajeetRamanuj", "html_url": "https://github.com/VishvajeetRamanuj", "followers_url": "https://api.github.com/users/VishvajeetRamanuj/followers", "following_url": "https://api.github.com/users/VishvajeetRamanuj/following{/other_user}", "gists_url": "https://api.github.com/users/VishvajeetRamanuj/gists{/gist_id}", "starred_url": "https://api.github.com/users/VishvajeetRamanuj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VishvajeetRamanuj/subscriptions", "organizations_url": "https://api.github.com/users/VishvajeetRamanuj/orgs", "repos_url": "https://api.github.com/users/VishvajeetRamanuj/repos", "events_url": "https://api.github.com/users/VishvajeetRamanuj/events{/privacy}", "received_events_url": "https://api.github.com/users/VishvajeetRamanuj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-20T15:14:34Z", "updated_at": "2020-08-21T04:05:20Z", "closed_at": "2020-08-20T15:18:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n### Description\r\n\r\nI describe the issue on [https://stackoverflow.com/questions/63507716/raise-valueerrormissing-scheme-in-request-url-s-self-url-valueerror-mi](stackoverflowl)\r\n\r\n### Steps to Reproduce\r\n\r\n1. create new project\r\n`https://stackoverflow.com/questions/63507716/raise-valueerrormissing-scheme-in-request-url-s-self-url-valueerror-mi`\r\n2. copy paste bellow spider code in my_spider.py\r\n```\r\nclass ExampleSpider(scrapy.Spider):\r\n    name = 'moneycontrol'\r\n    # allowed_domains = ['moneycontrol.com']\r\n    start_urls = ['https://www.moneycontrol.com/india/stockpricequote/']\r\n\r\n    def parse(self, response):\r\n        stoke_link_list = response.css(\"table a::attr(href)\").getall()\r\n\r\n        if response.css(\"span.span_price_wrap::text\").getall(): # value of this variable only present in first run\r\n            stock_name =  response.css(\"h1.pcstname::text\").get()\r\n            bse_price, nse_price = response.css(\"span.span_price_wrap::text\").getall()\r\n            print(stock_name + ' ' + bse_price + ' ' + nse_price)\r\n        else:\r\n            print('stock_name bse_price nse_price')\r\n\r\n        for link in stoke_link_list:\r\n            if link is not None:\r\n                next_page = response.urljoin(link)\r\n                # yield scrapy.Request(next_page, callback=self.parse)\r\n                yield response.follow(next_page, callback=self.parse)\r\n```\r\n3. run the crawler\r\n`scrapy crawl moneycontrol`\r\n\r\n\r\n**Expected behavior:** It should show me the price of stocks \r\n\r\n**Actual behavior:** It gives me error whcih is also explained in question\r\n\r\n**Reproduces how often:** can't say\r\n\r\n### Versions\r\n\r\n `scrapy version --verbose` \r\n```\r\nScrapy       : 2.2.1\r\nlxml         : 4.5.2.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 3.6.9 (default, Jul 17 2020, 12:50:27) - [GCC 8.4.0]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 3.0\r\nPlatform     : Linux-4.15.0-112-generic-x86_64-with-Ubuntu-18.04-bionic\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4741", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4741/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4741/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4741/events", "html_url": "https://github.com/scrapy/scrapy/issues/4741", "id": 682712674, "node_id": "MDU6SXNzdWU2ODI3MTI2NzQ=", "number": 4741, "title": "Remove appveyor.yml", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "Yogendra0Sharma", "id": 1375860, "node_id": "MDQ6VXNlcjEzNzU4NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1375860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yogendra0Sharma", "html_url": "https://github.com/Yogendra0Sharma", "followers_url": "https://api.github.com/users/Yogendra0Sharma/followers", "following_url": "https://api.github.com/users/Yogendra0Sharma/following{/other_user}", "gists_url": "https://api.github.com/users/Yogendra0Sharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yogendra0Sharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yogendra0Sharma/subscriptions", "organizations_url": "https://api.github.com/users/Yogendra0Sharma/orgs", "repos_url": "https://api.github.com/users/Yogendra0Sharma/repos", "events_url": "https://api.github.com/users/Yogendra0Sharma/events{/privacy}", "received_events_url": "https://api.github.com/users/Yogendra0Sharma/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Yogendra0Sharma", "id": 1375860, "node_id": "MDQ6VXNlcjEzNzU4NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1375860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yogendra0Sharma", "html_url": "https://github.com/Yogendra0Sharma", "followers_url": "https://api.github.com/users/Yogendra0Sharma/followers", "following_url": "https://api.github.com/users/Yogendra0Sharma/following{/other_user}", "gists_url": "https://api.github.com/users/Yogendra0Sharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yogendra0Sharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yogendra0Sharma/subscriptions", "organizations_url": "https://api.github.com/users/Yogendra0Sharma/orgs", "repos_url": "https://api.github.com/users/Yogendra0Sharma/repos", "events_url": "https://api.github.com/users/Yogendra0Sharma/events{/privacy}", "received_events_url": "https://api.github.com/users/Yogendra0Sharma/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-08-20T13:20:25Z", "updated_at": "2020-08-21T07:36:14Z", "closed_at": "2020-08-21T07:36:14Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "https://github.com/scrapy/scrapy/blob/master/appveyor.yml is a remnant from a past CI integration that is no longer needed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4728", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4728/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4728/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4728/events", "html_url": "https://github.com/scrapy/scrapy/issues/4728", "id": 679445124, "node_id": "MDU6SXNzdWU2Nzk0NDUxMjQ=", "number": 4728, "title": "how do i run it?", "user": {"login": "roger656", "id": 69346918, "node_id": "MDQ6VXNlcjY5MzQ2OTE4", "avatar_url": "https://avatars2.githubusercontent.com/u/69346918?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roger656", "html_url": "https://github.com/roger656", "followers_url": "https://api.github.com/users/roger656/followers", "following_url": "https://api.github.com/users/roger656/following{/other_user}", "gists_url": "https://api.github.com/users/roger656/gists{/gist_id}", "starred_url": "https://api.github.com/users/roger656/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roger656/subscriptions", "organizations_url": "https://api.github.com/users/roger656/orgs", "repos_url": "https://api.github.com/users/roger656/repos", "events_url": "https://api.github.com/users/roger656/events{/privacy}", "received_events_url": "https://api.github.com/users/roger656/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443680419, "node_id": "MDU6TGFiZWw0NDM2ODA0MTk=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/needs%20more%20info", "name": "needs more info", "color": "fef2c0", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-14T23:12:53Z", "updated_at": "2020-08-17T12:11:16Z", "closed_at": "2020-08-17T12:11:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4720", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4720/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4720/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4720/events", "html_url": "https://github.com/scrapy/scrapy/issues/4720", "id": 677093857, "node_id": "MDU6SXNzdWU2NzcwOTM4NTc=", "number": 4720, "title": "Smarter generator check for combined yield/return statements: ignore nested functions", "user": {"login": "soid", "id": 213141, "node_id": "MDQ6VXNlcjIxMzE0MQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/213141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soid", "html_url": "https://github.com/soid", "followers_url": "https://api.github.com/users/soid/followers", "following_url": "https://api.github.com/users/soid/following{/other_user}", "gists_url": "https://api.github.com/users/soid/gists{/gist_id}", "starred_url": "https://api.github.com/users/soid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soid/subscriptions", "organizations_url": "https://api.github.com/users/soid/orgs", "repos_url": "https://api.github.com/users/soid/repos", "events_url": "https://api.github.com/users/soid/events{/privacy}", "received_events_url": "https://api.github.com/users/soid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-11T18:21:20Z", "updated_at": "2020-08-20T13:22:08Z", "closed_at": "2020-08-20T13:22:08Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Summary\r\nCurrently, if spider methods are generators that yield results and contain nested function, then the following warning is issued:\r\n\r\n```\r\n[py.warnings] WARNING: /Library/Python/3.7/site-packages/scrapy/core/scraper.py:148: UserWarning: The \"MySpider.parse\" method is a generator and includes a \"return\" statement with a value different than None. This could lead to unexpected behaviour. Please see https://docs.python.org/3/reference/simple_stmts.html#the-return-statement for details about the semantics of the \"return\" statement within generators\r\n  warn_on_generator_with_return_value(spider, callback)\r\n```\r\n\r\nThe example of a simple spider that results in the warning:\r\n```\r\nimport scrapy\r\n\r\nclass MySpider(scrapy.Spider):\r\n    name = \"MySpider\"\r\n    start_urls = [\"https://scrapy.org\"]\r\n    \r\n    def parse(self, response):\r\n        \r\n        def is_external(url):\r\n            href = url.css('::attr(href)').get()\r\n            return href.startswith('http') and 'scrapy.org' not in href\r\n        \r\n        links = [link for link in response.css('a') if is_external(link)]\r\n        for link in links:\r\n            yield {'link': link.css('::attr(href)').get(), 'text': link.css('::text').get()}\r\n```\r\n\r\nI know it's a bit artificial example as the nested function can be moved, but there is nothing wrong with nested function conceptually.\r\n\r\n## Motivation\r\n\r\nI have a midsize spider function that includes some nested helper functions that I'd like to keep close to where they are called.\r\n\r\n## Describe alternatives you've considered\r\n\r\nMoving nested function out of the generator is an easy fix, but it constrains expressivity of the code.\r\n\r\n## Additional context\r\n\r\nRelated function: is_generator_with_return_value\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4719", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4719/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4719/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4719/events", "html_url": "https://github.com/scrapy/scrapy/issues/4719", "id": 677052706, "node_id": "MDU6SXNzdWU2NzcwNTI3MDY=", "number": 4719, "title": "Scrapy shell works with splash settings on macOS, the spider fails ", "user": {"login": "emadboctorx", "id": 58062537, "node_id": "MDQ6VXNlcjU4MDYyNTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/58062537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emadboctorx", "html_url": "https://github.com/emadboctorx", "followers_url": "https://api.github.com/users/emadboctorx/followers", "following_url": "https://api.github.com/users/emadboctorx/following{/other_user}", "gists_url": "https://api.github.com/users/emadboctorx/gists{/gist_id}", "starred_url": "https://api.github.com/users/emadboctorx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emadboctorx/subscriptions", "organizations_url": "https://api.github.com/users/emadboctorx/orgs", "repos_url": "https://api.github.com/users/emadboctorx/repos", "events_url": "https://api.github.com/users/emadboctorx/events{/privacy}", "received_events_url": "https://api.github.com/users/emadboctorx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-11T17:19:16Z", "updated_at": "2020-08-12T07:47:15Z", "closed_at": "2020-08-12T07:47:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to scrape the content from this [link][1] on my macOS, using `scrapy` with `scrapy_splash` settings and `BeautifulSoup` I followed the instructions in the [documentation][2] \r\n\r\n* I tested every single command in scrapy shell and each works perfectly fine, tested on several pages. when I run the spider with the same commands, it fails to detect any of the items.\r\n\r\n`settings.py` \r\n\r\n    BOT_NAME = 'stepstone'\r\n    SPIDER_MODULES = ['stepstone.spiders']\r\n    NEWSPIDER_MODULE = 'stepstone.spiders'\r\n    SPLASH_URL = 'http://0.0.0.0:8050'  # changed from the documentation's http://192.168.59.103:8050 which does not work \r\n    DOWNLOADER_MIDDLEWARES = {\r\n        'scrapy_splash.SplashCookiesMiddleware': 723,\r\n        'scrapy_splash.SplashMiddleware': 725,\r\n        'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\r\n    }\r\n    SPIDER_MIDDLEWARES = {\r\n        'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\r\n    }\r\n    DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\r\n    HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\r\n\r\nThe spider module:\r\n\r\n    from scrapy.spiders import Spider\r\n    from scrapy_splash import SplashRequest\r\n    from scrapy import Request\r\n    from bs4 import BeautifulSoup\r\n    \r\n    \r\n    class StepSpider(Spider):\r\n        name = 'step'\r\n        allowed_domains = ['www.stepstone.de']\r\n        start_urls = [\r\n            'https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs='\r\n            '%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entw'\r\n            'ickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%223000001'\r\n            '15%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geoc'\r\n            'ity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=home'\r\n            'pagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwic'\r\n            'kler%2Fin&ws=Deutschland&ra=30/'\r\n        ]\r\n    \r\n        @staticmethod\r\n        def extract_item(soup, extraction_path):\r\n            result = soup.find(*extraction_path)\r\n            if result:\r\n                return result.getText()\r\n    \r\n        def parse(self, response):\r\n            soup = BeautifulSoup(response.body, features='lxml')\r\n            listings = [\r\n                response.urljoin(item)\r\n                for item in response.xpath('//div/div/a/@href').extract()\r\n                if 'stellenangebote' in item\r\n            ]\r\n            yield from [\r\n                Request(\r\n                    url,\r\n                    callback=self.parse_item,\r\n                    cb_kwargs={'soup': soup},\r\n                    meta={'splash': {'args': {'html': 1, 'png': 1,}}},\r\n                )\r\n                for url in listings\r\n            ]\r\n            next_page = soup.find('a', {'data-at': 'pagination-next'})\r\n            if next_page:\r\n                yield SplashRequest(next_page.get('href'), self.parse)\r\n    \r\n        def parse_header(self, response, soup):\r\n            title = response.xpath('//h1/text()').get()\r\n            location = self.extract_item(\r\n                soup, ('li', {'class': 'at-listing__li: st-icons_location'})\r\n            )\r\n            contract_type = self.extract_item(\r\n                soup, ('li', {'class': 'at-listing__list-icons_contract-type'})\r\n            )\r\n            work_type = self.extract_item(\r\n                soup, ('li', {'class': 'at-listing__list-icons_work-type'})\r\n            )\r\n            return {\r\n                'title': title,\r\n                'location': location,\r\n                'contract_type': contract_type,\r\n                'work_type': work_type,\r\n            }\r\n    \r\n        def parse_body(self, response, soup):\r\n            titles = response.xpath('//h4/text()').extract()\r\n            intro = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-introduction-content'})\r\n            )\r\n            description = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-description-content'})\r\n            )\r\n            profile = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-profile-content'})\r\n            )\r\n            we_offer = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-weoffer-content'})\r\n            )\r\n            contact = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-contact-content'})\r\n            )\r\n            return {\r\n                title: text\r\n                for title, text in zip(\r\n                    titles, [intro, description, profile, we_offer, contact]\r\n                )\r\n            }\r\n    \r\n        def parse_item(self, response, soup):\r\n            items = self.parse_header(response, soup)\r\n            items.update(self.parse_body(response, soup))\r\n            yield items\r\n\r\nfull log:\r\n\r\n    2020-08-11 17:57:44 [scrapy.utils.log] INFO: Scrapy 2.2.1 started (bot: stepstone)\r\n    2020-08-11 17:57:44 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.3 (default, May 27 2020, 20:54:22) - [Clang 11.0.3 (clang-1103.0.32.59)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform macOS-10.15.6-x86_64-i386-64bit\r\n    2020-08-11 17:57:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\r\n    2020-08-11 17:57:44 [scrapy.crawler] INFO: Overridden settings:\r\n    {'BOT_NAME': 'stepstone',\r\n     'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter',\r\n     'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage',\r\n     'NEWSPIDER_MODULE': 'stepstone.spiders',\r\n     'SPIDER_MODULES': ['stepstone.spiders']}\r\n    2020-08-11 17:57:44 [scrapy.extensions.telnet] INFO: Telnet Password: 71c7bd3bdaf32c63\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled extensions:\r\n    ['scrapy.extensions.corestats.CoreStats',\r\n     'scrapy.extensions.telnet.TelnetConsole',\r\n     'scrapy.extensions.memusage.MemoryUsage',\r\n     'scrapy.extensions.logstats.LogStats']\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n    ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n     'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n     'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n     'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n     'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n     'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n     'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n     'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n     'scrapy_splash.SplashCookiesMiddleware',\r\n     'scrapy_splash.SplashMiddleware',\r\n     'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n     'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n     'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n    ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n     'scrapy_splash.SplashDeduplicateArgsMiddleware',\r\n     'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n     'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n     'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n     'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled item pipelines:\r\n    []\r\n    2020-08-11 17:57:44 [scrapy.core.engine] INFO: Spider opened\r\n    2020-08-11 17:57:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n    2020-08-11 17:57:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n    2020-08-11 17:57:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30/> (referer: None)\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: utf-8  confidence = 0.99\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-JP Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: GB2312 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-KR Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: CP949 Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: Big5 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-TW Taiwan confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.041278205445058724\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.5186494104315963\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: utf-8  confidence = 0.99\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-JP Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: GB2312 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-KR Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: CP949 Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: Big5 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-TW Taiwan confidence = 0.01\r\n    2020-08-11 17:57:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.stepstone.de/stellenangebote--JAVA-Software-Entwickler-m-w-d-Sueddeutschland-TECCON-Consulting-Engineering-GmbH--6582908-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=1_1_25_dynrl_m_0_0_0_0> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\r\n    2020-08-11 17:57:47 [py.warnings] WARNING: /usr/local/lib/python3.8/site-packages/scrapy_splash/request.py:41: ScrapyDeprecationWarning: Call to deprecated function to_native_str. Use to_unicode instead.\r\n      url = to_native_str(url)\r\n    \r\n    2020-08-11 17:57:50 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-fuer-Windowsapplikationen-m-w-d-Stockach-oder-Boeblingen-Baumer-MDS-GmbH--6568164-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=19_19_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler f\u00fcr Windowsapplikationen (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Herausforderung:': None, 'Sie verf\u00fcgen \u00fcber:': None, 'Wir bieten:': None, 'Kontakt:': None}\r\n    2020-08-11 17:57:51 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--JAVA-Software-Entwickler-m-w-d-Sueddeutschland-TECCON-Consulting-Engineering-GmbH--6582908-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=1_1_25_dynrl_m_0_0_0_0>\r\n    {'title': 'JAVA Software-Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Einleitung': None, 'Ihre Aufgaben': None, 'Ihr Profil': None, 'Wir bieten': None, 'Weitere Informationen': None}\r\n    2020-08-11 17:57:52 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-Business-Engineer-fuer-Blockchain-Team-in-Gruendung-w-m-d-Frankfurt-Main-Deutsche-Bahn-AG--6249570-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=16_16_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler / Business Engineer f\u00fcr Blockchain-Team in Gr\u00fcndung (w/m/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Was dich erwartet': None, 'Was wir erwarten': None, 'Wir bieten': None, 'Standort': None}\r\n    2020-08-11 17:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-w-m-d-Diagnose-und-Visualisierungssysteme-Mannheim-Halle-Stadler-Mannheim-GmbH--6615613-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=13_13_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler (w/m/d) Diagnose und Visualisierungssysteme', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben:': None, 'Ihr Profil:': None, 'Unser Angebot:': None, 'Begeistert?': None}\r\n    2020-08-11 17:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-m-w-d-Rosenheim-Agenda-Informationssysteme-GmbH-Co-KG--6590641-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=17_17_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben:': None, 'Ihr Profil:': None, 'Das spricht f\u00fcr uns:': None, 'Kontakt:': None, 'Standort': None}\r\n    2020-08-11 17:58:08 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:08 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-w-m-d-fuer-Fahrzeugsteuerung-Mannheim-Halle-Stadler-Mannheim-GmbH--6615612-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=11_11_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler (w/m/d) f\u00fcr Fahrzeugsteuerung', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben:': None, 'Ihr Profil:': None, 'Unser Angebot:': None, 'Begeistert?': None}\r\n    ^C2020-08-11 17:58:09 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \r\n    2020-08-11 17:58:09 [scrapy.core.engine] INFO: Closing spider (shutdown)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-m-w-d-Meissen-Staatliche-Porzellan-Manufaktur-Meissen-GmbH--6462761-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=14_14_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Wir gehen neue Wege': None, 'Ihre Aufgaben': None, 'unsere Anforderungen': None, 'unser Angebot': None, 'Kontakt': None}\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Agiler-Software-Entwickler-m-w-div-Dresden-Otto-Group-Solution-Provider-OSP-GmbH--4573007-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=8_8_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Agiler Software Entwickler (m/w/div)', 'location': None, 'contract_type': None, 'work_type': None, '\u00dcber uns': None, 'Was dich erwartet': None, 'Was du mitbringen solltest': None, 'Diese und weitere Benefits erwarten dich': None, 'Kontakt': None}\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-m-w-d-Essen-Lowell-Group--6615697-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=9_9_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben': None, 'Ihr Profil': None, 'Wir bieten': None, 'Kontakt': None, 'Standort': None}\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Softwareentwickler-m-w-d-Fullstack-Web-Boeblingen-Braunschweig-Deutschlandweit-Ingolstadt-Muenchen-Norddeutschland-Stuttgart-umlaut--6122455-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=15_15_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Softwareentwickler (m/w/d) - Fullstack Web', 'location': None, 'contract_type': None, 'work_type': None, 'our \u00f6ffer': None, 'y\u00f6u': None, 'top 5 reas\u00f6ns': None, 'c\u00f6ntact': None, 'Mitarbeiterbewertungen': None}\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-E-Commerce-m-w-d-Dresden-Otto-Group-Solution-Provider-OSP-GmbH--6550022-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=10_10_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler E-Commerce (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Was dich erwartet': None, 'Was du mitbringen solltest': None, 'Kontakt': None, 'Standort': None}\r\n    ^C2020-08-11 17:58:16 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&companyid=0&sourceofthesearchfield=homepagemex%3Ageneral&qs=[{\"id\"%3A216805%2C\"description\"%3A\"Software-Entwickler\\%2Fin\"%2C\"type\"%3A\"jd\"}%2C{\"id\"%3A300000115%2C\"description\"%3A\"Deutschland\"%2C\"type\"%3A\"geocity\"}]&cityid=300000115&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30&suid=90b7defb-2854-4c23-98bd-b39bc15a6922&of=25&action=paging_next via http://0.0.0.0:8050/render.html> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n\r\ndocker log:\r\n\r\n    2020-08-11 15:57:27+0000 [-] Log opened.\r\n    2020-08-11 15:57:27.990815 [-] Xvfb is started: ['Xvfb', ':2061643423', '-screen', '0', '1024x768x24', '-nolisten', 'tcp']\r\n    QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-splash'\r\n    2020-08-11 15:57:28.135258 [-] Splash version: 3.4.1\r\n    2020-08-11 15:57:28.203198 [-] Qt 5.13.1, PyQt 5.13.1, WebKit 602.1, Chromium 73.0.3683.105, sip 4.19.19, Twisted 19.7.0, Lua 5.2\r\n    2020-08-11 15:57:28.203826 [-] Python 3.6.9 (default, Nov  7 2019, 10:44:02) [GCC 8.3.0]\r\n    2020-08-11 15:57:28.204679 [-] Open files limit: 1048576\r\n    2020-08-11 15:57:28.205242 [-] Can't bump open files limit\r\n    2020-08-11 15:57:28.229336 [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles\r\n    2020-08-11 15:57:28.229855 [-] memory cache: enabled, private mode: enabled, js cross-domain access: disabled\r\n    2020-08-11 15:57:28.410540 [-] verbosity=1, slots=20, argument_cache_max_entries=500, max-timeout=90.0\r\n    2020-08-11 15:57:28.411484 [-] Web UI: enabled, Lua: enabled (sandbox: enabled), Webkit: enabled, Chromium: enabled\r\n    2020-08-11 15:57:28.412634 [-] Site starting on 8050\r\n    2020-08-11 15:57:28.412924 [-] Starting factory <twisted.web.server.Site object at 0x7fbfa77591d0>\r\n    2020-08-11 15:57:28.414172 [-] Server listening on http://0.0.0.0:8050\r\n    2020-08-11 15:57:49.583386 [events] {\"path\": \"/render.json\", \"rendertime\": 2.339588165283203, \"maxrss\": 236848, \"load\": [0.1, 0.05, 0.06], \"fds\": 102, \"active\": 7, \"qsize\": 0, \"_id\": 140461124347104, \"method\": \"POST\", \"timestamp\": 1597161469, \"user-agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\", \"args\": {\"headers\": {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en\", \"Cookie\": \"cfid=bef36179-b81a-44e3-9bd1-059f5406a911; cftoken=0; USER_HASH_ID=f896d04a-5348-455b-a0ab-ffd0d5f6e674; V5=1; UXUSER=BLACKLIST%3BA%3B%20%3B; STEPSTONEV5LANG=de; ONLINE_CF=14-190; dtCookie=35$77973CDF4397BDD4A3EF1CAFDB05C9FD\", \"Referer\": \"https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30/\", \"User-Agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\"}, \"html\": 1, \"png\": 1, \"url\": \"https://www.stepstone.de/stellenangebote--Software-Entwickler-fuer-Windowsapplikationen-m-w-d-Stockach-oder-Boeblingen-Baumer-MDS-GmbH--6568164-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=19_19_25_dynrl_m_0_0_0_0\", \"uid\": 140461124347104}, \"status_code\": 200, \"client_ip\": \"172.17.0.1\"}\r\n    2020-08-11 15:57:49.584498 [-] \"172.17.0.1\" - - [11/Aug/2020:15:57:48 +0000] \"POST /render.json HTTP/1.1\" 200 371319 \"-\" \"Scrapy/2.2.1 (+https://scrapy.org)\"\r\n    2020-08-11 15:57:49.777352 [events] {\"path\": \"/render.json\", \"rendertime\": 2.6071407794952393, \"maxrss\": 243100, \"load\": [0.1, 0.05, 0.06], \"fds\": 106, \"active\": 6, \"qsize\": 0, \"_id\": 140461124981984, \"method\": \"POST\", \"timestamp\": 1597161469, \"user-agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\", \"args\": {\"headers\": {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en\", \"Cookie\": \"cfid=bef36179-b81a-44e3-9bd1-059f5406a911; cftoken=0; USER_HASH_ID=f896d04a-5348-455b-a0ab-ffd0d5f6e674; V5=1; UXUSER=BLACKLIST%3BA%3B%20%3B; STEPSTONEV5LANG=de; ONLINE_CF=14-190; dtCookie=35$77973CDF4397BDD4A3EF1CAFDB05C9FD\", \"Referer\": \"https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30/\", \"User-Agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\"}, \"html\": 1, \"png\": 1, \"url\": \"https://www.stepstone.de/stellenangebote--JAVA-Software-Entwickler-m-w-d-Sueddeutschland-TECCON-Consulting-Engineering-GmbH--6582908-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=1_1_25_dynrl_m_0_0_0_0\", \"uid\": 140461124981984}, \"status_code\": 200, \"client_ip\": \"172.17.0.1\"}\r\n\r\n  [1]: https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30\r\n  [2]: https://github.com/scrapy-plugins/scrapy-splash\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4713", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4713/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4713/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4713/events", "html_url": "https://github.com/scrapy/scrapy/issues/4713", "id": 673642977, "node_id": "MDU6SXNzdWU2NzM2NDI5Nzc=", "number": 4713, "title": "How to install with Python 3.4.3 (in Windows XP 32 bits)?", "user": {"login": "tellts", "id": 5172660, "node_id": "MDQ6VXNlcjUxNzI2NjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5172660?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tellts", "html_url": "https://github.com/tellts", "followers_url": "https://api.github.com/users/tellts/followers", "following_url": "https://api.github.com/users/tellts/following{/other_user}", "gists_url": "https://api.github.com/users/tellts/gists{/gist_id}", "starred_url": "https://api.github.com/users/tellts/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tellts/subscriptions", "organizations_url": "https://api.github.com/users/tellts/orgs", "repos_url": "https://api.github.com/users/tellts/repos", "events_url": "https://api.github.com/users/tellts/events{/privacy}", "received_events_url": "https://api.github.com/users/tellts/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-05T15:44:26Z", "updated_at": "2020-08-06T06:22:12Z", "closed_at": "2020-08-06T06:22:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello. I have problems.\r\n`Microsoft Windows XP [\u0412\u0435\u0440\u0441\u0438\u044f 5.1.2600]\r\n(\u0421) \u041a\u043e\u0440\u043f\u043e\u0440\u0430\u0446\u0438\u044f \u041c\u0430\u0439\u043a\u0440\u043e\u0441\u043e\u0444\u0442, 1985-2001.\r\n\r\nC:\\Documents and Settings\\ia>pip install scrapy==1.7.4\r\nCollecting scrapy==1.7.4\r\n  Downloading https://files.pythonhosted.org/packages/dd/4f/640343805541c782ee49\r\nb14055a85da816cc118a8f48d01b56d2e5d12bf1/Scrapy-1.7.4-py2.py3-none-any.whl (234k\r\nB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235kB 211kB/s\r\nCollecting PyDispatcher>=2.0.5\r\n  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea\r\n9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\r\nRequirement already satisfied: lxml<=4.3.5; python_version == \"3.4\" in c:\\python\r\n34\\lib\\site-packages (from scrapy==1.7.4) (4.3.1)\r\nCollecting parsel>=1.5\r\n  Downloading https://files.pythonhosted.org/packages/23/1e/9b39d64cbab79d4362cd\r\nd7be7f5e9623d45c4a53b3f7522cd8210df52d8e/parsel-1.6.0-py2.py3-none-any.whl\r\nCollecting service-identity\r\n  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618\r\nd43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-an\r\ny.whl\r\nRequirement already satisfied: six>=1.5.2 in c:\\python34\\lib\\site-packages (from\r\n scrapy==1.7.4) (1.15.0)\r\nCollecting pyOpenSSL\r\n  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d3480\r\n39954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (\r\n53kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 218kB/s\r\nCollecting cssselect>=0.9\r\n  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6\r\nf91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\r\nCollecting queuelib\r\n  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8\r\naf3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\r\nCollecting Twisted<=19.2.0,>=13.1.0; python_version == \"3.4\"\r\n  Downloading https://files.pythonhosted.org/packages/f8/2b/a80a70f71eb2b86992ff\r\na5aaae41457791ae67faa70927fd16b76127c2b7/Twisted-19.2.0.tar.bz2 (3.1MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.1MB 52kB/s\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'C:\\Python34\\python.exe' -c 'import sys, setuptools, tokenize; sys\r\n.argv[0] = '\"'\"'C:\\\\DOCUME~1\\\\ia\\\\LOCALS~1\\\\Temp\\\\pip-install-ei0jjdyj\\\\Twisted\\\r\n\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\DOCUME~1\\\\ia\\\\LOCALS~1\\\\Temp\\\\pip-install-ei0j\r\njdyj\\\\Twisted\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__)\r\n;code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code\r\n, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\DOCUME~1\\ia\\LOCALS~1\\Temp\\\r\npip-install-ei0jjdyj\\Twisted\\pip-egg-info'\r\n         cwd: C:\\DOCUME~1\\ia\\LOCALS~1\\Temp\\pip-install-ei0jjdyj\\Twisted\\\r\n    Complete output (81 lines):\r\n    Traceback (most recent call last):\r\n      File \"C:\\Python34\\lib\\urllib\\request.py\", line 1182, in do_open\r\n        h.request(req.get_method(), req.selector, req.data, headers)\r\n      File \"C:\\Python34\\lib\\http\\client.py\", line 1088, in request\r\n        self._send_request(method, url, body, headers)\r\n      File \"C:\\Python34\\lib\\http\\client.py\", line 1126, in _send_request\r\n        self.endheaders(body)\r\n      File \"C:\\Python34\\lib\\http\\client.py\", line 1084, in endheaders\r\n        self._send_output(message_body)\r\n      File \"C:\\Python34\\lib\\http\\client.py\", line 922, in _send_output\r\n        self.send(msg)\r\n      File \"C:\\Python34\\lib\\http\\client.py\", line 857, in send\r\n        self.connect()\r\n      File \"C:\\Python34\\lib\\http\\client.py\", line 1231, in connect\r\n        server_hostname=server_hostname)\r\n      File \"C:\\Python34\\lib\\ssl.py\", line 365, in wrap_socket\r\n        _context=self)\r\n      File \"C:\\Python34\\lib\\ssl.py\", line 583, in __init__\r\n        self.do_handshake()\r\n      File \"C:\\Python34\\lib\\ssl.py\", line 810, in do_handshake\r\n        self._sslobj.do_handshake()\r\n    ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_s\r\nsl.c:600)\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 701\r\n, in open_url\r\n        return open_with_auth(url, self.opener)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 895\r\n, in _socket_timeout\r\n        return func(*args, **kwargs)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 100\r\n8, in open_with_auth\r\n        fp = opener(request)\r\n      File \"C:\\Python34\\lib\\urllib\\request.py\", line 161, in urlopen\r\n        return opener.open(url, data, timeout)\r\n      File \"C:\\Python34\\lib\\urllib\\request.py\", line 463, in open\r\n        response = self._open(req, data)\r\n      File \"C:\\Python34\\lib\\urllib\\request.py\", line 481, in _open\r\n        '_open', req)\r\n      File \"C:\\Python34\\lib\\urllib\\request.py\", line 441, in _call_chain\r\n        result = func(*args)\r\n      File \"C:\\Python34\\lib\\urllib\\request.py\", line 1225, in https_open\r\n        context=self._context, check_hostname=self._check_hostname)\r\n      File \"C:\\Python34\\lib\\urllib\\request.py\", line 1184, in do_open\r\n        raise URLError(err)\r\n    urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certi\r\nficate verify failed (_ssl.c:600)>\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"C:\\DOCUME~1\\ia\\LOCALS~1\\Temp\\pip-install-ei0jjdyj\\Twisted\\setup.py\",\r\n line 20, in <module>\r\n        setuptools.setup(**_setup[\"getSetupArgs\"]())\r\n      File \"C:\\Python34\\lib\\distutils\\core.py\", line 108, in setup\r\n        _setup_distribution = dist = klass(attrs)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\dist.py\", line 265, in __in\r\nit__\r\n        self.fetch_build_eggs(attrs['setup_requires'])\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\dist.py\", line 311, in fetc\r\nh_build_eggs\r\n        replace_conflicting=True,\r\n      File \"C:\\Python34\\lib\\site-packages\\pkg_resources\\__init__.py\", line 797,\r\nin resolve\r\n        dist = best[req.key] = env.best_match(req, ws, installer)\r\n      File \"C:\\Python34\\lib\\site-packages\\pkg_resources\\__init__.py\", line 1047,\r\n in best_match\r\n        return self.obtain(req, installer)\r\n      File \"C:\\Python34\\lib\\site-packages\\pkg_resources\\__init__.py\", line 1059,\r\n in obtain\r\n        return installer(requirement)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\dist.py\", line 378, in fetc\r\nh_build_egg\r\n        return cmd.easy_install(req)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\command\\easy_install.py\", l\r\nine 611, in easy_install\r\n        not self.always_copy, self.local_index\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 602\r\n, in fetch_distribution\r\n        return dist.clone(location=self.download(dist.location, tmpdir))\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 518\r\n, in download\r\n        found = self._download_url(scheme.group(1), spec, tmpdir)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 759\r\n, in _download_url\r\n        return self._attempt_download(url, filename)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 765\r\n, in _attempt_download\r\n        headers = self._download_to(url, filename)\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 665\r\n, in _download_to\r\n        fp = self.open_url(strip_fragment(url))\r\n      File \"C:\\Python34\\lib\\site-packages\\setuptools\\package_index.py\", line 715\r\n, in open_url\r\n        % (url, v.reason))\r\n    distutils.errors.DistutilsError: Download error for https://files.pythonhost\r\ned.org/packages/8f/26/02c4016aa95f45479eea37c90c34f8fab6775732ae62587a874b619ca0\r\n97/incremental-17.5.0.tar.gz#sha256=7b751696aaf36eebfab537e458929e194460051ccad2\r\n79c72b755a167eebd4b3: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\r\n (_ssl.c:600)\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check th\r\ne logs for full command output.\r\n\r\nC:\\Documents and Settings\\ia>\r\n\r\n\r\n\r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4711", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4711/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4711/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4711/events", "html_url": "https://github.com/scrapy/scrapy/issues/4711", "id": 672480386, "node_id": "MDU6SXNzdWU2NzI0ODAzODY=", "number": 4711, "title": "Unable to invoke instance method from scrapy subclass", "user": {"login": "rajesh-5445", "id": 65643930, "node_id": "MDQ6VXNlcjY1NjQzOTMw", "avatar_url": "https://avatars3.githubusercontent.com/u/65643930?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rajesh-5445", "html_url": "https://github.com/rajesh-5445", "followers_url": "https://api.github.com/users/rajesh-5445/followers", "following_url": "https://api.github.com/users/rajesh-5445/following{/other_user}", "gists_url": "https://api.github.com/users/rajesh-5445/gists{/gist_id}", "starred_url": "https://api.github.com/users/rajesh-5445/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rajesh-5445/subscriptions", "organizations_url": "https://api.github.com/users/rajesh-5445/orgs", "repos_url": "https://api.github.com/users/rajesh-5445/repos", "events_url": "https://api.github.com/users/rajesh-5445/events{/privacy}", "received_events_url": "https://api.github.com/users/rajesh-5445/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-04T03:36:57Z", "updated_at": "2020-08-04T06:56:41Z", "closed_at": "2020-08-04T06:56:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I might want to call/invoke an instance method (parse_next_page) from my scrapy sub class, but it does not do so. Can you please hep me out?\r\n\r\nCode:\r\nimport scrapy\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = 'quotes'\r\n    start_urls = [\r\n        'http://quotes.toscrape.com/tag/humor/',\r\n    ]\r\n\r\n    def parse(self, response):\r\n        for quote in response.css('div.quote'):\r\n            yield {\r\n                'author': quote.xpath('span/small/text()').get(),\r\n                'text': quote.css('span.text::text').get(),\r\n            }\r\n        print(\"*** call next page function\")\r\n        self.parse_next_page(response)    \r\n\r\n    def parse_next_page(self, response):\r\n        print(\"*** parsee next page function invoked\")\r\n        next_page = response.css('li.next a::attr(\"href\")').get()\r\n        if next_page is not None:\r\n            yield response.follow(next_page, self.parse)\r\n\r\nOutput:\r\n{'author': 'Terry Pratchett', 'text': '\u201cThe trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.\u201d'}\r\n2020-07-29 09:30:39 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\r\n{'author': 'Dr. Seuss', 'text': '\u201cThink left and think right and think low and think high. Oh, the thinks you can think up if only you try!\u201d'}\r\n2020-07-29 09:30:39 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/tag/humor/>\r\n{'author': 'George Carlin', 'text': '\u201cThe reason I talk to myself is because I\u2019m the only one whose answers I accept.\u201d'}\r\n*** call next page funcion\r\n2020-07-29 09:30:39 [scrapy.core.engine] INFO: Closing spider (finished)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4709", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4709/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4709/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4709/events", "html_url": "https://github.com/scrapy/scrapy/issues/4709", "id": 670138199, "node_id": "MDU6SXNzdWU2NzAxMzgxOTk=", "number": 4709, "title": "proxy support", "user": {"login": "yezz123", "id": 52716203, "node_id": "MDQ6VXNlcjUyNzE2MjAz", "avatar_url": "https://avatars1.githubusercontent.com/u/52716203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yezz123", "html_url": "https://github.com/yezz123", "followers_url": "https://api.github.com/users/yezz123/followers", "following_url": "https://api.github.com/users/yezz123/following{/other_user}", "gists_url": "https://api.github.com/users/yezz123/gists{/gist_id}", "starred_url": "https://api.github.com/users/yezz123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yezz123/subscriptions", "organizations_url": "https://api.github.com/users/yezz123/orgs", "repos_url": "https://api.github.com/users/yezz123/repos", "events_url": "https://api.github.com/users/yezz123/events{/privacy}", "received_events_url": "https://api.github.com/users/yezz123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-31T19:25:19Z", "updated_at": "2020-08-03T13:17:22Z", "closed_at": "2020-08-03T13:17:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Does Scrapy work with socks4/5 proxies?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4702", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4702/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4702/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4702/events", "html_url": "https://github.com/scrapy/scrapy/issues/4702", "id": 667748434, "node_id": "MDU6SXNzdWU2Njc3NDg0MzQ=", "number": 4702, "title": "Tests fail because of pytest 6.0.0", "user": {"login": "wRAR", "id": 241039, "node_id": "MDQ6VXNlcjI0MTAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/241039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wRAR", "html_url": "https://github.com/wRAR", "followers_url": "https://api.github.com/users/wRAR/followers", "following_url": "https://api.github.com/users/wRAR/following{/other_user}", "gists_url": "https://api.github.com/users/wRAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/wRAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wRAR/subscriptions", "organizations_url": "https://api.github.com/users/wRAR/orgs", "repos_url": "https://api.github.com/users/wRAR/repos", "events_url": "https://api.github.com/users/wRAR/events{/privacy}", "received_events_url": "https://api.github.com/users/wRAR/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443053983, "node_id": "MDU6TGFiZWw0NDMwNTM5ODM=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/CI", "name": "CI", "color": "d4c5f9", "default": false, "description": null}, {"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-07-29T10:53:38Z", "updated_at": "2020-07-30T06:27:26Z", "closed_at": "2020-07-30T06:27:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "See https://travis-ci.org/github/scrapy/scrapy/builds/712802460\r\n\r\n```\r\n.tox/pinned/lib/python3.5/site-packages/pluggy/hooks.py:286: in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n.tox/pinned/lib/python3.5/site-packages/pluggy/manager.py:93: in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n.tox/pinned/lib/python3.5/site-packages/pluggy/manager.py:87: in <lambda>\r\n    firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\n.tox/pinned/lib/python3.5/site-packages/sybil/integration/pytest.py:118: in pytest_collect_file\r\n    return SybilFile(path, parent, sybil)\r\n.tox/pinned/lib/python3.5/site-packages/_pytest/nodes.py:95: in __call__\r\n    warnings.warn(NODE_USE_FROM_PARENT.format(name=self.__name__), stacklevel=2)\r\nE   pytest.PytestDeprecationWarning: Direct construction of SybilFile has been deprecated, please use SybilFile.from_parent.\r\nE   See https://docs.pytest.org/en/stable/deprecations.html#node-construction-changed-to-node-from-parent for more details.\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4699", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4699/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4699/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4699/events", "html_url": "https://github.com/scrapy/scrapy/issues/4699", "id": 666259824, "node_id": "MDU6SXNzdWU2NjYyNTk4MjQ=", "number": 4699, "title": "Requests timing out when setting larger number of CONCURRENT_REQUESTS", "user": {"login": "MartinBorcin", "id": 58526542, "node_id": "MDQ6VXNlcjU4NTI2NTQy", "avatar_url": "https://avatars1.githubusercontent.com/u/58526542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MartinBorcin", "html_url": "https://github.com/MartinBorcin", "followers_url": "https://api.github.com/users/MartinBorcin/followers", "following_url": "https://api.github.com/users/MartinBorcin/following{/other_user}", "gists_url": "https://api.github.com/users/MartinBorcin/gists{/gist_id}", "starred_url": "https://api.github.com/users/MartinBorcin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MartinBorcin/subscriptions", "organizations_url": "https://api.github.com/users/MartinBorcin/orgs", "repos_url": "https://api.github.com/users/MartinBorcin/repos", "events_url": "https://api.github.com/users/MartinBorcin/events{/privacy}", "received_events_url": "https://api.github.com/users/MartinBorcin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-27T12:35:25Z", "updated_at": "2020-07-28T11:12:33Z", "closed_at": "2020-07-28T11:12:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI wanted to build a scrapy spider for broad-crawling of the \".sk\" domain, So I decided to start by downloading the front-page for every domain in a publicly available list of registered .sk domains. I set the spider up with custom_settings as recommended here (https://docs.scrapy.org/en/latest/topics/broad-crawls.html). However, whenever I increase the number of CONCURRENT_REQUESTS I start getting TimeoutErrors for all requests after cca 20 seconds of crawling (when CONCURRENT_REQUESTS=100). This is inconsistent in between individual crawls, sometimes this issue doesn't occur even after a minute. If I decrease the number of CONCURRENT_REQUESTS to 20, The issue persists, but instead of all request timing out after some time, only a couple of them do and then I receive a couple of responses and then a couple time out and so on... (The maximum number of CONCURRENT_REQUESTS that doesn't cause this error is about 10.) \r\nI've been trying to fix this for more than a week now with no luck. At first I thought there's a problem with the DNS, but now I'm using a custom one. In wireshark I also see that the DNS query leaves at the right time and the correct response is received as well. On the other hand, for every request that times out, new request is generated (since then there is space in the queue) and I can see the DNS query and response for that request in wireshark, but it is not followed by any HTTP GET request.\r\n\r\nmy spider:\r\n\r\n```python\r\nclass FrontPageSpider(Spider):\r\n    name = \"frontpagespider\"\r\n    custom_settings = {\r\n        \"DOWNLOAD_DELAY\": 3,\r\n        \"DOWNLOAD_TIMEOUT\": 15,\r\n        \"COOKIES_ENABLED\": False,\r\n        \"AJAXCRAWL_ENABLED\": True,\r\n        \"USER_AGENT\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36\",\r\n        \"ROBOTSTXT_OBEY\": False,\r\n        \"CONCURRENT_REQUESTS\": 100,\r\n        \"REACTOR_THREADPOOL_MAXSIZE\": 20,\r\n        \"LOG_LEVEL\": 'INFO',\r\n        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 1,\r\n        \"RETRY_ENABLED\": False,\r\n        \"DEPTH_PRIORITY\": 1,\r\n        \"SCHEDULER_PRIORITY_QUEUE\": 'scrapy.pqueues.DownloaderAwarePriorityQueue',\r\n        \"SCHEDULER_DISK_QUEUE\": 'scrapy.squeues.PickleFifoDiskQueue',\r\n        \"SCHEDULER_MEMORY_QUEUE\": 'scrapy.squeues.FifoMemoryQueue',\r\n        \"REDIRECT_ENABLED\": False\r\n    }\r\n\r\n    def start_requests(self):\r\n        with open(\"http_crawlable.txt\", \"r\") as f:\r\n            for line in f:\r\n                url = line.strip()\r\n                self.logger.info(f\"Now sending request to {url}...\")\r\n                yield Request(f\"{url}\", callback=self.parse)\r\n\r\n    def parse(self, response):\r\n        self.logger.info(f\"A response from {response.url} just arrived\")\r\n\r\nif __name__ == \"__main__\":\r\n    process = CrawlerProcess(get_project_settings())\r\n    process.crawl(\"frontpagespider\")\r\n    process.start()\r\n```\r\nNote: For ease of debugging, I have decided to only crawl domains that don't use SSL/TLS for now. I am also including the file `http_crawlable.txt` below, which contains a list of urls for some .sk domains that use http. A vast majority of these should work and be responsive.\r\n\r\nTo reproduce the bug, you can either simply run the script above with the file `http_crawlable.txt` in the same directory or run the spider from shell. The issue persists either way. \r\n\r\n**Expected behavior:** Spider sends about 100 requests, then for every response received, it logs it and sends a new request for a url loaded from the file. (Occasional errors are fine, since some of the urls may actually be broken)\r\n\r\n**Actual behavior:** Spider sends about 100 requests, everything seems to be going as expectd, but after about 20 seconds it stops, as if it was waiting for some response and then logs innumerable TimeoutErrors.\r\n\r\n**Reproduces how often:** Almost 100% of time, occasionally it runs as expected, although I have absolutely no idea how or why. \r\n\r\n### Versions\r\n```\r\nScrapy       : 2.2.0\r\nlxml         : 4.5.2.0\r\nlibxml2      : 2.9.5\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : Windows-10-10.0.18362-SP0\r\n```\r\n[http_crawlable.txt](https://github.com/scrapy/scrapy/files/4982332/http_crawlable.txt)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4696", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4696/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4696/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4696/events", "html_url": "https://github.com/scrapy/scrapy/issues/4696", "id": 665807893, "node_id": "MDU6SXNzdWU2NjU4MDc4OTM=", "number": 4696, "title": "<twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'CHUNK_LENGTH' state, still expecting more data to get to 'FINISHED' state.>]", "user": {"login": "sqyqyq", "id": 60164317, "node_id": "MDQ6VXNlcjYwMTY0MzE3", "avatar_url": "https://avatars2.githubusercontent.com/u/60164317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sqyqyq", "html_url": "https://github.com/sqyqyq", "followers_url": "https://api.github.com/users/sqyqyq/followers", "following_url": "https://api.github.com/users/sqyqyq/following{/other_user}", "gists_url": "https://api.github.com/users/sqyqyq/gists{/gist_id}", "starred_url": "https://api.github.com/users/sqyqyq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sqyqyq/subscriptions", "organizations_url": "https://api.github.com/users/sqyqyq/orgs", "repos_url": "https://api.github.com/users/sqyqyq/repos", "events_url": "https://api.github.com/users/sqyqyq/events{/privacy}", "received_events_url": "https://api.github.com/users/sqyqyq/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-26T15:55:48Z", "updated_at": "2020-07-28T11:06:20Z", "closed_at": "2020-07-28T11:06:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n   I am recently using scrapy. but when i am using proxy pool with it there is this kind of error, it is not happens in every request, just sometimes.\r\n\r\n`    <twisted.python.failure.Failure twisted.web.http._DataLoss: Chunked decoder in 'CHUNK_LENGTH' state, still expecting more data to get to 'FINISHED' state.>]`\r\n\r\nAnd looks like every time it happens 1150 times in my spider, the spider stops and freeze, even cannot quit by ctrl-c.\r\n\r\nI want to know how to handle this kind of error, just don't let it get out of 'CHUNK' state, release it, don't let it freeze. The error not happens in every request, So right now, I try to catch exception and let the request retry, but just like i said, it freeze after happened certain times\r\n\r\nIt's not user-agent or host problem because when I run it without proxy I never see this kind of error. And I used the setting `DOWNLOAD_FAIL_ON_DATALOSS = True` but doesn't work.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4684", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4684/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4684/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4684/events", "html_url": "https://github.com/scrapy/scrapy/issues/4684", "id": 658166511, "node_id": "MDU6SXNzdWU2NTgxNjY1MTE=", "number": 4684, "title": "Should `scrapy.utils.python.WeakKeyCache` be deprecated?", "user": {"login": "noviluni", "id": 22377678, "node_id": "MDQ6VXNlcjIyMzc3Njc4", "avatar_url": "https://avatars2.githubusercontent.com/u/22377678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/noviluni", "html_url": "https://github.com/noviluni", "followers_url": "https://api.github.com/users/noviluni/followers", "following_url": "https://api.github.com/users/noviluni/following{/other_user}", "gists_url": "https://api.github.com/users/noviluni/gists{/gist_id}", "starred_url": "https://api.github.com/users/noviluni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/noviluni/subscriptions", "organizations_url": "https://api.github.com/users/noviluni/orgs", "repos_url": "https://api.github.com/users/noviluni/repos", "events_url": "https://api.github.com/users/noviluni/events{/privacy}", "received_events_url": "https://api.github.com/users/noviluni/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-16T12:46:56Z", "updated_at": "2020-08-05T12:48:41Z", "closed_at": "2020-08-05T12:48:41Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Summary\r\nI can't find any reference to `scrapy.utils.python.WeakKeyCache` (except in tests) and I think that it should be deprecated.\r\n\r\n## Motivation\r\nKeeping non-used and non-documented code makes the codebase harder to maintain.\r\n\r\n## Describe alternatives you've considered\r\nAs it's tested we could keep it, but I don't think it's a good idea.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4678", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4678/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4678/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4678/events", "html_url": "https://github.com/scrapy/scrapy/issues/4678", "id": 656324285, "node_id": "MDU6SXNzdWU2NTYzMjQyODU=", "number": 4678, "title": "Enable a middleware only for specific spider?", "user": {"login": "laggardkernel", "id": 12206611, "node_id": "MDQ6VXNlcjEyMjA2NjEx", "avatar_url": "https://avatars3.githubusercontent.com/u/12206611?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laggardkernel", "html_url": "https://github.com/laggardkernel", "followers_url": "https://api.github.com/users/laggardkernel/followers", "following_url": "https://api.github.com/users/laggardkernel/following{/other_user}", "gists_url": "https://api.github.com/users/laggardkernel/gists{/gist_id}", "starred_url": "https://api.github.com/users/laggardkernel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laggardkernel/subscriptions", "organizations_url": "https://api.github.com/users/laggardkernel/orgs", "repos_url": "https://api.github.com/users/laggardkernel/repos", "events_url": "https://api.github.com/users/laggardkernel/events{/privacy}", "received_events_url": "https://api.github.com/users/laggardkernel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-14T04:56:12Z", "updated_at": "2020-07-14T13:49:20Z", "closed_at": "2020-07-14T13:49:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Sorry, settings are not correct in my spider.\r\n\r\n----\r\n\r\n<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your pull request, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#writing-patches and https://doc.scrapy.org/en/latest/contributing.html#submitting-patches\r\n\r\n-->\r\n\r\n## Summary\r\n\r\n`DOWNLOADER_MIDDLEWARES` in spider's `custom_settings` doesn't take effect. Can a middleware be enabled only for specific middleware.\r\n\r\n## Motivation\r\nI tried to use `scrapy-selenium` to parse webpage with dynamically loaded content. I organized my project with different spiders for different sites, and only some of the spiders need to use the `scrapy-selenium`.\r\n\r\n## Describe alternatives you've considered\r\nAFAIK, the `scrapy-selenium` pacakge init the webdriver in classmethod `@from_crawler`, one way to avoid the webdriver initialized for every spider is to delay its initialization when spider is opened with the help of `signals.spider_opened`?\r\n\r\nBut when I deploy the spiders, multiple webdriver are enabled, right?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4677", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4677/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4677/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4677/events", "html_url": "https://github.com/scrapy/scrapy/issues/4677", "id": 656253455, "node_id": "MDU6SXNzdWU2NTYyNTM0NTU=", "number": 4677, "title": "Pass item in FilesPipeline method", "user": {"login": "jvanz", "id": 1514798, "node_id": "MDQ6VXNlcjE1MTQ3OTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1514798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jvanz", "html_url": "https://github.com/jvanz", "followers_url": "https://api.github.com/users/jvanz/followers", "following_url": "https://api.github.com/users/jvanz/following{/other_user}", "gists_url": "https://api.github.com/users/jvanz/gists{/gist_id}", "starred_url": "https://api.github.com/users/jvanz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jvanz/subscriptions", "organizations_url": "https://api.github.com/users/jvanz/orgs", "repos_url": "https://api.github.com/users/jvanz/repos", "events_url": "https://api.github.com/users/jvanz/events{/privacy}", "received_events_url": "https://api.github.com/users/jvanz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-14T01:20:09Z", "updated_at": "2020-08-11T12:12:45Z", "closed_at": "2020-08-11T12:12:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Summary\r\n\r\nOne paragraph explanation of the feature.\r\nWhen called the method to define the path where the file should be stored it would be nice if the method receive the item as parameter. Then the method can dynamically define where the item should be stored based on the item properties.\r\n\r\n## Motivation\r\n\r\nI was working in the https://github.com/okfn-brasil/diario-oficial and we would like to store the item by its date property. But there is not way to do that without a hack. To workaround the issue we need to create a subclass of the request and add the item on it. Thus, we can get the item from the request object later in the method to define the path.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4671", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4671/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4671/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4671/events", "html_url": "https://github.com/scrapy/scrapy/issues/4671", "id": 653885668, "node_id": "MDU6SXNzdWU2NTM4ODU2Njg=", "number": 4671, "title": "Add docs for FEEDS.uri_params and FEED_URI_PARAMS", "user": {"login": "alex-ber", "id": 19841541, "node_id": "MDQ6VXNlcjE5ODQxNTQx", "avatar_url": "https://avatars1.githubusercontent.com/u/19841541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alex-ber", "html_url": "https://github.com/alex-ber", "followers_url": "https://api.github.com/users/alex-ber/followers", "following_url": "https://api.github.com/users/alex-ber/following{/other_user}", "gists_url": "https://api.github.com/users/alex-ber/gists{/gist_id}", "starred_url": "https://api.github.com/users/alex-ber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alex-ber/subscriptions", "organizations_url": "https://api.github.com/users/alex-ber/orgs", "repos_url": "https://api.github.com/users/alex-ber/repos", "events_url": "https://api.github.com/users/alex-ber/events{/privacy}", "received_events_url": "https://api.github.com/users/alex-ber/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-09T08:50:52Z", "updated_at": "2020-08-14T09:47:56Z", "closed_at": "2020-08-14T09:47:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "In https://docs.scrapy.org/en/latest/topics/feed-exports.html#feeds \r\n\r\nThere is missing entry \r\n\r\n`uri_params: falls back to FEED_URI_PARAMS`\r\n\r\nAlso FEED_URI_PARAMS setting description is missing (see https://github.com/scrapy/scrapy/issues/4670)\r\n\r\nHere \r\n\r\n\r\n\r\n    format: the serialization format to be used for the feed. See Serialization formats for possible values. Mandatory, no fallback setting\r\n\r\n    encoding: falls back to FEED_EXPORT_ENCODING\r\n\r\n    fields: falls back to FEED_EXPORT_FIELDS\r\n\r\n    indent: falls back to FEED_EXPORT_INDENT\r\n\r\n    store_empty: falls back to FEED_STORE_EMPTY\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4670", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4670/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4670/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4670/events", "html_url": "https://github.com/scrapy/scrapy/issues/4670", "id": 653882672, "node_id": "MDU6SXNzdWU2NTM4ODI2NzI=", "number": 4670, "title": "Documentation of FEED_URI_PARAMS is absent", "user": {"login": "alex-ber", "id": 19841541, "node_id": "MDQ6VXNlcjE5ODQxNTQx", "avatar_url": "https://avatars1.githubusercontent.com/u/19841541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alex-ber", "html_url": "https://github.com/alex-ber", "followers_url": "https://api.github.com/users/alex-ber/followers", "following_url": "https://api.github.com/users/alex-ber/following{/other_user}", "gists_url": "https://api.github.com/users/alex-ber/gists{/gist_id}", "starred_url": "https://api.github.com/users/alex-ber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alex-ber/subscriptions", "organizations_url": "https://api.github.com/users/alex-ber/orgs", "repos_url": "https://api.github.com/users/alex-ber/repos", "events_url": "https://api.github.com/users/alex-ber/events{/privacy}", "received_events_url": "https://api.github.com/users/alex-ber/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-09T08:46:43Z", "updated_at": "2020-07-09T12:20:09Z", "closed_at": "2020-07-09T12:20:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "See https://docs.scrapy.org/en/latest/topics/feed-exports.html\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4667", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4667/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4667/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4667/events", "html_url": "https://github.com/scrapy/scrapy/issues/4667", "id": 653402809, "node_id": "MDU6SXNzdWU2NTM0MDI4MDk=", "number": 4667, "title": "List of fields incorrectly accessed for dataclass items", "user": {"login": "tadejsv", "id": 11489772, "node_id": "MDQ6VXNlcjExNDg5Nzcy", "avatar_url": "https://avatars0.githubusercontent.com/u/11489772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tadejsv", "html_url": "https://github.com/tadejsv", "followers_url": "https://api.github.com/users/tadejsv/followers", "following_url": "https://api.github.com/users/tadejsv/following{/other_user}", "gists_url": "https://api.github.com/users/tadejsv/gists{/gist_id}", "starred_url": "https://api.github.com/users/tadejsv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tadejsv/subscriptions", "organizations_url": "https://api.github.com/users/tadejsv/orgs", "repos_url": "https://api.github.com/users/tadejsv/repos", "events_url": "https://api.github.com/users/tadejsv/events{/privacy}", "received_events_url": "https://api.github.com/users/tadejsv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/scrapy/scrapy/milestones/30", "html_url": "https://github.com/scrapy/scrapy/milestone/30", "labels_url": "https://api.github.com/repos/scrapy/scrapy/milestones/30/labels", "id": 5483684, "node_id": "MDk6TWlsZXN0b25lNTQ4MzY4NA==", "number": 30, "title": "v2.3", "description": "**Major goals:** [separate ItemLoaders library](https://github.com/scrapy/scrapy/pull/4516), and [coroutine syntax for `start_requests`](https://github.com/scrapy/scrapy/pull/4467)", "creator": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 6, "state": "closed", "created_at": "2020-06-01T04:21:57Z", "updated_at": "2020-08-05T15:01:11Z", "due_on": "2020-07-31T07:00:00Z", "closed_at": "2020-08-05T15:01:11Z"}, "comments": 1, "created_at": "2020-07-08T15:44:07Z", "updated_at": "2020-07-28T09:15:14Z", "closed_at": "2020-07-28T09:15:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nIf I make a `dataclass` item and want to export to csv, I get this error:\r\n\r\n```\r\n...\r\n  File \"/home/tadej/miniconda3/envs/main/lib/python3.7/site-packages/scrapy/exporters.py\", line 251, in _write_headers_and_set_fields_to_export\r\n    self.fields_to_export = list(item.fields.keys())\r\nAttributeError: 'CompanyItem' object has no attribute 'fields'\r\n```\r\nThe problem stems from here\r\n\r\nhttps://github.com/scrapy/scrapy/blob/master/scrapy/exporters.py#L243-L253\r\n\r\nThere should be an additional if case checking if the item is of type dataclass, and then accessing the fields differently, perhaps as\r\n```python\r\n[field.name for field in fields(item)]\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4664", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4664/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4664/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4664/events", "html_url": "https://github.com/scrapy/scrapy/issues/4664", "id": 652233462, "node_id": "MDU6SXNzdWU2NTIyMzM0NjI=", "number": 4664, "title": "Remove Command._copytree from startproject.py", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/scrapy/scrapy/milestones/31", "html_url": "https://github.com/scrapy/scrapy/milestone/31", "labels_url": "https://api.github.com/repos/scrapy/scrapy/milestones/31/labels", "id": 5627057, "node_id": "MDk6TWlsZXN0b25lNTYyNzA1Nw==", "number": 31, "title": "Python 3.8+", "description": "Changes that do not support Python 3.7, which [reaches end of life by June 27th, 2023](https://devguide.python.org/#status-of-python-branches).", "creator": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "open_issues": 1, "closed_issues": 1, "state": "open", "created_at": "2020-07-07T11:07:51Z", "updated_at": "2020-08-20T13:37:52Z", "due_on": "2023-06-27T07:00:00Z", "closed_at": null}, "comments": 0, "created_at": "2020-07-07T11:09:38Z", "updated_at": "2020-07-13T11:01:07Z", "closed_at": "2020-07-13T11:01:07Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Python 3.8 introduced `dirs_exist_ok`, which makes our method unnecessary.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4662", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4662/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4662/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4662/events", "html_url": "https://github.com/scrapy/scrapy/issues/4662", "id": 651709561, "node_id": "MDU6SXNzdWU2NTE3MDk1NjE=", "number": 4662, "title": "scrapy startproject alters executable permissions in local virtualenv", "user": {"login": "dhosterman", "id": 3520372, "node_id": "MDQ6VXNlcjM1MjAzNzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3520372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dhosterman", "html_url": "https://github.com/dhosterman", "followers_url": "https://api.github.com/users/dhosterman/followers", "following_url": "https://api.github.com/users/dhosterman/following{/other_user}", "gists_url": "https://api.github.com/users/dhosterman/gists{/gist_id}", "starred_url": "https://api.github.com/users/dhosterman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dhosterman/subscriptions", "organizations_url": "https://api.github.com/users/dhosterman/orgs", "repos_url": "https://api.github.com/users/dhosterman/repos", "events_url": "https://api.github.com/users/dhosterman/events{/privacy}", "received_events_url": "https://api.github.com/users/dhosterman/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/scrapy/scrapy/milestones/32", "html_url": "https://github.com/scrapy/scrapy/milestone/32", "labels_url": "https://api.github.com/repos/scrapy/scrapy/milestones/32/labels", "id": 5633575, "node_id": "MDk6TWlsZXN0b25lNTYzMzU3NQ==", "number": 32, "title": "v2.2.1", "description": "Bugfixes for the 2.2 release", "creator": {"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 2, "state": "closed", "created_at": "2020-07-08T17:03:09Z", "updated_at": "2020-08-05T15:01:15Z", "due_on": null, "closed_at": "2020-08-05T15:01:15Z"}, "comments": 1, "created_at": "2020-07-06T17:34:57Z", "updated_at": "2020-07-13T11:01:06Z", "closed_at": "2020-07-13T11:01:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nWhen running `scrapy startproject` after creating a virtualenv in .venv in the project directory, the executable files have their permissions changed.\r\n\r\n### Steps to Reproduce (using Poetry)\r\n\r\n1. Enter new project directory\r\n2. `poetry init -n`\r\n3. `poetry config virtualenvs.in-project true --local`\r\n4. `poetry add scrapy`\r\n5. `poetry shell`\r\n6. `scrapy startproject scraper .`\r\n7. `scrapy` <-- fails\r\n8. `python` <-- fails\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\nI expect to still be able to execute `scrapy` or `python` commands.\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\nI am unable to execute `scrapy` or `python` commands because everything in .venv/bin has had its executable permissions removed including the python executable that is the target of the `python` symlink.\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\n100%\r\n\r\n### Versions\r\n\r\nScrapy       : 2.2.0\r\nlxml         : 4.5.1.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 3.8.3 (default, Jun 27 2020, 12:47:10) - [Clang 11.0.0 (clang-1100.0.33.8)]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : macOS-10.15.5-x86_64-i386-64bit\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4660", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4660/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4660/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4660/events", "html_url": "https://github.com/scrapy/scrapy/issues/4660", "id": 651451223, "node_id": "MDU6SXNzdWU2NTE0NTEyMjM=", "number": 4660, "title": "Replace Headers.to_unicode_dict with Headers.to_str_dict", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-06T11:07:33Z", "updated_at": "2020-07-06T11:08:50Z", "closed_at": "2020-07-06T11:08:50Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The latter would be more consistent, now that we\u2019ve dropped Python 2 support.\r\n\r\nRelated to #4547", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4656", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4656/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4656/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4656/events", "html_url": "https://github.com/scrapy/scrapy/issues/4656", "id": 651112934, "node_id": "MDU6SXNzdWU2NTExMTI5MzQ=", "number": 4656, "title": "Scrapy spider works on MacOS Catalina 10.15.4(scrapy2.1.0), does NOT work on Ubuntu 19.10 (scrapy 1.7.3)", "user": {"login": "wangjiahong666", "id": 14320144, "node_id": "MDQ6VXNlcjE0MzIwMTQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/14320144?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangjiahong666", "html_url": "https://github.com/wangjiahong666", "followers_url": "https://api.github.com/users/wangjiahong666/followers", "following_url": "https://api.github.com/users/wangjiahong666/following{/other_user}", "gists_url": "https://api.github.com/users/wangjiahong666/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangjiahong666/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangjiahong666/subscriptions", "organizations_url": "https://api.github.com/users/wangjiahong666/orgs", "repos_url": "https://api.github.com/users/wangjiahong666/repos", "events_url": "https://api.github.com/users/wangjiahong666/events{/privacy}", "received_events_url": "https://api.github.com/users/wangjiahong666/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-05T17:39:51Z", "updated_at": "2020-07-10T22:03:32Z", "closed_at": "2020-07-10T22:03:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, \r\n\r\nI have a scrapy spider which works well on MacOs Catalina 10.15.4(scrapy2.1.0), but the spider does not work on  Ubuntu 19.10 (scrapy 1.7.3). I feel it might be the version reason. \r\n\r\nError log: \r\n\r\n`2020-07-05 17:30:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2020-07-05 17:30:59 [scrapy.core.engine] ERROR: Error while obtaining start requests\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/scrapy/core/engine.py\", line 127, in _next_request\r\n    request = next(slot.start_requests)\r\n  File \"/home/jiahong/Crawler/instagram_crawler/scrapy/ins_crawl/ins_crawl/spiders/ins2.py\", line 54, in start_requests\r\n    user = self.shared_data['entry_data']['ProfilePage'][0]['graphql']['user']\r\nKeyError: 'ProfilePage'\r\n`\r\n\r\nCould anyone help me with debug this? Or what direction should I go to debug this? \r\n\r\nLet me know if more info needed. \r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4655", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4655/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4655/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4655/events", "html_url": "https://github.com/scrapy/scrapy/issues/4655", "id": 650044702, "node_id": "MDU6SXNzdWU2NTAwNDQ3MDI=", "number": 4655, "title": "Documentation describing how to setup environment for scrapy contributions ", "user": {"login": "nb2838", "id": 61052993, "node_id": "MDQ6VXNlcjYxMDUyOTkz", "avatar_url": "https://avatars1.githubusercontent.com/u/61052993?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nb2838", "html_url": "https://github.com/nb2838", "followers_url": "https://api.github.com/users/nb2838/followers", "following_url": "https://api.github.com/users/nb2838/following{/other_user}", "gists_url": "https://api.github.com/users/nb2838/gists{/gist_id}", "starred_url": "https://api.github.com/users/nb2838/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nb2838/subscriptions", "organizations_url": "https://api.github.com/users/nb2838/orgs", "repos_url": "https://api.github.com/users/nb2838/repos", "events_url": "https://api.github.com/users/nb2838/events{/privacy}", "received_events_url": "https://api.github.com/users/nb2838/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-02T15:59:58Z", "updated_at": "2020-07-02T19:10:20Z", "closed_at": "2020-07-02T19:10:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey! I recently started to contributing to scrapy and noticed that there isn't any documentation on how to set-up the environment to edit the code in your own machine (setting up a virtual environment, building, etc). I think this would make the process a little bit more user friendly for new contributors. I was wondering if you thought this was a good idea and perhaps I could edit the docs https://docs.scrapy.org/en/latest/contributing.html to add this piece of information.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4647", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4647/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4647/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4647/events", "html_url": "https://github.com/scrapy/scrapy/issues/4647", "id": 645490215, "node_id": "MDU6SXNzdWU2NDU0OTAyMTU=", "number": 4647, "title": "Unwanted redirects", "user": {"login": "yilu1015", "id": 25930948, "node_id": "MDQ6VXNlcjI1OTMwOTQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/25930948?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yilu1015", "html_url": "https://github.com/yilu1015", "followers_url": "https://api.github.com/users/yilu1015/followers", "following_url": "https://api.github.com/users/yilu1015/following{/other_user}", "gists_url": "https://api.github.com/users/yilu1015/gists{/gist_id}", "starred_url": "https://api.github.com/users/yilu1015/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yilu1015/subscriptions", "organizations_url": "https://api.github.com/users/yilu1015/orgs", "repos_url": "https://api.github.com/users/yilu1015/repos", "events_url": "https://api.github.com/users/yilu1015/events{/privacy}", "received_events_url": "https://api.github.com/users/yilu1015/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-06-25T11:36:20Z", "updated_at": "2020-06-25T14:18:21Z", "closed_at": "2020-06-25T12:56:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "I ask scrapy to use the 'POST' method, but it automatically redirects me to GET. This persists even when I explicitly added REDIRECTS_ENABLED = False in settings.py. Originally, I thought the `302` redirect was an anti-spider response from the server; probably my IP was blocked. But I was able to receive a `200` status code using the `requests` library on Python, all using my current IP.\r\n \r\nWhy is this happening? How does `scrapy.FormRequest` work differently from `requests.post`?\r\n\r\nMy crawl log:\r\n```\r\n2020-06-24 08:33:26 [scrapy.core.engine] INFO: Spider opened\r\n2020-06-24 08:33:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-06-24 08:33:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2020-06-24 08:33:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://wenshuapp.court.gov.cn/authenticatin/require> from <POST http://wenshuapp.court.gov.cn/appinterface/rest.q4w/>\r\n2020-06-24 08:33:27 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET http://wenshuapp.court.gov.cn/sysadmin/login.q4w> from <GET http://wenshuapp.court.gov.cn/authenticatin/require>\r\n2020-06-24 08:33:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://wenshuapp.court.gov.cn/sysadmin/login.q4w> (referer: None)\r\n2020-06-24 08:33:28 [scrapy.core.scraper] ERROR: Spider error processing <GET http://wenshuapp.court.gov.cn/sysadmin/login.q4w> (referer: None)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4644", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4644/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4644/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4644/events", "html_url": "https://github.com/scrapy/scrapy/issues/4644", "id": 643819021, "node_id": "MDU6SXNzdWU2NDM4MTkwMjE=", "number": 4644, "title": "test_integration_downloader_aware_priority_queue raises exception", "user": {"login": "Lukas0907", "id": 591792, "node_id": "MDQ6VXNlcjU5MTc5Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/591792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lukas0907", "html_url": "https://github.com/Lukas0907", "followers_url": "https://api.github.com/users/Lukas0907/followers", "following_url": "https://api.github.com/users/Lukas0907/following{/other_user}", "gists_url": "https://api.github.com/users/Lukas0907/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lukas0907/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lukas0907/subscriptions", "organizations_url": "https://api.github.com/users/Lukas0907/orgs", "repos_url": "https://api.github.com/users/Lukas0907/repos", "events_url": "https://api.github.com/users/Lukas0907/events{/privacy}", "received_events_url": "https://api.github.com/users/Lukas0907/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-23T12:56:02Z", "updated_at": "2020-06-25T13:36:44Z", "closed_at": "2020-06-25T13:36:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\n\r\nThe test `tests/test_scheduler.py::TestIntegrationWithDownloaderAwareInMemory::test_integration_downloader_aware_priority_queue` raises the following exception:\r\n\r\n```\r\nERROR    scrapy.core.engine:engine.py:308 Stats close failure\r\nTraceback (most recent call last):\r\n  File \"/home/lukas/Projects/3rd/scrapy/.tox/py/lib/python3.8/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\r\n    current.result = callback(current.result, *args, **kw)\r\n  File \"/home/lukas/Projects/3rd/scrapy/scrapy/core/engine.py\", line 328, in <lambda>\r\n    dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\r\n  File \"/home/lukas/Projects/3rd/scrapy/scrapy/statscollectors.py\", line 48, in close_spider\r\n    self._persist_stats(self._stats, spider)\r\n  File \"/home/lukas/Projects/3rd/scrapy/scrapy/statscollectors.py\", line 61, in _persist_stats\r\n    self.spider_stats[spider.name] = stats\r\nTypeError: unhashable type: 'list'\r\n```\r\n\r\nThe exception is only logged by scrapy.core.engine and hence swallowed by py.test. I discovered it accidentally when using the `log_cli = true` pytest setting.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Set `log_cli = true` in pytest.ini\r\n2. Run `tox -e py -- tests/test_scheduler.py -k test_integration_downloader_aware_priority_queue`\r\n\r\n### Versions\r\n\r\nScrapy 2.1.0 / master\r\n\r\n### Additional context\r\n\r\nThe issue should be simple to fix, I will prepare a PR for it.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4641", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4641/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4641/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4641/events", "html_url": "https://github.com/scrapy/scrapy/issues/4641", "id": 643151800, "node_id": "MDU6SXNzdWU2NDMxNTE4MDA=", "number": 4641, "title": "S3FilesStore throws TypeError: __init__() missing 1 required positional argument: 'uri'", "user": {"login": "gesof", "id": 2393557, "node_id": "MDQ6VXNlcjIzOTM1NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2393557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gesof", "html_url": "https://github.com/gesof", "followers_url": "https://api.github.com/users/gesof/followers", "following_url": "https://api.github.com/users/gesof/following{/other_user}", "gists_url": "https://api.github.com/users/gesof/gists{/gist_id}", "starred_url": "https://api.github.com/users/gesof/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gesof/subscriptions", "organizations_url": "https://api.github.com/users/gesof/orgs", "repos_url": "https://api.github.com/users/gesof/repos", "events_url": "https://api.github.com/users/gesof/events{/privacy}", "received_events_url": "https://api.github.com/users/gesof/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-06-22T15:12:13Z", "updated_at": "2020-06-27T11:46:37Z", "closed_at": "2020-06-26T14:33:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "For some reason I keep getting annoying errors while trying to make the frontera/scrapy combination work and most of the time is due to lack of *really* working examples. Here is the last one:\r\n\r\nsettings.py\r\n```\r\n# export to S3 settings\r\nAWS_ACCESS_KEY_ID = '***'\r\nAWS_SECRET_ACCESS_KEY = '****'\r\n\r\nFEED_URI = 's3://somecrawler/%(name)s/%(today)s.html'\r\nFEEDS = {\r\n    's3': {\r\n        'format': 'html',\r\n        'encoding': 'utf8',\r\n        'store_empty': False,\r\n        'fields': ['content'],\r\n        'indent': 4,\r\n    }\r\n}\r\n\r\nFEED_FORMAT = 'jsonlines'\r\n\r\nITEM_PIPELINES = {\r\n    'scrapy.pipelines.files.S3FilesStore': 1\r\n}\r\n```\r\n\r\nPlease (2x) just be a little more elaborate in the documentation. Sample: https://docs.scrapy.org/en/latest/topics/feed-exports.html#feeds\r\n\r\n`A dictionary in which every key is a feed URI ..` followed by an example with 3 keys, none related to a S3 case. And there are only 3 storage backends to store in the documentation so I would expect to see one example for each. Just saying.. \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4639", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4639/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4639/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4639/events", "html_url": "https://github.com/scrapy/scrapy/issues/4639", "id": 642682260, "node_id": "MDU6SXNzdWU2NDI2ODIyNjA=", "number": 4639, "title": "Print weird None after every successfully crawled", "user": {"login": "jzwz", "id": 443466, "node_id": "MDQ6VXNlcjQ0MzQ2Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/443466?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jzwz", "html_url": "https://github.com/jzwz", "followers_url": "https://api.github.com/users/jzwz/followers", "following_url": "https://api.github.com/users/jzwz/following{/other_user}", "gists_url": "https://api.github.com/users/jzwz/gists{/gist_id}", "starred_url": "https://api.github.com/users/jzwz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jzwz/subscriptions", "organizations_url": "https://api.github.com/users/jzwz/orgs", "repos_url": "https://api.github.com/users/jzwz/repos", "events_url": "https://api.github.com/users/jzwz/events{/privacy}", "received_events_url": "https://api.github.com/users/jzwz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-06-22T01:18:39Z", "updated_at": "2020-06-24T05:51:18Z", "closed_at": "2020-06-24T05:51:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\n[Description of the issue]\r\n\r\n### Steps to Reproduce\r\n**Expected behavior:** [What you expect to happen]\r\nWhy print \"None\" after scraped successfully? Is that normal?\r\n\r\n**Actual behavior:** [What actually happens]\r\nScrapy prints \"None\" after scraped successfully every request.\r\n\r\n2020-06-22 12:51:05 [scrapy.core.scraper] DEBUG: Scraped from <200 https://url/>\r\nNone\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\nEvery time\r\n\r\n### Versions\r\nScrapy       : 2.1.0\r\nlxml         : 4.5.1.0\r\nlibxml2      : 2.9.5\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : Windows-10-10.0.18362-SP0\r\n\r\n\r\n### Additional context\r\nI debugged and invested scrapy code, in scrapy/core/scraper.py\r\nLine 254:\r\nlogkws = self.logformatter.scraped(output, response, spider)\r\nHere parameter \"output\" is None, meanwhile \"item\" in context is not None, is valid value. \r\n\r\nfollowing the calling in scrapy/logformatter.py\r\nLine 73: def scraped(self, item, response, spider)\r\n\"scraped\" method should receive first parameter names \"item\"\r\n\r\nMy question is at line 254 in scraper.py, \r\nshould be changed to below code.\r\nlogkws = self.logformatter.scraped(item, response, spider)\r\nIn this case, it won't print None after every successfully scraped. (I guess)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4638", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4638/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4638/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4638/events", "html_url": "https://github.com/scrapy/scrapy/issues/4638", "id": 642672506, "node_id": "MDU6SXNzdWU2NDI2NzI1MDY=", "number": 4638, "title": "Trying to install scrapy on a pi 4. get 404 error", "user": {"login": "Tricoms", "id": 22665381, "node_id": "MDQ6VXNlcjIyNjY1Mzgx", "avatar_url": "https://avatars3.githubusercontent.com/u/22665381?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tricoms", "html_url": "https://github.com/Tricoms", "followers_url": "https://api.github.com/users/Tricoms/followers", "following_url": "https://api.github.com/users/Tricoms/following{/other_user}", "gists_url": "https://api.github.com/users/Tricoms/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tricoms/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tricoms/subscriptions", "organizations_url": "https://api.github.com/users/Tricoms/orgs", "repos_url": "https://api.github.com/users/Tricoms/repos", "events_url": "https://api.github.com/users/Tricoms/events{/privacy}", "received_events_url": "https://api.github.com/users/Tricoms/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-22T00:35:13Z", "updated_at": "2020-06-22T07:55:16Z", "closed_at": "2020-06-22T07:55:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n pip install scrappy\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nCollecting scrappy\r\n  Using cached https://files.pythonhosted.org/packages/b4/df/dcd763f44aea90fbd58b4e7f7c8be95ade7d130cd21251fc0d93c295b1f0/Scrappy-0.3.0.alpha.4.tar.gz\r\nRequirement already satisfied: chardet in /usr/lib/python2.7/dist-packages (from scrappy) (3.0.4)\r\nCollecting guessit (from scrappy)\r\n  Using cached https://files.pythonhosted.org/packages/09/a1/c37ccc25872d463f94dbde723430c7129dc6d7f790c5b0030a5690e2f58a/guessit-3.1.1.tar.gz\r\nCollecting hachoir-core (from scrappy)\r\nCould not install packages due to an EnvironmentError: 404 Client Error: Not Found for url: https://pypi.org/simple/hachoir-core/\r\n\r\nIt's true the site  https://pypi.org/simple/hachoir-core/ is not there.\r\n\r\nAny help?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4633", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4633/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4633/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4633/events", "html_url": "https://github.com/scrapy/scrapy/issues/4633", "id": 640160799, "node_id": "MDU6SXNzdWU2NDAxNjA3OTk=", "number": 4633, "title": "Wrong request URL in response.request object from download middleware", "user": {"login": "amarynets", "id": 5521599, "node_id": "MDQ6VXNlcjU1MjE1OTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5521599?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amarynets", "html_url": "https://github.com/amarynets", "followers_url": "https://api.github.com/users/amarynets/followers", "following_url": "https://api.github.com/users/amarynets/following{/other_user}", "gists_url": "https://api.github.com/users/amarynets/gists{/gist_id}", "starred_url": "https://api.github.com/users/amarynets/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amarynets/subscriptions", "organizations_url": "https://api.github.com/users/amarynets/orgs", "repos_url": "https://api.github.com/users/amarynets/repos", "events_url": "https://api.github.com/users/amarynets/events{/privacy}", "received_events_url": "https://api.github.com/users/amarynets/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-17T05:49:26Z", "updated_at": "2020-06-17T15:05:07Z", "closed_at": "2020-06-17T14:04:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nI wrote my custom download middleware, which makes the request to my service(it lives on a different domain than a requested domain). After I receive a response from my service I replace URL in `response.url` and `response.request.url` to the original URL. But the issue is in `parse` function the URL in the `response.request` object is the URL from request to my service.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Original URL - http://example.com/some_page\r\n2. URL of the cache request - http://my_service.com/?url=http://example.com/some_page\r\n3. I replace the URL in middleware for `request` and `response` from step 2 to step 1\r\n4. In `parse` function `response.url` is correct but `request.url` is from step 2\r\n\r\n**Expected behavior:** request URL will be correct\r\n\r\n**Actual behavior:** request URL isn't correct\r\n\r\n**Reproduces how often:** each time\r\n\r\n### Versions\r\n\r\n1.8.0 - 2.1.0\r\n\r\n### Additional context\r\nHere is the code of my middleware\r\n```\r\nclass MyServiceDownloaderMiddleware:\r\n\r\n    def process_request(self, request, spider):\r\n        if request.meta.get('__service_search') or request.meta.get('__service_action'):\r\n            return None\r\n        elif request.url.startswith(spider.eligible_url):\r\n            qs = {\r\n                'url': request.url,\r\n            }\r\n            request.meta['__service_search'] = True\r\n            request.meta['__original_url'] = request.url\r\n\r\n            new_request = request.replace(url=f'https://my_service.com/?{urlencode(qs)}')\r\n            return new_request\r\n        return None\r\n\r\n    def process_response(self, request, response, spider):\r\n        if request.meta.get('__service_action'):\r\n            request = request.replace(url=request.meta['__original_url'])\r\n            response = response.replace(url=request.meta['__original_url'], request=request)\r\n            return response\r\n        elif request.meta.get('__service_search'):\r\n            b_attribute = response.css('div.b_attr:attr(u)').extract_first()\r\n            if not b_attribute:\r\n                request = request.replace(url=request.meta['__original_url'])\r\n                request.meta.pop('__original_url')\r\n                request.meta.pop('__service_search')\r\n                response = response.replace(status=404, url=request.meta['__original_url'], request=request)\r\n                \r\n                return response\r\n            _, _, d, w = b_attribute.split('|')\r\n            qs = {\r\n                'q': request.meta['__original_url'],\r\n                'd': d,\r\n                'w': w,\r\n            }\r\n            url = f'https://my_service.com/action/?{urlencode(qs)}'\r\n            new_request = request.replace(url=url)\r\n            new_request.meta['__service_action'] = True\r\n            return new_request\r\n        return response\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4631", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4631/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4631/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4631/events", "html_url": "https://github.com/scrapy/scrapy/issues/4631", "id": 638646765, "node_id": "MDU6SXNzdWU2Mzg2NDY3NjU=", "number": 4631, "title": "Why type pip install Scrapy will install 1.8.0 version ?", "user": {"login": "motogod", "id": 17781603, "node_id": "MDQ6VXNlcjE3NzgxNjAz", "avatar_url": "https://avatars1.githubusercontent.com/u/17781603?v=4", "gravatar_id": "", "url": "https://api.github.com/users/motogod", "html_url": "https://github.com/motogod", "followers_url": "https://api.github.com/users/motogod/followers", "following_url": "https://api.github.com/users/motogod/following{/other_user}", "gists_url": "https://api.github.com/users/motogod/gists{/gist_id}", "starred_url": "https://api.github.com/users/motogod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/motogod/subscriptions", "organizations_url": "https://api.github.com/users/motogod/orgs", "repos_url": "https://api.github.com/users/motogod/repos", "events_url": "https://api.github.com/users/motogod/events{/privacy}", "received_events_url": "https://api.github.com/users/motogod/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-06-15T08:24:25Z", "updated_at": "2020-06-16T02:16:05Z", "closed_at": "2020-06-16T02:16:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "mac: 10.15.5\r\n\r\nI have installed scrapy 1.8.0, I want to upgrade to 2.1.0\r\n\r\nSo I type `sudo pip install --upgrade scrapy`, it is not working.\r\n\r\nThen I try to reinstall scrapy on terminal.\r\nHere is my step:\r\n1. `python --version` (print Python 3.7.6)\r\n2. `sudo pip uninstall scrapy` (uninstall successfully)\r\n3. `sudo pip install Scrapy`\r\n4. `Scrapy --version`\r\n\r\nthe terminal shows `Scrapy 1.8.0 - no active project`\r\n\r\nWhy I can't install 2.1.0 version ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4628", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4628/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4628/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4628/events", "html_url": "https://github.com/scrapy/scrapy/issues/4628", "id": 638145573, "node_id": "MDU6SXNzdWU2MzgxNDU1NzM=", "number": 4628, "title": "Support choosing a file path based on item data in media pipelines", "user": {"login": "hosunrise", "id": 3882656, "node_id": "MDQ6VXNlcjM4ODI2NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/3882656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hosunrise", "html_url": "https://github.com/hosunrise", "followers_url": "https://api.github.com/users/hosunrise/followers", "following_url": "https://api.github.com/users/hosunrise/following{/other_user}", "gists_url": "https://api.github.com/users/hosunrise/gists{/gist_id}", "starred_url": "https://api.github.com/users/hosunrise/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hosunrise/subscriptions", "organizations_url": "https://api.github.com/users/hosunrise/orgs", "repos_url": "https://api.github.com/users/hosunrise/repos", "events_url": "https://api.github.com/users/hosunrise/events{/privacy}", "received_events_url": "https://api.github.com/users/hosunrise/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-13T11:26:56Z", "updated_at": "2020-08-11T12:12:45Z", "closed_at": "2020-08-11T12:12:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/scrapy/scrapy/blob/8b549392f924ddad9536e55c6120638daf688dfd/scrapy/pipelines/media.py#L77-L111", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4625", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4625/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4625/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4625/events", "html_url": "https://github.com/scrapy/scrapy/issues/4625", "id": 636775539, "node_id": "MDU6SXNzdWU2MzY3NzU1Mzk=", "number": 4625, "title": "request_to_dict using wrong header conversion", "user": {"login": "scientes", "id": 34819304, "node_id": "MDQ6VXNlcjM0ODE5MzA0", "avatar_url": "https://avatars3.githubusercontent.com/u/34819304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scientes", "html_url": "https://github.com/scientes", "followers_url": "https://api.github.com/users/scientes/followers", "following_url": "https://api.github.com/users/scientes/following{/other_user}", "gists_url": "https://api.github.com/users/scientes/gists{/gist_id}", "starred_url": "https://api.github.com/users/scientes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scientes/subscriptions", "organizations_url": "https://api.github.com/users/scientes/orgs", "repos_url": "https://api.github.com/users/scientes/repos", "events_url": "https://api.github.com/users/scientes/events{/privacy}", "received_events_url": "https://api.github.com/users/scientes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-11T07:08:59Z", "updated_at": "2020-07-14T09:21:30Z", "closed_at": "2020-07-14T09:21:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n### Description\r\n\r\nIm writing my own backend for storing queue/dupefilter and such for arangodb(similar to https://github.com/filyph/scrapy-sqlite)\r\n\r\nto store the Requests i need to convert them to json.\r\n\r\n### Steps to Reproduce\r\n\r\n1. encode request using request_to_dict\r\n2. json encode this\r\n\r\n**Expected behavior:** \r\n\r\nA json encoded request\r\n\r\n**Actual behavior:** \r\n\r\n```\r\nFile \"/home/scientes/.local/lib/python3.8/site-packages/twisted/internet/task.py\", line 517, in _oneWorkUnit\r\n    result = next(self._iterator)\r\n  File \"/home/scientes/.local/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 74, in <genexpr>\r\n    work = (callable(elem, *args, **named) for elem in iterable)\r\n  File \"/home/scientes/.local/lib/python3.8/site-packages/scrapy/core/scraper.py\", line 193, in _process_spidermw_output\r\n    self.crawler.engine.crawl(request=output, spider=spider)\r\n  File \"/home/scientes/.local/lib/python3.8/site-packages/scrapy/core/engine.py\", line 216, in crawl\r\n    self.schedule(request, spider)\r\n  File \"/home/scientes/.local/lib/python3.8/site-packages/scrapy/core/engine.py\", line 222, in schedule\r\n    if not self.slot.scheduler.enqueue_request(request):\r\n  File \"/usr/local/lib/python3.8/dist-packages/scrapyarango/scheduler.py\", line 106, in enqueue_request\r\n    self.queue_cls.push(request,self._downloader_interface.get_slot_key(request))\r\n  File \"/usr/local/lib/python3.8/dist-packages/scrapyarango/queue.py\", line 96, in push\r\n    raise e\r\n  File \"/usr/local/lib/python3.8/dist-packages/scrapyarango/queue.py\", line 91, in push\r\n    self.table.createDocument(doc).save()\r\n  File \"/home/scientes/.local/lib/python3.8/site-packages/pyArango/document.py\", line 256, in save\r\n    self._save(payload, waitForSync = False, **docArgs)\r\n  File \"/home/scientes/.local/lib/python3.8/site-packages/pyArango/document.py\", line 275, in _save\r\n    payload = json.dumps(payload, default=str)\r\n  File \"/usr/lib/python3.8/json/__init__.py\", line 234, in dumps\r\n    return cls(\r\n  File \"/usr/lib/python3.8/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.8/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\nTypeError: keys must be str, int, float, bool or None, not bytes\r\n\r\n```\r\n\r\n\r\n### Versions\r\n\r\nScrapy       : 2.1.0\r\nlxml         : 4.5.1.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 3.8.0 (default, Oct 28 2019, 16:14:01) - [GCC 8.3.0]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : Linux-4.19.84-microsoft-standard-x86_64-with-glibc2.27\r\n\r\n### Additional context\r\n\r\nthe request in dict form:\r\n\r\n```\r\n{'url': 'https://www.faz.net/aktuell/sport/fussball/dfb-pokal-fc-bayern-mit-etwas-dusel-gegen-frankfurt-ins-finale-16810113.html', 'callback': 'parse_faz', 'errback': None, 'method': 'GET', 'headers': {b'Referer': [b'https://www.faz.net/sitemap-news.xml']}, 'body': None, 'cookies': {}, 'meta': {'depth': 1}, '_encoding': 'utf-8', 'priority': 49999, 'dont_filter': False, 'flags': [], 'cb_kwargs': {}}\r\n```\r\nthe problem lies here: {b'Referer': [b'https://www.faz.net/sitemap-news.xml']} because binary is not a valid json key\r\n\r\nthe solution:\r\nin: https://github.com/scrapy/scrapy/blob/master/scrapy/utils/reqser.py\r\nin line 28 instead of `'headers': dict(request.headers),` use:\r\n`'headers': request.headers.to_unicode_dict(),`  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4621", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4621/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4621/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4621/events", "html_url": "https://github.com/scrapy/scrapy/issues/4621", "id": 636022807, "node_id": "MDU6SXNzdWU2MzYwMjI4MDc=", "number": 4621, "title": "Storage.store is called only for the first feed when empty", "user": {"login": "StasDeep", "id": 17574404, "node_id": "MDQ6VXNlcjE3NTc0NDA0", "avatar_url": "https://avatars0.githubusercontent.com/u/17574404?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StasDeep", "html_url": "https://github.com/StasDeep", "followers_url": "https://api.github.com/users/StasDeep/followers", "following_url": "https://api.github.com/users/StasDeep/following{/other_user}", "gists_url": "https://api.github.com/users/StasDeep/gists{/gist_id}", "starred_url": "https://api.github.com/users/StasDeep/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StasDeep/subscriptions", "organizations_url": "https://api.github.com/users/StasDeep/orgs", "repos_url": "https://api.github.com/users/StasDeep/repos", "events_url": "https://api.github.com/users/StasDeep/events{/privacy}", "received_events_url": "https://api.github.com/users/StasDeep/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-10T07:57:56Z", "updated_at": "2020-06-17T15:08:15Z", "closed_at": "2020-06-17T15:08:15Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\n\r\nWhen having `FEED_STORE_EMPTY=False` and multiple feeds, and the output is empty, `storage.store` is called until first empty feed slot is encountered.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Create a `test.py` file with [this spider](https://gist.github.com/StasDeep/c7f0b775eb4897b1012f2c76ce6296f5)\r\n2. `scrapy runspider test.py -L INFO`\r\n\r\n**Expected behavior:**\r\n\r\n```\r\n2020-06-10 10:50:27 [test] INFO: Storing in thread: gs://bucket/output.json\r\n2020-06-10 10:50:27 [test] INFO: Storing in thread: gs://bucket/output.csv\r\n```\r\n\r\n**Actual behavior:**\r\n\r\n```\r\n2020-06-10 10:50:27 [test] INFO: Storing in thread: gs://bucket/output.json\r\n```\r\n\r\n**Reproduces how often:** always\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.1.0\r\nlxml         : 4.5.1.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 3.7.4 (default, Sep  4 2019, 15:20:53) - [Clang 10.0.0 (clang-1000.10.44.4)]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : Darwin-19.4.0-x86_64-i386-64bit\r\n```\r\n\r\n### Reason\r\n\r\nIn `FeedExporter.close_spider`, it **returns** a deferred from inside a loop instead of adding it to `deferred_list`\r\n\r\n```\r\n        deferred_list = []\r\n        for slot in self.slots:\r\n            if not slot.itemcount and not slot.store_empty:\r\n                # We need to call slot.storage.store nonetheless to get the file\r\n                # properly closed.\r\n                return defer.maybeDeferred(slot.storage.store, slot.file)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4619", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4619/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4619/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4619/events", "html_url": "https://github.com/scrapy/scrapy/issues/4619", "id": 635582080, "node_id": "MDU6SXNzdWU2MzU1ODIwODA=", "number": 4619, "title": "Duplicated feed logs when having multiple feeds", "user": {"login": "StasDeep", "id": 17574404, "node_id": "MDQ6VXNlcjE3NTc0NDA0", "avatar_url": "https://avatars0.githubusercontent.com/u/17574404?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StasDeep", "html_url": "https://github.com/StasDeep", "followers_url": "https://api.github.com/users/StasDeep/followers", "following_url": "https://api.github.com/users/StasDeep/following{/other_user}", "gists_url": "https://api.github.com/users/StasDeep/gists{/gist_id}", "starred_url": "https://api.github.com/users/StasDeep/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StasDeep/subscriptions", "organizations_url": "https://api.github.com/users/StasDeep/orgs", "repos_url": "https://api.github.com/users/StasDeep/repos", "events_url": "https://api.github.com/users/StasDeep/events{/privacy}", "received_events_url": "https://api.github.com/users/StasDeep/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-09T16:27:31Z", "updated_at": "2020-06-23T10:33:49Z", "closed_at": "2020-06-23T10:33:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\n\r\nWhen I have multiple feeds and a blocking feed storage, I get duplicated feed logs.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Create a `test.py` file with this [spider and settings](https://gist.github.com/StasDeep/236922e448ac33b354cf5ea6612e8fde)\r\n2. `scrapy runspider test.py -L INFO`\r\n\r\n**Expected behavior:**\r\n\r\n```\r\n2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: gs://bucket/output.json\r\n2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv\r\n```\r\n\r\n**Actual behavior:**\r\n\r\n```\r\n2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv\r\n2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv\r\n```\r\n\r\n### Versions\r\n\r\n```\r\nScrapy       : 2.1.0\r\nlxml         : 4.5.1.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 3.7.4 (default, Sep  4 2019, 15:20:53) - [Clang 10.0.0 (clang-1000.10.44.4)]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : Darwin-19.4.0-x86_64-i386-64bit\r\n```\r\n\r\n### Additional context\r\n\r\nWe use custom GoogleCloudFeedStorage and it takes some time to store the data. While it's doing uploading, next iteration of a for loop inside `FeedExporter.close_spider` comes and creates new `log_args` object, although the last one is used in closure.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4617", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4617/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4617/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4617/events", "html_url": "https://github.com/scrapy/scrapy/issues/4617", "id": 634515109, "node_id": "MDU6SXNzdWU2MzQ1MTUxMDk=", "number": 4617, "title": "Settings not properly configured when calling get_project_settings()", "user": {"login": "Mystickal", "id": 19324916, "node_id": "MDQ6VXNlcjE5MzI0OTE2", "avatar_url": "https://avatars3.githubusercontent.com/u/19324916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mystickal", "html_url": "https://github.com/Mystickal", "followers_url": "https://api.github.com/users/Mystickal/followers", "following_url": "https://api.github.com/users/Mystickal/following{/other_user}", "gists_url": "https://api.github.com/users/Mystickal/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mystickal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mystickal/subscriptions", "organizations_url": "https://api.github.com/users/Mystickal/orgs", "repos_url": "https://api.github.com/users/Mystickal/repos", "events_url": "https://api.github.com/users/Mystickal/events{/privacy}", "received_events_url": "https://api.github.com/users/Mystickal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-08T11:08:45Z", "updated_at": "2020-06-09T07:27:48Z", "closed_at": "2020-06-09T07:27:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "So I'm trying to run scrapy from within the code, and when I try to run the code through pressing the run button in Visual Studio it gives me the following error:\r\n\r\n```\r\n**Exception has occurred: ImproperlyConfigured\r\nRequested settings, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.**\r\n```\r\n\r\nThough if I run it through the terminal by typing python filename.py it runs perfectly. I've tried searching online but can't seem to see anyone having the same problem. I do not have django (I believe?) on my codes so no idea what the problem is.\r\n\r\n**Spider File**\r\n```\r\nclass DocSpider(scrapy.Spider):\r\n    \"\"\"\r\n    This is the broad scraper, the name is doc_spider and can be invoked by making an object\r\n    of the CrawlerProcess() then calling the class of the Spider. It scrapes websites csv file\r\n    for the content and returns the results as a .json file.\r\n    \"\"\"\r\n\r\n    #Name of Spider\r\n    name = 'doc_spider'\r\n\r\n    #File of the URL list here\r\n    urlsList = pandas.read_csv('linksToScrape.csv')\r\n    urls = []\r\n    #Take the urls and insert them into a url list\r\n    for url in urlsList['urls']:\r\n        urls.append(url)\r\n\r\n    #Scrape through all the websites in the urls list\r\n    start_urls = urls\r\n\r\n    #This method will parse the results and will be called automatically\r\n    def parse(self, response):\r\n        data = {}\r\n        #Iterates through all <p> tags\r\n        for content in response.xpath('/html//body//div[@class]//div[@class]//p'):\r\n            if content:\r\n                #Append the current url\r\n                data['links'] = response.request.url\r\n                #Append the texts within the <p> tags\r\n                data['texts'] = \" \".join(content.xpath('//p/text()').extract())\r\n\r\n        yield data\r\n\r\n    def run_crawler(self):\r\n        if(os.path.exists(r\"scrape_results.json\")):\r\n            os.remove(r\"scrape_results.json\")\r\n\r\n        if __name__ ==  \"__main__\":\r\n\r\n            settings = get_project_settings()\r\n            settings.set('FEED_FORMAT', 'json')\r\n            settings.set('FEED_URI', 'scrape_results.json')\r\n            c = CrawlerProcess(settings)\r\n            c.crawl(DocSpider)\r\n            c.start(stop_after_crawl=True)\r\n\r\nds = DocSpider()\r\nds.run_crawler()\r\n```\r\n\r\n**Scrapy.cfg**\r\n```\r\n[settings]\r\ndefault = doc_spider.settings\r\n\r\n[deploy]\r\n#url = http://localhost:6800/\r\nproject = doc_spider\r\n```\r\n**Scrapy Version**: 2.0.1", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4614", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4614/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4614/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4614/events", "html_url": "https://github.com/scrapy/scrapy/issues/4614", "id": 633569405, "node_id": "MDU6SXNzdWU2MzM1Njk0MDU=", "number": 4614, "title": "After upgrading my mac to catalina, when running my scrapy project, python quit unexpectedly, which has not happened before", "user": {"login": "linupychiang", "id": 46371795, "node_id": "MDQ6VXNlcjQ2MzcxNzk1", "avatar_url": "https://avatars2.githubusercontent.com/u/46371795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linupychiang", "html_url": "https://github.com/linupychiang", "followers_url": "https://api.github.com/users/linupychiang/followers", "following_url": "https://api.github.com/users/linupychiang/following{/other_user}", "gists_url": "https://api.github.com/users/linupychiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/linupychiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linupychiang/subscriptions", "organizations_url": "https://api.github.com/users/linupychiang/orgs", "repos_url": "https://api.github.com/users/linupychiang/repos", "events_url": "https://api.github.com/users/linupychiang/events{/privacy}", "received_events_url": "https://api.github.com/users/linupychiang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-07T16:48:40Z", "updated_at": "2020-07-18T05:44:48Z", "closed_at": "2020-06-09T07:26:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nAfter upgrading my mac to catalina, when running my scrapy project, python quit unexpectedly, which has not happened before.\r\n### Versions\r\n- scrapy: 1.5.0\r\n- os: mac os catalina 10.15.5\r\n- python3.6.7\r\n### Additional context\r\n```\r\nfrom scrapy.utils.project import get_project_settings\r\nfrom scrapyuniversal.utils import get_config\r\nfrom scrapy.crawler import CrawlerProcess\r\nfrom multiprocessing import Process\r\n\r\n\r\ndef run(name):\r\n    custom_settings = get_config(name)\r\n    spider = custom_settings.get('spider', 'universal')\r\n    project_settings = get_project_settings()\r\n    settings = dict(project_settings.copy())\r\n    settings.update(custom_settings.get('settings'))\r\n    process = CrawlerProcess(settings)\r\n    process.crawl(spider, **{'name': name})\r\n    process.start()\r\n\r\n\r\nif __name__ == '__main__':\r\n    names = [\r\n        'mysite'\r\n    ]\r\n    for name in names:\r\n        p = Process(target=run, args=(name,))\r\n        p.start()\r\n        p.join()\r\n```\r\n\r\nWhen I run the code above, python quits unexpectedly.\r\n![image](https://user-images.githubusercontent.com/46371795/83974686-b857d400-a921-11ea-9491-b3eff7197d3d.png)\r\n\r\nThe console output is as follows:\r\n```\r\n2020-06-08 00:32:01 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapyuniversal)\r\n2020-06-08 00:32:01 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.9.0, Python 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14) - [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-19.5.0-x86_64-i386-64bit\r\n2020-06-08 00:32:01 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'scrapyuniversal', 'NEWSPIDER_MODULE': 'scrapyuniversal.spiders', 'SPIDER_MODULES': ['scrapyuniversal.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36'}\r\n2020-06-08 00:32:01 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n```\r\n\r\n---\r\n\r\nWhen I modified it, the code ran as expected\r\n\r\n```\r\nfrom scrapy.utils.project import get_project_settings\r\nfrom scrapyuniversal.utils import get_config\r\nfrom scrapy.crawler import CrawlerProcess\r\n\r\n\r\ndef run(name):\r\n    custom_settings = get_config(name)\r\n    spider = custom_settings.get('spider', 'universal')\r\n    project_settings = get_project_settings()\r\n    settings = dict(project_settings.copy())\r\n    settings.update(custom_settings.get('settings'))\r\n    process = CrawlerProcess(settings)\r\n    process.crawl(spider, **{'name': name})\r\n    process.start()\r\n\r\n\r\nif __name__ == '__main__':\r\n    name = 'mysite'\r\n    run(name)\r\n```\r\n\r\n---\r\n\r\nBut the important thing is that when the above two pieces of code can run normally on Linux, they can also run normally on mac os mojave 10.14.6.\r\n\r\nWhere is the problem, please help me, thank you!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4611", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4611/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4611/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4611/events", "html_url": "https://github.com/scrapy/scrapy/issues/4611", "id": 629677443, "node_id": "MDU6SXNzdWU2Mjk2Nzc0NDM=", "number": 4611, "title": "When i use proxy,all requests using the first HTTP connect tunnel, not each request established a connect ", "user": {"login": "kuaidaili-dev", "id": 32975725, "node_id": "MDQ6VXNlcjMyOTc1NzI1", "avatar_url": "https://avatars3.githubusercontent.com/u/32975725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kuaidaili-dev", "html_url": "https://github.com/kuaidaili-dev", "followers_url": "https://api.github.com/users/kuaidaili-dev/followers", "following_url": "https://api.github.com/users/kuaidaili-dev/following{/other_user}", "gists_url": "https://api.github.com/users/kuaidaili-dev/gists{/gist_id}", "starred_url": "https://api.github.com/users/kuaidaili-dev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kuaidaili-dev/subscriptions", "organizations_url": "https://api.github.com/users/kuaidaili-dev/orgs", "repos_url": "https://api.github.com/users/kuaidaili-dev/repos", "events_url": "https://api.github.com/users/kuaidaili-dev/events{/privacy}", "received_events_url": "https://api.github.com/users/kuaidaili-dev/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-03T04:13:24Z", "updated_at": "2020-07-14T09:20:39Z", "closed_at": "2020-07-14T09:20:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\n[Description of the issue]\r\nI use wireshark ,this is the data package \r\n![image](https://user-images.githubusercontent.com/32975725/83594785-b5ac5600-a592-11ea-844a-712d0baddb20.png)\r\nI submit 10 requests but just establish one connect connection,the left requests go through the fist connect tunnel,i do not know why.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4608", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4608/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4608/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4608/events", "html_url": "https://github.com/scrapy/scrapy/issues/4608", "id": 628670968, "node_id": "MDU6SXNzdWU2Mjg2NzA5Njg=", "number": 4608, "title": "iter() a generator in start_requests", "user": {"login": "jacty", "id": 4190959, "node_id": "MDQ6VXNlcjQxOTA5NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/4190959?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacty", "html_url": "https://github.com/jacty", "followers_url": "https://api.github.com/users/jacty/followers", "following_url": "https://api.github.com/users/jacty/following{/other_user}", "gists_url": "https://api.github.com/users/jacty/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacty/subscriptions", "organizations_url": "https://api.github.com/users/jacty/orgs", "repos_url": "https://api.github.com/users/jacty/repos", "events_url": "https://api.github.com/users/jacty/events{/privacy}", "received_events_url": "https://api.github.com/users/jacty/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-01T19:28:07Z", "updated_at": "2020-06-02T08:06:58Z", "closed_at": "2020-06-02T08:06:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/scrapy/scrapy/blob/c86a1035dd9b8b10acaf8f9e8bdb1b5494a287e2/scrapy/crawler.py#L88\r\n\r\nself.spider.start_requests() will return a generator for sure, however, I am not sure why we need to use iter() to wrap it again. Should we remove iter() in this case?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4602", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4602/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4602/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4602/events", "html_url": "https://github.com/scrapy/scrapy/issues/4602", "id": 626830583, "node_id": "MDU6SXNzdWU2MjY4MzA1ODM=", "number": 4602, "title": "Performance bottleneck with mono threaded Downloader Middleware", "user": {"login": "yonromai", "id": 1596570, "node_id": "MDQ6VXNlcjE1OTY1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1596570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yonromai", "html_url": "https://github.com/yonromai", "followers_url": "https://api.github.com/users/yonromai/followers", "following_url": "https://api.github.com/users/yonromai/following{/other_user}", "gists_url": "https://api.github.com/users/yonromai/gists{/gist_id}", "starred_url": "https://api.github.com/users/yonromai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yonromai/subscriptions", "organizations_url": "https://api.github.com/users/yonromai/orgs", "repos_url": "https://api.github.com/users/yonromai/repos", "events_url": "https://api.github.com/users/yonromai/events{/privacy}", "received_events_url": "https://api.github.com/users/yonromai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-28T21:33:59Z", "updated_at": "2020-05-29T15:03:08Z", "closed_at": "2020-05-29T14:45:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your pull request, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#writing-patches and https://doc.scrapy.org/en/latest/contributing.html#submitting-patches\r\n\r\n-->\r\n\r\n## Summary\r\n\r\nThere seems to be a performance bottleneck in the case where a Downloader Middleware is I/O heavy: I have a custom cache storage (very similar to `FilesystemCacheStorage`) which does some I/O. In the cases where the cache hit ratio is high, this cache becomes the overwhelming bottleneck. \r\n\r\n## Additional context\r\n\r\nI tried playing with both `REACTOR_THREADPOOL_MAXSIZE` and `CONCURRENT_REQUESTS` but it seems that no multi-threading is taking place to invoke the cache middleware since it's only ran from the main thread. I am not very familiar with Reactor thread pooling but I cannot find a nob to turn on multithreading at that level.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4601", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4601/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4601/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4601/events", "html_url": "https://github.com/scrapy/scrapy/issues/4601", "id": 626651477, "node_id": "MDU6SXNzdWU2MjY2NTE0Nzc=", "number": 4601, "title": "Using \":\" (colon) in the url raise a error", "user": {"login": "crawazaky", "id": 58865090, "node_id": "MDQ6VXNlcjU4ODY1MDkw", "avatar_url": "https://avatars3.githubusercontent.com/u/58865090?v=4", "gravatar_id": "", "url": "https://api.github.com/users/crawazaky", "html_url": "https://github.com/crawazaky", "followers_url": "https://api.github.com/users/crawazaky/followers", "following_url": "https://api.github.com/users/crawazaky/following{/other_user}", "gists_url": "https://api.github.com/users/crawazaky/gists{/gist_id}", "starred_url": "https://api.github.com/users/crawazaky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/crawazaky/subscriptions", "organizations_url": "https://api.github.com/users/crawazaky/orgs", "repos_url": "https://api.github.com/users/crawazaky/repos", "events_url": "https://api.github.com/users/crawazaky/events{/privacy}", "received_events_url": "https://api.github.com/users/crawazaky/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-28T16:36:55Z", "updated_at": "2020-05-28T18:22:20Z", "closed_at": "2020-05-28T18:22:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nTrying to request google.fr with a search request of this form:\r\n\r\nhttps://www.google.fr/search?q=site:website+keyword\r\n\r\nRaise a error:\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/vagrant/.local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\r\n    current.result = callback(current.result, *args, **kw)\r\n  File \"/home/vagrant/scrapy/tutorial1/tutorial1/spiders/quotes_spider.py\", line 124, in parse_google_to_fb\r\n    request = scrapy.Request(doc['lien_facebook'], meta={'dont_proxy': True}, callback=self.parse_facebook)\r\n  File \"/home/vagrant/.local/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 26, in __init__\r\n    self._set_url(url)\r\n  File \"/home/vagrant/.local/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 64, in _set_url\r\n    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\r\n`\r\n\r\nIt seems to come from the \":\" (colon) following the \"site\". Because taking it off fix the issue.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Request a google search in the following form: https://www.google.fr/search?q=site:website+keyword\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\nI expect the request to work just the same behavior than without the \"site:\" at the beginning of my google's search\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\nA error appear:\r\n\r\n`Traceback (most recent call last):\r\n  File \"/home/vagrant/.local/lib/python2.7/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\r\n    current.result = callback(current.result, *args, **kw)\r\n  File \"/home/vagrant/scrapy/tutorial1/tutorial1/spiders/quotes_spider.py\", line 124, in parse_google_to_fb\r\n    request = scrapy.Request(doc['lien_facebook'], meta={'dont_proxy': True}, callback=self.parse_facebook)\r\n  File \"/home/vagrant/.local/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 26, in __init__\r\n    self._set_url(url)\r\n  File \"/home/vagrant/.local/lib/python2.7/site-packages/scrapy/http/request/__init__.py\", line 64, in _set_url\r\n    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\r\n`\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n100 % with this kind of url\r\n### Versions\r\n\r\nScrapy       : 1.8.0\r\nlxml         : 4.5.1.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.6.0\r\nw3lib        : 1.22.0\r\nTwisted      : 20.3.0\r\nPython       : 2.7.17 (default, Apr 15 2020, 17:20:14) - [GCC 7.5.0]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : Linux-4.15.0-96-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\n### Additional context\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4598", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4598/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4598/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4598/events", "html_url": "https://github.com/scrapy/scrapy/issues/4598", "id": 625729624, "node_id": "MDU6SXNzdWU2MjU3Mjk2MjQ=", "number": 4598, "title": "eb_kwargs for errback", "user": {"login": "Shleif91", "id": 18234924, "node_id": "MDQ6VXNlcjE4MjM0OTI0", "avatar_url": "https://avatars0.githubusercontent.com/u/18234924?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Shleif91", "html_url": "https://github.com/Shleif91", "followers_url": "https://api.github.com/users/Shleif91/followers", "following_url": "https://api.github.com/users/Shleif91/following{/other_user}", "gists_url": "https://api.github.com/users/Shleif91/gists{/gist_id}", "starred_url": "https://api.github.com/users/Shleif91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Shleif91/subscriptions", "organizations_url": "https://api.github.com/users/Shleif91/orgs", "repos_url": "https://api.github.com/users/Shleif91/repos", "events_url": "https://api.github.com/users/Shleif91/events{/privacy}", "received_events_url": "https://api.github.com/users/Shleif91/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-27T14:10:02Z", "updated_at": "2020-06-18T10:01:39Z", "closed_at": "2020-06-18T10:01:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Since version 2.0 `cb_kwargs` has appeared which I use to transfer data to callback. \r\n\r\nNow need to handle errors in errback. How to forward data there (logically there should be `eb_kwargs`, but there is none)?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4597", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4597/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4597/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4597/events", "html_url": "https://github.com/scrapy/scrapy/issues/4597", "id": 625431390, "node_id": "MDU6SXNzdWU2MjU0MzEzOTA=", "number": 4597, "title": "KeyError in is_generator_with_return_value", "user": {"login": "wRAR", "id": 241039, "node_id": "MDQ6VXNlcjI0MTAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/241039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wRAR", "html_url": "https://github.com/wRAR", "followers_url": "https://api.github.com/users/wRAR/followers", "following_url": "https://api.github.com/users/wRAR/following{/other_user}", "gists_url": "https://api.github.com/users/wRAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/wRAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wRAR/subscriptions", "organizations_url": "https://api.github.com/users/wRAR/orgs", "repos_url": "https://api.github.com/users/wRAR/repos", "events_url": "https://api.github.com/users/wRAR/events{/privacy}", "received_events_url": "https://api.github.com/users/wRAR/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-27T06:56:02Z", "updated_at": "2020-05-27T16:42:05Z", "closed_at": "2020-05-27T16:42:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "\r\n```Python traceback\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1418, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"/app/python/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py\", line 42, in process_request\r\n    defer.returnValue((yield download_func(request=request, spider=spider)))\r\n  File \"/usr/local/lib/python3.8/site-packages/twisted/internet/defer.py\", line 1362, in returnValue\r\n    raise _DefGen_Return(val)\r\ntwisted.internet.defer._DefGen_Return: <200 https://www.example.com>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/app/python/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 55, in mustbe_deferred\r\n    result = f(*args, **kw)\r\n  File \"/app/python/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 60, in process_spider_input\r\n    return scrape_func(response, request, spider)\r\n  File \"/app/python/lib/python3.8/site-packages/scrapy/core/scraper.py\", line 148, in call_spider\r\n    warn_on_generator_with_return_value(spider, callback)\r\n  File \"/app/python/lib/python3.8/site-packages/scrapy/utils/misc.py\", line 202, in warn_on_generator_with_return_value\r\n    if is_generator_with_return_value(callable):\r\n  File \"/app/python/lib/python3.8/site-packages/scrapy/utils/misc.py\", line 180, in is_generator_with_return_value\r\n    return _generator_callbacks_cache[callable]\r\n  File \"/app/python/lib/python3.8/site-packages/scrapy/utils/datatypes.py\", line 281, in __getitem__\r\n    return super(LocalWeakReferencedCache, self).__getitem__(key)\r\n  File \"/usr/local/lib/python3.8/weakref.py\", line 383, in __getitem__\r\n    return self.data[ref(key)]\r\nKeyError: <weakref at 0x7f06ff011720; to 'method' at 0x7f07042b5e00 (parse_foo)>\r\n```\r\n\r\nThis is Scrapy 2.0.1. The problem happens only sometimes, but in different spiders in the same project.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4595", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4595/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4595/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4595/events", "html_url": "https://github.com/scrapy/scrapy/issues/4595", "id": 623872240, "node_id": "MDU6SXNzdWU2MjM4NzIyNDA=", "number": 4595, "title": "Installation error", "user": {"login": "rexraja", "id": 65858720, "node_id": "MDQ6VXNlcjY1ODU4NzIw", "avatar_url": "https://avatars1.githubusercontent.com/u/65858720?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rexraja", "html_url": "https://github.com/rexraja", "followers_url": "https://api.github.com/users/rexraja/followers", "following_url": "https://api.github.com/users/rexraja/following{/other_user}", "gists_url": "https://api.github.com/users/rexraja/gists{/gist_id}", "starred_url": "https://api.github.com/users/rexraja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rexraja/subscriptions", "organizations_url": "https://api.github.com/users/rexraja/orgs", "repos_url": "https://api.github.com/users/rexraja/repos", "events_url": "https://api.github.com/users/rexraja/events{/privacy}", "received_events_url": "https://api.github.com/users/rexraja/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-24T14:10:36Z", "updated_at": "2020-05-25T06:57:00Z", "closed_at": "2020-05-25T06:57:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nWhile installing scrapy in python virtual environment I am getting the below error. please help on this\r\n\r\nInstalling collected packages: pyasn1, pyasn1-modules, service-identity, twisted\r\n    Running setup.py install for twisted ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'c:\\users\\neha\\appdata\\local\\programs\\python\\python38-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\NEHA\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7hvhal6s\\\\twisted\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\NEHA\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7hvhal6s\\\\twisted\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\NEHA\\AppData\\Local\\Temp\\pip-record-wq7r0lth\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\neha\\appdata\\local\\programs\\python\\python38-32\\Include\\twisted'\r\n         cwd: C:\\Users\\NEHA\\AppData\\Local\\Temp\\pip-install-7hvhal6s\\twisted\\\r\n    Complete output (947 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build\\lib.win32-3.8\r\n    creating build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\copyright.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\plugin.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\_version.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\__init__.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\__main__.py -> build\\lib.win32-3.8\\twisted\r\n    creating build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\app.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\internet.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\reactors.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\service.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\strports.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\r\n    creating build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\avatar.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\checkers.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\endpoints.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\error.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\interfaces.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\ls.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\manhole.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\manhole_ssh.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\manhole_tap.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\mixin.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\recvline.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\stdio.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\tap.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\telnet.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\ttymodes.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\unix.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    creating build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\checkers.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\credentials.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\error.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\portal.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\strcred.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\_digest.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\__init__.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    creating build\\lib.win32-3.8\\twisted\\enterprise\r\n    copying src\\twisted\\enterprise\\adbapi.py -> build\\lib.win32-3.8\\twisted\\enterprise\r\n    copying src\\twisted\\enterprise\\__init__.py -> build\\lib.win32-3.8\\twisted\\enterprise\r\n    creating build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\abstract.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\address.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\asyncioreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\base.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\cfreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\default.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\defer.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\endpoints.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\epollreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\error.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\fdesc.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\gireactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\glib2reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\gtk2reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\gtk3reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\inotify.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\interfaces.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\kqreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\main.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\pollreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\posixbase.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\process.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\protocol.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\pyuisupport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\selectreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\serialport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\ssl.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\stdio.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\task.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\tcp.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\testing.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\threads.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\tksupport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\udp.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\unix.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\utils.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\win32eventreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\wxreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\wxsupport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_baseprocess.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_dumbwin32proc.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_glibbase.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_idna.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_newtls.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_pollingfile.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_posixserialport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_posixstdio.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_producer_helpers.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_resolver.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_signals.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_sslverify.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_threadedselect.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_win32serialport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_win32stdio.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\__init__.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    creating build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_buffer.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_capture.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_file.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_filter.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_flatten.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_format.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_global.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_io.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_json.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_legacy.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_levels.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_logger.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_observer.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_stdlib.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_util.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\__init__.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    creating build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\imap4.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\interfaces.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\pop3.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\pop3client.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\protocols.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\relay.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\smtp.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\_cred.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\_except.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\__init__.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    creating build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\authority.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\cache.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\client.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\common.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\dns.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\error.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\hosts.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\resolve.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\root.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\secondary.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\server.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\srvconnect.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\tap.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\_rfc1982.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\__init__.py -> build\\lib.win32-3.8\\twisted\\names\r\n    creating build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\ethernet.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\ip.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\raw.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\rawudp.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\testing.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\tuntap.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\__init__.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    creating build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\aot.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\crefutil.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\dirdbm.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\sob.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\styles.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\__init__.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    creating build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_anonymous.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_file.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_memory.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_sshkeys.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_unix.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_conch.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_core.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_ftp.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_inet.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_names.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_portforward.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_reactors.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_runner.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_socks.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_trial.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_web.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_words.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\__init__.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    creating build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\base.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\ipositioning.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\nmea.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\_sentence.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\__init__.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\amp.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\basic.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\dict.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\finger.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\ftp.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\htb.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\ident.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\loopback.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\memcache.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\pcp.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\policies.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\portforward.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\postfix.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\sip.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\socks.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\stateful.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\tls.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\wire.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    creating build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\compat.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\components.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\constants.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\context.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\deprecate.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\failure.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\fakepwd.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\filepath.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\formmethod.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\htmlizer.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\lockfile.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\log.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\logfile.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\modules.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\monkey.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\procutils.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\randbytes.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\rebuild.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\reflect.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\release.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\roots.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\runtime.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\sendmsg.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\shortcut.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\syslog.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\systemd.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\text.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\threadable.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\threadpool.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\url.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\urlpath.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\usage.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\util.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\versions.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\win32.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\zippath.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\zipstream.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_appdirs.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_inotify.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_oldstyle.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_release.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_setup.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_shellcomp.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_textattributes.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_tzhelper.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_url.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\__init__.py -> build\\lib.win32-3.8\\twisted\\python\r\n    creating build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\inetd.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\inetdconf.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\inetdtap.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\procmon.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\procmontap.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\__init__.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    creating build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\htmlizer.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\trial.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\twistd.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\_twistd_unix.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\_twistw.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\__init__.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    creating build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\banana.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\flavors.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\interfaces.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\jelly.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\pb.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\publish.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\util.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\__init__.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    creating build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\ftp.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\portforward.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\socks.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\__init__.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    creating build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\crash_test_dummy.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\iosim.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\mock_win32process.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\myrebuilder1.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\myrebuilder2.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\plugin_basic.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\plugin_extra1.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\plugin_extra2.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_cmdline.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_echoer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_fds.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_getargv.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_getenv.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_linger.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_reader.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_signal.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_stdinreader.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_tester.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_tty.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_twisted.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\proto_helpers.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\reflect_helper_IE.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\reflect_helper_VE.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\reflect_helper_ZDE.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\ssl_helpers.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_consumer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_halfclose.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_hostpeer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_lastwrite.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_loseconn.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_producer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_write.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_writeseq.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\testutils.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_abstract.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_adbapi.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_amp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_application.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_compat.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_context.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_cooperator.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_defer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_defgen.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_dict.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_dirdbm.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_error.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_factories.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_failure.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_fdesc.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_finger.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_formmethod.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ftp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ftp_options.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_htb.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ident.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_internet.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_iosim.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_iutils.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_lockfile.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_log.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_logfile.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_loopback.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_main.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_memcache.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_modules.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_monkey.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_news.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_nooldstyle.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_paths.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_pcp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_persisted.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_plugin.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_policies.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_postfix.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_process.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_protocols.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_randbytes.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_rebuild.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_reflect.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_roots.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_shortcut.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_sip.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_sob.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_socks.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ssl.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_sslverify.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_stateful.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_stdio.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_strerror.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_strports.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_task.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_tcp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_tcp_internals.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_text.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_threadable.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_threadpool.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_threads.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_tpfile.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_twistd.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_twisted.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_udp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_unix.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_usage.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\test\r\n    creating build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\itrial.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\reporter.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\runner.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\unittest.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\util.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\_asyncrunner.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\_asynctest.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\_synctest.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\__main__.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    creating build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\client.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\demo.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\distrib.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\domhelpers.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\error.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\guard.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\html.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\http.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\http_headers.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\iweb.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\microdom.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\proxy.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\resource.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\rewrite.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\script.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\server.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\static.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\sux.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\tap.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\template.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\twcgi.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\util.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\vhost.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\wsgi.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\xmlrpc.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_element.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_flatten.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_http2.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_newclient.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_responses.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_stan.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\__init__.py -> build\\lib.win32-3.8\\twisted\\web\r\n    creating build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\ewords.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\iwords.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\service.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\tap.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\xmpproutertap.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\r\n    creating build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_convenience.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_ithreads.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_memory.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_pool.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_team.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_threadworker.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\__init__.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    creating build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\_exit.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\_pidfile.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\_runner.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    creating build\\lib.win32-3.8\\twisted\\application\\test\r\n    copying src\\twisted\\application\\test\\test_internet.py -> build\\lib.win32-3.8\\twisted\\application\\test\r\n    copying src\\twisted\\application\\test\\test_service.py -> build\\lib.win32-3.8\\twisted\\application\\test\r\n    copying src\\twisted\\application\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\test\r\n    creating build\\lib.win32-3.8\\twisted\\application\\twist\r\n    copying src\\twisted\\application\\twist\\_options.py -> build\\lib.win32-3.8\\twisted\\application\\twist\r\n    copying src\\twisted\\application\\twist\\_twist.py -> build\\lib.win32-3.8\\twisted\\application\\twist\r\n    copying src\\twisted\\application\\twist\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\twist\r\n    creating build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\test_exit.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\test_pidfile.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\test_runner.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    creating build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    copying src\\twisted\\application\\twist\\test\\test_options.py -> build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    copying src\\twisted\\application\\twist\\test\\test_twist.py -> build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    copying src\\twisted\\application\\twist\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\agent.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\connect.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\default.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\direct.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\knownhosts.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\options.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\helper.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\insults.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\text.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\window.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    copying src\\twisted\\conch\\openssh_compat\\factory.py -> build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    copying src\\twisted\\conch\\openssh_compat\\primes.py -> build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    copying src\\twisted\\conch\\openssh_compat\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\cftp.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\ckeygen.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\conch.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\tkconch.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\address.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\agent.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\channel.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\common.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\connection.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\factory.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\filetransfer.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\forwarding.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\keys.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\service.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\session.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\sexpy.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\transport.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\userauth.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\_kex.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\keydata.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\loopback.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_address.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_agent.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_cftp.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_channel.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_checkers.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_ckeygen.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_conch.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_connection.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_default.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_endpoints.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_filetransfer.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_forwarding.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_helper.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_insults.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_keys.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_knownhosts.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_manhole.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_manhole_tap.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_mixin.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_openssh_compat.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_recvline.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_scripts.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_session.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_ssh.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_telnet.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_text.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_transport.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_unix.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_userauth.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_window.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    copying src\\twisted\\conch\\ui\\ansi.py -> build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    copying src\\twisted\\conch\\ui\\tkvt100.py -> build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    copying src\\twisted\\conch\\ui\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    creating build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_cramauth.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_cred.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_digestauth.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_simpleauth.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_strcred.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    creating build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\abstract.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\const.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\interfaces.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\reactor.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\setup.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\tcp.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\udp.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\__init__.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    creating build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\connectionmixins.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\fakeendpoint.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\modulehelpers.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_cli.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_connectionlost.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_gireactornocompat.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_helper.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\reactormixins.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_abstract.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_address.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_asyncioreactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_base.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_baseprocess.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_core.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_coroutines.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_default.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_endpoints.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_epollreactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_error.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_fdset.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_filedescriptor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_gireactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_glibbase.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_inlinecb.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_inotify.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_iocp.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_kqueuereactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_main.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_newtls.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_pollingfile.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_posixbase.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_posixprocess.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_process.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_protocol.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_resolver.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_serialport.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_sigchld.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_socket.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_stdio.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_tcp.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_testing.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_threads.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_time.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_tls.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_udp.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_udp_internals.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_unix.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_win32events.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_win32serialport.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\_posixifaces.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\_win32ifaces.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    creating build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_buffer.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_capture.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_file.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_filter.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_flatten.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_format.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_global.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_io.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_json.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_legacy.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_levels.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_logger.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_observer.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_stdlib.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    creating build\\lib.win32-3.8\\twisted\\mail\\scripts\r\n    copying src\\twisted\\mail\\scripts\\mailmail.py -> build\\lib.win32-3.8\\twisted\\mail\\scripts\r\n    creating build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\pop3testserver.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_imap.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_mailmail.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_pop3.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_pop3client.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_smtp.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    creating build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_cache.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_client.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_common.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_dns.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_examples.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_hosts.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_names.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_resolve.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_rfc1982.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_rootresolve.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_server.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_srvconnect.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    creating build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_ethernet.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_ip.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_rawudp.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_tuntap.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    creating build\\lib.win32-3.8\\twisted\\persisted\\test\r\n    copying src\\twisted\\persisted\\test\\test_styles.py -> build\\lib.win32-3.8\\twisted\\persisted\\test\r\n    copying src\\twisted\\persisted\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\persisted\\test\r\n    creating build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\receiver.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\test_base.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\test_nmea.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\test_sentence.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_exceptions.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_info.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_interfaces.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_v1parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_v2parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_wrapper.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    copying src\\twisted\\protocols\\test\\test_basic.py -> build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    copying src\\twisted\\protocols\\test\\test_tls.py -> build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    copying src\\twisted\\protocols\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_v1parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_v2parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_wrapper.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    creating build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\deprecatedattributes.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\modules_helpers.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\pullpipe.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_appdirs.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_components.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_constants.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_deprecate.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_dist3.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_fakepwd.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_htmlizer.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_inotify.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_release.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_runtime.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_sendmsg.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_setup.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_shellcomp.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_syslog.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_systemd.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_textattributes.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_tzhelper.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_url.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_urlpath.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_versions.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_zippath.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_zipstream.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    creating build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\test_inetdconf.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\test_procmon.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\test_procmontap.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    creating build\\lib.win32-3.8\\twisted\\scripts\\test\r\n    copying src\\twisted\\scripts\\test\\test_scripts.py -> build\\lib.win32-3.8\\twisted\\scripts\\test\r\n    copying src\\twisted\\scripts\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\scripts\\test\r\n    creating build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_banana.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_jelly.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_pb.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_pbfailure.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    creating build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\detests.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\erroneous.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockcustomsuite.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockcustomsuite2.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockcustomsuite3.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockdoctest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\moduleself.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\moduletest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\novars.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\ordertests.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\packages.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\sample.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\scripttest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\skipping.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\suppression.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_assertions.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_asyncassertions.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_deferred.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_doctest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_keyboard.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_loader.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_log.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_output.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_plugins.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_pyunitcompat.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_reporter.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_runner.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_script.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_suppression.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_testcase.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_tests.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_warning.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\weird.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    creating build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\distreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\disttrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\managercommands.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\options.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\worker.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\workercommands.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\workerreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\workertrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    creating build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_distreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_disttrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_options.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_worker.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_workerreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_workertrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    creating build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\injectionhelpers.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\requesthelper.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_agent.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_cgi.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_client.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_distrib.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_domhelpers.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_error.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_flatten.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_html.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_http.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_http2.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_httpauth.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_http_headers.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_newclient.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_proxy.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_resource.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_script.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_stan.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_static.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_template.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_vhost.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_web.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_webclient.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_web__responses.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_wsgi.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_xml.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_xmlrpc.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\_util.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    creating build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\basic.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\digest.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\wrapper.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\__init__.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    creating build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\baseaccount.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\basechat.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\basesupport.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\interfaces.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\ircsupport.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\locals.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\pbsupport.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    creating build\\lib.win32-3.8\\twisted\\words\\protocols\r\n    copying src\\twisted\\words\\protocols\\irc.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\r\n    copying src\\twisted\\words\\protocols\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\r\n    creating build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_basechat.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_basesupport.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_domish.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_irc.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_ircsupport.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_irc_service.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberclient.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbercomponent.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbererror.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberjid.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberjstrports.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbersasl.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbersaslmechanisms.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberxmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberxmppstringprep.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_service.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xishutil.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xmpproutertap.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xpath.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    creating build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\domish.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\utility.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\xmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\xpath.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\xpathparser.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    creating build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\client.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\component.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\error.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\ijabber.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\jid.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\jstrports.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\sasl.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\sasl_mechanisms.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\xmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\xmpp_stringprep.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    creating build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_convenience.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_memory.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_team.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_threadworker.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    running egg_info\r\n    writing src\\Twisted.egg-info\\PKG-INFO\r\n    writing dependency_links to src\\Twisted.egg-info\\dependency_links.txt\r\n    writing entry points to src\\Twisted.egg-info\\entry_points.txt\r\n    writing requirements to src\\Twisted.egg-info\\requires.txt\r\n    writing top-level names to src\\Twisted.egg-info\\top_level.txt\r\n    reading manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\r\n    reading manifest template 'MANIFEST.in'\r\n    warning: no previously-included files matching '*.misc' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.bugfix' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.doc' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.feature' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.removal' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching 'NEWS' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching 'README' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching 'newsfragments' found under directory 'src\\twisted'\r\n    warning: no previously-included files found matching 'src\\twisted\\topfiles\\CREDITS'\r\n    warning: no previously-included files found matching 'src\\twisted\\topfiles\\ChangeLog.Old'\r\n    warning: no previously-included files found matching 'pyproject.toml'\r\n    warning: no previously-included files found matching 'codecov.yml'\r\n    warning: no previously-included files found matching 'appveyor.yml'\r\n    warning: no previously-included files found matching '.coveralls.yml'\r\n    warning: no previously-included files found matching '.circleci'\r\n    warning: no previously-included files matching '*' found under directory '.circleci'\r\n    no previously-included directories found matching 'bin'\r\n    no previously-included directories found matching 'admin'\r\n    no previously-included directories found matching '.travis'\r\n    no previously-included directories found matching '.github'\r\n    warning: no previously-included files found matching 'docs\\historic\\2003'\r\n    warning: no previously-included files matching '*' found under directory 'docs\\historic\\2003'\r\n    writing manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\r\n    copying src\\twisted\\python\\twisted-completion.zsh -> build\\lib.win32-3.8\\twisted\\python\r\n    creating build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\python\\_pydoctortemplates\\common.html -> build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\python\\_pydoctortemplates\\index.html -> build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\python\\_pydoctortemplates\\summary.html -> build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\test\\cert.pem.no_trailing_newline -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\key.pem.no_trailing_newline -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\server.pem -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_defer.py.3only -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\internet\\iocpreactor\\notes.txt -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\test\\_awaittests.py.3only -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\_yieldfromtests.py.3only -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    creating build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\chain.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\not-a-certificate -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing1.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2-duplicate.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\mail\\test\\rfc822.message -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\python\\test\\_deprecatetests.py.3only -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\trial\\test\\_assertiontests.py.3only -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\words\\im\\instancemessenger.glade -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\xish\\xpathparser.g -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    running build_ext\r\n    building 'twisted.test.raiser' extension\r\n    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: 'c:\\users\\neha\\appdata\\local\\programs\\python\\python38-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\NEHA\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7hvhal6s\\\\twisted\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\NEHA\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7hvhal6s\\\\twisted\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\NEHA\\AppData\\Local\\Temp\\pip-record-wq7r0lth\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\neha\\appdata\\local\\programs\\python\\python38-32\\Include\\twisted' Check the logs for full command output.\r\n\r\n\r\n<!--\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\n[Description of the issue]\r\n\r\n### Steps to Reproduce\r\n\r\n1. [First Step]\r\n2. [Second Step]\r\n3. [and so on...]\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\n### Versions\r\n\r\nPlease paste here the output of executing `scrapy version --verbose` in the command line.\r\n\r\n### Additional context\r\n\r\nAny additional information, configuration, data or output from commands that might be necessary to reproduce or understand the issue. Please try not to include screenshots of code or the command line, paste the contents as text instead. You can use [GitHub Flavored Markdown](https://help.github.com/en/articles/creating-and-highlighting-code-blocks) to make the text look better.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4594", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4594/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4594/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4594/events", "html_url": "https://github.com/scrapy/scrapy/issues/4594", "id": 623568399, "node_id": "MDU6SXNzdWU2MjM1NjgzOTk=", "number": 4594, "title": "param 'CONCURRENT_REQUESTS ' no function when adding proxy in middlewares.py", "user": {"login": "zeguangzhang", "id": 17930575, "node_id": "MDQ6VXNlcjE3OTMwNTc1", "avatar_url": "https://avatars0.githubusercontent.com/u/17930575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zeguangzhang", "html_url": "https://github.com/zeguangzhang", "followers_url": "https://api.github.com/users/zeguangzhang/followers", "following_url": "https://api.github.com/users/zeguangzhang/following{/other_user}", "gists_url": "https://api.github.com/users/zeguangzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/zeguangzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zeguangzhang/subscriptions", "organizations_url": "https://api.github.com/users/zeguangzhang/orgs", "repos_url": "https://api.github.com/users/zeguangzhang/repos", "events_url": "https://api.github.com/users/zeguangzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/zeguangzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-23T03:08:06Z", "updated_at": "2020-05-25T06:59:56Z", "closed_at": "2020-05-25T06:59:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "# Description\r\nuse middlewares for using proxy to download,   but I find it cann't  be concurrent so that make param 'CONCURRENT_REQUESTS = 8'  no function, I test speed between 'CONCURRENT_REQUESTS = 8' and \r\nCONCURRENT_REQUESTS = 1, their speed is about equal.\r\n\r\n### Steps to Reproduce\r\n\r\n1. update settings.py :\r\nCONCURRENT_REQUESTS = 16\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy.downloadermiddleware.useragent.UserAgentMiddleware': None,\r\n    'spiderask.middlewares.BaikeSpiderUserAgentMiddleware': 533,\r\n    'spiderask.middlewares.SpideraskDownloaderMiddleware': 543,\r\n}\r\n\r\n2. update middlewares.py: (note : only update process_request)\r\n```\r\nclass SpideraskDownloaderMiddleware(object):\r\n    logger = logging.getLogger(__name__)\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        # This method is used by Scrapy to create your spiders.\r\n        s = cls()\r\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\r\n        return s\r\n\r\n    def process_request(self, request, spider):\r\n        r = requests.get('http://139.176.2.90:5558/random')\r\n        proxy = r.text\r\n        self.logger.info('\u83b7\u53d6\u4ee3\u7406\uff1a%s', proxy)\r\n        request.meta['proxy'] = 'http://' + proxy\r\n\r\n    def process_response(self, request, response, spider):\r\n        return response\r\n\r\n    def process_exception(self, request, exception, spider):\r\n        pass\r\n\r\n    def spider_opened(self, spider):\r\n        spider.logger.info('Spider opened: %s' % spider.name)\r\n```\r\n\r\n3. in spider \ud83d\udc4d parse define as follow:\r\n\r\n    def parse(self, response):\r\n        logger.info('response url=%s', response.url)\r\n\r\n        # anchors = response.css('a::attr(href)')\r\n        anchors = response.css('a[href*=answer]::attr(href)')\r\n        for a in anchors:\r\n            yield response.follow(a, callback=self.parse, dont_filter=False)\r\n\r\n        try:\r\n            # in order to debug this problem, I already stop to use pipeline,so now code is as follow:\r\n            item_page = {}\r\n            item_page['url'] = response.url\r\n            item_page['source'] = response.text\r\n            redis_cli.rpush('ask_queue_page_source', json.dumps(item_page))\r\n        except:\r\n            logger.error(\"parse error url =%s, excepiton=\", response.url, exc_info=True)\r\n\r\n**Expected behavior:**  param 'CONCURRENT_REQUESTS = 8'  function\r\n\r\n**Actual behavior:**  param 'CONCURRENT_REQUESTS = 8'  no function\r\n\r\n**Reproduces how often:** always\r\n\r\n### Versions\r\n(env_apider) zzg@C02CC49KMD6R Documents % scrapy version --verbose\r\nScrapy       : 2.1.0\r\nlxml         : 4.3.3.0\r\nlibxml2      : 2.9.9\r\ncssselect    : 1.0.3\r\nparsel       : 1.5.1\r\nw3lib        : 1.20.0\r\nTwisted      : 19.2.0\r\nPython       : 3.7.5 (v3.7.5:5c02a39a0b, Oct 14 2019, 18:49:57) - [Clang 6.0 (clang-600.0.57)]\r\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019)\r\ncryptography : 2.6.1\r\nPlatform     : Darwin-19.3.0-x86_64-i386-64bit\r\n\r\n### Additional context\r\nthanks\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4592", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4592/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4592/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4592/events", "html_url": "https://github.com/scrapy/scrapy/issues/4592", "id": 623264502, "node_id": "MDU6SXNzdWU2MjMyNjQ1MDI=", "number": 4592, "title": "Remove unneeded escape sequences from documentation API signatures", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-22T14:54:13Z", "updated_at": "2020-06-01T21:30:34Z", "closed_at": "2020-06-01T21:30:34Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "I\u2019ve seen a few cases in the documentation like these:\r\n\r\n```rst\r\n.. method:: crawl(\\*args, \\**kwargs)\r\n.. class:: Contract(method, \\*args)\r\n.. method:: from_crawler(crawler, \\*args, \\**kwargs)\r\n```\r\n\r\nThe `\\` preceding the `*` does not seem necessary, and is reaching the rendered documentation.\r\n\r\nNoticed while working on #4161.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4591", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4591/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4591/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4591/events", "html_url": "https://github.com/scrapy/scrapy/issues/4591", "id": 622981431, "node_id": "MDU6SXNzdWU2MjI5ODE0MzE=", "number": 4591, "title": "Download delay and concurrent request per IP not respected, potentially due to custom download middleware", "user": {"login": "gaara4896", "id": 31458338, "node_id": "MDQ6VXNlcjMxNDU4MzM4", "avatar_url": "https://avatars2.githubusercontent.com/u/31458338?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaara4896", "html_url": "https://github.com/gaara4896", "followers_url": "https://api.github.com/users/gaara4896/followers", "following_url": "https://api.github.com/users/gaara4896/following{/other_user}", "gists_url": "https://api.github.com/users/gaara4896/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaara4896/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaara4896/subscriptions", "organizations_url": "https://api.github.com/users/gaara4896/orgs", "repos_url": "https://api.github.com/users/gaara4896/repos", "events_url": "https://api.github.com/users/gaara4896/events{/privacy}", "received_events_url": "https://api.github.com/users/gaara4896/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-22T06:21:24Z", "updated_at": "2020-05-22T10:36:31Z", "closed_at": "2020-05-22T10:36:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I have a `settings.py` as below\r\n\r\n```\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'linked.middlewares.seleniumdownload.MySeleniumDownloadMiddleware': 100,\r\n}\r\n\r\nLOG_LEVEL = 'DEBUG'\r\n\r\n# Introduce an artifical delay to make use of parallelism. to speed up the\r\n# crawl.\r\nDOWNLOAD_DELAY = 10\r\n\r\nCONCURRENT_REQUESTS_PER_IP = 1\r\n```\r\n\r\nAnd below is my custom download middleware\r\n\r\n```\r\nclass MySeleniumDownloadMiddleware:\r\n    \"\"\"Scrapy middleware handling the requests using selenium\"\"\"\r\n\r\n    def __init__(self, driver):\r\n        self.driver = driver\r\n        self.cookies = self.driver.get_cookies()\r\n\r\n    @classmethod\r\n    def from_crawler(cls, crawler):\r\n        \"\"\"Initialize the middleware with the crawler settings\"\"\"\r\n\r\n        driver = init_chromium(crawler.settings.get('SELENIUM_HOSTNAME'))\r\n        login(driver, crawler.settings.get('MY_CREDENTIAL'))\r\n\r\n        middleware = cls(driver=driver)\r\n\r\n        crawler.signals.connect(middleware.spider_closed, signals.spider_closed)\r\n\r\n        return middleware\r\n\r\n\r\n    def process_request(self, request, spider):\r\n        \"\"\"Process a request using the selenium driver if applicable\"\"\"\r\n\r\n        try:\r\n            self.driver.get(request.url)\r\n        except WebDriverException:\r\n            self.driver = init_chromium(spider.settings.get('SELENIUM_HOSTNAME'))\r\n            recover_cookie_to_driver(self.driver, self.cookies)\r\n            self.driver.get(request.url)\r\n\r\n        body = str.encode(self.driver.page_source)\r\n\r\n        # Expose the driver via the \"meta\" attribute\r\n        request.meta.update({'driver': self.driver})\r\n\r\n        return HtmlResponse(\r\n            self.driver.current_url,\r\n            body=body,\r\n            encoding='utf-8',\r\n            request=request\r\n        )\r\n\r\n\r\n    def spider_closed(self):\r\n        \"\"\"Shutdown the driver when spider is closed\"\"\"\r\n        try:\r\n            self.driver.quit()\r\n        except WebDriverException:\r\n            pass\r\n```\r\n\r\nAs you can see, I has the download delay of 10 and concurrent request per ip of 1, what I would expect would be 6 request per second, but below is what I get\r\n\r\n```\r\n2020-05-22 13:14:58 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 12 pages/min), scraped 12 items (at 12 items/min)\r\n```\r\n\r\nMay I know what might be the potential cause of issue?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4586", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4586/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4586/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4586/events", "html_url": "https://github.com/scrapy/scrapy/issues/4586", "id": 620679572, "node_id": "MDU6SXNzdWU2MjA2Nzk1NzI=", "number": 4586, "title": "Unable to import scrapy", "user": {"login": "ShimaMasaeli", "id": 64580264, "node_id": "MDQ6VXNlcjY0NTgwMjY0", "avatar_url": "https://avatars2.githubusercontent.com/u/64580264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ShimaMasaeli", "html_url": "https://github.com/ShimaMasaeli", "followers_url": "https://api.github.com/users/ShimaMasaeli/followers", "following_url": "https://api.github.com/users/ShimaMasaeli/following{/other_user}", "gists_url": "https://api.github.com/users/ShimaMasaeli/gists{/gist_id}", "starred_url": "https://api.github.com/users/ShimaMasaeli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ShimaMasaeli/subscriptions", "organizations_url": "https://api.github.com/users/ShimaMasaeli/orgs", "repos_url": "https://api.github.com/users/ShimaMasaeli/repos", "events_url": "https://api.github.com/users/ShimaMasaeli/events{/privacy}", "received_events_url": "https://api.github.com/users/ShimaMasaeli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-19T05:02:31Z", "updated_at": "2020-05-19T06:04:51Z", "closed_at": "2020-05-19T06:04:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello everyone,\r\nI put this question in other forums but I didn't get any answer.\r\nI am trying to scrape a non-English website. However, I get this error inside the VS Code editor:\r\nUnable to import 'scrapy'pylint(import-error)\r\n\r\nThese are the important parts of codes I am using, I just didn't copy all the variables I am scraping:\r\nclass xxxxxxxxSpider(scrapy.Spider):\r\n\r\nname = 'xxxxxxx'\r\nallowed_domains = \r\n['www.drsaina.com/ConsultationDoctor/%D9%85%D8%B4%D8%A7%D9%88%D8%B1%D9%87- \r\n %D8%A2%D9%86%D9%84%D8%A7%DB%8C%D9%86']\r\n\r\nstart_urls =['https://www.drsaina.com/ConsultationDoctor/%D9%85%D8%B4%D8%A7%D9%88%D8%B1%D9%87 \r\n- \r\n %D8%A2%D9%86%D9%84%D8%A7%DB%8C%D9%86/']\r\n\r\ndef parse(self, response):\r\n    specialties = response.xpath(\"//h3\").getall()\r\n    for specialty in specialties:\r\n        name = specialty.xpath(\".//text()\").getall()\r\n        link = specialty.xpath(\".//@href\").getall()\r\n        yield response.follow(url=link, callback=self.parse_spacialty, meta = \r\n{'specialty_name':name})\r\n\r\ndef parse_spacialty(self, response):\r\n    specialty_name = response.request.meta['specialty_name']\r\n    specifications = response.xpath \r\n(\"/html/body/main/div[5]/div/div/div[2]/div[2]/ul/li[1]/div/div\")\r\n    for specification in specifications:\r\n        doctor_name = specification.xpath(\".//div[1]/a[2]/span/text()\").get()\r\n        doctor_specialty = specification.xpath \r\n(\".//div[1]/a[2]/label/text()\").get()\r\nyield{\r\n            'specialty_name':specialty_name,\r\n            'doctor_name':doctor_name,\r\n            'doctor_specialty':doctor_specialty,\r\n}\r\n\r\n\r\nAlso in prompt, when I run my crawler, this is the whole error I got:\r\n\r\n\r\n2020-05-17 08:48:05 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: yyyyy)\r\n2020-05-17 08:48:05 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9\r\n.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.7 (d\r\nefault, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (Op\r\nenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Windows-8.1-6.3.9600-SP\r\n0\r\n2020-05-17 08:48:05 [scrapy.utils.log] DEBUG: Using reactor: \r\ntwisted.internet.se\r\nlectreactor.SelectReactor\r\n2020-05-17 08:48:05 [scrapy.crawler] INFO: Overridden settings:\r\n{'BOT_NAME': 'yyyyy',\r\n'NEWSPIDER_MODULE': 'yyyyy.spiders',\r\n'ROBOTSTXT_OBEY': True,\r\n'SPIDER_MODULES': ['yyyyy.spiders']}\r\n2020-05-17 08:48:06 [scrapy.extensions.telnet] INFO: Telnet Password: \r\n6aefdde1ca\r\ne354cd\r\n2020-05-17 08:48:06 [scrapy.middleware] INFO: Enabled extensions:\r\n ['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.logstats.LogStats']\r\n 2020-05-17 08:48:07 [scrapy.middleware] INFO: Enabled downloader \r\nmiddleware:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n 2020-05-17 08:48:07 [scrapy.middleware] INFO: Enabled spider middleware:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2020-05-17 08:48:07 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2020-05-17 08:48:07 [scrapy.core.engine] INFO: Spider opened\r\n2020-05-17 08:48:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at \r\n0 page\r\nes/min), scraped 0 items (at 0 items/min)\r\n2020-05-17 08:48:07 [scrapy.extensions.telnet] INFO: Telnet console \r\nlistening on\r\n127.0.0.1:6023\r\n2020-05-17 08:48:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET \r\nhttps://www.d\r\nrsaina.com/robots.txt> (referer: None)\r\n2020-05-17 08:48:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET \r\nhttps://www.d\r\nrsaina.com/ConsultationDoctor/%D9%85%D8%B4%D8%A7%D9%88%D8%B1%D9%87- \r\n%D8%A2%D9%86%\r\nD9%84%D8%A7%DB%8C%D9%86/> (referer: None)\r\n2020-05-17 08:48:09 [scrapy.core.scraper] ERROR: Spider error processing \r\n<GET https:// \r\nwww.drsaina.com/ConsultationDoctor/%D9%85%D8%B4%D8%A7%D9%88%D8%B1%D9%87-%D\r\n8%A2%D9%86%D9%84%D8%A7%DB%8C%D9%86/> (referer: None)\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\n packages\\scrapy\\uti\r\nls\\defer.py\", line 117, in iter_errback\r\n    yield next(it)\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\uti\r\nls\\python.py\", line 345, in __next__\r\n    return next(self.data)\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\uti\r\nls\\python.py\", line 345, in __next__\r\n    return next(self.data)\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\cor\r\ne\\spidermw.py\", line 64, in _evaluate_iterable\r\n    for r in iterable:\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\spi\r\ndermiddlewares\\offsite.py\", line 29, in process_spider_output\r\n    for x in result:\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\cor\r\ne\\spidermw.py\", line 64, in _evaluate_iterable\r\n    for r in iterable:\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\spi\r\ndermiddlewares\\referer.py\", line 338, in <genexpr>\r\n    return (_set_referer(r) for r in result or ())\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\cor\r\ne\\spidermw.py\", line 64, in _evaluate_iterable\r\n    for r in iterable:\r\n    File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\n packages\\scrapy\\spi\r\ndermiddlewares\\urllength.py\", line 37, in <genexpr>\r\n     return (r for r in result or () if _filter(r))\r\n   File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\npackages\\scrapy\\cor\r\ne\\spidermw.py\", line 64, in _evaluate_iterable\r\n     for r in iterable:\r\n   File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\n packages\\scrapy\\spi\r\n dermiddlewares\\depth.py\", line 58, in <genexpr>\r\n     return (r for r in result or () if _filter(r))\r\n  File \"c:\\users\\pc\\anaconda3\\envs\\vitual_workspace\\lib\\site- \r\n packages\\scrapy\\cor\r\n  e\\spidermw.py\", line 64, in _evaluate_iterable\r\n     for r in iterable:\r\n    File \"C:\\Users\\pc\\projects\\yyyyy\\yyyyy\\spiders\\xxxxxxx.py\", line 19, \r\nin parse\r\n    name = specialty.xpath(\".//text()\").getall()\r\nAttributeError: 'str' object has no attribute 'xpath'\r\n2020-05-17 08:48:09 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2020-05-17 08:48:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 533,\r\n 'downloader/request_count': 2,\r\n 'downloader/request_method_count/GET': 2,\r\n 'downloader/response_bytes': 44999,\r\n 'downloader/response_count': 2,\r\n 'downloader/response_status_count/200': 2,\r\n 'elapsed_time_seconds': 1.774663,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2020, 5, 17, 4, 18, 9, 761477),\r\n 'log_count/DEBUG': 2,\r\n 'log_count/ERROR': 1,\r\n 'log_count/INFO': 10,\r\n 'response_received_count': 2,\r\n 'robotstxt/request_count': 1,\r\n 'robotstxt/response_count': 1,\r\n 'robotstxt/response_status_count/200': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'spider_exceptions/AttributeError': 1,\r\n 'start_time': datetime.datetime(2020, 5, 17, 4, 18, 7, 986814)}\r\n2020-05-17 08:48:09 [scrapy.core.engine] INFO: Spider closed (finished)\r\n\r\n\r\n\r\nCan someone help me out with this issue?\r\n\r\nBy the way, I am using the latest version of scrapy (2.1.0) and python 3", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4582", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4582/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4582/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4582/events", "html_url": "https://github.com/scrapy/scrapy/issues/4582", "id": 620017200, "node_id": "MDU6SXNzdWU2MjAwMTcyMDA=", "number": 4582, "title": "Scrapy 2.1. with Openssl 18 dont support RSA-MD5 cipher website", "user": {"login": "jgnan", "id": 1083557, "node_id": "MDQ6VXNlcjEwODM1NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1083557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgnan", "html_url": "https://github.com/jgnan", "followers_url": "https://api.github.com/users/jgnan/followers", "following_url": "https://api.github.com/users/jgnan/following{/other_user}", "gists_url": "https://api.github.com/users/jgnan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgnan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgnan/subscriptions", "organizations_url": "https://api.github.com/users/jgnan/orgs", "repos_url": "https://api.github.com/users/jgnan/repos", "events_url": "https://api.github.com/users/jgnan/events{/privacy}", "received_events_url": "https://api.github.com/users/jgnan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-18T08:37:08Z", "updated_at": "2020-07-20T09:40:20Z", "closed_at": "2020-05-18T20:54:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Dear scrapy community,\r\n\r\nI'm new to scrapy, and trying to crawl information from this site:\r\nhttps://www.365-china.cn/\r\n\r\nI only need the index page.\r\n\r\nAs it is a stander SSL website with formal SSL cert( which is identified by the chrome browser), I find it impossible to be crawl using scrapy(also python request), but is possible to grep content from curl, comment:\r\ncurl --location --request GET 'https://www.365-china.cn/'\r\n\r\nAs I'm a beginner to python3 and learn so many article from internet, I am still very confused and could not figure out what this problem is.\r\n\r\nMethods I'd tried:\r\n- https://github.com/scrapy/scrapy/issues/2916, try every option but not work. I realise what I want is to using http way to get https content, but I don't know how.\r\n- Write a customer context factory, but how? I even get fail by using requests...\r\n\r\nI don't know it should be an issue or a problem, but as an user I found it very difficult to me to solve this problem, please give me some instructions to solving this problem, thanks a lot.\r\n\r\nMy Environment:\r\n```\r\nScrapy       : 2.1.0\r\nTwisted      : 19.10.0\r\nPython       : 3.7.5\r\npyOpenSSL    : 19.1.0\r\ncryptography : 2.8\r\nPlatform: Mac OS 10.15.4\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4578", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4578/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4578/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4578/events", "html_url": "https://github.com/scrapy/scrapy/issues/4578", "id": 618600739, "node_id": "MDU6SXNzdWU2MTg2MDA3Mzk=", "number": 4578, "title": "Downloadable documentation is missing for versions 2.0 and 2.1 on readthedocs.org", "user": {"login": "IlnarSelimcan", "id": 23083718, "node_id": "MDQ6VXNlcjIzMDgzNzE4", "avatar_url": "https://avatars3.githubusercontent.com/u/23083718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IlnarSelimcan", "html_url": "https://github.com/IlnarSelimcan", "followers_url": "https://api.github.com/users/IlnarSelimcan/followers", "following_url": "https://api.github.com/users/IlnarSelimcan/following{/other_user}", "gists_url": "https://api.github.com/users/IlnarSelimcan/gists{/gist_id}", "starred_url": "https://api.github.com/users/IlnarSelimcan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IlnarSelimcan/subscriptions", "organizations_url": "https://api.github.com/users/IlnarSelimcan/orgs", "repos_url": "https://api.github.com/users/IlnarSelimcan/repos", "events_url": "https://api.github.com/users/IlnarSelimcan/events{/privacy}", "received_events_url": "https://api.github.com/users/IlnarSelimcan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-14T23:43:56Z", "updated_at": "2020-05-19T19:15:55Z", "closed_at": "2020-05-19T15:45:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "For some reason downloadable documentation on https://readthedocs.org/projects/scrapy/downloads/ is available only up to version 1.8.\r\n\r\nThat's a minor issue, but I think that I'm not the only one who prefers to read technical papers in the pdf format (to be able to take notes).\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4571", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4571/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4571/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4571/events", "html_url": "https://github.com/scrapy/scrapy/issues/4571", "id": 617117908, "node_id": "MDU6SXNzdWU2MTcxMTc5MDg=", "number": 4571, "title": "Json.loads may raise an error in settings.getdict('FEED')", "user": {"login": "jacty", "id": 4190959, "node_id": "MDQ6VXNlcjQxOTA5NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/4190959?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacty", "html_url": "https://github.com/jacty", "followers_url": "https://api.github.com/users/jacty/followers", "following_url": "https://api.github.com/users/jacty/following{/other_user}", "gists_url": "https://api.github.com/users/jacty/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacty/subscriptions", "organizations_url": "https://api.github.com/users/jacty/orgs", "repos_url": "https://api.github.com/users/jacty/repos", "events_url": "https://api.github.com/users/jacty/events{/privacy}", "received_events_url": "https://api.github.com/users/jacty/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-05-13T03:47:11Z", "updated_at": "2020-05-13T21:50:04Z", "closed_at": "2020-05-13T21:50:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/scrapy/scrapy/blob/8d1269bcbc81fa0bb5a69068e07bdbcb0dba8889/scrapy/settings/__init__.py#L197\r\n\r\nWhen setting's value is string, Json.loads may raise an JSONDecodeError error if my test has nothing wrong, I guess. I tried to use settings.getdict('BOT_NAME') and it raises the JSONDecodeError as I expressed. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4570", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4570/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4570/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4570/events", "html_url": "https://github.com/scrapy/scrapy/issues/4570", "id": 616660380, "node_id": "MDU6SXNzdWU2MTY2NjAzODA=", "number": 4570, "title": "Flake8 build error in Travis", "user": {"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 839225636, "node_id": "MDU6TGFiZWw4MzkyMjU2MzY=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/upstream%20issue", "name": "upstream issue", "color": "c2e0c6", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-05-12T13:32:53Z", "updated_at": "2020-05-13T21:02:04Z", "closed_at": "2020-05-13T21:02:04Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Flake8 checks are currently failing with the following for all files:\r\n```\r\n.tox/flake8/lib/python3.8/site-packages/pluggy/hooks.py:286: in __call__\r\n    return self._hookexec(self, self.get_hookimpls(), kwargs)\r\n.tox/flake8/lib/python3.8/site-packages/pluggy/manager.py:93: in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n.tox/flake8/lib/python3.8/site-packages/pluggy/manager.py:84: in <lambda>\r\n    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(\r\n.tox/flake8/lib/python3.8/site-packages/_pytest/runner.py:134: in pytest_runtest_call\r\n    item.runtest()\r\n.tox/flake8/lib/python3.8/site-packages/pytest_flake8.py:119: in runtest\r\n    found_errors, out, err = call(\r\n.tox/flake8/lib/python3.8/site-packages/py/_io/capture.py:150: in call\r\n    res = func(*args, **kwargs)\r\n.tox/flake8/lib/python3.8/site-packages/pytest_flake8.py:191: in check_file\r\n    app.parse_preliminary_options_and_args(args)\r\nE   AttributeError: 'Application' object has no attribute 'parse_preliminary_options_and_args'\r\n```\r\n\r\nFailed builds:\r\n* https://travis-ci.org/github/scrapy/scrapy/jobs/686060612\r\n* https://travis-ci.org/github/scrapy/scrapy/jobs/685973155\r\n\r\nI suspect there might have been some change in a recent release of `pytest-flake8`, and pinning the version would solve the problem.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4567", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4567/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4567/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4567/events", "html_url": "https://github.com/scrapy/scrapy/issues/4567", "id": 616101013, "node_id": "MDU6SXNzdWU2MTYxMDEwMTM=", "number": 4567, "title": "Addtion of pre-commit hooks", "user": {"login": "shubhank-saxena", "id": 29003047, "node_id": "MDQ6VXNlcjI5MDAzMDQ3", "avatar_url": "https://avatars3.githubusercontent.com/u/29003047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shubhank-saxena", "html_url": "https://github.com/shubhank-saxena", "followers_url": "https://api.github.com/users/shubhank-saxena/followers", "following_url": "https://api.github.com/users/shubhank-saxena/following{/other_user}", "gists_url": "https://api.github.com/users/shubhank-saxena/gists{/gist_id}", "starred_url": "https://api.github.com/users/shubhank-saxena/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shubhank-saxena/subscriptions", "organizations_url": "https://api.github.com/users/shubhank-saxena/orgs", "repos_url": "https://api.github.com/users/shubhank-saxena/repos", "events_url": "https://api.github.com/users/shubhank-saxena/events{/privacy}", "received_events_url": "https://api.github.com/users/shubhank-saxena/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-11T18:46:11Z", "updated_at": "2020-05-12T17:21:36Z", "closed_at": "2020-05-12T17:21:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your pull request, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#writing-patches and https://doc.scrapy.org/en/latest/contributing.html#submitting-patches\r\n\r\n-->\r\n\r\n## Summary\r\nPlanning to implement pre-commit hooks to take care of some lining issues.\r\n## Motivation\r\nWith lot of contributors coming in, it's pain to see travis tests failing and then working on listing issues. So we can setup pre-commit hooks to lint and run some pre-commit instructions before a contributor commits to the codebase\r\n\r\n@Gallaecio ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4565", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4565/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4565/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4565/events", "html_url": "https://github.com/scrapy/scrapy/issues/4565", "id": 615842795, "node_id": "MDU6SXNzdWU2MTU4NDI3OTU=", "number": 4565, "title": "AttributeError: module 'resource' has no attribute 'getrusage'", "user": {"login": "kingking888", "id": 44130236, "node_id": "MDQ6VXNlcjQ0MTMwMjM2", "avatar_url": "https://avatars1.githubusercontent.com/u/44130236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kingking888", "html_url": "https://github.com/kingking888", "followers_url": "https://api.github.com/users/kingking888/followers", "following_url": "https://api.github.com/users/kingking888/following{/other_user}", "gists_url": "https://api.github.com/users/kingking888/gists{/gist_id}", "starred_url": "https://api.github.com/users/kingking888/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kingking888/subscriptions", "organizations_url": "https://api.github.com/users/kingking888/orgs", "repos_url": "https://api.github.com/users/kingking888/repos", "events_url": "https://api.github.com/users/kingking888/events{/privacy}", "received_events_url": "https://api.github.com/users/kingking888/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2020-05-11T12:22:17Z", "updated_at": "2020-05-12T09:00:41Z", "closed_at": "2020-05-11T14:03:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "version : Scrapy 2.1.0\r\n\r\n```\r\n2020-05-11 20:05:28 [scrapy.core.engine] INFO: Spider opened\r\n2020-05-11 20:05:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-05-11 20:05:28 [dy] INFO: Spider opened: dy\r\n2020-05-11 20:05:28 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method MemoryUsage.engine_started of <scrapy.extensions.memusage.MemoryUsage object at 0x0000000004D3A358>>\r\nTraceback (most recent call last):\r\n  File \"D:\\microsoft\\python37\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 161, in maybeDeferred_coro\r\n    result = f(*args, **kw)\r\n  File \"D:\\microsoft\\python37\\lib\\site-packages\\pydispatch\\robustapply.py\", line 55, in robustApply\r\n    return receiver(*arguments, **named)\r\n  File \"D:\\microsoft\\python37\\lib\\site-packages\\scrapy\\extensions\\memusage.py\", line 55, in engine_started\r\n    self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\r\n  File \"D:\\microsoft\\python37\\lib\\site-packages\\scrapy\\extensions\\memusage.py\", line 48, in get_virtual_size\r\n    size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\r\nAttributeError: module 'resource' has no attribute 'getrusage'\r\n```\r\n\r\n```\r\n2020-05-11 20:05:43 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2020-05-11 20:05:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 6751,\r\n 'downloader/request_count': 14,\r\n 'downloader/request_method_count/GET': 14,\r\n 'downloader/response_bytes': 12380415,\r\n 'downloader/response_count': 14,\r\n 'downloader/response_status_count/200': 10,\r\n 'downloader/response_status_count/302': 4,\r\n 'elapsed_time_seconds': 14.631021,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2020, 5, 11, 12, 5, 43, 378200),\r\n 'item_scraped_count': 65,\r\n 'log_count/DEBUG': 85,\r\n 'log_count/ERROR': 1,\r\n 'log_count/INFO': 9,\r\n 'request_depth_max': 1,\r\n 'response_received_count': 10,\r\n 'scheduler/dequeued': 6,\r\n 'scheduler/dequeued/memory': 6,\r\n 'scheduler/enqueued': 6,\r\n 'scheduler/enqueued/memory': 6,\r\n 'start_time': datetime.datetime(2020, 5, 11, 12, 5, 28, 747179)}\r\n2020-05-11 20:05:43 [scrapy.core.engine] INFO: Spider closed (finished)\r\n2020-05-11 20:05:43 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method MemoryUsage.engine_stopped of <scrapy.extensions.memusage.MemoryUsage object at 0x0000000004D3A358>>\r\nTraceback (most recent call last):\r\n  File \"D:\\microsoft\\python37\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 161, in maybeDeferred_coro\r\n    result = f(*args, **kw)\r\n  File \"D:\\microsoft\\python37\\lib\\site-packages\\pydispatch\\robustapply.py\", line 55, in robustApply\r\n    return receiver(*arguments, **named)\r\n  File \"D:\\microsoft\\python37\\lib\\site-packages\\scrapy\\extensions\\memusage.py\", line 70, in engine_stopped\r\n    for tsk in self.tasks:\r\nAttributeError: 'MemoryUsage' object has no attribute 'tasks'\r\n```\r\n\r\n(edited for text formatting)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4561", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4561/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4561/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4561/events", "html_url": "https://github.com/scrapy/scrapy/issues/4561", "id": 615357517, "node_id": "MDU6SXNzdWU2MTUzNTc1MTc=", "number": 4561, "title": "`scrapy genspider` should not overwrite existing file", "user": {"login": "metaperl", "id": 21293, "node_id": "MDQ6VXNlcjIxMjkz", "avatar_url": "https://avatars3.githubusercontent.com/u/21293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/metaperl", "html_url": "https://github.com/metaperl", "followers_url": "https://api.github.com/users/metaperl/followers", "following_url": "https://api.github.com/users/metaperl/following{/other_user}", "gists_url": "https://api.github.com/users/metaperl/gists{/gist_id}", "starred_url": "https://api.github.com/users/metaperl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/metaperl/subscriptions", "organizations_url": "https://api.github.com/users/metaperl/orgs", "repos_url": "https://api.github.com/users/metaperl/repos", "events_url": "https://api.github.com/users/metaperl/events{/privacy}", "received_events_url": "https://api.github.com/users/metaperl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-05-10T10:01:18Z", "updated_at": "2020-08-17T08:45:53Z", "closed_at": "2020-08-17T08:45:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n## Summary\r\n\r\nIf the file mentioned in `scrapy genspider` already exists, then genspider should refuse to generate the the file.\r\n\r\n## Motivation\r\n\r\nAs it stands, existing code can be blown away if this command runs twice.\r\n\r\n## Describe alternatives you've considered\r\n\r\nPrompting the user for overwriting existing spider.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4556", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4556/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4556/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4556/events", "html_url": "https://github.com/scrapy/scrapy/issues/4556", "id": 614936669, "node_id": "MDU6SXNzdWU2MTQ5MzY2Njk=", "number": 4556, "title": "Cover chompjs in the documentation", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-08T19:24:21Z", "updated_at": "2020-05-12T18:42:28Z", "closed_at": "2020-05-12T18:42:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "We cover js2xml in the documentation. However, the library can be rather slow. For use cases where https://github.com/Nykakin/chompjs may be used instead, it should be encouraged.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4550", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4550/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4550/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4550/events", "html_url": "https://github.com/scrapy/scrapy/issues/4550", "id": 614231551, "node_id": "MDU6SXNzdWU2MTQyMzE1NTE=", "number": 4550, "title": "Remove # -*- coding: utf-8 -*-", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-07T17:41:53Z", "updated_at": "2020-05-08T13:45:20Z", "closed_at": "2020-05-08T13:45:20Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "We still got a few files starting with that line, from Python 2 times. Time to get rid of it for good.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4549", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4549/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4549/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4549/events", "html_url": "https://github.com/scrapy/scrapy/issues/4549", "id": 614068578, "node_id": "MDU6SXNzdWU2MTQwNjg1Nzg=", "number": 4549, "title": "Spiderloader: Add tests for duplicate locations", "user": {"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-07T13:42:51Z", "updated_at": "2020-05-12T15:10:10Z", "closed_at": "2020-05-12T15:10:10Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "See https://github.com/scrapy/scrapy/pull/4543/files/4c12a234ae65d49678a9840708ff5e7b9d6dcecc#diff-04eeb72b0ac2a03d00f2b1dcc3268ebc", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4548", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4548/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4548/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4548/events", "html_url": "https://github.com/scrapy/scrapy/issues/4548", "id": 613944813, "node_id": "MDU6SXNzdWU2MTM5NDQ4MTM=", "number": 4548, "title": "Increase code sharing between the crawl and runspider commands", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-07T10:25:13Z", "updated_at": "2020-06-03T07:06:13Z", "closed_at": "2020-06-03T07:06:13Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Their `add_options` and `process_options` methods are identical. They could be moved into a shared, parent class.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4547", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4547/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4547/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4547/events", "html_url": "https://github.com/scrapy/scrapy/issues/4547", "id": 613920912, "node_id": "MDU6SXNzdWU2MTM5MjA5MTI=", "number": 4547, "title": "Review unicode references", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-07T09:47:03Z", "updated_at": "2020-08-04T18:34:12Z", "closed_at": "2020-08-04T18:34:12Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The documentation still has some references to Python 2\u2019s `unicode`. We need to review them, and update them as needed (usually changing them to `str` or \u201cstring\u201d).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4546", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4546/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4546/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4546/events", "html_url": "https://github.com/scrapy/scrapy/issues/4546", "id": 613918679, "node_id": "MDU6SXNzdWU2MTM5MTg2Nzk=", "number": 4546, "title": "Deprecate Response.body_as_unicode", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-07T09:43:27Z", "updated_at": "2020-05-11T18:20:32Z", "closed_at": "2020-05-11T18:20:32Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4535", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4535/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4535/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4535/events", "html_url": "https://github.com/scrapy/scrapy/issues/4535", "id": 612416190, "node_id": "MDU6SXNzdWU2MTI0MTYxOTA=", "number": 4535, "title": "ImportError: No module named twisted.web", "user": {"login": "Strato-fortex", "id": 64832336, "node_id": "MDQ6VXNlcjY0ODMyMzM2", "avatar_url": "https://avatars1.githubusercontent.com/u/64832336?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Strato-fortex", "html_url": "https://github.com/Strato-fortex", "followers_url": "https://api.github.com/users/Strato-fortex/followers", "following_url": "https://api.github.com/users/Strato-fortex/following{/other_user}", "gists_url": "https://api.github.com/users/Strato-fortex/gists{/gist_id}", "starred_url": "https://api.github.com/users/Strato-fortex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Strato-fortex/subscriptions", "organizations_url": "https://api.github.com/users/Strato-fortex/orgs", "repos_url": "https://api.github.com/users/Strato-fortex/repos", "events_url": "https://api.github.com/users/Strato-fortex/events{/privacy}", "received_events_url": "https://api.github.com/users/Strato-fortex/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-05T08:04:36Z", "updated_at": "2020-05-05T12:30:50Z", "closed_at": "2020-05-05T12:30:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "sslstrip -i 8080\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/sslstrip\", line 27, in <module>\r\n    from twisted.web import http\r\nImportError: No module named twisted.web\r\n\r\nwhat i have to do in this case?\r\nHave i to install twisted-web?How?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4531", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4531/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4531/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4531/events", "html_url": "https://github.com/scrapy/scrapy/issues/4531", "id": 611009049, "node_id": "MDU6SXNzdWU2MTEwMDkwNDk=", "number": 4531, "title": "Scrapy throws timeout with http2 site", "user": {"login": "marizaga", "id": 12060080, "node_id": "MDQ6VXNlcjEyMDYwMDgw", "avatar_url": "https://avatars1.githubusercontent.com/u/12060080?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marizaga", "html_url": "https://github.com/marizaga", "followers_url": "https://api.github.com/users/marizaga/followers", "following_url": "https://api.github.com/users/marizaga/following{/other_user}", "gists_url": "https://api.github.com/users/marizaga/gists{/gist_id}", "starred_url": "https://api.github.com/users/marizaga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marizaga/subscriptions", "organizations_url": "https://api.github.com/users/marizaga/orgs", "repos_url": "https://api.github.com/users/marizaga/repos", "events_url": "https://api.github.com/users/marizaga/events{/privacy}", "received_events_url": "https://api.github.com/users/marizaga/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-01T22:43:13Z", "updated_at": "2020-05-04T17:19:03Z", "closed_at": "2020-05-04T17:19:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "**DESCRIPTION**\r\n\r\nA Timeout exception raises when crawl https://www.adidas.pe, I suspect the site only supports http2 and scrapy does not, curl can extract the information but if I force it to use http1.1 it also throws timeout.\r\n\r\n\r\n**settings.py:**\r\n\r\n```\r\nLOG_STDOUT = True\r\nMEMDEBUG_ENABLED = True\r\nDOWNLOAD_TIMEOUT = 25\r\nRETRY_ENABLED = False\r\nROBOTSTXT_OBEY = False\r\nDOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING = True\r\nDEFAULT_REQUEST_HEADERS = {\r\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\r\n        'Accept-Language': 'es-US,es-419;',\r\n        'accept-encoding': 'gzip, deflate'\r\n}\r\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'\r\n```\r\n\r\n**Installed packages and versions:**\r\n```\r\nscrapy version -v\r\n\r\nScrapy       : 2.0.1\r\nlxml         : 4.5.0.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.5.2\r\nw3lib        : 1.21.0\r\nTwisted      : 20.3.0\r\nPython       : 3.7.4 (default, Apr  1 2020, 23:03:35) - [GCC 6.3.0 20170516]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)\r\ncryptography : 2.9.2\r\nPlatform     : Linux-4.9.0-11-amd64-x86_64-with-debian-9.12\r\n```\r\n\r\n\r\n****\r\n**Testing with Scrapy and CURL:**\r\n```\r\nscrapy shell https://www.adidas.pe\r\n\r\n2020-05-01 16:51:32 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: tests)\r\n2020-05-01 16:51:32 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.7.4 (default, Apr  1 2020, 23:03:35) - [GCC 6.3.0 20170516], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.9.0-11-amd64-x86_64-with-debian-9.12\r\n2020-05-01 16:51:32 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\r\n2020-05-01 16:51:32 [scrapy.crawler] INFO: Overridden settings:\r\n{'BOT_NAME': 'tests',\r\n 'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True,\r\n 'DOWNLOAD_TIMEOUT': 25,\r\n 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\r\n 'LOGSTATS_INTERVAL': 0,\r\n 'LOG_STDOUT': True,\r\n 'MEMDEBUG_ENABLED': True,\r\n 'NEWSPIDER_MODULE': 'tests.spiders',\r\n 'RETRY_ENABLED': False,\r\n 'SPIDER_MODULES': ['tests.spiders'],\r\n 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\r\n               '(KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}\r\n2020-05-01 16:51:32 [scrapy.extensions.telnet] INFO: Telnet Password: ef6cffde752fb19f\r\n2020-05-01 16:51:32 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.memdebug.MemoryDebugger']\r\n2020-05-01 16:51:32 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2020-05-01 16:51:32 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2020-05-01 16:51:32 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2020-05-01 16:51:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\r\n2020-05-01 16:51:32 [scrapy.core.engine] INFO: Spider opened\r\n2020-05-01 16:51:32 [scrapy.core.downloader.tls] DEBUG: SSL connection to www.adidas.pe using protocol TLSv1.3, cipher TLS_AES_256_GCM_SHA384\r\n2020-05-01 16:51:32 [scrapy.core.downloader.tls] DEBUG: SSL connection certificate: issuer \"/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=GeoTrust RSA CA 2018\", subject \"/C=DE/ST=Bavaria/L=Herzogenaurach/O=adidas AG/OU=Global IT/CN=www.global.adidas.com\"\r\n2020-05-01 16:51:32 [scrapy.core.downloader.tls] DEBUG: SSL temp key: X25519, 253 bits\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/scrapy\", line 10, in <module>\r\n    sys.exit(execute())\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 145, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 99, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/cmdline.py\", line 153, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/commands/shell.py\", line 74, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/shell.py\", line 45, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/usr/local/lib/python3.7/site-packages/scrapy/shell.py\", line 113, in fetch\r\n    reactor, self._schedule, request, spider)\r\n  File \"/usr/local/lib/python3.7/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/usr/local/lib/python3.7/site-packages/twisted/python/failure.py\", line 488, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.adidas.pe took longer than 25.0 seconds..\r\n```\r\n\r\n**Failed test with CURL:**\r\n```\r\ncurl https://www.adidas.pe --http1.1 --max-time 25 -v -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\" -H \"accept-language: es-US,es-419;\" -H \"accept-encoding: gzip, deflate\" -o adidas.html\r\n\r\n* Rebuilt URL to: https://www.adidas.pe/\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0*   Trying 104.93.58.229...\r\n* TCP_NODELAY set\r\n  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0* Connected to www.adidas.pe (104.93.58.229) port 443 (#0)\r\n* ALPN, offering http/1.1\r\n* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH\r\n* successfully set certificate verify locations:\r\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\r\n  CApath: /etc/ssl/certs\r\n* TLSv1.2 (OUT), TLS header, Certificate Status (22):\r\n} [5 bytes data]\r\n* TLSv1.2 (OUT), TLS handshake, Client hello (1):\r\n} [512 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Server hello (2):\r\n{ [108 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\r\n{ [5537 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\r\n{ [333 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\r\n{ [4 bytes data]\r\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\r\n} [70 bytes data]\r\n* TLSv1.2 (OUT), TLS change cipher, Client hello (1):\r\n} [1 bytes data]\r\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\r\n} [16 bytes data]\r\n* TLSv1.2 (IN), TLS change cipher, Client hello (1):\r\n{ [1 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Finished (20):\r\n{ [16 bytes data]\r\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384\r\n* ALPN, server accepted to use http/1.1\r\n* Server certificate:\r\n*  subject: C=DE; ST=Bavaria; L=Herzogenaurach; O=adidas AG; OU=Global IT; CN=www.global.adidas.com\r\n*  start date: Jan 30 00:00:00 2020 GMT\r\n*  expire date: Dec 29 12:00:00 2020 GMT\r\n*  subjectAltName: host \"www.adidas.pe\" matched cert's \"www.adidas.pe\"\r\n*  issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=GeoTrust RSA CA 2018\r\n*  SSL certificate verify ok.\r\n} [5 bytes data]\r\n> GET / HTTP/1.1\r\n> Host: www.adidas.pe\r\n> Accept: */*\r\n> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\r\n> accept-language: es-US,es-419;\r\n> accept-encoding: gzip, deflate\r\n>\r\n  0     0    0     0    0     0      0      0 --:--:--  0:00:24 --:--:--     0* Operation timed out after 25000 milliseconds with 0 bytes received\r\n* Curl_http_done: called premature == 1\r\n* stopped the pause stream!\r\n  0     0    0     0    0     0      0      0 --:--:--  0:00:25 --:--:--     0\r\n* Closing connection 0\r\n} [5 bytes data]\r\n* TLSv1.2 (OUT), TLS alert, Client hello (1):\r\n} [2 bytes data]\r\ncurl: (28) Operation timed out after 25000 milliseconds with 0 bytes received\r\n```\r\n\r\n**Successful test with CURL:**\r\n\r\n```\r\ncurl https://www.adidas.pe --http2 --max-time 25 -v -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\" -H \"accept-language: es-US,es-419;\" -H \"accept-encoding: gzip, deflate\" -o adidas.html\r\n\r\n* Rebuilt URL to: https://www.adidas.pe/\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 104.93.58.229...\r\n* TCP_NODELAY set\r\n* Connected to www.adidas.pe (104.93.58.229) port 443 (#0)\r\n* ALPN, offering h2\r\n* ALPN, offering http/1.1\r\n* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH\r\n* successfully set certificate verify locations:\r\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\r\n  CApath: /etc/ssl/certs\r\n* TLSv1.2 (OUT), TLS header, Certificate Status (22):\r\n} [5 bytes data]\r\n* TLSv1.2 (OUT), TLS handshake, Client hello (1):\r\n} [512 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Server hello (2):\r\n{ [102 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Certificate (11):\r\n{ [5537 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Server key exchange (12):\r\n{ [333 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Server finished (14):\r\n{ [4 bytes data]\r\n* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\r\n} [70 bytes data]\r\n* TLSv1.2 (OUT), TLS change cipher, Client hello (1):\r\n} [1 bytes data]\r\n* TLSv1.2 (OUT), TLS handshake, Finished (20):\r\n} [16 bytes data]\r\n* TLSv1.2 (IN), TLS change cipher, Client hello (1):\r\n{ [1 bytes data]\r\n* TLSv1.2 (IN), TLS handshake, Finished (20):\r\n{ [16 bytes data]\r\n* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384\r\n* ALPN, server accepted to use h2\r\n* Server certificate:\r\n*  subject: C=DE; ST=Bavaria; L=Herzogenaurach; O=adidas AG; OU=Global IT; CN=www.global.adidas.com\r\n*  start date: Jan 30 00:00:00 2020 GMT\r\n*  expire date: Dec 29 12:00:00 2020 GMT\r\n*  subjectAltName: host \"www.adidas.pe\" matched cert's \"www.adidas.pe\"\r\n*  issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=GeoTrust RSA CA 2018\r\n*  SSL certificate verify ok.\r\n* Using HTTP2, server supports multi-use\r\n* Connection state changed (HTTP/2 confirmed)\r\n* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\r\n} [5 bytes data]\r\n* Using Stream ID: 1 (easy handle 0x564dee91eea0)\r\n} [5 bytes data]\r\n> GET / HTTP/1.1\r\n> Host: www.adidas.pe\r\n> Accept: */*\r\n> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\r\n> accept-language: es-US,es-419;\r\n> accept-encoding: gzip, deflate\r\n>\r\n{ [5 bytes data]\r\n* Connection state changed (MAX_CONCURRENT_STREAMS updated)!\r\n} [5 bytes data]\r\n< HTTP/2 200\r\n< content-type: text/html; charset=utf-8\r\n< vary: Accept-Encoding\r\n< server-timing: intid;desc=e9079c33383fd210\r\n< x-frame-options: SAMEORIGIN\r\n< x-content-type-options: nosniff\r\n< x-request-id: dd261749a9a43e1b1c4bbf4cda012016\r\n< content-encoding: gzip\r\n< x-akamai-transformed: 9 166248 0 pmb=mTOE,5\r\n< expires: Fri, 01 May 2020 22:31:51 GMT\r\n< cache-control: max-age=0, no-cache, no-store\r\n< pragma: no-cache\r\n< date: Fri, 01 May 2020 22:31:51 GMT\r\n< set-cookie: geo_ip=52.117.61.178\r\n< set-cookie: geo_country=US; path=/\r\n< set-cookie: onesite_country=PE; path=/\r\n< set-cookie: ak_bmsc=F93ABD35DA357BAEF36AA6A747603C0C60074BAE3467000057A3AC5E3F70A018~plPyeQB26uSsj3SNwXXEGgmqgEcFYs7DjwroQ/TdOTJxdXOFbVpxWIwSAe+1fEMXwmoWqosVxPmCFSSHsO/AGq9SuVsUmZy6tLjMWojl1QCPPr7L6RoumvRt4Iizuk5jvPCo15E/WCIbSLf3ue5Cglc5df0DOhriP+/DCZ1d92o2Sw4FaX4LtQ44Vf50ChN+sGv4+8/x0unciIVQaJ/tVNDiJYqOzfYGu5ZNiSctY6MnMSwgOWAHExIoEGv8+23I8OiLka8CZn2T+HRPOvKFgsO+U7AwFhmVxJIdgrW0Oc0LsrMj6X0aIuONH/NQ5tOQE71hw8mrPJyUcPGTJ+2Qzc4g==; expires=Sat, 02 May 2020 00:31:51 GMT; max-age=7200; path=/; domain=.adidas.pe; HttpOnly\r\n< set-cookie: akacd_generic_prod_grayling_adidas=3765825110~rv=4~id=dacf829aefc7506e875ec9bd0f144718; path=/;; Secure; SameSite=None\r\n< remaining_edge_ttl: -655\r\n< access-control-max-age: 86400\r\n< access-control-allow-credentials: true\r\n< access-control-allow-headers: *\r\n< access-control-allow-methods: GET,POST\r\n< strict-transport-security: max-age=31536000 ; includeSubDomains ; preload\r\n< set-cookie: bm_mi=7A66B23220463183ED41658D57D2EA48~mbdQy0MZg88Xo85GAqKOQUJMAhHyJL1bhk8hgmPY82TQAKLe/qALZlxra68PshipWlgaurtbHUPzRoq+vD4/c0ShXR9BT4DSF6BdgxNP7bjNdICgt0InIff5njdti9pTTNmynslfMUbUYjSx72VkUtFr1U3zmcaLzzznsgQs3E9CN+hhUoQk9Im/vbs0L5vjxUX+wRBFrdVr6gqwHLiFqQ==; Domain=.adidas.pe; Path=/; Max-Age=0; HttpOnly\r\n< set-cookie: bm_sz=5F01E98C8428A0DC76CF013A40BD62D9~YAAQrksHYOJCsL5xAQAAqgxe0gexc5CKe71Hfni57eviwp0936qKrsLHSlUbuWhg0b1M7obQRY6N7CYCwX0dlvnfTEPqBMT71ojdLJDtELLpwWNnS8Ey1TW3mZds3Vzont0s/bP7AVAvPCq4bw95/oOir4ycl637c5VYpFqKMMyMPJDaKSkiHdUfcgbPcQ==; Domain=.adidas.pe; Path=/; Expires=Sat, 02 May 2020 02:31:51 GMT; Max-Age=14400; HttpOnly\r\n< set-cookie: _abck=E8982021FDD0A1E2FCF640226FF0AB93~-1~YAAQrksHYONCsL5xAQAAqgxe0gOjHMa9sgBUvVhBCAVEwZkL+YV4MXImLZ2JKWVNxRxnQoBjlFpM0f6RpBLkhXG8Q7Azq9gfav7vL7jFHlo3TiVA6wEnJmLf5fPtn6naBE5BVvtZfeZ3Id/K1qhUOOfrbz/x+//ZTxULMDoKXdCKNghmLSaVlktvI3Q7hde6gRq3pWMi9BMkI/elsORrNm8b4EktKmlpLv8a82e/8ahFV5ZX36wy6t2oPoV5gj0VU6GP0uk8voIzN/jKheLHlxMYUrQQEHaYXIFbHRSABb15/RazJJmPcaA=~-1~-1~-1; Domain=.adidas.pe; Path=/; Expires=Fri, 08 May 2020 22:31:51 GMT; Max-Age=604800; Secure\r\n<\r\n{ [167 bytes data]\r\n* Curl_http_done: called premature == 0\r\n100  156k    0  156k    0     0   453k      0 --:--:-- --:--:-- --:--:--  454k\r\n* Connection #0 to host www.adidas.pe left intact\r\n```\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4529", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4529/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4529/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4529/events", "html_url": "https://github.com/scrapy/scrapy/issues/4529", "id": 610530677, "node_id": "MDU6SXNzdWU2MTA1MzA2Nzc=", "number": 4529, "title": "Bind request to response only if not bound already", "user": {"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 280218717, "node_id": "MDU6TGFiZWwyODAyMTg3MTc=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/discuss", "name": "discuss", "color": "cc317c", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-01T03:08:11Z", "updated_at": "2020-08-17T08:40:00Z", "closed_at": "2020-08-17T08:40:00Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Summary\r\nCurrently requests are bound to responses in [the engine](https://github.com/scrapy/scrapy/blob/2.1.0/scrapy/core/engine.py#L247) and/or in [the scraper](https://github.com/scrapy/scrapy/blob/2.1.0/scrapy/core/scraper.py#L149).\r\nI would like to start a conversation to figure out the implications of binding them only if the `response.request` attribute is not already set, i.e. something like this:\r\n\r\n```diff\r\ndiff --git scrapy/core/engine.py scrapy/core/engine.py\r\nindex 77d71846..72f1628a 100644\r\n--- scrapy/core/engine.py\r\n+++ scrapy/core/engine.py\r\n@@ -244,7 +244,8 @@ class ExecutionEngine:\r\n                     % (type(response), response)\r\n                 )\r\n             if isinstance(response, Response):\r\n-                response.request = request  # tie request to response received\r\n+                if response.request is None:\r\n+                    response.request = request\r\n                 logkws = self.logformatter.crawled(request, response, spider)\r\n                 if logkws is not None:\r\n                     logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\r\n\r\n\r\ndiff --git scrapy/core/scraper.py scrapy/core/scraper.py\r\nindex edbb4dd6..256935da 100644\r\n--- scrapy/core/scraper.py\r\n+++ scrapy/core/scraper.py\r\n@@ -146,7 +146,8 @@ class Scraper:\r\n                 self._log_download_errors, request_result, request, spider)\r\n \r\n     def call_spider(self, result, request, spider):\r\n-        result.request = request\r\n+        if not hasattr(result, \"request\") or result.request is None:\r\n+            result.request = request\r\n         dfd = defer_result(result)\r\n         callback = request.callback or spider.parse\r\n         warn_on_generator_with_return_value(spider, callback)\r\n```\r\n\r\n\r\n## Motivation\r\n\r\nRecently I've been working on a downloader middleware to transparently redirect requests to a service which takes an URL and returns its response body along with some other metadata. The `process_request` method replaces the URL, and the `process_response` decodes the response body and creates a new response, making it look like it came directly from the target website, not from the download service. At this point, I'd like to be able to restore the original request as well, but any request I return in the `Response.request` attribute gets overridden.\r\n\r\n## Describe alternatives you've considered\r\n\r\nATM I don't see a way to modify this behaviour without altering the Scrapy core.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4528", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4528/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4528/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4528/events", "html_url": "https://github.com/scrapy/scrapy/issues/4528", "id": 610220653, "node_id": "MDU6SXNzdWU2MTAyMjA2NTM=", "number": 4528, "title": "Fail or warn if from_crawler() returns None", "user": {"login": "c3pmark", "id": 42784895, "node_id": "MDQ6VXNlcjQyNzg0ODk1", "avatar_url": "https://avatars2.githubusercontent.com/u/42784895?v=4", "gravatar_id": "", "url": "https://api.github.com/users/c3pmark", "html_url": "https://github.com/c3pmark", "followers_url": "https://api.github.com/users/c3pmark/followers", "following_url": "https://api.github.com/users/c3pmark/following{/other_user}", "gists_url": "https://api.github.com/users/c3pmark/gists{/gist_id}", "starred_url": "https://api.github.com/users/c3pmark/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/c3pmark/subscriptions", "organizations_url": "https://api.github.com/users/c3pmark/orgs", "repos_url": "https://api.github.com/users/c3pmark/repos", "events_url": "https://api.github.com/users/c3pmark/events{/privacy}", "received_events_url": "https://api.github.com/users/c3pmark/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-30T16:29:56Z", "updated_at": "2020-05-11T18:35:26Z", "closed_at": "2020-05-11T18:35:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Summary\r\n\r\nGenerate a warning or error if from_crawler() for a middleware/extension/etc. returns None\r\n\r\n## Motivation\r\n\r\nI created a custom extension and connected signals in the from_crawler() classmethod, but neglected to return the new extension instance.  Scrapy still reported the extension under \"Enabled extensions\", but none of the signals worked, since the instance was immediately garbage collected and its signals were silently disconnected.\r\n\r\nThis was of course an error on my part, but it would have saved me a lot of debugging if I had gotten a warning that from_crawler() was returning None, or if the extension were removed from the \"Enabled extensions\" list.\r\n\r\nWould it be appropriate for utils.misc.create_instance() to raise an error or generate a warning if it's about to return None?  Or should MiddlewareManager treat create_instance() returning None the same as create_instance() raising NotConfigured?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4526", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4526/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4526/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4526/events", "html_url": "https://github.com/scrapy/scrapy/issues/4526", "id": 608731284, "node_id": "MDU6SXNzdWU2MDg3MzEyODQ=", "number": 4526, "title": "Getting full page text with proper formatting with scrapy", "user": {"login": "upworkap", "id": 54761068, "node_id": "MDQ6VXNlcjU0NzYxMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/54761068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/upworkap", "html_url": "https://github.com/upworkap", "followers_url": "https://api.github.com/users/upworkap/followers", "following_url": "https://api.github.com/users/upworkap/following{/other_user}", "gists_url": "https://api.github.com/users/upworkap/gists{/gist_id}", "starred_url": "https://api.github.com/users/upworkap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/upworkap/subscriptions", "organizations_url": "https://api.github.com/users/upworkap/orgs", "repos_url": "https://api.github.com/users/upworkap/repos", "events_url": "https://api.github.com/users/upworkap/events{/privacy}", "received_events_url": "https://api.github.com/users/upworkap/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-29T02:16:41Z", "updated_at": "2020-05-12T15:19:59Z", "closed_at": "2020-04-29T05:12:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Now I have trouble getting text from the page (WordPress post) with formatting as it can be get from the browser (ctrl+c) or selenium. Can you implement such function? Or at least post a docs about it.\r\n\r\nBy using xpath I am getting only text without line breaks, or text with too many line breaks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4523", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4523/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4523/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4523/events", "html_url": "https://github.com/scrapy/scrapy/issues/4523", "id": 608434370, "node_id": "MDU6SXNzdWU2MDg0MzQzNzA=", "number": 4523, "title": "I didn't find scrapy code Entrance after executing scrapy crawl ", "user": {"login": "zeguangzhang", "id": 17930575, "node_id": "MDQ6VXNlcjE3OTMwNTc1", "avatar_url": "https://avatars0.githubusercontent.com/u/17930575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zeguangzhang", "html_url": "https://github.com/zeguangzhang", "followers_url": "https://api.github.com/users/zeguangzhang/followers", "following_url": "https://api.github.com/users/zeguangzhang/following{/other_user}", "gists_url": "https://api.github.com/users/zeguangzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/zeguangzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zeguangzhang/subscriptions", "organizations_url": "https://api.github.com/users/zeguangzhang/orgs", "repos_url": "https://api.github.com/users/zeguangzhang/repos", "events_url": "https://api.github.com/users/zeguangzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/zeguangzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-28T16:02:57Z", "updated_at": "2020-04-29T07:47:03Z", "closed_at": "2020-04-29T07:47:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "scrapy source code forked, but I didn't find scrapy code Entrance after executing scrapy crawl {spider_name} command ,scrapy code Entrance  where\uff0cwaiting for you replay ,thanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4515", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4515/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4515/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4515/events", "html_url": "https://github.com/scrapy/scrapy/issues/4515", "id": 607729442, "node_id": "MDU6SXNzdWU2MDc3Mjk0NDI=", "number": 4515, "title": "Weird args and proxy behavior in lua script when resending a splash request", "user": {"login": "jeremfrs", "id": 8864964, "node_id": "MDQ6VXNlcjg4NjQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/8864964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeremfrs", "html_url": "https://github.com/jeremfrs", "followers_url": "https://api.github.com/users/jeremfrs/followers", "following_url": "https://api.github.com/users/jeremfrs/following{/other_user}", "gists_url": "https://api.github.com/users/jeremfrs/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeremfrs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeremfrs/subscriptions", "organizations_url": "https://api.github.com/users/jeremfrs/orgs", "repos_url": "https://api.github.com/users/jeremfrs/repos", "events_url": "https://api.github.com/users/jeremfrs/events{/privacy}", "received_events_url": "https://api.github.com/users/jeremfrs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-27T17:40:47Z", "updated_at": "2020-04-27T19:14:42Z", "closed_at": "2020-04-27T19:14:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I encoutered some weird args behaviors in lua.\r\n\r\nI create my SplashRequest like so:\r\n`yield SplashRequest(url=url,\r\n                callback=self.parse,\r\n                endpoint='execute',\r\n                args={\r\n                    'lua_source': self.LUA_SOURCE,\r\n                    'pproxy': \"NOT_WORKING_PROXY\",\r\n                    'wait': 3,\r\n                },\r\n                cache_args=['lua_source']\r\n            )`\r\n\r\nAnd when my proxy isn't working i catch it in my middleware and i send back the same request but with another proxy:\r\n\r\n```def process_response(self, request, response, spider):\r\ndef process_response(self, request, response, spider):\r\n        print(\"\\tEntering process_response-------------------------------\")\r\n\r\n        print(response.status)\r\n        if (response.status == 504 or response.status == 400):\r\n            meta = deepcopy(response.request.meta)\r\n            meta[\"splash\"][\"args\"][\"pproxy\"] = \"WORKING_PROXY\"\r\n            newrq = response.request.replace(meta=meta, dont_filter=True)\r\n            return newrq\r\n\r\n        return response\r\n```\r\n\r\nI double checked, newrq has the \"WORKING_PROXY\" and not the \"NOT_WORKING_PROXY\"\r\n\r\nSo now here is the lua code in self.LUA_SOURCE\r\n```\r\nfunction useproxy(splash)\r\n    local host = splash.args.pproxy\r\n    local port = 80\r\n\r\n    print(host)\r\n    splash:on_request(function (request)\r\n        print(host)\r\n        request:set_proxy{host, port, username=\"myuser\", password=\"mypwd\"}\r\n    end)\r\nend\r\n\r\nfunction main(splash)\r\n    local user_agent = \"Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36\"\r\n    splash:set_user_agent(user_agent)\r\n    assert(splash:go(splash.args.url))\r\n    useproxy(splash)\r\n    return splash:html()\r\nend\r\n```\r\n\r\nWhen the order of function in the main is EXACTLY like that, the proxy is set correctly and my request works BUT both print in useproxy print the old proxy (NOT_WORKING_PROXY). \r\nAnother weird behavior, if i remove the line that set the user agent, the proxy is set to the old one (NOT_WORKING_PROXY) and if i put useproxy before splash:go it's the same.\r\n\r\nJust so you know, my code is working perfectly fine if i remove my code in process_response and i don't return a new request.\r\n\r\nI think it is a bug related to lua but i'm not sure or maybe i'm stuck with a mistake, i don't know.\r\nIf you know a fix or if it's a mistake, let me know please.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4514", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4514/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4514/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4514/events", "html_url": "https://github.com/scrapy/scrapy/issues/4514", "id": 607615491, "node_id": "MDU6SXNzdWU2MDc2MTU0OTE=", "number": 4514, "title": "Outdated item pipeline example about Deferred", "user": {"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-27T14:53:50Z", "updated_at": "2020-05-22T18:12:58Z", "closed_at": "2020-05-22T18:12:58Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The [_Take screenshot of item_](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#take-screenshot-of-item) example says \"This example demonstrates how to return a Deferred from the process_item() method\", but that is no longer the case (async/await syntax is currently used).\r\nI think we should remove that sentence ~and add a reminder about [enabling the `asyncio` reactor](https://docs.scrapy.org/en/latest/topics/asyncio.html#installing-the-asyncio-reactor).~\r\n\r\nAlso, I wonder if it'd be good to also restore the previous `Deferred` example and keep both of them, since not all projects might be able/willing to switch to the `asyncio` reactor.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4511", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4511/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4511/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4511/events", "html_url": "https://github.com/scrapy/scrapy/issues/4511", "id": 605675288, "node_id": "MDU6SXNzdWU2MDU2NzUyODg=", "number": 4511, "title": "Remove the asyncio warning from the page about coroutines", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-23T16:29:00Z", "updated_at": "2020-04-27T17:45:19Z", "closed_at": "2020-04-27T17:45:19Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "It\u2019s a leftover from a point where both the coroutines and the asyncio pages were a single page.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4510", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4510/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4510/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4510/events", "html_url": "https://github.com/scrapy/scrapy/issues/4510", "id": 604187291, "node_id": "MDU6SXNzdWU2MDQxODcyOTE=", "number": 4510, "title": "configure_logging documentation example incomplete", "user": {"login": "vccolombo", "id": 19277813, "node_id": "MDQ6VXNlcjE5Mjc3ODEz", "avatar_url": "https://avatars1.githubusercontent.com/u/19277813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vccolombo", "html_url": "https://github.com/vccolombo", "followers_url": "https://api.github.com/users/vccolombo/followers", "following_url": "https://api.github.com/users/vccolombo/following{/other_user}", "gists_url": "https://api.github.com/users/vccolombo/gists{/gist_id}", "starred_url": "https://api.github.com/users/vccolombo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vccolombo/subscriptions", "organizations_url": "https://api.github.com/users/vccolombo/orgs", "repos_url": "https://api.github.com/users/vccolombo/repos", "events_url": "https://api.github.com/users/vccolombo/events{/privacy}", "received_events_url": "https://api.github.com/users/vccolombo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-21T18:18:48Z", "updated_at": "2020-05-20T17:23:43Z", "closed_at": "2020-05-20T17:23:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello!\r\n\r\nI think the [configure_logging](https://docs.scrapy.org/en/latest/topics/logging.html#scrapy.utils.log.configure_logging) example is missing something. It imports `configure_logging` but never uses it.\r\n\r\nCould you take a look at it please?\r\nThank you!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4502", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4502/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4502/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4502/events", "html_url": "https://github.com/scrapy/scrapy/issues/4502", "id": 601009203, "node_id": "MDU6SXNzdWU2MDEwMDkyMDM=", "number": 4502, "title": "Scrapy LinkExtractor not able to extracts absolute urls", "user": {"login": "rahulmarlabs", "id": 45551560, "node_id": "MDQ6VXNlcjQ1NTUxNTYw", "avatar_url": "https://avatars2.githubusercontent.com/u/45551560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rahulmarlabs", "html_url": "https://github.com/rahulmarlabs", "followers_url": "https://api.github.com/users/rahulmarlabs/followers", "following_url": "https://api.github.com/users/rahulmarlabs/following{/other_user}", "gists_url": "https://api.github.com/users/rahulmarlabs/gists{/gist_id}", "starred_url": "https://api.github.com/users/rahulmarlabs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rahulmarlabs/subscriptions", "organizations_url": "https://api.github.com/users/rahulmarlabs/orgs", "repos_url": "https://api.github.com/users/rahulmarlabs/repos", "events_url": "https://api.github.com/users/rahulmarlabs/events{/privacy}", "received_events_url": "https://api.github.com/users/rahulmarlabs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-16T12:29:17Z", "updated_at": "2020-04-16T13:54:31Z", "closed_at": "2020-04-16T13:54:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nScrapy LinkExtractor not able to extracts absolute urls\r\n\r\n### Steps to Reproduce\r\n\r\n```\r\nfrom scrapy.linkextractors import LinkExtractor\r\nfrom scrapy.http import HtmlResponse\r\n\r\nhtml =\"\"\"<html>\r\n<body>\r\n    <a href=\"/en/veterinary-regulatory/post-authorisation\" class=\"ema-context-banner__link\">Post-authorisation</a>\r\n    <a href=\"https://www.ema.europa.eu/documents/overview/ritonavir-mylan-epar-summary-public_fr.pdf\" target=\"_blank\" class=\"ecl-link ecl-link--ema-obvious ecl-file__translations-title\">French</a>\r\n</body>\r\n</html>\"\"\"\r\n\r\nresponse = HtmlResponse(url=\"http://www.ema.europa.eu\",body=bytes(html, encoding='utf8'))\r\nle = LinkExtractor()\r\n\r\nlinks = le.extract_links(response)\r\nprint(links)\r\n```\r\n\r\n`OUTPUT:  [Link(url='http://www.ema.europa.eu/en/veterinary-regulatory/post-authorisation', text='Post-authorisation', fragment='', nofollow=False)]`\r\n\r\n**Expected behavior:**  It is supposed to extract both the urls\r\n\r\n**Actual behavior:** But it does not extract absolute URL  \r\n\r\n**Reproduces how often:** Every time\r\n\r\n### Versions\r\n\r\nScrapy       : 2.0.1\r\nlxml         : 4.5.0.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.5.2\r\nw3lib        : 1.21.0\r\nTwisted      : 20.3.0\r\nPython       : 3.7.3 (default, Mar 27 2019, 22:11:17) - [GCC 7.3.0]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1f  31 Mar 2020)\r\ncryptography : 2.9\r\nPlatform     : Linux-5.3.0-40-generic-x86_64-with-debian-buster-sid\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4501", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4501/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4501/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4501/events", "html_url": "https://github.com/scrapy/scrapy/issues/4501", "id": 600809173, "node_id": "MDU6SXNzdWU2MDA4MDkxNzM=", "number": 4501, "title": "  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1", "user": {"login": "alirezadizaji", "id": 33565522, "node_id": "MDQ6VXNlcjMzNTY1NTIy", "avatar_url": "https://avatars3.githubusercontent.com/u/33565522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alirezadizaji", "html_url": "https://github.com/alirezadizaji", "followers_url": "https://api.github.com/users/alirezadizaji/followers", "following_url": "https://api.github.com/users/alirezadizaji/following{/other_user}", "gists_url": "https://api.github.com/users/alirezadizaji/gists{/gist_id}", "starred_url": "https://api.github.com/users/alirezadizaji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alirezadizaji/subscriptions", "organizations_url": "https://api.github.com/users/alirezadizaji/orgs", "repos_url": "https://api.github.com/users/alirezadizaji/repos", "events_url": "https://api.github.com/users/alirezadizaji/events{/privacy}", "received_events_url": "https://api.github.com/users/alirezadizaji/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-16T07:25:50Z", "updated_at": "2020-04-16T13:47:30Z", "closed_at": "2020-04-16T13:47:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\nHi :), I have a requirements.txt and I run it  by `pip install -r requirements.txt`.\r\nand in some point I got this error `Building wheel for gnureadline (setup.py) ... error`.\r\nI saw the same issues but they didn't help.\r\nhere is the whole error:\r\n` Building wheel for gnureadline (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /media/assignment1/.env/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-mdc0qyyt/gnureadline/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-mdc0qyyt/gnureadline/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-loxqpxxp\r\n       cwd: /tmp/pip-install-mdc0qyyt/gnureadline/\r\n  Complete output (475 lines):\r\n  \r\n  ============ Building the readline library ============\r\n  \r\n  readline-6.3/\r\n  readline-6.3/doc/\r\n  readline-6.3/doc/Makefile.in\r\n  readline-6.3/doc/texinfo.tex\r\n  readline-6.3/doc/version.texi\r\n  readline-6.3/doc/fdl.texi\r\n  readline-6.3/doc/rlman.texi\r\n  readline-6.3/doc/rltech.texi\r\n  readline-6.3/doc/rluser.texi\r\n  readline-6.3/doc/rluserman.texi\r\n  readline-6.3/doc/history.texi\r\n  readline-6.3/doc/hstech.texi\r\n  readline-6.3/doc/hsuser.texi\r\n  readline-6.3/doc/readline.3\r\n  readline-6.3/doc/history.3\r\n  readline-6.3/doc/texi2dvi\r\n  readline-6.3/doc/texi2html\r\n  readline-6.3/doc/readline.ps\r\n  readline-6.3/doc/history.ps\r\n  readline-6.3/doc/rluserman.ps\r\n  readline-6.3/doc/readline.dvi\r\n  readline-6.3/doc/history.dvi\r\n  readline-6.3/doc/rluserman.dvi\r\n  readline-6.3/doc/readline.info\r\n  readline-6.3/doc/history.info\r\n  readline-6.3/doc/rluserman.info\r\n  readline-6.3/doc/readline.html\r\n  readline-6.3/doc/history.html\r\n  readline-6.3/doc/rluserman.html\r\n  readline-6.3/doc/readline.0\r\n  readline-6.3/doc/history.0\r\n  readline-6.3/doc/readline_3.ps\r\n  readline-6.3/doc/history_3.ps\r\n  readline-6.3/doc/history.pdf\r\n  readline-6.3/doc/readline.pdf\r\n  readline-6.3/doc/rluserman.pdf\r\n  readline-6.3/examples/\r\n  readline-6.3/examples/autoconf/\r\n  readline-6.3/examples/autoconf/BASH_CHECK_LIB_TERMCAP\r\n  readline-6.3/examples/autoconf/RL_LIB_READLINE_VERSION\r\n  readline-6.3/examples/autoconf/wi_LIB_READLINE\r\n  readline-6.3/examples/rlfe/\r\n  readline-6.3/examples/rlfe/ChangeLog\r\n  readline-6.3/examples/rlfe/Makefile.in\r\n  readline-6.3/examples/rlfe/README\r\n  readline-6.3/examples/rlfe/config.h.in\r\n  readline-6.3/examples/rlfe/configure\r\n  readline-6.3/examples/rlfe/configure.in\r\n  readline-6.3/examples/rlfe/extern.h\r\n  readline-6.3/examples/rlfe/os.h\r\n  readline-6.3/examples/rlfe/pty.c\r\n  readline-6.3/examples/rlfe/rlfe.c\r\n  readline-6.3/examples/rlfe/screen.h\r\n  readline-6.3/examples/Makefile.in\r\n  readline-6.3/examples/excallback.c\r\n  readline-6.3/examples/fileman.c\r\n  readline-6.3/examples/manexamp.c\r\n  readline-6.3/examples/readlinebuf.h\r\n  readline-6.3/examples/rl-fgets.c\r\n  readline-6.3/examples/rlcat.c\r\n  readline-6.3/examples/rlevent.c\r\n  readline-6.3/examples/rltest.c\r\n  readline-6.3/examples/rl-callbacktest.c\r\n  readline-6.3/examples/rl.c\r\n  readline-6.3/examples/rlptytest.c\r\n  readline-6.3/examples/rlversion.c\r\n  readline-6.3/examples/histexamp.c\r\n  readline-6.3/examples/hist_erasedups.c\r\n  readline-6.3/examples/hist_purgecmd.c\r\n  readline-6.3/examples/Inputrc\r\n  readline-6.3/examples/rlwrap-0.30.tar.gz\r\n  readline-6.3/support/\r\n  readline-6.3/support/config.guess\r\n  readline-6.3/support/config.rpath\r\n  readline-6.3/support/config.sub\r\n  readline-6.3/support/install.sh\r\n  readline-6.3/support/mkdirs\r\n  readline-6.3/support/mkdist\r\n  readline-6.3/support/mkinstalldirs\r\n  readline-6.3/support/shobj-conf\r\n  readline-6.3/support/shlib-install\r\n  readline-6.3/support/wcwidth.c\r\n  readline-6.3/shlib/\r\n  readline-6.3/shlib/Makefile.in\r\n  readline-6.3/COPYING\r\n  readline-6.3/README\r\n  readline-6.3/MANIFEST\r\n  readline-6.3/INSTALL\r\n  readline-6.3/CHANGELOG\r\n  readline-6.3/CHANGES\r\n  readline-6.3/NEWS\r\n  readline-6.3/USAGE\r\n  readline-6.3/aclocal.m4\r\n  readline-6.3/config.h.in\r\n  readline-6.3/configure\r\n  readline-6.3/configure.ac\r\n  readline-6.3/Makefile.in\r\n  readline-6.3/ansi_stdlib.h\r\n  readline-6.3/chardefs.h\r\n  readline-6.3/colors.h\r\n  readline-6.3/history.h\r\n  readline-6.3/histlib.h\r\n  readline-6.3/keymaps.h\r\n  readline-6.3/parse-colors.h\r\n  readline-6.3/rlconf.h\r\n  readline-6.3/posixdir.h\r\n  readline-6.3/posixjmp.h\r\n  readline-6.3/posixselect.h\r\n  readline-6.3/posixstat.h\r\n  readline-6.3/readline.h\r\n  readline-6.3/rldefs.h\r\n  readline-6.3/rlmbutil.h\r\n  readline-6.3/rlprivate.h\r\n  readline-6.3/rlshell.h\r\n  readline-6.3/rlstdc.h\r\n  readline-6.3/rltty.h\r\n  readline-6.3/rltypedefs.h\r\n  readline-6.3/rlwinsize.h\r\n  readline-6.3/tcap.h\r\n  readline-6.3/tilde.h\r\n  readline-6.3/xmalloc.h\r\n  readline-6.3/bind.c\r\n  readline-6.3/callback.c\r\n  readline-6.3/colors.c\r\n  readline-6.3/compat.c\r\n  readline-6.3/complete.c\r\n  readline-6.3/display.c\r\n  readline-6.3/emacs_keymap.c\r\n  readline-6.3/funmap.c\r\n  readline-6.3/input.c\r\n  readline-6.3/isearch.c\r\n  readline-6.3/keymaps.c\r\n  readline-6.3/kill.c\r\n  readline-6.3/macro.c\r\n  readline-6.3/mbutil.c\r\n  readline-6.3/misc.c\r\n  readline-6.3/nls.c\r\n  readline-6.3/parens.c\r\n  readline-6.3/parse-colors.c\r\n  readline-6.3/readline.c\r\n  readline-6.3/rltty.c\r\n  readline-6.3/savestring.c\r\n  readline-6.3/search.c\r\n  readline-6.3/shell.c\r\n  readline-6.3/signals.c\r\n  readline-6.3/terminal.c\r\n  readline-6.3/text.c\r\n  readline-6.3/tilde.c\r\n  readline-6.3/undo.c\r\n  readline-6.3/util.c\r\n  readline-6.3/vi_keymap.c\r\n  readline-6.3/vi_mode.c\r\n  readline-6.3/xfree.c\r\n  readline-6.3/xmalloc.c\r\n  readline-6.3/history.c\r\n  readline-6.3/histexpand.c\r\n  readline-6.3/histfile.c\r\n  readline-6.3/histsearch.c\r\n  readline-6.3/patchlevel\r\n  patching file readline.c\r\n  patching file patchlevel\r\n  patching file readline.c\r\n  patching file patchlevel\r\n  patching file util.c\r\n  patching file patchlevel\r\n  checking build system type... x86_64-unknown-linux-gnu\r\n  checking host system type... x86_64-unknown-linux-gnu\r\n  \r\n  Beginning configuration for readline-6.3 for x86_64-unknown-linux-gnu\r\n  \r\n  checking whether make sets $(MAKE)... yes\r\n  checking for gcc... gcc\r\n  checking whether the C compiler works... yes\r\n  checking for C compiler default output file name... a.out\r\n  checking for suffix of executables...\r\n  checking whether we are cross compiling... no\r\n  checking for suffix of object files... o\r\n  checking whether we are using the GNU C compiler... yes\r\n  checking whether gcc accepts -g... yes\r\n  checking for gcc option to accept ISO C89... none needed\r\n  checking how to run the C preprocessor... gcc -E\r\n  checking for grep that handles long lines and -e... /bin/grep\r\n  checking for egrep... /bin/grep -E\r\n  checking for ANSI C header files... yes\r\n  checking for sys/types.h... yes\r\n  checking for sys/stat.h... yes\r\n  checking for stdlib.h... yes\r\n  checking for string.h... yes\r\n  checking for memory.h... yes\r\n  checking for strings.h... yes\r\n  checking for inttypes.h... yes\r\n  checking for stdint.h... yes\r\n  checking for unistd.h... yes\r\n  checking minix/config.h usability... no\r\n  checking minix/config.h presence... no\r\n  checking for minix/config.h... no\r\n  checking whether it is safe to define __EXTENSIONS__... yes\r\n  checking whether gcc needs -traditional... no\r\n  checking for a BSD-compatible install... /usr/bin/install -c\r\n  checking for ar... ar\r\n  checking for ranlib... ranlib\r\n  checking for an ANSI C-conforming const... yes\r\n  checking for function prototypes... yes\r\n  checking whether char is unsigned... no\r\n  checking for working volatile... yes\r\n  checking return type of signal handlers... void\r\n  checking for size_t... yes\r\n  checking for ssize_t... yes\r\n  checking for ANSI C header files... (cached) yes\r\n  checking whether stat file-mode macros are broken... no\r\n  checking for dirent.h that defines DIR... yes\r\n  checking for library containing opendir... none required\r\n  checking for fcntl... yes\r\n  checking for kill... yes\r\n  checking for lstat... yes\r\n  checking for memmove... yes\r\n  checking for putenv... yes\r\n  checking for select... yes\r\n  checking for setenv... yes\r\n  checking for setlocale... yes\r\n  checking for strcasecmp... yes\r\n  checking for strpbrk... yes\r\n  checking for tcgetattr... yes\r\n  checking for vsnprintf... yes\r\n  checking for isascii... yes\r\n  checking for isxdigit... yes\r\n  checking for getpwent... yes\r\n  checking for getpwnam... yes\r\n  checking for getpwuid... yes\r\n  checking for working strcoll... yes\r\n  checking fcntl.h usability... yes\r\n  checking fcntl.h presence... yes\r\n  checking for fcntl.h... yes\r\n  checking for unistd.h... (cached) yes\r\n  checking for stdlib.h... (cached) yes\r\n  checking varargs.h usability... no\r\n  checking varargs.h presence... no\r\n  checking for varargs.h... no\r\n  checking stdarg.h usability... yes\r\n  checking stdarg.h presence... yes\r\n  checking for stdarg.h... yes\r\n  checking stdbool.h usability... yes\r\n  checking stdbool.h presence... yes\r\n  checking for stdbool.h... yes\r\n  checking for string.h... (cached) yes\r\n  checking for strings.h... (cached) yes\r\n  checking limits.h usability... yes\r\n  checking limits.h presence... yes\r\n  checking for limits.h... yes\r\n  checking locale.h usability... yes\r\n  checking locale.h presence... yes\r\n  checking for locale.h... yes\r\n  checking pwd.h usability... yes\r\n  checking pwd.h presence... yes\r\n  checking for pwd.h... yes\r\n  checking for memory.h... (cached) yes\r\n  checking termcap.h usability... no\r\n  checking termcap.h presence... no\r\n  checking for termcap.h... no\r\n  checking termios.h usability... yes\r\n  checking termios.h presence... yes\r\n  checking for termios.h... yes\r\n  checking termio.h usability... yes\r\n  checking termio.h presence... yes\r\n  checking for termio.h... yes\r\n  checking sys/pte.h usability... no\r\n  checking sys/pte.h presence... no\r\n  checking for sys/pte.h... no\r\n  checking sys/stream.h usability... no\r\n  checking sys/stream.h presence... no\r\n  checking for sys/stream.h... no\r\n  checking sys/select.h usability... yes\r\n  checking sys/select.h presence... yes\r\n  checking for sys/select.h... yes\r\n  checking sys/file.h usability... yes\r\n  checking sys/file.h presence... yes\r\n  checking for sys/file.h... yes\r\n  checking for sys/ptem.h... no\r\n  checking for special C compiler options needed for large files... no\r\n  checking for _FILE_OFFSET_BITS value needed for large files... no\r\n  checking for type of signal functions... posix\r\n  checking if signal handlers must be reinstalled when invoked... no\r\n  checking for presence of POSIX-style sigsetjmp/siglongjmp... present\r\n  checking for lstat... yes\r\n  checking whether or not strcoll and strcmp differ... no\r\n  checking whether the ctype macros accept non-ascii characters... no\r\n  checking whether getpw functions are declared in pwd.h... yes\r\n  checking whether termios.h defines TIOCGWINSZ... no\r\n  checking whether sys/ioctl.h defines TIOCGWINSZ... yes\r\n  checking for sig_atomic_t in signal.h... yes\r\n  checking whether signal handlers are of type void... yes\r\n  checking for TIOCSTAT in sys/ioctl.h... no\r\n  checking for FIONREAD in sys/ioctl.h... yes\r\n  checking for speed_t in sys/types.h... no\r\n  checking for struct winsize in sys/ioctl.h and termios.h... sys/ioctl.h\r\n  checking for struct dirent.d_ino... yes\r\n  checking for struct dirent.d_fileno... yes\r\n  checking whether AUDIT_USER_TTY is declared... yes\r\n  checking for tgetent... no\r\n  checking for tgetent in -ltermcap... no\r\n  checking for tgetent in -ltinfo... no\r\n  checking for tgetent in -lcurses... no\r\n  checking for tgetent in -lncurses... no\r\n  checking which library has the termcap functions... using gnutermcap\r\n  checking wctype.h usability... yes\r\n  checking wctype.h presence... yes\r\n  checking for wctype.h... yes\r\n  checking wchar.h usability... yes\r\n  checking wchar.h presence... yes\r\n  checking for wchar.h... yes\r\n  checking langinfo.h usability... yes\r\n  checking langinfo.h presence... yes\r\n  checking for langinfo.h... yes\r\n  checking for mbrlen... yes\r\n  checking for mbscasecmp... no\r\n  checking for mbscmp... no\r\n  checking for mbsnrtowcs... yes\r\n  checking for mbsrtowcs... yes\r\n  checking for mbschr... no\r\n  checking for wcrtomb... yes\r\n  checking for wcscoll... yes\r\n  checking for wcsdup... yes\r\n  checking for wcwidth... yes\r\n  checking for wctype... yes\r\n  checking for wcswidth... yes\r\n  checking whether mbrtowc and mbstate_t are properly declared... yes\r\n  checking for iswlower... yes\r\n  checking for iswupper... yes\r\n  checking for towlower... yes\r\n  checking for towupper... yes\r\n  checking for iswctype... yes\r\n  checking for nl_langinfo and CODESET... yes\r\n  checking for wchar_t in wchar.h... yes\r\n  checking for wctype_t in wctype.h... yes\r\n  checking for wint_t in wctype.h... yes\r\n  checking for wcwidth broken with unicode combining characters...\r\n  checking configuration for building shared libraries... supported\r\n  configure: creating ./config.status\r\n  config.status: creating Makefile\r\n  config.status: creating doc/Makefile\r\n  config.status: creating examples/Makefile\r\n  config.status: creating shlib/Makefile\r\n  config.status: creating config.h\r\n  config.status: executing default commands\r\n  rm -f readline.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O readline.c\r\n  rm -f vi_mode.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O vi_mode.c\r\n  rm -f funmap.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O funmap.c\r\n  rm -f keymaps.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O keymaps.c\r\n  rm -f parens.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O parens.c\r\n  rm -f search.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O search.c\r\n  rm -f rltty.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O rltty.c\r\n  rm -f complete.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O complete.c\r\n  In file included from complete.c:62:0:\r\n  complete.c: In function \u2018fnwidth\u2019:\r\n  rlmbutil.h:132:23: warning: implicit declaration of function \u2018wcwidth\u2019; did you mean \u2018fnwidth\u2019? [-Wimplicit-function-declaration]\r\n   #  define WCWIDTH(wc) wcwidth(wc)\r\n                         ^\r\n  complete.c:746:12: note: in expansion of macro \u2018WCWIDTH\u2019\r\n          w = WCWIDTH (wc);\r\n              ^~~~~~~\r\n  rm -f bind.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O bind.c\r\n  rm -f isearch.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O isearch.c\r\n  rm -f display.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O display.c\r\n  In file included from display.c:50:0:\r\n  display.c: In function \u2018rl_redisplay\u2019:\r\n  rlmbutil.h:132:23: warning: implicit declaration of function \u2018wcwidth\u2019 [-Wimplicit-function-declaration]\r\n   #  define WCWIDTH(wc) wcwidth(wc)\r\n                         ^\r\n  display.c:804:15: note: in expansion of macro \u2018WCWIDTH\u2019\r\n          temp = WCWIDTH (wc);\r\n                 ^~~~~~~\r\n  rm -f signals.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O signals.c\r\n  rm -f util.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O util.c\r\n  rm -f kill.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O kill.c\r\n  rm -f undo.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O undo.c\r\n  rm -f macro.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O macro.c\r\n  rm -f input.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O input.c\r\n  rm -f callback.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O callback.c\r\n  rm -f terminal.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O terminal.c\r\n  rm -f text.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O text.c\r\n  rm -f nls.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O nls.c\r\n  rm -f misc.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O misc.c\r\n  rm -f history.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O history.c\r\n  rm -f histexpand.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O histexpand.c\r\n  rm -f histfile.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O histfile.c\r\n  rm -f histsearch.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O histsearch.c\r\n  rm -f shell.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O shell.c\r\n  rm -f mbutil.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O mbutil.c\r\n  In file included from mbutil.c:47:0:\r\n  mbutil.c: In function \u2018_rl_find_next_mbchar_internal\u2019:\r\n  rlmbutil.h:132:23: warning: implicit declaration of function \u2018wcwidth\u2019 [-Wimplicit-function-declaration]\r\n   #  define WCWIDTH(wc) wcwidth(wc)\r\n                         ^\r\n  mbutil.c:125:12: note: in expansion of macro \u2018WCWIDTH\u2019\r\n          if (WCWIDTH (wc) == 0)\r\n              ^~~~~~~\r\n  rm -f tilde.o\r\n  gcc -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O -DREADLINE_LIBRARY -c ./tilde.c\r\n  rm -f colors.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O colors.c\r\n  rm -f parse-colors.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O parse-colors.c\r\n  rm -f xmalloc.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O xmalloc.c\r\n  rm -f xfree.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O xfree.c\r\n  rm -f compat.o\r\n  gcc -c -DHAVE_CONFIG_H   -DNEED_EXTERN_PC -fPIC -I. -I. -DRL_LIBRARY_VERSION='\"6.3\"' -g -O compat.c\r\n  rm -f libreadline.a\r\n  ar cr libreadline.a readline.o vi_mode.o funmap.o keymaps.o parens.o search.o rltty.o complete.o bind.o isearch.o display.o signals.o util.o kill.o undo.o macro.o input.o callback.o terminal.o text.o nls.o misc.o history.o histexpand.o histfile.o histsearch.o shell.o mbutil.o tilde.o colors.o parse-colors.o xmalloc.o xfree.o compat.o\r\n  test -n \"ranlib\" && ranlib libreadline.a\r\n  rm -f libhistory.a\r\n  ar cr libhistory.a history.o histexpand.o histfile.o histsearch.o shell.o mbutil.o xmalloc.o xfree.o\r\n  test -n \"ranlib\" && ranlib libhistory.a\r\n  \r\n  ============ Building the readline extension module ============\r\n  \r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  copying readline.py -> build/lib.linux-x86_64-3.6\r\n  running egg_info\r\n  writing gnureadline.egg-info/PKG-INFO\r\n  writing dependency_links to gnureadline.egg-info/dependency_links.txt\r\n  writing top-level names to gnureadline.egg-info/top_level.txt\r\n  reading manifest file 'gnureadline.egg-info/SOURCES.txt'\r\n  reading manifest template 'MANIFEST.in'\r\n  writing manifest file 'gnureadline.egg-info/SOURCES.txt'\r\n  running build_ext\r\n  building 'gnureadline' extension\r\n  creating build/temp.linux-x86_64-3.6\r\n  creating build/temp.linux-x86_64-3.6/Modules\r\n  creating build/temp.linux-x86_64-3.6/Modules/3.x\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DHAVE_RL_CALLBACK -DHAVE_RL_CATCH_SIGNAL -DHAVE_RL_COMPLETION_APPEND_CHARACTER -DHAVE_RL_COMPLETION_DISPLAY_MATCHES_HOOK -DHAVE_RL_COMPLETION_MATCHES -DHAVE_RL_COMPLETION_SUPPRESS_APPEND -DHAVE_RL_PRE_INPUT_HOOK -I. -I/usr/include/python3.6m -I/media/assignment1/.env/include/python3.6m -c Modules/3.x/readline.c -o build/temp.linux-x86_64-3.6/Modules/3.x/readline.o\r\n  Modules/3.x/readline.c: In function \u2018PyInit_gnureadline\u2019:\r\n  Modules/3.x/readline.c:1207:34: warning: assignment from incompatible pointer type [-Wincompatible-pointer-types]\r\n       PyOS_ReadlineFunctionPointer = call_readline;\r\n                                    ^\r\n  x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/Modules/3.x/readline.o readline/libreadline.a readline/libhistory.a -lncurses -o build/lib.linux-x86_64-3.6/gnureadline.cpython-36m-x86_64-linux-gnu.so\r\n  /usr/bin/ld: cannot find -lncurses\r\n  collect2: error: ld returned 1 exit status\r\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for gnureadline\r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4496", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4496/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4496/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4496/events", "html_url": "https://github.com/scrapy/scrapy/issues/4496", "id": 600486666, "node_id": "MDU6SXNzdWU2MDA0ODY2NjY=", "number": 4496, "title": "Fix the hoverxref configuration", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/scrapy/scrapy/milestones/27", "html_url": "https://github.com/scrapy/scrapy/milestone/27", "labels_url": "https://api.github.com/repos/scrapy/scrapy/milestones/27/labels", "id": 5102632, "node_id": "MDk6TWlsZXN0b25lNTEwMjYzMg==", "number": 27, "title": "v2.1", "description": "", "creator": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 5, "state": "closed", "created_at": "2020-02-13T13:01:05Z", "updated_at": "2020-05-03T22:41:02Z", "due_on": "2020-04-24T07:00:00Z", "closed_at": "2020-05-03T22:41:02Z"}, "comments": 0, "created_at": "2020-04-15T18:00:50Z", "updated_at": "2020-04-16T13:19:46Z", "closed_at": "2020-04-16T13:19:46Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "> You shouldn't override hoverxref_version and hoverxref_project since they are taken automatically from Read the Docs.\r\n>\r\n> If you want to avoid your CI failing because of this, you can define the environment variables as Read the Docs does:\r\n> \r\n> READTHEDOCS_PROJECT=scrapy\r\n> READTHEDOCS_VERSION=''\r\n> \r\n> With the current configuration, all the versions built on Read the Docs will point to a different version on Read the Docs and this will conflict. For example, current master version in Read the Docs defines hoverxref_version='2.0.0' but that version does not exist on Read the Docs and the tooltip does not known where to get the content from.\r\n\r\n@humitos at https://github.com/scrapy/scrapy/pull/4480#discussion_r409026912", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4495", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4495/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4495/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4495/events", "html_url": "https://github.com/scrapy/scrapy/issues/4495", "id": 600484478, "node_id": "MDU6SXNzdWU2MDA0ODQ0Nzg=", "number": 4495, "title": "Extend hoverxref_roles", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 14483092, "node_id": "MDU6TGFiZWwxNDQ4MzA5Mg==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-15T17:57:20Z", "updated_at": "2020-06-01T05:18:13Z", "closed_at": "2020-06-01T05:18:13Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "@humitos [suggested](https://github.com/scrapy/scrapy/issues/4475#issuecomment-613350667) extending the `hoverxref_roles` setting of the corresponding Sphinx extension so that the display-on-hover behavior of the documentations works for things like signal or setting references.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4491", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4491/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4491/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4491/events", "html_url": "https://github.com/scrapy/scrapy/issues/4491", "id": 599499489, "node_id": "MDU6SXNzdWU1OTk0OTk0ODk=", "number": 4491, "title": "inifinit websites - crawl entire net", "user": {"login": "cullsin", "id": 5137598, "node_id": "MDQ6VXNlcjUxMzc1OTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5137598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cullsin", "html_url": "https://github.com/cullsin", "followers_url": "https://api.github.com/users/cullsin/followers", "following_url": "https://api.github.com/users/cullsin/following{/other_user}", "gists_url": "https://api.github.com/users/cullsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/cullsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cullsin/subscriptions", "organizations_url": "https://api.github.com/users/cullsin/orgs", "repos_url": "https://api.github.com/users/cullsin/repos", "events_url": "https://api.github.com/users/cullsin/events{/privacy}", "received_events_url": "https://api.github.com/users/cullsin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-14T11:28:10Z", "updated_at": "2020-04-14T13:02:05Z", "closed_at": "2020-04-14T13:02:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\nI want to crawl all the venture capital URLs in the internet. I can see that URL as an Input to scrapy. Can you please recommend how to do this ?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4490", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4490/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4490/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4490/events", "html_url": "https://github.com/scrapy/scrapy/issues/4490", "id": 599308576, "node_id": "MDU6SXNzdWU1OTkzMDg1NzY=", "number": 4490, "title": "scrapy JOBDIR function seems not suitable for huge sitemap scrap", "user": {"login": "royluo1984", "id": 26940212, "node_id": "MDQ6VXNlcjI2OTQwMjEy", "avatar_url": "https://avatars2.githubusercontent.com/u/26940212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/royluo1984", "html_url": "https://github.com/royluo1984", "followers_url": "https://api.github.com/users/royluo1984/followers", "following_url": "https://api.github.com/users/royluo1984/following{/other_user}", "gists_url": "https://api.github.com/users/royluo1984/gists{/gist_id}", "starred_url": "https://api.github.com/users/royluo1984/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/royluo1984/subscriptions", "organizations_url": "https://api.github.com/users/royluo1984/orgs", "repos_url": "https://api.github.com/users/royluo1984/repos", "events_url": "https://api.github.com/users/royluo1984/events{/privacy}", "received_events_url": "https://api.github.com/users/royluo1984/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-14T05:33:03Z", "updated_at": "2020-04-14T20:50:08Z", "closed_at": "2020-04-14T20:50:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Assuming a website has 200 zipped sitemap files and 2 million links can be scrapped.\r\n\r\n**My conclusion is, when JOBDIR is used, the scrapy will only parse a part of zipped sitemaps which around 40 zipped sitemaps, 340k+ links.**\r\n\r\nThe rest part of zipped sitemaps and links will stay unparsed\r\n\r\n**JOBDIR workflow and my problem**\r\n\r\nAs we know, scrapy will eat up huge amount of RAM when scraping a huge sitemap.\r\n\r\nSo I decide to use JOBDIR to avoid Memory overflow.\r\n\r\nI find out when JOBDIR is used, scrapy will parse the first 30 zipped sitemap files and store around 300~350k links in somespider/requests.queue/p0, and store the unique fingerprints in somespider/requests.seen.\r\n\r\nOnce the script is executed\r\n\r\n`scrapy crawl somespider -s JOBDIR=crawls/somespider-1`\r\nthe size of somespider/requests.queue/p0 and somespider/requests.seen increases at the beginning, then stop increasing at some point as the links of the first batch of zipped sitemap files are all parsed.\r\n\r\nIf you open p0 file with notepad, you will find about 340k links inside. This also happen to requests.seen file, you will find the same amount unique IDs.\r\n\r\nThe size of p0 file will decrease while scrapy crawling links as the parsed links will be removed from the p0 file.\r\n\r\nMy problem is, once the links in p0 file are all parsed, the spider will be finished. the rest of the zipped sitemaps will not continue parsing.\r\n\r\n**The fix that I am looking for when JOBDIR is used**\r\n\r\nIs there a way to force scrapy to parse all the zipped sitemaps at the beginning? even there are hundreds of zipped sitemaps. So all the zipped sitemaps that go to parse will be stored in p0.\r\n\r\nOR, there is a way to make scrapy to continue parse the rest part of zipped sitemaps once the links in p0 are all parsed.\r\n\r\nThank you very much for your time to read this long article, and I am sorry for my English skill. I wish I have made my question understandable.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4488", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4488/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4488/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4488/events", "html_url": "https://github.com/scrapy/scrapy/issues/4488", "id": 598719816, "node_id": "MDU6SXNzdWU1OTg3MTk4MTY=", "number": 4488, "title": "Twisted headers mapping does nothing for lowercase headers key", "user": {"login": "2239559319", "id": 34960995, "node_id": "MDQ6VXNlcjM0OTYwOTk1", "avatar_url": "https://avatars1.githubusercontent.com/u/34960995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/2239559319", "html_url": "https://github.com/2239559319", "followers_url": "https://api.github.com/users/2239559319/followers", "following_url": "https://api.github.com/users/2239559319/following{/other_user}", "gists_url": "https://api.github.com/users/2239559319/gists{/gist_id}", "starred_url": "https://api.github.com/users/2239559319/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/2239559319/subscriptions", "organizations_url": "https://api.github.com/users/2239559319/orgs", "repos_url": "https://api.github.com/users/2239559319/repos", "events_url": "https://api.github.com/users/2239559319/events{/privacy}", "received_events_url": "https://api.github.com/users/2239559319/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-13T07:14:23Z", "updated_at": "2020-04-13T10:30:04Z", "closed_at": "2020-04-13T10:30:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### headers lowercase\r\n\r\n[scrapy can not work when headers are lowercase ]\r\n\r\n### Steps to Reproduce\r\n\r\n1. i must use lowercase headers key and I use Twisted mapping headers and it doesn't work\r\n\r\n**Expected behavior:** scrapy run with no exception\r\n\r\n**Actual behavior:** scrapy and twisted does nothing for headers\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\n### Versions\r\n\r\nScrapy       : 2.0.1\r\nlxml         : 4.4.1.0\r\nlibxml2      : 2.9.5\r\ncssselect    : 1.1.0\r\nparsel       : 1.5.2\r\nw3lib        : 1.21.0\r\nTwisted      : 19.7.0\r\nPython       : 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\r\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1c  28 May 2019)\r\ncryptography : 2.7\r\nPlatform     : Windows-10-10.0.18362-SP0\r\n\r\n\r\n### Additional context\r\n\r\nI try to rewrite the Request and Headers class but it gives me a line `Please use app access!`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4487", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4487/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4487/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4487/events", "html_url": "https://github.com/scrapy/scrapy/issues/4487", "id": 598575777, "node_id": "MDU6SXNzdWU1OTg1NzU3Nzc=", "number": 4487, "title": "Pegando dados POST com scrapy - Web Scraping - Python", "user": {"login": "rafaelmrd", "id": 47335399, "node_id": "MDQ6VXNlcjQ3MzM1Mzk5", "avatar_url": "https://avatars2.githubusercontent.com/u/47335399?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rafaelmrd", "html_url": "https://github.com/rafaelmrd", "followers_url": "https://api.github.com/users/rafaelmrd/followers", "following_url": "https://api.github.com/users/rafaelmrd/following{/other_user}", "gists_url": "https://api.github.com/users/rafaelmrd/gists{/gist_id}", "starred_url": "https://api.github.com/users/rafaelmrd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rafaelmrd/subscriptions", "organizations_url": "https://api.github.com/users/rafaelmrd/orgs", "repos_url": "https://api.github.com/users/rafaelmrd/repos", "events_url": "https://api.github.com/users/rafaelmrd/events{/privacy}", "received_events_url": "https://api.github.com/users/rafaelmrd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-12T21:33:20Z", "updated_at": "2020-04-12T22:18:45Z", "closed_at": "2020-04-12T22:18:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "USA:\r\nI have a question about Scrapy using python.\r\nCan anyone tell me how I get data using POST method with scrapy or selenium.\r\n\r\nBR:\r\nTenho uma d\u00favida sobre o Scrapy usando python.\r\nAlgu\u00e9m sabe me informar como eu pego dados com metodo POST com scrapy ou selenium", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4482", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4482/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4482/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4482/events", "html_url": "https://github.com/scrapy/scrapy/issues/4482", "id": 598235980, "node_id": "MDU6SXNzdWU1OTgyMzU5ODA=", "number": 4482, "title": "Scrapy return nothing on 400 error when scraping https://www.watsons.com.sg/", "user": {"login": "pc2000sg", "id": 45007569, "node_id": "MDQ6VXNlcjQ1MDA3NTY5", "avatar_url": "https://avatars3.githubusercontent.com/u/45007569?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pc2000sg", "html_url": "https://github.com/pc2000sg", "followers_url": "https://api.github.com/users/pc2000sg/followers", "following_url": "https://api.github.com/users/pc2000sg/following{/other_user}", "gists_url": "https://api.github.com/users/pc2000sg/gists{/gist_id}", "starred_url": "https://api.github.com/users/pc2000sg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pc2000sg/subscriptions", "organizations_url": "https://api.github.com/users/pc2000sg/orgs", "repos_url": "https://api.github.com/users/pc2000sg/repos", "events_url": "https://api.github.com/users/pc2000sg/events{/privacy}", "received_events_url": "https://api.github.com/users/pc2000sg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-11T10:13:25Z", "updated_at": "2020-04-15T12:25:59Z", "closed_at": "2020-04-15T12:25:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi I am trying to scrape content from https://www.watsons.com.sg/. Tested it with scrapy shell and Postman software (with all request headers filled based on what I see  from inspect function in Chrome browser). For some unknown reasons the response body has no real content except:\r\n\r\n_**<body>\r\n\t<div class=\"nojavascript alert alert-error\">\r\n\t\t<p>The site requires JavaScript to be enabled! The browser you're using doesn't support JavaScript, or has\r\n\t\t\tJavaScript turned off. <br>Try again with a browser that has JavaScript turned on. <a\r\n\t\t\t\thref=\"https://www.enable-javascript.com/\" target=\"_blank\">Learn More</a></p>\r\n\t</div>\r\n</body>**_\r\n\r\nWonder if there is anyway to get around this issue? Any suggestions are appreciated. Thanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4478", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4478/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4478/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4478/events", "html_url": "https://github.com/scrapy/scrapy/issues/4478", "id": 596743675, "node_id": "MDU6SXNzdWU1OTY3NDM2NzU=", "number": 4478, "title": "Download file when parent contains something", "user": {"login": "cdric0701", "id": 6715950, "node_id": "MDQ6VXNlcjY3MTU5NTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/6715950?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cdric0701", "html_url": "https://github.com/cdric0701", "followers_url": "https://api.github.com/users/cdric0701/followers", "following_url": "https://api.github.com/users/cdric0701/following{/other_user}", "gists_url": "https://api.github.com/users/cdric0701/gists{/gist_id}", "starred_url": "https://api.github.com/users/cdric0701/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cdric0701/subscriptions", "organizations_url": "https://api.github.com/users/cdric0701/orgs", "repos_url": "https://api.github.com/users/cdric0701/repos", "events_url": "https://api.github.com/users/cdric0701/events{/privacy}", "received_events_url": "https://api.github.com/users/cdric0701/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-08T17:34:01Z", "updated_at": "2020-04-08T20:04:24Z", "closed_at": "2020-04-08T18:38:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to download a file that is contained in an **article** tag.\r\n\r\nThe one I want to target is the second which contains the phrase **'File Import 2 *'** and whose file is located in the footer of the item (article) ahref.\r\n\r\nHow can I make my crawler download only this href element?\r\n![second-level](https://user-images.githubusercontent.com/6715950/78814873-ae702e00-79cf-11ea-89be-875561832349.PNG)\r\n\r\nThank you in advance for your help !", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4476", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4476/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4476/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4476/events", "html_url": "https://github.com/scrapy/scrapy/issues/4476", "id": 595359809, "node_id": "MDU6SXNzdWU1OTUzNTk4MDk=", "number": 4476, "title": "Spider processing error.", "user": {"login": "H-Petes", "id": 62174033, "node_id": "MDQ6VXNlcjYyMTc0MDMz", "avatar_url": "https://avatars2.githubusercontent.com/u/62174033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/H-Petes", "html_url": "https://github.com/H-Petes", "followers_url": "https://api.github.com/users/H-Petes/followers", "following_url": "https://api.github.com/users/H-Petes/following{/other_user}", "gists_url": "https://api.github.com/users/H-Petes/gists{/gist_id}", "starred_url": "https://api.github.com/users/H-Petes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/H-Petes/subscriptions", "organizations_url": "https://api.github.com/users/H-Petes/orgs", "repos_url": "https://api.github.com/users/H-Petes/repos", "events_url": "https://api.github.com/users/H-Petes/events{/privacy}", "received_events_url": "https://api.github.com/users/H-Petes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-06T19:12:23Z", "updated_at": "2020-04-06T19:21:25Z", "closed_at": "2020-04-06T19:21:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "(scraping_tutorial) C:\\Users\\Pete\\PycharmProjects\\scraping_tutorial\\quotetutorial>scrapy crawl quotes\r\n2020-04-06 22:10:42 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: quotetutorial)\r\n2020-04-06 22:10:42 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.6 (default, Jan  8 2020, 20:2\r\n3:39) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Windows-10-10.0.17763-SP0\r\n2020-04-06 22:10:42 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'quotetutorial', 'NEWSPIDER_MODULE': 'quotetutorial.spiders', 'SPIDER_MODULES': ['quotetutorial.spiders']}\r\n2020-04-06 22:10:42 [scrapy.extensions.telnet] INFO: Telnet Password: 1ace5e11f19c6f62\r\n2020-04-06 22:10:42 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2020-04-06 22:10:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2020-04-06 22:10:43 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2020-04-06 22:10:44 [scrapy.middleware] INFO: Enabled item pipelines:\r\n['quotetutorial.pipelines.QuotetutorialPipeline']\r\n2020-04-06 22:10:44 [scrapy.core.engine] INFO: Spider opened\r\n2020-04-06 22:10:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-04-06 22:10:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2020-04-06 22:10:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: None)\r\n2020-04-06 22:10:46 [scrapy.core.scraper] ERROR: Error processing {'author': [],\r\n 'tag': [],\r\n 'title': ['\u201cA day without sunshine is like, you know, night.\u201d']}\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Pete\\Anaconda3\\envs\\scraping_tutorial\\lib\\site-packages\\twisted\\internet\\defer.py\", line 654, in _runCallbacks\r\n    current.result = callback(current.result, *args, **kw)\r\n  File \"C:\\Users\\Pete\\PycharmProjects\\scraping_tutorial\\quotetutorial\\quotetutorial\\pipelines.py\", line 38, in process_item\r\n    self.store_db(item)\r\n  File \"C:\\Users\\Pete\\PycharmProjects\\scraping_tutorial\\quotetutorial\\quotetutorial\\pipelines.py\", line 46, in store_db\r\n    item['author'][0],\r\nIndexError: list index out of range\r\n2020-04-06 22:10:46 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2020-04-06 22:10:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 218,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 2342,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2020, 4, 6, 19, 10, 46, 25869),\r\n 'log_count/DEBUG': 1,\r\n 'log_count/ERROR': 1,\r\n 'log_count/INFO': 9,\r\n 'response_received_count': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2020, 4, 6, 19, 10, 44, 695033)}\r\n2020-04-06 22:10:46 [scrapy.core.engine] INFO: Spider closed (finished)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4475", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4475/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4475/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4475/events", "html_url": "https://github.com/scrapy/scrapy/issues/4475", "id": 594729370, "node_id": "MDU6SXNzdWU1OTQ3MjkzNzA=", "number": 4475, "title": "The documentation cannot be built with Sphinx 3", "user": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}, {"id": 839225636, "node_id": "MDU6TGFiZWw4MzkyMjU2MzY=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/upstream%20issue", "name": "upstream issue", "color": "c2e0c6", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/scrapy/scrapy/milestones/27", "html_url": "https://github.com/scrapy/scrapy/milestone/27", "labels_url": "https://api.github.com/repos/scrapy/scrapy/milestones/27/labels", "id": 5102632, "node_id": "MDk6TWlsZXN0b25lNTEwMjYzMg==", "number": 27, "title": "v2.1", "description": "", "creator": {"login": "Gallaecio", "id": 705211, "node_id": "MDQ6VXNlcjcwNTIxMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/705211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gallaecio", "html_url": "https://github.com/Gallaecio", "followers_url": "https://api.github.com/users/Gallaecio/followers", "following_url": "https://api.github.com/users/Gallaecio/following{/other_user}", "gists_url": "https://api.github.com/users/Gallaecio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gallaecio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gallaecio/subscriptions", "organizations_url": "https://api.github.com/users/Gallaecio/orgs", "repos_url": "https://api.github.com/users/Gallaecio/repos", "events_url": "https://api.github.com/users/Gallaecio/events{/privacy}", "received_events_url": "https://api.github.com/users/Gallaecio/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 5, "state": "closed", "created_at": "2020-02-13T13:01:05Z", "updated_at": "2020-05-03T22:41:02Z", "due_on": "2020-04-24T07:00:00Z", "closed_at": "2020-05-03T22:41:02Z"}, "comments": 5, "created_at": "2020-04-06T00:29:27Z", "updated_at": "2020-04-15T17:42:50Z", "closed_at": "2020-04-15T17:42:50Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4473", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4473/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4473/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4473/events", "html_url": "https://github.com/scrapy/scrapy/issues/4473", "id": 594361254, "node_id": "MDU6SXNzdWU1OTQzNjEyNTQ=", "number": 4473, "title": "responsetypes.py ResponseTypes.from_headers typo as 'Content-type'", "user": {"login": "eric-et", "id": 48991139, "node_id": "MDQ6VXNlcjQ4OTkxMTM5", "avatar_url": "https://avatars2.githubusercontent.com/u/48991139?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eric-et", "html_url": "https://github.com/eric-et", "followers_url": "https://api.github.com/users/eric-et/followers", "following_url": "https://api.github.com/users/eric-et/following{/other_user}", "gists_url": "https://api.github.com/users/eric-et/gists{/gist_id}", "starred_url": "https://api.github.com/users/eric-et/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eric-et/subscriptions", "organizations_url": "https://api.github.com/users/eric-et/orgs", "repos_url": "https://api.github.com/users/eric-et/repos", "events_url": "https://api.github.com/users/eric-et/events{/privacy}", "received_events_url": "https://api.github.com/users/eric-et/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}, {"id": 80417179, "node_id": "MDU6TGFiZWw4MDQxNzE3OQ==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/good%20first%20issue", "name": "good first issue", "color": "bfe5bf", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-05T09:46:31Z", "updated_at": "2020-04-15T12:24:11Z", "closed_at": "2020-04-15T12:24:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Shall this line:\r\n\r\nhttps://github.com/scrapy/scrapy/blob/8845773d44329194ee73fa4985a94e768bf664e7/scrapy/responsetypes.py#L74\r\n\r\nbe changed to below?\r\n`\r\ncontent_type=headers[b'Content-Type'],\r\n`\r\nI guess this typo is with all the versions until today.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4470", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4470/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4470/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4470/events", "html_url": "https://github.com/scrapy/scrapy/issues/4470", "id": 593716187, "node_id": "MDU6SXNzdWU1OTM3MTYxODc=", "number": 4470, "title": "how to close spider immediately", "user": {"login": "ggqshr", "id": 36805478, "node_id": "MDQ6VXNlcjM2ODA1NDc4", "avatar_url": "https://avatars1.githubusercontent.com/u/36805478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ggqshr", "html_url": "https://github.com/ggqshr", "followers_url": "https://api.github.com/users/ggqshr/followers", "following_url": "https://api.github.com/users/ggqshr/following{/other_user}", "gists_url": "https://api.github.com/users/ggqshr/gists{/gist_id}", "starred_url": "https://api.github.com/users/ggqshr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ggqshr/subscriptions", "organizations_url": "https://api.github.com/users/ggqshr/orgs", "repos_url": "https://api.github.com/users/ggqshr/repos", "events_url": "https://api.github.com/users/ggqshr/events{/privacy}", "received_events_url": "https://api.github.com/users/ggqshr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-04T01:37:30Z", "updated_at": "2020-04-05T16:30:24Z", "closed_at": "2020-04-05T16:30:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "when i writing spider use some ip proxy ,  i want to close spider immediately when proxy ip resources ran out(by raise ReachDayMax Exception),now when catch the ReachDayMax Exception i just throw the CloseSpider exception in the spider callback, but i dont see the spider close , some time the scrapy will hang-up and dont shutdown immediately,\r\nso i want to ask how to do it.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4453", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4453/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4453/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4453/events", "html_url": "https://github.com/scrapy/scrapy/issues/4453", "id": 587454666, "node_id": "MDU6SXNzdWU1ODc0NTQ2NjY=", "number": 4453, "title": "Depreciation warnings when running tox ", "user": {"login": "joybh98", "id": 35984969, "node_id": "MDQ6VXNlcjM1OTg0OTY5", "avatar_url": "https://avatars1.githubusercontent.com/u/35984969?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joybh98", "html_url": "https://github.com/joybh98", "followers_url": "https://api.github.com/users/joybh98/followers", "following_url": "https://api.github.com/users/joybh98/following{/other_user}", "gists_url": "https://api.github.com/users/joybh98/gists{/gist_id}", "starred_url": "https://api.github.com/users/joybh98/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joybh98/subscriptions", "organizations_url": "https://api.github.com/users/joybh98/orgs", "repos_url": "https://api.github.com/users/joybh98/repos", "events_url": "https://api.github.com/users/joybh98/events{/privacy}", "received_events_url": "https://api.github.com/users/joybh98/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-03-25T05:35:35Z", "updated_at": "2020-03-27T00:55:23Z", "closed_at": "2020-03-27T00:55:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nWhen testing with `tox` there is a long `warning summary`, which show a various deprecation warnings.\r\n\r\n![Screenshot from 2020-03-25 11-00-19](https://user-images.githubusercontent.com/35984969/77504982-dc118f00-6e87-11ea-80a1-4f983af15ce3.png)\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n1. Navigate to Scrapy directory\r\n2. Run tox\r\n\r\n**Expected behavior:**\r\n -\r\n\r\n**Actual behavior:** \r\n\r\n-\r\n\r\n### Versions\r\n\r\nScrapy       : 2.0.0\r\nlxml         : 4.5.0.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.5.2\r\nw3lib        : 1.21.0\r\nTwisted      : 20.3.0\r\nPython       : 3.6.9 (default, Nov  7 2019, 10:44:02) - [GCC 8.3.0]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019)\r\ncryptography : 2.8\r\nPlatform     : Linux-5.3.0-42-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\n\r\n### Additional context\r\n\r\nI am using python 3.6.9, I don't know if that's what causes these, but I thought I should let everyone know as I think this is a refactoring issue.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4447", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4447/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4447/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4447/events", "html_url": "https://github.com/scrapy/scrapy/issues/4447", "id": 584910529, "node_id": "MDU6SXNzdWU1ODQ5MTA1Mjk=", "number": 4447, "title": "zope.interface 5.0.0 unsupported", "user": {"login": "wRAR", "id": 241039, "node_id": "MDQ6VXNlcjI0MTAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/241039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wRAR", "html_url": "https://github.com/wRAR", "followers_url": "https://api.github.com/users/wRAR/followers", "following_url": "https://api.github.com/users/wRAR/following{/other_user}", "gists_url": "https://api.github.com/users/wRAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/wRAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wRAR/subscriptions", "organizations_url": "https://api.github.com/users/wRAR/orgs", "repos_url": "https://api.github.com/users/wRAR/repos", "events_url": "https://api.github.com/users/wRAR/events{/privacy}", "received_events_url": "https://api.github.com/users/wRAR/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-20T08:20:46Z", "updated_at": "2020-03-20T18:18:43Z", "closed_at": "2020-03-20T18:18:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "zope.interface 5.0.0 was released yesterday and with it some of our tests fail: https://travis-ci.org/github/scrapy/scrapy/jobs/664648176 (actually it's just one test, `CrawlerRunnerTestCase.test_spider_manager_verify_interface`).\r\n\r\nI don't know if it's just a problem with that test or something in Scrapy is now broken too. \r\n\r\nChangelog: https://zopeinterface.readthedocs.io/en/latest/changes.html", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4444", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4444/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4444/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4444/events", "html_url": "https://github.com/scrapy/scrapy/issues/4444", "id": 584682674, "node_id": "MDU6SXNzdWU1ODQ2ODI2NzQ=", "number": 4444, "title": "Update redirect link from Python 2 docs to Python 3 docs", "user": {"login": "adityaa30", "id": 37837542, "node_id": "MDQ6VXNlcjM3ODM3NTQy", "avatar_url": "https://avatars2.githubusercontent.com/u/37837542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adityaa30", "html_url": "https://github.com/adityaa30", "followers_url": "https://api.github.com/users/adityaa30/followers", "following_url": "https://api.github.com/users/adityaa30/following{/other_user}", "gists_url": "https://api.github.com/users/adityaa30/gists{/gist_id}", "starred_url": "https://api.github.com/users/adityaa30/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adityaa30/subscriptions", "organizations_url": "https://api.github.com/users/adityaa30/orgs", "repos_url": "https://api.github.com/users/adityaa30/repos", "events_url": "https://api.github.com/users/adityaa30/events{/privacy}", "received_events_url": "https://api.github.com/users/adityaa30/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}, {"id": 183224248, "node_id": "MDU6TGFiZWwxODMyMjQyNDg=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-19T20:31:51Z", "updated_at": "2020-04-15T18:13:56Z", "closed_at": "2020-04-15T18:13:56Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\nThe documentation has many redirect links to `https://docs.python.org/2/*`. As Scrapy now supports Python 3.5+ I think they should be changed to `https://docs.python.org/3/*` respectively.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Go to [JsonItemExported](https://docs.scrapy.org/en/latest/topics/exporters.html#jsonitemexporter) and click on `JSONEncoder`\r\n2. You will be redirected to [Python 2](https://docs.python.org/2/library/json.html#json.JSONEncoder) docs.\r\n\r\nAbove steps can be repeated for many other pages in Scrapy docs.\r\n\r\n**Expected behavior:** Redirected to latest Python 3 docs\r\n\r\n**Actual behavior:** Redirected to Python 2 docs\r\n\r\n**Reproduces how often:** 100%\r\n\r\n### Versions\r\n\r\nINFO:scrapy.utils.log:Scrapy 2.0.0 started (bot: scrapybot)\r\nINFO:scrapy.utils.log:Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.6.10 (default, Feb 11 2020, 21:11:31) - [GCC 7.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Linux-5.3.0-40-generic-x86_64-with-debian-buster-sid\r\nDEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\r\nScrapy       : 2.0.0\r\nlxml         : 4.4.1.0\r\nlibxml2      : 2.9.9\r\ncssselect    : 1.1.0\r\nparsel       : 1.5.2\r\nw3lib        : 1.21.0\r\nTwisted      : 19.10.0\r\nPython       : 3.6.10 (default, Feb 11 2020, 21:11:31) - [GCC 7.4.0]\r\npyOpenSSL    : 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019)\r\ncryptography : 2.8\r\nPlatform     : Linux-5.3.0-40-generic-x86_64-with-debian-buster-sid\r\n\r\n### Additional context\r\n\r\nI am willing to fix this issue, waiting for an approval.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4443", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4443/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4443/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4443/events", "html_url": "https://github.com/scrapy/scrapy/issues/4443", "id": 583939235, "node_id": "MDU6SXNzdWU1ODM5MzkyMzU=", "number": 4443, "title": "Remove defer.returnValue calls", "user": {"login": "wRAR", "id": 241039, "node_id": "MDQ6VXNlcjI0MTAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/241039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wRAR", "html_url": "https://github.com/wRAR", "followers_url": "https://api.github.com/users/wRAR/followers", "following_url": "https://api.github.com/users/wRAR/following{/other_user}", "gists_url": "https://api.github.com/users/wRAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/wRAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wRAR/subscriptions", "organizations_url": "https://api.github.com/users/wRAR/orgs", "repos_url": "https://api.github.com/users/wRAR/repos", "events_url": "https://api.github.com/users/wRAR/events{/privacy}", "received_events_url": "https://api.github.com/users/wRAR/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545314542, "node_id": "MDU6TGFiZWw1NDUzMTQ1NDI=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/cleanup", "name": "cleanup", "color": "1d76db", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-03-18T18:58:38Z", "updated_at": "2020-04-15T17:45:53Z", "closed_at": "2020-04-15T17:45:53Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://twisted.readthedocs.io/en/latest/core/howto/defer-intro.html#inline-callbacks-using-yield says \"On Python 3, instead of writing returnValue(json.loads(responseBody)) you can instead write return json.loads(responseBody). This can be a significant readability advantage, but unfortunately if you need compatibility with Python 2, this isn\u2019t an option.\". ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4428", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4428/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4428/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4428/events", "html_url": "https://github.com/scrapy/scrapy/issues/4428", "id": 581591610, "node_id": "MDU6SXNzdWU1ODE1OTE2MTA=", "number": 4428, "title": "Export scrapy data from website into google spreadsheet", "user": {"login": "clairehuang77777", "id": 25604254, "node_id": "MDQ6VXNlcjI1NjA0MjU0", "avatar_url": "https://avatars2.githubusercontent.com/u/25604254?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clairehuang77777", "html_url": "https://github.com/clairehuang77777", "followers_url": "https://api.github.com/users/clairehuang77777/followers", "following_url": "https://api.github.com/users/clairehuang77777/following{/other_user}", "gists_url": "https://api.github.com/users/clairehuang77777/gists{/gist_id}", "starred_url": "https://api.github.com/users/clairehuang77777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clairehuang77777/subscriptions", "organizations_url": "https://api.github.com/users/clairehuang77777/orgs", "repos_url": "https://api.github.com/users/clairehuang77777/repos", "events_url": "https://api.github.com/users/clairehuang77777/events{/privacy}", "received_events_url": "https://api.github.com/users/clairehuang77777/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-15T09:10:00Z", "updated_at": "2020-03-15T20:04:35Z", "closed_at": "2020-03-15T20:04:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am willing to scrap a website for some information. The difficult part is, i want to export all the data in to the google sheets and make the crawler run after some specific intervals. I 'll be using scrapy for this purpose. Any suggestions on how can i do this \r\n(by making custom pipeline or any other way as i don't have much experience in writing custom pipelines)\r\n\r\nhere is the code i have now.\r\n````\r\n# -*- coding: utf-8 -*-\r\nimport scrapy\r\n# from scrapy.exceptions import CloseSpider\r\nfrom myFirstScrapyProject.items import MyfirstscrapyprojectItem\r\n\r\nclass PttSpider(scrapy.Spider):\r\n    count_page = 1\r\n    name = 'ptt'\r\n    allowed_domains = ['www.ptt.cc/']\r\n    start_urls = ['https://www.ptt.cc/bbs/e-shopping/search?q=%E8%9D%A6%E7%9A%AE']+['https://www.ptt.cc/bbs/e-seller/search?q=%E8%9D%A6%E7%9A%AE']\r\n    # start_urls = ['https://www.ptt.cc/bbs/e-shopping/index.html']\r\n\r\n    def parse(self, response):\r\n        items = MyfirstscrapyprojectItem()\r\n        for q in response.css('div.r-ent'):\r\n            items['push']=q.css('div.nrec > span.h1::text').extract_first()\r\n            items['title']=q.css('div.title > a::text').extract_first()\r\n            items['href']=q.css('div.title> a::attr(href)').extract_first()\r\n            items['date']=q.css('div.meta > div.date ::text').extract_first()\r\n            items['author']=q.css('div.meta > div.author ::text').extract_first()\r\n            yield(items)\r\n```\r\n\r\n```\r\nimport scrapy\r\nclass MyfirstscrapyprojectItem(scrapy.Item):\r\n    # define the fields for your item here like:\r\n    # name = scrapy.Field()\r\n    push = scrapy.Field()\r\n    title = scrapy.Field()\r\n    href = scrapy.Field()\r\n    date = scrapy.Field()\r\n    author = scrapy.Field()\r\n\r\n```\r\n\r\n```\r\nclass MyfirstscrapyprojectPipeline(object):\r\n    def process_item(self, item, spider):\r\n        return item\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4427", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4427/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4427/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4427/events", "html_url": "https://github.com/scrapy/scrapy/issues/4427", "id": 581513249, "node_id": "MDU6SXNzdWU1ODE1MTMyNDk=", "number": 4427, "title": "Error shows when running Scrapy crawl", "user": {"login": "clairehuang77777", "id": 25604254, "node_id": "MDQ6VXNlcjI1NjA0MjU0", "avatar_url": "https://avatars2.githubusercontent.com/u/25604254?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clairehuang77777", "html_url": "https://github.com/clairehuang77777", "followers_url": "https://api.github.com/users/clairehuang77777/followers", "following_url": "https://api.github.com/users/clairehuang77777/following{/other_user}", "gists_url": "https://api.github.com/users/clairehuang77777/gists{/gist_id}", "starred_url": "https://api.github.com/users/clairehuang77777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clairehuang77777/subscriptions", "organizations_url": "https://api.github.com/users/clairehuang77777/orgs", "repos_url": "https://api.github.com/users/clairehuang77777/repos", "events_url": "https://api.github.com/users/clairehuang77777/events{/privacy}", "received_events_url": "https://api.github.com/users/clairehuang77777/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-15T04:46:36Z", "updated_at": "2020-03-15T08:41:04Z", "closed_at": "2020-03-15T08:41:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "The error shows when running scrapy crawl\r\n\r\n> Traceback (most recent call last):\r\n>   File \"c:\\users\\claire.h\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 654, in _runCallbacks\r\n>     current.result = callback(current.result, *args, **kw)\r\n>   File \"c:\\users\\claire.h\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\scrapy\\utils\\defer.py\", line 154, in f\r\n>     return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))\r\n>   File \"C:\\Users\\claire.h\\Desktop\\python\\myFirstScrapyProject\\myFirstScrapyProject\\pipelines.py\", line 11, in process_item\r\n>     item['push']=int(item['push'])\r\n>   File \"c:\\users\\claire.h\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\scrapy\\item.py\", line 83, in __getitem__\r\n>     return self._values[key]\r\n> KeyError: 'push'\r\n\r\n\r\nthis is the code i try to run:\r\n```\r\n# -*- coding: utf-8 -*-\r\nimport scrapy\r\n# from scrapy.exceptions import CloseSpider\r\nfrom ..items import MyfirstscrapyprojectItem\r\n\r\nclass PttSpider(scrapy.Spider):\r\n    count_page = 1\r\n    name = 'ptt'\r\n    allowed_domains = ['www.ptt.cc/']\r\n    start_urls = ['https://www.ptt.cc/bbs/e-shopping/search?q=%E8%9D%A6%E7%9A%AE']+['https://www.ptt.cc/bbs/e-seller/search?q=%E8%9D%A6%E7%9A%AE']\r\n    # start_urls = ['https://www.ptt.cc/bbs/e-shopping/index.html']\r\n\r\n    def parse(self, response):\r\n        items = MyfirstscrapyprojectItem()\r\n        for q in response.css('div.r-ent'):\r\n            items['push']:q.css('div.nrec > span.h1::text').extract_first()\r\n            items['title']:q.css('div.title > a::text').extract_first()\r\n            items['href']:q.css('div.title> a::attr(href)').extract_first()\r\n            items['date']:q.css('div.meta > div.date ::text').extract_first()\r\n            items['author']:q.css('div.meta > div.author ::text').extract_first()\r\n            yield(items)\r\n\r\n```\r\n```\r\nclass MyfirstscrapyprojectItem(scrapy.Item):\r\n    # define the fields for your item here like:\r\n    # name = scrapy.Field()\r\n    #\u5b9a\u7fa9field\r\n    push = scrapy.Field()\r\n    title = scrapy.Field()\r\n    href = scrapy.Field()\r\n    date = scrapy.Field()\r\n    author = scrapy.Field()\r\n    pass\r\n```\r\n`class MyfirstscrapyprojectPipeline(object):\r\n    def process_item(self, item, spider):\r\n        item['push']= int(item['push'])\r\n        return item\r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4425", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4425/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4425/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4425/events", "html_url": "https://github.com/scrapy/scrapy/issues/4425", "id": 581094018, "node_id": "MDU6SXNzdWU1ODEwOTQwMTg=", "number": 4425, "title": "Error shows when using pip install scrapy command", "user": {"login": "clairehuang77777", "id": 25604254, "node_id": "MDQ6VXNlcjI1NjA0MjU0", "avatar_url": "https://avatars2.githubusercontent.com/u/25604254?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clairehuang77777", "html_url": "https://github.com/clairehuang77777", "followers_url": "https://api.github.com/users/clairehuang77777/followers", "following_url": "https://api.github.com/users/clairehuang77777/following{/other_user}", "gists_url": "https://api.github.com/users/clairehuang77777/gists{/gist_id}", "starred_url": "https://api.github.com/users/clairehuang77777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clairehuang77777/subscriptions", "organizations_url": "https://api.github.com/users/clairehuang77777/orgs", "repos_url": "https://api.github.com/users/clairehuang77777/repos", "events_url": "https://api.github.com/users/clairehuang77777/events{/privacy}", "received_events_url": "https://api.github.com/users/clairehuang77777/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-14T07:54:05Z", "updated_at": "2020-03-14T10:24:17Z", "closed_at": "2020-03-14T10:24:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I saw these error message when using pip install scrapy command....\r\ndon't know how to do...\r\n\r\n\r\n ERROR: Command errored out with exit status 1:\r\n     command: 'c:\\users\\claire.h\\appdata\\local\\programs\\python\\python38-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\claire.h\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2ywmump1\\\\Twisted\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\claire.h\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2ywmump1\\\\Twisted\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\claire.h\\AppData\\Local\\Temp\\pip-record-qwt9ehfv\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\claire.h\\appdata\\local\\programs\\python\\python38-32\\Include\\Twisted'\r\n         cwd: C:\\Users\\claire.h\\AppData\\Local\\Temp\\pip-install-2ywmump1\\Twisted\\\r\n    Complete output (946 lines):\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build\\lib.win32-3.8\r\n    creating build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\copyright.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\plugin.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\_version.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\__init__.py -> build\\lib.win32-3.8\\twisted\r\n    copying src\\twisted\\__main__.py -> build\\lib.win32-3.8\\twisted\r\n    creating build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\app.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\internet.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\reactors.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\service.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\strports.py -> build\\lib.win32-3.8\\twisted\\application\r\n    copying src\\twisted\\application\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\r\n    creating build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\avatar.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\checkers.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\endpoints.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\error.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\interfaces.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\ls.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\manhole.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\manhole_ssh.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\manhole_tap.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\mixin.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\recvline.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\stdio.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\tap.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\telnet.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\ttymodes.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\unix.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    copying src\\twisted\\conch\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\r\n    creating build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\checkers.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\credentials.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\error.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\portal.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\strcred.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\_digest.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    copying src\\twisted\\cred\\__init__.py -> build\\lib.win32-3.8\\twisted\\cred\r\n    creating build\\lib.win32-3.8\\twisted\\enterprise\r\n    copying src\\twisted\\enterprise\\adbapi.py -> build\\lib.win32-3.8\\twisted\\enterprise\r\n    copying src\\twisted\\enterprise\\__init__.py -> build\\lib.win32-3.8\\twisted\\enterprise\r\n    creating build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\abstract.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\address.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\asyncioreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\base.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\cfreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\default.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\defer.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\endpoints.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\epollreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\error.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\fdesc.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\gireactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\glib2reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\gtk2reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\gtk3reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\inotify.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\interfaces.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\kqreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\main.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\pollreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\posixbase.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\process.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\protocol.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\pyuisupport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\reactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\selectreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\serialport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\ssl.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\stdio.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\task.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\tcp.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\testing.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\threads.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\tksupport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\udp.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\unix.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\utils.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\win32eventreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\wxreactor.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\wxsupport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_baseprocess.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_dumbwin32proc.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_glibbase.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_idna.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_newtls.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_pollingfile.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_posixserialport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_posixstdio.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_producer_helpers.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_resolver.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_signals.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_sslverify.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_threadedselect.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_win32serialport.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\_win32stdio.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    copying src\\twisted\\internet\\__init__.py -> build\\lib.win32-3.8\\twisted\\internet\r\n    creating build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_buffer.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_capture.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_file.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_filter.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_flatten.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_format.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_global.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_io.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_json.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_legacy.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_levels.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_logger.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_observer.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_stdlib.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\_util.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    copying src\\twisted\\logger\\__init__.py -> build\\lib.win32-3.8\\twisted\\logger\r\n    creating build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\imap4.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\interfaces.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\pop3.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\pop3client.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\protocols.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\relay.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\smtp.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\_cred.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\_except.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    copying src\\twisted\\mail\\__init__.py -> build\\lib.win32-3.8\\twisted\\mail\r\n    creating build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\authority.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\cache.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\client.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\common.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\dns.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\error.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\hosts.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\resolve.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\root.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\secondary.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\server.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\srvconnect.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\tap.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\_rfc1982.py -> build\\lib.win32-3.8\\twisted\\names\r\n    copying src\\twisted\\names\\__init__.py -> build\\lib.win32-3.8\\twisted\\names\r\n    creating build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\ethernet.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\ip.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\raw.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\rawudp.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\testing.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\tuntap.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    copying src\\twisted\\pair\\__init__.py -> build\\lib.win32-3.8\\twisted\\pair\r\n    creating build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\aot.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\crefutil.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\dirdbm.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\sob.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\styles.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    copying src\\twisted\\persisted\\__init__.py -> build\\lib.win32-3.8\\twisted\\persisted\r\n    creating build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_anonymous.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_file.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_memory.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_sshkeys.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\cred_unix.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_conch.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_core.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_ftp.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_inet.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_names.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_portforward.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_reactors.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_runner.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_socks.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_trial.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_web.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\twisted_words.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    copying src\\twisted\\plugins\\__init__.py -> build\\lib.win32-3.8\\twisted\\plugins\r\n    creating build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\base.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\ipositioning.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\nmea.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\_sentence.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    copying src\\twisted\\positioning\\__init__.py -> build\\lib.win32-3.8\\twisted\\positioning\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\amp.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\basic.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\dict.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\finger.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\ftp.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\htb.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\ident.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\loopback.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\memcache.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\pcp.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\policies.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\portforward.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\postfix.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\sip.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\socks.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\stateful.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\tls.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\wire.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    copying src\\twisted\\protocols\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\r\n    creating build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\compat.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\components.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\constants.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\context.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\deprecate.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\failure.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\fakepwd.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\filepath.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\formmethod.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\htmlizer.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\lockfile.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\log.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\logfile.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\modules.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\monkey.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\procutils.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\randbytes.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\rebuild.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\reflect.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\release.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\roots.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\runtime.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\sendmsg.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\shortcut.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\syslog.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\systemd.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\text.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\threadable.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\threadpool.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\url.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\urlpath.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\usage.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\util.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\versions.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\win32.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\zippath.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\zipstream.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_appdirs.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_inotify.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_oldstyle.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_release.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_setup.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_shellcomp.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_textattributes.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_tzhelper.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\_url.py -> build\\lib.win32-3.8\\twisted\\python\r\n    copying src\\twisted\\python\\__init__.py -> build\\lib.win32-3.8\\twisted\\python\r\n    creating build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\inetd.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\inetdconf.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\inetdtap.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\procmon.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\procmontap.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    copying src\\twisted\\runner\\__init__.py -> build\\lib.win32-3.8\\twisted\\runner\r\n    creating build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\htmlizer.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\trial.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\twistd.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\_twistd_unix.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\_twistw.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    copying src\\twisted\\scripts\\__init__.py -> build\\lib.win32-3.8\\twisted\\scripts\r\n    creating build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\banana.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\flavors.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\interfaces.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\jelly.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\pb.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\publish.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\util.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    copying src\\twisted\\spread\\__init__.py -> build\\lib.win32-3.8\\twisted\\spread\r\n    creating build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\ftp.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\portforward.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\socks.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    copying src\\twisted\\tap\\__init__.py -> build\\lib.win32-3.8\\twisted\\tap\r\n    creating build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\crash_test_dummy.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\iosim.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\mock_win32process.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\myrebuilder1.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\myrebuilder2.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\plugin_basic.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\plugin_extra1.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\plugin_extra2.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_cmdline.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_echoer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_fds.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_getargv.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_getenv.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_linger.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_reader.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_signal.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_stdinreader.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_tester.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_tty.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\process_twisted.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\proto_helpers.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\reflect_helper_IE.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\reflect_helper_VE.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\reflect_helper_ZDE.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\ssl_helpers.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_consumer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_halfclose.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_hostpeer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_lastwrite.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_loseconn.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_producer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_write.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\stdio_test_writeseq.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\testutils.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_abstract.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_adbapi.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_amp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_application.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_compat.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_context.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_cooperator.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_defer.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_defgen.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_dict.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_dirdbm.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_error.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_factories.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_failure.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_fdesc.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_finger.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_formmethod.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ftp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ftp_options.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_htb.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ident.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_internet.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_iosim.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_iutils.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_lockfile.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_log.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_logfile.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_loopback.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_main.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_memcache.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_modules.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_monkey.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_nooldstyle.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_paths.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_pcp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_persisted.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_plugin.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_policies.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_postfix.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_process.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_protocols.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_randbytes.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_rebuild.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_reflect.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_roots.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_shortcut.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_sip.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_sob.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_socks.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_ssl.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_sslverify.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_stateful.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_stdio.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_strerror.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_strports.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_task.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_tcp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_tcp_internals.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_text.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_threadable.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_threadpool.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_threads.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_tpfile.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_twistd.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_twisted.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_udp.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_unix.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_usage.py -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\test\r\n    creating build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\itrial.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\reporter.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\runner.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\unittest.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\util.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\_asyncrunner.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\_asynctest.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\_synctest.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    copying src\\twisted\\trial\\__main__.py -> build\\lib.win32-3.8\\twisted\\trial\r\n    creating build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\client.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\demo.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\distrib.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\domhelpers.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\error.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\guard.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\html.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\http.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\http_headers.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\iweb.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\microdom.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\proxy.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\resource.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\rewrite.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\script.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\server.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\static.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\sux.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\tap.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\template.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\twcgi.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\util.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\vhost.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\wsgi.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\xmlrpc.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_element.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_flatten.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_http2.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_newclient.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_responses.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\_stan.py -> build\\lib.win32-3.8\\twisted\\web\r\n    copying src\\twisted\\web\\__init__.py -> build\\lib.win32-3.8\\twisted\\web\r\n    creating build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\ewords.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\iwords.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\service.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\tap.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\xmpproutertap.py -> build\\lib.win32-3.8\\twisted\\words\r\n    copying src\\twisted\\words\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\r\n    creating build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_convenience.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_ithreads.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_memory.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_pool.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_team.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\_threadworker.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    copying src\\twisted\\_threads\\__init__.py -> build\\lib.win32-3.8\\twisted\\_threads\r\n    creating build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\_exit.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\_pidfile.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\_runner.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    copying src\\twisted\\application\\runner\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\runner\r\n    creating build\\lib.win32-3.8\\twisted\\application\\test\r\n    copying src\\twisted\\application\\test\\test_internet.py -> build\\lib.win32-3.8\\twisted\\application\\test\r\n    copying src\\twisted\\application\\test\\test_service.py -> build\\lib.win32-3.8\\twisted\\application\\test\r\n    copying src\\twisted\\application\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\test\r\n    creating build\\lib.win32-3.8\\twisted\\application\\twist\r\n    copying src\\twisted\\application\\twist\\_options.py -> build\\lib.win32-3.8\\twisted\\application\\twist\r\n    copying src\\twisted\\application\\twist\\_twist.py -> build\\lib.win32-3.8\\twisted\\application\\twist\r\n    copying src\\twisted\\application\\twist\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\twist\r\n    creating build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\test_exit.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\test_pidfile.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\test_runner.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    copying src\\twisted\\application\\runner\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\runner\\test\r\n    creating build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    copying src\\twisted\\application\\twist\\test\\test_options.py -> build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    copying src\\twisted\\application\\twist\\test\\test_twist.py -> build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    copying src\\twisted\\application\\twist\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\application\\twist\\test\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\agent.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\connect.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\default.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\direct.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\knownhosts.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\options.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    copying src\\twisted\\conch\\client\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\client\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\helper.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\insults.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\text.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\window.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    copying src\\twisted\\conch\\insults\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\insults\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    copying src\\twisted\\conch\\openssh_compat\\factory.py -> build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    copying src\\twisted\\conch\\openssh_compat\\primes.py -> build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    copying src\\twisted\\conch\\openssh_compat\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\openssh_compat\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\cftp.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\ckeygen.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\conch.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\tkconch.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    copying src\\twisted\\conch\\scripts\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\scripts\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\address.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\agent.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\channel.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\common.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\connection.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\factory.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\filetransfer.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\forwarding.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\keys.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\service.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\session.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\sexpy.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\transport.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\userauth.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\_kex.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    copying src\\twisted\\conch\\ssh\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\ssh\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\keydata.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\loopback.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_address.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_agent.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_cftp.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_channel.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_checkers.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_ckeygen.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_conch.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_connection.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_default.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_endpoints.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_filetransfer.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_forwarding.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_helper.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_insults.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_keys.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_knownhosts.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_manhole.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_manhole_tap.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_mixin.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_openssh_compat.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_recvline.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_scripts.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_session.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_ssh.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_telnet.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_text.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_transport.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_unix.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_userauth.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\test_window.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    copying src\\twisted\\conch\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\test\r\n    creating build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    copying src\\twisted\\conch\\ui\\ansi.py -> build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    copying src\\twisted\\conch\\ui\\tkvt100.py -> build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    copying src\\twisted\\conch\\ui\\__init__.py -> build\\lib.win32-3.8\\twisted\\conch\\ui\r\n    creating build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_cramauth.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_cred.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_digestauth.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_simpleauth.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\test_strcred.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    copying src\\twisted\\cred\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\cred\\test\r\n    creating build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\abstract.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\const.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\interfaces.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\reactor.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\setup.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\tcp.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\udp.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\iocpreactor\\__init__.py -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    creating build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\connectionmixins.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\fakeendpoint.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\modulehelpers.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_cli.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_connectionlost.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_gireactornocompat.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\process_helper.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\reactormixins.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_abstract.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_address.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_asyncioreactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_base.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_baseprocess.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_core.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_coroutines.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_default.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_endpoints.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_epollreactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_error.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_fdset.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_filedescriptor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_gireactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_glibbase.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_inlinecb.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_inotify.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_iocp.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_kqueuereactor.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_main.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_newtls.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_pollingfile.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_posixbase.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_posixprocess.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_process.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_protocol.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_resolver.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_serialport.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_sigchld.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_socket.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_stdio.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_tcp.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_testing.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_threads.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_time.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_tls.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_udp.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_udp_internals.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_unix.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_win32events.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\test_win32serialport.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\_posixifaces.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\_win32ifaces.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    creating build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_buffer.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_capture.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_file.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_filter.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_flatten.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_format.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_global.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_io.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_json.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_legacy.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_levels.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_logger.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_observer.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_stdlib.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    copying src\\twisted\\logger\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\logger\\test\r\n    creating build\\lib.win32-3.8\\twisted\\mail\\scripts\r\n    copying src\\twisted\\mail\\scripts\\mailmail.py -> build\\lib.win32-3.8\\twisted\\mail\\scripts\r\n    creating build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\pop3testserver.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_imap.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_mailmail.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_pop3.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_pop3client.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\test_smtp.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\mail\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    creating build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_cache.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_client.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_common.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_dns.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_examples.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_hosts.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_names.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_resolve.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_rfc1982.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_rootresolve.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_server.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_srvconnect.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    copying src\\twisted\\names\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\names\\test\r\n    creating build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_ethernet.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_ip.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_rawudp.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\test_tuntap.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    copying src\\twisted\\pair\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\pair\\test\r\n    creating build\\lib.win32-3.8\\twisted\\persisted\\test\r\n    copying src\\twisted\\persisted\\test\\test_styles.py -> build\\lib.win32-3.8\\twisted\\persisted\\test\r\n    copying src\\twisted\\persisted\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\persisted\\test\r\n    creating build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\receiver.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\test_base.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\test_nmea.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\test_sentence.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    copying src\\twisted\\positioning\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\positioning\\test\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_exceptions.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_info.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_interfaces.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_v1parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_v2parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\_wrapper.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    copying src\\twisted\\protocols\\haproxy\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    copying src\\twisted\\protocols\\test\\test_basic.py -> build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    copying src\\twisted\\protocols\\test\\test_tls.py -> build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    copying src\\twisted\\protocols\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\\test\r\n    creating build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_v1parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_v2parser.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\test_wrapper.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    copying src\\twisted\\protocols\\haproxy\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\protocols\\haproxy\\test\r\n    creating build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\deprecatedattributes.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\modules_helpers.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\pullpipe.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_appdirs.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_components.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_constants.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_deprecate.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_dist3.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_fakepwd.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_htmlizer.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_inotify.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_release.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_runtime.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_sendmsg.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_setup.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_shellcomp.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_syslog.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_systemd.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_textattributes.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_tzhelper.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_url.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_urlpath.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_versions.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_zippath.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\test_zipstream.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\python\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    creating build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\test_inetdconf.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\test_procmon.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\test_procmontap.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    copying src\\twisted\\runner\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\runner\\test\r\n    creating build\\lib.win32-3.8\\twisted\\scripts\\test\r\n    copying src\\twisted\\scripts\\test\\test_scripts.py -> build\\lib.win32-3.8\\twisted\\scripts\\test\r\n    copying src\\twisted\\scripts\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\scripts\\test\r\n    creating build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_banana.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_jelly.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_pb.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\test_pbfailure.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    copying src\\twisted\\spread\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\spread\\test\r\n    creating build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\detests.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\erroneous.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockcustomsuite.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockcustomsuite2.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockcustomsuite3.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\mockdoctest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\moduleself.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\moduletest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\novars.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\ordertests.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\packages.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\sample.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\scripttest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\skipping.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\suppression.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_assertions.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_asyncassertions.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_deferred.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_doctest.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_keyboard.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_loader.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_log.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_output.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_plugins.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_pyunitcompat.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_reporter.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_runner.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_script.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_suppression.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_testcase.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_tests.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\test_warning.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\weird.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\trial\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    creating build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\distreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\disttrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\managercommands.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\options.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\worker.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\workercommands.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\workerreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\workertrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    copying src\\twisted\\trial\\_dist\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\r\n    creating build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_distreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_disttrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_options.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_worker.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_workerreporter.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\test_workertrial.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    copying src\\twisted\\trial\\_dist\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\trial\\_dist\\test\r\n    creating build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\injectionhelpers.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\requesthelper.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_agent.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_cgi.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_client.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_distrib.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_domhelpers.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_error.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_flatten.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_html.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_http.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_http2.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_httpauth.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_http_headers.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_newclient.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_proxy.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_resource.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_script.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_stan.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_static.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_template.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_util.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_vhost.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_web.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_webclient.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_web__responses.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_wsgi.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_xml.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\test_xmlrpc.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\_util.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    copying src\\twisted\\web\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\web\\test\r\n    creating build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\basic.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\digest.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\wrapper.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    copying src\\twisted\\web\\_auth\\__init__.py -> build\\lib.win32-3.8\\twisted\\web\\_auth\r\n    creating build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\baseaccount.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\basechat.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\basesupport.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\interfaces.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\ircsupport.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\locals.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\pbsupport.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\im\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    creating build\\lib.win32-3.8\\twisted\\words\\protocols\r\n    copying src\\twisted\\words\\protocols\\irc.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\r\n    copying src\\twisted\\words\\protocols\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\r\n    creating build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_basechat.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_basesupport.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_domish.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_irc.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_ircsupport.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_irc_service.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberclient.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbercomponent.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbererror.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberjid.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberjstrports.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbersasl.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabbersaslmechanisms.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberxmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_jabberxmppstringprep.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_service.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_tap.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xishutil.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xmpproutertap.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\test_xpath.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    copying src\\twisted\\words\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\test\r\n    creating build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\domish.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\utility.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\xmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\xpath.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\xpathparser.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    copying src\\twisted\\words\\xish\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    creating build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\client.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\component.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\error.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\ijabber.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\jid.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\jstrports.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\sasl.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\sasl_mechanisms.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\xmlstream.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\xmpp_stringprep.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    copying src\\twisted\\words\\protocols\\jabber\\__init__.py -> build\\lib.win32-3.8\\twisted\\words\\protocols\\jabber\r\n    creating build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_convenience.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_memory.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_team.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\test_threadworker.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    copying src\\twisted\\_threads\\test\\__init__.py -> build\\lib.win32-3.8\\twisted\\_threads\\test\r\n    running egg_info\r\n    writing src\\Twisted.egg-info\\PKG-INFO\r\n    writing dependency_links to src\\Twisted.egg-info\\dependency_links.txt\r\n    writing entry points to src\\Twisted.egg-info\\entry_points.txt\r\n    writing requirements to src\\Twisted.egg-info\\requires.txt\r\n    writing top-level names to src\\Twisted.egg-info\\top_level.txt\r\n    reading manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\r\n    reading manifest template 'MANIFEST.in'\r\n    warning: no previously-included files matching '*.misc' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.bugfix' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.doc' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.feature' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching '*.removal' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching 'NEWS' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching 'README' found under directory 'src\\twisted'\r\n    warning: no previously-included files matching 'newsfragments' found under directory 'src\\twisted'\r\n    warning: no previously-included files found matching 'src\\twisted\\topfiles\\CREDITS'\r\n    warning: no previously-included files found matching 'src\\twisted\\topfiles\\ChangeLog.Old'\r\n    warning: no previously-included files found matching 'pyproject.toml'\r\n    warning: no previously-included files found matching 'codecov.yml'\r\n    warning: no previously-included files found matching 'appveyor.yml'\r\n    warning: no previously-included files found matching '.coveralls.yml'\r\n    warning: no previously-included files found matching '.circleci'\r\n    warning: no previously-included files matching '*' found under directory '.circleci'\r\n    no previously-included directories found matching 'bin'\r\n    no previously-included directories found matching 'admin'\r\n    no previously-included directories found matching '.travis'\r\n    no previously-included directories found matching '.github'\r\n    warning: no previously-included files found matching 'docs\\historic\\2003'\r\n    warning: no previously-included files matching '*' found under directory 'docs\\historic\\2003'\r\n    writing manifest file 'src\\Twisted.egg-info\\SOURCES.txt'\r\n    copying src\\twisted\\python\\twisted-completion.zsh -> build\\lib.win32-3.8\\twisted\\python\r\n    creating build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\python\\_pydoctortemplates\\common.html -> build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\python\\_pydoctortemplates\\index.html -> build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\python\\_pydoctortemplates\\summary.html -> build\\lib.win32-3.8\\twisted\\python\\_pydoctortemplates\r\n    copying src\\twisted\\test\\cert.pem.no_trailing_newline -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\key.pem.no_trailing_newline -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\server.pem -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\test\\test_defer.py.3only -> build\\lib.win32-3.8\\twisted\\test\r\n    copying src\\twisted\\internet\\iocpreactor\\notes.txt -> build\\lib.win32-3.8\\twisted\\internet\\iocpreactor\r\n    copying src\\twisted\\internet\\test\\_awaittests.py.3only -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    copying src\\twisted\\internet\\test\\_yieldfromtests.py.3only -> build\\lib.win32-3.8\\twisted\\internet\\test\r\n    creating build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\chain.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\not-a-certificate -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing1.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2-duplicate.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\internet\\test\\fake_CAs\\thing2.pem -> build\\lib.win32-3.8\\twisted\\internet\\test\\fake_CAs\r\n    copying src\\twisted\\mail\\test\\rfc822.message -> build\\lib.win32-3.8\\twisted\\mail\\test\r\n    copying src\\twisted\\python\\test\\_deprecatetests.py.3only -> build\\lib.win32-3.8\\twisted\\python\\test\r\n    copying src\\twisted\\trial\\test\\_assertiontests.py.3only -> build\\lib.win32-3.8\\twisted\\trial\\test\r\n    copying src\\twisted\\words\\im\\instancemessenger.glade -> build\\lib.win32-3.8\\twisted\\words\\im\r\n    copying src\\twisted\\words\\xish\\xpathparser.g -> build\\lib.win32-3.8\\twisted\\words\\xish\r\n    running build_ext\r\n    building 'twisted.test.raiser' extension\r\n    error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: 'c:\\users\\claire.h\\appdata\\local\\programs\\python\\python38-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\claire.h\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2ywmump1\\\\Twisted\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\claire.h\\\\AppData\\\\Local\\\\Temp\\\\pip-install-2ywmump1\\\\Twisted\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\claire.h\\AppData\\Local\\Temp\\pip-record-qwt9ehfv\\install-record.txt' --single-version-externally-managed --compile --install-headers 'c:\\users\\claire.h\\appdata\\local\\programs\\python\\python38-32\\Include\\Twisted' Check the logs for full command output.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4423", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4423/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4423/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4423/events", "html_url": "https://github.com/scrapy/scrapy/issues/4423", "id": 580645367, "node_id": "MDU6SXNzdWU1ODA2NDUzNjc=", "number": 4423, "title": "scrapy hangs requesting http://www.hpe.com/", "user": {"login": "blushingpenguin", "id": 357947, "node_id": "MDQ6VXNlcjM1Nzk0Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/357947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blushingpenguin", "html_url": "https://github.com/blushingpenguin", "followers_url": "https://api.github.com/users/blushingpenguin/followers", "following_url": "https://api.github.com/users/blushingpenguin/following{/other_user}", "gists_url": "https://api.github.com/users/blushingpenguin/gists{/gist_id}", "starred_url": "https://api.github.com/users/blushingpenguin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blushingpenguin/subscriptions", "organizations_url": "https://api.github.com/users/blushingpenguin/orgs", "repos_url": "https://api.github.com/users/blushingpenguin/repos", "events_url": "https://api.github.com/users/blushingpenguin/events{/privacy}", "received_events_url": "https://api.github.com/users/blushingpenguin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-13T14:42:27Z", "updated_at": "2020-04-24T10:48:20Z", "closed_at": "2020-04-24T10:48:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\nScrapy hangs requesting a couple of sites, namely:\r\n\r\nhttp://www.hpe.com/\r\nand\r\nhttp://viz.ai/\r\n\r\nThese work fine in a browser.  Eventually the connection times out (after 3 minutes, which I assume is a default)\r\n\r\n### Steps to Reproduce\r\n\r\nscrapy shell http://www.hpe.com/\r\nscrapy shell http://viz.ai/\r\n\r\n**Expected behavior:** [What you expect to happen]\r\n\r\nThe pages should load as in a browser\r\n\r\n**Actual behavior:** [What actually happens]\r\n\r\nThe requests time out after 3 minutes\r\n\r\n**Reproduces how often:** [What percentage of the time does it reproduce?]\r\n\r\nEvery time\r\n\r\n### Versions\r\n\r\nPlease paste here the output of executing `scrapy version --verbose` in the command line.\r\nScrapy       : 2.0.0\r\nlxml         : 4.5.0.0\r\nlibxml2      : 2.9.10\r\ncssselect    : 1.1.0\r\nparsel       : 1.5.2\r\nw3lib        : 1.21.0\r\nTwisted      : 17.9.0\r\nPython       : 3.6.9 (default, Nov  7 2019, 10:44:02) - [GCC 8.3.0]\r\npyOpenSSL    : 17.5.0 (OpenSSL 1.1.1  11 Sep 2018)\r\ncryptography : 2.1.4\r\nPlatform     : Linux-4.15.0-1057-aws-x86_64-with-Ubuntu-18.04-bionic\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4419", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4419/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4419/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4419/events", "html_url": "https://github.com/scrapy/scrapy/issues/4419", "id": 579532014, "node_id": "MDU6SXNzdWU1Nzk1MzIwMTQ=", "number": 4419, "title": "Scrapy Splash Screenshot and calling coroutine objects", "user": {"login": "nicholas-mischke", "id": 13559470, "node_id": "MDQ6VXNlcjEzNTU5NDcw", "avatar_url": "https://avatars1.githubusercontent.com/u/13559470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nicholas-mischke", "html_url": "https://github.com/nicholas-mischke", "followers_url": "https://api.github.com/users/nicholas-mischke/followers", "following_url": "https://api.github.com/users/nicholas-mischke/following{/other_user}", "gists_url": "https://api.github.com/users/nicholas-mischke/gists{/gist_id}", "starred_url": "https://api.github.com/users/nicholas-mischke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nicholas-mischke/subscriptions", "organizations_url": "https://api.github.com/users/nicholas-mischke/orgs", "repos_url": "https://api.github.com/users/nicholas-mischke/repos", "events_url": "https://api.github.com/users/nicholas-mischke/events{/privacy}", "received_events_url": "https://api.github.com/users/nicholas-mischke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-11T20:38:49Z", "updated_at": "2020-03-12T09:26:22Z", "closed_at": "2020-03-12T09:26:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to save screenshots of scraped webpages with Scrapy Splash. I've copied and pasted the code found here into my pipeline folder: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\r\n\r\nHere's the code from the url:\r\n\r\n[](import scrapy\r\nimport hashlib\r\nfrom urllib.parse import quote\r\n\r\n\r\nclass ScreenshotPipeline(object):\r\n    \"\"\"Pipeline that uses Splash to render screenshot of\r\n    every Scrapy item.\"\"\"\r\n\r\n    SPLASH_URL = \"http://localhost:8050/render.png?url={}\"\r\n\r\n    async def process_item(self, item, spider):\r\n        encoded_item_url = quote(item[\"url\"])\r\n        screenshot_url = self.SPLASH_URL.format(encoded_item_url)\r\n        request = scrapy.Request(screenshot_url)\r\n        response = await spider.crawler.engine.download(request, spider)\r\n\r\n        if response.status != 200:\r\n            # Error happened, return item.\r\n            return item\r\n\r\n        # Save screenshot to file, filename will be hash of url.\r\n        url = item[\"url\"]\r\n        url_hash = hashlib.md5(url.encode(\"utf8\")).hexdigest()\r\n        filename = \"{}.png\".format(url_hash)\r\n        with open(filename, \"wb\") as f:\r\n            f.write(response.body)\r\n\r\n        # Store filename in item.\r\n        item[\"screenshot_filename\"] = filename\r\n        return item))\r\nI've also followed the instructions for setting up splash found here: https://github.com/scrapy-plugins/scrapy-splash\r\n\r\nWhen I call the command scrapy crawl myspider everything works correctly except the pipeline. This is the \"Error\" I'm seeing.\r\n\r\n<coroutine object ScreenshotPipeline.process_item at 0x7f29a9c7c8c0>\r\n\r\nIt appears to me that when process_item() is called, what really needs to happen is asyncio.run(process_item()) needs to be called instead? I've searched through the source code and have had a hard time finding where this function is called, and feel this method is a bit too complicated. \r\n\r\nIs there another reason why the example code will not work? \r\nThank you", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4415", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4415/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4415/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4415/events", "html_url": "https://github.com/scrapy/scrapy/issues/4415", "id": 577658246, "node_id": "MDU6SXNzdWU1Nzc2NTgyNDY=", "number": 4415, "title": ".", "user": {"login": "upworkap", "id": 54761068, "node_id": "MDQ6VXNlcjU0NzYxMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/54761068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/upworkap", "html_url": "https://github.com/upworkap", "followers_url": "https://api.github.com/users/upworkap/followers", "following_url": "https://api.github.com/users/upworkap/following{/other_user}", "gists_url": "https://api.github.com/users/upworkap/gists{/gist_id}", "starred_url": "https://api.github.com/users/upworkap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/upworkap/subscriptions", "organizations_url": "https://api.github.com/users/upworkap/orgs", "repos_url": "https://api.github.com/users/upworkap/repos", "events_url": "https://api.github.com/users/upworkap/events{/privacy}", "received_events_url": "https://api.github.com/users/upworkap/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-09T04:55:44Z", "updated_at": "2020-03-11T00:44:17Z", "closed_at": "2020-03-10T14:44:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4408", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4408/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4408/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4408/events", "html_url": "https://github.com/scrapy/scrapy/issues/4408", "id": 577045275, "node_id": "MDU6SXNzdWU1NzcwNDUyNzU=", "number": 4408, "title": "follow_all fails with an empty list of URLs", "user": {"login": "ivanprado", "id": 895720, "node_id": "MDQ6VXNlcjg5NTcyMA==", "avatar_url": "https://avatars1.githubusercontent.com/u/895720?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ivanprado", "html_url": "https://github.com/ivanprado", "followers_url": "https://api.github.com/users/ivanprado/followers", "following_url": "https://api.github.com/users/ivanprado/following{/other_user}", "gists_url": "https://api.github.com/users/ivanprado/gists{/gist_id}", "starred_url": "https://api.github.com/users/ivanprado/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ivanprado/subscriptions", "organizations_url": "https://api.github.com/users/ivanprado/orgs", "repos_url": "https://api.github.com/users/ivanprado/repos", "events_url": "https://api.github.com/users/ivanprado/events{/privacy}", "received_events_url": "https://api.github.com/users/ivanprado/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "elacuesta", "id": 1731933, "node_id": "MDQ6VXNlcjE3MzE5MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1731933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elacuesta", "html_url": "https://github.com/elacuesta", "followers_url": "https://api.github.com/users/elacuesta/followers", "following_url": "https://api.github.com/users/elacuesta/following{/other_user}", "gists_url": "https://api.github.com/users/elacuesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/elacuesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elacuesta/subscriptions", "organizations_url": "https://api.github.com/users/elacuesta/orgs", "repos_url": "https://api.github.com/users/elacuesta/repos", "events_url": "https://api.github.com/users/elacuesta/events{/privacy}", "received_events_url": "https://api.github.com/users/elacuesta/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/scrapy/scrapy/milestones/28", "html_url": "https://github.com/scrapy/scrapy/milestone/28", "labels_url": "https://api.github.com/repos/scrapy/scrapy/milestones/28/labels", "id": 5194901, "node_id": "MDk6TWlsZXN0b25lNTE5NDkwMQ==", "number": 28, "title": "v2.0.1", "description": "", "creator": {"login": "wRAR", "id": 241039, "node_id": "MDQ6VXNlcjI0MTAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/241039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wRAR", "html_url": "https://github.com/wRAR", "followers_url": "https://api.github.com/users/wRAR/followers", "following_url": "https://api.github.com/users/wRAR/following{/other_user}", "gists_url": "https://api.github.com/users/wRAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/wRAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wRAR/subscriptions", "organizations_url": "https://api.github.com/users/wRAR/orgs", "repos_url": "https://api.github.com/users/wRAR/repos", "events_url": "https://api.github.com/users/wRAR/events{/privacy}", "received_events_url": "https://api.github.com/users/wRAR/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 7, "state": "closed", "created_at": "2020-03-12T12:33:20Z", "updated_at": "2020-03-19T13:29:14Z", "due_on": "2020-03-31T07:00:00Z", "closed_at": "2020-03-19T13:29:14Z"}, "comments": 0, "created_at": "2020-03-06T16:36:29Z", "updated_at": "2020-03-12T13:44:49Z", "closed_at": "2020-03-12T13:44:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your issue, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#reporting-bugs\r\n\r\n-->\r\n\r\n### Description\r\n\r\n`follow_all` with an empty list of urls fails with `ValueError('Please supply exactly one of the following arguments: urls, css, xpath')`\r\n\r\nWhat I would expect instead is just an empty generator of requests. \r\n\r\n### Steps to Reproduce\r\n\r\n```py\r\nclass Spider(scrapy.Spider):\r\n\r\n    def parse(self, response):\r\n        yield from response.follow_all([], self.parse)\r\n```\r\n\r\n**Expected behavior:** \r\n\r\nNo error is raised\r\n\r\n**Actual behavior:**\r\n\r\n`ValueError('Please supply exactly one of the following arguments: urls, css, xpath')` exception is raised. \r\n\r\n\r\n### Versions\r\n\r\n2.0\r\n\r\n### Additional context\r\n\r\nI think the solution is just a matter of changing this line: https://github.com/scrapy/scrapy/blob/master/scrapy/http/response/text.py#L191\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4405", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4405/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4405/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4405/events", "html_url": "https://github.com/scrapy/scrapy/issues/4405", "id": 576059924, "node_id": "MDU6SXNzdWU1NzYwNTk5MjQ=", "number": 4405, "title": "is scrapy request shor connection? how to change it become long connection?", "user": {"login": "ayay129", "id": 31658398, "node_id": "MDQ6VXNlcjMxNjU4Mzk4", "avatar_url": "https://avatars1.githubusercontent.com/u/31658398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ayay129", "html_url": "https://github.com/ayay129", "followers_url": "https://api.github.com/users/ayay129/followers", "following_url": "https://api.github.com/users/ayay129/following{/other_user}", "gists_url": "https://api.github.com/users/ayay129/gists{/gist_id}", "starred_url": "https://api.github.com/users/ayay129/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ayay129/subscriptions", "organizations_url": "https://api.github.com/users/ayay129/orgs", "repos_url": "https://api.github.com/users/ayay129/repos", "events_url": "https://api.github.com/users/ayay129/events{/privacy}", "received_events_url": "https://api.github.com/users/ayay129/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-05T07:55:37Z", "updated_at": "2020-03-10T12:34:54Z", "closed_at": "2020-03-10T12:34:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\n\r\nThanks for taking an interest in Scrapy!\r\n\r\nIf you have a question that starts with \"How to...\", please see the Scrapy Community page: https://scrapy.org/community/.\r\nThe GitHub issue tracker's purpose is to deal with bug reports and feature requests for the project itself.\r\n\r\nKeep in mind that by filing an issue, you are expected to comply with Scrapy's Code of Conduct, including treating everyone with respect: https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md\r\n\r\nThe following is a suggested template to structure your pull request, you can find more guidelines at https://doc.scrapy.org/en/latest/contributing.html#writing-patches and https://doc.scrapy.org/en/latest/contributing.html#submitting-patches\r\n\r\n-->\r\n\r\n## Summary\r\n\r\nOne paragraph explanation of the feature.\r\n\r\n## Motivation\r\n\r\nWhy are we doing this? What use cases does it support? What is the expected outcome?\r\n\r\n## Describe alternatives you've considered\r\n\r\nA clear and concise description of the alternative solutions you've considered. Be sure to explain why Scrapy's existing customizability isn't suitable for this feature.\r\n\r\n## Additional context\r\n\r\nAny additional information about the feature request here.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4401", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4401/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4401/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4401/events", "html_url": "https://github.com/scrapy/scrapy/issues/4401", "id": 575481096, "node_id": "MDU6SXNzdWU1NzU0ODEwOTY=", "number": 4401, "title": "Easy to break installing the custom reactor", "user": {"login": "wRAR", "id": 241039, "node_id": "MDQ6VXNlcjI0MTAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/241039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wRAR", "html_url": "https://github.com/wRAR", "followers_url": "https://api.github.com/users/wRAR/followers", "following_url": "https://api.github.com/users/wRAR/following{/other_user}", "gists_url": "https://api.github.com/users/wRAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/wRAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wRAR/subscriptions", "organizations_url": "https://api.github.com/users/wRAR/orgs", "repos_url": "https://api.github.com/users/wRAR/repos", "events_url": "https://api.github.com/users/wRAR/events{/privacy}", "received_events_url": "https://api.github.com/users/wRAR/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/scrapy/scrapy/milestones/28", "html_url": "https://github.com/scrapy/scrapy/milestone/28", "labels_url": "https://api.github.com/repos/scrapy/scrapy/milestones/28/labels", "id": 5194901, "node_id": "MDk6TWlsZXN0b25lNTE5NDkwMQ==", "number": 28, "title": "v2.0.1", "description": "", "creator": {"login": "wRAR", "id": 241039, "node_id": "MDQ6VXNlcjI0MTAzOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/241039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wRAR", "html_url": "https://github.com/wRAR", "followers_url": "https://api.github.com/users/wRAR/followers", "following_url": "https://api.github.com/users/wRAR/following{/other_user}", "gists_url": "https://api.github.com/users/wRAR/gists{/gist_id}", "starred_url": "https://api.github.com/users/wRAR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wRAR/subscriptions", "organizations_url": "https://api.github.com/users/wRAR/orgs", "repos_url": "https://api.github.com/users/wRAR/repos", "events_url": "https://api.github.com/users/wRAR/events{/privacy}", "received_events_url": "https://api.github.com/users/wRAR/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 7, "state": "closed", "created_at": "2020-03-12T12:33:20Z", "updated_at": "2020-03-19T13:29:14Z", "due_on": "2020-03-31T07:00:00Z", "closed_at": "2020-03-19T13:29:14Z"}, "comments": 12, "created_at": "2020-03-04T15:07:05Z", "updated_at": "2020-03-14T09:39:01Z", "closed_at": "2020-03-14T09:39:01Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I created a spider that imports `scrapy.mail` and got \"wrong reactor installed\" after setting `TWISTED_REACTOR`. This is because `scrapy.mail` imports `twisted.internet.reactor` at the top level and because `scrapy crawl` imports all spider modules.\r\n\r\nSo I think we should kill all top-level `twisted.internet.reactor` imports (it was partially done when adding the feature, so that importing `CrawlerProcess` doesn't install the reactor) and add documentation that the user code shouldn't do similar top-level imports.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4397", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4397/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4397/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4397/events", "html_url": "https://github.com/scrapy/scrapy/issues/4397", "id": 574210358, "node_id": "MDU6SXNzdWU1NzQyMTAzNTg=", "number": 4397, "title": "\"Empty separator\" error while pasting .split(\"some-non-ascii-character\") into scrapy shell.", "user": {"login": "upworkap", "id": 54761068, "node_id": "MDQ6VXNlcjU0NzYxMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/54761068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/upworkap", "html_url": "https://github.com/upworkap", "followers_url": "https://api.github.com/users/upworkap/followers", "following_url": "https://api.github.com/users/upworkap/following{/other_user}", "gists_url": "https://api.github.com/users/upworkap/gists{/gist_id}", "starred_url": "https://api.github.com/users/upworkap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/upworkap/subscriptions", "organizations_url": "https://api.github.com/users/upworkap/orgs", "repos_url": "https://api.github.com/users/upworkap/repos", "events_url": "https://api.github.com/users/upworkap/events{/privacy}", "received_events_url": "https://api.github.com/users/upworkap/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-02T19:56:52Z", "updated_at": "2020-03-05T07:40:51Z", "closed_at": "2020-03-05T07:40:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to use scrapy shell and paste non-ascii characters?\r\nI think it is related to IPython.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4396", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4396/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4396/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4396/events", "html_url": "https://github.com/scrapy/scrapy/issues/4396", "id": 574146751, "node_id": "MDU6SXNzdWU1NzQxNDY3NTE=", "number": 4396, "title": "docs: Wrong year in footer notes", "user": {"login": "Urahara", "id": 1462548, "node_id": "MDQ6VXNlcjE0NjI1NDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1462548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Urahara", "html_url": "https://github.com/Urahara", "followers_url": "https://api.github.com/users/Urahara/followers", "following_url": "https://api.github.com/users/Urahara/following{/other_user}", "gists_url": "https://api.github.com/users/Urahara/gists{/gist_id}", "starred_url": "https://api.github.com/users/Urahara/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Urahara/subscriptions", "organizations_url": "https://api.github.com/users/Urahara/orgs", "repos_url": "https://api.github.com/users/Urahara/repos", "events_url": "https://api.github.com/users/Urahara/events{/privacy}", "received_events_url": "https://api.github.com/users/Urahara/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-02T17:59:23Z", "updated_at": "2020-03-02T19:30:12Z", "closed_at": "2020-03-02T19:30:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nThe footer of generated docs of https://docs.scrapy.org/ are 2018, _eg:_ \"\u00a9 Copyright 2008\u2013**2018**, Scrapy developers Revision be2e910d.\"\r\n\r\n### Steps to Reproduce\r\n\r\n1. Go to https://docs.scrapy.org/\r\n2. See footer\r\n\r\n**Expected behavior:** The year should year current year: 2020. _eg:_ \"\u00a9 Copyright 2008\u2013**2020**, Scrapy developers Revision be2e910d.\"\r\n\r\n**Actual behavior:** The year that show is 2018.\r\n\r\n**Reproduces how often:** All time.\r\n\r\n### Versions\r\n\r\nAll.\r\n\r\n### Additional context\r\n\r\nWhen building docs locally this error does not occurs.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4394", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4394/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4394/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4394/events", "html_url": "https://github.com/scrapy/scrapy/issues/4394", "id": 574001017, "node_id": "MDU6SXNzdWU1NzQwMDEwMTc=", "number": 4394, "title": "How to handle errors from spider properly?", "user": {"login": "Urahara", "id": 1462548, "node_id": "MDQ6VXNlcjE0NjI1NDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1462548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Urahara", "html_url": "https://github.com/Urahara", "followers_url": "https://api.github.com/users/Urahara/followers", "following_url": "https://api.github.com/users/Urahara/following{/other_user}", "gists_url": "https://api.github.com/users/Urahara/gists{/gist_id}", "starred_url": "https://api.github.com/users/Urahara/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Urahara/subscriptions", "organizations_url": "https://api.github.com/users/Urahara/orgs", "repos_url": "https://api.github.com/users/Urahara/repos", "events_url": "https://api.github.com/users/Urahara/events{/privacy}", "received_events_url": "https://api.github.com/users/Urahara/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-02T14:11:21Z", "updated_at": "2020-03-02T19:27:55Z", "closed_at": "2020-03-02T19:27:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "- Is  `yield` errors like http status, bad responses from spider considered a bad practice?\r\n- What Scrapy recommends to users do in workflow of spiders to handle errors?\r\n\r\nI tried using  `stderr ` but without success. (Like when i encounter a error in my workflow i quit spider and put a message in there)\r\n\r\nI'm asking here because i think this is out of scope of SO.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy/scrapy/issues/4393", "repository_url": "https://api.github.com/repos/scrapy/scrapy", "labels_url": "https://api.github.com/repos/scrapy/scrapy/issues/4393/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy/scrapy/issues/4393/comments", "events_url": "https://api.github.com/repos/scrapy/scrapy/issues/4393/events", "html_url": "https://github.com/scrapy/scrapy/issues/4393", "id": 573885381, "node_id": "MDU6SXNzdWU1NzM4ODUzODE=", "number": 4393, "title": "scrapy parse emits ANSI color sequences in the Windows terminal", "user": {"login": "A-hoy", "id": 55936555, "node_id": "MDQ6VXNlcjU1OTM2NTU1", "avatar_url": "https://avatars2.githubusercontent.com/u/55936555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/A-hoy", "html_url": "https://github.com/A-hoy", "followers_url": "https://api.github.com/users/A-hoy/followers", "following_url": "https://api.github.com/users/A-hoy/following{/other_user}", "gists_url": "https://api.github.com/users/A-hoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/A-hoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/A-hoy/subscriptions", "organizations_url": "https://api.github.com/users/A-hoy/orgs", "repos_url": "https://api.github.com/users/A-hoy/repos", "events_url": "https://api.github.com/users/A-hoy/events{/privacy}", "received_events_url": "https://api.github.com/users/A-hoy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 318523681, "node_id": "MDU6TGFiZWwzMTg1MjM2ODE=", "url": "https://api.github.com/repos/scrapy/scrapy/labels/Windows", "name": "Windows", "color": "fad8c7", "default": false, "description": null}, {"id": 13907246, "node_id": "MDU6TGFiZWwxMzkwNzI0Ng==", "url": "https://api.github.com/repos/scrapy/scrapy/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-03-02T10:45:39Z", "updated_at": "2020-07-28T11:13:20Z", "closed_at": "2020-07-28T11:13:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\ni try to use `scrapy parse` command in cmd\uff08anaconda env\uff09\uff0cbut when it logs Scraped Items and Requests, there are full of  garbled code which i show you below\uff08Additional context\uff09. I have no idea of it, but it seems to have nothing to do with character encoding,\r\n\r\n### Steps to Reproduce\r\nrunning command\r\n```\r\n>>> scrapy parse --spider=quotes3 http://quotes.toscrape.com/page/1/\r\n```\r\nthe `quotes3` spider shows below\uff08Additional context\uff09\r\n\r\n**Expected behavior:** without garbled code\r\n\r\n**Actual behavior:** garbled code appears\r\n\r\n**Reproduces how often:** every time\r\n\r\n### Versions\r\nScrapy       : 1.8.0\r\nlxml         : 4.4.1.0\r\nlibxml2      : 2.9.9\r\ncssselect    : 1.1.0\r\nparsel       : 1.5.2\r\nw3lib        : 1.21.0\r\nTwisted      : 19.10.0\r\nPython       : 3.7.4 (default, Aug  9 2019, 18:22:51) [MSC v.1915 32 bit (Intel)]\r\npyOpenSSL    : 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019)\r\ncryptography : 2.7\r\nPlatform     : Windows-10-10.0.18362-SP0\r\n\r\n### Additional context\r\nspider.py\r\n```\r\nclass QuotesSpider3(scrapy.Spider):\r\n    name = 'quotes3'\r\n    start_urls = ['http://quotes.toscrape.com/page/1/']\r\n\r\n    def parse(self, response):\r\n        for quote in response.xpath('//div[@class=\"quote\"]'):\r\n            yield {\r\n                'text':\r\n                quote.xpath('span[@class=\"text\"]/text()').get(),\r\n                'author':\r\n                quote.xpath('.//small[@class=\"author\"]/text()').get(),\r\n                'tags':\r\n                quote.xpath(\r\n                    'div[@class=\"tags\"]//a[@class=\"tag\"]/text()').getall()\r\n            }\r\n\r\n        next_page = response.xpath('//li[@class=\"next\"]//a/@href').get()\r\n        if next_page is not None:\r\n            next_page = response.urljoin(next_page)\r\n            yield scrapy.Request(next_page, callback=self.parse)\r\n```\r\n\r\n```\r\n(base) D:\\scrapy_project>chcp\r\nActive code page: 65001\r\n\r\n(base) D:\\scrapy_project>scrapy parse --spider=quotes3 http://quotes.toscrape.com/page/1/\r\n2020-03-02 18:27:36 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: demo1)\r\n2020-03-02 18:27:36 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug  9 2019, 18:22:51) [MSC v.1915 32 bit (Intel)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.7, Platform Windows-10-10.0.18362-SP0\r\n2020-03-02 18:27:36 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'demo1', 'NEWSPIDER_MODULE': 'demo1.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['demo1.spiders']}\r\n2020-03-02 18:27:36 [scrapy.extensions.telnet] INFO: Telnet Password: 67f8ee92329c4e99\r\n2020-03-02 18:27:37 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2020-03-02 18:27:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2020-03-02 18:27:39 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2020-03-02 18:27:39 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2020-03-02 18:27:39 [scrapy.core.engine] INFO: Spider opened\r\n2020-03-02 18:27:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-03-02 18:27:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2020-03-02 18:27:44 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\r\n2020-03-02 18:27:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\r\n2020-03-02 18:27:52 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2020-03-02 18:27:52 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 453,\r\n 'downloader/request_count': 2,\r\n 'downloader/request_method_count/GET': 2,\r\n 'downloader/response_bytes': 2719,\r\n 'downloader/response_count': 2,\r\n 'downloader/response_status_count/200': 1,\r\n 'downloader/response_status_count/404': 1,\r\n 'elapsed_time_seconds': 12.789971,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2020, 3, 2, 10, 27, 52, 44220),\r\n 'log_count/DEBUG': 2,\r\n 'log_count/INFO': 10,\r\n 'response_received_count': 2,\r\n 'robotstxt/request_count': 1,\r\n 'robotstxt/response_count': 1,\r\n 'robotstxt/response_status_count/404': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2020, 3, 2, 10, 27, 39, 254249)}\r\n2020-03-02 18:27:52 [scrapy.core.engine] INFO: Spider closed (finished)\r\n\r\n>>> STATUS DEPTH LEVEL 1 <<<\r\n# Scraped Items  ------------------------------------------------------------\r\n[{\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mAlbert Einstein\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mchange\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mdeep-thoughts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mthinking\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mworld\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cThe world as we have created it is a process of our thinking. It \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mcannot be changed without changing our thinking.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mJ.K. Rowling\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mabilities\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mchoices\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cIt is our choices, Harry, that show what we truly are, far more \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mthan our abilities.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mAlbert Einstein\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33minspirational\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlife\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlive\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmiracle\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mmiracles\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cThere are only two ways to live your life. One is as though \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mnothing is a miracle. The other is as though everything is a \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mmiracle.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mJane Austen\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33maliteracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mbooks\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mclassic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mhumor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cThe person, be it gentleman or lady, who has not pleasure in a \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mgood novel, must be intolerably stupid.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mMarilyn Monroe\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mbe-yourself\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minspirational\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m\u201cImperfection is beauty, madness is genius and it\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms better to be \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mabsolutely ridiculous than absolutely boring.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mAlbert Einstein\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33madulthood\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msuccess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cTry not to become a man of success. Rather become a man of \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mvalue.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mAndr\u00e9 Gide\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mlife\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mlove\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cIt is better to be hated for what you are than to be loved for \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m'\u001b[39;49;00m\u001b[33mwhat you are not.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mThomas A. Edison\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33medison\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mfailure\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minspirational\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mparaphrased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m\u201cI have not failed. I\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mve just found 10,000 ways that won\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt work.\u201d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mEleanor Roosevelt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mmisattributed-eleanor-roosevelt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cA woman is like a tea bag; you never know how strong it is until \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n          \u001b[33m\"\u001b[39;49;00m\u001b[33mit\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms in hot water.\u201d\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m},\r\n {\u001b[33m'\u001b[39;49;00m\u001b[33mauthor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33mSteve Martin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtags\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m'\u001b[39;49;00m\u001b[33mhumor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mobvious\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msimile\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n  \u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[33m'\u001b[39;49;00m\u001b[33m\u201cA day without sunshine is like, you know, night.\u201d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m}]\r\n\r\n# Requests  -----------------------------------------------------------------\r\n[<GET http://quotes.toscrape.com/page/\u001b[34m2\u001b[39;49;00m/>]\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}]}