{"total_count": 69, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/176", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/176/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/176/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/176/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/176", "id": 649760393, "node_id": "MDU6SXNzdWU2NDk3NjAzOTM=", "number": 176, "title": "Stuck with errors related to % sign for --benchmark-compare-fail", "user": {"login": "trubesv", "id": 14898388, "node_id": "MDQ6VXNlcjE0ODk4Mzg4", "avatar_url": "https://avatars2.githubusercontent.com/u/14898388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trubesv", "html_url": "https://github.com/trubesv", "followers_url": "https://api.github.com/users/trubesv/followers", "following_url": "https://api.github.com/users/trubesv/following{/other_user}", "gists_url": "https://api.github.com/users/trubesv/gists{/gist_id}", "starred_url": "https://api.github.com/users/trubesv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trubesv/subscriptions", "organizations_url": "https://api.github.com/users/trubesv/orgs", "repos_url": "https://api.github.com/users/trubesv/repos", "events_url": "https://api.github.com/users/trubesv/events{/privacy}", "received_events_url": "https://api.github.com/users/trubesv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-02T08:56:57Z", "updated_at": "2020-07-02T10:17:08Z", "closed_at": "2020-07-02T10:17:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, first of all thanks for this great tool :)\r\n\r\nI am stuck in a kind of deadlock situation I can't seem to resolve, maybe I am missing something obvious.\r\n\r\nFor instance I have the following:\r\n```\r\n# setup.cfg\r\n[tool:pytest]\r\naddopts = --benchmark-compare-fail=min:6%\r\n```\r\nNaturally it works fine with `pytest`.\r\n\r\nTrouble comes when I run `pip install <something>` in a directory containing that file:\r\n```\r\nERROR: Exception:\r\nTraceback (most recent call last):\r\n  File \"/home/trubesv/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 186, in _main\r\n    status = self.run(options, args)\r\n  File \"/home/trubesv/.local/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 253, in run\r\n    options.use_user_site = decide_user_install(\r\n  File \"/home/trubesv/.local/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 604, in decide_user_install\r\n    if site_packages_writable(root=root_path, isolated=isolated_mode):\r\n  File \"/home/trubesv/.local/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 549, in site_packages_writable\r\n    test_writable_dir(d) for d in set(get_lib_location_guesses(**kwargs))\r\n  File \"/home/trubesv/.local/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 543, in get_lib_location_guesses\r\n    scheme = distutils_scheme('', *args, **kwargs)\r\n  File \"/home/trubesv/.local/lib/python3.8/site-packages/pip/_internal/locations.py\", line 109, in distutils_scheme\r\n    d.parse_config_files()\r\n  File \"/usr/lib/python3.8/distutils/dist.py\", line 413, in parse_config_files\r\n    val = parser.get(section,opt)\r\n  File \"/usr/lib/python3.8/configparser.py\", line 799, in get\r\n    return self._interpolation.before_get(self, section, option, value,\r\n  File \"/usr/lib/python3.8/configparser.py\", line 395, in before_get\r\n    self._interpolate_some(parser, option, L, value, section, defaults, 1)\r\n  File \"/usr/lib/python3.8/configparser.py\", line 442, in _interpolate_some\r\n    raise InterpolationSyntaxError(\r\nconfigparser.InterpolationSyntaxError: '%' must be followed by '%' or '(', found: '%'\r\n```\r\nThis seems fine according to the python \"configfile\" spec.\r\n\r\nAfter adding another `%` as suggested, `pip install` succeed but by running `pytest`, I got:\r\n```\r\nERROR: usage: pytest [options] [file_or_dir] [file_or_dir] [...]\r\npytest: error: argument --benchmark-compare-fail: Could not parse value: 'min:6%%'\r\n```\r\nSo now I am stuck in a state where I can't install requirements or I can't run tests :(\r\n\r\nIs there a quick fix I can use/implement on my side?\r\nIs this a `pytest-benchmark` issue or a `pytest` one?\r\nShould it be escalated as a `pip` issue maybe?\r\n\r\nThanks for your time :)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/174", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/174/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/174/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/174/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/174", "id": 637292479, "node_id": "MDU6SXNzdWU2MzcyOTI0Nzk=", "number": 174, "title": "Add Ability To Overwrite BenchmarkFixture Class", "user": {"login": "rmorshea", "id": 4041990, "node_id": "MDQ6VXNlcjQwNDE5OTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/4041990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmorshea", "html_url": "https://github.com/rmorshea", "followers_url": "https://api.github.com/users/rmorshea/followers", "following_url": "https://api.github.com/users/rmorshea/following{/other_user}", "gists_url": "https://api.github.com/users/rmorshea/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmorshea/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmorshea/subscriptions", "organizations_url": "https://api.github.com/users/rmorshea/orgs", "repos_url": "https://api.github.com/users/rmorshea/repos", "events_url": "https://api.github.com/users/rmorshea/events{/privacy}", "received_events_url": "https://api.github.com/users/rmorshea/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-11T20:12:20Z", "updated_at": "2020-06-11T21:40:44Z", "closed_at": "2020-06-11T21:40:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "There's some custom things that I'd like to customize about the `BenchmarkFixture` class however there's not a simple way to do this. If there were a `benchmark_type` fixture which was used by `benchmark` to construct the `BenchmarkFixture` instance I'd be able to control this.\r\n\r\nThe changes might look something like this:\r\n\r\n```python\r\n@pytest.fixture(scope=\"function\")\r\ndef benchmark_type():\r\n    return BenchmarkFixture\r\n\r\n@pytest.fixture(scope=\"function\")\r\ndef benchmark(request, benchmark_type):\r\n    ...\r\n    fixture = benchmark_type(...)\r\n    ...\r\n    return fixture\r\n```\r\n\r\nThis way I could define my own `benchmark_type` fixture.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/173", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/173/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/173/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/173/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/173", "id": 625036846, "node_id": "MDU6SXNzdWU2MjUwMzY4NDY=", "number": 173, "title": "Error on compare", "user": {"login": "mnicely", "id": 50021634, "node_id": "MDQ6VXNlcjUwMDIxNjM0", "avatar_url": "https://avatars0.githubusercontent.com/u/50021634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mnicely", "html_url": "https://github.com/mnicely", "followers_url": "https://api.github.com/users/mnicely/followers", "following_url": "https://api.github.com/users/mnicely/following{/other_user}", "gists_url": "https://api.github.com/users/mnicely/gists{/gist_id}", "starred_url": "https://api.github.com/users/mnicely/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mnicely/subscriptions", "organizations_url": "https://api.github.com/users/mnicely/orgs", "repos_url": "https://api.github.com/users/mnicely/repos", "events_url": "https://api.github.com/users/mnicely/events{/privacy}", "received_events_url": "https://api.github.com/users/mnicely/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-26T16:58:00Z", "updated_at": "2020-08-17T17:19:03Z", "closed_at": "2020-08-17T17:19:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Running \r\npytest 5.4.2\r\npytest-benchmark 3.2.3\r\n\r\nI'm getting the following error on `pytest-benchmark compare 0001 0002`\r\n\r\n```bash\r\n/home/belt/anaconda3/envs/cusignal/bin/pytest-benchmark: 2: exec: /home/conda/feedstock_root/build_artifacts/pytest-benchmark_1585050181913/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac/bin/python: not found\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/168", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/168/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/168/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/168/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/168", "id": 602049347, "node_id": "MDU6SXNzdWU2MDIwNDkzNDc=", "number": 168, "title": "benchmark-compare=NUM option without NUM set, runs as if no benchmark-compare was set", "user": {"login": "hcuche", "id": 88721, "node_id": "MDQ6VXNlcjg4NzIx", "avatar_url": "https://avatars2.githubusercontent.com/u/88721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hcuche", "html_url": "https://github.com/hcuche", "followers_url": "https://api.github.com/users/hcuche/followers", "following_url": "https://api.github.com/users/hcuche/following{/other_user}", "gists_url": "https://api.github.com/users/hcuche/gists{/gist_id}", "starred_url": "https://api.github.com/users/hcuche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hcuche/subscriptions", "organizations_url": "https://api.github.com/users/hcuche/orgs", "repos_url": "https://api.github.com/users/hcuche/repos", "events_url": "https://api.github.com/users/hcuche/events{/privacy}", "received_events_url": "https://api.github.com/users/hcuche/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-17T15:16:27Z", "updated_at": "2020-05-11T13:34:19Z", "closed_at": "2020-04-17T15:36:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I try to compare the last saved reference (using `--benchmark-autosave` or `--benchmark-save=some-name` options), with the current run (using `--benchmark-compare=` as explain in the [documentation](https://pytest-benchmark.readthedocs.io/en/latest/usage.html#commandline-options))\r\n\r\n```\r\n--benchmark-compare=NUM\r\n \tCompare the current run against run NUM (or prefix of _id in elasticsearch) or the latest saved run if unspecified.\r\n```\r\nThere is no comparison made, only a standard benchmark run.\r\n\r\nI have the following tests/test_perf.py file\r\n```python\r\ndef fake_print():\r\n    print(\"Fake\")\r\n\r\ndef test_perf_cortex(benchmark):\r\n    benchmark(fake_print)\r\n```\r\n\r\nWhen I run `--benchmark-autosave` the file is well generated in .benchmarks/ :\r\n```\r\n$ python3 -m pytest --benchmark-autosave tests      \r\n(...)                                                                                                                                 \r\n$ ls -R .benchmarks                                                                                                                                                                              .benchmarks:\r\nLinux-CPython-3.6-64bit/\r\n\r\n.benchmarks/Linux-CPython-3.6-64bit:\r\n0001_unversioned_20200417_145821.json\r\n```\r\n\r\nThen running the compare option using the saved **NUM** works fine:\r\n```\r\n$ python3 -m pytest --benchmark-compare=0001 tests/\r\n```\r\nbut running without **NUM**, runs pytest as if no `--benchmark-compare` was set\r\n```\r\n$ python3 -m pytest --benchmark-compare= tests/\r\n```\r\n\r\n\r\n==================\r\nUbuntu 18.04\r\n```\r\n$ uname -a\r\nLinux xxxxxx 4.15.0-96-generic #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\nPython version and dependencies\r\n```\r\npython=3.6.9\r\npytest=5.4.1\r\npytest-benchmark=3.2.3\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/167", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/167/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/167/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/167/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/167", "id": 602049322, "node_id": "MDU6SXNzdWU2MDIwNDkzMjI=", "number": 167, "title": "benchmark-compare=NUM option without NUM set mrun as if no benchmark-compare was set", "user": {"login": "hcuche", "id": 88721, "node_id": "MDQ6VXNlcjg4NzIx", "avatar_url": "https://avatars2.githubusercontent.com/u/88721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hcuche", "html_url": "https://github.com/hcuche", "followers_url": "https://api.github.com/users/hcuche/followers", "following_url": "https://api.github.com/users/hcuche/following{/other_user}", "gists_url": "https://api.github.com/users/hcuche/gists{/gist_id}", "starred_url": "https://api.github.com/users/hcuche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hcuche/subscriptions", "organizations_url": "https://api.github.com/users/hcuche/orgs", "repos_url": "https://api.github.com/users/hcuche/repos", "events_url": "https://api.github.com/users/hcuche/events{/privacy}", "received_events_url": "https://api.github.com/users/hcuche/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-17T15:16:25Z", "updated_at": "2020-04-17T15:21:47Z", "closed_at": "2020-04-17T15:21:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I try to compare the last saved reference (using `--benchmark-autosave` or `--benchmark-save=some-name` options), with the current run (using `--benchmark-compare=` as explain in the [documentation](https://pytest-benchmark.readthedocs.io/en/latest/usage.html#commandline-options))\r\n\r\n```\r\n--benchmark-compare=NUM\r\n \tCompare the current run against run NUM (or prefix of _id in elasticsearch) or the latest saved run if unspecified.\r\n```\r\nThere is no comparison made, only a standard benchmark run.\r\n\r\nI have the following tests/test_perf.py file\r\n```python\r\ndef fake_print():\r\n    print(\"Fake\")\r\n\r\ndef test_perf_cortex(benchmark):\r\n    benchmark(fake_print)\r\n```\r\n\r\nWhen I run `--benchmark-autosave` the file is well generated in .benchmarks/ :\r\n```\r\n$ python3 -m pytest --benchmark-autosave tests      \r\n(...)                                                                                                                                 \r\n$ ls -R .benchmarks                                                                                                                                                                              .benchmarks:\r\nLinux-CPython-3.6-64bit/\r\n\r\n.benchmarks/Linux-CPython-3.6-64bit:\r\n0001_unversioned_20200417_145821.json\r\n```\r\n\r\nThen running the compare option using the saved **NUM** works fine:\r\n```\r\n$ python3 -m pytest --benchmark-compare=0001 tests/\r\n```\r\nbut running without **NUM**, run pytest as if there were no `--benchmark-compare` set\r\n```\r\n$ python3 -m pytest --benchmark-compare= tests/\r\n```\r\n\r\n\r\n==================\r\nUbuntu 18.04\r\n```\r\n$ uname -a\r\nLinux xxxxxx 4.15.0-96-generic #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\n\r\nPython version and dependencies\r\n```\r\npython=3.6.9\r\npytest=5.4.1\r\npytest-benchmark=3.2.3\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/164", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/164/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/164/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/164/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/164", "id": 575920743, "node_id": "MDU6SXNzdWU1NzU5MjA3NDM=", "number": 164, "title": "Docs mention benchmark-name, should it be benchmark-save?", "user": {"login": "langston-barrett", "id": 4294323, "node_id": "MDQ6VXNlcjQyOTQzMjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4294323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/langston-barrett", "html_url": "https://github.com/langston-barrett", "followers_url": "https://api.github.com/users/langston-barrett/followers", "following_url": "https://api.github.com/users/langston-barrett/following{/other_user}", "gists_url": "https://api.github.com/users/langston-barrett/gists{/gist_id}", "starred_url": "https://api.github.com/users/langston-barrett/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/langston-barrett/subscriptions", "organizations_url": "https://api.github.com/users/langston-barrett/orgs", "repos_url": "https://api.github.com/users/langston-barrett/repos", "events_url": "https://api.github.com/users/langston-barrett/events{/privacy}", "received_events_url": "https://api.github.com/users/langston-barrett/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-05T00:55:08Z", "updated_at": "2020-03-08T04:13:33Z", "closed_at": "2020-03-08T04:13:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\n$ grep -ri \"benchmark-name\"\r\ndocs/usage.rst:  --benchmark-name=FORMAT\r\ndocs/comparing.rst:* ``--benchmark-name=foobar`` works similarly, but saves a file like ``0001_foobar.json``. It's there in case you want to\r\n\r\n$ grep -ri \"benchmark-save\"\r\ntests/test_benchmark.py:        \"  --benchmark-save=NAME\",\r\ntests/test_benchmark.py:        \"  --benchmark-save-data\",\r\ntests/test_benchmark.py:    result = testdir.runpytest_subprocess('--doctest-modules', '--benchmark-save=asd:f?', test)\r\ntests/test_benchmark.py:        \"*: error: argument --benchmark-save: Must not contain any of these characters: /:*?<>|\\\\ (it has ':?')\",\r\ntests/test_benchmark.py:    result = testdir.runpytest_subprocess('--doctest-modules', '--benchmark-save=', test)\r\ntests/test_benchmark.py:        \"*: error: argument --benchmark-save: Can't be empty.\",\r\ntests/test_benchmark.py:    result = testdir.runpytest_subprocess('--doctest-modules', '--benchmark-save=foobar',\r\ntests/test_benchmark.py:    result = testdir.runpytest_subprocess('--doctest-modules', '--benchmark-save=foobar',\r\nsrc/pytest_benchmark/plugin.py:             \"(when --benchmark-save or --benchmark-autosave are used). For backwards compatibility unexpected values \"\r\nsrc/pytest_benchmark/plugin.py:        \"--benchmark-save\",\r\nsrc/pytest_benchmark/plugin.py:        \"--benchmark-save-data\",\r\nsrc/pytest_benchmark/plugin.py:        help=\"Use this to make --benchmark-save and --benchmark-autosave include all the timing data,\"\r\nCHANGELOG.rst:* Improved ``--help`` text for ``--benchmark-histogram``, ``--benchmark-save`` and ``--benchmark-autosave``.\r\nCHANGELOG.rst:* Added benchmark data storage(the ``--benchmark-save`` and ``--benchmark-autosave`` command line arguments).\r\ndocs/usage.rst:  --benchmark-save=NAME\r\ndocs/usage.rst:  --benchmark-save-data\r\ndocs/usage.rst:                        Use this to make --benchmark-save and --benchmark-\r\ndocs/comparing.rst:To store a run just add ``--benchmark-autosave`` or ``--benchmark-save=some-name`` to your pytest arguments. All the files are\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/162", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/162/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/162/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/162/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/162", "id": 565587273, "node_id": "MDU6SXNzdWU1NjU1ODcyNzM=", "number": 162, "title": "First test is always significantly slower than the rest", "user": {"login": "mschwager", "id": 1724818, "node_id": "MDQ6VXNlcjE3MjQ4MTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1724818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mschwager", "html_url": "https://github.com/mschwager", "followers_url": "https://api.github.com/users/mschwager/followers", "following_url": "https://api.github.com/users/mschwager/following{/other_user}", "gists_url": "https://api.github.com/users/mschwager/gists{/gist_id}", "starred_url": "https://api.github.com/users/mschwager/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mschwager/subscriptions", "organizations_url": "https://api.github.com/users/mschwager/orgs", "repos_url": "https://api.github.com/users/mschwager/repos", "events_url": "https://api.github.com/users/mschwager/events{/privacy}", "received_events_url": "https://api.github.com/users/mschwager/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-14T22:27:42Z", "updated_at": "2020-02-14T23:41:07Z", "closed_at": "2020-02-14T23:41:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there,\r\n\r\nThanks for making `pytest-benchmark` - it's been really useful when benchmarking Python code!\r\n\r\nI've been running into an issue where the first test of a parametrized function is always significantly slower. I haven't been able to pin down if this is an issue with my code, `pytest`, `pytest-benchmark`, or something else. I'd appreciate if you took a quick look to see if there are any glaring deficiencies.\r\n\r\nHere's the command I've been running to benchmark:\r\n\r\n```\r\n$ pytest -k test_benchmark_individual  --benchmark-sort=name --benchmark-columns=mean,stddev,median,outliers,rounds --benchmark-py-file django/tests/admin_views/tests.py tests/test_benchmark/\r\n=============================================================================================== test session starts ===============================================================================================\r\nplatform linux -- Python 3.6.9, pytest-4.6.9, py-1.8.1, pluggy-0.13.1\r\nbenchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\nrootdir: .\r\nplugins: cov-2.8.1, benchmark-3.2.3\r\ncollected 39 items / 1 deselected / 38 selected\r\n\r\ntests/test_benchmark/test_benchmark.py ......................................                                                                                                                               [100%]\r\n\r\n---------------------------------------------------------------- benchmark: 38 tests -----------------------------------------------------------------\r\nName (time in ms)                                                                Mean            StdDev             Median            Outliers  Rounds\r\n------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntest_benchmark_individual[DUO101-YieldReturnStatementLinter]                  60.5664 (>1000.0)  0.5839 (>1000.0)  60.4186 (>1000.0)       2;2      17\r\ntest_benchmark_individual[DUO102-BadRandomGeneratorUseLinter]                  0.0043 (1.0)      0.0010 (1.66)      0.0043 (1.0)       196;990   32388\r\ntest_benchmark_individual[DUO103-BadPickleUseLinter]                           0.0050 (1.16)     0.0010 (1.73)      0.0050 (1.16)     343;2088   58259\r\ntest_benchmark_individual[DUO104-BadEvalUseLinter]                             0.0044 (1.02)     0.0009 (1.61)      0.0044 (1.03)     372;2138   69512\r\ntest_benchmark_individual[DUO105-BadExecUseLinter]                             0.0046 (1.06)     0.0012 (2.08)      0.0045 (1.04)     974;7137   70166\r\ntest_benchmark_individual[DUO106-BadOSUseLinter]                               0.0050 (1.16)     0.0012 (2.01)      0.0049 (1.15)     452;3233   57941\r\ntest_benchmark_individual[DUO107-BadXMLUseLinter]                              0.0045 (1.03)     0.0009 (1.54)      0.0044 (1.03)     416;3562   66195\r\ntest_benchmark_individual[DUO108-BadInputUseLinter]                            0.0047 (1.07)     0.0007 (1.24)      0.0046 (1.08)     473;1531   64587\r\ntest_benchmark_individual[DUO109-BadYAMLUseLinter]                             0.0050 (1.15)     0.0006 (1.03)      0.0049 (1.16)     437;1368   59347\r\ntest_benchmark_individual[DUO110-BadCompileUseLinter]                          0.0045 (1.03)     0.0006 (1.03)      0.0044 (1.04)     557;1782   65540\r\ntest_benchmark_individual[DUO111-BadSysUseLinter]                              0.0049 (1.14)     0.0006 (1.0)       0.0049 (1.15)     716;1537   60263\r\ntest_benchmark_individual[DUO112-BadZipfileUseLinter]                          0.0045 (1.03)     0.0011 (1.97)      0.0044 (1.02)     616;3329   67029\r\ntest_benchmark_individual[DUO113-InlineCallbacksYieldStatementLinter]          0.0045 (1.03)     0.0012 (2.03)      0.0043 (1.01)    1744;3402   61002\r\ntest_benchmark_individual[DUO114-ReturnValueInInlineCallbacksLinter]           0.0045 (1.03)     0.0013 (2.27)      0.0043 (1.00)    1618;5075   66450\r\ntest_benchmark_individual[DUO115-BadTarfileUseLinter]                          0.0045 (1.03)     0.0010 (1.71)      0.0044 (1.03)     418;2729   65433\r\ntest_benchmark_individual[DUO116-BadSubprocessUseLinter]                       0.0044 (1.02)     0.0009 (1.64)      0.0043 (1.02)     431;4460   65639\r\ntest_benchmark_individual[DUO117-BadDlUseLinter]                               0.0044 (1.02)     0.0008 (1.38)      0.0044 (1.02)     485;2122   64927\r\ntest_benchmark_individual[DUO118-BadGlUseLinter]                               0.0044 (1.01)     0.0006 (1.06)      0.0043 (1.01)     469;1993   68064\r\ntest_benchmark_individual[DUO119-BadShelveUseLinter]                           0.0044 (1.01)     0.0007 (1.27)      0.0044 (1.02)     483;1793   65450\r\ntest_benchmark_individual[DUO120-BadMarshalUseLinter]                          0.0044 (1.02)     0.0008 (1.40)      0.0044 (1.03)     457;1639   64305\r\ntest_benchmark_individual[DUO121-BadTempfileUseLinter]                         0.0052 (1.20)     0.0226 (39.39)     0.0050 (1.17)       9;2597   58814\r\ntest_benchmark_individual[DUO122-BadSSLModuleAttributeUseLinter]               0.0051 (1.18)     0.0011 (1.85)      0.0050 (1.17)     459;4561   59211\r\ntest_benchmark_individual[DUO123-BadRequestsUseLinter]                         0.0045 (1.03)     0.0009 (1.65)      0.0044 (1.03)     403;2164   65092\r\ntest_benchmark_individual[DUO124-BadXmlrpcUseLinter]                           0.0045 (1.03)     0.0008 (1.47)      0.0044 (1.03)     348;2306   65864\r\ntest_benchmark_individual[DUO125-BadCommandsUseLinter]                         0.0044 (1.02)     0.0008 (1.46)      0.0044 (1.03)     537;2105   68447\r\ntest_benchmark_individual[DUO126-BadPopen2UseLinter]                           0.0045 (1.04)     0.0012 (2.05)      0.0044 (1.03)     739;4439   61798\r\ntest_benchmark_individual[DUO127-BadDuoClientUseLinter]                        0.0044 (1.02)     0.0008 (1.46)      0.0044 (1.02)     480;3519   64529\r\ntest_benchmark_individual[DUO128-BadOneLoginKwargUseLinter]                    0.0046 (1.07)     0.0016 (2.70)      0.0044 (1.03)    1324;4321   64621\r\ntest_benchmark_individual[DUO129-BadOneLoginModuleAttributeUseLinter]          0.0051 (1.17)     0.0012 (2.04)      0.0049 (1.15)    1099;3091   57218\r\ntest_benchmark_individual[DUO130-BadHashlibUseLinter]                          0.0056 (1.29)     0.0023 (4.05)      0.0050 (1.17)    1905;4155   44874\r\ntest_benchmark_individual[DUO131-BadUrllib3ModuleAttributeUseLinter]           0.0081 (1.86)     0.0049 (8.48)      0.0055 (1.30)    1591;1359    8732\r\ntest_benchmark_individual[DUO132-BadUrllib3KwargUseLinter]                     0.0046 (1.05)     0.0015 (2.56)      0.0044 (1.02)    984;10357   70602\r\ntest_benchmark_individual[DUO133-BadPycryptoUseLinter]                         0.0045 (1.04)     0.0011 (1.89)      0.0044 (1.02)     906;9640   68236\r\ntest_benchmark_individual[DUO134-BadCryptographyModuleAttributeUseLinter]      0.0049 (1.13)     0.0008 (1.37)      0.0048 (1.13)    1037;3096   58935\r\ntest_benchmark_individual[DUO135-BadDefusedxmlUseLinter]                       0.0044 (1.02)     0.0007 (1.17)      0.0044 (1.02)    1352;3805   68316\r\ntest_benchmark_individual[DUO136-BadXmlsecModuleAttributeUseLinter]            0.0052 (1.20)     0.0019 (3.31)      0.0049 (1.14)    1646;4573   58518\r\ntest_benchmark_individual[DUO137-BadItsDangerousKwargUseLinter]                0.0044 (1.02)     0.0007 (1.24)      0.0044 (1.02)     396;2103   65059\r\ntest_benchmark_individual[DUO138-BadReCatastrophicUseLinter]                   0.0055 (1.27)     0.0012 (2.01)      0.0054 (1.26)     615;2311   51280\r\n------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nLegend:\r\n  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\r\n  OPS: Operations Per Second, computed as 1 / Mean\r\n==================================================================================== 38 passed, 1 deselected in 20.64 seconds =====================================================================================\r\n```\r\n\r\nAs you can see, the first test is 1000x+ slower.\r\n\r\nThings I've tried:\r\n- Changing the test that runs first. The first test is always the slowest, even if I change what test runs first.\r\n- Tweaking the fixtures scope.\r\n- Tweaking the benchmark timer.\r\n- Tweaking the benchmark max time.\r\n- Tweaking warmup setting and iterations.\r\n\r\nRunning cprofile gives the following results:\r\n\r\n```\r\n$ pytest -k test_benchmark_individual  --benchmark-sort=name --benchmark-cprofile=tottime --benchmark-py-file django/tests/admin_views/tests.py tests/test_benchmark/\r\n=============================================================================================== test session starts ===============================================================================================\r\nplatform linux -- Python 3.6.9, pytest-4.6.9, py-1.8.1, pluggy-0.13.1\r\nbenchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\nrootdir: .\r\nplugins: cov-2.8.1, benchmark-3.2.3\r\ncollected 4 items / 1 deselected / 3 selected                                                                                                                                                                     \r\n\r\ntests/test_benchmark/test_benchmark.py ...                                                                                                                                                                  [100%]\r\n\r\n\r\n--------------------------------------------------------------------------------------------------------- benchmark: 3 tests ---------------------------------------------------------------------------------------------------------\r\nName (time in ms)                                                     Min                Max               Mean            StdDev             Median               IQR            Outliers           OPS            Rounds  Iterations\r\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\ntest_benchmark_individual[DUO101-YieldReturnStatementLinter]      59.6968 (>1000.0)  62.6193 (>1000.0)  60.5421 (>1000.0)  0.8533 (>1000.0)  60.2830 (>1000.0)  0.6874 (>1000.0)       2;2       16.5174 (0.00)         17           1\r\ntest_benchmark_individual[DUO102-BadRandomGeneratorUseLinter]      0.0040 (1.0)       0.0357 (1.04)      0.0043 (1.0)      0.0006 (1.0)       0.0043 (1.0)      0.0001 (1.0)      421;1081  231,004.2785 (1.0)       58879           1\r\ntest_benchmark_individual[DUO103-BadPickleUseLinter]               0.0047 (1.16)      0.0341 (1.0)       0.0050 (1.17)     0.0007 (1.25)      0.0050 (1.16)     0.0001 (1.23)     453;1915  198,150.0590 (0.86)      59517           1\r\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nLegend:\r\n  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\r\n  OPS: Operations Per Second, computed as 1 / Mean\r\n---------------------------------------------------------------------------------------------- cProfile (time in s) -----------------------------------------------------------------------------------------------\r\ndlint/tests/test_benchmark/test_benchmark.py::test_benchmark_individual[DUO101-YieldReturnStatementLinter]\r\nncalls\ttottime\tpercall\tcumtime\tpercall\tfilename:lineno(function)\r\n40017/1\t0.0386\t0.0386\t0.1126\t0.1126\t/usr/lib/python3.6/ast.py:255(generic_visit)\r\n40017/1\t0.0276\t0.0276\t0.1128\t0.1128\tdlint/dlint/multi.py:22(visit)\r\n100142\t0.0199\t0.0000\t0.0199\t0.0000\t~:0(<built-in method builtins.getattr>)\r\n100074\t0.0155\t0.0000\t0.0221\t0.0000\t/usr/lib/python3.6/ast.py:166(iter_fields)\r\n121366\t0.0109\t0.0000\t0.0109\t0.0000\t~:0(<built-in method builtins.isinstance>)\r\n395\t0.0002\t0.0000\t0.0003\t0.0000\tdlint/dlint/linters/twisted/yield_return_statement.py:27(visit_FunctionDef)\r\n34\t0.0001\t0.0000\t0.0001\t0.0000\tvirtualenvs/dlint-HXa--U8o/lib/python3.6/copy.py:66(copy)\r\n395\t0.0001\t0.0000\t0.0001\t0.0000\tdlint/dlint/multi.py:34(recurse_visit)\r\n1\t0.0000\t0.0000\t0.0002\t0.0002\tdlint/dlint/namespace.py:21(from_module_node)\r\n34\t0.0000\t0.0000\t0.0000\t0.0000\tvirtualenvs/dlint-HXa--U8o/lib/python3.6/copy.py:268(_reconstruct)\r\n34\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<built-in method builtins.hasattr>)\r\n1\t0.0000\t0.0000\t0.1128\t0.1128\tdlint/tests/test_benchmark/test_benchmark.py:54(<lambda>)\r\n34\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<method '__reduce_ex__' of 'object' objects>)\r\n34\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<method 'update' of 'dict' objects>)\r\n34\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<method 'append' of 'list' objects>)\r\n68\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<method 'get' of 'dict' objects>)\r\n1\t0.0000\t0.0000\t0.1128\t0.1128\tdlint/dlint/extension.py:79(run)\r\n34\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<built-in method builtins.issubclass>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/base.py:22(__init__)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/extension.py:80(<listcomp>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/multi.py:16(__init__)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/base.py:28(get_results)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/tests/test_benchmark/test_benchmark.py:30(get_linter_classes)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<method 'disable' of '_lsprof.Profiler' objects>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/namespace.py:17(__init__)\r\n\r\ndlint/tests/test_benchmark/test_benchmark.py::test_benchmark_individual[DUO102-BadRandomGeneratorUseLinter]\r\nncalls\ttottime\tpercall\tcumtime\tpercall\tfilename:lineno(function)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/multi.py:22(visit)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/namespace.py:21(from_module_node)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/tests/test_benchmark/test_benchmark.py:54(<lambda>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/extension.py:79(run)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/extension.py:80(<listcomp>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/multi.py:16(__init__)\r\n2\t0.0000\t0.0000\t0.0000\t0.0000\t/usr/lib/python3.6/ast.py:166(iter_fields)\r\n2\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<built-in method builtins.getattr>)\r\n2\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<built-in method builtins.isinstance>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<method 'disable' of '_lsprof.Profiler' objects>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/base.py:22(__init__)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/base.py:28(get_results)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/namespace.py:17(__init__)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/tests/test_benchmark/test_benchmark.py:30(get_linter_classes)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\t/usr/lib/python3.6/ast.py:255(generic_visit)\r\n\r\ndlint/tests/test_benchmark/test_benchmark.py::test_benchmark_individual[DUO103-BadPickleUseLinter]\r\nncalls\ttottime\tpercall\tcumtime\tpercall\tfilename:lineno(function)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/namespace.py:21(from_module_node)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/multi.py:22(visit)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/extension.py:79(run)\r\n2\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<built-in method builtins.getattr>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/tests/test_benchmark/test_benchmark.py:54(<lambda>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/extension.py:80(<listcomp>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/base.py:22(__init__)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/tests/test_benchmark/test_benchmark.py:30(get_linter_classes)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/helpers/bad_module_attribute_use.py:41(get_results)\r\n2\t0.0000\t0.0000\t0.0000\t0.0000\t/usr/lib/python3.6/ast.py:166(iter_fields)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\t/usr/lib/python3.6/ast.py:255(generic_visit)\r\n2\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<built-in method builtins.isinstance>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\t~:0(<method 'disable' of '_lsprof.Profiler' objects>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/multi.py:16(__init__)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/namespace.py:17(__init__)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/helpers/bad_module_attribute_use.py:44(<listcomp>)\r\n1\t0.0000\t0.0000\t0.0000\t0.0000\tdlint/dlint/linters/helpers/bad_module_attribute_use.py:36(__init__)\r\n\r\n===================================================================================== 3 passed, 1 deselected in 3.26 seconds ======================================================================================\r\n```\r\n\r\nThis makes me think that the first test isn't slow so much as the following tests simply aren't running. All their runtimes are 0.0000. I'm not sure what would be going on there.\r\n\r\nHere are the relevant files:\r\n- [`test_benchmark.py`](https://github.com/dlint-py/dlint/blob/master/tests/test_benchmark/test_benchmark.py) - this file contains the benchmarking code. `test_benchmark_individual` in particular is where I'm having these issues.\r\n- [`conftest.py`](https://github.com/dlint-py/dlint/blob/master/tests/test_benchmark/conftest.py) - this is where `--benchmark-py-file` comes from.\r\n- [`extension.py`](https://github.com/dlint-py/dlint/blob/master/dlint/extension.py) - this is the original extension, which gets modified in the tests to only run a single linter (so we can test them individually).\r\n- [`linters`](https://github.com/dlint-py/dlint/tree/master/dlint/linters) - here are all the individual linters.\r\n- [`__init__.py`](dlint/linters/__init__.py) - here's where `linters.ALL` is defined.\r\n\r\nThe above commands should work if you want to pull down the repository and try it out locally.\r\n\r\nI initially thought it may be a caching issue and the cache gets warmed up after the initial run, but there's no obvious caching in my code that would cause this. Further, enabling warm up didn't change anything. I've tried tweaking many things in my code and the benchmarking configuration with no luck. Do you have any ideas?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/159", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/159/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/159/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/159/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/159", "id": 558603006, "node_id": "MDU6SXNzdWU1NTg2MDMwMDY=", "number": 159, "title": "Instructions on how to work with the results?", "user": {"login": "Nachtfeuer", "id": 11331611, "node_id": "MDQ6VXNlcjExMzMxNjEx", "avatar_url": "https://avatars0.githubusercontent.com/u/11331611?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nachtfeuer", "html_url": "https://github.com/Nachtfeuer", "followers_url": "https://api.github.com/users/Nachtfeuer/followers", "following_url": "https://api.github.com/users/Nachtfeuer/following{/other_user}", "gists_url": "https://api.github.com/users/Nachtfeuer/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nachtfeuer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nachtfeuer/subscriptions", "organizations_url": "https://api.github.com/users/Nachtfeuer/orgs", "repos_url": "https://api.github.com/users/Nachtfeuer/repos", "events_url": "https://api.github.com/users/Nachtfeuer/events{/privacy}", "received_events_url": "https://api.github.com/users/Nachtfeuer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-01T21:48:38Z", "updated_at": "2020-02-03T18:05:13Z", "closed_at": "2020-02-03T18:05:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nAbout the link: https://travis-ci.org/Nachtfeuer/engine/jobs/644884454\r\n\r\nNice how the bechmarking is displaying the measurements and I can - of course - understand\r\nif a method or function takes seconds, milli seconds, micro seconds or nano seconds to have an idea whether it's slow or fast.\r\n\r\nI see now a few green, a few white and a few red colors but it doesn't tell me whether it's bad.\r\nEven more it doesn't tell me whether the performance is faster or slower than in the last build.\r\n\r\nAny helpful comments on what I should do in that direction?\r\n\r\nKind Regards,\r\nThomas", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/157", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/157/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/157/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/157/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/157", "id": 501279801, "node_id": "MDU6SXNzdWU1MDEyNzk4MDE=", "number": 157, "title": "Dependabot couldn't fetch all your path-based dependencies", "user": {"login": "dependabot-preview[bot]", "id": 27856297, "node_id": "MDM6Qm90Mjc4NTYyOTc=", "avatar_url": "https://avatars3.githubusercontent.com/in/2141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dependabot-preview%5Bbot%5D", "html_url": "https://github.com/apps/dependabot-preview", "followers_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/followers", "following_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/repos", "events_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-10-02T05:09:40Z", "updated_at": "2019-10-03T04:57:17Z", "closed_at": "2019-10-03T04:57:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Dependabot couldn't fetch one or more of your project's path-based Python dependencies. The affected dependencies were `docs/setup.py`.\n\nTo use path-based dependencies with Dependabot the paths must be relative and resolve to a directory in this project's source code.\n\nYou can mention @dependabot in the comments below to contact the Dependabot team.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/146", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/146/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/146/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/146/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/146", "id": 398393657, "node_id": "MDU6SXNzdWUzOTgzOTM2NTc=", "number": 146, "title": "Changelog version", "user": {"login": "ofek", "id": 9677399, "node_id": "MDQ6VXNlcjk2NzczOTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/9677399?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ofek", "html_url": "https://github.com/ofek", "followers_url": "https://api.github.com/users/ofek/followers", "following_url": "https://api.github.com/users/ofek/following{/other_user}", "gists_url": "https://api.github.com/users/ofek/gists{/gist_id}", "starred_url": "https://api.github.com/users/ofek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ofek/subscriptions", "organizations_url": "https://api.github.com/users/ofek/orgs", "repos_url": "https://api.github.com/users/ofek/repos", "events_url": "https://api.github.com/users/ofek/events{/privacy}", "received_events_url": "https://api.github.com/users/ofek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-11T18:00:19Z", "updated_at": "2019-01-12T03:45:22Z", "closed_at": "2019-01-12T03:44:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Is this correct? https://github.com/ionelmc/pytest-benchmark/commit/d1ab766220c564c8fb295f3cf68c133698fce5b0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/145", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/145/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/145/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/145/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/145", "id": 398223448, "node_id": "MDU6SXNzdWUzOTgyMjM0NDg=", "number": 145, "title": "AttributeError: 'Flake8Item' object has no attribute 'funcargs' in pytest_runtest_makereport", "user": {"login": "j08lue", "id": 3404817, "node_id": "MDQ6VXNlcjM0MDQ4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/3404817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/j08lue", "html_url": "https://github.com/j08lue", "followers_url": "https://api.github.com/users/j08lue/followers", "following_url": "https://api.github.com/users/j08lue/following{/other_user}", "gists_url": "https://api.github.com/users/j08lue/gists{/gist_id}", "starred_url": "https://api.github.com/users/j08lue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/j08lue/subscriptions", "organizations_url": "https://api.github.com/users/j08lue/orgs", "repos_url": "https://api.github.com/users/j08lue/repos", "events_url": "https://api.github.com/users/j08lue/events{/privacy}", "received_events_url": "https://api.github.com/users/j08lue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-01-11T10:20:27Z", "updated_at": "2019-01-11T10:34:24Z", "closed_at": "2019-01-11T10:34:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "It seems like the [recent implementation](https://github.com/ionelmc/pytest-benchmark/commit/b8050d7931a14dea504854f4a5ff31106d1834af#diff-86fa1f257e9ae055cb3c8e65862e1e25) of `pytest_run_makereport` breaks `pytest-flake8` that passes a `Flake8Item` to `pytest_run_makereport`.\r\n\r\nhttps://github.com/ionelmc/pytest-benchmark/blob/c8b86f256c3a8993e79dcd5d6d37ef9277e7115d/src/pytest_benchmark/plugin.py#L431-L436\r\n\r\nHere is my setup:\r\n\r\n```\r\nplatform linux -- Python 3.7.1, pytest-3.10.0, py-1.7.0, pluggy-0.8.0 -- /home/travis/virtualenv/python3.7.1/bin/python\r\ncachedir: .pytest_cache\r\nbenchmark: 3.2.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\nrootdir: /home/travis/build/DHI-GRAS/terracotta, inifile: setup.cfg\r\nplugins: mypy-0.3.2, flake8-1.0.2, cov-2.6.1, benchmark-3.2.1\r\n```\r\n\r\nAnd here is the full traceback:\r\n\r\n```\r\nINTERNALERROR> Traceback (most recent call last):\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/_pytest/main.py\", line 185, in wrap_session\r\nINTERNALERROR>     session.exitstatus = doit(config, session) or 0\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/_pytest/main.py\", line 225, in _main\r\nINTERNALERROR>     config.hook.pytest_runtestloop(session=session)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/hooks.py\", line 284, in __call__\r\nINTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/manager.py\", line 67, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/manager.py\", line 61, in <lambda>\r\nINTERNALERROR>     firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/callers.py\", line 203, in _multicall\r\nINTERNALERROR>     gen.send(outcome)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/callers.py\", line 80, in get_result\r\nINTERNALERROR>     raise ex[1].with_traceback(ex[2])\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/callers.py\", line 187, in _multicall\r\nINTERNALERROR>     res = hook_impl.function(*args)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/_pytest/main.py\", line 246, in pytest_runtestloop\r\nINTERNALERROR>     item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/hooks.py\", line 284, in __call__\r\nINTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/manager.py\", line 67, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/manager.py\", line 61, in <lambda>\r\nINTERNALERROR>     firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/callers.py\", line 208, in _multicall\r\nINTERNALERROR>     return outcome.get_result()\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/callers.py\", line 80, in get_result\r\nINTERNALERROR>     raise ex[1].with_traceback(ex[2])\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/callers.py\", line 187, in _multicall\r\nINTERNALERROR>     res = hook_impl.function(*args)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/_pytest/runner.py\", line 77, in pytest_runtest_protocol\r\nINTERNALERROR>     runtestprotocol(item, nextitem=nextitem)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/_pytest/runner.py\", line 86, in runtestprotocol\r\nINTERNALERROR>     rep = call_and_report(item, \"setup\", log)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/_pytest/runner.py\", line 174, in call_and_report\r\nINTERNALERROR>     report = hook.pytest_runtest_makereport(item=item, call=call)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/hooks.py\", line 284, in __call__\r\nINTERNALERROR>     return self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/manager.py\", line 67, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/manager.py\", line 61, in <lambda>\r\nINTERNALERROR>     firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pluggy/callers.py\", line 203, in _multicall\r\nINTERNALERROR>     gen.send(outcome)\r\nINTERNALERROR>   File \"/home/travis/virtualenv/python3.7.1/lib/python3.7/site-packages/pytest_benchmark/plugin.py\", line 434, in pytest_runtest_makereport\r\nINTERNALERROR>     fixture = item.funcargs.get(\"benchmark\")\r\nINTERNALERROR> AttributeError: 'Flake8Item' object has no attribute 'funcargs'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/144", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/144/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/144/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/144/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/144", "id": 397840893, "node_id": "MDU6SXNzdWUzOTc4NDA4OTM=", "number": 144, "title": "pytest_benchmark.__version__ now includes the string '__version__'", "user": {"login": "dalonsoa", "id": 6095790, "node_id": "MDQ6VXNlcjYwOTU3OTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6095790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dalonsoa", "html_url": "https://github.com/dalonsoa", "followers_url": "https://api.github.com/users/dalonsoa/followers", "following_url": "https://api.github.com/users/dalonsoa/following{/other_user}", "gists_url": "https://api.github.com/users/dalonsoa/gists{/gist_id}", "starred_url": "https://api.github.com/users/dalonsoa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dalonsoa/subscriptions", "organizations_url": "https://api.github.com/users/dalonsoa/orgs", "repos_url": "https://api.github.com/users/dalonsoa/repos", "events_url": "https://api.github.com/users/dalonsoa/events{/privacy}", "received_events_url": "https://api.github.com/users/dalonsoa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-01-10T13:54:38Z", "updated_at": "2019-01-10T15:37:14Z", "closed_at": "2019-01-10T15:37:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "Contrary to the previous versions, now `pytest_benchmark.__version__` contains `\"__version__ = '3.2.0'\"` rather than just `'3.2.0'` as it has been until now. \r\n\r\nThis seems a bit weird and I take it is a bug. Otherwise, is there any reason for this change?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/143", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/143/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/143/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/143/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/143", "id": 397153957, "node_id": "MDU6SXNzdWUzOTcxNTM5NTc=", "number": 143, "title": "test_commit_info_error fails with new git", "user": {"login": "felixonmars", "id": 1006477, "node_id": "MDQ6VXNlcjEwMDY0Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1006477?v=4", "gravatar_id": "", "url": "https://api.github.com/users/felixonmars", "html_url": "https://github.com/felixonmars", "followers_url": "https://api.github.com/users/felixonmars/followers", "following_url": "https://api.github.com/users/felixonmars/following{/other_user}", "gists_url": "https://api.github.com/users/felixonmars/gists{/gist_id}", "starred_url": "https://api.github.com/users/felixonmars/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/felixonmars/subscriptions", "organizations_url": "https://api.github.com/users/felixonmars/orgs", "repos_url": "https://api.github.com/users/felixonmars/repos", "events_url": "https://api.github.com/users/felixonmars/events{/privacy}", "received_events_url": "https://api.github.com/users/felixonmars/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-01-09T00:34:07Z", "updated_at": "2019-01-10T15:37:14Z", "closed_at": "2019-01-10T15:37:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "With git version 2.20.1:\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n____________________________ test_commit_info_error ____________________________\r\n/build/python-pytest-benchmark/src/pytest-benchmark-3.2.0/tests/test_utils.py:123: in test_commit_info_error\r\n    assert info['error'].lower() == 'CalledProcessError(128, ' \\\r\nE   assert \"calledproces...ot set).\\\\n')\" == \"calledprocess...s): .git\\\\n')\"\r\nE     Skipping 51 identical leading characters in diff, use -v to show\r\nE     - y (or any parent up to mount point /)\\nstopping at filesystem boundary (git_discovery_across_filesystem not set).\\n')\r\nE     + y (or any of the parent directories): .git\\n')\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/142", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/142/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/142/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/142/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/142", "id": 396772364, "node_id": "MDU6SXNzdWUzOTY3NzIzNjQ=", "number": 142, "title": "Error with pytest < 3.8.0: cannot import name 'PytestWarning'", "user": {"login": "bhavin192", "id": 5154532, "node_id": "MDQ6VXNlcjUxNTQ1MzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/5154532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bhavin192", "html_url": "https://github.com/bhavin192", "followers_url": "https://api.github.com/users/bhavin192/followers", "following_url": "https://api.github.com/users/bhavin192/following{/other_user}", "gists_url": "https://api.github.com/users/bhavin192/gists{/gist_id}", "starred_url": "https://api.github.com/users/bhavin192/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bhavin192/subscriptions", "organizations_url": "https://api.github.com/users/bhavin192/orgs", "repos_url": "https://api.github.com/users/bhavin192/repos", "events_url": "https://api.github.com/users/bhavin192/events{/privacy}", "received_events_url": "https://api.github.com/users/bhavin192/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-08T06:51:30Z", "updated_at": "2019-01-10T15:37:14Z", "closed_at": "2019-01-10T15:37:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "When pytest version is < 3.8.0 it throws this error, \r\n```\r\n  File \"/home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages/pytest_benchmark/logger.py\", line 8, in <module>\r\n    from pytest import PytestWarning\r\nImportError: cannot import name 'PytestWarning'\r\n```\r\n`PytestWarning` was added in the version 3.8.0 of pytest \r\nhttps://github.com/pytest-dev/pytest/commit/0100f61b62411621f8c5f886221bcbbe6f094a16#diff-890658eb4f24166cc6fe257d643951ad\r\n\r\nComplete log: https://travis-ci.org/jaegertracing/jaeger-client-python/jobs/476485428#L801-L803\r\npytest version 3.7.4: https://travis-ci.org/jaegertracing/jaeger-client-python/jobs/476485428#L552\r\n\r\nI think the dependency should be >= 3.8 instead of 3.6 here \r\nhttps://github.com/ionelmc/pytest-benchmark/blob/9d6b12bbbeb056fec46afce5bf6aa56bbbd12db4/setup.py#L70\r\n\r\nI would like to make the change if we are planning to do that :) \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/141", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/141/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/141/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/141/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/141", "id": 396282389, "node_id": "MDU6SXNzdWUzOTYyODIzODk=", "number": 141, "title": "Error with pytest==4.1.0", "user": {"login": "joshbode", "id": 878107, "node_id": "MDQ6VXNlcjg3ODEwNw==", "avatar_url": "https://avatars3.githubusercontent.com/u/878107?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joshbode", "html_url": "https://github.com/joshbode", "followers_url": "https://api.github.com/users/joshbode/followers", "following_url": "https://api.github.com/users/joshbode/following{/other_user}", "gists_url": "https://api.github.com/users/joshbode/gists{/gist_id}", "starred_url": "https://api.github.com/users/joshbode/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joshbode/subscriptions", "organizations_url": "https://api.github.com/users/joshbode/orgs", "repos_url": "https://api.github.com/users/joshbode/repos", "events_url": "https://api.github.com/users/joshbode/events{/privacy}", "received_events_url": "https://api.github.com/users/joshbode/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-01-06T18:33:15Z", "updated_at": "2019-01-07T16:29:53Z", "closed_at": "2019-01-06T18:38:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI've just upgraded to `pytest==4.1.0` today and I'm getting an error with `pytest-benchmark==3.1.1`:\r\n\r\n```\r\nINTERNALERROR> Traceback (most recent call last):\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/_pytest/main.py\", line 199, in wrap_session\r\nINTERNALERROR>     config._do_configure()\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/_pytest/config/__init__.py\", line 636, in _do_configure\r\nINTERNALERROR>     self.hook.pytest_configure.call_historic(kwargs=dict(config=self))\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pluggy/hooks.py\", line 306, in call_historic\r\nINTERNALERROR>     res = self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pluggy/manager.py\", line 67, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pluggy/manager.py\", line 61, in <lambda>\r\nINTERNALERROR>     firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pluggy/callers.py\", line 208, in _multicall\r\nINTERNALERROR>     return outcome.get_result()\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pluggy/callers.py\", line 80, in get_result\r\nINTERNALERROR>     raise ex[1].with_traceback(ex[2])\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pluggy/callers.py\", line 187, in _multicall\r\nINTERNALERROR>     res = hook_impl.function(*args)\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pytest_benchmark/plugin.py\", line 427, in pytest_configure\r\nINTERNALERROR>     bs = config._benchmarksession = BenchmarkSession(config)\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pytest_benchmark/session.py\", line 31, in __init__\r\nINTERNALERROR>     self.logger = Logger(self.verbose, config)\r\nINTERNALERROR>   File \"/Users/josh/.virtualenvs/default/lib/python3.7/site-packages/pytest_benchmark/logger.py\", line 16, in __init__\r\nINTERNALERROR>     self.pytest_warn = config.warn\r\nINTERNALERROR> AttributeError: 'Config' object has no attribute 'warn'\r\n```\r\n\r\nIf I downgrade back to `pytest==4.0.2` there is no error.\r\n\r\nI'm on Python 3.7 on macOS", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/139", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/139/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/139/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/139/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/139", "id": 395380665, "node_id": "MDU6SXNzdWUzOTUzODA2NjU=", "number": 139, "title": "BenchmarkFixture._timer isn't callable when wrapped by NameWrapper", "user": {"login": "cygnusv", "id": 2564234, "node_id": "MDQ6VXNlcjI1NjQyMzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/2564234?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cygnusv", "html_url": "https://github.com/cygnusv", "followers_url": "https://api.github.com/users/cygnusv/followers", "following_url": "https://api.github.com/users/cygnusv/following{/other_user}", "gists_url": "https://api.github.com/users/cygnusv/gists{/gist_id}", "starred_url": "https://api.github.com/users/cygnusv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cygnusv/subscriptions", "organizations_url": "https://api.github.com/users/cygnusv/orgs", "repos_url": "https://api.github.com/users/cygnusv/repos", "events_url": "https://api.github.com/users/cygnusv/events{/privacy}", "received_events_url": "https://api.github.com/users/cygnusv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-02T21:07:57Z", "updated_at": "2019-01-03T03:07:41Z", "closed_at": "2019-01-03T03:07:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting this error when trying to benchmark my code using `benchmark.pedantic`:\r\n```\r\n../../../.local/share/virtualenvs/pyUmbral-SXnxXn8S/lib/python3.7/site-packages/pytest_benchmark/fixture.py:134: in pedantic\r\n    warmup_rounds=warmup_rounds, iterations=iterations)\r\n../../../.local/share/virtualenvs/pyUmbral-SXnxXn8S/lib/python3.7/site-packages/pytest_benchmark/fixture.py:214: in _raw_pedantic\r\n    duration = runner(loops_range)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nloops_range = range(0, 20), timer = NameWrapper(<built-in function perf_counter>)\r\n\r\n    def runner(loops_range, timer=self._timer):\r\n        gc_enabled = gc.isenabled()\r\n        if self._disable_gc:\r\n            gc.disable()\r\n        tracer = sys.gettrace()\r\n        sys.settrace(None)\r\n        try:\r\n            if loops_range:\r\n>               start = timer()\r\nE               TypeError: 'NameWrapper' object is not callable\r\n```\r\n\r\nThe problem is that the `NameWrapper` class in `utils.py` doesn't allow to `__call__` the wrapped object, in this case `BenchmarkFixture._timer`, which is indeed callable.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/137", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/137/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/137/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/137/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/137", "id": 392942885, "node_id": "MDU6SXNzdWUzOTI5NDI4ODU=", "number": 137, "title": "DeprecationWarning: 'U' mode is deprecated", "user": {"login": "stanislavlevin", "id": 31205609, "node_id": "MDQ6VXNlcjMxMjA1NjA5", "avatar_url": "https://avatars2.githubusercontent.com/u/31205609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stanislavlevin", "html_url": "https://github.com/stanislavlevin", "followers_url": "https://api.github.com/users/stanislavlevin/followers", "following_url": "https://api.github.com/users/stanislavlevin/following{/other_user}", "gists_url": "https://api.github.com/users/stanislavlevin/gists{/gist_id}", "starred_url": "https://api.github.com/users/stanislavlevin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stanislavlevin/subscriptions", "organizations_url": "https://api.github.com/users/stanislavlevin/orgs", "repos_url": "https://api.github.com/users/stanislavlevin/repos", "events_url": "https://api.github.com/users/stanislavlevin/events{/privacy}", "received_events_url": "https://api.github.com/users/stanislavlevin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-12-20T08:42:50Z", "updated_at": "2018-12-20T21:01:02Z", "closed_at": "2018-12-20T21:01:02Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "There is a deprecation about 'U' open mode (https://bugs.python.org/issue15204).\r\n\r\nFor example, running pytest-benchmark's tests against Python3.6 I have:\r\n```\r\n  /usr/lib64/python3.6/pathlib.py:1181: DeprecationWarning: 'U' mode is deprecated\r\n    opener=self._opener)\r\n\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n============ 203 passed, 9 skipped, 930 warnings in 435.37 seconds =============\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/135", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/135/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/135/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/135/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/135", "id": 392532630, "node_id": "MDU6SXNzdWUzOTI1MzI2MzA=", "number": 135, "title": "Test \"test_commit_info_error\" fails", "user": {"login": "stanislavlevin", "id": 31205609, "node_id": "MDQ6VXNlcjMxMjA1NjA5", "avatar_url": "https://avatars2.githubusercontent.com/u/31205609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stanislavlevin", "html_url": "https://github.com/stanislavlevin", "followers_url": "https://api.github.com/users/stanislavlevin/followers", "following_url": "https://api.github.com/users/stanislavlevin/following{/other_user}", "gists_url": "https://api.github.com/users/stanislavlevin/gists{/gist_id}", "starred_url": "https://api.github.com/users/stanislavlevin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stanislavlevin/subscriptions", "organizations_url": "https://api.github.com/users/stanislavlevin/orgs", "repos_url": "https://api.github.com/users/stanislavlevin/repos", "events_url": "https://api.github.com/users/stanislavlevin/events{/privacy}", "received_events_url": "https://api.github.com/users/stanislavlevin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-12-19T10:23:14Z", "updated_at": "2018-12-20T20:02:04Z", "closed_at": "2018-12-20T20:02:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I run pytest-benchmark's tests during RPM build.\r\n\"test_commit_info_error\" fails with:\r\n```\r\n___________________________________ test_commit_info_error ____________________________________\r\n/usr/src/RPM/BUILD/python-module-pytest-benchmark-3.1.1/tests/test_utils.py:123: in test_commit_info_error\r\n    assert info['error'] == 'CalledProcessError(128, ' \\\r\nE   assert \"CalledProces...s): .git\\\\n')\" == \"CalledProcess...s): .git\\\\n')\"\r\nE     - CalledProcessError(128, 'fatal: not a git repository (or any of the parent directories): .git\\\r\nE     ?                                 ^\r\nE     + CalledProcessError(128, 'fatal: Not a git repository (or any of the parent directories): .git\\\r\nE     ?                                 ^\r\nE       ')\r\n```\r\nI have no idea why we should check for case sensitive.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/133", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/133/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/133/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/133/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/133", "id": 392526464, "node_id": "MDU6SXNzdWUzOTI1MjY0NjQ=", "number": 133, "title": "Test \"test_parse_elasticsearch_storage\" fails", "user": {"login": "stanislavlevin", "id": 31205609, "node_id": "MDQ6VXNlcjMxMjA1NjA5", "avatar_url": "https://avatars2.githubusercontent.com/u/31205609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stanislavlevin", "html_url": "https://github.com/stanislavlevin", "followers_url": "https://api.github.com/users/stanislavlevin/followers", "following_url": "https://api.github.com/users/stanislavlevin/following{/other_user}", "gists_url": "https://api.github.com/users/stanislavlevin/gists{/gist_id}", "starred_url": "https://api.github.com/users/stanislavlevin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stanislavlevin/subscriptions", "organizations_url": "https://api.github.com/users/stanislavlevin/orgs", "repos_url": "https://api.github.com/users/stanislavlevin/repos", "events_url": "https://api.github.com/users/stanislavlevin/events{/privacy}", "received_events_url": "https://api.github.com/users/stanislavlevin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-12-19T10:08:22Z", "updated_at": "2018-12-20T19:46:52Z", "closed_at": "2018-12-20T19:46:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I run pytest-benchmark's tests during RPM build.\r\n\"test_parse_elasticsearch_storage\" fails with:\r\n```\r\n______________________________ test_parse_elasticsearch_storage _______________________________\r\ntests/test_utils.py:183: in test_parse_elasticsearch_storage\r\n    assert parse_elasticsearch_storage(\"http://localhost:9200\") == (\r\nE   AssertionError: assert (['http://loc...chmark-3.1.1') == (['http://loca...st-benchmark')\r\nE     At index 3 diff: 'python-module-pytest-benchmark-3.1.1' != 'pytest-benchmark'\r\nE     Full diff:\r\nE     + (['http://localhost:9200'], 'benchmark', 'benchmark', 'pytest-benchmark')\r\nE     - (['http://localhost:9200'],\r\nE     -  'benchmark',\r\nE     -  'benchmark',\r\nE     -  'python-module-pytest-benchmark-3.1.1')\r\n```\r\nBasename for directory as you can see is ```python-module-pytest-benchmark-3.1.1```, but the expected one is ```pytest-benchmark```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/132", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/132/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/132/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/132/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/132", "id": 391042183, "node_id": "MDU6SXNzdWUzOTEwNDIxODM=", "number": 132, "title": "Comparing existing benchmarks without running the benchmark again", "user": {"login": "mariokostelac", "id": 1917451, "node_id": "MDQ6VXNlcjE5MTc0NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1917451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariokostelac", "html_url": "https://github.com/mariokostelac", "followers_url": "https://api.github.com/users/mariokostelac/followers", "following_url": "https://api.github.com/users/mariokostelac/following{/other_user}", "gists_url": "https://api.github.com/users/mariokostelac/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariokostelac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariokostelac/subscriptions", "organizations_url": "https://api.github.com/users/mariokostelac/orgs", "repos_url": "https://api.github.com/users/mariokostelac/repos", "events_url": "https://api.github.com/users/mariokostelac/events{/privacy}", "received_events_url": "https://api.github.com/users/mariokostelac/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1178438242, "node_id": "MDU6TGFiZWwxMTc4NDM4MjQy", "url": "https://api.github.com/repos/ionelmc/pytest-benchmark/labels/documentation", "name": "documentation", "color": "ffffff", "default": true, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2", "html_url": "https://github.com/ionelmc/pytest-benchmark/milestone/2", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2/labels", "id": 1303156, "node_id": "MDk6TWlsZXN0b25lMTMwMzE1Ng==", "number": 2, "title": "v3.2.0", "description": "", "creator": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "open_issues": 2, "closed_issues": 4, "state": "open", "created_at": "2015-09-13T17:46:53Z", "updated_at": "2019-01-10T15:44:58Z", "due_on": null, "closed_at": null}, "comments": 4, "created_at": "2018-12-14T09:59:24Z", "updated_at": "2019-01-07T01:33:16Z", "closed_at": "2019-01-07T01:33:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am wondering whether there is a way to compare existing runs without running the benchmark again.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/128", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/128/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/128/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/128/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/128", "id": 378479208, "node_id": "MDU6SXNzdWUzNzg0NzkyMDg=", "number": 128, "title": "Docs Request: Setup functions", "user": {"login": "one-t", "id": 28987580, "node_id": "MDQ6VXNlcjI4OTg3NTgw", "avatar_url": "https://avatars3.githubusercontent.com/u/28987580?v=4", "gravatar_id": "", "url": "https://api.github.com/users/one-t", "html_url": "https://github.com/one-t", "followers_url": "https://api.github.com/users/one-t/followers", "following_url": "https://api.github.com/users/one-t/following{/other_user}", "gists_url": "https://api.github.com/users/one-t/gists{/gist_id}", "starred_url": "https://api.github.com/users/one-t/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/one-t/subscriptions", "organizations_url": "https://api.github.com/users/one-t/orgs", "repos_url": "https://api.github.com/users/one-t/repos", "events_url": "https://api.github.com/users/one-t/events{/privacy}", "received_events_url": "https://api.github.com/users/one-t/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1178438242, "node_id": "MDU6TGFiZWwxMTc4NDM4MjQy", "url": "https://api.github.com/repos/ionelmc/pytest-benchmark/labels/documentation", "name": "documentation", "color": "ffffff", "default": true, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2", "html_url": "https://github.com/ionelmc/pytest-benchmark/milestone/2", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2/labels", "id": 1303156, "node_id": "MDk6TWlsZXN0b25lMTMwMzE1Ng==", "number": 2, "title": "v3.2.0", "description": "", "creator": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "open_issues": 2, "closed_issues": 4, "state": "open", "created_at": "2015-09-13T17:46:53Z", "updated_at": "2019-01-10T15:44:58Z", "due_on": null, "closed_at": null}, "comments": 0, "created_at": "2018-11-07T21:36:04Z", "updated_at": "2019-01-07T01:33:16Z", "closed_at": "2019-01-07T01:33:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "The current documentation doesn't provide an example of a working setup function for use with pedantic mode. This would be extremely helpful to newcomers getting started with pytest-benchmark.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/125", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/125/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/125/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/125/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/125", "id": 373343784, "node_id": "MDU6SXNzdWUzNzMzNDM3ODQ=", "number": 125, "title": "\"Benchmark fixture was not used at all in this test!\" for skipped tests", "user": {"login": "The-Compiler", "id": 625793, "node_id": "MDQ6VXNlcjYyNTc5Mw==", "avatar_url": "https://avatars0.githubusercontent.com/u/625793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/The-Compiler", "html_url": "https://github.com/The-Compiler", "followers_url": "https://api.github.com/users/The-Compiler/followers", "following_url": "https://api.github.com/users/The-Compiler/following{/other_user}", "gists_url": "https://api.github.com/users/The-Compiler/gists{/gist_id}", "starred_url": "https://api.github.com/users/The-Compiler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/The-Compiler/subscriptions", "organizations_url": "https://api.github.com/users/The-Compiler/orgs", "repos_url": "https://api.github.com/users/The-Compiler/repos", "events_url": "https://api.github.com/users/The-Compiler/events{/privacy}", "received_events_url": "https://api.github.com/users/The-Compiler/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-24T07:34:55Z", "updated_at": "2019-01-02T20:54:36Z", "closed_at": "2019-01-02T20:54:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "With pytest 3.9.2 and pytest-bdd 3.0.0, when a test is skipped via `pytest.skip` like this:\r\n\r\n```python\r\nimport pytest\r\n\r\ndef test_foo(benchmark):\r\n    pytest.skip('bla')\r\n    benchmark(lambda: None)\r\n```\r\n\r\nI get:\r\n\r\n```\r\ntest_bench.py::test_foo\r\n  Benchmark fixture was not used at all in this test!\r\n```\r\n\r\nwhich is a bit odd, as that's what I'd expect when skipping the test.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/124", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/124/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/124/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/124/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/124", "id": 373338320, "node_id": "MDU6SXNzdWUzNzMzMzgzMjA=", "number": 124, "title": "Node.warn(code, message) form has been deprecated, use Node.warn(warning_instance) instead", "user": {"login": "The-Compiler", "id": 625793, "node_id": "MDQ6VXNlcjYyNTc5Mw==", "avatar_url": "https://avatars0.githubusercontent.com/u/625793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/The-Compiler", "html_url": "https://github.com/The-Compiler", "followers_url": "https://api.github.com/users/The-Compiler/followers", "following_url": "https://api.github.com/users/The-Compiler/following{/other_user}", "gists_url": "https://api.github.com/users/The-Compiler/gists{/gist_id}", "starred_url": "https://api.github.com/users/The-Compiler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/The-Compiler/subscriptions", "organizations_url": "https://api.github.com/users/The-Compiler/orgs", "repos_url": "https://api.github.com/users/The-Compiler/repos", "events_url": "https://api.github.com/users/The-Compiler/events{/privacy}", "received_events_url": "https://api.github.com/users/The-Compiler/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-10-24T07:18:44Z", "updated_at": "2019-01-07T16:17:31Z", "closed_at": "2019-01-02T20:54:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "With pytest 3.9.2 and pytest-bdd 3.0.0 I get this:\r\n\r\n```\r\n________________ ERROR at teardown of test_match_benchmark[webkit] _________________\r\n\r\ntp = <class '_pytest.warning_types.RemovedInPytest4Warning'>, value = None\r\ntb = None\r\n\r\n    def reraise(tp, value, tb=None):\r\n        try:\r\n            if value is None:\r\n                value = tp()\r\n            if value.__traceback__ is not tb:\r\n                raise value.with_traceback(tb)\r\n>           raise value\r\n\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/six.py:693: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n.tox/py37-pyqt511/lib/python3.7/site-packages/six.py:693: in reraise\r\n    raise value\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/six.py:693: in reraise\r\n    raise value\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/pytest_benchmark/fixture.py:252: in _cleanup\r\n    warner=self._warner, suspend=True)\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/pytest_benchmark/logger.py:40: in warn\r\n    warner(code=code, message=text)\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/six.py:693: in reraise\r\n    raise value\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/six.py:693: in reraise\r\n    raise value\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/six.py:693: in reraise\r\n    raise value\r\n.tox/py37-pyqt511/lib/python3.7/site-packages/pytest_benchmark/fixture.py:252: in _cleanup\r\n    warner=self._warner, suspend=True)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <pytest_benchmark.logger.Logger object at 0x7f5e2c139400>\r\ncode = 'BENCHMARK-U1', text = 'Benchmark fixture was not used at all in this test!'\r\nwarner = <bound method Node.warn of <Function 'test_show_benchmark[webkit]'>>\r\nsuspend = True, fslocation = None\r\n\r\n    def warn(self, code, text, warner=None, suspend=False, fslocation=None):\r\n        if self.verbose:\r\n            if suspend and self.capman:\r\n                self.capman.suspendcapture(in_=True)\r\n            self.term.line(\"\")\r\n            self.term.sep(\"-\", red=True, bold=True)\r\n            self.term.write(\" WARNING: \", red=True, bold=True)\r\n            self.term.line(text, red=True)\r\n            self.term.sep(\"-\", red=True, bold=True)\r\n            if suspend and self.capman:\r\n                self.capman.resumecapture()\r\n        if warner is None:\r\n            warner = self.pytest_warn\r\n        if fslocation and self.pytest_warn_has_fslocation:\r\n            warner(code=code, message=text, fslocation=fslocation)\r\n        else:\r\n>           warner(code=code, message=text)\r\nE           _pytest.warning_types.RemovedInPytest4Warning: Node.warn(code, message) form has been deprecated, use Node.warn(warning_instance) instead.\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/122", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/122/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/122/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/122/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/122", "id": 365558064, "node_id": "MDU6SXNzdWUzNjU1NTgwNjQ=", "number": 122, "title": "MarkInfo objects are deprecated since Pytest-3.6.0", "user": {"login": "daa", "id": 167235, "node_id": "MDQ6VXNlcjE2NzIzNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/167235?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daa", "html_url": "https://github.com/daa", "followers_url": "https://api.github.com/users/daa/followers", "following_url": "https://api.github.com/users/daa/following{/other_user}", "gists_url": "https://api.github.com/users/daa/gists{/gist_id}", "starred_url": "https://api.github.com/users/daa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daa/subscriptions", "organizations_url": "https://api.github.com/users/daa/orgs", "repos_url": "https://api.github.com/users/daa/repos", "events_url": "https://api.github.com/users/daa/events{/privacy}", "received_events_url": "https://api.github.com/users/daa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-01T17:13:49Z", "updated_at": "2019-01-02T19:44:47Z", "closed_at": "2019-01-02T19:44:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "``pytest-3.6.0`` issues following warning:\r\n```\r\nsite-packages/pytest_benchmark/plugin.py:392: RemovedInPytest4Warning: MarkInfo objects are deprecated as they contain merged marks which are hard to deal with correctly.\r\nPlease use node.get_closest_marker(name) or node.iter_markers(name).\r\nDocs: https://docs.pytest.org/en/latest/mark.html#updating-code\r\n  options = marker.kwargs if marker else {}\r\n```\r\n- Release notes: https://docs.pytest.org/en/latest/changelog.html#pytest-3-6-0-2018-05-23\r\n- Code update information: https://docs.pytest.org/en/latest/mark.html#marker-revamp-and-iteration\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/119", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/119/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/119/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/119/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/119", "id": 351393794, "node_id": "MDU6SXNzdWUzNTEzOTM3OTQ=", "number": 119, "title": "New release", "user": {"login": "ofek", "id": 9677399, "node_id": "MDQ6VXNlcjk2NzczOTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/9677399?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ofek", "html_url": "https://github.com/ofek", "followers_url": "https://api.github.com/users/ofek/followers", "following_url": "https://api.github.com/users/ofek/following{/other_user}", "gists_url": "https://api.github.com/users/ofek/gists{/gist_id}", "starred_url": "https://api.github.com/users/ofek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ofek/subscriptions", "organizations_url": "https://api.github.com/users/ofek/orgs", "repos_url": "https://api.github.com/users/ofek/repos", "events_url": "https://api.github.com/users/ofek/events{/privacy}", "received_events_url": "https://api.github.com/users/ofek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2", "html_url": "https://github.com/ionelmc/pytest-benchmark/milestone/2", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2/labels", "id": 1303156, "node_id": "MDk6TWlsZXN0b25lMTMwMzE1Ng==", "number": 2, "title": "v3.2.0", "description": "", "creator": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "open_issues": 2, "closed_issues": 4, "state": "open", "created_at": "2015-09-13T17:46:53Z", "updated_at": "2019-01-10T15:44:58Z", "due_on": null, "closed_at": null}, "comments": 7, "created_at": "2018-08-16T22:42:04Z", "updated_at": "2019-01-10T15:44:58Z", "closed_at": "2019-01-10T15:44:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "@ionelmc Could we please get a new version up on PyPI? :shipit: ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/118", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/118/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/118/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/118/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/118", "id": 343859141, "node_id": "MDU6SXNzdWUzNDM4NTkxNDE=", "number": 118, "title": "Running benchmark for all tests?", "user": {"login": "reece", "id": 109453, "node_id": "MDQ6VXNlcjEwOTQ1Mw==", "avatar_url": "https://avatars2.githubusercontent.com/u/109453?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reece", "html_url": "https://github.com/reece", "followers_url": "https://api.github.com/users/reece/followers", "following_url": "https://api.github.com/users/reece/following{/other_user}", "gists_url": "https://api.github.com/users/reece/gists{/gist_id}", "starred_url": "https://api.github.com/users/reece/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reece/subscriptions", "organizations_url": "https://api.github.com/users/reece/orgs", "repos_url": "https://api.github.com/users/reece/repos", "events_url": "https://api.github.com/users/reece/events{/privacy}", "received_events_url": "https://api.github.com/users/reece/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-24T02:19:07Z", "updated_at": "2018-08-10T04:12:33Z", "closed_at": "2018-08-10T04:11:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'd like to benchmark all tests. As far as I can tell from the docs, running benchmark requires writing a wrapper using the provided fixture. Is there a simpler way?\r\n\r\nIdeally, I'd be able to provide a regexp of tests to benchmark and/or a simple decorator to wrap classes for me.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/112", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/112/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/112/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/112/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/112", "id": 333059299, "node_id": "MDU6SXNzdWUzMzMwNTkyOTk=", "number": 112, "title": "Output of --help is inconsistent b/w python versions", "user": {"login": "nehaljwani", "id": 1779189, "node_id": "MDQ6VXNlcjE3NzkxODk=", "avatar_url": "https://avatars0.githubusercontent.com/u/1779189?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nehaljwani", "html_url": "https://github.com/nehaljwani", "followers_url": "https://api.github.com/users/nehaljwani/followers", "following_url": "https://api.github.com/users/nehaljwani/following{/other_user}", "gists_url": "https://api.github.com/users/nehaljwani/gists{/gist_id}", "starred_url": "https://api.github.com/users/nehaljwani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nehaljwani/subscriptions", "organizations_url": "https://api.github.com/users/nehaljwani/orgs", "repos_url": "https://api.github.com/users/nehaljwani/repos", "events_url": "https://api.github.com/users/nehaljwani/events{/privacy}", "received_events_url": "https://api.github.com/users/nehaljwani/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-17T12:32:16Z", "updated_at": "2019-01-02T16:33:34Z", "closed_at": "2019-01-02T16:33:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "With python3:\r\n```\r\n$ pytest-benchmark --help\r\nusage: py.test-benchmark help [-h] [command]\r\n\r\nDisplay help and exit.\r\n\r\npositional arguments:\r\n  command\r\n\r\noptional arguments:\r\n  -h, --help  show this help message and exit\r\n```\r\n\r\nWith python2:\r\n```py\r\n$ pytest-benchmark --help\r\nusage: py.test-benchmark [-h [COMMAND]] [--storage URI] [--netrc [NETRC]]\r\n                         [--verbose]\r\n                         {help,list,compare} ...\r\npy.test-benchmark: error: too few arguments\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/107", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/107/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/107/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/107/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/107", "id": 323860779, "node_id": "MDU6SXNzdWUzMjM4NjA3Nzk=", "number": 107, "title": "Spurious FileExistsError", "user": {"login": "alexrudy", "id": 427688, "node_id": "MDQ6VXNlcjQyNzY4OA==", "avatar_url": "https://avatars0.githubusercontent.com/u/427688?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexrudy", "html_url": "https://github.com/alexrudy", "followers_url": "https://api.github.com/users/alexrudy/followers", "following_url": "https://api.github.com/users/alexrudy/following{/other_user}", "gists_url": "https://api.github.com/users/alexrudy/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexrudy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexrudy/subscriptions", "organizations_url": "https://api.github.com/users/alexrudy/orgs", "repos_url": "https://api.github.com/users/alexrudy/repos", "events_url": "https://api.github.com/users/alexrudy/events{/privacy}", "received_events_url": "https://api.github.com/users/alexrudy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-05-17T03:34:46Z", "updated_at": "2019-01-02T16:33:33Z", "closed_at": "2019-01-02T16:33:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "On https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/storage/file.py#L15, the check for `exists` followed by the call to mkdir() exhibits a race condition, which I sometimes manage to provoke with [detox](https://github.com/tox-dev/detox). A better pattern would be to try the .mkdir() and catch the appropriate OSError.\r\n\r\nThis is hard to reproduce (yay race conditions) but I've seen it several times now, and it happens when benchmark creates the storage directory even if it won't put anything there, so it is hard to avoid.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/104", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/104/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/104/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/104/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/104", "id": 317613338, "node_id": "MDU6SXNzdWUzMTc2MTMzMzg=", "number": 104, "title": "\"not found!\" spam for folders out of version control", "user": {"login": "ml31415", "id": 2386612, "node_id": "MDQ6VXNlcjIzODY2MTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/2386612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ml31415", "html_url": "https://github.com/ml31415", "followers_url": "https://api.github.com/users/ml31415/followers", "following_url": "https://api.github.com/users/ml31415/following{/other_user}", "gists_url": "https://api.github.com/users/ml31415/gists{/gist_id}", "starred_url": "https://api.github.com/users/ml31415/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ml31415/subscriptions", "organizations_url": "https://api.github.com/users/ml31415/orgs", "repos_url": "https://api.github.com/users/ml31415/repos", "events_url": "https://api.github.com/users/ml31415/events{/privacy}", "received_events_url": "https://api.github.com/users/ml31415/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-04-25T12:48:36Z", "updated_at": "2018-06-06T10:47:10Z", "closed_at": "2018-06-06T10:47:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Calling pytest in folders out of version control leads to an very ugly \"not found!\" line, when pytest-benchmark is installed.\r\n\r\n```\r\nmichael@computer:~/somefolder# pytest\r\nnot found!\r\n========================================== test session starts ==========================================\r\nplatform linux2 -- Python 2.7.12, pytest-3.3.2, py-1.5.2, pluggy-0.6.0\r\nbenchmark: 3.1.1 (defaults: timer=time.time disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\n```\r\n\r\nThe issue is the call to `hg` in https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/utils.py#L136 I suppose, attaching `--quiet` should already fix the issue.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/102", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/102/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/102/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/102/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/102", "id": 301023449, "node_id": "MDU6SXNzdWUzMDEwMjM0NDk=", "number": 102, "title": "[Bug] Running with --benchmark-histogram on a specific file causes misnaming", "user": {"login": "matthewfeickert", "id": 5142394, "node_id": "MDQ6VXNlcjUxNDIzOTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/5142394?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthewfeickert", "html_url": "https://github.com/matthewfeickert", "followers_url": "https://api.github.com/users/matthewfeickert/followers", "following_url": "https://api.github.com/users/matthewfeickert/following{/other_user}", "gists_url": "https://api.github.com/users/matthewfeickert/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthewfeickert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthewfeickert/subscriptions", "organizations_url": "https://api.github.com/users/matthewfeickert/orgs", "repos_url": "https://api.github.com/users/matthewfeickert/repos", "events_url": "https://api.github.com/users/matthewfeickert/events{/privacy}", "received_events_url": "https://api.github.com/users/matthewfeickert/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-28T13:27:58Z", "updated_at": "2018-06-06T11:59:22Z", "closed_at": "2018-06-06T11:59:22Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "# Description\r\n\r\nAs outlined in the [command line options](http://pytest-benchmark.readthedocs.io/en/stable/usage.html#commandline-options) for `--benchmark-histogram`\r\n>--benchmark-histogram=FILENAME-PREFIX\r\n> Plot graphs of min/max/avg/stddev over time in FILENAME-PREFIX-test_name.svg. If FILENAME-PREFIX contains slashes (\u2018/\u2019) then directories will be created. Default: \u2018benchmark_\\<date>_\\<time>\u2019, example: \u2018benchmark_20150811_041632\u2019.\r\n\r\nto pass a name prefix other than the default one should pass that after the `--benchmark-histogram` flag. However, if one wants to run on a specific test file _and_ use the default naming scheme this causes the file path to be interpreted as the file prefix and all tests are run.\r\n\r\nWould it be reasonable to require that for a non-default name to be used that the `=` would be required?\r\n\r\n# Expected Behavior\r\n\r\n```\r\npytest --benchmark-histogram tests/test_benchmark.py\r\n```\r\nThis would run the the tests in `test_benchmark.py` and then save the histogram with the default name. So after running this you would expect to find\r\n```\r\n$ ls *.svg\r\nbenchmark_<date>_<time>.svg\r\n```\r\n\r\nThis works as expected\r\n```\r\n$ pytest --benchmark-histogram=example tests/test_benchmark.py\r\n$ ls example.svg \r\nexample.svg\r\n```\r\n\r\n# Actual Behavior\r\n\r\n```\r\npytest --benchmark-histogram tests/test_benchmark.py\r\n```\r\nThis interprets `tests/test_benchmark.py` as the desired name of the file and runs all the tests instead of only the tests in `tests/test_benchmark.py` \r\n```\r\n$ pytest --benchmark-histogram tests/test_benchmark.py\r\n$ ls *.svg\r\n$ ls tests/test_benchmark.py.svg \r\ntests/test_benchmark.py.svg\r\n```\r\n# Operating system name and version\r\n\r\nOS: Ubuntu 16.04 LTS\r\n```\r\n$ uname -a\r\nLinux ThinkPad-Edge-E540 4.4.0-116-generic #140-Ubuntu SMP Mon Feb 12 21:23:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n```\r\nVersion of `pytest-benchmark`: [v3.1.1](https://github.com/ionelmc/pytest-benchmark/releases/tag/v3.1.1)\r\n```\r\n$ python --version\r\nPython 3.6.4 :: Anaconda, Inc.\r\n$ py.test --version\r\nThis is pytest version 3.4.0, imported from /home/mcf/anaconda3/envs/pyhf/lib/python3.6/site-packages/pytest.py\r\nsetuptools registered plugins:\r\n  pytest-cov-2.5.1 at /home/mcf/anaconda3/envs/pyhf/lib/python3.6/site-packages/pytest_cov/plugin.py\r\n  pytest-benchmark-3.1.1 at /home/mcf/anaconda3/envs/pyhf/lib/python3.6/site-packages/pytest_benchmark/plugin.py\r\n```\r\n\r\n# Steps to Reproduce\r\n\r\nWith directory `tests` and a file `tests/test_benchmark.py` run\r\n```\r\npytest --benchmark-histogram tests/test_benchmark.py\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/101", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/101/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/101/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/101/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/101", "id": 301010049, "node_id": "MDU6SXNzdWUzMDEwMTAwNDk=", "number": 101, "title": "Ability to select parameter for --benchmark-group-by=param", "user": {"login": "matthewfeickert", "id": 5142394, "node_id": "MDQ6VXNlcjUxNDIzOTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/5142394?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthewfeickert", "html_url": "https://github.com/matthewfeickert", "followers_url": "https://api.github.com/users/matthewfeickert/followers", "following_url": "https://api.github.com/users/matthewfeickert/following{/other_user}", "gists_url": "https://api.github.com/users/matthewfeickert/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthewfeickert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthewfeickert/subscriptions", "organizations_url": "https://api.github.com/users/matthewfeickert/orgs", "repos_url": "https://api.github.com/users/matthewfeickert/repos", "events_url": "https://api.github.com/users/matthewfeickert/events{/privacy}", "received_events_url": "https://api.github.com/users/matthewfeickert/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 136501660, "node_id": "MDU6TGFiZWwxMzY1MDE2NjA=", "url": "https://api.github.com/repos/ionelmc/pytest-benchmark/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-28T12:43:37Z", "updated_at": "2019-01-07T20:58:49Z", "closed_at": "2019-01-07T20:58:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I don't know if this is trying to be too complicated with the output, but is it possible to further refine the `param` label in the [`--benchmark-group-by` command line option](http://pytest-benchmark.readthedocs.io/en/stable/usage.html#commandline-options)?\r\n\r\nAn example of why one might want to do this is if I want to benchmark a function's performance with respect to the number of bins in a histogram in a framework that supports different backends.\r\n\r\n```python\r\nbins = [1, 2, 3, 4, 5, 10, 15, 20, 25, 50, 100]\r\nbin_ids = ['{}_bins'.format(n_bins) for n_bins in bins]\r\n\r\n@pytest.mark.parametrize('n_bins', bins, ids=bin_ids)\r\n@pytest.mark.parametrize('backend', [numpy_backend(),\r\n                                     tensorflow_backend(session=tf.Session()),\r\n                                     pytorch_backend(),\r\n                                     mxnet_backend()],\r\n                         ids=['numpy', 'tensorflow', 'pytorch', 'mxnet'])\r\ndef test_logpdf(benchmark, backend, n_bins):\r\n    ...\r\n    # Things that are taking in information from backend and n_bins happen\r\n    ...\r\n    assert benchmark(logpdf, source) is not None\r\n```\r\nWhen [looking at the output of the benchmark](https://github.com/diana-hep/pyhf/pull/92#issuecomment-369038921) in the generated plot from using the `--benchmark-histogram` option it would be nice to be able to see the backends grouped together.\r\n\r\nExample of current output from `pytest --benchmark-histogram` (no grouping):\r\n![example plot](https://user-images.githubusercontent.com/5142394/36756873-88b13610-1c10-11e8-9b45-3cdba5dba887.png)\r\n\r\nSo if the there were more then one parameterization for a test would there be anyway to do something like `--benchmark-group-by=param[0]`? I haven't looked at the underlying code so this might be a dumb thing to suggest.\r\n\r\n# Relevant Issues and Pull Requests\r\n\r\nI think that this is somewhat along the lines of Issue #77, but different in the fact that it is even more granular.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/100", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/100/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/100/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/100/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/100", "id": 295865096, "node_id": "MDU6SXNzdWUyOTU4NjUwOTY=", "number": 100, "title": "TypeError: run() takes from 1 to 2 positional arguments but 3 were given", "user": {"login": "arekbulski", "id": 5385838, "node_id": "MDQ6VXNlcjUzODU4Mzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5385838?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arekbulski", "html_url": "https://github.com/arekbulski", "followers_url": "https://api.github.com/users/arekbulski/followers", "following_url": "https://api.github.com/users/arekbulski/following{/other_user}", "gists_url": "https://api.github.com/users/arekbulski/gists{/gist_id}", "starred_url": "https://api.github.com/users/arekbulski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arekbulski/subscriptions", "organizations_url": "https://api.github.com/users/arekbulski/orgs", "repos_url": "https://api.github.com/users/arekbulski/repos", "events_url": "https://api.github.com/users/arekbulski/events{/privacy}", "received_events_url": "https://api.github.com/users/arekbulski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-02-09T13:07:25Z", "updated_at": "2018-02-11T05:01:25Z", "closed_at": "2018-02-11T05:01:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using `--benchmark-disable` and got stuck when I needed to preserve an instance between test cases, some of which use \"self\" and some use \"benchmark\", only latter fail. And I dont understand squat of what is the problem with it. Please help. \r\n\r\n```py\r\n\r\n@pytest.mark.xfail(not supportscompiler, reason=\"compiler requires Python 3.6\")\r\nclass TestCompiler(unittest.TestCase):\r\n\r\n    def setUp(self):\r\n        self.example = Struct()\r\n\r\n    def test_benchmark_parse1(self):\r\n        d = self.example\r\n        data = bytes(1000)\r\n        d.parse(data)\r\n\r\n    def test_benchmark_parse2(benchmark):\r\n        d = benchmark.example\r\n        data = bytes(1000)\r\n        benchmark(d.parse, data)\r\n\r\n```\r\n\r\n```bash\r\n$ tox\r\nGLOB sdist-make: /media/arkadiusz/MAIN/GitHub/construct/setup.py\r\npython36 inst-nodeps: /media/arkadiusz/MAIN/GitHub/construct/.tox/dist/construct-2.9.28.zip\r\npython36 installed: attrs==17.4.0,construct==2.9.28,coverage==4.5,enum34==1.1.6,numpy==1.14.0,pkg-resources==0.0.0,pluggy==0.6.0,py==1.5.2,py-cpuinfo==3.3.0,pytest==3.4.0,pytest-benchmark==3.1.1,pytest-cov==2.5.1,six==1.11.0\r\npython36 runtests: PYTHONHASHSEED='4064277830'\r\npython36 runtests: commands[0] | py.test --benchmark-disable --showlocals\r\n========================================== test session starts ==========================================\r\nplatform linux -- Python 3.6.3, pytest-3.4.0, py-1.5.2, pluggy-0.6.0\r\nbenchmark: 3.1.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\nrootdir: /media/arkadiusz/MAIN/GitHub/construct, inifile:\r\nplugins: cov-2.5.1, benchmark-3.1.1\r\ncollected 323 items                                                                                     \r\n\r\n=============================================== FAILURES ================================================\r\n__________________________________ TestCompiler.test_benchmark_parse2 ___________________________________\r\n\r\nbenchmark = <test_compiler.TestCompiler testMethod=test_benchmark_parse2>\r\n\r\n    def test_benchmark_parse2(benchmark):\r\n        d = benchmark.example\r\n        data = bytes(1000)\r\n>       benchmark(d.parse, data)\r\nE       TypeError: run() takes from 1 to 2 positional arguments but 3 were given\r\n\r\nbenchmark  = <test_compiler.TestCompiler testMethod=test_benchmark_parse2>\r\nd          = <Struct: None>\r\ndata       = b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00...00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\r\n\r\ntests/test_compiler.py:20: TypeError\r\n============================ 1 failed, 315 passed, 7 xfailed in 4.55 seconds ============================\r\nERROR: InvocationError: '/media/arkadiusz/MAIN/GitHub/construct/.tox/python36/bin/py.test --benchmark-disable --showlocals'\r\n________________________________________________ summary ________________________________________________\r\nERROR:   python36: commands failed\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/99", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/99/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/99/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/99/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/99", "id": 295627792, "node_id": "MDU6SXNzdWUyOTU2Mjc3OTI=", "number": 99, "title": "need help with writing tox.ini", "user": {"login": "arekbulski", "id": 5385838, "node_id": "MDQ6VXNlcjUzODU4Mzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5385838?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arekbulski", "html_url": "https://github.com/arekbulski", "followers_url": "https://api.github.com/users/arekbulski/followers", "following_url": "https://api.github.com/users/arekbulski/following{/other_user}", "gists_url": "https://api.github.com/users/arekbulski/gists{/gist_id}", "starred_url": "https://api.github.com/users/arekbulski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arekbulski/subscriptions", "organizations_url": "https://api.github.com/users/arekbulski/orgs", "repos_url": "https://api.github.com/users/arekbulski/repos", "events_url": "https://api.github.com/users/arekbulski/events{/privacy}", "received_events_url": "https://api.github.com/users/arekbulski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-02-08T18:59:11Z", "updated_at": "2018-02-11T05:00:00Z", "closed_at": "2018-02-11T05:00:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "I need some assisstance with getting tox to run the suite on both PY27 PY36 and PYPY and dont know how to write it exactly. Could you help me out? Its not entire benchmark-related.\r\n```ini\r\n[tox]\r\nenvlist=\r\n    python36\r\n\r\n[testenv]\r\ndeps=\r\n    pytest\r\n    pytest-benchmark\r\n    pytest-cov\r\n    enum34\r\n    numpy\r\n\r\ncommands=\r\n    py.test --benchmark-disable --showlocals {posargs}\r\n\r\n[testenv:verb]\r\ncommands=\r\n    py.test --benchmark-disable --fulltrace --showlocals --verbose {posargs}\r\n\r\n[testenv:cover]\r\ncommands=\r\n    py.test --benchmark-disable --cov {envsitepackagesdir}/construct --cov-report html --cov-report term --verbose {posargs}\r\n\r\n[testenv:bench]\r\ncommands=\r\n    py.test --benchmark-only --benchmark-columns=min,max,mean,stddev,median,rounds --benchmark-sort=name --benchmark-compare {posargs}\r\n\r\n[testenv:benchsave]\r\ncommands=\r\n    py.test --benchmark-only --benchmark-columns=min,max,mean,stddev,median,rounds --benchmark-sort=name --benchmark-autosave {posargs}\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/91", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/91/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/91/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/91/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/91", "id": 268163597, "node_id": "MDU6SXNzdWUyNjgxNjM1OTc=", "number": 91, "title": "Table Output Control", "user": {"login": "jdhardy", "id": 108149, "node_id": "MDQ6VXNlcjEwODE0OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/108149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdhardy", "html_url": "https://github.com/jdhardy", "followers_url": "https://api.github.com/users/jdhardy/followers", "following_url": "https://api.github.com/users/jdhardy/following{/other_user}", "gists_url": "https://api.github.com/users/jdhardy/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdhardy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdhardy/subscriptions", "organizations_url": "https://api.github.com/users/jdhardy/orgs", "repos_url": "https://api.github.com/users/jdhardy/repos", "events_url": "https://api.github.com/users/jdhardy/events{/privacy}", "received_events_url": "https://api.github.com/users/jdhardy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-24T19:51:18Z", "updated_at": "2017-10-26T19:02:09Z", "closed_at": "2017-10-26T19:02:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "The table output is a bit too verbose for my liking, but I can't see a way to control which columns are emitted (and looking at tables.py it seems to be hardcoded). A `--benchmark-cols=min,median,ops` argument would be ideal.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/90", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/90/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/90/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/90/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/90", "id": 260387516, "node_id": "MDU6SXNzdWUyNjAzODc1MTY=", "number": 90, "title": "Sort histogram x-axes ascending (by Trial)", "user": {"login": "makmanalp", "id": 161965, "node_id": "MDQ6VXNlcjE2MTk2NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/161965?v=4", "gravatar_id": "", "url": "https://api.github.com/users/makmanalp", "html_url": "https://github.com/makmanalp", "followers_url": "https://api.github.com/users/makmanalp/followers", "following_url": "https://api.github.com/users/makmanalp/following{/other_user}", "gists_url": "https://api.github.com/users/makmanalp/gists{/gist_id}", "starred_url": "https://api.github.com/users/makmanalp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/makmanalp/subscriptions", "organizations_url": "https://api.github.com/users/makmanalp/orgs", "repos_url": "https://api.github.com/users/makmanalp/repos", "events_url": "https://api.github.com/users/makmanalp/events{/privacy}", "received_events_url": "https://api.github.com/users/makmanalp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-25T19:24:19Z", "updated_at": "2017-09-25T19:47:43Z", "closed_at": "2017-09-25T19:47:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "In a given histogram image (e.g. let's say I ran `py.test-benchmark compare --name=short --group-by=name --histogram blah/`), it would be awesome to be able to see what the progress in performance in one benchmark has been like across benchmark runs. If you sort the graph by Trial, then this is easy to see.\r\n\r\nI think it just requires a modification here:\r\n\r\nhttps://github.com/ionelmc/pytest-benchmark/blob/9953995c680f861608d64692b0b8604fd9995488/src/pytest_benchmark/histogram.py#L94\r\n\r\nTo change the loop to `for row in sorted(benchmarks, key=lambda x: x['name'])`. Let me know if you'd like me to attempt a PR, or if you'd like to tackle it yourself, @ionelmc !", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/89", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/89/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/89/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/89/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/89", "id": 258636320, "node_id": "MDU6SXNzdWUyNTg2MzYzMjA=", "number": 89, "title": "cprofile doesn't use setup", "user": {"login": "bryanculbertson", "id": 144028, "node_id": "MDQ6VXNlcjE0NDAyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/144028?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bryanculbertson", "html_url": "https://github.com/bryanculbertson", "followers_url": "https://api.github.com/users/bryanculbertson/followers", "following_url": "https://api.github.com/users/bryanculbertson/following{/other_user}", "gists_url": "https://api.github.com/users/bryanculbertson/gists{/gist_id}", "starred_url": "https://api.github.com/users/bryanculbertson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bryanculbertson/subscriptions", "organizations_url": "https://api.github.com/users/bryanculbertson/orgs", "repos_url": "https://api.github.com/users/bryanculbertson/repos", "events_url": "https://api.github.com/users/bryanculbertson/events{/privacy}", "received_events_url": "https://api.github.com/users/bryanculbertson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-09-18T22:08:20Z", "updated_at": "2019-01-02T22:40:05Z", "closed_at": "2019-01-02T22:40:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using cprofile `pytest-benchmark` re-runs the `target` method without first running `setup` ([see code](https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/fixture.py#L225)). For methods that mutate an arg this means the cprofile returns incorrect results for the benchmark.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/88", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/88/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/88/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/88/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/88", "id": 254773724, "node_id": "MDU6SXNzdWUyNTQ3NzM3MjQ=", "number": 88, "title": "Test detection with unittest.TestCase base class", "user": {"login": "JackMedley", "id": 5593919, "node_id": "MDQ6VXNlcjU1OTM5MTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5593919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JackMedley", "html_url": "https://github.com/JackMedley", "followers_url": "https://api.github.com/users/JackMedley/followers", "following_url": "https://api.github.com/users/JackMedley/following{/other_user}", "gists_url": "https://api.github.com/users/JackMedley/gists{/gist_id}", "starred_url": "https://api.github.com/users/JackMedley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JackMedley/subscriptions", "organizations_url": "https://api.github.com/users/JackMedley/orgs", "repos_url": "https://api.github.com/users/JackMedley/repos", "events_url": "https://api.github.com/users/JackMedley/events{/privacy}", "received_events_url": "https://api.github.com/users/JackMedley/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-01T23:45:42Z", "updated_at": "2017-09-02T13:18:41Z", "closed_at": "2017-09-02T13:18:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "I use unittest.TestCase as the base class for my testing but I'm having some problems using this with pytest-benchmark.  \r\n\r\n```\r\ndef something(duration=0.00001):\r\n\ttime.sleep(duration)\r\n\treturn 123\r\n\r\nclass TestClass1(object):\r\n\tdef test_simple(self, benchmark):\r\n\t\tresult = benchmark(something)\r\n\t\tassert result == 123\r\n\r\nclass TestClass2(unittest.TestCase):\r\n\tdef test_simple(self, benchmark):\r\n\t\tresult = benchmark(something)\r\n\t\tassert result == 123\r\n```\r\n\r\nTestClass1 passes fine but TestCase2 fails with\r\n```\r\n    def run(self, result=None):\r\n        orig_result = result\r\n        if result is None:\r\n            result = self.defaultTestResult()\r\n            startTestRun = getattr(result, 'startTestRun', None)\r\n            if startTestRun is not None:\r\n                startTestRun()\r\n\r\n        self._resultForDoCleanups = result\r\n        result.startTest(self)\r\n\r\n        testMethod = getattr(self, self._testMethodName)\r\n        if (getattr(self.__class__, \"__unittest_skip__\", False) or\r\n            getattr(testMethod, \"__unittest_skip__\", False)):\r\n            # If the class or method was skipped.\r\n            try:\r\n                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')\r\n                            or getattr(testMethod, '__unittest_skip_why__', ''))\r\n                self._addSkip(result, skip_why)\r\n            finally:\r\n                result.stopTest(self)\r\n            return\r\n        try:\r\n            success = False\r\n            try:\r\n                self.setUp()\r\n            except SkipTest as e:\r\n                self._addSkip(result, str(e))\r\n            except KeyboardInterrupt:\r\n                raise\r\n            except:\r\n                result.addError(self, sys.exc_info())\r\n            else:\r\n                try:\r\n>                   testMethod()\r\nE                   TypeError: test_simple() takes exactly 2 arguments (1 given)\r\n```\r\n\r\nIs there a way around this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/86", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/86/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/86/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/86/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/86", "id": 249896172, "node_id": "MDU6SXNzdWUyNDk4OTYxNzI=", "number": 86, "title": "New filtering of benchmark tests filters out more than it should", "user": {"login": "drebs", "id": 75865, "node_id": "MDQ6VXNlcjc1ODY1", "avatar_url": "https://avatars1.githubusercontent.com/u/75865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drebs", "html_url": "https://github.com/drebs", "followers_url": "https://api.github.com/users/drebs/followers", "following_url": "https://api.github.com/users/drebs/following{/other_user}", "gists_url": "https://api.github.com/users/drebs/gists{/gist_id}", "starred_url": "https://api.github.com/users/drebs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drebs/subscriptions", "organizations_url": "https://api.github.com/users/drebs/orgs", "repos_url": "https://api.github.com/users/drebs/repos", "events_url": "https://api.github.com/users/drebs/events{/privacy}", "received_events_url": "https://api.github.com/users/drebs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-08-13T20:14:56Z", "updated_at": "2019-01-02T22:19:47Z", "closed_at": "2019-01-02T22:19:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I have proposed a modification to filter non-benchmark tests using the `pytest_collect_modifyitems()` hook instead of  `pytest_runtest_call()` (#84). That modification introduced:\r\n\r\n- **a bug:** the filtering is finishing too early because of identation problem in the last lines of the filtering function.\r\n- **a conceptual issue:** previous to the modification, the fixtures were instantiated and  the class of the `benchmark` fixture was checked to ensure that the parameter was indeed a benchmark fixture and not just something else called \"benchmark\". Now the filtering happens before fixtures are instantiated, so we don't have a way to check against their classes. Instead, i proposed to check if there was an argument called `benchmark`, and select the test if so.\r\n\r\nI will provide a fix for the bug, and would like to discuss a modification to the conceptual issue.\r\n\r\nI have a use case where I create a bunch of other fixtures that use the `benchmark` fixture and modify its behaviour. In this case, if my test functions receive something like `modified_benchmark` they will not be selected if the check is against the exact string of the parameter name.\r\n\r\nOne simple possibility is to check if there's a test argument name containing `benchmark` as a substring. This would select all tests that have some parameter matching `.*benchmark.*`. Does that make sense? Is there a better way to do it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/84", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/84/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/84/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/84/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/84", "id": 247206150, "node_id": "MDU6SXNzdWUyNDcyMDYxNTA=", "number": 84, "title": "Fixtures are executed even when all tests that use it are skipped", "user": {"login": "drebs", "id": 75865, "node_id": "MDQ6VXNlcjc1ODY1", "avatar_url": "https://avatars1.githubusercontent.com/u/75865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drebs", "html_url": "https://github.com/drebs", "followers_url": "https://api.github.com/users/drebs/followers", "following_url": "https://api.github.com/users/drebs/following{/other_user}", "gists_url": "https://api.github.com/users/drebs/gists{/gist_id}", "starred_url": "https://api.github.com/users/drebs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drebs/subscriptions", "organizations_url": "https://api.github.com/users/drebs/orgs", "repos_url": "https://api.github.com/users/drebs/repos", "events_url": "https://api.github.com/users/drebs/events{/privacy}", "received_events_url": "https://api.github.com/users/drebs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-08-01T21:24:03Z", "updated_at": "2017-08-05T19:56:26Z", "closed_at": "2017-08-05T19:56:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I have [an example](https://0xacab.org/drebs/soledad/-/jobs/15880) of a fixture being executed even when all the tests that use it are being skipped.\r\n\r\nIn that example, tests that use `pytest-benchmark` live in the `testing/tests/benchmarks` folder, and all others should be skipped. I have introduced new (non-benchmark) tests that use fixtures in the `testing/tests/responsiveness` folder, and those fixtures are being executed even though the corresponding tests are correctly skipped.\r\n\r\nI can provide details on how to reproduce that if you need it (but you may already know what this is about as we talked about it in IRC).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/82", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/82/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/82/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/82/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/82", "id": 245350484, "node_id": "MDU6SXNzdWUyNDUzNTA0ODQ=", "number": 82, "title": "git as hard requirement and 3.1.0", "user": {"login": "hredestig", "id": 1300566, "node_id": "MDQ6VXNlcjEzMDA1NjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1300566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hredestig", "html_url": "https://github.com/hredestig", "followers_url": "https://api.github.com/users/hredestig/followers", "following_url": "https://api.github.com/users/hredestig/following{/other_user}", "gists_url": "https://api.github.com/users/hredestig/gists{/gist_id}", "starred_url": "https://api.github.com/users/hredestig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hredestig/subscriptions", "organizations_url": "https://api.github.com/users/hredestig/orgs", "repos_url": "https://api.github.com/users/hredestig/repos", "events_url": "https://api.github.com/users/hredestig/events{/privacy}", "received_events_url": "https://api.github.com/users/hredestig/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-07-25T10:05:32Z", "updated_at": "2017-07-26T12:11:23Z", "closed_at": "2017-07-25T18:56:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "With pytest-benchmark I could on a system without git use the --benchmark-skip option to run my tests. With 3.1.0 I get traceback below. Is it intentional that git must be available even if user just wants to run the regular tests?\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/venv/bin/pytest\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/config.py\", line 49, in main\r\n    config = _prepareconfig(args, plugins)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/config.py\", line 164, in _prepareconfig\r\n    pluginmanager=pluginmanager, args=args)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 745, in __call__\r\n    return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 339, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 334, in <lambda>\r\n    _MultiCall(methods, kwargs, hook.spec_opts).execute()\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 613, in execute\r\n    return _wrapped_call(hook_impl.function(*args), self.execute)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 250, in _wrapped_call\r\n    wrap_controller.send(call_outcome)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/helpconfig.py\", line 67, in pytest_cmdline_parse\r\n    config = outcome.get_result()\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 279, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 265, in __init__\r\n    self.result = func()\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 614, in execute\r\n    res = hook_impl.function(*args)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/config.py\", line 934, in pytest_cmdline_parse\r\n    self.parse(args)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/config.py\", line 1106, in parse\r\n    self._preparse(args, addopts=addopts)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/config.py\", line 1068, in _preparse\r\n    self.pluginmanager.load_setuptools_entrypoints('pytest11')\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 516, in load_setuptools_entrypoints\r\n    self.register(plugin, name=ep.name)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/config.py\", line 258, in register\r\n    ret = super(PytestPluginManager, self).register(plugin, name)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 371, in register\r\n    hook._maybe_apply_history(hookimpl)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 768, in _maybe_apply_history\r\n    res = self._hookexec(self, [method], kwargs)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 339, in _hookexec\r\n    return self._inner_hookexec(hook, methods, kwargs)\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 334, in <lambda>\r\n    _MultiCall(methods, kwargs, hook.spec_opts).execute()\r\n  File \"/venv/lib/python3.5/site-packages/_pytest/vendored_packages/pluggy.py\", line 614, in execute\r\n    res = hook_impl.function(*args)\r\n  File \"/venv/lib/python3.5/site-packages/pytest_benchmark/plugin.py\", line 198, in pytest_addoption\r\n    tag = get_tag()\r\n  File \"/venv/lib/python3.5/site-packages/pytest_benchmark/utils.py\", line 87, in get_tag\r\n    info = get_commit_info(project_name)\r\n  File \"/venv/lib/python3.5/site-packages/pytest_benchmark/utils.py\", line 182, in get_commit_info\r\n    branch = get_branch_info()\r\n  File \"/venv/lib/python3.5/site-packages/pytest_benchmark/utils.py\", line 164, in get_branch_info\r\n    branch = cmd('git rev-parse --abbrev-ref HEAD').strip()\r\n  File \"/venv/lib/python3.5/site-packages/pytest_benchmark/utils.py\", line 160, in cmd\r\n    return check_output(args, stderr=subprocess.STDOUT, universal_newlines=True)\r\n  File \"/usr/lib/python3.5/subprocess.py\", line 316, in check_output\r\n    **kwargs).stdout\r\n  File \"/usr/lib/python3.5/subprocess.py\", line 383, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n  File \"/usr/lib/python3.5/subprocess.py\", line 676, in __init__\r\n    restore_signals, start_new_session)\r\n  File \"/usr/lib/python3.5/subprocess.py\", line 1282, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'git'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/81", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/81/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/81/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/81/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/81", "id": 245103200, "node_id": "MDU6SXNzdWUyNDUxMDMyMDA=", "number": 81, "title": "KeyError: 'ops' with pytest-benchmark compare", "user": {"login": "The-Compiler", "id": 625793, "node_id": "MDQ6VXNlcjYyNTc5Mw==", "avatar_url": "https://avatars0.githubusercontent.com/u/625793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/The-Compiler", "html_url": "https://github.com/The-Compiler", "followers_url": "https://api.github.com/users/The-Compiler/followers", "following_url": "https://api.github.com/users/The-Compiler/following{/other_user}", "gists_url": "https://api.github.com/users/The-Compiler/gists{/gist_id}", "starred_url": "https://api.github.com/users/The-Compiler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/The-Compiler/subscriptions", "organizations_url": "https://api.github.com/users/The-Compiler/orgs", "repos_url": "https://api.github.com/users/The-Compiler/repos", "events_url": "https://api.github.com/users/The-Compiler/events{/privacy}", "received_events_url": "https://api.github.com/users/The-Compiler/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-24T14:42:30Z", "updated_at": "2017-07-26T11:56:01Z", "closed_at": "2017-07-25T11:40:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I only have one saved benchmark currently, which is written by 3.0.0: \r\n[0001_x.json.txt](https://github.com/ionelmc/pytest-benchmark/files/1170147/0001_x.json.txt) (renamed so I can upload it here)\r\n\r\n\r\nWhen I do `pytest-benchmark compare` with 3.1.0, I get:\r\n\r\n```\r\nComputing stats ...Traceback (most recent call last):\r\n  File \"./.tox/py36/bin/pytest-benchmark\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/florian/proj/qutebrowser/git/.tox/py36/lib/python3.6/site-packages/pytest_benchmark/cli.py\", line 162, in main\r\n    results_table.display(TerminalReporter(), groups, progress_reporter=report_noprogress)\r\n  File \"/home/florian/proj/qutebrowser/git/.tox/py36/lib/python3.6/site-packages/pytest_benchmark/table.py\", line 39, in display\r\n    benchmarks, tr, \"{line} ({pos}/{total})\", line=line))\r\n  File \"/home/florian/proj/qutebrowser/git/.tox/py36/lib/python3.6/site-packages/pytest_benchmark/table.py\", line 38, in <genexpr>\r\n    worst[prop] = min(bench[prop] for _, bench in progress_reporter(\r\nKeyError: 'ops'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/72", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/72/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/72/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/72/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/72", "id": 217274276, "node_id": "MDU6SXNzdWUyMTcyNzQyNzY=", "number": 72, "title": "Allow elasticsearch authentication besides encoding credentials into url", "user": {"login": "varac", "id": 488213, "node_id": "MDQ6VXNlcjQ4ODIxMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/488213?v=4", "gravatar_id": "", "url": "https://api.github.com/users/varac", "html_url": "https://github.com/varac", "followers_url": "https://api.github.com/users/varac/followers", "following_url": "https://api.github.com/users/varac/following{/other_user}", "gists_url": "https://api.github.com/users/varac/gists{/gist_id}", "starred_url": "https://api.github.com/users/varac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/varac/subscriptions", "organizations_url": "https://api.github.com/users/varac/orgs", "repos_url": "https://api.github.com/users/varac/repos", "events_url": "https://api.github.com/users/varac/events{/privacy}", "received_events_url": "https://api.github.com/users/varac/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-03-27T14:58:17Z", "updated_at": "2017-04-10T03:21:34Z", "closed_at": "2017-04-10T03:21:34Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currrently the only way of using credentials to authenticate against an elasticsearch instance is to encode the username+password in the url.\r\nThis is bad when run in CI, i.e. [gitlab unfortunatly still doesn't support protection against leaking secret variables](https://gitlab.com/gitlab-org/gitlab-ce/issues/13784). URls including credentials leak quite easy.\r\nA better way would be to use a config file or a `.netrc` file for credentials.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/70", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/70/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/70/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/70/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/70", "id": 216206478, "node_id": "MDU6SXNzdWUyMTYyMDY0Nzg=", "number": 70, "title": "Invalid commit_info.project when no slash is present in git remote url", "user": {"login": "varac", "id": 488213, "node_id": "MDQ6VXNlcjQ4ODIxMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/488213?v=4", "gravatar_id": "", "url": "https://api.github.com/users/varac", "html_url": "https://github.com/varac", "followers_url": "https://api.github.com/users/varac/followers", "following_url": "https://api.github.com/users/varac/following{/other_user}", "gists_url": "https://api.github.com/users/varac/gists{/gist_id}", "starred_url": "https://api.github.com/users/varac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/varac/subscriptions", "organizations_url": "https://api.github.com/users/varac/orgs", "repos_url": "https://api.github.com/users/varac/repos", "events_url": "https://api.github.com/users/varac/events{/privacy}", "received_events_url": "https://api.github.com/users/varac/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 136501655, "node_id": "MDU6TGFiZWwxMzY1MDE2NTU=", "url": "https://api.github.com/repos/ionelmc/pytest-benchmark/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-22T21:00:08Z", "updated_at": "2017-03-24T10:05:39Z", "closed_at": "2017-03-23T23:30:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I have a project that I'd like to benchmark with a git remote (ssh) url without a slash (`gitolite@leap.se:soledad.git`).\r\n\r\nTests are run from a subdirectory, and the `commit_info.project` is wrongly set to the name of the subdir.\r\nHowever, [this regular expression in get_project_name()](https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/utils.py#L104) depends on a slash present in the url, otherwise it fails.\r\nI'm not a python nor RE wizard so I won't come up with a PR, I'd appreciate a fix !", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/56", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/56/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/56/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/56/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/56", "id": 165877653, "node_id": "MDU6SXNzdWUxNjU4Nzc2NTM=", "number": 56, "title": "please give a complete example?", "user": {"login": "wumpus", "id": 2142266, "node_id": "MDQ6VXNlcjIxNDIyNjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/2142266?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wumpus", "html_url": "https://github.com/wumpus", "followers_url": "https://api.github.com/users/wumpus/followers", "following_url": "https://api.github.com/users/wumpus/following{/other_user}", "gists_url": "https://api.github.com/users/wumpus/gists{/gist_id}", "starred_url": "https://api.github.com/users/wumpus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wumpus/subscriptions", "organizations_url": "https://api.github.com/users/wumpus/orgs", "repos_url": "https://api.github.com/users/wumpus/repos", "events_url": "https://api.github.com/users/wumpus/events{/privacy}", "received_events_url": "https://api.github.com/users/wumpus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-07-15T21:06:29Z", "updated_at": "2017-02-23T02:18:48Z", "closed_at": "2017-02-23T02:18:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm unfamiliar with \"py.test\" or what a \"fixture\" is. Reading your docs, there is no complete example of a source file with appropriate command-line. Also, there didn't seem to be any pointers from your documentation to some docs about py.test that might explain. Sure, I can use google to find the info, but maybe it could be more obvious?\n\n(and heck, googling didn't help! I see I'm supposed to run it under py.test, so that's why there's no import in the file... but... ? ... ok a few minutes of experiment, I eventually got it.)\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/55", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/55/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/55/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/55/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/55", "id": 162845936, "node_id": "MDU6SXNzdWUxNjI4NDU5MzY=", "number": 55, "title": "Ops/Sec", "user": {"login": "thedrow", "id": 48936, "node_id": "MDQ6VXNlcjQ4OTM2", "avatar_url": "https://avatars2.githubusercontent.com/u/48936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thedrow", "html_url": "https://github.com/thedrow", "followers_url": "https://api.github.com/users/thedrow/followers", "following_url": "https://api.github.com/users/thedrow/following{/other_user}", "gists_url": "https://api.github.com/users/thedrow/gists{/gist_id}", "starred_url": "https://api.github.com/users/thedrow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thedrow/subscriptions", "organizations_url": "https://api.github.com/users/thedrow/orgs", "repos_url": "https://api.github.com/users/thedrow/repos", "events_url": "https://api.github.com/users/thedrow/events{/privacy}", "received_events_url": "https://api.github.com/users/thedrow/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-06-29T05:49:55Z", "updated_at": "2017-07-25T11:23:43Z", "closed_at": "2017-07-25T11:23:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to display how many benchmarks were run per second?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/54", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/54/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/54/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/54/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/54", "id": 157196760, "node_id": "MDU6SXNzdWUxNTcxOTY3NjA=", "number": 54, "title": "help: estimate BigO for multiple functions?", "user": {"login": "yitang", "id": 6054101, "node_id": "MDQ6VXNlcjYwNTQxMDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/6054101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yitang", "html_url": "https://github.com/yitang", "followers_url": "https://api.github.com/users/yitang/followers", "following_url": "https://api.github.com/users/yitang/following{/other_user}", "gists_url": "https://api.github.com/users/yitang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yitang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yitang/subscriptions", "organizations_url": "https://api.github.com/users/yitang/orgs", "repos_url": "https://api.github.com/users/yitang/repos", "events_url": "https://api.github.com/users/yitang/events{/privacy}", "received_events_url": "https://api.github.com/users/yitang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2016-05-27T12:34:05Z", "updated_at": "2016-05-30T11:37:33Z", "closed_at": "2016-05-27T13:53:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I'd like to use this package to estimate the BigO for multiple functions. I wonder what's the practical way to implement it. Currently I can get the benchmark statistics for one input size, and I have to manually change the input size and run it again to get a curve for function v.s. input size. \n\nthe code is like this \n\n``` python\nsize = 100\nx = np.random.randn(size)\n\ndef test_f1(benchmark):\n    benchmark(f1)\n\ndef test_f2(benchmark):\n    benchmark(f2)\n\ndef test_f3(benchmark):\n    benchmark(f3)\n```\n\nThanks.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/50", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/50/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/50/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/50/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/50", "id": 150724094, "node_id": "MDU6SXNzdWUxNTA3MjQwOTQ=", "number": 50, "title": "Error in pygal.graph.box import is_list_like", "user": {"login": "kirotawa", "id": 476400, "node_id": "MDQ6VXNlcjQ3NjQwMA==", "avatar_url": "https://avatars0.githubusercontent.com/u/476400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kirotawa", "html_url": "https://github.com/kirotawa", "followers_url": "https://api.github.com/users/kirotawa/followers", "following_url": "https://api.github.com/users/kirotawa/following{/other_user}", "gists_url": "https://api.github.com/users/kirotawa/gists{/gist_id}", "starred_url": "https://api.github.com/users/kirotawa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kirotawa/subscriptions", "organizations_url": "https://api.github.com/users/kirotawa/orgs", "repos_url": "https://api.github.com/users/kirotawa/repos", "events_url": "https://api.github.com/users/kirotawa/events{/privacy}", "received_events_url": "https://api.github.com/users/kirotawa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-04-25T01:40:59Z", "updated_at": "2017-03-27T19:28:23Z", "closed_at": "2017-03-27T19:28:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "version: pytest-benchmark-3.0.0 \nHow to reproduce:\npy.test <file> --benchmark-histogram \n\noutput:\n\nFile \"/usr/local/lib/python2.7/dist-packages/pytest_benchmark/histogram.py\", line 8, in <module>\n    raise ImportError(exc.args, \"Please install pygal and pygaljs or pytest-benchmark[histogram]\")\nImportError: (('cannot import name is_list_like',), 'Please install pygal and pygaljs or pytest-benchmark[histogram]')\n\nIt seems to be an issue in pygal. I also tried: from pygal.graph.box import is_list_like, and it raises a error. \n\nWhat is the proper way to workaround it?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/47", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/47/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/47/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/47/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/47", "id": 140510619, "node_id": "MDU6SXNzdWUxNDA1MTA2MTk=", "number": 47, "title": "Which test to run when packaging for linux distribution", "user": {"login": "DamienCassou", "id": 217543, "node_id": "MDQ6VXNlcjIxNzU0Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/217543?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DamienCassou", "html_url": "https://github.com/DamienCassou", "followers_url": "https://api.github.com/users/DamienCassou/followers", "following_url": "https://api.github.com/users/DamienCassou/following{/other_user}", "gists_url": "https://api.github.com/users/DamienCassou/gists{/gist_id}", "starred_url": "https://api.github.com/users/DamienCassou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DamienCassou/subscriptions", "organizations_url": "https://api.github.com/users/DamienCassou/orgs", "repos_url": "https://api.github.com/users/DamienCassou/repos", "events_url": "https://api.github.com/users/DamienCassou/events{/privacy}", "received_events_url": "https://api.github.com/users/DamienCassou/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-03-13T17:50:18Z", "updated_at": "2016-03-13T18:34:19Z", "closed_at": "2016-03-13T18:34:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\n\nI'm packaging pytest-benchmark for nixos.org. Which tests should I run to make sure the packaging is ok? I saw you recommend nox but I would like a test for the current python environment, not for all of them. Moreover, I'm not interested in benchmark tests of pytest-benchmark. Thanks\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/46", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/46/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/46/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/46/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/46", "id": 139264222, "node_id": "MDU6SXNzdWUxMzkyNjQyMjI=", "number": 46, "title": "Save raw data", "user": {"login": "astrojuanlu", "id": 316517, "node_id": "MDQ6VXNlcjMxNjUxNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/316517?v=4", "gravatar_id": "", "url": "https://api.github.com/users/astrojuanlu", "html_url": "https://github.com/astrojuanlu", "followers_url": "https://api.github.com/users/astrojuanlu/followers", "following_url": "https://api.github.com/users/astrojuanlu/following{/other_user}", "gists_url": "https://api.github.com/users/astrojuanlu/gists{/gist_id}", "starred_url": "https://api.github.com/users/astrojuanlu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/astrojuanlu/subscriptions", "organizations_url": "https://api.github.com/users/astrojuanlu/orgs", "repos_url": "https://api.github.com/users/astrojuanlu/repos", "events_url": "https://api.github.com/users/astrojuanlu/events{/privacy}", "received_events_url": "https://api.github.com/users/astrojuanlu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-03-08T12:19:27Z", "updated_at": "2016-03-08T13:29:11Z", "closed_at": "2016-03-08T13:29:02Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently only the relevant statistics are saved to the `.json` file, but it would be nice to add some `--benchmark-full` command line option to actually save the elapsed time of each run too. I would like to take this results and plot them using tools other than pygal, so exporting them in some way would be helpful.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/43", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/43/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/43/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/43/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/43", "id": 135673012, "node_id": "MDU6SXNzdWUxMzU2NzMwMTI=", "number": 43, "title": "Use sample code on package page, but got error", "user": {"login": "Asoul", "id": 1904659, "node_id": "MDQ6VXNlcjE5MDQ2NTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1904659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Asoul", "html_url": "https://github.com/Asoul", "followers_url": "https://api.github.com/users/Asoul/followers", "following_url": "https://api.github.com/users/Asoul/following{/other_user}", "gists_url": "https://api.github.com/users/Asoul/gists{/gist_id}", "starred_url": "https://api.github.com/users/Asoul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Asoul/subscriptions", "organizations_url": "https://api.github.com/users/Asoul/orgs", "repos_url": "https://api.github.com/users/Asoul/repos", "events_url": "https://api.github.com/users/Asoul/events{/privacy}", "received_events_url": "https://api.github.com/users/Asoul/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2016-02-23T08:45:27Z", "updated_at": "2018-05-21T13:55:19Z", "closed_at": "2016-02-24T03:57:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "I use the sample code on [package page](https://pypi.python.org/pypi/pytest-benchmark). Below is my error:\n\n```\n[gw1] darwin -- Python 3.4.3 /Users/asoul/.virtualenvs/lms/bin/python3\n/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/unittest/case.py:58: in testPartExecutor\n    yield\n/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/unittest/case.py:577: in run\n    testMethod()\nE   TypeError: test_my_stuff() missing 1 required positional argument: 'benchmark'\n```\n\nWhat should I do? Thanks :)\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/42", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/42/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/42/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/42/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/42", "id": 125006999, "node_id": "MDU6SXNzdWUxMjUwMDY5OTk=", "number": 42, "title": "pytest-benchmark fails with `setup.py:tests_require`", "user": {"login": "foxx", "id": 651797, "node_id": "MDQ6VXNlcjY1MTc5Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/651797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/foxx", "html_url": "https://github.com/foxx", "followers_url": "https://api.github.com/users/foxx/followers", "following_url": "https://api.github.com/users/foxx/following{/other_user}", "gists_url": "https://api.github.com/users/foxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/foxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/foxx/subscriptions", "organizations_url": "https://api.github.com/users/foxx/orgs", "repos_url": "https://api.github.com/users/foxx/repos", "events_url": "https://api.github.com/users/foxx/events{/privacy}", "received_events_url": "https://api.github.com/users/foxx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-01-05T17:00:42Z", "updated_at": "2016-01-05T23:08:21Z", "closed_at": "2016-01-05T23:08:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following is inside `setup.py`;\n\n```\n    tests_require=[\n        'pytest-benchmark>=3.0',\n        'pytest-raisesregexp>=2.1',\n        'pytest-cov>=2.2.0',\n        'pytest>=2.8.5',\n        'webtest>=2.0.20',\n        'tox'\n    ]\n```\n\nHowever, running `python setup.py test` results in the following;\n\n```\nSearching for pytest-benchmark>=3.0\nReading https://pypi.python.org/simple/pytest-benchmark/\nBest match: pytest-benchmark 3.0.0\nDownloading https://pypi.python.org/packages/source/p/pytest-benchmark/pytest-benchmark-3.0.0.zip#md5=f8ab8e438f039366e3765168ad831b4c\nProcessing pytest-benchmark-3.0.0.zip\nWriting /tmp/easy_install-tcebs675/pytest-benchmark-3.0.0/setup.cfg\nRunning pytest-benchmark-3.0.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-tcebs675/pytest-benchmark-3.0.0/egg-dist-tmp-8ifylcko\nerror: Setup script exited with error in pytest-benchmark setup command: Invalid environment marker: python_version < \"3.4\"\n```\n\nIf I install the package first with `pip install pytest-benchmark`, then the error goes away.\n\nI'm guessing it has something to do with this [line](https://github.com/ionelmc/pytest-benchmark/blob/efda8a505d72e2849eceb5bc37dd05324669ec71/setup.py#L67).\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/41", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/41/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/41/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/41/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/41", "id": 125002090, "node_id": "MDU6SXNzdWUxMjUwMDIwOTA=", "number": 41, "title": "Colors appear to be broken", "user": {"login": "foxx", "id": 651797, "node_id": "MDQ6VXNlcjY1MTc5Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/651797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/foxx", "html_url": "https://github.com/foxx", "followers_url": "https://api.github.com/users/foxx/followers", "following_url": "https://api.github.com/users/foxx/following{/other_user}", "gists_url": "https://api.github.com/users/foxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/foxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/foxx/subscriptions", "organizations_url": "https://api.github.com/users/foxx/orgs", "repos_url": "https://api.github.com/users/foxx/repos", "events_url": "https://api.github.com/users/foxx/events{/privacy}", "received_events_url": "https://api.github.com/users/foxx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-01-05T16:36:43Z", "updated_at": "2016-01-05T22:43:31Z", "closed_at": "2016-01-05T22:43:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "As you can see, the colors appear to be broken on the `(*)` line, they are solid black, where as everything else is shaded \"correctly\". Any ideas?\n\n![image](https://cloud.githubusercontent.com/assets/651797/12120837/6df48f4e-b3ca-11e5-8f8c-4e017fd35cfa.png)\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/40", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/40/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/40/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/40/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/40", "id": 121488088, "node_id": "MDU6SXNzdWUxMjE0ODgwODg=", "number": 40, "title": "Make test suite fail if benchmark is unsatisfactory", "user": {"login": "alexprengere", "id": 2138730, "node_id": "MDQ6VXNlcjIxMzg3MzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2138730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexprengere", "html_url": "https://github.com/alexprengere", "followers_url": "https://api.github.com/users/alexprengere/followers", "following_url": "https://api.github.com/users/alexprengere/following{/other_user}", "gists_url": "https://api.github.com/users/alexprengere/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexprengere/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexprengere/subscriptions", "organizations_url": "https://api.github.com/users/alexprengere/orgs", "repos_url": "https://api.github.com/users/alexprengere/repos", "events_url": "https://api.github.com/users/alexprengere/events{/privacy}", "received_events_url": "https://api.github.com/users/alexprengere/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2015-12-10T13:45:22Z", "updated_at": "2016-04-10T00:23:53Z", "closed_at": "2016-04-10T00:23:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\nI read the documentation and hope I did not miss something completely obvious.\n\nMy use case: I would like to use `pytest-benchmark` for continuous integration. A part of my test suite is actually performance tests, i.e. making sure that my `toto` function does not take longer than, let's say, 20ms.\n\nI would like the test suite to _fail_ is some modifications in the code make `toto()` exceeds the 20ms. I am aware of the `--benchmark-compare-fail=EXPR`, but I think what I am looking for is more specific.\n\nI have no idea what would be the best way to describe this, perhaps:\n\n``` python\n@pytest.mark.benchmark(fail_at=0.02)\ndef test_toto(benchmark):\n    benchmark(toto)\n```\n\nOr maybe:\n\n``` python\ndef test_toto(benchmark):\n    results = benchmark(toto, fail_at=0.02)\n```\n\nOr provide a way for the user to access the results of the benchmark? Before using `pytest-benchmark`, I would do something like this:\n\n``` python\ndef test_toto(benchmark):\n    results = benchmark(toto)\n    if results.average_time > 0.02:\n        pytest.fail('Exceeding 20ms!')\n```\n\nWould this make sense in `pytest-benchmark`?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/25", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/25/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/25/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/25/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/25", "id": 106225468, "node_id": "MDU6SXNzdWUxMDYyMjU0Njg=", "number": 25, "title": "The tests fail due to version bump", "user": {"login": "aldanor", "id": 2418513, "node_id": "MDQ6VXNlcjI0MTg1MTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2418513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aldanor", "html_url": "https://github.com/aldanor", "followers_url": "https://api.github.com/users/aldanor/followers", "following_url": "https://api.github.com/users/aldanor/following{/other_user}", "gists_url": "https://api.github.com/users/aldanor/gists{/gist_id}", "starred_url": "https://api.github.com/users/aldanor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aldanor/subscriptions", "organizations_url": "https://api.github.com/users/aldanor/orgs", "repos_url": "https://api.github.com/users/aldanor/repos", "events_url": "https://api.github.com/users/aldanor/events{/privacy}", "received_events_url": "https://api.github.com/users/aldanor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2015-09-13T16:18:58Z", "updated_at": "2015-09-13T17:58:41Z", "closed_at": "2015-09-13T17:58:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://travis-ci.org/ionelmc/pytest-benchmark/jobs/80087971\n\nLooks like you'd need to update those JSON files in test_storage with 3.0.0a1.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/21", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/21/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/21/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/21/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/21", "id": 106223464, "node_id": "MDU6SXNzdWUxMDYyMjM0NjQ=", "number": 21, "title": "Allow to configure columns reported", "user": {"login": "aldanor", "id": 2418513, "node_id": "MDQ6VXNlcjI0MTg1MTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2418513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aldanor", "html_url": "https://github.com/aldanor", "followers_url": "https://api.github.com/users/aldanor/followers", "following_url": "https://api.github.com/users/aldanor/following{/other_user}", "gists_url": "https://api.github.com/users/aldanor/gists{/gist_id}", "starred_url": "https://api.github.com/users/aldanor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aldanor/subscriptions", "organizations_url": "https://api.github.com/users/aldanor/orgs", "repos_url": "https://api.github.com/users/aldanor/repos", "events_url": "https://api.github.com/users/aldanor/events{/privacy}", "received_events_url": "https://api.github.com/users/aldanor/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 259855285, "node_id": "MDU6TGFiZWwyNTk4NTUyODU=", "url": "https://api.github.com/repos/ionelmc/pytest-benchmark/labels/reporting", "name": "reporting", "color": "fbca04", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2", "html_url": "https://github.com/ionelmc/pytest-benchmark/milestone/2", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/milestones/2/labels", "id": 1303156, "node_id": "MDk6TWlsZXN0b25lMTMwMzE1Ng==", "number": 2, "title": "v3.2.0", "description": "", "creator": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "open_issues": 2, "closed_issues": 4, "state": "open", "created_at": "2015-09-13T17:46:53Z", "updated_at": "2019-01-10T15:44:58Z", "due_on": null, "closed_at": null}, "comments": 4, "created_at": "2015-09-13T15:34:59Z", "updated_at": "2017-09-25T19:42:29Z", "closed_at": "2017-09-25T19:42:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "Referring again to the output of pytest-benchmark's test suite itself, I'd point out that it is 188 characters wide which is a lot more than 80 or 100.\n\nIn a lot of cases, you would just want some mix of mean/median/min/max; especially so in the manual mode when you know exactly what you are doing.\n\nLike, I could look at the outliers, stddev and IQR while designing the benchmark, but when it's being reported at the end of the test suite run, I'll probably just look at min/mean.\n\nWith this in mind, it would be then nice if the report could be configured to include/exclude custom stat columns, like `stats=['min', 'mean'], sort='min'` (or command line parameters). Maybe the default setting should be not to output absolutely every stat that it computes, too.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/19", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/19/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/19/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/19/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/19", "id": 105991042, "node_id": "MDU6SXNzdWUxMDU5OTEwNDI=", "number": 19, "title": "Marker can't be found on py2?", "user": {"login": "aldanor", "id": 2418513, "node_id": "MDQ6VXNlcjI0MTg1MTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2418513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aldanor", "html_url": "https://github.com/aldanor", "followers_url": "https://api.github.com/users/aldanor/followers", "following_url": "https://api.github.com/users/aldanor/following{/other_user}", "gists_url": "https://api.github.com/users/aldanor/gists{/gist_id}", "starred_url": "https://api.github.com/users/aldanor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aldanor/subscriptions", "organizations_url": "https://api.github.com/users/aldanor/orgs", "repos_url": "https://api.github.com/users/aldanor/repos", "events_url": "https://api.github.com/users/aldanor/events{/privacy}", "received_events_url": "https://api.github.com/users/aldanor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2015-09-11T11:19:20Z", "updated_at": "2015-09-12T18:56:22Z", "closed_at": "2015-09-12T18:56:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Was trying to run `lazy-object-proxy` tests manually using pytest and the latest pytest-benchmark from git, and weirdly enough, it works just fine on Python 3, but doesn't work on Python 2 due to pytest being unable to find the marker. The two environments are pretty much identical aside from the Python version, and the same version of pytest-benchmark is properly installed in both. Any ideas why this could happen?\n\n``` python\ntests/test_lazy_object_proxy.py:1900: in <module>\n    @pytest.mark.benchmark(group=\"prototypes\")\n../envs/py2/lib/python2.7/site-packages/_pytest/mark.py:183: in __getattr__\n    self._check(name)\n../envs/py2/lib/python2.7/site-packages/_pytest/mark.py:198: in _check\n    raise AttributeError(\"%r not a registered marker\" % (name,))\nE   AttributeError: 'benchmark' not a registered marker\n_____________________________________________________ ERROR collecting tests/test_lazy_object_proxy.py ______________________________________________________\ntests/test_lazy_object_proxy.py:1900: in <module>\n    @pytest.mark.benchmark(group=\"prototypes\")\n../envs/py2/lib/python2.7/site-packages/_pytest/mark.py:183: in __getattr__\n    self._check(name)\n../envs/py2/lib/python2.7/site-packages/_pytest/mark.py:198: in _check\n    raise AttributeError(\"%r not a registered marker\" % (name,))\nE   AttributeError: 'benchmark' not a registered marker\n```\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/18", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/18/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/18/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/18/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/18", "id": 105982490, "node_id": "MDU6SXNzdWUxMDU5ODI0OTA=", "number": 18, "title": "Release a new version on pypi?", "user": {"login": "aldanor", "id": 2418513, "node_id": "MDQ6VXNlcjI0MTg1MTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2418513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aldanor", "html_url": "https://github.com/aldanor", "followers_url": "https://api.github.com/users/aldanor/followers", "following_url": "https://api.github.com/users/aldanor/following{/other_user}", "gists_url": "https://api.github.com/users/aldanor/gists{/gist_id}", "starred_url": "https://api.github.com/users/aldanor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aldanor/subscriptions", "organizations_url": "https://api.github.com/users/aldanor/orgs", "repos_url": "https://api.github.com/users/aldanor/repos", "events_url": "https://api.github.com/users/aldanor/events{/privacy}", "received_events_url": "https://api.github.com/users/aldanor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2015-09-11T10:14:23Z", "updated_at": "2015-10-06T13:49:39Z", "closed_at": "2015-09-13T11:48:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "The current version that's out there has a Python 3 syntax error (`except` clause) so it's broken and not installable -- might make sense to release a new one since that error has been fixed?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/16", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/16/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/16/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/16/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/16", "id": 99435756, "node_id": "MDU6SXNzdWU5OTQzNTc1Ng==", "number": 16, "title": "Disable tracers when benchmarking", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2015-08-06T13:36:30Z", "updated_at": "2015-08-10T00:24:14Z", "closed_at": "2015-08-10T00:24:14Z", "author_association": "OWNER", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/15", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/15/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/15/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/15/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/15", "id": 98853442, "node_id": "MDU6SXNzdWU5ODg1MzQ0Mg==", "number": 15, "title": "Add benchmark.weave", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2015-08-03T22:55:18Z", "updated_at": "2015-08-10T00:32:05Z", "closed_at": "2015-08-10T00:32:05Z", "author_association": "OWNER", "active_lock_reason": null, "body": "The `weave` attribute, as a shorthand for the `benchmark_weave` fixture.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/14", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/14/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/14/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/14/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/14", "id": 96928566, "node_id": "MDU6SXNzdWU5NjkyODU2Ng==", "number": 14, "title": "benchmarking side-effectful code", "user": {"login": "vmagdin", "id": 6867120, "node_id": "MDQ6VXNlcjY4NjcxMjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/6867120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vmagdin", "html_url": "https://github.com/vmagdin", "followers_url": "https://api.github.com/users/vmagdin/followers", "following_url": "https://api.github.com/users/vmagdin/following{/other_user}", "gists_url": "https://api.github.com/users/vmagdin/gists{/gist_id}", "starred_url": "https://api.github.com/users/vmagdin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vmagdin/subscriptions", "organizations_url": "https://api.github.com/users/vmagdin/orgs", "repos_url": "https://api.github.com/users/vmagdin/repos", "events_url": "https://api.github.com/users/vmagdin/events{/privacy}", "received_events_url": "https://api.github.com/users/vmagdin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2015-07-23T23:46:44Z", "updated_at": "2015-09-12T22:17:38Z", "closed_at": "2015-09-12T21:18:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm working on benchmarking some functions that modify their input. The input is a list of dictionaries, and the code sorts the list and also adds things to the dictionaries. As a result of this, the benchmarks are not totally true, because the first iteration modifies the input and the consequent iterations have to do much less work, because the input is already sorted. \n\nI can \"fix\" this problem by doing something (expensive) like deepcopy before the function I'm benchmarking is run, however, this will add to the running time statistics. Any thoughts?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/13", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/13/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/13/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/13/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/13", "id": 96546726, "node_id": "MDU6SXNzdWU5NjU0NjcyNg==", "number": 13, "title": "Better grouping", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2015-07-22T11:40:15Z", "updated_at": "2015-08-11T01:11:26Z", "closed_at": "2015-08-11T01:11:26Z", "author_association": "OWNER", "active_lock_reason": null, "body": "- Group by test's params (eg: fixtures etc)\n- Group by test's name\n\nNew options:\n- `--benchmark-group-by=params`\n- `--benchmark-group-by=group` (default)\n- `--benchmark-group-by=testname`\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/12", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/12/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/12/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/12/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/12", "id": 96534840, "node_id": "MDU6SXNzdWU5NjUzNDg0MA==", "number": 12, "title": "On PyPy use pypytools.clonefunc in the benchmark decorator", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2015-07-22T10:47:48Z", "updated_at": "2015-08-10T00:24:14Z", "closed_at": "2015-08-10T00:24:14Z", "author_association": "OWNER", "active_lock_reason": null, "body": "This will make the JIT specialize the function each time.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/11", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/11/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/11/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/11/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/11", "id": 96534691, "node_id": "MDU6SXNzdWU5NjUzNDY5MQ==", "number": 11, "title": "Graph plotting", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2015-07-22T10:46:39Z", "updated_at": "2015-08-11T01:23:48Z", "closed_at": "2015-08-11T01:23:48Z", "author_association": "OWNER", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/10", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/10/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/10/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/10/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/10", "id": 87619447, "node_id": "MDU6SXNzdWU4NzYxOTQ0Nw==", "number": 10, "title": "Number of benchmark warmup rounds is sometimes not enough for PyPy", "user": {"login": "thedrow", "id": 48936, "node_id": "MDQ6VXNlcjQ4OTM2", "avatar_url": "https://avatars2.githubusercontent.com/u/48936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thedrow", "html_url": "https://github.com/thedrow", "followers_url": "https://api.github.com/users/thedrow/followers", "following_url": "https://api.github.com/users/thedrow/following{/other_user}", "gists_url": "https://api.github.com/users/thedrow/gists{/gist_id}", "starred_url": "https://api.github.com/users/thedrow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thedrow/subscriptions", "organizations_url": "https://api.github.com/users/thedrow/orgs", "repos_url": "https://api.github.com/users/thedrow/repos", "events_url": "https://api.github.com/users/thedrow/events{/privacy}", "received_events_url": "https://api.github.com/users/thedrow/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 277701070, "node_id": "MDU6TGFiZWwyNzc3MDEwNzA=", "url": "https://api.github.com/repos/ionelmc/pytest-benchmark/labels/measurement", "name": "measurement", "color": "fef2c0", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2015-06-12T05:59:26Z", "updated_at": "2017-03-26T20:28:32Z", "closed_at": "2017-03-26T20:28:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "See https://travis-ci.org/thedrow/drf-benchmarks/jobs/66443524#L420 for example.\nI need a way to specify the minimum of warmup rounds for a benchmark so that I'll be able to verify that the JIT has been triggered.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/9", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/9/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/9/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/9/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/9", "id": 86858180, "node_id": "MDU6SXNzdWU4Njg1ODE4MA==", "number": 9, "title": "Feature request: number=1 or max_rounds", "user": {"login": "gavingc", "id": 652234, "node_id": "MDQ6VXNlcjY1MjIzNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/652234?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gavingc", "html_url": "https://github.com/gavingc", "followers_url": "https://api.github.com/users/gavingc/followers", "following_url": "https://api.github.com/users/gavingc/following{/other_user}", "gists_url": "https://api.github.com/users/gavingc/gists{/gist_id}", "starred_url": "https://api.github.com/users/gavingc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gavingc/subscriptions", "organizations_url": "https://api.github.com/users/gavingc/orgs", "repos_url": "https://api.github.com/users/gavingc/repos", "events_url": "https://api.github.com/users/gavingc/events{/privacy}", "received_events_url": "https://api.github.com/users/gavingc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 21, "created_at": "2015-06-10T06:25:55Z", "updated_at": "2015-09-13T11:39:21Z", "closed_at": "2015-09-12T21:18:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I realise how much effort has gone into getting a reasonable average benchmark.\n\nHowever I have just run into a use case where the unit under test must run exactly once.\nIt's not so much a benchmark as indicative.\nThe unit is inserting objects into a database (within a complex seq) so runs after the first are not representative.\n\nA bit of an edge case I know. \n\nFor now I'm using:\nt = timeit.timeit(sync_objects, number=1)\nassert t < 1\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/7", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/7/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/7/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/7/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/7", "id": 59207423, "node_id": "MDU6SXNzdWU1OTIwNzQyMw==", "number": 7, "title": "Issue with Xdist plugin: impossibile to serialize", "user": {"login": "JayZar21", "id": 5012925, "node_id": "MDQ6VXNlcjUwMTI5MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5012925?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JayZar21", "html_url": "https://github.com/JayZar21", "followers_url": "https://api.github.com/users/JayZar21/followers", "following_url": "https://api.github.com/users/JayZar21/following{/other_user}", "gists_url": "https://api.github.com/users/JayZar21/gists{/gist_id}", "starred_url": "https://api.github.com/users/JayZar21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JayZar21/subscriptions", "organizations_url": "https://api.github.com/users/JayZar21/orgs", "repos_url": "https://api.github.com/users/JayZar21/repos", "events_url": "https://api.github.com/users/JayZar21/events{/privacy}", "received_events_url": "https://api.github.com/users/JayZar21/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2015-02-27T08:59:05Z", "updated_at": "2019-05-16T15:33:18Z", "closed_at": "2015-03-11T16:53:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have different test written with pytest-benchmark. I use also the Xdist plugin to distribute my test on more than a process.\nStarting py.test with Xdist run gives this output:\n\n```\npy.test -n 2\n ============================= test session starts =============================\nplatform win32 -- Python 2.7.3 -- py-1.4.26 -- pytest-2.6.4\nplugins: benchmark, xdist\ngw0 C / gw1 IINTERNALERROR> Traceback (most recent call last):\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\_pytest\\main.py\", line 82,\nin wrap_session\nINTERNALERROR>     config.hook.pytest_sessionstart(session=session)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\_pytest\\core.py\", line 413,\n in __call__\nINTERNALERROR>     return self._docall(methods, kwargs)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\_pytest\\core.py\", line 424,\n in _docall\nINTERNALERROR>     res = mc.execute()\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\_pytest\\core.py\", line 315,\n in execute\nINTERNALERROR>     res = method(**kwargs)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\xdist\\dsession.py\", line 48\n0, in pytest_sessionstart\nINTERNALERROR>     nodes = self.nodemanager.setup_nodes(putevent=self.queue.put)\n\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\xdist\\slavemanage.py\", line\n 45, in setup_nodes\nINTERNALERROR>     nodes.append(self.setup_node(spec, putevent))\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\xdist\\slavemanage.py\", line\n 54, in setup_node\nINTERNALERROR>     node.setup()\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\xdist\\slavemanage.py\", line\n 223, in setup\nINTERNALERROR>     self.channel.send((self.slaveinput, args, option_dict))\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 681, in send\nINTERNALERROR>     self.gateway._send(Message.CHANNEL_DATA, self.id, dumps_inter\nnal(item))\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1285, in dumps_internal\nINTERNALERROR>     return _Serializer().save(obj)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1303, in save\nINTERNALERROR>     self._save(obj)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1321, in _save\nINTERNALERROR>     dispatch(self, obj)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1402, in save_tuple\nINTERNALERROR>     self._save(item)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1321, in _save\nINTERNALERROR>     dispatch(self, obj)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1398, in save_dict\nINTERNALERROR>     self._write_setitem(key, value)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1392, in _write_setitem\nINTERNALERROR>     self._save(value)\nINTERNALERROR>   File \"C:\\Python27\\lib\\site-packages\\execnet\\gateway_base.py\", l\nine 1319, in _save\nINTERNALERROR>     raise DumpError(\"can't serialize %s\" % (tp,))\nINTERNALERROR> DumpError: can't serialize <class 'pytest_benchmark.plugin.NameWr\napper'>\n```\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/5", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/5/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/5/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/5/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/5", "id": 52197343, "node_id": "MDU6SXNzdWU1MjE5NzM0Mw==", "number": 5, "title": "Indicate somehow that some benchmarks have error", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2014-12-17T03:25:45Z", "updated_at": "2015-11-01T15:18:13Z", "closed_at": "2015-11-01T15:18:13Z", "author_association": "OWNER", "active_lock_reason": null, "body": "And remove any recorded timings?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/2", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/2/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/2/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/2/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/2", "id": 45539589, "node_id": "MDU6SXNzdWU0NTUzOTU4OQ==", "number": 2, "title": "Save results to file and compare", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2014-10-11T03:05:44Z", "updated_at": "2015-08-11T01:11:27Z", "closed_at": "2015-08-11T01:11:27Z", "author_association": "OWNER", "active_lock_reason": null, "body": "```\n16:36 <hpk> ionelmc: what i'd need would be a way to compare agsinst prior benchmarks\n16:37 <ionelmc> hpk: that means you'd need a way to measure relative times against a \"control benchmark\"\n16:37 <hpk> ionelmc: i.e. \"py.test --bench-compare path-to-old-benchmarkresults\"\n16:37 <ionelmc> because machines don't have same perf\n16:38 <hpk> yes, writing out of results as well as comparing against them and getting errors when they slowed too much\n16:38 <hpk> so probably a \"py.test --bench-as-control-sample\" and \"py.test --bench\"\n16:38 <hpk> (conceptually)\n16:38 <ionelmc> hpk: in other words, you'd be comparing percentages, not actual timings\n16:39 <hpk> i'd be looking how much a benchmark deviates\n16:39 <hpk> would report all deviations and say that >10% slowdown is an error or so\n16:39 <ionelmc> nooo, i thing you missed my point\n16:40 <ionelmc> so, you have a \"control test\"\n16:40 <ionelmc> that does something, whatever, something simple\n16:40 <hpk> what i said was not directly related to what you said before -- more what i think would be useful for myself\n16:40 <ionelmc> and the other tests compare to that\n16:40 <ionelmc> eg: 50% slower than the \"control bench\"\n16:41 <hpk> might be useful for some people, not for me, i guess\n16:41 <ionelmc> and in the file you only save percentages (the relative values to thecontrol test)\n16:41 <ionelmc> otherwise saving to a file is not practical\n16:41 <ionelmc> i'm thinking travis\n16:41 <hpk> ah, now i get it\n16:41 <ionelmc> i run it locally but travis is gonna be very unpredictable\n16:41 <ionelmc> ever between runs\n16:41 <hpk> i don't know if this coulid work\n16:42 <hpk> but it's an interesting idea\n16:42 <ionelmc> so the only reliable thing to compare against is a \"contro test\" that is ran in the same session\n16:42 <hpk> question is if you can do a control test that makes sense\n16:42 <ionelmc> eg, i wanna benchmark instance creation of some objects\n16:42 <hpk> and where the relation \"realtest versus controltest\" is stable\n16:42 <hpk> across machines and interpreters\n16:43 <hpk> i somehow doubt it\n16:43 <ionelmc> and the control is \"object()\"\n16:43 <ionelmc> ofcourse some things will be slower on some interpreters\n16:43 <hpk> you need to try and run such things on multiple machines, including travis, to find out if it's viable i guess\n16:44 <ionelmc> i think it's best to just have a nice way to look at historical data\n16:44 <ionelmc> eg, a separate service that records timings\n16:44 <hpk> what i proposed should work without having to figure out control tests but you need a somewhat repeatable environment\n16:44 <ionelmc> like coveralls but for benchmarks\n16:45 <ionelmc> https://coveralls.io/\n16:45 <ionelmc> coveralls integrates well into travis\n16:46 <ionelmc> hpk: well, repeatable environments are a luxury\n16:47 <ionelmc> with all the crazy virtualization and even crazy cpu scaling (intel turboboost) it's fairly hard\n16:47 <hpk> ionelmc: yes -- the other question is if it's possible to count CPU ticks used for a computation rather than time\n16:48 <hpk> ionelmc: but it's even hard within the lifetime of one process\n16:48 <ionelmc> hmmmm\n16:48 <ionelmc> that should work\n16:48 <ionelmc> you only need to have the same cpu then\n16:49 <hpk> on travis i guess during 60 seconds of a test run you might experience different speeds\n16:49 <hpk> so doing a control run first, then benchmarks might or might not be accurate enough\n16:49 <ionelmc> wildly different i'd add :)\n17:05 <hpk> for me it all becomes only useful with the comparison feature, but then it would be very useful\n17:05 <hpk> (i am currently doing some benchmarking but manually, and i'd love to have a more systematic approach)\n17:06 <ionelmc> hpk: so you're basically assuming consistent environments, like, you're not going to use it on travis\n17:06 <hpk> yes\n17:06 <ionelmc> only locally, to get feedback on perf regression\n17:07 <hpk> yes, so if pytest had that, prior to pytest-2.7 would check it didn't regress\n17:07 <hpk> or even just for a PR\n17:07 <ionelmc> yeah, sounds very useful\n17:08 <hpk> and then integrate the web code behind http://speed.pypy.org/ :)\n17:12 <hpk> i'd be fine with just terminal reporting, already, though :)\n17:16 <ionelmc> ok, what would be an error situation\n17:17 <ionelmc> compare against minimums\n17:17 <ionelmc> what's a good default for error threshold ?\n17:30 <hpk> ionelmc: no clue, 10% maybe?\n```\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/1", "repository_url": "https://api.github.com/repos/ionelmc/pytest-benchmark", "labels_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/1/labels{/name}", "comments_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/1/comments", "events_url": "https://api.github.com/repos/ionelmc/pytest-benchmark/issues/1/events", "html_url": "https://github.com/ionelmc/pytest-benchmark/issues/1", "id": 45539521, "node_id": "MDU6SXNzdWU0NTUzOTUyMQ==", "number": 1, "title": "Add function wrapper support", "user": {"login": "ionelmc", "id": 129501, "node_id": "MDQ6VXNlcjEyOTUwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/129501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ionelmc", "html_url": "https://github.com/ionelmc", "followers_url": "https://api.github.com/users/ionelmc/followers", "following_url": "https://api.github.com/users/ionelmc/following{/other_user}", "gists_url": "https://api.github.com/users/ionelmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ionelmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ionelmc/subscriptions", "organizations_url": "https://api.github.com/users/ionelmc/orgs", "repos_url": "https://api.github.com/users/ionelmc/repos", "events_url": "https://api.github.com/users/ionelmc/events{/privacy}", "received_events_url": "https://api.github.com/users/ionelmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2014-10-11T03:02:35Z", "updated_at": "2015-02-02T05:07:03Z", "closed_at": "2015-02-02T05:07:03Z", "author_association": "OWNER", "active_lock_reason": null, "body": "Eg:\n\n```\ndef test_stuff(benchmark):\n    assert benchmark(func)(1, 2, 3) = 'blabla'\n```\n", "performed_via_github_app": null, "score": 1.0}]}