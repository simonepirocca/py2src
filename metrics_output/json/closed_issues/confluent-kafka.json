{"total_count": 443, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/937", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/937/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/937/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/937/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/937", "id": 682006420, "node_id": "MDU6SXNzdWU2ODIwMDY0MjA=", "number": 937, "title": "Bug: Future in AdminClient is set before the topic is created", "user": {"login": "gshaikov", "id": 28565097, "node_id": "MDQ6VXNlcjI4NTY1MDk3", "avatar_url": "https://avatars0.githubusercontent.com/u/28565097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gshaikov", "html_url": "https://github.com/gshaikov", "followers_url": "https://api.github.com/users/gshaikov/followers", "following_url": "https://api.github.com/users/gshaikov/following{/other_user}", "gists_url": "https://api.github.com/users/gshaikov/gists{/gist_id}", "starred_url": "https://api.github.com/users/gshaikov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gshaikov/subscriptions", "organizations_url": "https://api.github.com/users/gshaikov/orgs", "repos_url": "https://api.github.com/users/gshaikov/repos", "events_url": "https://api.github.com/users/gshaikov/events{/privacy}", "received_events_url": "https://api.github.com/users/gshaikov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-19T17:13:43Z", "updated_at": "2020-08-19T17:43:31Z", "closed_at": "2020-08-19T17:43:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n`AdminClient.create_topics` returns a map of Futures, each is supposed to block until an underlying topic gets created.\r\nHowever, the Futures are set to None before the topics are created.\r\n\r\nThe method in question:\r\nhttps://github.com/confluentinc/confluent-kafka-python/blob/5aa6439df081ead048932ee395085236ef0bca9d/confluent_kafka/admin/__init__.py#L292\r\n\r\nHow to reproduce\r\n================\r\n`confluent-kafka-python==1.5.0`\r\n`librdkafka==1.5.0`\r\nA cluster with broker `confluentinc/cp-server:5.4.0` running locally\r\n\r\nThis script illustrates the problem. The gist: after Future is set, the topic is still not in the list retrieved by Consumer.\r\n```\r\nfrom uuid import uuid4\r\nfrom time import sleep\r\n\r\nfrom confluent_kafka.avro import Consumer\r\nfrom confluent_kafka.admin import AdminClient, NewTopic\r\n\r\n\r\ndef topic_exists(topic_name: str) -> bool:\r\n    \"\"\"\r\n    Return True if topic with topic_name exists; False otherwise\r\n    \"\"\"\r\n    consumer_config = {\r\n        \"bootstrap.servers\": \"localhost:9092\",\r\n        \"group.id\": \"topic_exists_checker\",\r\n    }\r\n    con = Consumer(consumer_config)\r\n    cluster_metadata = con.list_topics()\r\n    con.close()\r\n    return topic_name in cluster_metadata.topics\r\n\r\n\r\ndef create_new_topic(topic_name: str) -> None:\r\n    \"\"\"\r\n    Create a topic with topic_name.\r\n    \"\"\"\r\n    admin_client = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\r\n    new_topic = NewTopic(topic_name, num_partitions=1, replication_factor=1)\r\n    futures_map = admin_client.create_topics([new_topic])\r\n    for topic, future in futures_map.items():\r\n        future.result()  # <---- supposed to block until the topic is actually created; doesn't block though\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    topic_name = str(uuid4())\r\n\r\n    assert topic_exists(topic_name) is False\r\n\r\n    create_new_topic(topic_name)\r\n\r\n    # sleep(1)  # <---- without sleeping for 1 second, the line below fails; uncommenting this line makes the assert pass\r\n\r\n    assert topic_exists(topic_name) is True\r\n\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [X] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [X] Apache Kafka broker version:\r\n - [X] Client configuration: `{...}`\r\n - [X] Operating system: MacOS 10.15.5\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/928", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/928/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/928/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/928/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/928", "id": 675592337, "node_id": "MDU6SXNzdWU2NzU1OTIzMzc=", "number": 928, "title": "got undefined symbol: : rd_kafka_consumer_group_metadata_write error", "user": {"login": "lcalviny", "id": 54179415, "node_id": "MDQ6VXNlcjU0MTc5NDE1", "avatar_url": "https://avatars0.githubusercontent.com/u/54179415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lcalviny", "html_url": "https://github.com/lcalviny", "followers_url": "https://api.github.com/users/lcalviny/followers", "following_url": "https://api.github.com/users/lcalviny/following{/other_user}", "gists_url": "https://api.github.com/users/lcalviny/gists{/gist_id}", "starred_url": "https://api.github.com/users/lcalviny/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lcalviny/subscriptions", "organizations_url": "https://api.github.com/users/lcalviny/orgs", "repos_url": "https://api.github.com/users/lcalviny/repos", "events_url": "https://api.github.com/users/lcalviny/events{/privacy}", "received_events_url": "https://api.github.com/users/lcalviny/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-08T21:41:07Z", "updated_at": "2020-08-18T14:14:12Z", "closed_at": "2020-08-18T14:14:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nfrom confluent_kafka.avro import AvroProducer gets ImportError: python3.7/site-packages/confluent_kafka/cimpl.cpython-37m-x86_64-linux-gnu.so: undefined symbol: rd_kafka_consumer_group_metadata_write\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n1, sudo yum install librdkafka-devel python-devel\r\n2, pip install confluent-kafka\r\n3, ln -s /usr/lib64/librdkafka-cedc1a93.so.1 python3.7/site-packages/confluent_kafka.libs/librdkafka.so.1\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/921", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/921/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/921/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/921/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/921", "id": 665250584, "node_id": "MDU6SXNzdWU2NjUyNTA1ODQ=", "number": 921, "title": "Tarball of 1.5.0 is not published on PyPi", "user": {"login": "dimon222", "id": 2671025, "node_id": "MDQ6VXNlcjI2NzEwMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/2671025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dimon222", "html_url": "https://github.com/dimon222", "followers_url": "https://api.github.com/users/dimon222/followers", "following_url": "https://api.github.com/users/dimon222/following{/other_user}", "gists_url": "https://api.github.com/users/dimon222/gists{/gist_id}", "starred_url": "https://api.github.com/users/dimon222/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dimon222/subscriptions", "organizations_url": "https://api.github.com/users/dimon222/orgs", "repos_url": "https://api.github.com/users/dimon222/repos", "events_url": "https://api.github.com/users/dimon222/events{/privacy}", "received_events_url": "https://api.github.com/users/dimon222/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-24T15:36:45Z", "updated_at": "2020-08-08T01:00:32Z", "closed_at": "2020-08-08T01:00:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nTarball with source code wasn't published for new version 1.5.0 to Pypi. Only wheels are there.\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n`pip install --no-binary :all: \"confluent-kafka==1.5.0\"`\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: Red Hat Linux 7\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/913", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/913/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/913/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/913/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/913", "id": 655389381, "node_id": "MDU6SXNzdWU2NTUzODkzODE=", "number": 913, "title": "Give developers ability to pre-load and check Avro registry schema IDs.", "user": {"login": "arthurlm", "id": 1571398, "node_id": "MDQ6VXNlcjE1NzEzOTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1571398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arthurlm", "html_url": "https://github.com/arthurlm", "followers_url": "https://api.github.com/users/arthurlm/followers", "following_url": "https://api.github.com/users/arthurlm/following{/other_user}", "gists_url": "https://api.github.com/users/arthurlm/gists{/gist_id}", "starred_url": "https://api.github.com/users/arthurlm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arthurlm/subscriptions", "organizations_url": "https://api.github.com/users/arthurlm/orgs", "repos_url": "https://api.github.com/users/arthurlm/repos", "events_url": "https://api.github.com/users/arthurlm/events{/privacy}", "received_events_url": "https://api.github.com/users/arthurlm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-12T12:17:49Z", "updated_at": "2020-08-13T09:36:12Z", "closed_at": "2020-08-13T09:36:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "# Description\r\n\r\nGive developers ability to pre-load and check Avro registry schema IDs.\r\n\r\n# Details description\r\n\r\nProblem is on function `AvroSerializer.__call__(self, obj, ctx)`.\r\n\r\nCurrently it does the serialization __and__ loading data from registry.\r\n\r\nThis results in really poor performances when inserting the first message into Kafka.\r\nIt is particularly visible in applications which restart often and does not produce messages frequently.\r\n\r\nSecond problem is when `auto.register.schemas` equals `False`.\r\nThe app crash only on first message insertion and not at init.\r\n\r\nMoreover (I have not tested it on my own), there might also be an error when same serializer instance is used for multiple topics.\r\nProblem may comes from having only one `self._schema_id` per `AvroSerializer`.\r\n\r\n# Example usage\r\n\r\nLoad / init the app (non critical section):\r\n\r\n```python\r\navro_serializer = AvroSerializer(json.dumps(schema), schema_registry)\r\n\r\n# Load registry info at init (feature request)\r\navro_serializer.load_registry_schema_id(SerializationContext(topic_name1, MessageField.VALUE))\r\navro_serializer.load_registry_schema_id(SerializationContext(topic_name2, MessageField.VALUE))\r\navro_serializer.load_registry_schema_id(SerializationContext(topic_name3, MessageField.KEY))\r\n\r\nproducer = SerializingProducer({\r\n    'bootstrap.servers': ...,\r\n    'key.serializer': avro_serializer,\r\n    'value.serializer': avro_serializer,\r\n})\r\n```\r\n\r\nCritical section (app need constant performances):\r\n\r\n```python\r\nproducer.produce(...)\r\n```\r\n\r\n# Example of implementation\r\n\r\nSee below a possible implementation for the new `__call__` function.\r\nPlease note that I have not run tests suite on it.\r\n\r\n```python\r\nclass AvroSerializer(Serializer):\r\n    ...\r\n\r\n    def load_registry_schema_id(self, ctx):\r\n        subject = self._subject_name_func(ctx, self._schema_name)\r\n\r\n        # Check to ensure this schema has been registered under subject_name.\r\n        if self._auto_register and subject not in self._known_subjects:\r\n            # The schema name will always be the same. We can't however register\r\n            # a schema without a subject so we set the schema_id here to handle\r\n            # the initial registration.\r\n            self._schema_ids[subject] = self._registry.register_schema(subject,\r\n                                                                       self._schema)\r\n            self._known_subjects.add(subject)\r\n        elif not self._auto_register and subject not in self._known_subjects:\r\n            registered_schema = self._registry.lookup_schema(subject,\r\n                                                             self._schema)\r\n            self._schema_ids[subject] = registered_schema.schema_id\r\n            self._known_subjects.add(subject)\r\n\r\n        return self._schema_ids[subject]\r\n\r\n    def __call__(self, obj, ctx):\r\n        if obj is None:\r\n            return None\r\n\r\n        schema_id = self.load_registry_schema_id(ctx)\r\n\r\n        if self._to_dict is not None:\r\n            value = self._to_dict(obj, ctx)\r\n        else:\r\n            value = obj\r\n\r\n        with _ContextStringIO() as fo:\r\n            # Write the magic byte and schema ID in network byte order (big endian)\r\n            fo.write(pack('>bI', _MAGIC_BYTE, schema_id))\r\n            # write the record to the rest of the buffer\r\n            schemaless_writer(fo, self._parsed_schema, value)\r\n\r\n            return fo.getvalue()\r\n```\r\n\r\nPlease note that `self._known_subjects` is only keep here for backward compatibility.\r\nIt can be replace by `self._schema_ids.keys()`.\r\n\r\n## Checklist\r\n\r\n- _Versions_: `confluent_kafka.version(1.4.2)`\r\n- _Operating system_: ALL\r\n- _Critical issue_: NO\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/911", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/911/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/911/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/911/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/911", "id": 653422350, "node_id": "MDU6SXNzdWU2NTM0MjIzNTA=", "number": 911, "title": "ProduceException, does it exist?", "user": {"login": "dmitrypolo", "id": 20093793, "node_id": "MDQ6VXNlcjIwMDkzNzkz", "avatar_url": "https://avatars1.githubusercontent.com/u/20093793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmitrypolo", "html_url": "https://github.com/dmitrypolo", "followers_url": "https://api.github.com/users/dmitrypolo/followers", "following_url": "https://api.github.com/users/dmitrypolo/following{/other_user}", "gists_url": "https://api.github.com/users/dmitrypolo/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmitrypolo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmitrypolo/subscriptions", "organizations_url": "https://api.github.com/users/dmitrypolo/orgs", "repos_url": "https://api.github.com/users/dmitrypolo/repos", "events_url": "https://api.github.com/users/dmitrypolo/events{/privacy}", "received_events_url": "https://api.github.com/users/dmitrypolo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-08T16:11:20Z", "updated_at": "2020-07-09T13:35:07Z", "closed_at": "2020-07-09T13:34:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nThe doc-string for the `SerializingProducer` mentions an exception called `ProduceException` as shown below.\r\nhttps://github.com/confluentinc/confluent-kafka-python/blob/624cdb8fe459b8910410f379cf2658c8d096436b/confluent_kafka/serializing_producer.py#L160\r\nHowever this doesn't seem to be an actual class in the repository. What is the proper error we should be using besides the serialization errors?\r\n\r\n`confluent_kafka` version `1.4.2`.\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/908", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/908/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/908/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/908/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/908", "id": 650590849, "node_id": "MDU6SXNzdWU2NTA1OTA4NDk=", "number": 908, "title": "Read kafka messages in parallel using confluent kafka in python", "user": {"login": "ankitpdc", "id": 51477387, "node_id": "MDQ6VXNlcjUxNDc3Mzg3", "avatar_url": "https://avatars2.githubusercontent.com/u/51477387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankitpdc", "html_url": "https://github.com/ankitpdc", "followers_url": "https://api.github.com/users/ankitpdc/followers", "following_url": "https://api.github.com/users/ankitpdc/following{/other_user}", "gists_url": "https://api.github.com/users/ankitpdc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankitpdc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankitpdc/subscriptions", "organizations_url": "https://api.github.com/users/ankitpdc/orgs", "repos_url": "https://api.github.com/users/ankitpdc/repos", "events_url": "https://api.github.com/users/ankitpdc/events{/privacy}", "received_events_url": "https://api.github.com/users/ankitpdc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-03T13:15:27Z", "updated_at": "2020-07-12T09:53:51Z", "closed_at": "2020-07-12T09:53:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description`\r\n===========\r\nI am new to Kafka. I have implemented a complete pipeline, reading messages from one Kafka topic, processing and finally storing output to another Kafka topic. Now, when I am trying to run the same in parallel, I can not find any reference code for the same. I have 3 partitions as of now, and I am looking forward to running 3 processes in parallel.\r\nI am using confluent Kafka package in python for consuming and producing the message.\r\n\r\nHow to reproduce\r\n================\r\n\r\n#################################\r\n\r\n```\r\nfrom confluent_kafka import Consumer\r\nconsumer = Consumer({\r\n        'bootstrap.servers': os.environ.get('BOOTSTRAP_SERVERS'),\r\n        'group.id': 'consumer_name',\r\n        'auto.offset.reset': 'earliest',\r\n        'enable.auto.commit': True\r\n    })\r\ninput_consumer_topic = os.environ.get('KAFKA_TOPIC')\r\npartition_number_in_topic = 3\r\n\r\ndef get_file_name_and_process_file():\r\n       consumer.subscribe([input_consumer_topic])\r\n        while True:\r\n            try:\r\n                msg = consumer.poll(10)\r\n            except SerializerError as e:\r\n                raise SerializerError\r\n            except Exception:\r\n                pass\r\n            if msg:\r\n                if msg.error():\r\n                    return\r\n                input_json = unpack(msg.value())  # unpack function written unpack messages, not shared here\r\n                output = some_fucntion(input_json)\r\n            else:\r\n                print(\"No Message!!\")\r\n```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/904", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/904/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/904/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/904/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/904", "id": 648140289, "node_id": "MDU6SXNzdWU2NDgxNDAyODk=", "number": 904, "title": "Get count of pending messages in kafka topic overall or per each partition", "user": {"login": "brshravan-tech", "id": 43596034, "node_id": "MDQ6VXNlcjQzNTk2MDM0", "avatar_url": "https://avatars0.githubusercontent.com/u/43596034?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brshravan-tech", "html_url": "https://github.com/brshravan-tech", "followers_url": "https://api.github.com/users/brshravan-tech/followers", "following_url": "https://api.github.com/users/brshravan-tech/following{/other_user}", "gists_url": "https://api.github.com/users/brshravan-tech/gists{/gist_id}", "starred_url": "https://api.github.com/users/brshravan-tech/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brshravan-tech/subscriptions", "organizations_url": "https://api.github.com/users/brshravan-tech/orgs", "repos_url": "https://api.github.com/users/brshravan-tech/repos", "events_url": "https://api.github.com/users/brshravan-tech/events{/privacy}", "received_events_url": "https://api.github.com/users/brshravan-tech/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-30T12:41:06Z", "updated_at": "2020-07-02T14:27:55Z", "closed_at": "2020-07-02T14:27:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nIs there any API to get count of pending messages in kafka topic (overall or per each partition).\r\n\r\nI am connecting to Azure EventHub using the confluent-kafka python library and following https://docs.confluent.io/current/clients/confluent-kafka-python/ for API references but could not find any API to get details of pending messages\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/902", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/902/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/902/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/902/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/902", "id": 645789547, "node_id": "MDU6SXNzdWU2NDU3ODk1NDc=", "number": 902, "title": "Check topic memory usage", "user": {"login": "tatianafrank", "id": 11413181, "node_id": "MDQ6VXNlcjExNDEzMTgx", "avatar_url": "https://avatars0.githubusercontent.com/u/11413181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianafrank", "html_url": "https://github.com/tatianafrank", "followers_url": "https://api.github.com/users/tatianafrank/followers", "following_url": "https://api.github.com/users/tatianafrank/following{/other_user}", "gists_url": "https://api.github.com/users/tatianafrank/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianafrank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianafrank/subscriptions", "organizations_url": "https://api.github.com/users/tatianafrank/orgs", "repos_url": "https://api.github.com/users/tatianafrank/repos", "events_url": "https://api.github.com/users/tatianafrank/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianafrank/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-25T19:10:24Z", "updated_at": "2020-06-29T07:37:27Z", "closed_at": "2020-06-29T07:37:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nIs there a way to check the total memory usage of a topic? We have a retention policy set to a max number of bytes and we're trying to check if we are hitting that max because messages are missing and we think that may be why. \r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): 1.4\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/901", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/901/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/901/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/901/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/901", "id": 645736502, "node_id": "MDU6SXNzdWU2NDU3MzY1MDI=", "number": 901, "title": "Troubles with kerberos+ssl builds", "user": {"login": "e-kolkhoz", "id": 9208205, "node_id": "MDQ6VXNlcjkyMDgyMDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9208205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/e-kolkhoz", "html_url": "https://github.com/e-kolkhoz", "followers_url": "https://api.github.com/users/e-kolkhoz/followers", "following_url": "https://api.github.com/users/e-kolkhoz/following{/other_user}", "gists_url": "https://api.github.com/users/e-kolkhoz/gists{/gist_id}", "starred_url": "https://api.github.com/users/e-kolkhoz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/e-kolkhoz/subscriptions", "organizations_url": "https://api.github.com/users/e-kolkhoz/orgs", "repos_url": "https://api.github.com/users/e-kolkhoz/repos", "events_url": "https://api.github.com/users/e-kolkhoz/events{/privacy}", "received_events_url": "https://api.github.com/users/e-kolkhoz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-25T17:34:19Z", "updated_at": "2020-08-07T10:37:54Z", "closed_at": "2020-08-07T10:37:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWe have a project with consumer ssl connection to one kafka cluster (Cluster A) and sasl/gssapi/kerberos producer connection to another kafka cluster (Cluster B).\r\n\r\n1. This https://github.com/edenhill/librdkafka/wiki/Using-SSL-with-librdkafka works fine with Cluster A (SSL)\r\n `pip3 install confluent-kafka==1.2.0 `\r\n\r\n2. This https://github.com/edenhill/librdkafka/wiki/Using-SASL-with-librdkafka works ok with Cluster B (SASL_PLAINTEXT)\r\n```\r\napt install librdkafka-dev\r\npip3 install --no-binary :all: confluent-kafka\r\n\r\n```\r\nBUT! this no-binary driver build doesn't work with SSL anymore. \r\n`%7|1593100516.978|BROKERFAIL|rdkafka#consumer-1| [thrd:ssl://clusterA_kafka_ip_here:9292/bootstrap]: ssl://clusterA_kafka_ip_here:9292/bootstrap: failed: err: Local: Broker transport failure: (errno: Success)\r\n`\r\nThen if we install vanilla driver `pip3 install confluent-kafka==1.2.0 ` this error disappears.\r\n\r\n3. This chinese workaround (with spoofing librdkafka binary on vanilla python driver) http://fuxkdb.com/2019/03/08/%E4%BD%BFconfluent_kafka%E6%94%AF%E6%8C%81SASL_PLAINTEXT/ (may use googletranslate to read) \r\nworks fine with SSL (!) on our sandbox kafka cluster (Cluster C), but on Cluster A we have another error\r\n\r\n`%7|1592995306.337|FAIL|rdkafka#consumer-1| [thrd:ssl://clusterA_kafka_ip_here:9292/bootstrap]: ssl://clusterA_kafka_ip_here:9292/bootstrap: SSL handshake failed: ../ssl/record/ssl3_record.c:332: error: 1408F10B:SSL routines: ssl3_get_record:wrong version number (after 27 ms in state CONNECT) (_SSL)\r\n`\r\n\r\nWhich versions (linux distro, confluent-kafka-python, librdkafka) should we use to establish connection with Cluster A (SSL) and produce messages to Cluster B (SASL_PLAINTEXT, sasl/gssapi/kerberos)?  \r\n\r\nWhat are we doing wrong with the procedure? May be we should use another way to connect both clusters?\r\n\r\nHow to reproduce\r\n================\r\non debian:bullseye-slim\r\n```\r\napt install librdkafka-dev\r\npip3 install --no-binary :all: confluent-kafka\r\n```\r\nin python3.6\r\n```python\r\nfrom confluent_kafka import Producer, Consumer\r\nconfigs = {\"bootstrap.servers\": kafka_servers,\r\n           \"security.protocol\": \"ssl\",\r\n           \"ssl.ca.location\":\"/etc/kafka/ssl/kafka-ca-cert\",\r\n           \"ssl.certificate.location\":\"/etc/kafka/ssl/kafka-client.pem\",\r\n           \"ssl.key.location\":\"/etc/kafka/ssl/kafka-client.key\",\r\n           \"ssl.key.password\":ssl_pass,\r\n    }\r\nconfigs['group.id'] = some_group\r\n\r\nc = Consumer(configs)\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [+] confluent-kafka-python==1.2.0, librdkafka=('1.4.2', 17040127):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [+] Operating system: debian:bullseye-slim (docker image)\r\n - [+] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [+] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/894", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/894/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/894/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/894/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/894", "id": 635179157, "node_id": "MDU6SXNzdWU2MzUxNzkxNTc=", "number": 894, "title": "Support for KIP-429: Incremental rebalancing", "user": {"login": "bry00", "id": 14224386, "node_id": "MDQ6VXNlcjE0MjI0Mzg2", "avatar_url": "https://avatars0.githubusercontent.com/u/14224386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bry00", "html_url": "https://github.com/bry00", "followers_url": "https://api.github.com/users/bry00/followers", "following_url": "https://api.github.com/users/bry00/following{/other_user}", "gists_url": "https://api.github.com/users/bry00/gists{/gist_id}", "starred_url": "https://api.github.com/users/bry00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bry00/subscriptions", "organizations_url": "https://api.github.com/users/bry00/orgs", "repos_url": "https://api.github.com/users/bry00/repos", "events_url": "https://api.github.com/users/bry00/events{/privacy}", "received_events_url": "https://api.github.com/users/bry00/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-09T07:25:01Z", "updated_at": "2020-06-15T04:16:10Z", "closed_at": "2020-06-15T04:16:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Implement support for [KIP-429: Incremental rebalancing](https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/890", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/890/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/890/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/890/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/890", "id": 630690672, "node_id": "MDU6SXNzdWU2MzA2OTA2NzI=", "number": 890, "title": "Wrong Documentation in Schema Registry Client in confluent kafka 1.4.2", "user": {"login": "safaozturk93", "id": 15890268, "node_id": "MDQ6VXNlcjE1ODkwMjY4", "avatar_url": "https://avatars3.githubusercontent.com/u/15890268?v=4", "gravatar_id": "", "url": "https://api.github.com/users/safaozturk93", "html_url": "https://github.com/safaozturk93", "followers_url": "https://api.github.com/users/safaozturk93/followers", "following_url": "https://api.github.com/users/safaozturk93/following{/other_user}", "gists_url": "https://api.github.com/users/safaozturk93/gists{/gist_id}", "starred_url": "https://api.github.com/users/safaozturk93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/safaozturk93/subscriptions", "organizations_url": "https://api.github.com/users/safaozturk93/orgs", "repos_url": "https://api.github.com/users/safaozturk93/repos", "events_url": "https://api.github.com/users/safaozturk93/events{/privacy}", "received_events_url": "https://api.github.com/users/safaozturk93/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 932257743, "node_id": "MDU6TGFiZWw5MzIyNTc3NDM=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/docs", "name": "docs", "color": "c5def5", "default": false, "description": "Improve docs"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-04T10:08:47Z", "updated_at": "2020-06-16T14:34:40Z", "closed_at": "2020-06-16T14:34:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nFor Schema Registry Client, `ssl.ca.location` specified with `*` as a required field.\r\n\r\nhttps://github.com/confluentinc/confluent-kafka-python/blob/master/confluent_kafka/schema_registry/schema_registry_client.py#L263\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [1.4.2] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/889", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/889/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/889/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/889/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/889", "id": 627998027, "node_id": "MDU6SXNzdWU2Mjc5OTgwMjc=", "number": 889, "title": "confluent-kafka not installing in Python 3.8", "user": {"login": "dineshTivo", "id": 56425141, "node_id": "MDQ6VXNlcjU2NDI1MTQx", "avatar_url": "https://avatars1.githubusercontent.com/u/56425141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dineshTivo", "html_url": "https://github.com/dineshTivo", "followers_url": "https://api.github.com/users/dineshTivo/followers", "following_url": "https://api.github.com/users/dineshTivo/following{/other_user}", "gists_url": "https://api.github.com/users/dineshTivo/gists{/gist_id}", "starred_url": "https://api.github.com/users/dineshTivo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dineshTivo/subscriptions", "organizations_url": "https://api.github.com/users/dineshTivo/orgs", "repos_url": "https://api.github.com/users/dineshTivo/repos", "events_url": "https://api.github.com/users/dineshTivo/events{/privacy}", "received_events_url": "https://api.github.com/users/dineshTivo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-31T16:35:41Z", "updated_at": "2020-06-15T03:59:24Z", "closed_at": "2020-06-15T03:59:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWhen try to install confluent-kafka through _python -m pip install confluent-kafka_ command in windows 10 for Python 3.8, getting below error message whereas it was installed successfully in Python 3.7.\r\n\r\nNote: Wheel file not available for Python 3.8 Windows build in https://pypi.org/project/confluent-kafka/#files\r\n\r\n_creating build\\temp.win32-3.8\\Release\\Users\\dprabhu\\AppData\\Local\\Temp\\pip-install-zghtb4kp\\confluent-kafka\\confluent_kafka\\src\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD \"-IC:\\Program Files (x86)\\Python38-32\\include\" \"-IC:\\Program Files (x86)\\Python38-32\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt\" /TcC:\\Users\\dprabhu\\AppData\\Local\\Temp\\pip-install-zghtb4kp\\confluent-kafka\\confluent_kafka\\src\\confluent_kafka.c /Fobuild\\temp.win32-3.8\\Release\\Users\\dprabhu\\AppData\\Local\\Temp\\pip-install-zghtb4kp\\confluent-kafka\\confluent_kafka\\src\\confluent_kafka.obj\r\n  confluent_kafka.c\r\n  c:\\users\\dprabhu\\appdata\\local\\temp\\pip-install-zghtb4kp\\confluent-kafka\\confluent_kafka\\src\\confluent_kafka.h(22): fatal error C1083: Cannot open include file: 'librdkafka/rdkafka.h': No such file or directory\r\n  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\BIN\\\\cl.exe' failed with exit status 2_\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for confluent-kafka\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n _python -m pip install confluent-kafka_ command in windows 10 for Python 3.8\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): 1.4.2\r\n - [ ] Apache Kafka broker version: \r\n - [ ] Client configuration: `{...}`\r\n - [] Operating system: Windows 10\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary): \r\n C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD \"-IC:\\Program Files (x86)\\Python38-32\\include\" \"-IC:\\Program Files (x86)\\Python38-32\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt\" /TcC:\\Users\\dprabhu\\AppData\\Local\\Temp\\pip-install-zghtb4kp\\confluent-kafka\\confluent_kafka\\src\\confluent_kafka.c /Fobuild\\temp.win32-3.8\\Release\\Users\\dprabhu\\AppData\\Local\\Temp\\pip-install-zghtb4kp\\confluent-kafka\\confluent_kafka\\src\\confluent_kafka.obj\r\n  confluent_kafka.c\r\n  c:\\users\\dprabhu\\appdata\\local\\temp\\pip-install-zghtb4kp\\confluent-kafka\\confluent_kafka\\src\\confluent_kafka.h(22): fatal error C1083: Cannot open include file: 'librdkafka/rdkafka.h': No such file or directory\r\n  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 14.0\\\\VC\\\\BIN\\\\cl.exe' failed with exit status 2\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/888", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/888/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/888/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/888/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/888", "id": 627730435, "node_id": "MDU6SXNzdWU2Mjc3MzA0MzU=", "number": 888, "title": "Message timed out", "user": {"login": "hhhhman", "id": 30335689, "node_id": "MDQ6VXNlcjMwMzM1Njg5", "avatar_url": "https://avatars0.githubusercontent.com/u/30335689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hhhhman", "html_url": "https://github.com/hhhhman", "followers_url": "https://api.github.com/users/hhhhman/followers", "following_url": "https://api.github.com/users/hhhhman/following{/other_user}", "gists_url": "https://api.github.com/users/hhhhman/gists{/gist_id}", "starred_url": "https://api.github.com/users/hhhhman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hhhhman/subscriptions", "organizations_url": "https://api.github.com/users/hhhhman/orgs", "repos_url": "https://api.github.com/users/hhhhman/repos", "events_url": "https://api.github.com/users/hhhhman/events{/privacy}", "received_events_url": "https://api.github.com/users/hhhhman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-30T13:18:52Z", "updated_at": "2020-05-31T01:47:41Z", "closed_at": "2020-05-31T01:47:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nl use docker-compse.yml\r\n### ### ### ### ### ### ### ### ### \r\nversion: '2'\r\nservices:\r\n  zookeeper:\r\n    image: confluentinc/cp-zookeeper:latest\r\n    network_mode: host\r\n    environment:\r\n      ZOOKEEPER_CLIENT_PORT: 32181\r\n      ZOOKEEPER_TICK_TIME: 2000\r\n\r\n  kafka:\r\n    image: confluentinc/cp-kafka:latest\r\n    network_mode: host\r\n    depends_on:\r\n      - zookeeper\r\n    ports:\r\n      - 9092:9092\r\n    environment:\r\n      KAFKA_BROKER_ID: 100\r\n      KAFKA_ZOOKEEPER_CONNECT: 192.168.162.138:32181  \r\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://192.168.162.138:29092,PLAINTEXT_HOST://192.168.162.138:9092 \r\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\r\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\r\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\r\n      KAFKA_NUM_PARTITIONS: 3\r\n\r\n\r\n### ### ### ### ### ### ### ### ### ### ### \r\n\r\n_l can produce and comsumer in my container\r\nbut l git librdkafka and confluent-kafka-python\r\nand use the example with producer  in con-kafka-python_\r\n### ### ### ### ### ### ### ### ### ### ### ### \r\n\r\nfrom confluent_kafka import Producer\r\n\r\n\r\np = Producer({'bootstrap.servers': '192.168.162.138:29092'})\r\n\r\ndef delivery_report(err, msg):\r\n    \"\"\" Called once for each message produced to indicate delivery result.\r\n        Triggered by poll() or flush(). \"\"\"\r\n    if err is not None:\r\n        print('Message delivery failed: {}'.format(err))\r\n    else:\r\n        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\r\n\r\nfor data in [\"eee%d\"%i for i in range(10)]:\r\n    # Trigger any available delivery report callbacks from previous produce() calls\r\n    p.poll(0)\r\n\r\n    # Asynchronously produce a message, the delivery report callback\r\n    # will be triggered from poll() above, or flush() below, when the message has\r\n    # been successfully delivered or failed permanently.\r\n    p.produce('mytopic', data.encode('utf-8'), callback=delivery_report)\r\n\r\n# Wait for any outstanding messages to be delivered and delivery report\r\n# callbacks to be triggered.\r\np.flush()\r\n\r\nif __name__ == '__main__':\r\n    pass\r\n\r\n### ### ### ### ### ### ### ### ### ### ### ### ### ### \r\n\r\nit raise \r\n\r\n> Message` delivery failed: KafkaError{code=_MSG_TIMED_OUT,val=-192,str=\"Local: Message timed \r\n\r\n- out\"}\r\n\r\n\r\n\r\nwhy????????????????????????????????????????????????????????????????????????????????\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/885", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/885/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/885/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/885/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/885", "id": 625437298, "node_id": "MDU6SXNzdWU2MjU0MzcyOTg=", "number": 885, "title": "Is poll must there are no callbacks?", "user": {"login": "vikramindian", "id": 22010146, "node_id": "MDQ6VXNlcjIyMDEwMTQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/22010146?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vikramindian", "html_url": "https://github.com/vikramindian", "followers_url": "https://api.github.com/users/vikramindian/followers", "following_url": "https://api.github.com/users/vikramindian/following{/other_user}", "gists_url": "https://api.github.com/users/vikramindian/gists{/gist_id}", "starred_url": "https://api.github.com/users/vikramindian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vikramindian/subscriptions", "organizations_url": "https://api.github.com/users/vikramindian/orgs", "repos_url": "https://api.github.com/users/vikramindian/repos", "events_url": "https://api.github.com/users/vikramindian/events{/privacy}", "received_events_url": "https://api.github.com/users/vikramindian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-27T07:06:44Z", "updated_at": "2020-06-15T04:00:01Z", "closed_at": "2020-06-15T04:00:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nHi, \r\nIs calling poll() mandatory when we do not register any callbacks (both error and delivery) for the producer?\r\nWhat all things poll() do, apart from triggering callbacks?\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/884", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/884/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/884/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/884/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/884", "id": 625173050, "node_id": "MDU6SXNzdWU2MjUxNzMwNTA=", "number": 884, "title": "Producing message to MSK", "user": {"login": "ahoora08", "id": 24634145, "node_id": "MDQ6VXNlcjI0NjM0MTQ1", "avatar_url": "https://avatars3.githubusercontent.com/u/24634145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahoora08", "html_url": "https://github.com/ahoora08", "followers_url": "https://api.github.com/users/ahoora08/followers", "following_url": "https://api.github.com/users/ahoora08/following{/other_user}", "gists_url": "https://api.github.com/users/ahoora08/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahoora08/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahoora08/subscriptions", "organizations_url": "https://api.github.com/users/ahoora08/orgs", "repos_url": "https://api.github.com/users/ahoora08/repos", "events_url": "https://api.github.com/users/ahoora08/events{/privacy}", "received_events_url": "https://api.github.com/users/ahoora08/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-26T20:20:35Z", "updated_at": "2020-05-27T13:53:51Z", "closed_at": "2020-05-27T13:53:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/877", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/877/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/877/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/877/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/877", "id": 621969737, "node_id": "MDU6SXNzdWU2MjE5Njk3Mzc=", "number": 877, "title": "Provide manylinux2014 wheel for aarch64", "user": {"login": "hrw", "id": 185508, "node_id": "MDQ6VXNlcjE4NTUwOA==", "avatar_url": "https://avatars3.githubusercontent.com/u/185508?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hrw", "html_url": "https://github.com/hrw", "followers_url": "https://api.github.com/users/hrw/followers", "following_url": "https://api.github.com/users/hrw/following{/other_user}", "gists_url": "https://api.github.com/users/hrw/gists{/gist_id}", "starred_url": "https://api.github.com/users/hrw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hrw/subscriptions", "organizations_url": "https://api.github.com/users/hrw/orgs", "repos_url": "https://api.github.com/users/hrw/repos", "events_url": "https://api.github.com/users/hrw/events{/privacy}", "received_events_url": "https://api.github.com/users/hrw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-20T17:53:13Z", "updated_at": "2020-06-15T04:15:22Z", "closed_at": "2020-06-15T04:15:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nconfluent_kafka is not installable on AArch64 architecture. Debian stable, Ubuntu xenial/bionic/focal, CentOS 7/8 all have too old 'librdkafka' package (as 1.4.0+ is needed).\r\n\r\nHow to reproduce\r\n================\r\n\r\npip install confluent_kafka\r\n\r\nSuggestion\r\n=======\r\n\r\nI see that you are using Travis CI to build wheels. Will take a look and try to add aarch64 manylinux2014 target there.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/875", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/875/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/875/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/875/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/875", "id": 621623382, "node_id": "MDU6SXNzdWU2MjE2MjMzODI=", "number": 875, "title": "Problem installing on raspberry pi 4", "user": {"login": "DeWarmte", "id": 62253114, "node_id": "MDQ6VXNlcjYyMjUzMTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/62253114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DeWarmte", "html_url": "https://github.com/DeWarmte", "followers_url": "https://api.github.com/users/DeWarmte/followers", "following_url": "https://api.github.com/users/DeWarmte/following{/other_user}", "gists_url": "https://api.github.com/users/DeWarmte/gists{/gist_id}", "starred_url": "https://api.github.com/users/DeWarmte/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DeWarmte/subscriptions", "organizations_url": "https://api.github.com/users/DeWarmte/orgs", "repos_url": "https://api.github.com/users/DeWarmte/repos", "events_url": "https://api.github.com/users/DeWarmte/events{/privacy}", "received_events_url": "https://api.github.com/users/DeWarmte/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-20T09:49:50Z", "updated_at": "2020-05-20T15:04:16Z", "closed_at": "2020-05-20T11:35:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nError message on installation using `pip3 install confluent-kafka`.\r\nI suspect it has something to do with the dependency librdkafka-dev being to low version. However I'm unable to upgrade the version so unable to resolve this problem. \r\nIn the error code I found the link: \"http://docs.confluent.io/current/installation.html\". From here I downloaded the zip file and unzipped it as instructed but this did not resolve the problem.\r\n\r\nSystem version\r\n================\r\npi@raspberrypi:~ $ `cat /etc/os-release`\r\nPRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\"\r\nNAME=\"Raspbian GNU/Linux\"\r\nVERSION_ID=\"10\"\r\nVERSION=\"10 (buster)\"\r\nVERSION_CODENAME=buster\r\nID=raspbian\r\nID_LIKE=debian\r\nHOME_URL=\"http://www.raspbian.org/\"\r\nSUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\"\r\nBUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\"\r\n\r\n\r\n\r\nComplete code & feedback log\r\n===============\r\npi@raspberrypi:~ $ `apt policy librdkafka-dev`\r\nlibrdkafka-dev:\r\n  Installed: 0.11.6-1.1\r\n  Candidate: 0.11.6-1.1\r\n  Version table:\r\n *** 0.11.6-1.1 500\r\n        500 http://raspbian.raspberrypi.org/raspbian buster/main armhf Packages\r\n        100 /var/lib/dpkg/status\r\npi@raspberrypi:~ $ `sudo apt update`\r\nHit:1 http://archive.raspberrypi.org/debian buster InRelease\r\nHit:2 http://raspbian.raspberrypi.org/raspbian buster InRelease\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nAll packages are up to date.\r\npi@raspberrypi:~ $ `pip3 install confluent-kafka`\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nCollecting confluent-kafka\r\n  Using cached https://files.pythonhosted.org/packages/28/51/710ab74d6d9435870bed97af21dea04a5828e1e3b2b8ca80fd79bae01b99/confluent-kafka-1.4.1.tar.gz\r\nBuilding wheels for collected packages: confluent-kafka\r\n  Running setup.py bdist_wheel for confluent-kafka ... error\r\n  Complete output from command /usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-gix8w43o/confluent-kafka/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-eoae1kv1 --python-tag cp37:\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-armv7l-3.7\r\n  creating build/lib.linux-armv7l-3.7/examples\r\n  copying examples/consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/protobuf_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/json_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/confluent_cloud.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/protobuf_consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/adminapi.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/avro_consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/asyncio.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/json_consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/sasl_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/eos-transactions.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/__init__.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/user_pb2.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/producer.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/avro-cli.py -> build/lib.linux-armv7l-3.7/examples\r\n  copying examples/avro_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n  creating build/lib.linux-armv7l-3.7/confluent_kafka\r\n  copying confluent_kafka/deserializing_consumer.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n  copying confluent_kafka/error.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n  copying confluent_kafka/serializing_producer.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n  copying confluent_kafka/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n  creating build/lib.linux-armv7l-3.7/examples/tests\r\n  copying examples/tests/conftest.py -> build/lib.linux-armv7l-3.7/examples/tests\r\n  copying examples/tests/__init__.py -> build/lib.linux-armv7l-3.7/examples/tests\r\n  creating build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n  copying confluent_kafka/kafkatest/verifiable_producer.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n  copying confluent_kafka/kafkatest/verifiable_consumer.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n  copying confluent_kafka/kafkatest/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n  copying confluent_kafka/kafkatest/verifiable_client.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n  creating build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n  copying confluent_kafka/schema_registry/json_schema.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n  copying confluent_kafka/schema_registry/avro.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n  copying confluent_kafka/schema_registry/protobuf.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n  copying confluent_kafka/schema_registry/schema_registry_client.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n  copying confluent_kafka/schema_registry/error.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n  copying confluent_kafka/schema_registry/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n  creating build/lib.linux-armv7l-3.7/confluent_kafka/admin\r\n  copying confluent_kafka/admin/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/admin\r\n  creating build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n  copying confluent_kafka/avro/cached_schema_registry_client.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n  copying confluent_kafka/avro/error.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n  copying confluent_kafka/avro/load.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n  copying confluent_kafka/avro/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n  creating build/lib.linux-armv7l-3.7/confluent_kafka/serialization\r\n  copying confluent_kafka/serialization/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/serialization\r\n  creating build/lib.linux-armv7l-3.7/confluent_kafka/avro/serializer\r\n  copying confluent_kafka/avro/serializer/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro/serializer\r\n  copying confluent_kafka/avro/serializer/message_serializer.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro/serializer\r\n  running build_ext\r\n  building 'confluent_kafka.cimpl' extension\r\n  creating build/temp.linux-armv7l-3.7\r\n  creating build/temp.linux-armv7l-3.7/tmp\r\n  creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o\r\n  creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka\r\n  creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka\r\n  creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src\r\n  arm-linux-gnueabihf-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c -o build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.o\r\n  In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:65:2: error: #error \"confluent-kafka-python requires librdkafka v1.4.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"\r\n   #error \"confluent-kafka-python requires librdkafka v1.4.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"\r\n    ^~~~~\r\n  In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:189:46: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n   PyObject *KafkaError_new_from_error_destroy (rd_kafka_error_t *error);\r\n                                                ^~~~~~~~~~~~~~~~\r\n                                                rd_kafka_event_t\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:386:31: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019\r\n   PyObject *c_cgmd_to_py (const rd_kafka_consumer_group_metadata_t *cgmd);\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:387:1: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019; did you mean \u2018rd_kafka_metadata_t\u2019?\r\n   rd_kafka_consumer_group_metadata_t *py_to_c_cgmd (PyObject *obj);\r\n   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   rd_kafka_metadata_t\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:387:46: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n   PyObject *KafkaError_new_from_error_destroy (rd_kafka_error_t *error) {\r\n                                                ^~~~~~~~~~~~~~~~\r\n                                                rd_kafka_event_t\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1294:31: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019\r\n   PyObject *c_cgmd_to_py (const rd_kafka_consumer_group_metadata_t *cgmd) {\r\n                                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: In function \u2018c_cgmd_to_py\u2019:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1298:9: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n           rd_kafka_error_t *error;\r\n           ^~~~~~~~~~~~~~~~\r\n           rd_kafka_event_t\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1300:17: warning: implicit declaration of function \u2018rd_kafka_consumer_group_metadata_write\u2019; did you mean \u2018rd_kafka_consume_start_queue\u2019? [-Wimplicit-function-declaration]\r\n           error = rd_kafka_consumer_group_metadata_write(cgmd, &buffer, &size);\r\n                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                   rd_kafka_consume_start_queue\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1300:15: warning: assignment to \u2018int *\u2019 from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n           error = rd_kafka_consumer_group_metadata_write(cgmd, &buffer, &size);\r\n                 ^\r\n  In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: warning: implicit declaration of function \u2018KafkaError_new_from_error_destroy\u2019; did you mean \u2018cfl_PyErr_from_error_destroy\u2019? [-Wimplicit-function-declaration]\r\n     PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: note: in definition of macro \u2018cfl_PyErr_from_error_destroy\u2019\r\n     PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: warning: initialization of \u2018PyObject *\u2019 {aka \u2018struct _object *\u2019} from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n     PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: note: in definition of macro \u2018cfl_PyErr_from_error_destroy\u2019\r\n     PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: At top level:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1317:1: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019; did you mean \u2018rd_kafka_metadata_t\u2019?\r\n   rd_kafka_consumer_group_metadata_t *py_to_c_cgmd (PyObject *obj) {\r\n   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   rd_kafka_metadata_t\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: In function \u2018py_to_c_cgmd\u2019:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1318:9: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019; did you mean \u2018rd_kafka_metadata_t\u2019?\r\n           rd_kafka_consumer_group_metadata_t *cgmd;\r\n           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           rd_kafka_metadata_t\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1319:9: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n           rd_kafka_error_t *error;\r\n           ^~~~~~~~~~~~~~~~\r\n           rd_kafka_event_t\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1326:17: warning: implicit declaration of function \u2018rd_kafka_consumer_group_metadata_read\u2019; did you mean \u2018rd_kafka_consume_start_queue\u2019? [-Wimplicit-function-declaration]\r\n           error = rd_kafka_consumer_group_metadata_read(&cgmd,\r\n                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                   rd_kafka_consume_start_queue\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1326:15: warning: assignment to \u2018int *\u2019 from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n           error = rd_kafka_consumer_group_metadata_read(&cgmd,\r\n                 ^\r\n  In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: warning: initialization of \u2018PyObject *\u2019 {aka \u2018struct _object *\u2019} from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n     PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: note: in definition of macro \u2018cfl_PyErr_from_error_destroy\u2019\r\n     PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: In function \u2018error_cb\u2019:\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1356:20: error: \u2018RD_KAFKA_RESP_ERR__FATAL\u2019 undeclared (first use in this function); did you mean \u2018RD_KAFKA_RESP_ERR__FAIL\u2019?\r\n           if (err == RD_KAFKA_RESP_ERR__FATAL) {\r\n                      ^~~~~~~~~~~~~~~~~~~~~~~~\r\n                      RD_KAFKA_RESP_ERR__FAIL\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1356:20: note: each undeclared identifier is reported only once for each function it appears in\r\n  /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1358:23: warning: implicit declaration of function \u2018rd_kafka_fatal_error\u2019; did you mean \u2018rd_kafka_last_error\u2019? [-Wimplicit-function-declaration]\r\n                   err = rd_kafka_fatal_error(rk, errstr, sizeof(errstr));\r\n                         ^~~~~~~~~~~~~~~~~~~~\r\n                         rd_kafka_last_error\r\n  error: command 'arm-linux-gnueabihf-gcc' failed with exit status 1\r\n  \r\n  ----------------------------------------\r\n  Failed building wheel for confluent-kafka\r\n  Running setup.py clean for confluent-kafka\r\nFailed to build confluent-kafka\r\nInstalling collected packages: confluent-kafka\r\n  Running setup.py install for confluent-kafka ... error\r\n    Complete output from command /usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-gix8w43o/confluent-kafka/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-vfh1yxgc/install-record.txt --single-version-externally-managed --compile --user --prefix=:\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.linux-armv7l-3.7\r\n    creating build/lib.linux-armv7l-3.7/examples\r\n    copying examples/consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/protobuf_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/json_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/confluent_cloud.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/protobuf_consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/adminapi.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/avro_consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/asyncio.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/json_consumer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/sasl_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/eos-transactions.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/__init__.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/user_pb2.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/producer.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/avro-cli.py -> build/lib.linux-armv7l-3.7/examples\r\n    copying examples/avro_producer.py -> build/lib.linux-armv7l-3.7/examples\r\n    creating build/lib.linux-armv7l-3.7/confluent_kafka\r\n    copying confluent_kafka/deserializing_consumer.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n    copying confluent_kafka/error.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n    copying confluent_kafka/serializing_producer.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n    copying confluent_kafka/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka\r\n    creating build/lib.linux-armv7l-3.7/examples/tests\r\n    copying examples/tests/conftest.py -> build/lib.linux-armv7l-3.7/examples/tests\r\n    copying examples/tests/__init__.py -> build/lib.linux-armv7l-3.7/examples/tests\r\n    creating build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n    copying confluent_kafka/kafkatest/verifiable_producer.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n    copying confluent_kafka/kafkatest/verifiable_consumer.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n    copying confluent_kafka/kafkatest/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n    copying confluent_kafka/kafkatest/verifiable_client.py -> build/lib.linux-armv7l-3.7/confluent_kafka/kafkatest\r\n    creating build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n    copying confluent_kafka/schema_registry/json_schema.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n    copying confluent_kafka/schema_registry/avro.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n    copying confluent_kafka/schema_registry/protobuf.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n    copying confluent_kafka/schema_registry/schema_registry_client.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n    copying confluent_kafka/schema_registry/error.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n    copying confluent_kafka/schema_registry/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/schema_registry\r\n    creating build/lib.linux-armv7l-3.7/confluent_kafka/admin\r\n    copying confluent_kafka/admin/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/admin\r\n    creating build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n    copying confluent_kafka/avro/cached_schema_registry_client.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n    copying confluent_kafka/avro/error.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n    copying confluent_kafka/avro/load.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n    copying confluent_kafka/avro/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro\r\n    creating build/lib.linux-armv7l-3.7/confluent_kafka/serialization\r\n    copying confluent_kafka/serialization/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/serialization\r\n    creating build/lib.linux-armv7l-3.7/confluent_kafka/avro/serializer\r\n    copying confluent_kafka/avro/serializer/__init__.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro/serializer\r\n    copying confluent_kafka/avro/serializer/message_serializer.py -> build/lib.linux-armv7l-3.7/confluent_kafka/avro/serializer\r\n    running build_ext\r\n    building 'confluent_kafka.cimpl' extension\r\n    creating build/temp.linux-armv7l-3.7\r\n    creating build/temp.linux-armv7l-3.7/tmp\r\n    creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o\r\n    creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka\r\n    creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka\r\n    creating build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src\r\n    arm-linux-gnueabihf-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -c /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c -o build/temp.linux-armv7l-3.7/tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.o\r\n    In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:65:2: error: #error \"confluent-kafka-python requires librdkafka v1.4.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"\r\n     #error \"confluent-kafka-python requires librdkafka v1.4.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"\r\n      ^~~~~\r\n    In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:189:46: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n     PyObject *KafkaError_new_from_error_destroy (rd_kafka_error_t *error);\r\n                                                  ^~~~~~~~~~~~~~~~\r\n                                                  rd_kafka_event_t\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:386:31: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019\r\n     PyObject *c_cgmd_to_py (const rd_kafka_consumer_group_metadata_t *cgmd);\r\n                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:387:1: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019; did you mean \u2018rd_kafka_metadata_t\u2019?\r\n     rd_kafka_consumer_group_metadata_t *py_to_c_cgmd (PyObject *obj);\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n     rd_kafka_metadata_t\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:387:46: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n     PyObject *KafkaError_new_from_error_destroy (rd_kafka_error_t *error) {\r\n                                                  ^~~~~~~~~~~~~~~~\r\n                                                  rd_kafka_event_t\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1294:31: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019\r\n     PyObject *c_cgmd_to_py (const rd_kafka_consumer_group_metadata_t *cgmd) {\r\n                                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: In function \u2018c_cgmd_to_py\u2019:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1298:9: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n             rd_kafka_error_t *error;\r\n             ^~~~~~~~~~~~~~~~\r\n             rd_kafka_event_t\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1300:17: warning: implicit declaration of function \u2018rd_kafka_consumer_group_metadata_write\u2019; did you mean \u2018rd_kafka_consume_start_queue\u2019? [-Wimplicit-function-declaration]\r\n             error = rd_kafka_consumer_group_metadata_write(cgmd, &buffer, &size);\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                     rd_kafka_consume_start_queue\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1300:15: warning: assignment to \u2018int *\u2019 from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n             error = rd_kafka_consumer_group_metadata_write(cgmd, &buffer, &size);\r\n                   ^\r\n    In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: warning: implicit declaration of function \u2018KafkaError_new_from_error_destroy\u2019; did you mean \u2018cfl_PyErr_from_error_destroy\u2019? [-Wimplicit-function-declaration]\r\n       PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: note: in definition of macro \u2018cfl_PyErr_from_error_destroy\u2019\r\n       PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: warning: initialization of \u2018PyObject *\u2019 {aka \u2018struct _object *\u2019} from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n       PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: note: in definition of macro \u2018cfl_PyErr_from_error_destroy\u2019\r\n       PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: At top level:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1317:1: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019; did you mean \u2018rd_kafka_metadata_t\u2019?\r\n     rd_kafka_consumer_group_metadata_t *py_to_c_cgmd (PyObject *obj) {\r\n     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n     rd_kafka_metadata_t\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: In function \u2018py_to_c_cgmd\u2019:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1318:9: error: unknown type name \u2018rd_kafka_consumer_group_metadata_t\u2019; did you mean \u2018rd_kafka_metadata_t\u2019?\r\n             rd_kafka_consumer_group_metadata_t *cgmd;\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n             rd_kafka_metadata_t\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1319:9: error: unknown type name \u2018rd_kafka_error_t\u2019; did you mean \u2018rd_kafka_event_t\u2019?\r\n             rd_kafka_error_t *error;\r\n             ^~~~~~~~~~~~~~~~\r\n             rd_kafka_event_t\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1326:17: warning: implicit declaration of function \u2018rd_kafka_consumer_group_metadata_read\u2019; did you mean \u2018rd_kafka_consume_start_queue\u2019? [-Wimplicit-function-declaration]\r\n             error = rd_kafka_consumer_group_metadata_read(&cgmd,\r\n                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                     rd_kafka_consume_start_queue\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1326:15: warning: assignment to \u2018int *\u2019 from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n             error = rd_kafka_consumer_group_metadata_read(&cgmd,\r\n                   ^\r\n    In file included from /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:17:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: warning: initialization of \u2018PyObject *\u2019 {aka \u2018struct _object *\u2019} from \u2018int\u2019 makes pointer from integer without a cast [-Wint-conversion]\r\n       PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.h:206:19: note: in definition of macro \u2018cfl_PyErr_from_error_destroy\u2019\r\n       PyObject *_eo = KafkaError_new_from_error_destroy(error); \\\r\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c: In function \u2018error_cb\u2019:\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1356:20: error: \u2018RD_KAFKA_RESP_ERR__FATAL\u2019 undeclared (first use in this function); did you mean \u2018RD_KAFKA_RESP_ERR__FAIL\u2019?\r\n             if (err == RD_KAFKA_RESP_ERR__FATAL) {\r\n                        ^~~~~~~~~~~~~~~~~~~~~~~~\r\n                        RD_KAFKA_RESP_ERR__FAIL\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1356:20: note: each undeclared identifier is reported only once for each function it appears in\r\n    /tmp/pip-install-gix8w43o/confluent-kafka/confluent_kafka/src/confluent_kafka.c:1358:23: warning: implicit declaration of function \u2018rd_kafka_fatal_error\u2019; did you mean \u2018rd_kafka_last_error\u2019? [-Wimplicit-function-declaration]\r\n                     err = rd_kafka_fatal_error(rk, errstr, sizeof(errstr));\r\n                           ^~~~~~~~~~~~~~~~~~~~\r\n                           rd_kafka_last_error\r\n    error: command 'arm-linux-gnueabihf-gcc' failed with exit status 1\r\n    \r\n    ----------------------------------------\r\nCommand \"/usr/bin/python3 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-gix8w43o/confluent-kafka/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-record-vfh1yxgc/install-record.txt --single-version-externally-managed --compile --user --prefix=\" failed with error code 1 in /tmp/pip-install-gix8w43o/confluent-kafka/\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/871", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/871/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/871/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/871/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/871", "id": 620051430, "node_id": "MDU6SXNzdWU2MjAwNTE0MzA=", "number": 871, "title": "TypeError: an integer is required(got type cimpl.KafkaError)", "user": {"login": "demougin2u", "id": 22254873, "node_id": "MDQ6VXNlcjIyMjU0ODcz", "avatar_url": "https://avatars1.githubusercontent.com/u/22254873?v=4", "gravatar_id": "", "url": "https://api.github.com/users/demougin2u", "html_url": "https://github.com/demougin2u", "followers_url": "https://api.github.com/users/demougin2u/followers", "following_url": "https://api.github.com/users/demougin2u/following{/other_user}", "gists_url": "https://api.github.com/users/demougin2u/gists{/gist_id}", "starred_url": "https://api.github.com/users/demougin2u/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/demougin2u/subscriptions", "organizations_url": "https://api.github.com/users/demougin2u/orgs", "repos_url": "https://api.github.com/users/demougin2u/repos", "events_url": "https://api.github.com/users/demougin2u/events{/privacy}", "received_events_url": "https://api.github.com/users/demougin2u/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-18T09:24:31Z", "updated_at": "2020-05-20T11:48:10Z", "closed_at": "2020-05-20T11:48:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nConsumer poll method raise a TypeError instead of KafkaError when broker is down\r\n\r\n\r\nHow to reproduce\r\n================\r\n- Just use this example script : https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/avro_consumer.py\r\n- Stop the broker while script is running\r\n- See the TypeError (may not appear every time, just start&stop again)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"avro_consumer.py\", line 126, in <module>\r\n    main(parser.parse_args())\r\n  File \"avro_consumer.py\", line 96, in main\r\n    msg = consumer.poll(1.0)\r\n  File \"/home/ddemougin/.local/lib/python3.7/site-packages/confluent_kafka/deserializing_consumer.py\", line 131, in poll\r\n    raise ConsumeError(msg.error(), message=msg)\r\n  File \"/home/ddemougin/.local/lib/python3.7/site-packages/confluent_kafka/error.py\", line 44, in __init__\r\n    kafka_error = KafkaError(error_code)\r\nTypeError: an integer is required (got type cimpl.KafkaError)\r\n```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): \r\nconfluent-kafka-python version : ('1.4.1', 17039616) and librdkafka version : ('1.4.0', 17039615)\r\n - [x] Apache Kafka broker version: \r\n5.1.4, run as `confluentinc/cp-kafka:5.1.4` docker container.\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: \r\ndebian 10.4\r\n - [x] Provide client logs (with `'debug': '..'` as necessary) : \r\n```\r\nTraceback (most recent call last):\r\n  File \"avro_consumer.py\", line 126, in <module>\r\n    main(parser.parse_args())\r\n  File \"avro_consumer.py\", line 96, in main\r\n    msg = consumer.poll(1.0)\r\n  File \"/home/ddemougin/.local/lib/python3.7/site-packages/confluent_kafka/deserializing_consumer.py\", line 131, in poll\r\n    raise ConsumeError(msg.error(), message=msg)\r\n  File \"/home/ddemougin/.local/lib/python3.7/site-packages/confluent_kafka/error.py\", line 44, in __init__\r\n    kafka_error = KafkaError(error_code)\r\nTypeError: an integer is required (got type cimpl.KafkaError)\r\n```\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/866", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/866/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/866/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/866/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/866", "id": 618729905, "node_id": "MDU6SXNzdWU2MTg3Mjk5MDU=", "number": 866, "title": "pip installation fails on python 3.8 (windows 10 enterprise)", "user": {"login": "dingobar", "id": 41419288, "node_id": "MDQ6VXNlcjQxNDE5Mjg4", "avatar_url": "https://avatars0.githubusercontent.com/u/41419288?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dingobar", "html_url": "https://github.com/dingobar", "followers_url": "https://api.github.com/users/dingobar/followers", "following_url": "https://api.github.com/users/dingobar/following{/other_user}", "gists_url": "https://api.github.com/users/dingobar/gists{/gist_id}", "starred_url": "https://api.github.com/users/dingobar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dingobar/subscriptions", "organizations_url": "https://api.github.com/users/dingobar/orgs", "repos_url": "https://api.github.com/users/dingobar/repos", "events_url": "https://api.github.com/users/dingobar/events{/privacy}", "received_events_url": "https://api.github.com/users/dingobar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-15T06:24:55Z", "updated_at": "2020-06-15T04:13:52Z", "closed_at": "2020-06-15T04:13:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nThe installation fails when Visual Studio Build Tools 2019 tries to build I guess? I thought the librdkafka was pre-build in the binaries but I guess not? Or is there something else up here?\r\n\r\nI cannot use this package with python 3.8 before this is resolved sadly...\r\n\r\nPip installing works fine when done in a docker container running ubuntu 20.04.\r\n\r\n```bash\r\n    C:\\Users\\username\\AppData\\Local\\Temp\\pip-install-zh3stp7d\\confluent-kafka\\confluent_kafka\\src\\confluent_kafka.h(22): fatal error C1083: Cannot open include file: 'librdkafka/rdkafka.h': No such file or directory\r\n    error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.25.28610\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2\r\n```\r\n\r\nHow to reproduce\r\n================\r\n\r\n```python\r\npip install confluent-kafka # on a windows machine and in a py3.8 virtualenv\r\n```\r\n\r\nAttached log file of the installation.\r\n[console_output.log](https://github.com/confluentinc/confluent-kafka-python/files/4632607/console_output.log)\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] Operating system: Windows 10 Enterprise\r\n - [x] Provide client logs (with `'debug': '..'` as necessary)\r\n - [x] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/862", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/862/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/862/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/862/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/862", "id": 615088685, "node_id": "MDU6SXNzdWU2MTUwODg2ODU=", "number": 862, "title": "Please release version 1.4.2", "user": {"login": "gandalf013", "id": 128740, "node_id": "MDQ6VXNlcjEyODc0MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/128740?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gandalf013", "html_url": "https://github.com/gandalf013", "followers_url": "https://api.github.com/users/gandalf013/followers", "following_url": "https://api.github.com/users/gandalf013/following{/other_user}", "gists_url": "https://api.github.com/users/gandalf013/gists{/gist_id}", "starred_url": "https://api.github.com/users/gandalf013/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gandalf013/subscriptions", "organizations_url": "https://api.github.com/users/gandalf013/orgs", "repos_url": "https://api.github.com/users/gandalf013/repos", "events_url": "https://api.github.com/users/gandalf013/events{/privacy}", "received_events_url": "https://api.github.com/users/gandalf013/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-09T04:25:10Z", "updated_at": "2020-05-25T18:08:37Z", "closed_at": "2020-05-25T18:08:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nOur code is affected by [librdkafka issue 2782](https://github.com/edenhill/librdkafka/issues/2782), which seems to have been fixed in version 1.4.2.  It would be great to see updated Python packages that use the fixed librdkafka.\r\n\r\n(I know it's only been 3 days so this might already be in the works, apologies in that case.)\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\nhttps://github.com/edenhill/librdkafka/issues/2782\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): ('1.0.1', 16777472)\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system: Linux (Ubuntu 18.04)\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [x] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/860", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/860/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/860/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/860/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/860", "id": 614342568, "node_id": "MDU6SXNzdWU2MTQzNDI1Njg=", "number": 860, "title": "Producer error_cb is not called when messages are rejected by the broker for being too large", "user": {"login": "spenczar", "id": 880413, "node_id": "MDQ6VXNlcjg4MDQxMw==", "avatar_url": "https://avatars1.githubusercontent.com/u/880413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/spenczar", "html_url": "https://github.com/spenczar", "followers_url": "https://api.github.com/users/spenczar/followers", "following_url": "https://api.github.com/users/spenczar/following{/other_user}", "gists_url": "https://api.github.com/users/spenczar/gists{/gist_id}", "starred_url": "https://api.github.com/users/spenczar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/spenczar/subscriptions", "organizations_url": "https://api.github.com/users/spenczar/orgs", "repos_url": "https://api.github.com/users/spenczar/repos", "events_url": "https://api.github.com/users/spenczar/events{/privacy}", "received_events_url": "https://api.github.com/users/spenczar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-07T21:04:00Z", "updated_at": "2020-05-08T07:39:22Z", "closed_at": "2020-05-08T07:39:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI have a producer configured with a `message.max.bytes` of `10_000_000` (10MB), and an error_cb which loudly prints errors. I have a broker with `message.max.bytes` left at the default, `1_000_000` (1MB).\r\n\r\nI call `producer.produce`, sending a message which is 2MB. From turning on debug logging with `\"debug\": \"msg\"` in my producer config, I can see that the message is rejected for being too large, but I didn't get an error in the error_cb like I would have expected.\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\nWith a broker up and running at `localhost:9092` with `message.max.bytes` left at the default:\r\n```python\r\nimport confluent_kafka\r\ndef error_callback(err):\r\n    print(\"callback hit!\")\r\n    raise(err)\r\np = confluent_kafka.Producer({\r\n    \"bootstrap.servers\": \"localhost:9092\",\r\n    \"message.max.bytes\": 10_000_000,\r\n    \"error_cb\": error_callback,\r\n    \"debug\": \"msg\",\r\n})\r\np.produce(\"test-topic\", \"a\" * int(2e6))\r\np.flush()\r\n```\r\nWhen I run this program I see this:\r\n```%7|1588885316.913|INIT|rdkafka#producer-1| [thrd:app]: librdkafka v1.4.0 (0x10400ff) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, STATIC_LINKING GCC GXX PKGCONFIG INSTALL GNULD LDS LIBDL PLUGINS ZLIB SSL ZSTD HDRHISTOGRAM SYSLOG SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0x40)\r\n%7|1588885316.915|PROTOERR|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Protocol parse failure for ApiVersion v3 at 3/6 (rd_kafka_handle_ApiVersion:1911) (incorrect broker.version.fallback?)\r\n%7|1588885316.915|PROTOERR|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: ApiArrayCnt -1 out of range\r\nbugdemo.py:11: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\r\n  p.produce(\"test-topic\", \"a\" * int(2e6))\r\n%7|1588885318.915|PRODUCE|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/1: test-topic [0]: Produce MessageSet with 1 message(s) (2000074 bytes, ApiVersion 7, MsgVersion 2, MsgId 0, BaseSeq -1, PID{Invalid}, uncompressed)\r\n%7|1588885318.922|MSGSET|rdkafka#producer-1| [thrd:localhost:9092/bootstrap]: localhost:9092/1: test-topic [0]: MessageSet with 1 message(s) (MsgId 0, BaseSeq -1) encountered error: Broker: Message size too large (actions Permanent,MsgNotPersisted)\r\n%7|1588885318.931|DESTROY|rdkafka#producer-1| [thrd:app]: Terminating instance (destroy flags none (0x0))\r\n%7|1588885318.931|DESTROY|rdkafka#producer-1| [thrd:main]: Destroy internal\r\n```\r\n\r\nNote in particular the `encountered error: Broker: Message size too large (actions Permanent,MsgNotPersisted)` line, but the absence of any `callback hit!` text.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n```\r\n>>> import confluent_kafka\r\n>>> confluent_kafka.version()\r\n('1.4.1', 17039616)\r\n>>> confluent_kafka.libversion()\r\n('1.4.0', 17039615)\r\n```\r\n - [x] Apache Kafka broker version:\r\n5.1.4, run as `confluentinc/cp-kafka:5.1.4` docker container.\r\n\r\n - [x] Client configuration: `{...}`\r\nSee above.\r\n - [x] Operating system:\r\nLinux 5.4.0 (x86_64)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/848", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/848/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/848/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/848/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/848", "id": 609362695, "node_id": "MDU6SXNzdWU2MDkzNjI2OTU=", "number": 848, "title": "Protobuf Schema Registration fails at requests authentication", "user": {"login": "naoko", "id": 615292, "node_id": "MDQ6VXNlcjYxNTI5Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/615292?v=4", "gravatar_id": "", "url": "https://api.github.com/users/naoko", "html_url": "https://github.com/naoko", "followers_url": "https://api.github.com/users/naoko/followers", "following_url": "https://api.github.com/users/naoko/following{/other_user}", "gists_url": "https://api.github.com/users/naoko/gists{/gist_id}", "starred_url": "https://api.github.com/users/naoko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/naoko/subscriptions", "organizations_url": "https://api.github.com/users/naoko/orgs", "repos_url": "https://api.github.com/users/naoko/repos", "events_url": "https://api.github.com/users/naoko/events{/privacy}", "received_events_url": "https://api.github.com/users/naoko/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-29T21:11:13Z", "updated_at": "2020-05-21T11:56:29Z", "closed_at": "2020-05-21T11:56:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nwhen calling register_schema, it generates\r\n```\r\nsite-packages/requests/models.py\", line 549, in prepare_auth\r\n    r = auth(self)\r\nTypeError: 'list' object is not callable\r\n```\r\nbecause\r\nhttps://github.com/confluentinc/confluent-kafka-python/blob/master/confluent_kafka/schema_registry/schema_registry_client.py#L113\r\nis list but expecting tuple\r\n\r\nHow to reproduce\r\n================\r\n\r\nconf = {\"url\": \"<your-url>\", \"basic.auth.user.info\": \"<api_key>:<secret>\"}\r\ncall SchemaRegistryClient(conf).register_schema(subject, schema)\r\n\r\nafter i change it the code from `self.session.auth = userinfo` to `self.session.auth = tuple(userinfo)` things worked.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): python 3.8, confluent-kafka==1.4.1\r\n - [x] Apache Kafka broker version: confluent cloud\r\n - [x] Operating system: Ubuntu\r\n - [x] Provide broker log excerpts\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/846", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/846/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/846/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/846/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/846", "id": 609092210, "node_id": "MDU6SXNzdWU2MDkwOTIyMTA=", "number": 846, "title": "SchemaRegistryClient connection is failing with basic auth", "user": {"login": "jod3", "id": 4204822, "node_id": "MDQ6VXNlcjQyMDQ4MjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/4204822?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jod3", "html_url": "https://github.com/jod3", "followers_url": "https://api.github.com/users/jod3/followers", "following_url": "https://api.github.com/users/jod3/following{/other_user}", "gists_url": "https://api.github.com/users/jod3/gists{/gist_id}", "starred_url": "https://api.github.com/users/jod3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jod3/subscriptions", "organizations_url": "https://api.github.com/users/jod3/orgs", "repos_url": "https://api.github.com/users/jod3/repos", "events_url": "https://api.github.com/users/jod3/events{/privacy}", "received_events_url": "https://api.github.com/users/jod3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-29T14:14:10Z", "updated_at": "2020-05-21T11:56:54Z", "closed_at": "2020-05-21T11:56:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWhen passing basic authentication to the SchemaRegistryClient using \"basic.auth.user.info\" in the conf dict this gets split into an array. The request object requests a tuple. \r\n\r\nDocs: https://docs.confluent.io/current/clients/confluent-kafka-python/index.html#schemaregistryclient\r\n     \r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n- Setup a HttpBasic protected schema registry\r\n\r\n- Try to connect to that schema registry with Basic credentials provided in this format {{username}}:{{password}} in the conf dict passed to the SchemaRegistryClient constructor\r\n\r\n- Results in below stack trace\r\n\r\n\r\n\r\n\r\n```\r\nException has occurred: TypeError\r\n'list' object is not callable\r\n  File \"{some-conda-env}/site-packages/requests/models.py\", line 549, in prepare_auth\r\n    r = auth(self)\r\n\r\n  File \"{some-conda-env}//site-packages/requests/models.py\", line 318, in prepare\r\n    self.prepare_auth(auth, url)\r\n\r\n  File \"{some-conda-env}//site-packages/requests/sessions.py\", line 459, in prepare_request\r\n    hooks=merge_hooks(request.hooks, self.hooks),\r\n\r\n  File \"{some-conda-env}//site-packages/requests/sessions.py\", line 516, in request\r\n    prep = self.prepare_request(req)\r\n\r\n  File \"{some-conda-env}//site-packages/confluent_kafka/schema_registry/schema_registry_client.py\", line 169, in send_request\r\n    headers=headers, data=body, params=query)\r\n\r\n  File \"{some-conda-env}//site-packages/confluent_kafka/schema_registry/schema_registry_client.py\", line 124, in get\r\n    return self.send_request(url, method='GET', query=query)\r\n\r\n  File \"{some-conda-env}//site-packages/confluent_kafka/schema_registry/schema_registry_client.py\", line 439, in get_subjects\r\n    return self._rest_client.get('subjects')\r\n\r\n```\r\n\r\n**Possible Fix**\r\n\r\ncast list returned from split to tuple\r\n\r\n`userinfo = tuple(conf_copy.pop('basic.auth.user.info', '').split(':'))\r\n`\r\n\r\n\r\n**Version Details:**\r\nconfluent-kafka: ('1.4.1', 17039616)\r\nkafka: confluent cloud\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/845", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/845/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/845/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/845/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/845", "id": 608954360, "node_id": "MDU6SXNzdWU2MDg5NTQzNjA=", "number": 845, "title": "Usage of Schema Registry for Protbuf", "user": {"login": "hfjn", "id": 2960924, "node_id": "MDQ6VXNlcjI5NjA5MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2960924?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hfjn", "html_url": "https://github.com/hfjn", "followers_url": "https://api.github.com/users/hfjn/followers", "following_url": "https://api.github.com/users/hfjn/following{/other_user}", "gists_url": "https://api.github.com/users/hfjn/gists{/gist_id}", "starred_url": "https://api.github.com/users/hfjn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hfjn/subscriptions", "organizations_url": "https://api.github.com/users/hfjn/orgs", "repos_url": "https://api.github.com/users/hfjn/repos", "events_url": "https://api.github.com/users/hfjn/events{/privacy}", "received_events_url": "https://api.github.com/users/hfjn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-29T10:43:51Z", "updated_at": "2020-04-29T22:55:04Z", "closed_at": "2020-04-29T22:55:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I'm trying to set up a consumer which deserializes protobuf messages written by a .NET Confluent Producer. The Schema resides in the Schema Registry. \r\nIf I understand the [example code](https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/protobuf_consumer.py) correctly it never uses the schema registry but simply reads in generated protobuf code to deserialize the kafka messages. \r\nIs the schema registry support for protobuf just a WIP or am I missing something? I'm super confused right now.\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/844", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/844/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/844/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/844/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/844", "id": 608931371, "node_id": "MDU6SXNzdWU2MDg5MzEzNzE=", "number": 844, "title": "AvroConsumer on SSL", "user": {"login": "sauravrout", "id": 17978639, "node_id": "MDQ6VXNlcjE3OTc4NjM5", "avatar_url": "https://avatars3.githubusercontent.com/u/17978639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sauravrout", "html_url": "https://github.com/sauravrout", "followers_url": "https://api.github.com/users/sauravrout/followers", "following_url": "https://api.github.com/users/sauravrout/following{/other_user}", "gists_url": "https://api.github.com/users/sauravrout/gists{/gist_id}", "starred_url": "https://api.github.com/users/sauravrout/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sauravrout/subscriptions", "organizations_url": "https://api.github.com/users/sauravrout/orgs", "repos_url": "https://api.github.com/users/sauravrout/repos", "events_url": "https://api.github.com/users/sauravrout/events{/privacy}", "received_events_url": "https://api.github.com/users/sauravrout/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-29T10:05:09Z", "updated_at": "2020-05-03T14:37:08Z", "closed_at": "2020-05-03T14:37:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to read a topic using SSL protocol in python. I have been provided with two jks files(keystore and truststore). I am converting these jks files to .pem using below commands. \r\n\r\n```\r\nkeytool -exportcert -alias abcd-dev-raw -keystore abcd-dev-raw.keystore.jks -rfc -file certificate.pem\r\nkeytool -v -importkeystore -srckeystore rxclaim-dev-raw.keystore.jks -srcalias abcd-dev-raw -destkeystore cert_and_key.p12 -deststoretype PKCS12\r\nopenssl pkcs12 -in cert_and_key.p12 -nocerts -nodes -out key.pem\r\nkeytool -exportcert -alias abcd-dev-raw -keystore abcd-dev-raw.keystore.jks -rfc -file CARoot.pem\r\n```\r\nThing I notice here is, I am not using truststore file anywhere. Isn't it necessary? I see my Scala/java colleagues are using both jks files. Am I missing anything here?\r\n\r\nHere is my python code. It is running without fetching anything from topic giving me no error. I suspect it is not able to connect to kafka using the above generated certificate. Where as using console commands and scala program I am able to read data from kafka topic.\r\n\r\n```\r\nfrom datetime import datetime\r\nfrom confluent_kafka.avro import AvroConsumer\r\nfrom confluent_kafka.avro.serializer import SerializerError\r\nimport ssl\r\n\r\nc = AvroConsumer({\r\n    'bootstrap.servers': 'abcde-xyz-test.optum.com:443',\r\n    'security.protocol': 'ssl',\r\n    'ssl.keystore.password' : 'xxxxxxxx',\r\n    'ssl.key.password': 'xxxxxxxx',\r\n    'ssl.ca.location' : '/Users/user/Desktop/kafka/raw_certs/CARoot.pem',\r\n    'ssl.key.location':'/Users/user/Desktop/kafka/raw_certs/key.pem',\r\n    'ssl.certificate.location':'/Users/user/Desktop/kafka/raw_certs/certificate.pem',\r\n    'schema.registry.url': 'http://abcd-xyz-schema.optum.com',\r\n    'group.id': 'abc',\r\n    \"default.topic.config\": {\"auto.offset.reset\": \"earliest\"}\r\n})\r\n\r\nc.subscribe(['mytoic'])\r\n\r\nwhile True:\r\n    try:\r\n        msg = c.poll(10)\r\n\r\n    except SerializerError as e:\r\n        print(\"Message deserialization failed for {}: {}\".format(msg, e))\r\n        break\r\n\r\n    if msg is None:\r\n        now = datetime.now()\r\n        current_time = now.strftime(\"%D %H:%M:%S\")\r\n        print(current_time, \": No new msg, sleeping for 10 sec\")\r\n        continue\r\n\r\n    if msg.error():\r\n        print(\"AvroConsumer error: {}\".format(msg.error()))\r\n        continue\r\n\r\n    print(msg.value())\r\n\r\nc.close()\r\n```\r\n\r\nAny help on this is highly appreciated.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/842", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/842/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/842/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/842/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/842", "id": 607368525, "node_id": "MDU6SXNzdWU2MDczNjg1MjU=", "number": 842, "title": "producer.init_transactions() always timing out", "user": {"login": "aarondwi", "id": 22184900, "node_id": "MDQ6VXNlcjIyMTg0OTAw", "avatar_url": "https://avatars1.githubusercontent.com/u/22184900?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aarondwi", "html_url": "https://github.com/aarondwi", "followers_url": "https://api.github.com/users/aarondwi/followers", "following_url": "https://api.github.com/users/aarondwi/following{/other_user}", "gists_url": "https://api.github.com/users/aarondwi/gists{/gist_id}", "starred_url": "https://api.github.com/users/aarondwi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aarondwi/subscriptions", "organizations_url": "https://api.github.com/users/aarondwi/orgs", "repos_url": "https://api.github.com/users/aarondwi/repos", "events_url": "https://api.github.com/users/aarondwi/events{/privacy}", "received_events_url": "https://api.github.com/users/aarondwi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-04-27T09:02:54Z", "updated_at": "2020-04-27T11:36:22Z", "closed_at": "2020-04-27T11:36:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI want to try the transaction API, and in the docs, I need to initialiaze the producer with `transactional.id`, to ensure zombies worker are fenced properly. But after setting up the producer for the first time (new worker, absolutely no zombie, before setting the consumer up), call to `init_transactions`  always returns error `Failed to initialize Producer ID: Local: Timed out`. And yes, kafka is already running.\r\n\r\nHow to reproduce\r\n================\r\nI don't know if this is reproducible (no issues found regarding this, meaning most users using this found no issue?), but here goes:\r\n\r\n```python\r\nfrom confluent_kafka import SerializingProducer, Producer\r\nproducer = Producer({\r\n  'bootstrap.servers': 'localhost:9092',\r\n  # 'linger.ms': 3,\r\n  # 'compression.type': 'gzip',\r\n  # 'key.serializer': str.encode('utf-8'),\r\n  # 'value.serializer': str.encode('utf-8'),\r\n  'transactional.id': 'worker',\r\n  'transaction.timeout.ms': 60000,\r\n  'enable.idempotence': True\r\n})\r\nproducer.init_transactions()\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): `('1.4.1', 17039616)` and `('1.4.0', 17039615)`\r\n - [x] Apache Kafka broker version: `confluent-platform 5.4.1 from branch 5.4.1-post`\r\n - [X] Client configuration: `python version 3.7.3`\r\n - [x] Operating system: `Windows 10 Pro 64-bit`\r\n - [x] Provide client logs (with `'debug': '..'` as necessary): `KafkaError{code=_TIMED_OUT,val=-185,str=\"Failed to initialize Producer ID: Local: Timed out\"}`\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/835", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/835/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/835/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/835/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/835", "id": 600429451, "node_id": "MDU6SXNzdWU2MDA0Mjk0NTE=", "number": 835, "title": "Log append timestamp is not respected", "user": {"login": "MichaelLiZhou", "id": 10261388, "node_id": "MDQ6VXNlcjEwMjYxMzg4", "avatar_url": "https://avatars3.githubusercontent.com/u/10261388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MichaelLiZhou", "html_url": "https://github.com/MichaelLiZhou", "followers_url": "https://api.github.com/users/MichaelLiZhou/followers", "following_url": "https://api.github.com/users/MichaelLiZhou/following{/other_user}", "gists_url": "https://api.github.com/users/MichaelLiZhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/MichaelLiZhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MichaelLiZhou/subscriptions", "organizations_url": "https://api.github.com/users/MichaelLiZhou/orgs", "repos_url": "https://api.github.com/users/MichaelLiZhou/repos", "events_url": "https://api.github.com/users/MichaelLiZhou/events{/privacy}", "received_events_url": "https://api.github.com/users/MichaelLiZhou/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-15T16:28:08Z", "updated_at": "2020-04-15T17:37:43Z", "closed_at": "2020-04-15T17:37:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI tried to get the timestamp for TIMESTAMP_LOG_APPEND_TIME - Broker receive time, though the response from my consumer is always the value I set when using the producer method [produce](https://docs.confluent.io/current/clients/confluent-kafka-python/#confluent_kafka.Producer.produce). \r\n\r\nProducer Config\r\n```\r\n            {\r\n                \"client.id\": \"producer-1\",\r\n                \"bootstrap.servers\": \"xxx\",\r\n                'api.version.request': True,\r\n                \"default.topic.config\": {\r\n                    \"request.required.acks\": -1\r\n                }\r\n            }\r\n```\r\n\r\nHow to reproduce\r\n================\r\nProduce a messages with `timestamp=2` or `timestamp=TIMESTAMP_LOG_APPEND_TIME`\r\n```\r\nproducer.produce(topic='test', value=str(msg_value).encode('utf-8'), timestamp=2)\r\n```\r\nWhen consuming the message, the timestamp is always a tuple where the second value is the number you pass into timestamp\r\n```\r\n(1,2)\r\n```\r\n\r\nIs this the correct way to get the log append timestamp?\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`1.4.0` and `1.4.0`):\r\n - [ ] Apache Kafka broker version:\r\n - [x] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/831", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/831/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/831/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/831/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/831", "id": 598852135, "node_id": "MDU6SXNzdWU1OTg4NTIxMzU=", "number": 831, "title": "Memory leak when assign/unassign consumer without consume", "user": {"login": "vane4karock", "id": 30746426, "node_id": "MDQ6VXNlcjMwNzQ2NDI2", "avatar_url": "https://avatars2.githubusercontent.com/u/30746426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vane4karock", "html_url": "https://github.com/vane4karock", "followers_url": "https://api.github.com/users/vane4karock/followers", "following_url": "https://api.github.com/users/vane4karock/following{/other_user}", "gists_url": "https://api.github.com/users/vane4karock/gists{/gist_id}", "starred_url": "https://api.github.com/users/vane4karock/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vane4karock/subscriptions", "organizations_url": "https://api.github.com/users/vane4karock/orgs", "repos_url": "https://api.github.com/users/vane4karock/repos", "events_url": "https://api.github.com/users/vane4karock/events{/privacy}", "received_events_url": "https://api.github.com/users/vane4karock/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-13T12:05:38Z", "updated_at": "2020-04-14T07:36:20Z", "closed_at": "2020-04-14T07:36:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nFound that when assign / unassign consumer without consume, my app start use more memory.\r\n\r\nIn the end of  investigation found that it is some metadata that consumer store in heap.\r\n`\r\n{ \"name\": \"controller_2#consumer-1\", \"client_id\": \"controller_2\", \"type\": \"consumer\", \"ts\":12976427600, \"time\":1586772699, \"replyq\":6458, \"msg_cnt\":0, \"msg_size\":0, \"msg_max\":0, \"msg_size_max\":0, \"simple_cnt\":0, \"metadata_cache_cnt\":1, \"brokers\":{ \"localhost:9092/1\": { \"name\":\"localhost:9092/1\", \"nodeid\":1, \"nodename\":\"localhost:9092\", \"source\":\"learned\", \"state\":\"UP\", \"stateage\":3499950259, \"outbuf_cnt\":0, \"outbuf_msg_cnt\":0, \"waitresp_cnt\":1, \"waitresp_msg_cnt\":0, \"tx\":20644, \"txbytes\":1865746, \"txerrs\":0, \"txretries\":0, \"req_timeouts\":0, \"rx\":20643, \"rxbytes\":1851988, \"rxerrs\":0, \"rxcorriderrs\":0, \"rxpartial\":0, \"zbuf_grow\":0, \"buf_grow\":0, \"wakeups\":46970, \"connects\":1, \"disconnects\":0, \"int_latency\": { \"min\":0, \"max\":0, \"avg\":0, \"sum\":0, \"stddev\": 0, \"p50\": 0, \"p75\": 0, \"p90\": 0, \"p95\": 0, \"p99\": 0, \"p99_99\": 0, ... }\r\n`\r\n\r\nThat is bad, because servers sometimes stop sending data more then few hours.\r\nLooks like some BUG.\r\n\r\nHow to reproduce\r\n================\r\n```python\r\nimport os\r\nimport time\r\n\r\nfrom confluent_kafka import Consumer, TopicPartition\r\n\r\ndef assign_to(consumer, data, topics):\r\n    assign_lst = []\r\n    for tpc in topics:\r\n        if data.get(tpc) is None:\r\n            cmd = consumer.list_topics(topic=tpc)\r\n            part_lst = list(cmd.topics[tpc].partitions.keys())\r\n            data[tpc] = [TopicPartition(tpc, part, 0) for part in part_lst]\r\n\r\n        try:\r\n            assign_lst += data[tpc]\r\n        except KeyError:\r\n            print('ERROR')\r\n\r\n    consumer.assign(assign_lst)\r\n\r\nconfig = {\r\n    \"bootstrap.servers\": \"localhost:9092\",\r\n    \"group.id\": \"fc_kafka_parser_parsed_consumers\",\r\n    \"auto.offset.reset\": \"earliest\",\r\n    \"enable.auto.commit\": False,\r\n    \"client.id\": \"controller_2\",\r\n    \"socket.keepalive.enable\": True,\r\n    \"statistics.interval.ms\": 500,\r\n    \"session.timeout.ms\": 90000,\r\n    \"heartbeat.interval.ms\": 1000,\r\n    \"max.poll.interval.ms\": 86400000\r\n}\r\n\r\nprint(os.getpid())\r\nc = Consumer(config)\r\n\r\nwhile True:\r\n    print('ASSIGN')\r\n    assign_to(c, {}, ['gameStat'])\r\n    print('SLEEP 5 SEC...')\r\n    time.sleep(5)\r\n    print('UNASSIGN')\r\n    c.unassign()\r\n```\r\n\r\nAfter some time (2 hours in my case) in console:\r\n```bash\r\npmap -x <pid>\r\n```\r\nFind many big anon blocks using 65500+ memory, like this:\r\n```bash\r\n00007fe32c000000   83012   83012   83012 rw---   [ anon ]\r\n00007fe32c000000       0       0       0 rw---   [ anon ]\r\n00007fe331111000   48060       0       0 -----   [ anon ]\r\n00007fe331111000       0       0       0 -----   [ anon ]\r\n00007fe334000000     132      24      24 rw---   [ anon ]\r\n00007fe334000000       0       0       0 rw---   [ anon ]\r\n00007fe334021000   65404       0       0 -----   [ anon ]\r\n00007fe334021000       0       0       0 -----   [ anon ]\r\n00007fe338000000     132      40      40 rw---   [ anon ]\r\n00007fe338000000       0       0       0 rw---   [ anon ]\r\n00007fe338021000   65404       0       0 -----   [ anon ]\r\n00007fe338021000       0       0       0 -----   [ anon ]\r\n00007fe33f5e4000       4       0       0 -----   [ anon ]\r\n00007fe33f5e4000       0       0       0 -----   [ anon ]\r\n```\r\nThen we using `gdb` to dump the content of these blocks:\r\n```bash\r\nsudo gdb -pid <pid>\r\ndump memory mem.bin 0x00007fe32c000000 0x00007fe32c000000+83012\r\n```\r\n```bash\r\ncat mem.bin | strings\r\n```\r\nAnd this is consumer metadata.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n```python\r\nimport confluent_kafka\r\nconfluent_kafka.version()\r\n('1.4.0', 17039360)\r\nconfluent_kafka.libversion()\r\n('1.4.0', 17039615)\r\n```\r\n - [x] Apache Kafka broker version:\r\n```bash\r\nroot@remotehost:~# /usr/local/kafka/bin/kafka-topics.sh --version\r\n2.3.0 (Commit:fc1aaa116b661c8a)\r\n```\r\n - [x] Client configuration: `See above`\r\n - [x] Operating system: `Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-55-generic x86_64)`\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/830", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/830/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/830/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/830/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/830", "id": 598220461, "node_id": "MDU6SXNzdWU1OTgyMjA0NjE=", "number": 830, "title": "Cannot pipenv install confluent-kafka 1.4.0 from source", "user": {"login": "vvaradan-isp", "id": 30667541, "node_id": "MDQ6VXNlcjMwNjY3NTQx", "avatar_url": "https://avatars1.githubusercontent.com/u/30667541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vvaradan-isp", "html_url": "https://github.com/vvaradan-isp", "followers_url": "https://api.github.com/users/vvaradan-isp/followers", "following_url": "https://api.github.com/users/vvaradan-isp/following{/other_user}", "gists_url": "https://api.github.com/users/vvaradan-isp/gists{/gist_id}", "starred_url": "https://api.github.com/users/vvaradan-isp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vvaradan-isp/subscriptions", "organizations_url": "https://api.github.com/users/vvaradan-isp/orgs", "repos_url": "https://api.github.com/users/vvaradan-isp/repos", "events_url": "https://api.github.com/users/vvaradan-isp/events{/privacy}", "received_events_url": "https://api.github.com/users/vvaradan-isp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2020-04-11T08:35:22Z", "updated_at": "2020-05-21T12:43:33Z", "closed_at": "2020-05-21T12:43:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n`pipenv install confluent-kafka` works fine, but `PIP_NO_BINARY=confluent-kafka pipenv install confluent-kafka==1.4.0` fails indicating that version 1.4.0 is not available. Max version available is shown to be 1.3.0. Also, `PIP_NO_BINARY=confluent-kafka pipenv install confluent-kafka==1.3.0` works fine.\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n```\r\n$ PIP_NO_BINARY=confluent-kafka pipenv install confluent-kafka==1.4.0\r\nInstalling confluent-kafka==1.4.0\u2026\r\nAdding confluent-kafka to Pipfile's [packages]\u2026\r\n\u2714 Installation Succeeded\r\nPipfile.lock (feeb6a) out of date, updating to (d72df4)\u2026\r\nLocking [dev-packages] dependencies\u2026\r\n\u2714 Success!\r\nLocking [packages] dependencies\u2026\r\n\u2714 Success!\r\nUpdated Pipfile.lock (feeb6a)!\r\nInstalling dependencies from Pipfile.lock (feeb6a)\u2026\r\nAn error occurred while installing confluent-kafka==1.4.0 --hash=sha256:11b9d863b5f8e4da24e0497dc527d0bc170f78e5e15b85f4e170946dd6e86247 --hash=sha256:138eb1eb80a26eabbc00acf9d23d049132d2dbe262601ecd5d17c9078ce399e7 --hash=sha256:240be96c88f09ea1b0767875db1b0b4a1f8dd7b68bb7e89fcf6dbab53fd7a620 --hash=sha256:39a354e727d198fb0f644530d33867199d53e7e25384cf6518147897c46404ff --hash=sha256:39b6d05ec01ce98be583b890b2a80c23e3d752773f108f60b39fa6d8d54cfbca --hash=sha256:3f067b85ebc11f2f72367bd129e746ad32737f24cdd25cedf5780a643d36295b --hash=sha256:403cdca6bda6af5d32f66c4b8920b098c8912b3b703d4500fc15c375368aae22 --hash=sha256:4bb577bb8ccb921857fda601b4282936d696f29ea825fafea237bb76b2f5707e --hash=sha256:4ec0fc40eea26d21586e09eea9046efc3f24df78911b14decf36f613568bfa7e --hash=sha256:54b101bfe4b073712cd2d4523ecda56138eb3737f4825dcf3575bb9183334774 --hash=sha256:57f7a2570587fef85ef0a97c11f05ef2e9e9929fd752351974b75e2c8c65e585 --hash=sha256:5bc72cafd9a857618b3ef1f851bc3c9e023182cd74ec3a711d0f335f31ae1803 --hash=sha256:6e99142b37ea58274f42ddd40ecea46520d14cd40689e03ed2dfedb2863c127e --hash=sha256:74c68a1c6c63eaf4056e55b5162e9f56f26fbd872571660ab629195c63b92198 --hash=sha256:79ec9c47ff2f76d5fcac9345b61d2e2cb6a659c96ac74cf0f78e424b06755822 --hash=sha256:80ee541bf779a8bf38dee7b2014350459df527ed98cf68d1777f30e18d307d1e --hash=sha256:9b2493f898e044dcd222ef64ea8594352c2e229f6d150e6640d9aea15d206aef --hash=sha256:9fcf8f2952c265edfa11556add670d7f24f58a835a8d245d4b5bf9747adc1913 --hash=sha256:a35e997cbaba4d28816b1d83255a65796faffbc39b00f414f8b9eb1c9d967a0f --hash=sha256:c58cf16770fb7fa2defab727c57a4cd4f03e083f6e7d7a4494c7bd42ed15ab37 --hash=sha256:cc28538c6e8af7a75adaa68e1f5905ba60a7b24f2bcc239e0d4dc07dcfd72889 --hash=sha256:d454a17697dafe422934fc917e5e905ebd4fb5c682d5bcffc261167748da6ffe --hash=sha256:f5b3e323714ecd824690c85cf8b06fcca47f7f2cdfe636184d8df23ae0e1f31a --hash=sha256:f8a505d22186f416cc8e4d6c7bbca862e961352d0fef2de870bfc841e071c072 --hash=sha256:fe3617035bad6360bf50b52d140c904a772512fae95f12f769b4a8923a2f2e68! Will try again.\r\n  \ud83d\udc0d   \u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589 4/4 \u2014 00:00:01\r\nInstalling initially failed dependencies\u2026\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.8/site-packages/pipenv/core.py\", line 1983, in do_install\r\n[pipenv.exceptions.InstallError]:       do_init(\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.8/site-packages/pipenv/core.py\", line 1246, in do_init\r\n[pipenv.exceptions.InstallError]:       do_install_dependencies(\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.8/site-packages/pipenv/core.py\", line 858, in do_install_dependencies\r\n[pipenv.exceptions.InstallError]:       batch_install(\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.8/site-packages/pipenv/core.py\", line 763, in batch_install\r\n[pipenv.exceptions.InstallError]:       _cleanup_procs(procs, not blocking, failed_deps_queue, retry=retry)\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.8/site-packages/pipenv/core.py\", line 681, in _cleanup_procs\r\n[pipenv.exceptions.InstallError]:       raise exceptions.InstallError(c.dep.name, extra=err_lines)\r\n[pipenv.exceptions.InstallError]: ['Looking in indexes: https://pypi.python.org/simple']\r\n[pipenv.exceptions.InstallError]: ['ERROR: Could not find a version that satisfies the requirement confluent-kafka==1.4.0 (from -r /var/folders/h5/ds18pks91694txcsjt16k9lm0000gn/T/pipenv-vjkrm31h-requirements/pipenv-ixir4ei4-requirement.txt (line 1)) (from versions: 0.9.1.1, 0.9.1.2, 0.9.2, 0.9.4, 0.11.0, 0.11.4, 0.11.5, 0.11.6, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.3.0)', 'ERROR: No matching distribution found for confluent-kafka==1.4.0 (from -r /var/folders/h5/ds18pks91694txcsjt16k9lm0000gn/T/pipenv-vjkrm31h-requirements/pipenv-ixir4ei4-requirement.txt (line 1))']\r\n```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/826", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/826/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/826/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/826/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/826", "id": 596638004, "node_id": "MDU6SXNzdWU1OTY2MzgwMDQ=", "number": 826, "title": "Cannot produce messages", "user": {"login": "srtamrakar", "id": 32105794, "node_id": "MDQ6VXNlcjMyMTA1Nzk0", "avatar_url": "https://avatars1.githubusercontent.com/u/32105794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srtamrakar", "html_url": "https://github.com/srtamrakar", "followers_url": "https://api.github.com/users/srtamrakar/followers", "following_url": "https://api.github.com/users/srtamrakar/following{/other_user}", "gists_url": "https://api.github.com/users/srtamrakar/gists{/gist_id}", "starred_url": "https://api.github.com/users/srtamrakar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srtamrakar/subscriptions", "organizations_url": "https://api.github.com/users/srtamrakar/orgs", "repos_url": "https://api.github.com/users/srtamrakar/repos", "events_url": "https://api.github.com/users/srtamrakar/events{/privacy}", "received_events_url": "https://api.github.com/users/srtamrakar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-08T14:49:58Z", "updated_at": "2020-04-09T12:09:24Z", "closed_at": "2020-04-09T12:09:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nMessages are not produced. They are neither visible in the Confluent Cloud dashboard, nor consumable.\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n```\r\nimport json\r\nfrom datetime import datetime\r\nfrom confluent_kafka import Producer, KafkaError\r\n \r\n\r\nCONFIG = {\r\n    \"bootstrap.servers\": \"########.eu-central-1.aws.confluent.cloud:####\",\r\n    \"security.protocol\": \"SSL\",\r\n    \"sasl.mechanisms\": \"PLAIN\",\r\n    \"sasl.username\": \"########\",\r\n    \"sasl.password\": \"########\",\r\n    \"socket.timeout.ms\": 10000,\r\n}\r\nTOPIC = \"#################\"\r\n\r\n \r\ndef get_data(website: str, profile_id: int):\r\n    return {\r\n        \"website\": website,\r\n        \"source\": \"source_random\",\r\n        \"profile_id\": profile_id,\r\n        \"created_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\r\n    }\r\n\r\n \r\ndef delivery_report(err, msg):\r\n    \"\"\" Called once for each message produced to indicate delivery result.\r\n        Triggered by poll() or flush(). \"\"\"\r\n    if err is not None:\r\n        print(\"Message delivery failed: {}\".format(err))\r\n    else:\r\n        print(\"Message delivered to {} [{}]\".format(msg.topic(), msg.partition()))\r\n\r\n \r\nproducer = Producer(CONFIG)\r\nproducer.poll(0)\r\nrecord_key = \"key\"\r\nrecord_value = json.dumps(get_data(record_key, 1)).encode('utf-8')\r\nprint(f\"Producing record: {record_key} => {record_value}\")\r\nproducer.produce(topic=TOPIC, key=record_key, value=record_value, callback=delivery_report)\r\nproducer.poll(1)\r\n```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): ('1.3.0', 16973824) and ('1.3.0', 16974079)\r\n - [x] Apache Kafka broker version: 2.4.1\r\n - [x] Client configuration: `{...}`\r\n - [x] Operating system: macOS 10.15.3 \r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [x] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/818", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/818/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/818/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/818/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/818", "id": 592806849, "node_id": "MDU6SXNzdWU1OTI4MDY4NDk=", "number": 818, "title": "Issue installing librdkafka-devel", "user": {"login": "khoanguyen123", "id": 6828860, "node_id": "MDQ6VXNlcjY4Mjg4NjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6828860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khoanguyen123", "html_url": "https://github.com/khoanguyen123", "followers_url": "https://api.github.com/users/khoanguyen123/followers", "following_url": "https://api.github.com/users/khoanguyen123/following{/other_user}", "gists_url": "https://api.github.com/users/khoanguyen123/gists{/gist_id}", "starred_url": "https://api.github.com/users/khoanguyen123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khoanguyen123/subscriptions", "organizations_url": "https://api.github.com/users/khoanguyen123/orgs", "repos_url": "https://api.github.com/users/khoanguyen123/repos", "events_url": "https://api.github.com/users/khoanguyen123/events{/privacy}", "received_events_url": "https://api.github.com/users/khoanguyen123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-02T17:31:16Z", "updated_at": "2020-04-02T17:36:42Z", "closed_at": "2020-04-02T17:36:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nCan't install rpm librdkafka-devel on RHEL 6 due to unresolved dependencies. Error messages:\r\n\r\n> yum install librdkafka-devel\r\nLoaded plugins: enabled_repos_upload, package_upload, product-id, search-disabled-repos, security, subscription-manager\r\n\r\n>Setting up Install Process\r\nResolving Dependencies\r\n--> Running transaction check\r\n---> Package librdkafka-devel.x86_64 0:1.3.0_confluent5.4.1-1.el7 will be installed\r\n--> Processing Dependency: librdkafka1 = 1.3.0_confluent5.4.1 for package: librdkafka-devel-1.3.0_confluent5.4.1-1.el7.x86_64\r\n--> Processing Dependency: librdkafka++.so.1()(64bit) for package: librdkafka-devel-1.3.0_confluent5.4.1-1.el7.x86_64\r\n--> Processing Dependency: librdkafka.so.1()(64bit) for package: librdkafka-devel-1.3.0_confluent5.4.1-1.el7.x86_64\r\n--> Running transaction check\r\n---> Package librdkafka1.x86_64 0:1.3.0_confluent5.4.1-1.el7 will be installed\r\n--> Processing Dependency: libstdc++.so.6(GLIBCXX_3.4.15)(64bit) for package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64\r\n--> Processing Dependency: libc.so.6(GLIBC_2.16)(64bit) for package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64\r\n--> Processing Dependency: openssl-libs for package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64\r\n--> Processing Dependency: libsasl2.so.3()(64bit) for package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64\r\n--> Finished Dependency Resolution\r\nError: Package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64 (Confluent.dist)\r\n           Requires: libstdc++.so.6(GLIBCXX_3.4.15)(64bit)\r\nError: Package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64 (Confluent.dist)\r\n           Requires: openssl-libs\r\nError: Package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64 (Confluent.dist)\r\n           Requires: libc.so.6(GLIBC_2.16)(64bit)\r\nError: Package: librdkafka1-1.3.0_confluent5.4.1-1.el7.x86_64 (Confluent.dist)\r\n           Requires: libsasl2.so.3()(64bit)\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n`sudo rpm --import https://packages.confluent.io/rpm/5.4/archive.key`\r\n\r\n`sudo yum install librdkafka-devel`\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: RHEL 6\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/806", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/806/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/806/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/806/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/806", "id": 585743731, "node_id": "MDU6SXNzdWU1ODU3NDM3MzE=", "number": 806, "title": "Exception raising", "user": {"login": "royreznik", "id": 23289491, "node_id": "MDQ6VXNlcjIzMjg5NDkx", "avatar_url": "https://avatars3.githubusercontent.com/u/23289491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/royreznik", "html_url": "https://github.com/royreznik", "followers_url": "https://api.github.com/users/royreznik/followers", "following_url": "https://api.github.com/users/royreznik/following{/other_user}", "gists_url": "https://api.github.com/users/royreznik/gists{/gist_id}", "starred_url": "https://api.github.com/users/royreznik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/royreznik/subscriptions", "organizations_url": "https://api.github.com/users/royreznik/orgs", "repos_url": "https://api.github.com/users/royreznik/repos", "events_url": "https://api.github.com/users/royreznik/events{/privacy}", "received_events_url": "https://api.github.com/users/royreznik/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948253, "node_id": "MDU6TGFiZWwzNTc5NDgyNTM=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-22T15:27:32Z", "updated_at": "2020-06-15T16:14:42Z", "closed_at": "2020-06-15T16:14:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "After working with your client for a long time, i found out that a have written those line over and over:\r\n```python\r\nif msg.error():\r\n                raise Exception()\r\n```\r\nIn the consumer, most of the exception you will get will make `msg.error` be not `None`.\r\nI think it will be much better api, to throw an exception after getting one, and not make the exception apear in the `msg.value()`.\r\n\r\nis there a plan to change this into exception base? or would you like to get a PR about this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/803", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/803/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/803/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/803/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/803", "id": 582451267, "node_id": "MDU6SXNzdWU1ODI0NTEyNjc=", "number": 803, "title": "BUG: mismatch `sasl.mechanisms` in cached_schema_registry_client.py", "user": {"login": "andharris", "id": 1380450, "node_id": "MDQ6VXNlcjEzODA0NTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1380450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andharris", "html_url": "https://github.com/andharris", "followers_url": "https://api.github.com/users/andharris/followers", "following_url": "https://api.github.com/users/andharris/following{/other_user}", "gists_url": "https://api.github.com/users/andharris/gists{/gist_id}", "starred_url": "https://api.github.com/users/andharris/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andharris/subscriptions", "organizations_url": "https://api.github.com/users/andharris/orgs", "repos_url": "https://api.github.com/users/andharris/repos", "events_url": "https://api.github.com/users/andharris/events{/privacy}", "received_events_url": "https://api.github.com/users/andharris/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-16T16:47:51Z", "updated_at": "2020-03-26T11:37:33Z", "closed_at": "2020-03-26T11:37:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWhen `basic.auth.credentials.source` is set to `SASL_INHERIT`, then `sasl.mechanisms` (plural) is passed to the [schema registry conf](https://github.com/confluentinc/confluent-kafka-python/blob/master/confluent_kafka/avro/__init__.py#L35-L39), however when configuring the auth in `CachedSchemaRegistryClient`, then `sasl.mechanism` (singular) is [popped](https://github.com/confluentinc/confluent-kafka-python/blob/master/confluent_kafka/avro/cached_schema_registry_client.py#L138). This causes the check at the end of `CachedSchemaRegistryClient.__init__` to [raise a `ValueError` ](https://github.com/confluentinc/confluent-kafka-python/blob/master/confluent_kafka/avro/cached_schema_registry_client.py#L116-L117).\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n```\r\nfrom confluent_kafka.avro import AvroProducer\r\n\r\nconfig = {\r\n    'schema.registry.url': 'http://test.com:9091',\r\n    'schema.registry.basic.auth.credentials.source': 'SASL_INHERIT',\r\n}\r\n\r\nAvroProducer(config)\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-88b3fcdf0322> in <module>\r\n      6 }\r\n      7 \r\n----> 8 AvroProducer(config)\r\n\r\n/usr/local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py in __init__(self, config, default_key_schema, default_value_schema, schema_registry)\r\n     42 \r\n     43         if schema_registry is None:\r\n---> 44             schema_registry = CachedSchemaRegistryClient(sr_conf)\r\n     45         elif sr_conf.get(\"url\", None) is not None:\r\n     46             raise ValueError(\"Cannot pass schema_registry along with schema.registry.url config\")\r\n\r\n/usr/local/lib/python3.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py in __init__(self, url, max_schemas_per_subject, ca_location, cert_location, key_location)\r\n    114 \r\n    115         if len(conf) > 0:\r\n--> 116             raise ValueError(\"Unrecognized configuration properties: {}\".format(conf.keys()))\r\n    117 \r\n    118     def __del__(self):\r\n\r\nValueError: Unrecognized configuration properties: dict_keys(['sasl.mechanisms'])\r\n\r\n```\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): ('1.2.0', 16908288)\r\n - [x] Apache Kafka broker version: na\r\n - [x] Client configuration: `{...}` see reproducible example\r\n - [x] Operating system: macOS\r\n - [x] Provide client logs (with `'debug': '..'` as necessary) na\r\n - [x] Provide broker log excerpts na\r\n - [x] Critical issue: bug\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/801", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/801/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/801/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/801/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/801", "id": 581756374, "node_id": "MDU6SXNzdWU1ODE3NTYzNzQ=", "number": 801, "title": "avro.io is missing after replacement avro with avro-python3", "user": {"login": "mykhas", "id": 1077154, "node_id": "MDQ6VXNlcjEwNzcxNTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1077154?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mykhas", "html_url": "https://github.com/mykhas", "followers_url": "https://api.github.com/users/mykhas/followers", "following_url": "https://api.github.com/users/mykhas/following{/other_user}", "gists_url": "https://api.github.com/users/mykhas/gists{/gist_id}", "starred_url": "https://api.github.com/users/mykhas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mykhas/subscriptions", "organizations_url": "https://api.github.com/users/mykhas/orgs", "repos_url": "https://api.github.com/users/mykhas/repos", "events_url": "https://api.github.com/users/mykhas/events{/privacy}", "received_events_url": "https://api.github.com/users/mykhas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-15T17:35:59Z", "updated_at": "2020-03-26T12:05:03Z", "closed_at": "2020-03-20T18:01:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI'm working through the Kafka course on Udacity, and as a part of it I'm trying to load avro scheme loading locally. Unfortunately, I'm getting an error ```AttributeError: module 'avro.schema' has no attribute 'Parse'``` while executing it having pip3's `avro` installed locally. This issues was reported several times already (#600, #790), so I've just replaced it with `avro-python3`. Now, it fails even sooner giving the next error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"exercise3.4.error.py\", line 1, in <module>\r\n    from confluent_kafka import avro, Consumer, Producer\r\n  File \"/usr/local/lib/python3.7/site-packages/confluent_kafka/avro/__init__.py\", line 13, in <module>\r\n    from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\r\n  File \"/usr/local/lib/python3.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 29, in <module>\r\n    import avro.io\r\nModuleNotFoundError: No module named 'avro.io'\r\n```\r\n\r\nHow to reproduce\r\n================\r\nCode to reproduce:\r\n```from confluent_kafka import avro, Consumer, Producer\r\nfrom confluent_kafka.avro import AvroConsumer, AvroProducer, CachedSchemaRegistryClient\r\n\r\nschema = avro.loads(\"\"\"{\r\n    \"type\": \"record\",\r\n    \"name\": \"click_event\",\r\n    \"namespace\": \"com.udacity.lesson3.exercise4\",\r\n    \"fields\": [\r\n        {\"name\": \"email\", \"type\": \"string\"},\r\n        {\"name\": \"timestamp\", \"type\": \"string\"},\r\n        {\"name\": \"uri\", \"type\": \"string\"},\r\n        {\"name\": \"number\", \"type\": \"int\"}\r\n    ]\r\n}\"\"\")\r\n```\r\n\r\n```\r\n\u279c  UDSND pip3 list | grep avro\r\navro-python3              1.9.2.1\r\nfastavro                  0.22.13\r\n\u279c  UDSND pip3 list | grep kafka\r\nconfluent-kafka           1.3.0\r\n\u279c  UDSND python3 --version\r\nPython 3.7.6\r\n\u279c  UDSND pip3 --version\r\npip 19.3.1 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)\r\n```\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [X] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): ('1.3.0', 16973824), ('1.3.0', 16974079)\r\n - [X] Apache Kafka broker version: 2.4.0\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system: MacOS\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/793", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/793/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/793/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/793/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/793", "id": 574134901, "node_id": "MDU6SXNzdWU1NzQxMzQ5MDE=", "number": 793, "title": "Consumer takes ~5 seconds to poll the first message", "user": {"login": "JosepSampe", "id": 10448186, "node_id": "MDQ6VXNlcjEwNDQ4MTg2", "avatar_url": "https://avatars1.githubusercontent.com/u/10448186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JosepSampe", "html_url": "https://github.com/JosepSampe", "followers_url": "https://api.github.com/users/JosepSampe/followers", "following_url": "https://api.github.com/users/JosepSampe/following{/other_user}", "gists_url": "https://api.github.com/users/JosepSampe/gists{/gist_id}", "starred_url": "https://api.github.com/users/JosepSampe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JosepSampe/subscriptions", "organizations_url": "https://api.github.com/users/JosepSampe/orgs", "repos_url": "https://api.github.com/users/JosepSampe/repos", "events_url": "https://api.github.com/users/JosepSampe/events{/privacy}", "received_events_url": "https://api.github.com/users/JosepSampe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-02T17:38:05Z", "updated_at": "2020-03-26T15:46:28Z", "closed_at": "2020-03-26T15:46:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Each time I create a Consumer, the first poll takes ~5 seconds. Subsequent polls run much faster (as expected). However, I have an app that stops for a while and then starts again, having to create the Consumer each time, and then having these 5 seconds of overhead each time.\r\n\r\n- I have a kafka installation (kafka_2.12-2.2.0) on a single VM.\r\n- I have a topic with 5K messages before start the consumer.\r\n- The VM and the consumer are located in the same local network (no interference by other \r\nservices)\r\n- I use confluent-kafka 1.3.0\r\n```\r\nimport json\r\nimport time\r\nfrom confluent_kafka import Consumer\r\n\r\nTOPIC='mytopic'\r\n\r\ndef consume_events():\r\n\r\n    config = {'bootstrap.servers': '192.168.2.51:9092',\r\n              'group.id': 'test-group',\r\n              'default.topic.config': {'auto.offset.reset': 'earliest'},\r\n              'enable.auto.commit': False\r\n              }\r\n    consumer = Consumer(config)\r\n    consumer.subscribe([TOPIC])\r\n    start = None\r\n    total = 0\r\n    while True:\r\n        try:\r\n            message = consumer.poll(timeout=10)\r\n            start = time.time() if not start else start\r\n            payload = message.value().decode('utf-8')\r\n            event = json.loads(payload)\r\n            total = total+1\r\n            #print(\"[{}] Received event {}\".format(TOPIC, total))\r\n            end = time.time()\r\n        except Exception as e:\r\n            break\r\n\r\n    print('Total time: {}'.format(end-start))\r\n\r\n\r\nif __name__ == '__main__':\r\n    consume_events()\r\n```\r\nIs this the expected behavior? why is this overhead present each time I start to poll messages?\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/790", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/790/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/790/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/790/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/790", "id": 572203521, "node_id": "MDU6SXNzdWU1NzIyMDM1MjE=", "number": 790, "title": "module 'avro.schema' has no attribute 'Parse'", "user": {"login": "juliechen727", "id": 20132744, "node_id": "MDQ6VXNlcjIwMTMyNzQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/20132744?v=4", "gravatar_id": "", "url": "https://api.github.com/users/juliechen727", "html_url": "https://github.com/juliechen727", "followers_url": "https://api.github.com/users/juliechen727/followers", "following_url": "https://api.github.com/users/juliechen727/following{/other_user}", "gists_url": "https://api.github.com/users/juliechen727/gists{/gist_id}", "starred_url": "https://api.github.com/users/juliechen727/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/juliechen727/subscriptions", "organizations_url": "https://api.github.com/users/juliechen727/orgs", "repos_url": "https://api.github.com/users/juliechen727/repos", "events_url": "https://api.github.com/users/juliechen727/events{/privacy}", "received_events_url": "https://api.github.com/users/juliechen727/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-27T16:35:42Z", "updated_at": "2020-04-13T01:55:20Z", "closed_at": "2020-04-13T01:54:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWhen I trying to run the AvroProducer example, I got an AttributeError said \r\n```\r\nException has occurred: AttributeError\r\nmodule 'avro.schema' has no attribute 'Parse'\r\n  File \"C:\\Projects\\TEST\\web-scrape-python\\demo-github.py\", line 33, in <module>\r\n    value_schema = avro.loads(value_schema_str)\r\n```\r\nI'm using python 3.7.5\r\n\r\nHow to reproduce\r\n================\r\n```\r\nfrom confluent_kafka import avro\r\nfrom confluent_kafka.avro import AvroProducer\r\n\r\n\r\nvalue_schema_str = \"\"\"\r\n{\r\n   \"namespace\": \"my.test\",\r\n   \"name\": \"value\",\r\n   \"type\": \"record\",\r\n   \"fields\" : [\r\n     {\r\n       \"name\" : \"name\",\r\n       \"type\" : \"string\"\r\n     }\r\n   ]\r\n}\r\n\"\"\"\r\n\r\nkey_schema_str = \"\"\"\r\n{\r\n   \"namespace\": \"my.test\",\r\n   \"name\": \"key\",\r\n   \"type\": \"record\",\r\n   \"fields\" : [\r\n     {\r\n       \"name\" : \"name\",\r\n       \"type\" : \"string\"\r\n     }\r\n   ]\r\n}\r\n\"\"\"\r\n\r\nvalue_schema = avro.loads(value_schema_str)\r\nkey_schema = avro.loads(key_schema_str)\r\nvalue = {\"name\": \"Value\"}\r\nkey = {\"name\": \"Key\"}\r\n\r\n\r\ndef delivery_report(err, msg):\r\n    \"\"\" Called once for each message produced to indicate delivery result.\r\n        Triggered by poll() or flush(). \"\"\"\r\n    if err is not None:\r\n        print('Message delivery failed: {}'.format(err))\r\n    else:\r\n        print('Message delivered to {} [{}]'.format(\r\n            msg.topic(), msg.partition()))\r\n\r\n\r\navroProducer = AvroProducer({\r\n    'bootstrap.servers': 'localhost:29092',\r\n    'on_delivery': delivery_report,\r\n    'schema.registry.url': 'http://localhost:8081'\r\n}, default_key_schema=key_schema, default_value_schema=value_schema)\r\n\r\navroProducer.produce(topic='my_topic', value=value, key=key)\r\navroProducer.flush()\r\n\r\n```\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): 2.4.0\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: windows 10\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/789", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/789/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/789/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/789/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/789", "id": 572053880, "node_id": "MDU6SXNzdWU1NzIwNTM4ODA=", "number": 789, "title": "request(s) timed out: disconnect in combination with all brokers down", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-27T12:35:26Z", "updated_at": "2020-07-02T04:55:56Z", "closed_at": "2020-07-02T04:55:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nSeveral times a day (varying from a few times up to around 30) we get the following combination of log lines in our logging:\r\n\r\n```\r\nWed Feb 26 13:56:52 2020, 59876: INF KafkaProducerConnection reported an error: other. Err code: -185. Reason: ssl://node3.hostname:9093/3: 1 request(s) timed out: disconnect (after 2699273ms in state UP).\r\n\r\nWed Feb 26 13:56:52 2020, 59876: ERR KafkaProducerConnection reported an error: all brokers down. Err code: -187. Reason: 3/3 brokers are down. (ConfluentKafkaProducerConnection)\r\n```\r\n\r\nThe combination of 2 events happens almost at the same time, and those happens several times a day.\r\n\r\nA bit of clarification of the log lines: those are printed by our error handling function:\r\n\r\n```\r\ndef _error_handling(self, err):\r\n\tloglvl = self._logger.Informational\r\n\terrtype = \"other\"\r\n\tif err.code() == KafkaError._ALL_BROKERS_DOWN:\r\n\t    self._in_error_state = True\r\n\t    loglvl = self._logger.Error\r\n\t    errtype = \"all brokers down\"\r\n\telif err.code() == KafkaError._TRANSPORT:\r\n\t    loglvl = self._logger.Warning\r\n\t    errtype = \"one broker down\"\r\n\tself._logger.log(\"%s reported an error: %s. Err code: %d. Reason: %s.\" % (self._short_identifier, errtype, err.code(), err.str()), loglvl)\r\n```\r\n\r\nThe questions we have:\r\n- We wonder why this is happening.\r\n- We don't understand that one node is timing out and \"all brokers down\" is reported just after the timeout.\r\n- What can we do to fix this?\r\n- Is there something we can do to prevent the timeout? Increasing the timeout time for example. Or trigger a function on the producer to keep the connection alive, or somehting.\r\n\r\nHow to reproduce\r\n================\r\nWe have an application running with around 10 consumers and 2 producers. The error only occurs with the producers. And it occurs with all 3 nodes. The application runs as a python application on a Debian environment. If connects to a Kafka cluster with 3 nodes. We just started using confluent-kafka because pykafka is not maintained anymore. We are now running in an acceptance environment where not very much messages are produced.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): \r\n\r\nconfluent_kafka.version()\r\n('1.3.0', 16973824)\r\n\r\nconfluent_kafka.libversion()\r\n('1.3.0', 16974079)\r\n\r\n - [x] Apache Kafka broker version: 2.2.1\r\n - [x] Client configuration: \r\n\r\n`{'bootstrap.servers': 'node1:9093,node2:9093,node3:9093,\r\n'linger.ms': 5,\r\n'error_cb': self._error_handling,\r\n'security.protocol': 'SSL',\r\n'ssl.ca.location': 'cafile',\r\n'ssl.certificate.location': 'location',\r\n'ssl.key.location': 'keyfile',\r\n'ssl.key.password': 'keyfile_password}`\r\n\r\n - [x] Operating system: Debian Linux\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/783", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/783/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/783/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/783/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/783", "id": 568976760, "node_id": "MDU6SXNzdWU1Njg5NzY3NjA=", "number": 783, "title": "poll timeout is ignored", "user": {"login": "OblackatO", "id": 25124288, "node_id": "MDQ6VXNlcjI1MTI0Mjg4", "avatar_url": "https://avatars2.githubusercontent.com/u/25124288?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OblackatO", "html_url": "https://github.com/OblackatO", "followers_url": "https://api.github.com/users/OblackatO/followers", "following_url": "https://api.github.com/users/OblackatO/following{/other_user}", "gists_url": "https://api.github.com/users/OblackatO/gists{/gist_id}", "starred_url": "https://api.github.com/users/OblackatO/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OblackatO/subscriptions", "organizations_url": "https://api.github.com/users/OblackatO/orgs", "repos_url": "https://api.github.com/users/OblackatO/repos", "events_url": "https://api.github.com/users/OblackatO/events{/privacy}", "received_events_url": "https://api.github.com/users/OblackatO/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-02-21T14:08:31Z", "updated_at": "2020-02-21T15:46:49Z", "closed_at": "2020-02-21T15:46:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWhen creating a simple Producer, the timeout of poll is ignored. I use poll() right after creating a Producer to check if the kafka broker(s) is up and running. \r\nEverything seems correct, because I got two kafka errors that are supposed to happen: _TRANSPORT && _ALL_BROKERS_DOWN. \r\nThe only problem is that the timeout that I specify in poll() is not respected, no matter what value I specify.\r\n\r\nHow to reproduce\r\n================\r\n```\r\nfrom confluent_kafka import Producer\r\n\r\ndef error(err):\r\n  print(\"An error occurred: {}\".format(err))\r\n\r\ndef delivery(err, msg):\r\n  if err is not None:\r\n    print(\"Error occurred while delivering msg\")\r\n  else:\r\n    print(\"{} has been delivered.\".format(msg))\r\nconf={\r\n  'bootstrap.servers': 'non_working_broker',\r\n  'error_cb': error,\r\n  'on_delivery': delivery\r\n}\r\n\r\nproducer = Producer(conf)\r\nproducer.poll(timeout=5.0)\r\n```\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n - [ X] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n   - ('1.3.0', 16973824) and  ('1.3.0', 16974079)\r\n - [ ] Apache Kafka broker version:\r\n - [ X] Client configuration: `{...}`: On the code\r\n - [ X] Operating system: Ubuntu 19.04 (Disco Dingo)\r\n - [ X] Provide client logs (with `'debug': '..'` as necessary) :\r\n%7|1582294700.989|BROKER|rdkafka#producer-1| [thrd:app]: non_working_broker:9092/bootstrap: Added new broker with NodeId -1\r\n%7|1582294700.989|CONNECT|rdkafka#producer-1| [thrd:app]: non_working_broker:9092/bootstrap: Selected for cluster connection: bootstrap servers added (broker has 0 connection attempt(s))\r\n%7|1582294700.989|BRKMAIN|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Enter main broker thread\r\n%7|1582294700.989|BRKMAIN|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Enter main broker thread\r\n%7|1582294700.989|CONNECT|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Received CONNECT op\r\n%7|1582294700.989|STATE|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1582294700.989|CONNECT|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1582294700.989|STATE|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1582294700.989|INIT|rdkafka#producer-1| [thrd:app]: librdkafka v1.3.0 (0x10300ff) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, GCC GXX PKGCONFIG INSTALL GNULD LDS LIBDL PLUGINS ZLIB SSL ZSTD HDRHISTOGRAM SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0x2)\r\nAn error occurred: KafkaError{code=_RESOLVE,val=-193,str=\"non_working_broker:9092/bootstrap: Failed to resolve 'non_working_broker:9092': Temporary failure in name resolution (after 23ms in state CONNECT)\"}\r\nAn error occurred: KafkaError{code=_ALL_BROKERS_DOWN,val=-187,str=\"1/1 brokers are down\"}\r\n%7|1582294701.012|BROKERFAIL|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: failed: err: Local: Host resolution failure: (errno: Bad address)\r\n%7|1582294701.012|STATE|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Broker changed state CONNECT -> DOWN\r\n%7|1582294701.013|DESTROY|rdkafka#producer-1| [thrd:app]: Terminating instance (destroy flags none (0x0))\r\n%7|1582294701.013|DESTROY|rdkafka#producer-1| [thrd:main]: Destroy internal\r\n%7|1582294701.013|DESTROY|rdkafka#producer-1| [thrd:main]: Removing all topics\r\n%7|1582294701.013|DESTROY|rdkafka#producer-1| [thrd:main]: Sending TERMINATE to non_working_broker:9092/bootstrap\r\n%7|1582294701.013|TERM|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Received TERMINATE op in state DOWN: 1 refcnts, 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs\r\n%7|1582294701.013|BROKERFAIL|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: failed: err: Local: Broker handle destroyed: (errno: Bad address)\r\n%7|1582294701.013|FAIL|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Client is terminating (after 1ms in state DOWN)\r\n%7|1582294701.013|TERMINATE|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: Handle is terminating in state DOWN: 1 refcnts (0x20750e0), 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs: failed 0 request(s) in retry+outbuf\r\n%7|1582294701.013|BROKERFAIL|rdkafka#producer-1| [thrd:non_working_broker:9092/bootstrap]: non_working_broker:9092/bootstrap: failed: err: Local: Broker handle destroyed: (errno: Bad address)\r\n%7|1582294701.013|TERM|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Received TERMINATE op in state INIT: 1 refcnts, 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs\r\n%7|1582294701.013|BROKERFAIL|rdkafka#producer-1| [thrd::0/internal]: :0/internal: failed: err: Local: Broker handle destroyed: (errno: Success)\r\n%7|1582294701.013|FAIL|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Client is terminating (after 25ms in state INIT)\r\n%7|1582294701.013|STATE|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Broker changed state INIT -> DOWN\r\n%7|1582294701.013|TERMINATE|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Handle is terminating in state DOWN: 1 refcnts (0x2074350), 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs: failed 0 request(s) in retry+outbuf\r\n%7|1582294701.013|BROKERFAIL|rdkafka#producer-1| [thrd::0/internal]: :0/internal: failed: err: Local: Broker handle destroyed: (errno: Success)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/782", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/782/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/782/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/782/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/782", "id": 567515926, "node_id": "MDU6SXNzdWU1Njc1MTU5MjY=", "number": 782, "title": "on_assign with multiple consumers not dispatched to the correct consumer when group.id are the same", "user": {"login": "0x26res", "id": 80800, "node_id": "MDQ6VXNlcjgwODAw", "avatar_url": "https://avatars1.githubusercontent.com/u/80800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0x26res", "html_url": "https://github.com/0x26res", "followers_url": "https://api.github.com/users/0x26res/followers", "following_url": "https://api.github.com/users/0x26res/following{/other_user}", "gists_url": "https://api.github.com/users/0x26res/gists{/gist_id}", "starred_url": "https://api.github.com/users/0x26res/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0x26res/subscriptions", "organizations_url": "https://api.github.com/users/0x26res/orgs", "repos_url": "https://api.github.com/users/0x26res/repos", "events_url": "https://api.github.com/users/0x26res/events{/privacy}", "received_events_url": "https://api.github.com/users/0x26res/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-19T11:50:51Z", "updated_at": "2020-02-26T09:28:16Z", "closed_at": "2020-02-26T09:28:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nI have two `Consumer`s, they both have the same `group.id`, but they subscribe to different topics (topic1 and topic2) and have a different on_assign callback regIstered\r\n\r\nI can subscribe the first consumer to topic1 and poll until on_assign is called.\r\nBut when I subscribe to the second consumer to topic2 and poll until on_assign is called, it hangs.\r\n\r\nNote that:\r\n* if I keep polling the first consumer after subscribing the second consumer, both on_assign call back are called, with their respective topic (In other world, on_assign callback for topic1 is called a second time, on_assign callback for topic2 is called)\r\n* it works fine when I have different group.id.\r\n\r\n\r\nHow to reproduce\r\n================\r\n* Broken version:\r\n```\r\ndef subscribe_consumer(consumer, topic):\r\n    print('SUBSCRIBING', topic)\r\n    assigned = threading.Event()\r\n\r\n    def on_assign(consumer, partitions):\r\n        print(f\"ASSIGNED {topic} {partitions}\")\r\n        assigned.set()\r\n\r\n    consumer.subscribe([topic], on_assign=on_assign)\r\n    while not assigned.is_set():\r\n        msg = consumer.poll(1)\r\n        if msg:\r\n            print(msg.error(), msg.value())\r\n    print('SUBSCRIBED', topic)\r\n\r\nprint(confluent_kafka.__version__)\r\ngroup_id = str(uuid.uuid4())\r\nprint(group_id)\r\n    \r\nconsumer1 = Consumer(\r\n     {\r\n         'bootstrap.servers': bootstrap_servers,\r\n         'group.id': group_id, \r\n         'enable.auto.commit': False},\r\n    )\r\n\r\nsubscribe_consumer(consumer1, topic1)\r\n\r\n\r\nconsumer2 = Consumer(\r\n     {\r\n         'bootstrap.servers': bootstrap_servers,\r\n         'group.id': group_id,\r\n         'enable.auto.commit': False},\r\n    )\r\n\r\nsubscribe_consumer(consumer2, topic2)\r\n```\r\nPrints:\r\n```\r\n1.3.0\r\n6761b369-df8b-4955-a318-ed87063fa685\r\nSUBSCRIBING topic1\r\nASSIGNED topic1 <topic1 partitions>\r\nSUBSCRIBED topic1\r\nSUBSCRIBING topic2\r\n```\r\nAnd hangs from there...\r\n\r\n* Working version with different group.id:\r\n```\r\ndef subscribe_consumer(consumer, topic):\r\n    print('SUBSCRIBING', topic)\r\n    assigned = threading.Event()\r\n\r\n    def on_assign(consumer, partitions):\r\n        print(f\"ASSIGNED {topic} {partitions}\")\r\n        assigned.set()\r\n\r\n    consumer.subscribe([topic], on_assign=on_assign)\r\n    while not assigned.is_set():\r\n        msg = consumer.poll(1)\r\n        if msg:\r\n            print(msg.error(), msg.value())\r\n    print('SUBSCRIBED', topic)\r\n\r\nprint(confluent_kafka.__version__)\r\ngroup_id = str(uuid.uuid4())\r\nprint(group_id)\r\n    \r\nconsumer1 = Consumer(\r\n     {\r\n         'bootstrap.servers': bootstrap_servers,\r\n         'group.id': group_id, \r\n         'enable.auto.commit': False},\r\n    )\r\n\r\nsubscribe_consumer(consumer1, topic1)\r\n\r\n\r\nconsumer2 = Consumer(\r\n     {\r\n         'bootstrap.servers': bootstrap_servers,\r\n         'group.id': group_id + '_topic2',\r\n         'enable.auto.commit': False},\r\n    )\r\n\r\nsubscribe_consumer(consumer2, topic2)\r\n```\r\nPrints:\r\n```\r\n1.3.0\r\n0d8c8d41-211f-424f-8962-c916c99a7e78\r\nSUBSCRIBING topic1\r\nASSIGNED topic1 <topic1 partitions>\r\nSUBSCRIBED topic1\r\nSUBSCRIBING topic2\r\nASSIGNED topic2 <topic2 partitions>\r\nSUBSCRIBED topic2\r\n```\r\n* Working version, polling consumer1 while subscribing consumer2\r\n```\r\ndef subscribe_consumer(consumer, topic):\r\n    print('SUBSCRIBING', topic)\r\n    assigned = threading.Event()\r\n\r\n    def on_assign(consumer, partitions):\r\n        print(f\"ASSIGNED {topic} {partitions}\")\r\n        assigned.set()\r\n\r\n    consumer.subscribe([topic], on_assign=on_assign)\r\n    while not assigned.is_set():\r\n        msg = consumer.poll(1)\r\n        if msg:\r\n            print(msg.error(), msg.value())\r\n    print('SUBSCRIBED', topic)\r\n\r\nprint(confluent_kafka.__version__)\r\ngroup_id = str(uuid.uuid4())\r\nprint(group_id)\r\n    \r\nconsumer1 = Consumer(\r\n     {\r\n         'bootstrap.servers': bootstrap_servers,\r\n         'group.id': group_id, \r\n         'enable.auto.commit': False},\r\n    )\r\n\r\nsubscribe_consumer(consumer1, topic1)\r\n\r\ndef poll(consumer):\r\n    while True:\r\n        msg = consumer.poll(1)\r\n        if msg:\r\n            print(msg.error(), msg.value(), msg.topic())\r\n\r\nthread = threading.Thread(target=lambda: poll(consumer1))\r\nthread.start()\r\n\r\nconsumer2 = Consumer(\r\n     {\r\n         'bootstrap.servers': bootstrap_servers,\r\n         'group.id': group_id,\r\n         'enable.auto.commit': False},\r\n    )\r\n\r\nsubscribe_consumer(consumer2, topic2)\r\n```\r\nPrints (note the spurious call to ASSIGNED on topic1 the second time:\r\n```\r\n1.3.0\r\n0d8c8d41-211f-424f-8962-c916c99a7e78\r\nSUBSCRIBING topic1\r\nASSIGNED topic1 <topic1 partitions>\r\nSUBSCRIBED topic1\r\nSUBSCRIBING topic2\r\nASSIGNED topic1 <topic1 partitions>\r\nASSIGNED topic2 <topic2 partitions>\r\nSUBSCRIBED topic2\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python ('1.3.0', 16973824) and librdkafka version ('1.3.0', 16974079)\r\n - [x] Apache Kafka broker version: 2.2\r\n - [x] Client configuration: \r\n```\r\n{\r\n         'bootstrap.servers': bootstrap_servers,\r\n         'group.id': group_id,\r\n         'enable.auto.commit': False\r\n}\r\n```\r\n - [x] Operating system: centos 7\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/780", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/780/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/780/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/780/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/780", "id": 566891456, "node_id": "MDU6SXNzdWU1NjY4OTE0NTY=", "number": 780, "title": "Kafka producer becomes unresponsive when trying to connect to a non-existing broker", "user": {"login": "JoaoVasques", "id": 1440120, "node_id": "MDQ6VXNlcjE0NDAxMjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1440120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JoaoVasques", "html_url": "https://github.com/JoaoVasques", "followers_url": "https://api.github.com/users/JoaoVasques/followers", "following_url": "https://api.github.com/users/JoaoVasques/following{/other_user}", "gists_url": "https://api.github.com/users/JoaoVasques/gists{/gist_id}", "starred_url": "https://api.github.com/users/JoaoVasques/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JoaoVasques/subscriptions", "organizations_url": "https://api.github.com/users/JoaoVasques/orgs", "repos_url": "https://api.github.com/users/JoaoVasques/repos", "events_url": "https://api.github.com/users/JoaoVasques/events{/privacy}", "received_events_url": "https://api.github.com/users/JoaoVasques/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-18T13:26:00Z", "updated_at": "2020-02-26T10:29:59Z", "closed_at": "2020-02-25T19:56:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nKafka producer blocks forever when connecting to a broker list that does not exist.\r\n\r\nHow to reproduce\r\n================\r\n\r\nRun the following set of commands where `mybroker1,mybroker2` are invalid broker names. if you do you will notice that the console blocks forever. The expected behaviour, in my opinion, is that the client would crash when trying to setup a producer with invalid brokers.\r\n\r\n```python\r\nfrom confluent_kafka import Producer\r\n\r\np = Producer({'bootstrap.servers': 'mybroker1,mybroker2'})\r\n\r\ndef delivery_report(err, msg):\r\n    \"\"\" Called once for each message produced to indicate delivery result.\r\n        Triggered by poll() or flush(). \"\"\"\r\n    if err is not None:\r\n        print('Message delivery failed: {}'.format(err))\r\n    else:\r\n        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\r\n\r\nsome_data_source = [\"ola\", \"adeus\"]\r\n\r\nfor data in some_data_source:\r\n    # Trigger any available delivery report callbacks from previous produce() calls\r\n    p.poll(0)\r\n    # Asynchronously produce a message, the delivery report callback\r\n    # will be triggered from poll() above, or flush() below, when the message has\r\n    # been successfully delivered or failed permanently.\r\n    p.produce('mytopic', data.encode('utf-8'), callback=delivery_report)\r\n# Wait for any outstanding messages to be delivered and delivery report\r\n# callbacks to be triggered.\r\np.flush()\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python version 1.3.0\r\n - [ ] Apache Kafka broker version: N/A\r\n - [ ] Client configuration: `{'bootstrap.servers': 'mybroker1,mybroker2'}`\r\n - [ ] Operating system: MacOS Majove 10.14\r\n - [ ] Provide client logs : N/A\r\n - [ ] Provide broker log excerpts: N/A\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/777", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/777/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/777/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/777/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/777", "id": 564183345, "node_id": "MDU6SXNzdWU1NjQxODMzNDU=", "number": 777, "title": "Failed to create topic automatedTestTopic: KafkaError{code=_TIMED_OUT,val=-185,str=\"Timed out waiting for controller\"}", "user": {"login": "AnamikaN", "id": 25681678, "node_id": "MDQ6VXNlcjI1NjgxNjc4", "avatar_url": "https://avatars3.githubusercontent.com/u/25681678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnamikaN", "html_url": "https://github.com/AnamikaN", "followers_url": "https://api.github.com/users/AnamikaN/followers", "following_url": "https://api.github.com/users/AnamikaN/following{/other_user}", "gists_url": "https://api.github.com/users/AnamikaN/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnamikaN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnamikaN/subscriptions", "organizations_url": "https://api.github.com/users/AnamikaN/orgs", "repos_url": "https://api.github.com/users/AnamikaN/repos", "events_url": "https://api.github.com/users/AnamikaN/events{/privacy}", "received_events_url": "https://api.github.com/users/AnamikaN/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-12T18:24:42Z", "updated_at": "2020-02-13T18:51:19Z", "closed_at": "2020-02-13T18:51:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nI am getting below error while trying to create topic in confluent kafka using adminclient.\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\nfrom confluent_kafka.admin import AdminClient, NewTopic\r\n\r\na = AdminClient({'bootstrap.servers': 'xx.xxx.xxx.xxx:9093'})\r\n\r\nnew_topics = [NewTopic(topic, num_partitions=8, replication_factor=3) for topic in [\"automatedTestTopic\"]]\r\n\r\nfs = a.create_topics(new_topics)\r\nfor topic, f in fs.items():\r\n    try:\r\n        f.result()  # The result itself is None\r\n        print(\"Topic {} created\".format(topic))\r\n    except Exception as e:\r\n        print(\"Failed to create topic {}: {}\".format(topic, e))\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version(1.3.0)` and `confluent_kafka.libversion(1.3.0)`):\r\n - [x] Apache Kafka broker version: 2.0.0\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\nIn my broker's server.properties, i have below settings. Do I need to have some special settings in adminClient because SASL?\r\n\r\nadvertised.listeners=SASL_PLAINTEXT://xx.xxx.xxx.xxx:9092,SASL_SSL://xx.xxx.xxx.xxx:9093\r\nauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer\r\nauto.create.topics.enable=false\r\n\r\nlistener.name.internal.ssl.endpoint.identification.algorithm=\r\nlisteners=SASL_PLAINTEXT://:9092,SASL_SSL://:9093\r\nlog.dirs=/kafkadata/kafka-logs\r\nmax.message.bytes=15728640\r\nmessage.max.bytes=15728640\r\nmetric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter\r\nreplica.fetch.max.bytes=15728640\r\nsasl.enabled.mechanisms=PLAIN\r\nsasl.mechanism.inter.broker.protocol=PLAIN\r\nsecurity.inter.broker.protocol=SASL_SSL\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/775", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/775/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/775/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/775/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/775", "id": 559968291, "node_id": "MDU6SXNzdWU1NTk5NjgyOTE=", "number": 775, "title": "Kafka-python package is deleting values from app config", "user": {"login": "agates4", "id": 2314309, "node_id": "MDQ6VXNlcjIzMTQzMDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/2314309?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agates4", "html_url": "https://github.com/agates4", "followers_url": "https://api.github.com/users/agates4/followers", "following_url": "https://api.github.com/users/agates4/following{/other_user}", "gists_url": "https://api.github.com/users/agates4/gists{/gist_id}", "starred_url": "https://api.github.com/users/agates4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agates4/subscriptions", "organizations_url": "https://api.github.com/users/agates4/orgs", "repos_url": "https://api.github.com/users/agates4/repos", "events_url": "https://api.github.com/users/agates4/events{/privacy}", "received_events_url": "https://api.github.com/users/agates4/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-04T20:44:48Z", "updated_at": "2020-02-10T11:28:37Z", "closed_at": "2020-02-10T11:07:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nhttps://github.com/confluentinc/confluent-kafka-python/blob/master/confluent_kafka/avro/__init__.py#L32\r\n\r\npython does direct mutations to original memory location of dictionaries. with the line I linked, this will remove the value of this config from our passed through dict. if I'm using app.config, this means that the confluent kafka package is actually modifying my app's config value. \r\n\r\nHow to reproduce\r\n================\r\npass it a dict with \"schema.registry.url\"\r\n\r\nproduce a message\r\n\r\nprint the original dict after the message was produced\r\n\r\nnotice that the dict no longer contains a value for \"schema.registry.url\"\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\nlatest version \r\n - [X] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [X] Apache Kafka broker version:\r\n - [X] Client configuration: `{...}`\r\n - [X] Operating system:\r\n - [X] Provide client logs (with `'debug': '..'` as necessary)\r\n - [X] Provide broker log excerpts\r\n - [X] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/774", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/774/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/774/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/774/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/774", "id": 557637008, "node_id": "MDU6SXNzdWU1NTc2MzcwMDg=", "number": 774, "title": "Confluent kafka related error while executing python file in linkedin datahub quickstart guide", "user": {"login": "PriyaShaji", "id": 35895465, "node_id": "MDQ6VXNlcjM1ODk1NDY1", "avatar_url": "https://avatars3.githubusercontent.com/u/35895465?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PriyaShaji", "html_url": "https://github.com/PriyaShaji", "followers_url": "https://api.github.com/users/PriyaShaji/followers", "following_url": "https://api.github.com/users/PriyaShaji/following{/other_user}", "gists_url": "https://api.github.com/users/PriyaShaji/gists{/gist_id}", "starred_url": "https://api.github.com/users/PriyaShaji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PriyaShaji/subscriptions", "organizations_url": "https://api.github.com/users/PriyaShaji/orgs", "repos_url": "https://api.github.com/users/PriyaShaji/repos", "events_url": "https://api.github.com/users/PriyaShaji/events{/privacy}", "received_events_url": "https://api.github.com/users/PriyaShaji/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-30T17:16:09Z", "updated_at": "2020-02-11T18:01:57Z", "closed_at": "2020-02-11T18:01:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n\r\n===========\r\nI am trying to execute linkedin datahub quickstart guide: [linkedin datahub](https://github.com/linkedin/datahub)\r\nWhen I run the last command to ingest sample data `python mce_cli.py produce -d bootstrap_mce.dat`, it gives an error which I guess is related to confluent-kafka.\r\n\r\nlink to the mce-cli.py file: [mce-cli.py](https://github.com/linkedin/datahub/blob/master/metadata-ingestion/mce-cli/mce_cli.py)\r\n\r\nlink to quickstart docker-compose.yml file: [docker-compose.yml](https://github.com/linkedin/datahub/blob/master/docker/quickstart/docker-compose.yml)\r\n\r\n-------------------\r\n\r\n(NAME) t5ftsa06614:mce-cli pshaji$ python mce_cli.py produce -d bootstrap_mce.dat\r\nProducing MetadataChangeEvent records to topic MetadataChangeEvent. ^c to exit.\r\nTraceback (most recent call last):\r\nFile \"mce_cli.py\", line 108, in\r\nmain(parser.parse_args())\r\nFile \"mce_cli.py\", line 88, in main\r\nproduce(conf, args.data_file, args.schema_record)\r\nFile \"mce_cli.py\", line 32, in produce\r\nproducer.produce(topic=topic, value=content)\r\nFile \"/Users/pshaji/.local/lib/python2.7/site-packages/confluent_kafka/avro/init.py\", line 80, in produce\r\nvalue = self._serializer.encode_record_with_schema(topic, value_schema, value)\r\nFile \"/Users/pshaji/.local/lib/python2.7/site-packages/confluent_kafka/avro/serializer/message_serializer.py\", line 105, in encode_record_with_schema\r\nschema_id = self.registry_client.register(subject, schema)\r\nFile \"/Users/pshaji/.local/lib/python2.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py\", line 217, in register\r\nresult, code = self._send_request(url, method='POST', body=body)\r\nFile \"/Users/pshaji/.local/lib/python2.7/site-packages/confluent_kafka/avro/cached_schema_registry_client.py\", line 166, in _send_request\r\nresponse = self._session.request(method, url, headers=_headers, json=body)\r\nFile \"/Users/pshaji/.local/lib/python2.7/site-packages/requests/sessions.py\", line 533, in request\r\nresp = self.send(prep, **send_kwargs)\r\nFile \"/Users/pshaji/.local/lib/python2.7/site-packages/requests/sessions.py\", line 646, in send\r\nr = adapter.send(request, **kwargs)\r\nFile \"/Users/pshaji/.local/lib/python2.7/site-packages/requests/adapters.py\", line 498, in send\r\nraise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: ('Connection aborted.', BadStatusLine('No status line received - the server has closed the connection',))\r\n\r\n---------------------------------------------------------------\r\n\r\nHow to reproduce\r\n\r\n================\r\n\r\nThe environment I am working is:\r\nMac OS, 10.14.6\r\njava version \"1.8.0_221\"\r\nJava(TM) SE Runtime Environment (build 1.8.0_221-b11)\r\nJava HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode)\r\n\r\nUnable to resolve this issue. \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/772", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/772/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/772/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/772/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/772", "id": 556048890, "node_id": "MDU6SXNzdWU1NTYwNDg4OTA=", "number": 772, "title": "Why there is no transitive compatibility level such as BACKWARD_TRANSITIVE and so on?", "user": {"login": "mpyatishev", "id": 3088773, "node_id": "MDQ6VXNlcjMwODg3NzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/3088773?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpyatishev", "html_url": "https://github.com/mpyatishev", "followers_url": "https://api.github.com/users/mpyatishev/followers", "following_url": "https://api.github.com/users/mpyatishev/following{/other_user}", "gists_url": "https://api.github.com/users/mpyatishev/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpyatishev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpyatishev/subscriptions", "organizations_url": "https://api.github.com/users/mpyatishev/orgs", "repos_url": "https://api.github.com/users/mpyatishev/repos", "events_url": "https://api.github.com/users/mpyatishev/events{/privacy}", "received_events_url": "https://api.github.com/users/mpyatishev/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-28T08:15:05Z", "updated_at": "2020-02-11T05:17:26Z", "closed_at": "2020-02-11T05:17:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nWhy there is no transitive compatibility level such as BACKWARD_TRANSITIVE and so on?\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\nTry to set compatibility level to BACKWARD_TRANSITIVE\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python: 1.3.0\r\n - [ ] librdkafka: 1.2.1\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/759", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/759/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/759/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/759/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/759", "id": 546410036, "node_id": "MDU6SXNzdWU1NDY0MTAwMzY=", "number": 759, "title": "Memory Issue when consumer backlog is high", "user": {"login": "arunshankerprasad", "id": 194829, "node_id": "MDQ6VXNlcjE5NDgyOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/194829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arunshankerprasad", "html_url": "https://github.com/arunshankerprasad", "followers_url": "https://api.github.com/users/arunshankerprasad/followers", "following_url": "https://api.github.com/users/arunshankerprasad/following{/other_user}", "gists_url": "https://api.github.com/users/arunshankerprasad/gists{/gist_id}", "starred_url": "https://api.github.com/users/arunshankerprasad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arunshankerprasad/subscriptions", "organizations_url": "https://api.github.com/users/arunshankerprasad/orgs", "repos_url": "https://api.github.com/users/arunshankerprasad/repos", "events_url": "https://api.github.com/users/arunshankerprasad/events{/privacy}", "received_events_url": "https://api.github.com/users/arunshankerprasad/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-07T17:25:48Z", "updated_at": "2020-01-07T22:06:13Z", "closed_at": "2020-01-07T22:06:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nWe noticed an issue with the consumers using up a lot of memory when the backlog/lag increases. We were able to consistently reproduce this as well. Also confirmed that this is not application specific, since we commented out all of our application code and it was just polling messages and doing nothing with it.\r\n\r\nThe issue seems to only happen if we have a huge backlog/lag built up in Kafka. In our use-case, we saw that the backlog was around 20K messages, each of about 175KB in size, all JSON strings. We run the application in Kubernetes, and the pods have a request limit of 1GB RAM, when we have a backlog of messages, as soon as the pod starts up, it seems to be requesting for over 1GB of RAM, causing the pod to be terminated. We are only polling for *1 message* at a time, so it should not have required this much resources. We did try updating several config values, like setting `fetch.message.max.bytes`, `max.partition.fetch.bytes`, but nothing seems to have worked.\r\n\r\nUnclear as to how the consumer works internally, does it build a buffer internally, even if we are polling for 1 message at a time?\r\n\r\nTo resolve this issue we switched to using the [kafka-python](https://github.com/dpkp/kafka-python/) library, and are not seeing any memory issues when using it.\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n* Build a backlog/lag in Kafka for a topic\r\n* Start the consumer and poll for 1 message at a time\r\n* Monitor memory usage, it will spike based on the backlog\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): confluent-kafka: 0.11.6 & 1.1.0 librdkafka: 1.0.0\r\n - [x] Apache Kafka broker version: 1.1.0\r\n - [x] Client configuration: `{\"bootstrap.servers\":[],\"group.id\":\"group_id\",\"auto.offset.reset\":\"earliest\",\"max.poll.interval.ms\":300000}`\r\n - [x] Operating system: CentOS 7.5\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/754", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/754/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/754/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/754/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/754", "id": 542898998, "node_id": "MDU6SXNzdWU1NDI4OTg5OTg=", "number": 754, "title": "Python client doesn't do anything", "user": {"login": "fsw0422", "id": 5856657, "node_id": "MDQ6VXNlcjU4NTY2NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/5856657?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fsw0422", "html_url": "https://github.com/fsw0422", "followers_url": "https://api.github.com/users/fsw0422/followers", "following_url": "https://api.github.com/users/fsw0422/following{/other_user}", "gists_url": "https://api.github.com/users/fsw0422/gists{/gist_id}", "starred_url": "https://api.github.com/users/fsw0422/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fsw0422/subscriptions", "organizations_url": "https://api.github.com/users/fsw0422/orgs", "repos_url": "https://api.github.com/users/fsw0422/repos", "events_url": "https://api.github.com/users/fsw0422/events{/privacy}", "received_events_url": "https://api.github.com/users/fsw0422/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-27T14:51:33Z", "updated_at": "2019-12-28T14:42:53Z", "closed_at": "2019-12-28T14:42:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI'm experiencing a strange behavior where when I run the program, nothing happens (no exceptions, no logs, just nothing) when I navigate to the code i see that it goes down to a path where there is no implementation but `pass` I checked the library dependencies in the venv directory and it seems I have the cimpl.so file. Do you have any idea about this?\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\ninstall with `pip install confluent-kafka`\r\nsimple Kafka admin create topic logic\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system: Mac OSX\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/751", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/751/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/751/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/751/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/751", "id": 541250216, "node_id": "MDU6SXNzdWU1NDEyNTAyMTY=", "number": 751, "title": "Need manylinux1 wheels for 1.3.0", "user": {"login": "kerrick-lyft", "id": 15618653, "node_id": "MDQ6VXNlcjE1NjE4NjUz", "avatar_url": "https://avatars1.githubusercontent.com/u/15618653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kerrick-lyft", "html_url": "https://github.com/kerrick-lyft", "followers_url": "https://api.github.com/users/kerrick-lyft/followers", "following_url": "https://api.github.com/users/kerrick-lyft/following{/other_user}", "gists_url": "https://api.github.com/users/kerrick-lyft/gists{/gist_id}", "starred_url": "https://api.github.com/users/kerrick-lyft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kerrick-lyft/subscriptions", "organizations_url": "https://api.github.com/users/kerrick-lyft/orgs", "repos_url": "https://api.github.com/users/kerrick-lyft/repos", "events_url": "https://api.github.com/users/kerrick-lyft/events{/privacy}", "received_events_url": "https://api.github.com/users/kerrick-lyft/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-12-21T00:02:01Z", "updated_at": "2020-01-24T19:48:43Z", "closed_at": "2020-01-24T19:48:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nFor `confluent-kafka` 1.2.0 and earlier, `manylinux1` wheel files are provided on PyPI (see https://pypi.org/project/confluent-kafka/1.2.0/#files).\r\n\r\nFor 1.3.0, no `manylinux1` files were published, only `manylinux2010` (see https://pypi.org/project/confluent-kafka/1.3.0/#files).\r\n\r\nIn some of our environments at Lyft, we use an older version of `pip` that does not support the `manylinux2010` files, only the older `manylinux1` files, and so instead of using the binary wheel package, `pip` tries to compile the source package. This prevents us from using 1.3.0 or later of the package, because we do not have the necessary environment set up to compile the package from source.\r\n\r\nHow to reproduce\r\n================\r\n1. Install `pip==1.18.1` or earlier.\r\n2. Install `confluent-kafka` using pip.\r\n\r\npip will attempt to build confluent-kafka from source.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): 1.3.0\r\n - [x] Apache Kafka broker version: N/A\r\n - [x] Client configuration: N/A\r\n - [x] Operating system: Ubuntu Xenial 16.04.6 LTS\r\n - [x] Provide client logs (with `'debug': '..'` as necessary): N/A\r\n - [x] Provide broker log excerpts: N/A\r\n - [x] Critical issue: No", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/741", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/741/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/741/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/741/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/741", "id": 536971112, "node_id": "MDU6SXNzdWU1MzY5NzExMTI=", "number": 741, "title": "AvroProducer fails silently when not setting `\"security.protocol\": \"ssl\"`", "user": {"login": "ruurtjan", "id": 1055478, "node_id": "MDQ6VXNlcjEwNTU0Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1055478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ruurtjan", "html_url": "https://github.com/ruurtjan", "followers_url": "https://api.github.com/users/ruurtjan/followers", "following_url": "https://api.github.com/users/ruurtjan/following{/other_user}", "gists_url": "https://api.github.com/users/ruurtjan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ruurtjan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ruurtjan/subscriptions", "organizations_url": "https://api.github.com/users/ruurtjan/orgs", "repos_url": "https://api.github.com/users/ruurtjan/repos", "events_url": "https://api.github.com/users/ruurtjan/events{/privacy}", "received_events_url": "https://api.github.com/users/ruurtjan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948252, "node_id": "MDU6TGFiZWwzNTc5NDgyNTI=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/duplicate", "name": "duplicate", "color": "cccccc", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2019-12-12T13:15:10Z", "updated_at": "2019-12-16T10:44:47Z", "closed_at": "2019-12-16T10:44:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nToday, we forgot to set the security protocol to ssl. This resulted in flush taking a couple of minutes, then proceeding without storing anything to Kafka. The schemas got persisted to the schema registry though.\r\n\r\nExpected behaviour is some error message, preferably indicating that you forgot to specify the security protocol.\r\n\r\nOutput with debug on:\r\n```\r\n%7|1576156120.005|BROKER|rdkafka#producer-1| [thrd:app]: [...broker1...]:9094/bootstrap: Added new broker with NodeId -1\r\n%7|1576156120.005|BRKMAIN|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Enter main broker thread\r\n%7|1576156120.005|BRKMAIN|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Enter main broker thread\r\n%7|1576156120.005|BROKER|rdkafka#producer-1| [thrd:app]: [...broker2...]:9094/bootstrap: Added new broker with NodeId -1\r\n%7|1576156120.005|BROKER|rdkafka#producer-1| [thrd:app]: [...broker3...]:9094/bootstrap: Added new broker with NodeId -1\r\n%7|1576156120.005|BRKMAIN|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Enter main broker thread\r\n%7|1576156120.005|CONNECT|rdkafka#producer-1| [thrd:app]: [...broker3...]:9094/bootstrap: Selected for cluster connection: bootstrap servers added (broker has 0 connection attempt(s))\r\n%7|1576156120.005|INIT|rdkafka#producer-1| [thrd:app]: librdkafka v1.2.0 (0x10200ff) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, GCC GXX PKGCONFIG INSTALL GNULD LDS LIBDL PLUGINS ZLIB SSL ZSTD HDRHISTOGRAM SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0x46)\r\n%7|1576156120.005|BRKMAIN|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Enter main broker thread\r\n%7|1576156120.005|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Received CONNECT op\r\n%7|1576156120.005|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1576156120.005|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1576156120.005|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\nget_producer\r\nproduce\r\n%7|1576156120.038|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Connecting to ipv4#10.212.4.49:9094 (plaintext) with socket 11\r\n%7|1576156120.074|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Connected to ipv4#10.212.4.49:9094\r\n%7|1576156120.074|CONNECTED|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Connected (#1)\r\n%7|1576156120.074|FEATURE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion\r\n%7|1576156120.074|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1576156120.104|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156120.104|FEATURE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1576156120.104|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1576156120.104|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156120.104|FAIL|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1576156120.250|TOPIC|rdkafka#producer-1| [thrd:app]: New local topic: ruurtjanTest\r\n%7|1576156120.250|TOPPARNEW|rdkafka#producer-1| [thrd:app]: NEW ruurtjanTest [-1] 0x2772ed0 (at rd_kafka_topic_new0:393)\r\nflush\r\n%7|1576156121.005|NOINFO|rdkafka#producer-1| [thrd:main]: Topic ruurtjanTest metadata information unknown\r\n%7|1576156121.005|NOINFO|rdkafka#producer-1| [thrd:main]: Topic ruurtjanTest partition count is zero: should refresh metadata\r\n%7|1576156121.005|CONNECT|rdkafka#producer-1| [thrd:main]: [...broker1...]:9094/bootstrap: Selected for cluster connection: refresh unavailable topics (broker has 0 connection attempt(s))\r\n%7|1576156121.005|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1576156121.005|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1576156121.005|CONNECT|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Received CONNECT op\r\n%7|1576156121.005|STATE|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1576156121.005|CONNECT|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1576156121.005|STATE|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1576156121.036|CONNECT|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Connecting to ipv4#10.212.11.81:9094 (plaintext) with socket 11\r\n%7|1576156121.067|CONNECT|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Connected to ipv4#10.212.11.81:9094\r\n%7|1576156121.067|CONNECTED|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Connected (#1)\r\n%7|1576156121.067|FEATURE|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion\r\n%7|1576156121.067|STATE|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1576156121.104|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state DOWN -> INIT\r\n%7|1576156121.118|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156121.118|FEATURE|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1576156121.118|STATE|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1576156121.118|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156121.118|FAIL|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker1...]:9094/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1576156121.118|CONNECT|rdkafka#producer-1| [thrd:[...broker1...].]: [...broker2...]:9094/bootstrap: Selected for cluster connection: broker down (broker has 0 connection attempt(s))\r\n%7|1576156121.118|CONNECT|rdkafka#producer-1| [thrd:[...broker1...].]: Not selecting any broker for cluster connection: still suppressed for 49ms: broker down\r\n%7|1576156121.118|CONNECT|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Received CONNECT op\r\n%7|1576156121.118|STATE|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1576156121.118|CONNECT|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1576156121.118|STATE|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1576156121.177|CONNECT|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Connecting to ipv4#10.212.1.221:9094 (plaintext) with socket 11\r\n%7|1576156121.209|CONNECT|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Connected to ipv4#10.212.1.221:9094\r\n%7|1576156121.209|CONNECTED|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Connected (#1)\r\n%7|1576156121.209|FEATURE|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion\r\n%7|1576156121.209|STATE|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1576156121.248|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156121.248|FEATURE|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1576156121.248|STATE|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1576156121.248|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156121.248|FAIL|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker2...]:9094/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1576156121.248|CONNECT|rdkafka#producer-1| [thrd:[...broker2...].]: [...broker3...]:9094/bootstrap: Selected for cluster connection: broker down (broker has 1 connection attempt(s))\r\n%7|1576156121.248|CONNECT|rdkafka#producer-1| [thrd:[...broker2...].]: Not selecting any broker for cluster connection: still suppressed for 49ms: broker down\r\n%7|1576156121.248|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Received CONNECT op\r\n%7|1576156121.248|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1576156121.248|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1576156121.248|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1576156121.320|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Connecting to ipv4#10.212.4.49:9094 (plaintext) with socket 11\r\n%7|1576156121.353|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Connected to ipv4#10.212.4.49:9094\r\n%7|1576156121.353|CONNECTED|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Connected (#2)\r\n%7|1576156121.353|FEATURE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion\r\n%7|1576156121.353|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1576156121.382|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156121.382|FEATURE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1576156121.382|STATE|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1576156121.382|BROKERFAIL|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1576156121.382|FAIL|rdkafka#producer-1| [thrd:[...broker3...].]: [...broker3...]:9094/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1576156121.382|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: Cluster connection already in progress: broker down\r\n%7|1576156121.382|CONNECT|rdkafka#producer-1| [thrd:[...broker3...].]: Not selecting any broker for cluster connection: still suppressed for 49ms: broker down\r\n```\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n```\r\n    producer = AvroProducer({\r\n        # \"security.protocol\": \"ssl\",\r\n        'bootstrap.servers': '...',\r\n        'schema.registry.url': '...'\r\n    }, default_key_schema=key_schema, default_value_schema=value_schema)\r\n\r\n    producer.produce(topic=\"...\",\r\n                     value={'...': '...'},\r\n                     key={'...': '...'})\r\n    producer.flush()\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): ('1.2.0', 16908288) and ('1.2.0', 16908543)\r\n - [x] Apache Kafka broker version: 2.2.1\r\n - [x] Client configuration: `{...}` see 'how to reproduce'\r\n - [x] Operating system: Ubuntu\r\n - [x] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/739", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/739/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/739/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/739/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/739", "id": 535958298, "node_id": "MDU6SXNzdWU1MzU5NTgyOTg=", "number": 739, "title": "Forced Rebalancing Quirks", "user": {"login": "lexwraith", "id": 2289157, "node_id": "MDQ6VXNlcjIyODkxNTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2289157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lexwraith", "html_url": "https://github.com/lexwraith", "followers_url": "https://api.github.com/users/lexwraith/followers", "following_url": "https://api.github.com/users/lexwraith/following{/other_user}", "gists_url": "https://api.github.com/users/lexwraith/gists{/gist_id}", "starred_url": "https://api.github.com/users/lexwraith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lexwraith/subscriptions", "organizations_url": "https://api.github.com/users/lexwraith/orgs", "repos_url": "https://api.github.com/users/lexwraith/repos", "events_url": "https://api.github.com/users/lexwraith/events{/privacy}", "received_events_url": "https://api.github.com/users/lexwraith/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-12-10T20:00:48Z", "updated_at": "2019-12-11T15:35:49Z", "closed_at": "2019-12-11T15:35:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI wrote a small test to mimic cluster conditions and try out rebalancing and noticed some quirks that I think I'm misunderstanding:\r\n\r\n* `AdminClient.delete_topics` with both timeouts doesn't seem to actually block the broker or guarantee deletion of the topics\r\n* Having a consumer on two partitions, then introducing another consumer in the same group, does not seem to guarantee a rebalance, even after a period of time\r\n* It's very hard to allocate the right amount of time for operations using `time.sleep`. It might be better go wrap these async functions with `asyncio` or something but might not be an option atm\r\n\r\nLog (the docker container running kafka/zookeeper is running in bg):\r\n```\r\nDeleting topics\r\nCreating topics\r\nAdd the messages to the log\r\nInitialize the consumers and start processing with A as the lead\r\nLet consumer A work a little, then start consumer b, which will trigger a rebalance\r\nConsumer A subscribing\r\nBlocking consumer A on cluster connection\r\nConsumer A ready for consuming\r\n[2019-12-10 19:52:40,605] INFO [GroupCoordinator 0]: Preparing to rebalance group vegetable in state PreparingRebalance with old generation 20 (__consumer_offsets-37) (reason: Adding new member rdkafka-0db89c59-c325-404d-9edc-599e01298f96) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:52:43,606] INFO [GroupCoordinator 0]: Stabilized group vegetable generation 21 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:52:43,613] INFO [GroupCoordinator 0]: Assignment received from leader for group vegetable for generation 21 (kafka.coordinator.group.GroupCoordinator)\r\nConsumer A got  [<cimpl.Message object at 0x10d33e138>]\r\n0 70\r\nReleasing lock for B to start\r\nConsumer B subscribing\r\nBlocking consumer B on cluster connection\r\nConsumer B ready for consuming\r\n[2019-12-10 19:52:45,652] INFO [GroupCoordinator 0]: Preparing to rebalance group vegetable in state PreparingRebalance with old generation 21 (__consumer_offsets-37) (reason: Adding new member rdkafka-5294864f-8f9e-4fe8-9774-d365200dd65a) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:52:49,644] INFO [GroupCoordinator 0]: Stabilized group vegetable generation 22 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:52:49,659] INFO [GroupCoordinator 0]: Assignment received from leader for group vegetable for generation 22 (kafka.coordinator.group.GroupCoordinator)\r\nConsumer A got  [<cimpl.Message object at 0x10d33e1b0>]\r\n0 71\r\nConsumer A got  [<cimpl.Message object at 0x10d33e138>]\r\n0 72\r\nConsumer A got  [<cimpl.Message object at 0x10d33e1b0>]\r\n0 73\r\nConsumer A got  [<cimpl.Message object at 0x10d33e138>]\r\n0 74\r\nConsumer A got  [<cimpl.Message object at 0x10d33e1b0>]\r\nConsumer B got  []\r\nConsumer B closing\r\n[2019-12-10 19:52:54,663] INFO [GroupCoordinator 0]: Member rdkafka-5294864f-8f9e-4fe8-9774-d365200dd65a in group vegetable has left, removing it from the group (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:52:54,664] INFO [GroupCoordinator 0]: Preparing to rebalance group vegetable in state PreparingRebalance with old generation 22 (__consumer_offsets-37) (reason: removing member rdkafka-5294864f-8f9e-4fe8-9774-d365200dd65a on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)\r\n0 75\r\nConsumer A got  [<cimpl.Message object at 0x10d33e138>]\r\n0 76\r\nConsumer A got  [<cimpl.Message object at 0x10d33e1b0>]\r\n0 77\r\n[2019-12-10 19:52:56,704] INFO [GroupCoordinator 0]: Stabilized group vegetable generation 23 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:52:56,708] INFO [GroupCoordinator 0]: Assignment received from leader for group vegetable for generation 23 (kafka.coordinator.group.GroupCoordinator)\r\nConsumer A got  [<cimpl.Message object at 0x10d33e138>]\r\n0 78\r\nConsumer A got  [<cimpl.Message object at 0x10d33e1b0>]\r\n0 79\r\nConsumer A got  []\r\nConsumer A closing\r\n[2019-12-10 19:53:08,701] INFO [GroupCoordinator 0]: Member rdkafka-0db89c59-c325-404d-9edc-599e01298f96 in group vegetable has left, removing it from the group (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:53:08,701] INFO [GroupCoordinator 0]: Preparing to rebalance group vegetable in state PreparingRebalance with old generation 23 (__consumer_offsets-37) (reason: removing member rdkafka-0db89c59-c325-404d-9edc-599e01298f96 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-12-10 19:53:08,702] INFO [GroupCoordinator 0]: Group vegetable with generation 24 is now empty (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)\r\n['0-70', '0-71', '0-72', '0-73', '0-74', '0-75', '0-76', '0-77', '0-78', '0-79'] []\r\n``` \r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\nScript:\r\n\r\n```\r\nimport time\r\nfrom multiprocessing import Process, Lock, Queue\r\n\r\nfrom confluent_kafka import Consumer, Producer\r\nfrom confluent_kafka.admin import NewTopic, AdminClient\r\n\r\nadmin = AdminClient({'bootstrap.servers': 'localhost:8082'})\r\n\r\n\r\ndef test_force_rebalance():\r\n    lock = Lock()\r\n    queue_a = Queue()\r\n    queue_b = Queue()\r\n    TOPIC = 'rooster'\r\n\r\n    print('Deleting topics')\r\n    admin.delete_topics([TOPIC], operation_timeout=30, request_timeout=30)\r\n\r\n    print('Creating topics')\r\n    admin.create_topics([NewTopic(TOPIC, 2, 1)])\r\n\r\n    def run_producer():\r\n        client = Producer({\r\n            'bootstrap.servers': 'localhost:9092',\r\n            'group.id': 'sunlight'\r\n            })\r\n        client.list_topics(timeout=10)\r\n\r\n        for _ in range(10):\r\n            client.produce(TOPIC, b'hello world')\r\n\r\n        client.flush(timeout=5)\r\n\r\n    def run_consumer_a(queue, lock):\r\n        lock.acquire()\r\n        has_lock = True\r\n\r\n        client = Consumer({\r\n            'bootstrap.servers': 'localhost:9092',\r\n            'group.id': 'vegetable',\r\n            'auto.offset.reset': 'earliest',\r\n            'enable.auto.commit': False\r\n            })\r\n        print('Consumer A subscribing')\r\n        client.subscribe([TOPIC])\r\n        print('Blocking consumer A on cluster connection')\r\n        client.list_topics(timeout=10)\r\n        print('Consumer A ready for consuming')\r\n\r\n        while True:\r\n            messages = client.consume(1, timeout=10)\r\n            print('Consumer A got ', messages)\r\n            if len(messages) == 0:\r\n                break\r\n\r\n            time.sleep(1)\r\n            message = messages[0]\r\n            partition = message.partition()\r\n            offset = message.offset()\r\n            print(partition, offset)\r\n\r\n            if has_lock:\r\n                # Implies that A has processed at least 1 message\r\n                has_lock = False\r\n                print('Releasing lock for B to start')\r\n                lock.release()\r\n                time.sleep(5)\r\n\r\n            client.commit(message)\r\n            queue.put(f'{str(partition)}-{str(offset)}')\r\n\r\n        print('Consumer A closing')\r\n\r\n    def run_consumer_b(queue, lock):\r\n        client = Consumer({\r\n            'bootstrap.servers': 'localhost:9092',\r\n            'group.id': 'vegetable',\r\n            'auto.offset.reset': 'earliest',\r\n            'enable.auto.commit': False\r\n            })\r\n        print('Consumer B subscribing')\r\n        client.subscribe([TOPIC])\r\n        print('Blocking consumer B on cluster connection')\r\n        client.list_topics(timeout=10)\r\n        print('Consumer B ready for consuming')\r\n\r\n        while True:\r\n            messages = client.consume(1, timeout=10)\r\n            print('Consumer B got ', messages)\r\n            if len(messages) == 0:\r\n                break\r\n\r\n            message = messages[0]\r\n            partition = message.partition()\r\n            offset = message.offset()\r\n            print(partition, offset)\r\n\r\n            client.commit(message)\r\n            queue.put(f'{str(partition)}-{str(offset)}')\r\n\r\n        print('Consumer B closing')\r\n\r\n    print('Add the messages to the log')\r\n    producer = Process(target=run_producer)\r\n    producer.start()\r\n    producer.join()\r\n\r\n    print('Initialize the consumers and start processing with A as the lead')\r\n    consumer_a = Process(target=run_consumer_a, args=(queue_a, lock))\r\n    consumer_b = Process(target=run_consumer_b, args=(queue_b, lock))\r\n    consumer_a.start()\r\n\r\n    print('Let consumer A work a little, then start consumer b, which will trigger a rebalance')\r\n    time.sleep(5)\r\n    lock.acquire()\r\n    consumer_b.start()\r\n    lock.release()\r\n\r\n    consumer_a.join()\r\n    consumer_b.join()\r\n\r\n    consumed_a = []\r\n    while not queue_a.empty():\r\n        consumed_a.append(queue_a.get())\r\n\r\n    consumed_b = []\r\n    while not queue_b.empty():\r\n        consumed_b.append(queue_b.get())\r\n\r\n    print(consumed_a, consumed_b)\r\n\r\nif __name__ == '__main__':\r\n    test_force_rebalance()\r\n```\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n```\r\n>>> import confluent_kafka\r\n>>> confluent_kafka.libversion()\r\n('1.2.0', 16908543)\r\n>>> confluent_kafka.version()\r\n('1.2.0', 16908288)\r\n```\r\n\r\n - [x] Apache Kafka broker version: 2.2.1\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: Mac OSX Mojave 10.14.5\r\n - [x] Provide client logs (with `'debug': '..'` as necessary)\r\n - [x] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/737", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/737/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/737/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/737/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/737", "id": 535621083, "node_id": "MDU6SXNzdWU1MzU2MjEwODM=", "number": 737, "title": "Receive message with size 0 (empty)", "user": {"login": "giangstrider", "id": 12697164, "node_id": "MDQ6VXNlcjEyNjk3MTY0", "avatar_url": "https://avatars2.githubusercontent.com/u/12697164?v=4", "gravatar_id": "", "url": "https://api.github.com/users/giangstrider", "html_url": "https://github.com/giangstrider", "followers_url": "https://api.github.com/users/giangstrider/followers", "following_url": "https://api.github.com/users/giangstrider/following{/other_user}", "gists_url": "https://api.github.com/users/giangstrider/gists{/gist_id}", "starred_url": "https://api.github.com/users/giangstrider/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/giangstrider/subscriptions", "organizations_url": "https://api.github.com/users/giangstrider/orgs", "repos_url": "https://api.github.com/users/giangstrider/repos", "events_url": "https://api.github.com/users/giangstrider/events{/privacy}", "received_events_url": "https://api.github.com/users/giangstrider/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-10T10:03:32Z", "updated_at": "2019-12-17T12:16:42Z", "closed_at": "2019-12-17T12:16:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI try to write Avro consumer for Confluent Cloud, the connection working fine, just issues getting empty size of message from topic.\r\n\r\nI not sure why is it.\r\nThanks for your time!\r\n\r\nHow to reproduce\r\n================\r\nRun the python code\r\n```python\r\nc = AvroConsumer({\r\n        'bootstrap.servers': BOOTSTRAP_BROKERS,\r\n        'client.id': 'giang_test',\r\n        'sasl.mechanisms': 'PLAIN',\r\n        'sasl.password': SASL_PASSWORD,\r\n        'sasl.username': SASL_USERNAME,\r\n        'security.protocol': 'SASL_SSL',\r\n        'schema.registry.url': SCHEMA_REGISTRY_URL,\r\n        'schema.registry.basic.auth.credentials.source': BASIC_AUTH_CREDENTIALS_SOURCE,\r\n        'schema.registry.basic.auth.user.info': BASIC_AUTH_USER_INFO,\r\n        # 'schema.registry.ssl.ca.location': SSL_CA_LOCATION,\r\n        'group.id': GROUP_ID,\r\n        'auto.offset.reset': 'earliest',\r\n        'debug': 'all',\r\n        'api.version.request': True\r\n    })\r\n\r\n    # Subscribe to topic\r\n    c.subscribe(TOPICS.split(','))\r\n\r\n    while True:\r\n        try:\r\n            msg = c.poll(10)\r\n        except SerializerError as e:\r\n            print(\"Message deserialization failed {}\".format(e))\r\n            continue\r\n        if msg is None:\r\n            continue\r\n        if msg.error():\r\n            print(\"AvroConsumer error: {}\".format(msg.error()))\r\n            continue\r\n        print('\\n\\nMsg key = ', msg.key(), '\\nMsg Value = ', msg.value())\r\n        print(msg.value())\r\n        # c.commit()\r\n    \r\n    c.close()\r\n```\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n - [ ] Operating system: Macos\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\nHere is my log \r\n```\r\n%7|1575971662.453|SEND|giang_test#consumer-1| [thrd:sasl_ssl://b4-xxx.aws.confluent.cloud:9092]: sasl_ssl://b4-xxx.aws.confluent.cloud:9092/4: Sent FetchRequest (v4, 96 bytes @ 0, CorrId 47)\r\n%7|1575971662.531|RECV|giang_test#consumer-1| [thrd:sasl_ssl://b5-xxx.aws.confluent.cloud:9092]: sasl_ssl://b5-xxx.aws.confluent.cloud:9092/5: Received FetchResponse (v4, 73 bytes, CorrId 47, rtt 124.13ms)\r\n%7|1575971662.531|FETCH|giang_test#consumer-1| [thrd:sasl_ssl://b5-xxx.aws.confluent.cloud:9092]: sasl_ssl://b5-xxx.aws.confluent.cloud:9092/5: Topic MY_TOPIC [1] MessageSet size 0, error \"Success\", MaxOffset 466421, LSO 466421, Ver 2/2\r\n%7|1575971662.531|FETCH|giang_test#consumer-1| [thrd:sasl_ssl://b5-xxx.aws.confluent.cloud:9092]: sasl_ssl://b5-xxx.aws.confluent.cloud:9092/5: Fetch topic MY_TOPIC [1] at offset 466421 (v2)\r\n%7|1575971662.531|FETCH|giang_test#consumer-1| [thrd:sasl_ssl://b5-xxx.aws.confluent.cloud:9092]: sasl_ssl://b5-xxx.aws.confluent.cloud:9092/5: Fetch 1/1/1 toppar(s)\r\n%7|1575971662.532|SEND|giang_test#consumer-1| [thrd:sasl_ssl://b5-xxx.aws.confluent.cloud:9092]: sasl_ssl://b5-xxx.aws.confluent.cloud:9092/5: Sent FetchRequest (v4, 96 bytes @ 0, CorrId 48)\r\n%7|1575971662.532|RECV|giang_test#consumer-1| [thrd:sasl_ssl://b0-xxx.aws.confluent.cloud:9092]: sasl_ssl://b0-xxx.aws.confluent.cloud:9092/0: Received FetchResponse (v4, 73 bytes, CorrId 47, rtt 124.51ms)\r\n%7|1575971662.532|FETCH|giang_test#consumer-1| [thrd:sasl_ssl://b0-xxx.aws.confluent.cloud:9092]: sasl_ssl://b0-xxx.aws.confluent.cloud:9092/0: Topic MY_TOPIC [2] MessageSet size 0, error \"Success\", MaxOffset 466716, LSO 466716, Ver 2/2\r\n%7|1575971662.532|FETCH|giang_test#consumer-1| [thrd:sasl_ssl://b0-xxx.aws.confluent.cloud:9092]: sasl_ssl://b0-xxx.aws.confluent.cloud:9092/0: Fetch topic MY_TOPIC [2] at offset 466716 (v2)\r\n%7|1575971662.532|FETCH|giang_test#consumer-1| [thrd:sasl_ssl://b0-xxx.aws.confluent.cloud:9092]: sasl_ssl://b0-xxx.aws.confluent.cloud:9092/0: Fetch 1/1/1 toppar(s)\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/732", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/732/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/732/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/732/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/732", "id": 532580853, "node_id": "MDU6SXNzdWU1MzI1ODA4NTM=", "number": 732, "title": "ApiVersionRequest fail.", "user": {"login": "tuscior", "id": 22934957, "node_id": "MDQ6VXNlcjIyOTM0OTU3", "avatar_url": "https://avatars3.githubusercontent.com/u/22934957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tuscior", "html_url": "https://github.com/tuscior", "followers_url": "https://api.github.com/users/tuscior/followers", "following_url": "https://api.github.com/users/tuscior/following{/other_user}", "gists_url": "https://api.github.com/users/tuscior/gists{/gist_id}", "starred_url": "https://api.github.com/users/tuscior/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tuscior/subscriptions", "organizations_url": "https://api.github.com/users/tuscior/orgs", "repos_url": "https://api.github.com/users/tuscior/repos", "events_url": "https://api.github.com/users/tuscior/events{/privacy}", "received_events_url": "https://api.github.com/users/tuscior/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-12-04T10:14:11Z", "updated_at": "2019-12-05T11:02:49Z", "closed_at": "2019-12-05T11:02:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI tried produce very basic message but I can't do it. In the logs I see that broker may be old. The thing is I am using Confluent Cloud, Confluent Kafka version is '1.2.0'. I can't see how I can check Kafka broker version trough CLI. I tried multiple options with SASL mechanism and security protocol(plaintext, ssl, sasl_plaintext, sasl_ssl). I tried to add fallback to all 4 versions available in the https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md : 0.9.0, 0.8.2, 0.8.1, 0.8.0. I tried disable API versioning etc. No results. I have successfully been able to work with that cloud trough node.js env without any issue but I need it in python as well.\r\n\r\nConfig:\r\n` 'bootstrap.servers': 'pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092',\r\n 'sasl.username': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',\r\n'sasl.password': 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',`\r\n\r\n`%7|1575453679.230|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Received CONNECT op\r\n%7|1575453679.230|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1575453679.230|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1575453679.230|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1575453679.346|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connecting to ipv4#35.183.154.152:9092 (plaintext) with socket 15\r\n%7|1575453679.478|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected to ipv4#35.183.154.152:9092\r\n%7|1575453679.478|CONNECTED|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected (#1)\r\n%7|1575453679.478|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features +ApiVersion to ApiVersion\r\n%7|1575453679.478|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1575453679.478|SEND|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Sent ApiVersionRequest (v0, 25 bytes @ 0, CorrId 1)\r\n%7|1575453679.612|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1575453679.612|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1575453679.612|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1575453679.612|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1575453679.612|FAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1575453680.232|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: no cluster connection\r\n%7|1575453680.617|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state DOWN -> INIT\r\n%7|1575453681.237|CONNECT|rdkafka#producer-1| [thrd:main]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Selected for cluster connection: no cluster connection (broker has 1 connection attempt(s))\r\n%7|1575453681.237|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Received CONNECT op\r\n%7|1575453681.237|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1575453681.237|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1575453681.237|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1575453681.239|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connecting to ipv4#35.183.154.152:9092 (plaintext) with socket 15\r\nProducing message\r\n%7|1575453681.363|TOPIC|rdkafka#producer-1| [thrd:app]: New local topic: odoo.test.topic\r\n%7|1575453681.363|TOPPARNEW|rdkafka#producer-1| [thrd:app]: NEW odoo.test.topic [-1] 0x7fbd90603640 (at rd_kafka_topic_new0:393)\r\n%7|1575453681.369|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected to ipv4#35.183.154.152:9092\r\n%7|1575453681.369|CONNECTED|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected (#2)\r\n%7|1575453681.369|APIVERSION|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Using (configuration fallback) 0.10.0 protocol features\r\n%7|1575453681.369|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features to ApiVersion\r\n%7|1575453681.369|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1575453681.370|SEND|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Sent ApiVersionRequest (v0, 25 bytes @ 0, CorrId 2)\r\n%7|1575453681.640|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Resource temporarily unavailable)\r\n%7|1575453681.640|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1575453681.640|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1575453681.640|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Resource temporarily unavailable)\r\n%7|1575453681.640|FAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1575453681.640|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Cluster connection already in progress: broker down\r\n%7|1575453681.640|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Not selecting any broker for cluster connection: still suppressed for 49ms: broker down\r\n%7|1575453681.640|METADATA|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453681.640|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state DOWN -> INIT\r\n%7|1575453682.239|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic metadata information unknown\r\n%7|1575453682.239|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic partition count is zero: should refresh metadata\r\n%7|1575453682.239|CONNECT|rdkafka#producer-1| [thrd:main]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Selected for cluster connection: refresh unavailable topics (broker has 2 connection attempt(s))\r\n%7|1575453682.239|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1575453682.239|METADATA|rdkafka#producer-1| [thrd:main]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453682.239|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1575453682.239|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Received CONNECT op\r\n%7|1575453682.239|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1575453682.239|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1575453682.239|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1575453682.241|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connecting to ipv4#35.182.89.141:9092 (plaintext) with socket 15\r\n%7|1575453682.371|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected to ipv4#35.182.89.141:9092\r\n%7|1575453682.371|CONNECTED|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected (#3)\r\n%7|1575453682.371|APIVERSION|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Using (configuration fallback) 0.10.0 protocol features\r\n%7|1575453682.371|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features to ApiVersion\r\n%7|1575453682.371|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1575453682.372|SEND|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Sent ApiVersionRequest (v0, 25 bytes @ 0, CorrId 3)\r\n%7|1575453682.506|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Resource temporarily unavailable)\r\n%7|1575453682.507|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1575453682.507|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1575453682.507|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Resource temporarily unavailable)\r\n%7|1575453682.507|FAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1575453682.507|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Cluster connection already in progress: broker down\r\n%7|1575453682.507|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Not selecting any broker for cluster connection: still suppressed for 49ms: broker down\r\n%7|1575453682.507|METADATA|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453683.244|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic metadata information unknown\r\n%7|1575453683.244|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic partition count is zero: should refresh metadata\r\n%7|1575453683.244|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: refresh unavailable topics\r\n%7|1575453683.244|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1575453683.244|METADATA|rdkafka#producer-1| [thrd:main]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453683.244|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1575453683.508|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state DOWN -> INIT\r\n%7|1575453684.249|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic metadata information unknown\r\n%7|1575453684.249|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic partition count is zero: should refresh metadata\r\n%7|1575453684.249|CONNECT|rdkafka#producer-1| [thrd:main]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Selected for cluster connection: refresh unavailable topics (broker has 3 connection attempt(s))\r\n%7|1575453684.249|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1575453684.249|METADATA|rdkafka#producer-1| [thrd:main]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453684.249|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1575453684.249|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Received CONNECT op\r\n%7|1575453684.249|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1575453684.249|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1575453684.249|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1575453684.250|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connecting to ipv4#35.183.154.152:9092 (plaintext) with socket 15\r\n%7|1575453684.390|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected to ipv4#35.183.154.152:9092\r\n%7|1575453684.390|CONNECTED|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected (#4)\r\n%7|1575453684.390|APIVERSION|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Using (configuration fallback) 0.10.0 protocol features\r\n%7|1575453684.390|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features to ApiVersion\r\n%7|1575453684.390|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1575453684.390|SEND|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Sent ApiVersionRequest (v0, 25 bytes @ 0, CorrId 4)\r\n%7|1575453684.532|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1575453684.532|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1575453684.532|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1575453684.532|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Connection reset by peer)\r\n%7|1575453684.532|FAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1575453684.533|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Cluster connection already in progress: broker down\r\n%7|1575453684.533|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Not selecting any broker for cluster connection: still suppressed for 49ms: broker down\r\n%7|1575453684.533|METADATA|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453684.533|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state DOWN -> INIT\r\n%7|1575453685.252|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic metadata information unknown\r\n%7|1575453685.252|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic partition count is zero: should refresh metadata\r\n%7|1575453685.252|CONNECT|rdkafka#producer-1| [thrd:main]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Selected for cluster connection: refresh unavailable topics (broker has 4 connection attempt(s))\r\n%7|1575453685.252|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1575453685.252|METADATA|rdkafka#producer-1| [thrd:main]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453685.252|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1575453685.252|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Received CONNECT op\r\n%7|1575453685.252|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1575453685.252|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1575453685.252|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1575453685.253|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connecting to ipv4#35.182.89.141:9092 (plaintext) with socket 15\r\n%7|1575453685.383|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected to ipv4#35.182.89.141:9092\r\n%7|1575453685.383|CONNECTED|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected (#5)\r\n%7|1575453685.383|APIVERSION|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Using (configuration fallback) 0.10.0 protocol features\r\n%7|1575453685.383|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features to ApiVersion\r\n%7|1575453685.383|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1575453685.383|SEND|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Sent ApiVersionRequest (v0, 25 bytes @ 0, CorrId 5)\r\n%7|1575453685.517|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Resource temporarily unavailable)\r\n%7|1575453685.517|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1575453685.517|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN\r\n%7|1575453685.517|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Resource temporarily unavailable)\r\n%7|1575453685.517|FAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: ApiVersionRequest failed: Local: Broker transport failure: probably due to old broker version (after 0ms in state DOWN)\r\n%7|1575453685.517|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Cluster connection already in progress: broker down\r\n%7|1575453685.517|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Not selecting any broker for cluster connection: still suppressed for 49ms: broker down\r\n%7|1575453685.517|METADATA|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453686.255|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic metadata information unknown\r\n%7|1575453686.255|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic partition count is zero: should refresh metadata\r\n%7|1575453686.255|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: refresh unavailable topics\r\n%7|1575453686.255|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1575453686.255|METADATA|rdkafka#producer-1| [thrd:main]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453686.255|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1575453686.518|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state DOWN -> INIT\r\n%7|1575453687.255|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic metadata information unknown\r\n%7|1575453687.255|NOINFO|rdkafka#producer-1| [thrd:main]: Topic odoo.test.topic partition count is zero: should refresh metadata\r\n%7|1575453687.255|CONNECT|rdkafka#producer-1| [thrd:main]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Selected for cluster connection: refresh unavailable topics (broker has 5 connection attempt(s))\r\n%7|1575453687.255|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1575453687.255|METADATA|rdkafka#producer-1| [thrd:main]: Skipping metadata refresh of 1 topic(s): no usable brokers\r\n%7|1575453687.255|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1575453687.255|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Received CONNECT op\r\n%7|1575453687.255|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1575453687.255|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1575453687.255|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1575453687.257|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connecting to ipv4#35.182.89.141:9092 (plaintext) with socket 15\r\n%7|1575453687.386|CONNECT|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected to ipv4#35.182.89.141:9092\r\n%7|1575453687.386|CONNECTED|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Connected (#6)\r\n%7|1575453687.386|APIVERSION|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Using (configuration fallback) 0.10.0 protocol features\r\n%7|1575453687.386|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features to ApiVersion\r\n%7|1575453687.386|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state CONNECT -> APIVERSION_QUERY\r\n%7|1575453687.386|SEND|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Sent ApiVersionRequest (v0, 25 bytes @ 0, CorrId 6)\r\n%7|1575453687.531|BROKERFAIL|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker transport failure: (errno: Resource temporarily unavailable)\r\n%7|1575453687.531|FEATURE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Updated enabled protocol features -ApiVersion to \r\n%7|1575453687.531|STATE|rdkafka#producer-1| [thrd:pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap]: pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state APIVERSION_QUERY -> DOWN`\r\n\r\n\r\nHow to reproduce\r\n================\r\nSince its very basic producer setup\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): '1.2.0'\r\n - [ ] Apache Kafka broker version: I don't see any option to check broker version trough Confluent CLI\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: MacOS Mojavie 10.14.6\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/731", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/731/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/731/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/731/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/731", "id": 531668366, "node_id": "MDU6SXNzdWU1MzE2NjgzNjY=", "number": 731, "title": "Kafka python3 client invoke unassign does not work", "user": {"login": "yelijun", "id": 3347175, "node_id": "MDQ6VXNlcjMzNDcxNzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/3347175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yelijun", "html_url": "https://github.com/yelijun", "followers_url": "https://api.github.com/users/yelijun/followers", "following_url": "https://api.github.com/users/yelijun/following{/other_user}", "gists_url": "https://api.github.com/users/yelijun/gists{/gist_id}", "starred_url": "https://api.github.com/users/yelijun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yelijun/subscriptions", "organizations_url": "https://api.github.com/users/yelijun/orgs", "repos_url": "https://api.github.com/users/yelijun/repos", "events_url": "https://api.github.com/users/yelijun/events{/privacy}", "received_events_url": "https://api.github.com/users/yelijun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-12-03T02:25:53Z", "updated_at": "2019-12-03T03:35:34Z", "closed_at": "2019-12-03T03:35:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI have used the confluent kafka client, and i assign three topics with each partition and offset,\r\nduring the consume I invoke the unassign method with one partition, but it does not work\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n1. Create a kafka consumer\r\n2. Assign with different topics and partition and offset\r\n3. Consume for 10 seconds and invoke unassign method, such as consumer.unassign(partition)\r\n4. Then invoke assignment() still find the topic and partition exist\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): confluent-kafka (1.2.0)\r\n - [ ] Apache Kafka broker version: 1.1.0\r\n - [ ] Client configuration: `{'bootstrap.servers':'''group.id':'','client.id':'','default.topic.config':{'auto.offset.reset':'earliest'},'schema.registry.url':'',}`\r\n - [ ] Operating system: Centos 7 / Mac os\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/728", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/728/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/728/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/728/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/728", "id": 529905907, "node_id": "MDU6SXNzdWU1Mjk5MDU5MDc=", "number": 728, "title": "The Avro Producer is enable to product anything and fails with a timeout error.", "user": {"login": "odooadvantage", "id": 35006183, "node_id": "MDQ6VXNlcjM1MDA2MTgz", "avatar_url": "https://avatars0.githubusercontent.com/u/35006183?v=4", "gravatar_id": "", "url": "https://api.github.com/users/odooadvantage", "html_url": "https://github.com/odooadvantage", "followers_url": "https://api.github.com/users/odooadvantage/followers", "following_url": "https://api.github.com/users/odooadvantage/following{/other_user}", "gists_url": "https://api.github.com/users/odooadvantage/gists{/gist_id}", "starred_url": "https://api.github.com/users/odooadvantage/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/odooadvantage/subscriptions", "organizations_url": "https://api.github.com/users/odooadvantage/orgs", "repos_url": "https://api.github.com/users/odooadvantage/repos", "events_url": "https://api.github.com/users/odooadvantage/events{/privacy}", "received_events_url": "https://api.github.com/users/odooadvantage/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-28T13:14:15Z", "updated_at": "2019-11-29T11:41:08Z", "closed_at": "2019-11-29T11:41:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI'm trying to connect to the confluent cloud using this python library but we are unable to send any messages. It fails with a message timeout error. Can anyone please help with this? \r\nThanks!\r\n\r\nParameters and code\r\n==============\r\nWe are setting the following config parameters.\r\n`conf = {\r\n            'bootstrap.servers': config.bootstrap_server,\r\n            'sasl.username': config.api_key,\r\n            'sasl.password': config.api_secret,\r\n            'schema.registry.url': config.schema_registry_url,\r\n            'security.protocol': 'SASL_PLAINTEXT',\r\n            'debug': 'broker,queue,msg,protocol,security',\r\n            'schema.registry.basic.auth.credentials.source': 'USER_INFO',\r\n            'schema.registry.basic.auth.user.info':  config.basic_auth\r\n        }`\r\n\r\nand posting the msgs like this:\r\n`producer.produce(topic=topic, value={\"InventoryLevel\": value}, key={\"SKU\": key}, callback=acked)`\r\n\r\nLogs\r\n====\r\nHere are some logs about it.\r\n4946332.677|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1574946332.678|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1574946333.677|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: refresh unavailable topics\r\n%7|1574946333.677|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1574946333.677|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1574946334.678|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: refresh unavailable topics\r\n%7|1574946334.678|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1574946334.678|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1574946335.679|TIMEOUT|rdkafka#producer-1| [thrd:main]: odoo.products.inventory: 80 message(s) timed out\r\n%7|1574946335.679|CONNECT|rdkafka#producer-1| [thrd:main]: Cluster connection already in progress: refresh unavailable topics\r\n%7|1574946335.679|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: refresh unavailable topics\r\n%7|1574946335.679|CONNECT|rdkafka#producer-1| [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 49ms: no cluster connection\r\n%7|1574946335.693|DESTROY|rdkafka#producer-1| [thrd:app]: Terminating instance (destroy flags none (0x0))\r\n%7|1574946335.694|DESTROY|rdkafka#producer-1| [thrd:main]: Destroy internal\r\n%7|1574946335.694|DESTROY|rdkafka#producer-1| [thrd:main]: Removing all topics\r\n%7|1574946335.694|DESTROY|rdkafka#producer-1| [thrd:main]: Sending TERMINATE to sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap\r\n%7|1574946335.694|TERM|rdkafka#producer-1| [thrd:sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:909]: sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Received TERMINATE op in state TRY_CONNECT: 1 refcnts, 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs\r\n%7|1574946335.694|BROKERFAIL|rdkafka#producer-1| [thrd:sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:909]: sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: failed: err: Local: Broker handle destroyed: (errno: No error)\r\n%7|1574946335.694|FAIL|rdkafka#producer-1| [thrd:sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:909]: sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Client is terminating (after 7017ms in state TRY_CONNECT)\r\n%7|1574946335.694|STATE|rdkafka#producer-1| [thrd:sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:909]: sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> DOWN\r\n%7|1574946335.694|TERM|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Received TERMINATE op in state INIT: 1 refcnts, 0 toppar(s), 0 active toppar(s), 0 outbufs, 0 waitresps, 0 retrybufs\r\n%7|1574946335.694|BUFQ|rdkafka#producer-1| [thrd:sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:909]: sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Purging bufq with 0 buffers\r\n%7|1574946335.694|BROKERFAIL|rdkafka#producer-1| [thrd::0/internal]: :0/internal: failed: err: Local: Broker handle destroyed: (errno: No error)\r\n%7|1574946335.694|BUFQ|rdkafka#producer-1| [thrd:sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:909]: sasl_plaintext://pkc-4v2jl.ca-central-1.aws.confluent.cloud:9092/bootstrap: Purging bufq with 0 buffers\r\n\r\n\r\nSome Extra Information\r\n===============\r\nconfluent-kafka-python version: ('1.2.0', 16908288)\r\nlibrdkafka version: ('1.2.0', 16908543)\r\nOperating system: Windows\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/722", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/722/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/722/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/722/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/722", "id": 526728784, "node_id": "MDU6SXNzdWU1MjY3Mjg3ODQ=", "number": 722, "title": "SSL handshake error gets swallowed", "user": {"login": "naartjie", "id": 514563, "node_id": "MDQ6VXNlcjUxNDU2Mw==", "avatar_url": "https://avatars0.githubusercontent.com/u/514563?v=4", "gravatar_id": "", "url": "https://api.github.com/users/naartjie", "html_url": "https://github.com/naartjie", "followers_url": "https://api.github.com/users/naartjie/followers", "following_url": "https://api.github.com/users/naartjie/following{/other_user}", "gists_url": "https://api.github.com/users/naartjie/gists{/gist_id}", "starred_url": "https://api.github.com/users/naartjie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/naartjie/subscriptions", "organizations_url": "https://api.github.com/users/naartjie/orgs", "repos_url": "https://api.github.com/users/naartjie/repos", "events_url": "https://api.github.com/users/naartjie/events{/privacy}", "received_events_url": "https://api.github.com/users/naartjie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-11-21T17:09:48Z", "updated_at": "2019-12-17T12:25:06Z", "closed_at": "2019-12-17T12:25:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nThis issue might be with the way librdkafka does the error propagation, and the issue might need to be logged there. I am logging it here, because I encountered it while using this library.\r\n\r\nIt seems when there is an error in the SSL handshake connecting to the broker, the client code doesn't get an error, and looks like it's silently swallowed. You can see it in the logs by setting debug in config:\r\n\r\n```python\r\n 'debug' : 'security,broker'\r\n```\r\nThe logs show something like this:\r\n\r\n```\r\nsasl_ssl://xyz.gcp.confluent.cloud:9092/bootstrap: failed: err: Local: SSL error: (errno: Undefined error: 0)\r\n```\r\n\r\n(Here is another issue which appears to have the same problem: #697)\r\n\r\nHow to reproduce\r\n================\r\n\r\nSet the SSL CA manually, to one which does not match the CA of the broker. I used [this one](https://www.geotrust.com/resources/root_certificates/certificates/GeoTrust_Global_CA2.pem) to test, since it doesn't match Confluent Cloud's one, it gets us a repro:\r\n\r\n```python\r\n  'ssl.ca.location': '/dir/invalid-ca.pem',\r\n```\r\n\r\nThe error won't be reported in client code - the client just sits there waiting, but it will be visible in librdkafka debug logs.\r\n\r\nChecklist\r\n=========\r\n\r\n - confluent-kafka-python: 1.2.0\r\n - librdkafka version: 1.2.0\r\n - Apache Kafka broker version: Confluent Cloud\r\n - Client configuration:\r\n\r\n```python\r\n{\r\n    'bootstrap.servers': 'xyz.gcp.confluent.cloud:9092',\r\n    'sasl.mechanisms': 'PLAIN',\r\n    'security.protocol': 'SASL_SSL',\r\n    'sasl.username': '${USERNAME}',\r\n    'sasl.password': '${PASSWORD}'\r\n}\r\n```\r\n - Operating system: OSX 10.14.6\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/721", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/721/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/721/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/721/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/721", "id": 525809855, "node_id": "MDU6SXNzdWU1MjU4MDk4NTU=", "number": 721, "title": "No route to host failure", "user": {"login": "joukosusi", "id": 20793539, "node_id": "MDQ6VXNlcjIwNzkzNTM5", "avatar_url": "https://avatars2.githubusercontent.com/u/20793539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joukosusi", "html_url": "https://github.com/joukosusi", "followers_url": "https://api.github.com/users/joukosusi/followers", "following_url": "https://api.github.com/users/joukosusi/following{/other_user}", "gists_url": "https://api.github.com/users/joukosusi/gists{/gist_id}", "starred_url": "https://api.github.com/users/joukosusi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joukosusi/subscriptions", "organizations_url": "https://api.github.com/users/joukosusi/orgs", "repos_url": "https://api.github.com/users/joukosusi/repos", "events_url": "https://api.github.com/users/joukosusi/events{/privacy}", "received_events_url": "https://api.github.com/users/joukosusi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-20T13:39:23Z", "updated_at": "2019-11-20T15:09:26Z", "closed_at": "2019-11-20T15:09:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\ni see a  \"No route to host failure\" when use a nginx to forward connections to kafka. Could u help me on this?\r\n\r\n...\r\n%7|1574255921.562|REQERR|rdkafka#consumer-1| [thrd:main]: GroupCoordinator/0: HeartbeatRequest failed: Local: Broker transport failure: actions Retry\r\n%7|1574255921.562|RETRY|rdkafka#consumer-1| [thrd:GroupCoordinator]: GroupCoordinator/0: Retrying HeartbeatRequest (v0, 95 bytes, retry 1/2, prev CorrId 0) in 100ms\r\n[2019-11-20 21:18:41,562][36115][140412040099648-MainThread][ERROR]<kafka.py:737> _TRANSPORT(-195) >> GroupCoordinator: Connect to ipv4#20.123.4.96:9092 failed: No route to host (after 7026ms in state CONNECT)\r\n%7|1574255922.020|CGRPQUERY|rdkafka#consumer-1| [thrd:main]: Group \"RAW_DATA_ROUTER_SYSTEM\": no broker available for coordinator query: intervaled in state query-coord\r\n...\r\n\r\n\r\nkafka hosts config: kafka-sasl-server 20.123.4.98\r\nnginx hosts config: kafka-sasl-server 20.123.4.96\r\nconsumer client hosts config:  kafka-sasl-server 20.123.4.96\r\n\r\nnginx config:\r\n...\r\nstream {\r\n    log_format proxy '$remote_addr [$time_local] '\r\n                 '$protocol $status $bytes_sent $bytes_received '\r\n                 '$session_time \"$upstream_addr\" '\r\n                 '\"$upstream_bytes_sent\" \"$upstream_bytes_received\" \"$upstream_connect_time\"';\r\n    access_log  logs/tcp-access.log proxy;\r\n\r\n    upstream ssl{\r\n        hash $remote_addr consistent;\r\n        server 20.123.4.98:9092 max_fails=3 fail_timeout=10s;\r\n    }\r\n    server {\r\n        listen 9092;\r\n        proxy_timeout 20s;\r\n        proxy_pass ssl;\r\n    }\r\n}\r\n...\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n    import functools\r\n    logger=log.LogHandler(\"test_kafka\", path=\".\", log_level=\"DEBUG\")\r\n    config = {\r\n        'sasl.mechanism': 'SCRAM-SHA-256',\r\n        'sasl.username': 'test',\r\n        'sasl.password': '123',\r\n        'ssl.key.location': '/root/ssl/client.key.unsecure',\r\n        'ssl.ca.location': '/root/ssl/ca-cert',\r\n        'ssl.certificate.location': '/root/ssl/client.crt',\r\n        'ssl.key.password': 123456,\r\n        'group.id':'RAW_DATA_ROUTER_SYSTEM',\r\n        'bootstrap.servers': \"kafka-sasl-server:9092\",\r\n        'security.protocol': \"SASL_SSL\",\r\n        'api.version.request': True,\r\n        'broker.version.fallback': \"1.1.1\",\r\n        'log.connection.close': False,\r\n        'heartbeat.interval.ms': 10000,\r\n        'log_level': 6,\r\n        'message.max.bytes': 314572800,\r\n        'statistics.interval.ms': 0,\r\n        'session.timeout.ms': 60000,\r\n        'debug': 'all',\r\n        'enable.auto.commit': True,\r\n        'auto.commit.interval.ms': 1000,\r\n        'fetch.min.bytes': 1024 * 1024,\r\n        'fetch.wait.max.ms': 1000,\r\n        'fetch.message.max.bytes': 1048576,\r\n        'error_cb': functools.partial(_error_cb, logger),\r\n        'default.topic.config': {\r\n            'auto.offset.reset': 'earliest',\r\n        },\r\n    }\r\n    _consumer = Consumer(**config)\r\n    _consumer.subscribe([\"TOPIC_ANALYSIS_OFFLINE_JOB_RESULT_PIPE\"])\r\n    while True:\r\n        result = _consumer.poll(timeout=0.00001)\r\n        if not result:\r\n            continue\r\n        if result.error():\r\n            if result.error().code() == KafkaError._PARTITION_EOF:\r\n                logger.warning('{0} [{1}] reached end at offset {2}'.format(\r\n                    result.topic(), result.partition(), result.offset()\r\n                ))\r\n            else:\r\n                logger.error('encourage error:\\n{}'.format(json.dumps(\r\n                    {\r\n                        'name': result.error().name(),\r\n                        'code': result.error().code(),\r\n                        'description': result.error().str()\r\n                    }, indent=1)))\r\n        else:\r\n             print(result)\r\n\r\n\r\nlogs\r\n[kafka.log](https://github.com/confluentinc/confluent-kafka-python/files/3869468/kafka.log)\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [\u2714 ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): 1.2.0 1.2.0\r\n - [\u2714 ] Apache Kafka broker version: 1.1.1\r\n - [ \u2714] Client configuration: `{...}`\r\n - [ \u2714] Operating system: centos7\r\n - [ \u2714] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/720", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/720/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/720/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/720/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/720", "id": 525359565, "node_id": "MDU6SXNzdWU1MjUzNTk1NjU=", "number": 720, "title": "produce() right after assign() makes poll unable to get messages", "user": {"login": "robooo", "id": 7584540, "node_id": "MDQ6VXNlcjc1ODQ1NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7584540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robooo", "html_url": "https://github.com/robooo", "followers_url": "https://api.github.com/users/robooo/followers", "following_url": "https://api.github.com/users/robooo/following{/other_user}", "gists_url": "https://api.github.com/users/robooo/gists{/gist_id}", "starred_url": "https://api.github.com/users/robooo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robooo/subscriptions", "organizations_url": "https://api.github.com/users/robooo/orgs", "repos_url": "https://api.github.com/users/robooo/repos", "events_url": "https://api.github.com/users/robooo/events{/privacy}", "received_events_url": "https://api.github.com/users/robooo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-11-20T01:13:46Z", "updated_at": "2020-04-05T10:47:24Z", "closed_at": "2020-04-05T10:47:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "How to reproduce\r\n================\r\nCreate producer + create consumer as described below, assign the consumer to the topic partition with OFFSET_END right after this produce some message (use flush to wait till it will be delivered) to topic and then try to poll the message. \r\n\r\n-> empty list is returned\r\n-> when I use 5 second sleep right after the assign() the message is polled without any issue\r\nin documentation there is after the assignment \"starts consuming.\", so is that consuming start interrupted with the produce call? or the assignment was not complete? is there some other way how to fix this instead of sleep?\r\n\r\nThank you!\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):('1.2.0', 16908288), ('1.2.0', 16908543)\r\n - [ ] Client configuration: `{'enable.auto.commit': True,\r\n                'default.topic.config': {\r\n                    'auto.offset.reset': 'latest'\r\n                },}`\r\n - [ ] Operating system: Ubuntu 18\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/719", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/719/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/719/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/719/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/719", "id": 525358723, "node_id": "MDU6SXNzdWU1MjUzNTg3MjM=", "number": 719, "title": "Questions: No messages with consumer's poll() when using 'auto.offset.reset': 'latest' ", "user": {"login": "robooo", "id": 7584540, "node_id": "MDQ6VXNlcjc1ODQ1NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7584540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robooo", "html_url": "https://github.com/robooo", "followers_url": "https://api.github.com/users/robooo/followers", "following_url": "https://api.github.com/users/robooo/following{/other_user}", "gists_url": "https://api.github.com/users/robooo/gists{/gist_id}", "starred_url": "https://api.github.com/users/robooo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robooo/subscriptions", "organizations_url": "https://api.github.com/users/robooo/orgs", "repos_url": "https://api.github.com/users/robooo/repos", "events_url": "https://api.github.com/users/robooo/events{/privacy}", "received_events_url": "https://api.github.com/users/robooo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-20T01:10:55Z", "updated_at": "2019-11-21T18:10:29Z", "closed_at": "2019-11-21T08:03:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nI ran into this issue during the usage of one consumer with  `'auto.offset.reset': 'earliest'` and another one with `'auto.offset.reset': 'latest'`. I use it like:\r\n```\r\ninit consumer + producer\r\nsubscribe to topic\r\nproduce data\r\nflush\r\npoll messages\r\n```\r\nThe `'auto.offset.reset': 'earliest'` returns all messages + the new messages which were delivered after the subscription but the  `'auto.offset.reset': 'latest'` will return only `None`'s\r\nEven in the examples you are always using `'auto.offset.reset': 'earliest'`.\r\n\r\n**1. Shouldn't be then the earliest default?**\r\n\r\nMaybe it's related to https://github.com/confluentinc/confluent-kafka-python/issues/373\r\nbecause I found that when I do \"dummy poll\" before produce() then I'm able to get messages with `'auto.offset.reset': 'latest'` poll(). (so that dummy poll + next produced message will set correct offset for the consumer with `'auto.offset.reset': 'latest'`?)\r\n**2. If the logic is to run consumer+subscribe+poll in cycle, in thread and the producer in another one, could you make clear example of this case? with some description why this won't work sequentially**\r\n\r\nNote: I know I can use assign() to OFFSET_END, but I wanted to make the usability same and user will just change the `earliest` -> `latest` and in `earliest` scenario he gets all the data + new produced after subscription and in the `latest` only produced after subscription.\r\n\r\nI will appreciate any help because I can't find the correct answer to my questions.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):('1.2.0', 16908288), ('1.2.0', 16908543)\r\n - [ ] Client configuration: `{'enable.auto.commit': True,\r\n                'default.topic.config': {\r\n                    'auto.offset.reset': 'latest'\r\n                },}`\r\n - [ ] Operating system: Ubuntu 18\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/717", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/717/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/717/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/717/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/717", "id": 523297160, "node_id": "MDU6SXNzdWU1MjMyOTcxNjA=", "number": 717, "title": "Recommendations to achieve behavior of max.poll.records in python", "user": {"login": "bpradeep20", "id": 3871141, "node_id": "MDQ6VXNlcjM4NzExNDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3871141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bpradeep20", "html_url": "https://github.com/bpradeep20", "followers_url": "https://api.github.com/users/bpradeep20/followers", "following_url": "https://api.github.com/users/bpradeep20/following{/other_user}", "gists_url": "https://api.github.com/users/bpradeep20/gists{/gist_id}", "starred_url": "https://api.github.com/users/bpradeep20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bpradeep20/subscriptions", "organizations_url": "https://api.github.com/users/bpradeep20/orgs", "repos_url": "https://api.github.com/users/bpradeep20/repos", "events_url": "https://api.github.com/users/bpradeep20/events{/privacy}", "received_events_url": "https://api.github.com/users/bpradeep20/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-15T07:12:34Z", "updated_at": "2019-11-21T08:05:22Z", "closed_at": "2019-11-21T08:05:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nIs there any recommendation to achieve behavior of 'max.poll.records' property of Java consumer in python using consumer.poll()?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/715", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/715/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/715/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/715/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/715", "id": 519095541, "node_id": "MDU6SXNzdWU1MTkwOTU1NDE=", "number": 715, "title": "No difference in producer performance with different 'acks' setting", "user": {"login": "vikramindian", "id": 22010146, "node_id": "MDQ6VXNlcjIyMDEwMTQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/22010146?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vikramindian", "html_url": "https://github.com/vikramindian", "followers_url": "https://api.github.com/users/vikramindian/followers", "following_url": "https://api.github.com/users/vikramindian/following{/other_user}", "gists_url": "https://api.github.com/users/vikramindian/gists{/gist_id}", "starred_url": "https://api.github.com/users/vikramindian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vikramindian/subscriptions", "organizations_url": "https://api.github.com/users/vikramindian/orgs", "repos_url": "https://api.github.com/users/vikramindian/repos", "events_url": "https://api.github.com/users/vikramindian/events{/privacy}", "received_events_url": "https://api.github.com/users/vikramindian/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-07T07:40:28Z", "updated_at": "2019-11-21T08:05:39Z", "closed_at": "2019-11-21T08:05:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nI made some changes to the confluent kafka python producer example application and ran it for doing some performance tests.\r\nI sent messages to a topic (partition = 1, replication-factor = 3) continuously in a loop by setting acks to 1 initially and then to -1.\r\n\r\nTest Details:\r\n- message size = 100 bytes\r\n- 5 node cluster\r\n\r\n\r\nObservations:\r\n1. There is no difference in throughput (msgs/sec) numbers in both the tests (i.e with acks = 1 and acks = -1 and keeping all other configs same)\r\n2. Even on a different topic with higher partitions (like with topic of partitions = 5, replication-factor = 3), the throughput did not change. In all three cases it stood around 80,000 msgs/sec.\r\n\r\nQuestion:\r\n\r\n1. Why is this happening? This was not the case with Java API, where acks = 1 setting had higher throughput that acks = -1.\r\n \r\n\r\n\r\nHow to reproduce\r\n================\r\nRun the below code. I made changes to []( https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/producer.py) and added command line options to accept configs.\r\n\r\n`\r\nimport argparse\r\nimport logging\r\nfrom   threading                import Lock, Timer\r\nfrom   confluent_kafka          import Producer\r\n\r\nimport os\r\nimport random\r\nimport signal\r\nimport time\r\nimport uuid\r\nimport sys\r\n\r\nTEST_DATA      = \"TestData\"\r\n\r\n\r\ntotal_msgs_sent = 0\r\ninterval_msgs_sent = 0\r\nlock = Lock()\r\n\r\n\r\ndef get_test_msg_len(msg_size, test_data):\r\n    \"\"\"\r\n    Returns appropriate length for test message.\r\n    \"\"\"\r\n    test_data_len = len(test_data)\r\n    count = 0 if msg_size % test_data_len == 0 else 1\r\n    count = count + (msg_size // test_data_len)\r\n\r\n    return count\r\n\r\ndef get_text_msg(msg_size):\r\n    \"\"\"\r\n    Returns a test message of given length.\r\n    \"\"\"\r\n    return TEST_DATA * get_test_msg_len(msg_size, TEST_DATA)\r\n\t\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n\r\n    parser.add_argument(\r\n        \"--kafka_hosts\",\r\n        type=str,\r\n       required=True,\r\n        help = \"The kafka hosts to connect to.\")\r\n    parser.add_argument(\r\n        \"--client_id\",\r\n        type=str,\r\n        required=True,\r\n        help = \"The client id for this producer.\")\r\n    parser.add_argument(\r\n        \"--topic_prefix\",\r\n        type=str,\r\n        help = \"The topic prefix to used when publishing on multiple topics.\" \r\n\t\t       \"If --topic_name is also specified then it will take priority.\")\r\n    parser.add_argument(\r\n        \"--topic_count\",\r\n        type=int, default = 5,\r\n        help = \"The number of topics to produce on when using --topic_prefix.\"  \r\n\t\t       \"Default is 5.\")\r\n    parser.add_argument(\r\n        \"--topic_name\",\r\n        type = str,\r\n        help = \"The topic on which this producer will send messages.\")\r\n    parser.add_argument(\r\n        \"--num_msgs\",\r\n        type=int, default = 10000,\r\n        help = \"Total number of messages to be sent. 0 means infinite.\")\r\n    parser.add_argument(\r\n        \"--msg_size\",\r\n        type=int, default = 100,\r\n        help = \"Size of each message to be sent, in bytes. Defaults to 100 bytes.\")\r\n    parser.add_argument(\r\n        \"--sleep_time\", \r\n        type=int, default = 1,\r\n        help = \"The interval between two consecutive bursts of messages, in\" \r\n               \" seconds.\")\r\n    parser.add_argument(\r\n        \"--stats_interval\",\r\n        type=int, default = 30,\r\n        help = \"Time (in seconds) after which stats are printed.\")\r\n    parser.add_argument(\r\n        \"--msgs_per_burst\",\r\n        type = int, default = 100,\r\n        help = \"The number of messages to be sent in a single burst before \" \r\n               \"sleeping.\")\r\n    parser.add_argument(\r\n        \"--ack_mode\",\r\n        type = str, default = '1',\r\n        help = \"Acknowledgement mode for producer. Valid values are 0,1, and -1 \")\r\n    parser.add_argument(\r\n        \"--debug_mode\",\r\n        type = str,\r\n        help = \"Debugging level for producer\")\r\n\r\n\r\n    #parser.set_defaults()i\r\n\r\n    opts  = parser.parse_args()\r\n\r\n    logging.basicConfig(filename=\"{}.python_producer.log\".format(opts.client_id),\r\n                    format='%(asctime)s %(message)s',\r\n                    filemode='a')\r\n    logger=logging.getLogger()\r\n    logger.setLevel(logging.DEBUG)\r\n\r\n    print(opts)\r\n\r\n    if not opts.topic_prefix and not opts.topic_name:\r\n        logger.critical(\"Please specify the topic name or prefix.\")\r\n    elif opts.topic_name:\r\n        opts.topic_count = 1\r\n\r\n    producer_conf = {}\t\t\r\n    producer_conf['bootstrap.servers'] =  opts.kafka_hosts\r\n    producer_conf['client.id'] = opts.client_id \r\n    producer_conf['acks'] = int(opts.ack_mode)\r\n    print(producer_conf)\r\n\t\r\n    if opts.debug_mode:\r\n        producer_conf['debug'] = opts.debug_mode\r\n\t\t\t\r\n    def handler(signum, frame):\r\n        print(\"Interrupted with the signal number: \" + str(signum))\r\n        os._exit(os.EX_OK)\r\n\r\n\r\n    def delivery_callback(err, msg):\r\n        logging.basicConfig(filename=\"callback_details.log\",\r\n                    format='%(asctime)s %(message)s',\r\n                    filemode='a')\r\n        logger1=logging.getLogger()\r\n        logger1.setLevel(logging.DEBUG)\r\n\r\n        if err:\r\n            logger1.info('%% Message failed delivery: %s\\n' % err)\r\n        else:\r\n            logger1.info('%% Message delivered to %s [%d] @ %d\\n' %\r\n                             (msg.topic(), msg.partition(), msg.offset()))\r\n\r\n\r\n    # Configure to handle the interrupt signal.\r\n    signal.signal(signal.SIGTERM, handler)\r\n    signal.signal(signal.SIGINT, handler)\r\n\r\n    def log_stats():\r\n        global total_msgs_sent, interval_msgs_sent, lock\r\n        def get_rate(msgs_sent, interval):\r\n            if interval > 0:\r\n                return round(float(msgs_sent) / (interval * 1000), 2)\r\n            else:\r\n                return 0\r\n\r\n        with lock:\r\n            rate = get_rate(interval_msgs_sent, opts.stats_interval)\r\n            logger.info(\"Interval Count: \" + str(interval_msgs_sent) + \" @ \" + \\\r\n                  str(rate) + \"K/s, Total Count: \" + str(total_msgs_sent))\r\n            interval_msgs_sent = 0\r\n\r\n        Timer(opts.stats_interval, log_stats).start()\r\n\r\n    Timer(opts.stats_interval, log_stats).start()\r\n\r\n    producer = Producer(**producer_conf)\r\n\r\n    global total_msgs_sent, interval_msgs_sent, lock\r\n\r\n    send_shutdown = False\r\n    send_call_latency = 0\r\n\r\n    logger.info(\"Starting to send messages\")\r\n\r\n    while True:\r\n        for msg_num in range(opts.msgs_per_burst):\r\n            msg = get_text_msg(opts.msg_size)\r\n            for topic_num in range(opts.topic_count):\r\n                msg_key = random.randrange(1, 10000, 1)\r\n                send_time = time.time() * 1000\r\n                if opts.topic_name:\r\n                    topic_name = opts.topic_name\r\n                else:\r\n                    topic_name = opts.topic_prefix + '-' + str(topic_num)\r\n                try:\r\n                    producer.produce(topic_name, key = str(msg_key), value = msg, callback = delivery_callback)\r\n                except BufferError:\r\n                    logger.warning('Local producer queue is full ({} messages awaiting delivery): try again\\n'.format(len(producer)))\r\n\r\n                \r\n                send_call_latency += time.time() * 1000 - send_time\r\n                producer.poll(0)\r\n\r\n                with lock:\r\n                    total_msgs_sent += 1\r\n                    interval_msgs_sent += 1\r\n                    if opts.num_msgs!=0 and total_msgs_sent >= (opts.num_msgs - 1):\r\n                        send_shutdown = True\r\n                        break\r\n            if send_shutdown:\r\n                break\r\n        if send_shutdown:\r\n            break\r\n        if opts.sleep_time:\r\n            time.sleep(opts.sleep_time)\r\n\t\t\t\r\n    producer.flush()\t\t\r\n    logger.info(\"Sent {} messages.\".format(total_msgs_sent))\t\r\n\t\t\t\t\t\r\n    os._exit(os.EX_OK)\r\n\r\n# Call the main function now.\r\nif __name__ == \"__main__\" : main()\r\n`\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/710", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/710/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/710/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/710/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/710", "id": 513221218, "node_id": "MDU6SXNzdWU1MTMyMjEyMTg=", "number": 710, "title": "Python sample doesn't work on CentOS 7", "user": {"login": "yokawasa", "id": 82332, "node_id": "MDQ6VXNlcjgyMzMy", "avatar_url": "https://avatars0.githubusercontent.com/u/82332?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yokawasa", "html_url": "https://github.com/yokawasa", "followers_url": "https://api.github.com/users/yokawasa/followers", "following_url": "https://api.github.com/users/yokawasa/following{/other_user}", "gists_url": "https://api.github.com/users/yokawasa/gists{/gist_id}", "starred_url": "https://api.github.com/users/yokawasa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yokawasa/subscriptions", "organizations_url": "https://api.github.com/users/yokawasa/orgs", "repos_url": "https://api.github.com/users/yokawasa/repos", "events_url": "https://api.github.com/users/yokawasa/events{/privacy}", "received_events_url": "https://api.github.com/users/yokawasa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-10-28T10:33:47Z", "updated_at": "2019-10-28T14:02:09Z", "closed_at": "2019-10-28T13:54:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nthe sample code I've tested - `confluent_cloud.py`\r\n- https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/confluent_cloud.py\r\n\r\nI tried to run the sample but the program doesn't go further from [this part](https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/confluent_cloud.py#L77). It doesn't flush and wait forever..  \r\n\r\n```\r\npython confluent_cloud.py\r\n```\r\n\r\nI tested the same sample on  Ubuntu 18.04.3 LTS and found it workd well with the same configuration.  I don't know why it doesn't work on CentOS\r\n\r\nHow to reproduce\r\n================\r\n\r\n```bash\r\ngit clone https://github.com/confluentinc/confluent-kafka-python.git\r\ncd confluent-kafka-python/example\r\n# Added info on 'bootstrap.servers', 'sasl.mechanisms',  'security.protocol',  'sasl.username',   'sasl.password'\r\nvi  confluent_cloud.py\r\n# install prerequisite \r\npip install confluent-kafka\r\npip install \"confluent-kafka[avro]\"\r\npip install --no-binary :all: confluent-kafka\r\n\r\n# finally run the sample\r\npython confluent_cloud.py\r\n```\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n```bash\r\n$ yum list |grep confluent-kafka\r\nconfluent-kafka-2.12.noarch             5.3.1-1                        @Confluent\r\nconfluent-kafka-connect-elasticsearch.noarch\r\nconfluent-kafka-connect-jdbc.noarch     5.3.1-1                        @Confluent\r\nconfluent-kafka-connect-jms.noarch      5.3.1-1                        @Confluent\r\nconfluent-kafka-connect-replicator.noarch\r\nconfluent-kafka-connect-s3.noarch       5.3.1-1                        @Confluent\r\nconfluent-kafka-connect-storage-common.noarch\r\nconfluent-kafka-mqtt.noarch             5.3.1-1                        @Confluent\r\nconfluent-kafka-rest.noarch             5.3.1-1                        @Confluent\r\nconfluent-kafka-2.11.noarch             5.3.1-1                        Confluent\r\n```\r\n\r\n - [ ] Apache Kafka broker version:\r\n\r\n - [ ] Client configuration: `{...}`\r\n```\r\n    'bootstrap.servers': '*****************.gcp.confluent.cloud:9092',\r\n    'sasl.mechanisms': 'PLAIN',\r\n    'security.protocol': 'SASL_SSL',\r\n    'sasl.username': '5CNWEYHVQNX4WFBC',\r\n    'sasl.password': '******************************'\r\n```\r\n - [ ] Operating system:\r\n```\r\n$ cat /etc/centos-release\r\nCentOS Linux release 7.7.1908 (Core)\r\n\r\n$ python --version\r\nPython 3.6.8\r\n```\r\n\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n```\r\nNO LOG came out\r\n```\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/708", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/708/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/708/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/708/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/708", "id": 512764760, "node_id": "MDU6SXNzdWU1MTI3NjQ3NjA=", "number": 708, "title": "Provide better documentation for SSL setup (especially dealling with keystores/truststores)", "user": {"login": "rootVIII", "id": 30498791, "node_id": "MDQ6VXNlcjMwNDk4Nzkx", "avatar_url": "https://avatars2.githubusercontent.com/u/30498791?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rootVIII", "html_url": "https://github.com/rootVIII", "followers_url": "https://api.github.com/users/rootVIII/followers", "following_url": "https://api.github.com/users/rootVIII/following{/other_user}", "gists_url": "https://api.github.com/users/rootVIII/gists{/gist_id}", "starred_url": "https://api.github.com/users/rootVIII/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rootVIII/subscriptions", "organizations_url": "https://api.github.com/users/rootVIII/orgs", "repos_url": "https://api.github.com/users/rootVIII/repos", "events_url": "https://api.github.com/users/rootVIII/events{/privacy}", "received_events_url": "https://api.github.com/users/rootVIII/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-26T00:09:01Z", "updated_at": "2019-10-28T10:16:18Z", "closed_at": "2019-10-26T02:55:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm extracting a CARoot.pem, a certificate chain, and a private key from a keystore. This worked for me at one point in time but it is now not working for my consumer authentication. I guess that's besides the point.\r\n\r\nIn reality there should be better documentation for this procedure. Maybe at least mention that the keystore/truststore cannot be used directly with the Python client like you can with the bin/ scripts. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/707", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/707/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/707/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/707/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/707", "id": 512436857, "node_id": "MDU6SXNzdWU1MTI0MzY4NTc=", "number": 707, "title": "Are there missing feature as compared to Kafka's Java client APIs", "user": {"login": "bpradeep20", "id": 3871141, "node_id": "MDQ6VXNlcjM4NzExNDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3871141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bpradeep20", "html_url": "https://github.com/bpradeep20", "followers_url": "https://api.github.com/users/bpradeep20/followers", "following_url": "https://api.github.com/users/bpradeep20/following{/other_user}", "gists_url": "https://api.github.com/users/bpradeep20/gists{/gist_id}", "starred_url": "https://api.github.com/users/bpradeep20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bpradeep20/subscriptions", "organizations_url": "https://api.github.com/users/bpradeep20/orgs", "repos_url": "https://api.github.com/users/bpradeep20/repos", "events_url": "https://api.github.com/users/bpradeep20/events{/privacy}", "received_events_url": "https://api.github.com/users/bpradeep20/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-25T10:31:23Z", "updated_at": "2019-10-25T11:26:12Z", "closed_at": "2019-10-25T11:26:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am not able to verify if all features which are supported by KAfka's java client APIs are also available with confluent-kafka python APIs. \r\n\r\nHas someone from community has verified this already. This will help me in avoiding extra efforts if this is already verified.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/706", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/706/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/706/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/706/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/706", "id": 512436184, "node_id": "MDU6SXNzdWU1MTI0MzYxODQ=", "number": 706, "title": "Are there missing feature as compared to Kafka's Java client A", "user": {"login": "bpradeep20", "id": 3871141, "node_id": "MDQ6VXNlcjM4NzExNDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3871141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bpradeep20", "html_url": "https://github.com/bpradeep20", "followers_url": "https://api.github.com/users/bpradeep20/followers", "following_url": "https://api.github.com/users/bpradeep20/following{/other_user}", "gists_url": "https://api.github.com/users/bpradeep20/gists{/gist_id}", "starred_url": "https://api.github.com/users/bpradeep20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bpradeep20/subscriptions", "organizations_url": "https://api.github.com/users/bpradeep20/orgs", "repos_url": "https://api.github.com/users/bpradeep20/repos", "events_url": "https://api.github.com/users/bpradeep20/events{/privacy}", "received_events_url": "https://api.github.com/users/bpradeep20/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948252, "node_id": "MDU6TGFiZWwzNTc5NDgyNTI=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/duplicate", "name": "duplicate", "color": "cccccc", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-10-25T10:29:51Z", "updated_at": "2019-10-25T11:24:41Z", "closed_at": "2019-10-25T11:24:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/703", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/703/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/703/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/703/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/703", "id": 512025257, "node_id": "MDU6SXNzdWU1MTIwMjUyNTc=", "number": 703, "title": "Consumer stopped consuming after 100000 messages", "user": {"login": "bpradeep20", "id": 3871141, "node_id": "MDQ6VXNlcjM4NzExNDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3871141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bpradeep20", "html_url": "https://github.com/bpradeep20", "followers_url": "https://api.github.com/users/bpradeep20/followers", "following_url": "https://api.github.com/users/bpradeep20/following{/other_user}", "gists_url": "https://api.github.com/users/bpradeep20/gists{/gist_id}", "starred_url": "https://api.github.com/users/bpradeep20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bpradeep20/subscriptions", "organizations_url": "https://api.github.com/users/bpradeep20/orgs", "repos_url": "https://api.github.com/users/bpradeep20/repos", "events_url": "https://api.github.com/users/bpradeep20/events{/privacy}", "received_events_url": "https://api.github.com/users/bpradeep20/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-10-24T15:36:58Z", "updated_at": "2019-12-17T13:03:12Z", "closed_at": "2019-12-17T13:03:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nConsumer is stopping after consuming 100000 messages and I am not observing any issue printed on my console.\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\nYou can run below code (please change bootstrap_servers  with the Kafka server you are running). This code is modified from https://github.com/ActivisionGameScience/python-kafka-benchmark/blob/master/Kafka%20Client%20Benchmarking.ipynb.\r\n\r\n```\r\nmsg_count = 1000000\r\nmsg_size = 100\r\nmsg_payload = ('kafkatest' * 20).encode()[:msg_size]\r\nprint(msg_payload)\r\nprint(len(msg_payload))\r\n\r\nbootstrap_servers = 'localhost1:9092,localhost2:9092,localhost3:9092,localhost4:9092,localhost5:9092'\r\n\r\nimport time\r\n\r\nproducer_timings = {}\r\nconsumer_timings = {}\r\n\r\ndef calculate_thoughput(timing, n_messages=1000000, msg_size=100):\r\n    print(\"Processed {0} messsages in {1:.2f} seconds\".format(n_messages, timing))\r\n    print(\"{0:.2f} MB/s\".format((msg_size * n_messages) / timing / (1024*1024)))\r\n    print(\"{0:.2f} Msgs/s\".format(n_messages / timing))\r\n\r\nimport confluent_kafka\r\ntopic = 'test_topic'\r\ndef confluent_kafka_producer_performance():\r\n    \r\n    conf = {'bootstrap.servers': bootstrap_servers,  'security.protocol':'SASL_PLAINTEXT',\r\n            'sasl.kerberos.service.name':'kafka',\r\n            'sasl.mechanism':'GSSAPI'}\r\n    producer = confluent_kafka.Producer(**conf)\r\n    messages_to_retry = 0\r\n\r\n    producer_start = time.time()\r\n    for i in range(msg_count):\r\n        try:\r\n            producer.produce(topic, key=\"key_{}\".format(i), value=msg_payload)      \r\n        except BufferError as e:\r\n            messages_to_retry += 1\r\n\r\n    print(\"Message to retry: \" + str(messages_to_retry))\r\n\r\n    # hacky retry messages that over filled the local buffer\r\n    for i in range(messages_to_retry):\r\n        producer.poll(0)\r\n        try:\r\n            producer.produce(topic, key=\"key1_{}\".format(i), value=msg_payload)\r\n        except BufferError as e:\r\n            producer.poll(0)\r\n            print(\"Failed \" + str(i))\r\n            producer.produce(topic,key=\"key1_{}\".format(i), value=msg_payload)\r\n\r\n    producer.flush()\r\n\r\n    return time.time() - producer_start\r\n\r\nproducer_timings['confluent_kafka_producer'] = confluent_kafka_producer_performance()\r\ncalculate_thoughput(producer_timings['confluent_kafka_producer'])\r\n\r\nimport confluent_kafka\r\nimport uuid\r\n\r\ndef confluent_kafka_consumer_performance():\r\n    \r\n    msg_consumed_count = 0\r\n    conf = {'bootstrap.servers': bootstrap_servers,\r\n            'group.id': 'test.abc',\r\n            'session.timeout.ms': 6000,\r\n            'security.protocol':'SASL_PLAINTEXT',\r\n            'sasl.kerberos.service.name':'kafka',\r\n            'sasl.mechanism':'GSSAPI'\r\n    }\r\n\r\n\r\n    consumer = confluent_kafka.Consumer(**conf)\r\n\r\n    consumer_start = time.time()\r\n    # This is the same as pykafka, subscribing to a topic will start a background thread\r\n    consumer.subscribe([topic])\r\n\r\n    while True:\r\n        msg = consumer.poll(1)\r\n        if msg:\r\n            msg_consumed_count += 1\r\n                         \r\n        if msg_consumed_count >= msg_count:\r\n            break\r\n                    \r\n    consumer_timing = time.time() - consumer_start\r\n    consumer.close()    \r\n    return consumer_timing\r\n\r\n\r\n_ = confluent_kafka_consumer_performance() # Warm cache\r\nconsumer_timings['confluent_kafka_consumer'] = confluent_kafka_consumer_performance()\r\ncalculate_thoughput(consumer_timings['confluent_kafka_consumer'])\r\n\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/697", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/697/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/697/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/697/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/697", "id": 509307354, "node_id": "MDU6SXNzdWU1MDkzMDczNTQ=", "number": 697, "title": "failed: err: Local: SSL error: (errno: Undefined error: 0)", "user": {"login": "Suresh-KM", "id": 53583575, "node_id": "MDQ6VXNlcjUzNTgzNTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/53583575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Suresh-KM", "html_url": "https://github.com/Suresh-KM", "followers_url": "https://api.github.com/users/Suresh-KM/followers", "following_url": "https://api.github.com/users/Suresh-KM/following{/other_user}", "gists_url": "https://api.github.com/users/Suresh-KM/gists{/gist_id}", "starred_url": "https://api.github.com/users/Suresh-KM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Suresh-KM/subscriptions", "organizations_url": "https://api.github.com/users/Suresh-KM/orgs", "repos_url": "https://api.github.com/users/Suresh-KM/repos", "events_url": "https://api.github.com/users/Suresh-KM/events{/privacy}", "received_events_url": "https://api.github.com/users/Suresh-KM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-10-18T20:58:10Z", "updated_at": "2019-11-25T20:59:04Z", "closed_at": "2019-11-25T18:48:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nWhen trying to produce messages the broker fails and does not send the message.\r\n\r\n%7|1571426373.697|SASL|rdkafka#producer-1| [thrd:app]: Selected provider PLAIN (builtin) for SASL mechanism PLAIN\r\n%7|1571426373.697|OPENSSL|rdkafka#producer-1| [thrd:app]: librdkafka built with OpenSSL version 0x1000212f\r\n%7|1571426373.700|BRKMAIN|rdkafka#producer-1| [thrd::0/internal]: :0/internal: Enter main broker thread\r\n%7|1571426373.700|BROKER|rdkafka#producer-1| [thrd:app]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Added new broker with NodeId -1\r\n%7|1571426373.700|BRKMAIN|rdkafka#producer-1| [thrd:sasl_ssl://x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Enter main broker thread\r\n%7|1571426373.700|CONNECT|rdkafka#producer-1| [thrd:app]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Selected for cluster connection: bootstrap servers added (broker has 0 connection attempt(s))\r\n%7|1571426373.700|CONNECT|rdkafka#producer-1| [thrd:sasl_ssl:/x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Received CONNECT op\r\n%7|1571426373.700|INIT|rdkafka#producer-1| [thrd:app]: librdkafka v1.0.0 (0x10000ff) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd, GCC GXX PKGCONFIG OSXLD LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM SNAPPY SOCKEM SASL_SCRAM CRC32C_HW, debug 0x282)\r\n%7|1571426373.700|STATE|rdkafka#producer-1| [thrd:sasl_ssl://x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n%7|1571426373.700|CONNECT|rdkafka#producer-1| [thrd:sasl_ssl://x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n%7|1571426373.700|STATE|rdkafka#producer-1| [thrd:sasl_ssl://x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n%7|1571426373.865|CONNECT|rdkafka#producer-1| [thrd:sasl_ssl://x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Connecting to ipv4#x.x.x.x:9092 (sasl_ssl) with socket 11\r\n%7|1571426373.886|CONNECT|rdkafka#producer-1| [thrd:sasl_ssl://x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: Connected to ipv4#x.x.x.x:9092\r\n%7|1571426373.965|BROKERFAIL|rdkafka#producer-1| [thrd:sasl_ssl:/x.gcp.confluent.cloud:9092/bootstra]: sasl_ssl://x.gcp.confluent.cloud:9092/bootstrap: failed: err: Local: SSL error: (errno: Undefined error: 0)\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version:`1.2.0` and `1.2.1`\r\n - [ ] Apache Kafka broker version:\r\n - [x] Client configuration: `{\r\n    'bootstrap.servers': bootstrap,\r\n    'broker.version.fallback': '0.10.0.0',\r\n    'sasl.mechanisms': 'PLAIN',\r\n    'security.protocol': 'SASL_SSL',\r\n    'sasl.username': username,\r\n    'sasl.password': password\r\n    }`\r\n - [x] Operating system: OS X\r\n - [x] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/696", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/696/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/696/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/696/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/696", "id": 509245283, "node_id": "MDU6SXNzdWU1MDkyNDUyODM=", "number": 696, "title": "Python 3.8 package installation fails in Docker Build", "user": {"login": "justineyster", "id": 6627492, "node_id": "MDQ6VXNlcjY2Mjc0OTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/6627492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justineyster", "html_url": "https://github.com/justineyster", "followers_url": "https://api.github.com/users/justineyster/followers", "following_url": "https://api.github.com/users/justineyster/following{/other_user}", "gists_url": "https://api.github.com/users/justineyster/gists{/gist_id}", "starred_url": "https://api.github.com/users/justineyster/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justineyster/subscriptions", "organizations_url": "https://api.github.com/users/justineyster/orgs", "repos_url": "https://api.github.com/users/justineyster/repos", "events_url": "https://api.github.com/users/justineyster/events{/privacy}", "received_events_url": "https://api.github.com/users/justineyster/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-10-18T18:30:42Z", "updated_at": "2020-02-10T10:49:46Z", "closed_at": "2019-12-17T12:16:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nCannot install confluent-kafka on docker image running Python 3.8 image. Was forced to roll back to Python 3.7\r\n\r\n\r\nHow to reproduce\r\n================\r\nAttempt to build a docker image with python:3 or python 3.8 as the base image. We used pipenv in our project. `pip install confluent-kafka` will result in the same error: `ERROR: ERROR: Package installation failed...`\r\n\r\nWithin the stack trace: `#error \"confluent-kafka-python requires librdkafka v1.0.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"'`\r\n\r\nFull stack of failed package installation: https://gist.github.com/justineyster/736b6dfa384d32bf1d5333ddd3b307a8\r\n\r\nIn an attempt to mitigate, I successfully installed librdkafka with `apt-get update && apt-get install librdkafka` before running the install but the failure and error message was the same.\r\n\r\nThe only thing that fixed the error was rolling our Docker image back to `python:3.7`.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): latest of both\r\n - [ ] Apache Kafka broker version: unsure\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system: `python:3` Docker image, based on Debian \r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/688", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/688/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/688/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/688/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/688", "id": 504848280, "node_id": "MDU6SXNzdWU1MDQ4NDgyODA=", "number": 688, "title": "Consumer Code is not not creating Client ID or topic information over Conflunet Control Center ", "user": {"login": "NiravLangaliya", "id": 13895686, "node_id": "MDQ6VXNlcjEzODk1Njg2", "avatar_url": "https://avatars2.githubusercontent.com/u/13895686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NiravLangaliya", "html_url": "https://github.com/NiravLangaliya", "followers_url": "https://api.github.com/users/NiravLangaliya/followers", "following_url": "https://api.github.com/users/NiravLangaliya/following{/other_user}", "gists_url": "https://api.github.com/users/NiravLangaliya/gists{/gist_id}", "starred_url": "https://api.github.com/users/NiravLangaliya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NiravLangaliya/subscriptions", "organizations_url": "https://api.github.com/users/NiravLangaliya/orgs", "repos_url": "https://api.github.com/users/NiravLangaliya/repos", "events_url": "https://api.github.com/users/NiravLangaliya/events{/privacy}", "received_events_url": "https://api.github.com/users/NiravLangaliya/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-09T19:30:55Z", "updated_at": "2019-12-17T14:04:09Z", "closed_at": "2019-12-17T14:04:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nI have created the topic 'EDW_TEST_DATA' and producing data and I do see at confluent control center that my Consume group is created as per group.ID mentioned in python code but I am not able to see that client ID is created and also no topic is listed in consumption at the control center. \r\n\r\n\r\n![image](https://user-images.githubusercontent.com/13895686/66513651-71fb5e00-ea90-11e9-9694-85fb64d6a9d4.png)\r\n\r\n\r\nCode: \r\n\r\nfrom confluent_kafka import KafkaError\r\nfrom confluent_kafka.avro import AvroConsumer\r\nfrom confluent_kafka.avro.serializer import SerializerError\r\nimport threading,time , pprint\r\nimport json\r\n\r\nsettings = {\r\n            'bootstrap.servers': 'XXXXXX:9092,'\r\n            ,'group.id': 'edwdevgroupid12'\r\n            ,'client.id': '200'\r\n            ,'enable.auto.commit': False\r\n            ,'session.timeout.ms': 6000\r\n            ,'default.topic.config': {'auto.offset.reset': 'smallest'}\r\n            ,'schema.registry.url': 'http://XXXXXXX:8081'\r\n          }\r\n\r\n\r\ndef run(name):\r\n    c = AvroConsumer(settings)\r\n\r\n    c.subscribe(['EDW_TEST_DATA'])\r\n\r\n    while True:\r\n        try:\r\n            msg = c.poll(10)\r\n\r\n        except SerializerError as e:\r\n            print(\"Message deserialization failed for {}: {}\".format(msg, e))\r\n            break\r\n\r\n        if msg is None:\r\n            continue\r\n\r\n        if msg.error():\r\n            print(\"AvroConsumer error: {}\".format(msg.error()))\r\n            continue\r\n\r\n        print(\"%s Data processed %s [%d] at offset %d with key %s:\\n\" % (name, msg.topic(), msg.partition(), msg.offset(), str(msg.key())))\r\n        print (msg.value()['Name'])\r\n        print(msg.value()['Contact']['Email'])\r\n        print(msg.value()['Contact']['PhoneNumber'])\r\n        print(msg.value()['LocationDetails']['CountryCode'])\r\n        print(msg.value()['LocationDetails']['City'])\r\n        print(msg.value()['LocationDetails']['Country'])\r\n        print(msg.value()['LocationDetails']['PostalCode'])\r\n        print(msg.value()['LocationDetails']['Address'])\r\n\r\n    print(\"Shutting down consumer..\")\r\n\r\n    c.close()\r\n\r\n\r\nworkers = []\r\nfor index in range(4):\r\n    w = threading.Thread(target=run, args=(index,))\r\n    w.start()\r\n    workers.append(w)\r\ntime.sleep(2)\r\n\r\nfor w in workers:\r\n    w.join()\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):('1.1.0', 16842752) and ('1.1.0', 16843007)\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/686", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/686/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/686/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/686/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/686", "id": 504026537, "node_id": "MDU6SXNzdWU1MDQwMjY1Mzc=", "number": 686, "title": "Missing topic error only appears for second message produced", "user": {"login": "jamesjrg", "id": 110452, "node_id": "MDQ6VXNlcjExMDQ1Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/110452?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesjrg", "html_url": "https://github.com/jamesjrg", "followers_url": "https://api.github.com/users/jamesjrg/followers", "following_url": "https://api.github.com/users/jamesjrg/following{/other_user}", "gists_url": "https://api.github.com/users/jamesjrg/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesjrg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesjrg/subscriptions", "organizations_url": "https://api.github.com/users/jamesjrg/orgs", "repos_url": "https://api.github.com/users/jamesjrg/repos", "events_url": "https://api.github.com/users/jamesjrg/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesjrg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-08T12:54:29Z", "updated_at": "2019-10-09T19:46:24Z", "closed_at": "2019-10-09T13:45:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nIf you use AvroConsumer to produce a message on a non-existing topic, then the production silently fails - no error is printed.\r\n\r\nIf you do the same thing a second time, an error message is printed, as expected.\r\n\r\nHow to reproduce\r\n================\r\n```\r\nfrom confluent_kafka import avro\r\nfrom confluent_kafka.avro import AvroConsumer, AvroProducer\r\n\r\nvalue_schema_str = \"\"\"\r\n{\r\n   \"namespace\": \"my.test\",\r\n   \"name\": \"value\",\r\n   \"type\": \"record\",\r\n   \"fields\" : [\r\n     {\r\n       \"name\" : \"name\",\r\n       \"type\" : \"string\"\r\n     }\r\n   ]\r\n}\r\n\"\"\"\r\n\r\nkey_schema_str = \"\"\"\r\n{\r\n   \"namespace\": \"my.test\",\r\n   \"name\": \"key\",\r\n   \"type\": \"record\",\r\n   \"fields\" : [\r\n     {\r\n       \"name\" : \"name\",\r\n       \"type\" : \"string\"\r\n     }\r\n   ]\r\n}\r\n\"\"\"\r\n\r\nvalue_schema = avro.loads(value_schema_str)\r\nkey_schema = avro.loads(key_schema_str)\r\nvalue = {\"name\": \"Value\"}\r\nkey = {\"name\": \"Key\"}\r\n\r\np = AvroProducer({\r\n    \"bootstrap.servers\": \",\".join(......),\r\n    \"schema.registry.url\": ......,\r\n    }, default_key_schema=key_schema, default_value_schema=value_schema)\r\np.produce(topic='my_topic', value=value, key=key) # no error, despite missing topic\r\np.flush() # no error, despite missing topic\r\np.produce(topic='my_topic', value=value, key=key) # error, as expected\r\n```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n'1.2.0', 16908288\r\n'1.2.0', 16908543\r\n\r\n - [x] Apache Kafka broker version:\r\n5.2.1\r\n\r\n - [x] Operating system: the broker is running in Docker in Linux Mint, the client in the host OS\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/684", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/684/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/684/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/684/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/684", "id": 502376661, "node_id": "MDU6SXNzdWU1MDIzNzY2NjE=", "number": 684, "title": "unable to register schema errorcode 503", "user": {"login": "test1370", "id": 25205754, "node_id": "MDQ6VXNlcjI1MjA1NzU0", "avatar_url": "https://avatars1.githubusercontent.com/u/25205754?v=4", "gravatar_id": "", "url": "https://api.github.com/users/test1370", "html_url": "https://github.com/test1370", "followers_url": "https://api.github.com/users/test1370/followers", "following_url": "https://api.github.com/users/test1370/following{/other_user}", "gists_url": "https://api.github.com/users/test1370/gists{/gist_id}", "starred_url": "https://api.github.com/users/test1370/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/test1370/subscriptions", "organizations_url": "https://api.github.com/users/test1370/orgs", "repos_url": "https://api.github.com/users/test1370/repos", "events_url": "https://api.github.com/users/test1370/events{/privacy}", "received_events_url": "https://api.github.com/users/test1370/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-04T01:25:36Z", "updated_at": "2019-12-17T13:56:19Z", "closed_at": "2019-12-17T13:56:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI am trying to use producer in python using confluent-avro modules. everything is running fine(kafka server, schemaReg url). I can post string messages to the server but not Avro!!\r\n\r\nThis is the code i am using..\r\navroProducer = AvroProducer({\r\n            'bootstrap.servers': 'server1',\r\n            'schema.registry.url': 'url',\r\n            'api.version.request': 'true'\r\n          },  default_key_schema=schema, default_value_schema=schema)\r\n        result = avroProducer.produce(topic='topic1', value=value, key=key)\r\n        avroProducer.flush()\r\n\r\nOutput:\r\nraise ClientError(\"Unable to register schema. Error code:\" + str(code))\r\nconfluent_kafka.avro.error.ClientError: Unable to register schema. Error code:503\r\n\r\nIt would be great if some one helpss.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/682", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/682/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/682/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/682/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/682", "id": 502260188, "node_id": "MDU6SXNzdWU1MDIyNjAxODg=", "number": 682, "title": "Timed out waiting for controller while creating topic", "user": {"login": "NiravLangaliya", "id": 13895686, "node_id": "MDQ6VXNlcjEzODk1Njg2", "avatar_url": "https://avatars2.githubusercontent.com/u/13895686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NiravLangaliya", "html_url": "https://github.com/NiravLangaliya", "followers_url": "https://api.github.com/users/NiravLangaliya/followers", "following_url": "https://api.github.com/users/NiravLangaliya/following{/other_user}", "gists_url": "https://api.github.com/users/NiravLangaliya/gists{/gist_id}", "starred_url": "https://api.github.com/users/NiravLangaliya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NiravLangaliya/subscriptions", "organizations_url": "https://api.github.com/users/NiravLangaliya/orgs", "repos_url": "https://api.github.com/users/NiravLangaliya/repos", "events_url": "https://api.github.com/users/NiravLangaliya/events{/privacy}", "received_events_url": "https://api.github.com/users/NiravLangaliya/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-03T19:43:24Z", "updated_at": "2019-10-09T19:20:38Z", "closed_at": "2019-10-09T19:20:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nI am able to list existing topics but I am not able to create a new topic or change partitions to an existing topic\r\n\r\nFailed to create topic EDW_TEST_DATA: KafkaError{code=_TIMED_OUT,val=-185,str=\"Timed out waiting for controller\"}\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\nimport confluent_kafka.admin, pprint\r\n\r\n\r\nconf = {'bootstrap.servers': 'localhost:9092'}\r\n\r\nkafka_admin = confluent_kafka.admin.AdminClient(conf)\r\n\r\ntopics = ['EDW_TEST_DATA']\r\nnew_topics = [confluent_kafka.admin.NewTopic(topic, num_partitions=3, replication_factor=1) for topic in topics]\r\n\r\nfs = kafka_admin.create_topics(new_topics)\r\n\r\nfor topic, f in fs.items():\r\n    try:\r\n        f.result()  # The result itself is None\r\n        print(\"Topic {} created\".format(topic))\r\n    except Exception as e:\r\n        print(\"Failed to create topic {}: {}\".format(topic, e))\r\n\r\n\r\npprint.pprint(kafka_admin.list_topics().topics)\r\n\r\ntopic_list = []\r\ntopic_list.append(confluent_kafka.admin.NewPartitions('EDW_EMP_DATA1', 3))\r\nkafka_admin.create_partitions(topic_list)\r\n\r\npprint.pprint(kafka_admin.list_topics().topics)\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [X ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):  ('1.1.0', 16842752)  and ('1.1.0', 16843007)\r\n - [X] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [X ] Operating system: MacOS 10.14\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/681", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/681/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/681/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/681/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/681", "id": 501761766, "node_id": "MDU6SXNzdWU1MDE3NjE3NjY=", "number": 681, "title": "Question: How does max.poll.interval.ms differ between Java and rdkafka poll() api?", "user": {"login": "mkmoisen", "id": 2534631, "node_id": "MDQ6VXNlcjI1MzQ2MzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2534631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mkmoisen", "html_url": "https://github.com/mkmoisen", "followers_url": "https://api.github.com/users/mkmoisen/followers", "following_url": "https://api.github.com/users/mkmoisen/following{/other_user}", "gists_url": "https://api.github.com/users/mkmoisen/gists{/gist_id}", "starred_url": "https://api.github.com/users/mkmoisen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mkmoisen/subscriptions", "organizations_url": "https://api.github.com/users/mkmoisen/orgs", "repos_url": "https://api.github.com/users/mkmoisen/repos", "events_url": "https://api.github.com/users/mkmoisen/events{/privacy}", "received_events_url": "https://api.github.com/users/mkmoisen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-02T22:39:58Z", "updated_at": "2019-12-17T13:56:03Z", "closed_at": "2019-12-17T13:56:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Under the [Detecting Consumer Failures](https://kafka.apache.org/23/javadoc/index.html?org/apache/kafka/clients/consumer/KafkaConsumer.html) of the Java Consumer API, it states that if `poll` is not called at least as frequently as `max.poll.interval.ms`, then the client will leave the group and trigger a rebalance.\r\n\r\nThe rdkafka poll API differs from the Java API: the Java API returns an arraylist of messages to the calling code, while rdkafka returns a single message to the calling code; I think behind the scenes, rdkafka has an internal queue that actually received a batch of messages from the broker, but only one is returned to the user.\r\n\r\nSo in the rdkafka api, will calling poll() cause the clock to reset on the `max.poll.interval.ms` upon each and every invocation of poll(), or will the clock only reset once the internal queue is depleted?\r\n\r\nThis is not a bug, just a question.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): ('1.2.0', 16908288) and ('1.2.0', 16908543)\r\n - [x] Apache Kafka broker version: 2.3\r\n - [x] Client configuration: `{...}` n/a\r\n - [x] Operating system: Linux, Windows\r\n - [x] Provide client logs (with `'debug': '..'` as necessary) n/a\r\n - [x] Provide broker log excerpts n/a\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/669", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/669/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/669/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/669/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/669", "id": 496802984, "node_id": "MDU6SXNzdWU0OTY4MDI5ODQ=", "number": 669, "title": "Improve speed of Kafka Producer", "user": {"login": "QuanVan95", "id": 29303948, "node_id": "MDQ6VXNlcjI5MzAzOTQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/29303948?v=4", "gravatar_id": "", "url": "https://api.github.com/users/QuanVan95", "html_url": "https://github.com/QuanVan95", "followers_url": "https://api.github.com/users/QuanVan95/followers", "following_url": "https://api.github.com/users/QuanVan95/following{/other_user}", "gists_url": "https://api.github.com/users/QuanVan95/gists{/gist_id}", "starred_url": "https://api.github.com/users/QuanVan95/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/QuanVan95/subscriptions", "organizations_url": "https://api.github.com/users/QuanVan95/orgs", "repos_url": "https://api.github.com/users/QuanVan95/repos", "events_url": "https://api.github.com/users/QuanVan95/events{/privacy}", "received_events_url": "https://api.github.com/users/QuanVan95/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-22T16:42:39Z", "updated_at": "2019-09-24T01:53:50Z", "closed_at": "2019-09-24T01:53:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nHi Mr. Edenhill,\r\nI'm working with Kafka Producer - Python, when I tried to produce messages to Kafka, for example, 10k message. It took too long(>7s for 10k simple message). I reduce the config producer.poll(0.001) (I tried to reduce this config more, but it only works with min is 0.001) it will run faster than poll(0.001++) \r\nCould u please recommend for me the right config to improve the speed of Producer and Consumer as well?\r\nThank you so much, hope to hear from you soon!\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/666", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/666/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/666/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/666/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/666", "id": 488490787, "node_id": "MDU6SXNzdWU0ODg0OTA3ODc=", "number": 666, "title": "segmentation fault when producer callback ", "user": {"login": "amamidela", "id": 15055588, "node_id": "MDQ6VXNlcjE1MDU1NTg4", "avatar_url": "https://avatars1.githubusercontent.com/u/15055588?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amamidela", "html_url": "https://github.com/amamidela", "followers_url": "https://api.github.com/users/amamidela/followers", "following_url": "https://api.github.com/users/amamidela/following{/other_user}", "gists_url": "https://api.github.com/users/amamidela/gists{/gist_id}", "starred_url": "https://api.github.com/users/amamidela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amamidela/subscriptions", "organizations_url": "https://api.github.com/users/amamidela/orgs", "repos_url": "https://api.github.com/users/amamidela/repos", "events_url": "https://api.github.com/users/amamidela/events{/privacy}", "received_events_url": "https://api.github.com/users/amamidela/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948255, "node_id": "MDU6TGFiZWwzNTc5NDgyNTU=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/invalid", "name": "invalid", "color": "e6e6e6", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-09-03T09:24:07Z", "updated_at": "2019-09-08T11:07:53Z", "closed_at": "2019-09-08T11:07:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nTrying simple kafka producer example here is snippet- it throws segmenation fault when i try \r\nproducer->poll(1000)\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\nproducer::producer()\r\n{\r\n\terrstr = \" \";\r\n\tbrokers = \" \";\r\n\tdo_conf_dump = false;\r\n\t//produce = NULL;\r\n}\r\n\r\nproducer::~producer()\r\n{\r\n\t//delete producer;\r\n}\r\n\r\nstatic bool run = true;\r\n\r\nstatic void sigterm(int sig)\r\n{\r\n\trun = false;\r\n}\r\n\r\nclass ExampleDeliveryReportCb : public RdKafka::DeliveryReportCb\r\n{\r\npublic:\r\n\tvoid dr_cb(RdKafka::Message &message)\r\n\t{\r\n\t\tstd::string status_name;\r\n\t\tswitch (message.status())\r\n\t\t{\r\n\t\tcase RdKafka::Message::MSG_STATUS_NOT_PERSISTED:\r\n\t\t\tstatus_name = \"NotPersisted\";\r\n\t\t\tbreak;\r\n\t\tcase RdKafka::Message::MSG_STATUS_POSSIBLY_PERSISTED:\r\n\t\t\tstatus_name = \"PossiblyPersisted\";\r\n\t\t\tbreak;\r\n\t\tcase RdKafka::Message::MSG_STATUS_PERSISTED:\r\n\t\t\tstatus_name = \"Persisted\";\r\n\t\t\tbreak;\r\n\t\tdefault:\r\n\t\t\tstatus_name = \"Unknown?\";\r\n\t\t\tbreak;\r\n\t\t}\r\n\t\tstd::cout << \"Message delivery for (\" << message.len() << \" bytes): \" << status_name << \": \" << message.errstr() << std::endl;\r\n\t\tif (message.key())\r\n\t\t\tstd::cout << \"Key: \" << *(message.key()) << \";\" << std::endl;\r\n\t}\r\n};\r\n\r\nclass ExampleEventCb : public RdKafka::EventCb\r\n{\r\npublic:\r\n\tvoid event_cb(RdKafka::Event &event)\r\n\t{\r\n\t\tswitch (event.type())\r\n\t\t{\r\n\t\tcase RdKafka::Event::EVENT_ERROR:\r\n\t\t\tif (event.fatal())\r\n\t\t\t{\r\n\t\t\t\tstd::cerr << \"FATAL \";\r\n\t\t\t\trun = false;\r\n\t\t\t}\r\n\t\t\tstd::cerr << \"ERROR (\" << RdKafka::err2str(event.err()) << \"): \" << event.str() << std::endl;\r\n\t\t\tbreak;\r\n\r\n\t\tcase RdKafka::Event::EVENT_STATS:\r\n\t\t\tstd::cerr << \"\\\"STATS\\\": \" << event.str() << std::endl;\r\n\t\t\tbreak;\r\n\r\n\t\tcase RdKafka::Event::EVENT_LOG:\r\n\t\t\tfprintf(stderr, \"LOG-%i-%s: %s\\n\",\r\n\t\t\t\t\tevent.severity(), event.fac().c_str(), event.str().c_str());\r\n\t\t\tbreak;\r\n\r\n\t\tdefault:\r\n\t\t\tstd::cerr << \"EVENT \" << event.type() << \" (\" << RdKafka::err2str(event.err()) << \"): \" << event.str() << std::endl;\r\n\t\t\tbreak;\r\n\t\t}\r\n\t}\r\n};\r\n\r\nRdKafka::Producer* producer::initializeProducer()\r\n{\r\n\r\n\tRdKafka::Conf *conf = RdKafka::Conf::create(RdKafka::Conf::CONF_GLOBAL);\r\n\tRdKafka::Conf *tconf = RdKafka::Conf::create(RdKafka::Conf::CONF_TOPIC);\r\n\r\n\tconf->set(\"metadata.broker.list\", BROKER, errstr);\r\n\r\n\tExampleEventCb ex_event_cb;\r\n\tconf->set(\"event_cb\", &ex_event_cb, errstr);\r\n\r\n\tExampleDeliveryReportCb ex_dr_cb;\r\n\tconf->set(\"dr_cb\", &ex_dr_cb, errstr);\r\n\tconf->set(\"default_topic_conf\", tconf, errstr);\r\n\r\n\t//getProducer(conf, tconf);\r\n\r\n\tRdKafka::Producer  *producer = RdKafka::Producer::create(conf, errstr);\r\n\tif (!producer)\r\n\t{\r\n\t\tstd::cerr << \"Failed to create producer: \" << errstr << std::endl;\r\n\t\texit(1);\r\n\t}\r\n\r\n\tstd::cout << \"% Created producer \" << producer->name() << std::endl;\r\n\r\n\t/*\r\n     * Create topic handle.\r\n     */\r\n\tRdKafka::Topic *topic = RdKafka::Topic::create(producer, PRODUCER_TOPIC,\r\n\t\t\t\t\t\t\t\t\t\t\t\t   tconf, errstr);\r\n\tif (!topic)\r\n\t{\r\n\t\tstd::cerr << \"Failed to create topic: \" << errstr << std::endl;\r\n\t\texit(1);\r\n\t}\r\n\r\n\treturn producer;\r\n}\r\n\r\n/* void producer::getProducer(RdKafka::Conf *conf, RdKafka::Conf *tconf)\r\n{\r\n\tproduce = RdKafka::Producer::create(conf, errstr);\r\n\tif (!produce)\r\n\t{\r\n\t\tstd::cerr << \"Failed to create producer: \" << errstr << std::endl;\r\n\t\texit(1);\r\n\t}\r\n\r\n\tstd::cout << \"% Created producer \" << produce->name() << std::endl;\r\n\r\n\tRdKafka::Topic *topic = RdKafka::Topic::create(produce, PRODUCER_TOPIC,\r\n\t\t\t\t\t\t\t\t\t\t\t\t   tconf, errstr);\r\n\tif (!topic)\r\n\t{\r\n\t\tstd::cerr << \"Failed to create topic: \" << errstr << std::endl;\r\n\t\texit(1);\r\n\t}\r\n}*/\r\n\r\nvoid producer::publishMessage(std::string buffer)\r\n{\r\n\tint32_t partition = RdKafka::Topic::PARTITION_UA;\r\n\r\n\tRdKafka::Producer *producer = initializeProducer();\r\n\t/*\r\n       * Produce message\r\n       */\r\n\tRdKafka::ErrorCode resp =\r\n\t\tproducer->produce(PRODUCER_TOPIC, partition,\r\n\t\t\t\t\t\t  RdKafka::Producer::RK_MSG_COPY /* Copy payload */,\r\n\t\t\t\t\t\t  /* Value */\r\n\t\t\t\t\t\t  const_cast<char *>(buffer.c_str()), buffer.size(),\r\n\t\t\t\t\t\t  /* Key */\r\n\t\t\t\t\t\t  NULL, 0,\r\n\t\t\t\t\t\t  /* Timestamp (defaults to now) */\r\n\t\t\t\t\t\t  0,\r\n\t\t\t\t\t\t  /* Message headers, if any */\r\n\t\t\t\t\t\t  NULL,\r\n\t\t\t\t\t\t  /* Per-message opaque value passed to\r\n                           * delivery report */\r\n\t\t\t\t\t\t  NULL);\r\n\tif (resp != RdKafka::ERR_NO_ERROR)\r\n\t{\r\n\t\tstd::cerr << \"% Produce failed: \" << RdKafka::err2str(resp) << std::endl;\r\n\t\t//delete headers; /* Headers are automatically deleted on produce() success. */\r\n\t}\r\n\telse\r\n\t{\r\n\t\tstd::cerr << \"% Produced message (\" << buffer.size() << \" bytes)\" << \"buffer =%s\"<<buffer<< std::endl;\r\n\t\tprintf(\"outq_len: %d\\n\", producer->outq_len());\r\n\r\n\t}\r\n\r\n\tproducer->poll(0);\r\n\r\n\trun = true;\r\n\r\n\twhile (run && producer->outq_len() > 0)\r\n\t{\r\n\t\tstd::cerr << \"Waiting for \" << producer->outq_len() << std::endl;\r\n\t\tstd::cout<<\"coming here\"<<std::endl;\r\n\t\tproducer->poll(1000);\r\n\t\tstd::cout<<\"coming here1\"<<std::endl;\r\n\t}\r\n\tdelete producer;\r\n\t//delete conf;\r\n\t//delete tconf;\r\n}\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/659", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/659/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/659/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/659/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/659", "id": 483467345, "node_id": "MDU6SXNzdWU0ODM0NjczNDU=", "number": 659, "title": "Support disable auto register schema", "user": {"login": "Matthijsy", "id": 5302372, "node_id": "MDQ6VXNlcjUzMDIzNzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5302372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Matthijsy", "html_url": "https://github.com/Matthijsy", "followers_url": "https://api.github.com/users/Matthijsy/followers", "following_url": "https://api.github.com/users/Matthijsy/following{/other_user}", "gists_url": "https://api.github.com/users/Matthijsy/gists{/gist_id}", "starred_url": "https://api.github.com/users/Matthijsy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Matthijsy/subscriptions", "organizations_url": "https://api.github.com/users/Matthijsy/orgs", "repos_url": "https://api.github.com/users/Matthijsy/repos", "events_url": "https://api.github.com/users/Matthijsy/events{/privacy}", "received_events_url": "https://api.github.com/users/Matthijsy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948253, "node_id": "MDU6TGFiZWwzNTc5NDgyNTM=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-08-21T14:46:00Z", "updated_at": "2020-08-09T09:32:10Z", "closed_at": "2020-01-29T12:41:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nCurrently the schemas you submit when producing messages using the AvroProducer are always registered in the schema registry. However, the documentation recommends to [disable this](https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#auto-schema-registration). I couldn't find a configuration to do this. Did I miss it or is it not supported at the moment?\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\nProduce messages using the AvroProducer.\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\nVersions:\r\nconfluent-kafka-python==1.1.0\r\nlibrdkafka==1.1.0\r\nbroker: enterprise:5.2.1\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/651", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/651/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/651/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/651/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/651", "id": 477399410, "node_id": "MDU6SXNzdWU0NzczOTk0MTA=", "number": 651, "title": "Can't find end offset / Offset won't committed by producer", "user": {"login": "eeepmb", "id": 52211100, "node_id": "MDQ6VXNlcjUyMjExMTAw", "avatar_url": "https://avatars1.githubusercontent.com/u/52211100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eeepmb", "html_url": "https://github.com/eeepmb", "followers_url": "https://api.github.com/users/eeepmb/followers", "following_url": "https://api.github.com/users/eeepmb/following{/other_user}", "gists_url": "https://api.github.com/users/eeepmb/gists{/gist_id}", "starred_url": "https://api.github.com/users/eeepmb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eeepmb/subscriptions", "organizations_url": "https://api.github.com/users/eeepmb/orgs", "repos_url": "https://api.github.com/users/eeepmb/repos", "events_url": "https://api.github.com/users/eeepmb/events{/privacy}", "received_events_url": "https://api.github.com/users/eeepmb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rnpridgeon", "id": 5313961, "node_id": "MDQ6VXNlcjUzMTM5NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5313961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rnpridgeon", "html_url": "https://github.com/rnpridgeon", "followers_url": "https://api.github.com/users/rnpridgeon/followers", "following_url": "https://api.github.com/users/rnpridgeon/following{/other_user}", "gists_url": "https://api.github.com/users/rnpridgeon/gists{/gist_id}", "starred_url": "https://api.github.com/users/rnpridgeon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rnpridgeon/subscriptions", "organizations_url": "https://api.github.com/users/rnpridgeon/orgs", "repos_url": "https://api.github.com/users/rnpridgeon/repos", "events_url": "https://api.github.com/users/rnpridgeon/events{/privacy}", "received_events_url": "https://api.github.com/users/rnpridgeon/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rnpridgeon", "id": 5313961, "node_id": "MDQ6VXNlcjUzMTM5NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5313961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rnpridgeon", "html_url": "https://github.com/rnpridgeon", "followers_url": "https://api.github.com/users/rnpridgeon/followers", "following_url": "https://api.github.com/users/rnpridgeon/following{/other_user}", "gists_url": "https://api.github.com/users/rnpridgeon/gists{/gist_id}", "starred_url": "https://api.github.com/users/rnpridgeon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rnpridgeon/subscriptions", "organizations_url": "https://api.github.com/users/rnpridgeon/orgs", "repos_url": "https://api.github.com/users/rnpridgeon/repos", "events_url": "https://api.github.com/users/rnpridgeon/events{/privacy}", "received_events_url": "https://api.github.com/users/rnpridgeon/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2019-08-06T14:00:52Z", "updated_at": "2019-08-13T09:42:50Z", "closed_at": "2019-08-13T09:10:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nHi guys,\r\ndue to performace reasons I started moving from the kafka-python lib to this one and right now I'm facing some odd behaviours. \r\nIn general my script works like this; I have an input topic (a KSQL stream) from which the script consumes messages. The script then runs through some logic and eventually sends the message to an output topic. Whenever the script restarts it shall get each last message from input and output topic and compare them to acknowledge nothing gets lost. \r\n\r\nHowever I'm unable to receive the latest offset of the output topic and it drives me nuts.\r\nI have the feeling that something is wrong with the confluent producer in my script because when I first run the `kafka example` (second code snippet) and then the `confluent example` the offset for the output topic changes. If I process messages with the `confluent example` the offset stays untouched. \r\n\r\nI tried to trim the scripts as good as possible and hope the snippets are not to long. If needed I can also provide logs of the two scripts. \r\n\r\n\r\nHow to reproduce\r\n================\r\nSee my description above and the code below. \r\n\r\n```\r\n# (1) Example with confluent_kafka\r\n\r\nfrom confluent_kafka import Producer, Consumer, TopicPartition, KafkaError, KafkaException\r\nfrom confluent_kafka.admin import ClusterMetadata, TopicMetadata\r\nimport kafka\r\nimport logging\r\nimport json\r\nimport os\r\nimport sys\r\nimport signal\r\nimport time\r\n\r\n# create logger\r\nmodule_logger = logging.getLogger('confluent_application.Verify')\r\nrecord_metadata = []\r\n\r\nclass Verify():\r\n    def __init__(self, config):\r\n        self.logger = logging.getLogger('confluent_application.Verify')\r\n        self.logger.info('Verify __init__')\r\n\r\n        self.workerID=config['KAFKA']['WORKER_ID']\r\n        self.workerGroupID = f'verify_{self.workerID}'\r\n\r\n        inputServers=config['KAFKA']['INPUT']['BOOTSTRAP_SERVERS']\r\n        self.inputTopic=config['KAFKA']['INPUT']['TOPIC']\r\n\r\n        outputServers=config['KAFKA']['OUTPUT']['BOOTSTRAP_SERVERS']\r\n        self.outputTopic=config['KAFKA']['OUTPUT']['TOPIC']\r\n\r\n        inputConsumerConf = {'bootstrap.servers': inputServers,\r\n                             'group.id': self.workerGroupID\r\n                            }\r\n        self.inputConsumer = Consumer(inputConsumerConf, logger=self.logger)\r\n\r\n\r\n        outputConsumerConf = {'bootstrap.servers': outputServers,\r\n                             'group.id': self.workerGroupID,\r\n                             'auto.offset.reset': 'latest',\r\n                             'enable.auto.commit': True,\r\n                             'enable.auto.offset.store': True\r\n                            }\r\n        self.outputConsumer = Consumer(outputConsumerConf, logger=self.logger)\r\n\r\n        producerConf = {'bootstrap.servers': outputServers\r\n                        }\r\n        self.producer = Producer(**producerConf, logger=self.logger)\r\n\r\n        self.logger.info('Consumers and producer started.')\r\n        \r\n        inputTopics=self.inputConsumer.list_topics(topic=self.inputTopic, timeout=10)\r\n        outputTopics=self.outputConsumer.list_topics(topic=self.outputTopic, timeout=10)\r\n        partitionNumInput = len(inputTopics.topics[self.inputTopic].partitions)\r\n        partitionNumOutput = len(outputTopics.topics[self.outputTopic].partitions)\r\n\r\n        #check if num partitions match and workerID is a valid value.\r\n        if((partitionNumInput == partitionNumOutput) and (self.workerID <= partitionNumInput)):\r\n            # Assign partitions of input & output topics to worker\r\n            self.logger.info('Accepted. Assigning partitions.')\r\n\r\n            # assigning inputConsumer\r\n            # TODO should adapt functions as soon as they are available in the confluent package\r\n            kac = kafka.KafkaAdminClient(bootstrap_servers=inputServers, client_id=self.workerGroupID)\r\n            inputTopicPartitionOffsets=kac.list_consumer_group_offsets(group_id=self.workerGroupID, partitions=[kafka.TopicPartition(self.inputTopic, self.workerID)])\r\n            self.inputOffset=inputTopicPartitionOffsets[kafka.TopicPartition(self.inputTopic, self.workerID)][0]\r\n            self.inputConsumer.assign([TopicPartition(self.inputTopic, self.workerID, self.inputOffset)])\r\n            inputAssignedPartitions = self.inputConsumer.assignment()\r\n            self.logger.info(f'inputConsumer assigned to: {inputAssignedPartitions}')\r\n\r\n            # assigning outputConsumer\r\n            self.outputOffset = self.outputConsumer.get_watermark_offsets(TopicPartition(self.outputTopic, self.workerID))[1]\r\n            self.outputConsumer.assign([TopicPartition(self.outputTopic, self.workerID, self.outputOffset)])\r\n            outputAssignedPartitions = self.outputConsumer.assignment()\r\n            self.logger.info(f'outputConsumer assigned to: {outputAssignedPartitions}')\r\n        else:\r\n            self.logger.error(f\"Sorry, num of input ({partitionNumInput}) and output ({partitionNumOutput}) topic partitions don't match or worker ID ({self.workerID}) is not a valid value. Exiting now.\")\r\n            sys.exit()\r\n\r\n\r\n    def acknowledgeLastEntry(self):\r\n        self.inputConsumer.seek(TopicPartition(topic=self.inputTopic, partition=self.workerID, offset=self.inputOffset-1))\r\n        messageIn = self.inputConsumer.consume(num_messages=1, timeout=10)[0]\r\n        my_json=messageIn.value().decode('utf8').replace(\"'\", '\"')\r\n        hdk=json.loads(my_json)['HDK'].encode('utf-8')\r\n        self.logger.info(f'Input: {messageIn.key()} - {hdk}')\r\n\r\n        self.outputConsumer.seek(TopicPartition(topic=self.outputTopic, partition=self.workerID, offset=self.outputOffset-1))\r\n        messageOut = self.outputConsumer.consume(num_messages=1, timeout=10)[0]\r\n        self.logger.info(f'Output: {messageOut.key()} - {messageOut.value()}')\r\n        self.outputConsumer.close()\r\n            \r\n        self.logger.info(\"Acknowledge last entry.\")\r\n        \r\n        if((messageIn.key() != messageOut.key()) or (hdk != messageOut.value())):\r\n            try:\r\n                self.producer.produce(topic=self.outputTopic, value=hdk, key=messageIn.key(), callback=delivery_callback)\r\n            except BufferError:\r\n                self.logger.info('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' %len(self.producer))\r\n\r\n            self.logger.info(\"Had to send message to output.\")\r\n\r\n\r\n    def restoreState(self):\r\n        self.logger.info('starting restoreState')\r\n\r\n        if(self.outputOffset <= 0 ):\r\n            self.logger.info(\"Congrats! You're the first one. Let's go ..\")\r\n            \r\n        else:\r\n            self.acknowledgeLastEntry()\r\n            \r\n        self.logger.info('finished restoreState')\r\n\r\n\r\n    def runVerifier(self):\r\n        self.logger.info(\"Starting runVerifier now ..\")\r\n\r\n        print((\"Starting stream now ..\\n\"))\r\n\r\n        try:\r\n            while True:\r\n                message = self.inputConsumer.poll(timeout=10)\r\n\r\n                if message is None:\r\n                    continue\r\n                if message.error():\r\n                    raise KafkaException(message.error())\r\n                else:\r\n                    # Proper message\r\n                    self.logger.info(f'Received message. Offset: {message.offset()}')\r\n                    self.inputOffset = message.offset()\r\n\r\n                    my_json=message.value().decode('utf8').replace(\"'\", '\"')\r\n                    hdk=json.loads(my_json)['HDK'].encode('utf-8')\r\n\r\n                    # Do some logic before writeEntry\r\n            self.logger.info('Do some logic.')\r\n                    pass\r\n                    self.writeEntry(key=message.key(), value=hdk)\r\n\r\n                    self.logger.info(f'Message sucessfully processed.')\r\n\r\n        except KeyboardInterrupt:\r\n            self.logger.info('Aborted by user')\r\n\r\n        finally:\r\n            # Close down consumer to commit final offsets.\r\n            self.inputConsumer.close()\r\n            self.logger.info(f'Last offsets:\\nInput: {self.inputOffset}\\nOutput: {self.outputOffset}\\n\\n')\r\n\r\n\r\n    def writeEntry(self, key, value):\r\n        self.logger.info(f'Writing entry.')\r\n\r\n        try:\r\n            self.logger.info(f'Sending message ...')\r\n            self.producer.produce(topic=self.outputTopic, value=value, key=key, callback=delivery_callback)\r\n\r\n        except BufferError:\r\n            self.logger.info('%% Local producer queue is full (%d messages awaiting delivery): try again' %len(self.producer))\r\n            \r\n        self.producer.poll(timeout=10)\r\n\r\n        self.logger.info('%% Waiting for %d deliveries' % len(self.producer))\r\n\r\n        self.producer.flush()\r\n        self.outputOffset = record_metadata[0].offset()\r\n        record_metadata.clear()\r\n\r\ndef delivery_callback(err, msg):\r\n    if err:\r\n        module_logger.error(f'Message failed delivery: {err}')\r\n    else:\r\n        record_metadata.append(msg)\r\n        module_logger.info(f'Message delivered @ {msg.offset()}.')\r\n\r\n\r\ndef main():\r\n    logger = logging.getLogger('confluent_application')\r\n    logger.setLevel(logging.DEBUG)\r\n    fh = logging.FileHandler('confluent_verify.log')\r\n    fh.setLevel(logging.DEBUG)\r\n    ch = logging.StreamHandler()\r\n    ch.setLevel(logging.ERROR)\r\n    formatter = logging.Formatter('%(asctime)s - %(levelname)6s - %(lineno)4s:%(name)s.%(funcName)s - %(message)s')\r\n    fh.setFormatter(formatter)\r\n    ch.setFormatter(formatter)\r\n\r\n    logger.addHandler(fh)\r\n    logger.addHandler(ch)\r\n\r\n    logger.info('Logger configured and started. Running main now.')\r\n\r\n    configfile = sys.argv[1]\r\n\r\n    logger.info('Opening and loading cfg file.')\r\n    with open(configfile, 'r') as f:\r\n        config = json.load(f)\r\n    f.close()\r\n    logger.info('Done.')\r\n\r\n    logger.info('Initializing an instance of Verify')\r\n    verifyWorker = Verify(config)\r\n    logger.info('Done. Restoring state now.')\r\n    verifyWorker.restoreState()\r\n    logger.info('Calling verifyWorker.runVerifier.')\r\n    verifyWorker.runVerifier()\r\n    logger.info('Finished verifyWorker.runVerifier.\\n\\n')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n\r\n\r\n\r\n\r\n```\r\n# (2) Example with kafka-python \r\n\r\nfrom kafka import KafkaProducer, KafkaConsumer, TopicPartition, KafkaAdminClient\r\nfrom kafka.errors import KafkaError\r\nimport logging\r\nimport json\r\nimport os\r\nimport sys\r\nimport signal\r\nimport time\r\n\r\n# create logger\r\nmodule_logger = logging.getLogger('kafka_application.Verify')\r\n\r\ndef on_send_success(record_metadata):\r\n    record_metadata.topic\r\n    record_metadata.partition\r\n    record_metadata.offset\r\n\r\ndef on_send_error(self, excp):\r\n    self.log.error('I am an errback', exc_info=excp)\r\n    # handle exception\r\n\r\nclass Verify():\r\n\r\n    def __init__(self, config):\r\n        self.logger = logging.getLogger('kafka_application.Verify')\r\n        self.logger.info('Verify __init__')\r\n\r\n        self.workerID=config['KAFKA']['WORKER_ID']\r\n        self.workerGroupID = f'rocky_{self.workerID}'\r\n\r\n        inputServers=[config['KAFKA']['INPUT']['BOOTSTRAP_SERVERS']]\r\n        self.inputTopic=config['KAFKA']['INPUT']['TOPIC']\r\n\r\n        outputServers=[config['KAFKA']['OUTPUT']['BOOTSTRAP_SERVERS']]\r\n        self.outputTopic=config['KAFKA']['OUTPUT']['TOPIC']\r\n\r\n        self.inputConsumer = KafkaConsumer(\r\n                                group_id=self.workerGroupID,\r\n                                bootstrap_servers=inputServers,\r\n                                enable_auto_commit=True)\r\n\r\n        self.outputConsumer = KafkaConsumer(\r\n                                group_id=self.workerGroupID,\r\n                                bootstrap_servers=outputServers,\r\n                                enable_auto_commit=True)\r\n\r\n        self.producer = KafkaProducer(bootstrap_servers=outputServers)\r\n\r\n        self.logger.info('Consumers and producer started.')\r\n        \r\n        # need to do this because partitions_for_topic is blocking\r\n        self.inputConsumer.topics()\r\n        self.outputConsumer.topics()\r\n        partitionNumInput = len(self.inputConsumer.partitions_for_topic(self.inputTopic))\r\n        partitionNumOutput = len(self.outputConsumer.partitions_for_topic(self.outputTopic))\r\n\r\n        #check if num partitions match and workerID is a valid value\r\n        if((partitionNumInput == partitionNumOutput) and (self.workerID <= partitionNumInput)):\r\n            self.logger.info('Accepted. Assigning partitions.')\r\n\r\n            # assigning inputConsumer\r\n            self.assignedInputTopicPartition=TopicPartition(self.inputTopic, self.workerID)\r\n            kac = KafkaAdminClient(bootstrap_servers=inputServers, client_id=self.workerGroupID)\r\n            inputTopicPartitionOffsets=kac.list_consumer_group_offsets(group_id=self.workerGroupID, partitions=[self.assignedInputTopicPartition])\r\n            self.inputOffset=inputTopicPartitionOffsets[self.assignedInputTopicPartition][0]\r\n            self.inputConsumer.assign([self.assignedInputTopicPartition])\r\n            self.logger.info(f'assignedInputTopicPartition: {self.assignedInputTopicPartition}')\r\n            self.logger.info(f'inputTopicPartitionOffsets: {inputTopicPartitionOffsets}')\r\n            self.logger.info(f'inputTopicOffset: {self.inputOffset}')\r\n\r\n            # assigning outputConsumer\r\n            self.assignedOutputTopicPartition=TopicPartition(self.outputTopic, self.workerID)\r\n            self.outputConsumer.assign([self.assignedOutputTopicPartition])\r\n            outputTopicPartitionOffsets=self.outputConsumer.end_offsets([self.assignedOutputTopicPartition])\r\n            self.outputOffset=outputTopicPartitionOffsets[self.assignedOutputTopicPartition]\r\n            self.logger.info(f'assignedOutputTopicPartition: {self.assignedOutputTopicPartition}')\r\n            self.logger.info(f'outputTopicPartitionOffsets: {outputTopicPartitionOffsets}')\r\n            self.logger.info(f'outputTopicOffset: {self.outputOffset}')\r\n\r\n            self.restoreState()\r\n        else:\r\n            self.logger.error(f\"Sorry, num of input ({partitionNumInput}) and output ({partitionNumOutput}) topic partitions don't match or worker ID ({self.workerID}) is not a valid value. Exiting now.\")\r\n            sys.exit()\r\n\r\n    def restoreState(self):\r\n        self.logger.info('starting restoreState')\r\n\r\n        if(self.outputOffset == 0 ):\r\n            self.logger.info(\"Congrats! You're the first one. Let's go ..\")\r\n\r\n        else:\r\n            self.acknowledgeLastEntry()\r\n            \r\n        self.logger.info('finished restoreState')\r\n\r\n    def acknowledgeLastEntry(self):\r\n        self.inputConsumer.seek(partition=self.assignedInputTopicPartition, offset=self.inputOffset-1)\r\n\r\n        for messageIn in self.inputConsumer:\r\n            my_json=messageIn.value.decode('utf8').replace(\"'\", '\"')\r\n            hdk=json.loads(my_json)['HDK'].encode('utf-8')\r\n            self.logger.info(f'Input: {messageIn.key} - {hdk}')\r\n            break\r\n\r\n        self.outputConsumer.seek(partition=self.assignedOutputTopicPartition, offset=self.outputOffset-1)\r\n\r\n        for messageOut in self.outputConsumer:\r\n            self.logger.info(f'Output: {messageOut.key} - {messageOut.value}')\r\n            break\r\n            \r\n        self.logger.info(\"Acknowledge last entry.\")\r\n        \r\n        if((messageIn.key != messageOut.key) or (hdk != messageOut.value)):\r\n            future = self.producer.send(self.outputTopic, key=messageIn.key, value=hdk, partition=self.workerID)\r\n\r\n            try:\r\n                record_metadata = future.get(timeout=10)\r\n            except KafkaError:\r\n                # Decide what to do if produce request failed...\r\n                self.logger.exception(\"Exception occurred\")\r\n                pass\r\n\r\n            self.logger.info(\"Had to send message to output.\")\r\n                \r\n\r\n    def runVerifier(self):\r\n        self.logger.info(\"Starting runVerifier now ..\")\r\n\r\n        print((\"Starting stream now ..\"))\r\n\r\n        for message in self.inputConsumer:\r\n            self.logger.info(f'Received message. Offset: {message.offset}')\r\n            self.inputOffset = message.offset\r\n\r\n            my_json=message.value.decode('utf8').replace(\"'\", '\"')\r\n            hdk=json.loads(my_json)['HDK'].encode('utf-8')\r\n\r\n            # Do some logic before writeEntry\r\n            self.logger.info('Do some logic.')\r\n            pass\r\n            self.writeEntry(key=message.key, value=hdk)\r\n            \r\n            self.logger.info(f'Message processed')\r\n\r\n\r\n    def writeEntry(self, key, value):\r\n        self.logger.info(f'Writing entry. {key} - {value}')\r\n\r\n        self.logger.info(f'Sending message ...')\r\n        future = self.producer.send(self.outputTopic, key=key, value=value, partition=self.workerID)\r\n\r\n        try:\r\n            record_metadata = future.get(timeout=10)\r\n        except KafkaError:\r\n            self.logger.exception()\r\n            pass\r\n\r\n        self.logger.info(f'Message delivered @ {record_metadata.offset}')\r\n        self.outputOffset = record_metadata.offset\r\n\r\ndef main():\r\n    logger = logging.getLogger('kafka_application')\r\n    logger.setLevel(logging.DEBUG)\r\n    fh = logging.FileHandler('kafka_verify.log')\r\n    fh.setLevel(logging.DEBUG)\r\n    ch = logging.StreamHandler()\r\n    ch.setLevel(logging.ERROR)\r\n    formatter = logging.Formatter('%(asctime)s - %(levelname)6s - %(lineno)4s:%(name)s.%(funcName)s - %(message)s')\r\n    fh.setFormatter(formatter)\r\n    ch.setFormatter(formatter)\r\n\r\n    logger.addHandler(fh)\r\n    logger.addHandler(ch)\r\n\r\n    logger.info('Logger configured and started. Running main now.')\r\n    try:\r\n        configfile = sys.argv[1]\r\n\r\n        logger.info('Opening and loading cfg file.')\r\n        with open(configfile, 'r') as f:\r\n            config = json.load(f)\r\n        logger.info('Done.')\r\n\r\n        logger.info('creating an instance of Verify')\r\n        verifyWorker = Verify(config)\r\n        logger.info('Done.')\r\n        logger.info('Calling verifyWorker.runVerifier.')\r\n        verifyWorker.runVerifier()\r\n        logger.info('finished verifyWorker.runVerifier.\\n\\n')\r\n\r\n    except KeyboardInterrupt:\r\n        logger.info(\"Aborted by user.\")\r\n    finally:\r\n        verifyWorker.inputConsumer.close()\r\n        verifyWorker.outputConsumer.close()\r\n        logger.info(f'Last offsets:\\nInput: {verifyWorker.inputOffset}\\nOutput: {verifyWorker.outputOffset}\\n')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python (1.1.0) and librdkafka version (`confluent_kafka.version(5.2.0)` and `confluent_kafka.libversion(librdkafka v1.0.0)`):\r\n - [x] Apache Kafka broker version: 2.2.0\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: macOS 10.14.6\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/649", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/649/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/649/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/649/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/649", "id": 474828332, "node_id": "MDU6SXNzdWU0NzQ4MjgzMzI=", "number": 649, "title": "Alpine pip install failing due to version mismatch (\"confluent-kafka-python requires librdkafka v1.0.0 or later)", "user": {"login": "ahonnecke", "id": 419355, "node_id": "MDQ6VXNlcjQxOTM1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/419355?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahonnecke", "html_url": "https://github.com/ahonnecke", "followers_url": "https://api.github.com/users/ahonnecke/followers", "following_url": "https://api.github.com/users/ahonnecke/following{/other_user}", "gists_url": "https://api.github.com/users/ahonnecke/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahonnecke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahonnecke/subscriptions", "organizations_url": "https://api.github.com/users/ahonnecke/orgs", "repos_url": "https://api.github.com/users/ahonnecke/repos", "events_url": "https://api.github.com/users/ahonnecke/events{/privacy}", "received_events_url": "https://api.github.com/users/ahonnecke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-07-30T21:25:16Z", "updated_at": "2019-08-08T15:18:51Z", "closed_at": "2019-08-08T15:18:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nCannot effect a pip install of confluent-kafka on alpine.  It seems as though the pip install is failing to find the alpine librdkafka package.\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n```\r\n/app # apk add librdkafka-dev\r\nOK: 200 MiB in 49 packages\r\n/app # apk add librdkafka\r\nOK: 200 MiB in 49 packages\r\n/app # pipenv install confluent-kafka\r\n/app # pipenv install confluent-kafka\r\nCreating a Pipfile for this project\u2026\r\nInstalling confluent-kafka\u2026\r\nAdding confluent-kafka to Pipfile's [packages]\u2026\r\n\u2714 Installation Succeeded \r\nPipfile.lock not found, creating\u2026\r\nLocking [dev-packages] dependencies\u2026\r\nLocking [packages] dependencies\u2026\r\n\u2714 Success! \r\nUpdated Pipfile.lock (6928a6)!\r\nInstalling dependencies from Pipfile.lock (6928a6)\u2026\r\n  \ud83d\udc0d   \u2589An error occurred while installing confluent-kafka==1.1.0 --hash=sha256:13d0146850c046b9e3dabbcb60bd7c4f02c7d4754b693266ee5bb0884faea2c7 --hash=sha256:1505de9c652f9b841ba600f35e6a7f4cae2e239e9b8255212433756d17c1aeed --hash=sha256:1832373eee96b0ef246c773ec2613c382bf4577f0d42e2ce688e8d35ca373e69 --hash=sha256:3001d09e5efa357eb9f3166ef54703166d7b662eb55841b760a346313ca717ee --hash=sha256:31b934aa821e4d6aff06dd260b14c03ca58a532bb160032acff573e2de0a4467 --hash=sha256:359d776fb0381147e4c9981676d3907f9dbd12733597a6fec455e5a516728ae2 --hash=sha256:472f63ee352a1464ff40325cc3a86aae2685716985bb2c9cad7e574f4203d664 --hash=sha256:53ec15b8b76109489c77c0fe3d2f5ff71398cef2db74053c4818f6667a14471d --hash=sha256:6299420c462b274095d4624cf246dc90e49d6d3c1acd8b7222d39eb407476301 --hash=sha256:8a497aee87d01891090c9c59ee760387bc41c718a358dadc926a6cae36dfa4fa --hash=sha256:8bd48e88b2d9ba42b58e8c5c1a266a4a7945757d1ae1c2bad3c0fbdf7b52c5e5 --hash=sha256:92684d50215c111025ebb40c813f695fd9df69f763f04e9b25fc4fd67e5c0d06 --hash=sha256:957417782592136c91c81d7161be8c6fc334f65d229a672d18c4cc85a0a09532 --hash=sha256:a41eda5c84e153e0ec8d854c064ebe242a8a6993f9069f6ae83dfd5f9dda1ac6 --hash=sha256:b4e5b7f5f597d4a40e0148e1fe509ab800231542ce7e8765265e8c251e6e3983 --hash=sha256:c41d12f4e1deb54e1fa6e40260d95aa7b1c1ebfa4bbddb8a03f06fde3611da71 --hash=sha256:c8b75e6e803e1a4346d12f212ead3871f86cf21e4d6b5028e8701f8e9c8f105b --hash=sha256:c8dee478a46a9352224fce1d12756cff6cf50e2684121a118141c7908ae9ed3e --hash=sha256:d072825c0f9dec85ebc5eaa887c324914cb3631c884b35fc3c173663d4222a3a --hash=sha256:d328d11edfe049cbdd6bf0792bcff9a9afce26c25124f198fe0ea1f910b529b3 --hash=sha256:d52708474daec030399cf45a6bff55b13d631cdba6cc6546da351f675a901d44 --hash=sha256:dd02a60145961957ad293aed30a435de2069024aec134575bea937e50df211b3 --hash=sha256:eb900ef15b340a87185412b906cbef2450ab68907b84988a6af7fc8e1b017b6c --hash=sha256:fa5d985e30fbcbccaf8b9b1e52137abfbbe0f913fc1baddb96b9476824013411 --hash=sha256:fb8ba7ddf85c80eb4d2e3c2f1b362793b9f5c78950bc1e93f60423241071fd9b --hash=sha256:ff80ceca738f52d9058d146b7b063092a88281362606e6c917af2a9e60087a3d! Will try again.\r\n  \ud83d\udc0d   \u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589 1/1 \u2014 00:00:02\r\nInstalling initially failed dependencies\u2026\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 1992, in do_install\r\n[pipenv.exceptions.InstallError]:       skip_lock=skip_lock,\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 1253, in do_init\r\n[pipenv.exceptions.InstallError]:       pypi_mirror=pypi_mirror,\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 859, in do_install_dependencies\r\n[pipenv.exceptions.InstallError]:       retry_list, procs, failed_deps_queue, requirements_dir, **install_kwargs\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 763, in batch_install\r\n[pipenv.exceptions.InstallError]:       _cleanup_procs(procs, not blocking, failed_deps_queue, retry=retry)\r\n[pipenv.exceptions.InstallError]:   File \"/usr/local/lib/python3.6/site-packages/pipenv/core.py\", line 681, in _cleanup_procs\r\n[pipenv.exceptions.InstallError]:       raise exceptions.InstallError(c.dep.name, extra=err_lines)\r\n[pipenv.exceptions.InstallError]: ['Collecting confluent-kafka==1.1.0 (from -r /tmp/pipenv-zv9o0wzm-requirements/pipenv-3p2qy9ad-requirement.txt (line 1))', '  Using cached https://files.pythonhosted.org/packages/c7/27/e7f6d54dafb050dcb66622742d8a39c5742ca6aa00c337b043738da78abf/confluent-kafka-1.1.0.tar.gz', 'Building wheels for collected packages: confluent-kafka', '  Building wheel for confluent-kafka (setup.py): started', \"  Building wheel for confluent-kafka (setup.py): finished with status 'error'\", '  Running setup.py clean for confluent-kafka', 'Failed to build confluent-kafka', 'Installing collected packages: confluent-kafka', '  Running setup.py install for confluent-kafka: started', \"    Running setup.py install for confluent-kafka: finished with status 'error'\"]\r\n[pipenv.exceptions.InstallError]: ['ERROR: Command errored out with exit status 1:', '   command: /root/.local/share/virtualenvs/app-4PlAip0Q/bin/python -u -c \\'import sys, setuptools, tokenize; sys.argv[0] = \\'\"\\'\"\\'/tmp/pip-install-4ewf1g71/confluent-kafka/setup.py\\'\"\\'\"\\'; __file__=\\'\"\\'\"\\'/tmp/pip-install-4ewf1g71/confluent-kafka/setup.py\\'\"\\'\"\\';f=getattr(tokenize, \\'\"\\'\"\\'open\\'\"\\'\"\\', open)(__file__);code=f.read().replace(\\'\"\\'\"\\'\\\\r\\\\n\\'\"\\'\"\\', \\'\"\\'\"\\'\\\\n\\'\"\\'\"\\');f.close();exec(compile(code, __file__, \\'\"\\'\"\\'exec\\'\"\\'\"\\'))\\' bdist_wheel -d /tmp/pip-wheel-v1v69s67 --python-tag cp36', '       cwd: /tmp/pip-install-4ewf1g71/confluent-kafka/', '  Complete output (43 lines):', '  running bdist_wheel', '  running build', '  running build_py', '  creating build', '  creating build/lib.linux-x86_64-3.6', '  creating build/lib.linux-x86_64-3.6/confluent_kafka', '  copying confluent_kafka/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka', '  creating build/lib.linux-x86_64-3.6/confluent_kafka/admin', '  copying confluent_kafka/admin/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/admin', '  creating build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '  copying confluent_kafka/kafkatest/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '  copying confluent_kafka/kafkatest/verifiable_producer.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '  copying confluent_kafka/kafkatest/verifiable_consumer.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '  copying confluent_kafka/kafkatest/verifiable_client.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '  creating build/lib.linux-x86_64-3.6/confluent_kafka/avro', '  copying confluent_kafka/avro/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '  copying confluent_kafka/avro/cached_schema_registry_client.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '  copying confluent_kafka/avro/load.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '  copying confluent_kafka/avro/error.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '  creating build/lib.linux-x86_64-3.6/confluent_kafka/avro/serializer', '  copying confluent_kafka/avro/serializer/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro/serializer', '  copying confluent_kafka/avro/serializer/message_serializer.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro/serializer', '  warning: build_py: byte-compiling is disabled, skipping.', '  ', '  running build_ext', \"  building 'confluent_kafka.cimpl' extension\", '  creating build/temp.linux-x86_64-3.6', '  creating build/temp.linux-x86_64-3.6/confluent_kafka', '  creating build/temp.linux-x86_64-3.6/confluent_kafka/src', '  gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DTHREAD_STACK_SIZE=0x100000 -fPIC -I/usr/local/include/python3.6m -c confluent_kafka/src/confluent_kafka.c -o build/temp.linux-x86_64-3.6/confluent_kafka/src/confluent_kafka.o', '  In file included from confluent_kafka/src/confluent_kafka.c:17:0:', '  confluent_kafka/src/confluent_kafka.h:55:2: error: #error \"confluent-kafka-python requires librdkafka v1.0.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"', '   #error \"confluent-kafka-python requires librdkafka v1.0.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"', '    ^~~~~', \"  confluent_kafka/src/confluent_kafka.c: In function 'error_cb':\", \"  confluent_kafka/src/confluent_kafka.c:1226:20: error: 'RD_KAFKA_RESP_ERR__FATAL' undeclared (first use in this function)\", '           if (err == RD_KAFKA_RESP_ERR__FATAL) {', '                      ^~~~~~~~~~~~~~~~~~~~~~~~', '  confluent_kafka/src/confluent_kafka.c:1226:20: note: each undeclared identifier is reported only once for each function it appears in', \"  confluent_kafka/src/confluent_kafka.c:1228:23: warning: implicit declaration of function 'rd_kafka_fatal_error' [-Wimplicit-function-declaration]\", '                   err = rd_kafka_fatal_error(rk, errstr, sizeof(errstr));', '                         ^~~~~~~~~~~~~~~~~~~~', \"  error: command 'gcc' failed with exit status 1\", '  ----------------------------------------', '  ERROR: Failed building wheel for confluent-kafka', '    ERROR: Command errored out with exit status 1:', '     command: /root/.local/share/virtualenvs/app-4PlAip0Q/bin/python -u -c \\'import sys, setuptools, tokenize; sys.argv[0] = \\'\"\\'\"\\'/tmp/pip-install-4ewf1g71/confluent-kafka/setup.py\\'\"\\'\"\\'; __file__=\\'\"\\'\"\\'/tmp/pip-install-4ewf1g71/confluent-kafka/setup.py\\'\"\\'\"\\';f=getattr(tokenize, \\'\"\\'\"\\'open\\'\"\\'\"\\', open)(__file__);code=f.read().replace(\\'\"\\'\"\\'\\\\r\\\\n\\'\"\\'\"\\', \\'\"\\'\"\\'\\\\n\\'\"\\'\"\\');f.close();exec(compile(code, __file__, \\'\"\\'\"\\'exec\\'\"\\'\"\\'))\\' install --record /tmp/pip-record-p7gfnkl8/install-record.txt --single-version-externally-managed --compile --install-headers /root/.local/share/virtualenvs/app-4PlAip0Q/include/site/python3.6/confluent-kafka', '         cwd: /tmp/pip-install-4ewf1g71/confluent-kafka/', '    Complete output (43 lines):', '    running install', '    running build', '    running build_py', '    creating build', '    creating build/lib.linux-x86_64-3.6', '    creating build/lib.linux-x86_64-3.6/confluent_kafka', '    copying confluent_kafka/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka', '    creating build/lib.linux-x86_64-3.6/confluent_kafka/admin', '    copying confluent_kafka/admin/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/admin', '    creating build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '    copying confluent_kafka/kafkatest/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '    copying confluent_kafka/kafkatest/verifiable_producer.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '    copying confluent_kafka/kafkatest/verifiable_consumer.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '    copying confluent_kafka/kafkatest/verifiable_client.py -> build/lib.linux-x86_64-3.6/confluent_kafka/kafkatest', '    creating build/lib.linux-x86_64-3.6/confluent_kafka/avro', '    copying confluent_kafka/avro/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '    copying confluent_kafka/avro/cached_schema_registry_client.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '    copying confluent_kafka/avro/load.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '    copying confluent_kafka/avro/error.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro', '    creating build/lib.linux-x86_64-3.6/confluent_kafka/avro/serializer', '    copying confluent_kafka/avro/serializer/__init__.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro/serializer', '    copying confluent_kafka/avro/serializer/message_serializer.py -> build/lib.linux-x86_64-3.6/confluent_kafka/avro/serializer', '    warning: build_py: byte-compiling is disabled, skipping.', '    ', '    running build_ext', \"    building 'confluent_kafka.cimpl' extension\", '    creating build/temp.linux-x86_64-3.6', '    creating build/temp.linux-x86_64-3.6/confluent_kafka', '    creating build/temp.linux-x86_64-3.6/confluent_kafka/src', '    gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -DTHREAD_STACK_SIZE=0x100000 -fPIC -I/usr/local/include/python3.6m -c confluent_kafka/src/confluent_kafka.c -o build/temp.linux-x86_64-3.6/confluent_kafka/src/confluent_kafka.o', '    In file included from confluent_kafka/src/confluent_kafka.c:17:0:', '    confluent_kafka/src/confluent_kafka.h:55:2: error: #error \"confluent-kafka-python requires librdkafka v1.0.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"', '     #error \"confluent-kafka-python requires librdkafka v1.0.0 or later. Install the latest version of librdkafka from the Confluent repositories, see http://docs.confluent.io/current/installation.html\"', '      ^~~~~', \"    confluent_kafka/src/confluent_kafka.c: In function 'error_cb':\", \"    confluent_kafka/src/confluent_kafka.c:1226:20: error: 'RD_KAFKA_RESP_ERR__FATAL' undeclared (first use in this function)\", '             if (err == RD_KAFKA_RESP_ERR__FATAL) {', '                        ^~~~~~~~~~~~~~~~~~~~~~~~', '    confluent_kafka/src/confluent_kafka.c:1226:20: note: each undeclared identifier is reported only once for each function it appears in', \"    confluent_kafka/src/confluent_kafka.c:1228:23: warning: implicit declaration of function 'rd_kafka_fatal_error' [-Wimplicit-function-declaration]\", '                     err = rd_kafka_fatal_error(rk, errstr, sizeof(errstr));', '                           ^~~~~~~~~~~~~~~~~~~~', \"    error: command 'gcc' failed with exit status 1\", '    ----------------------------------------', 'ERROR: Command errored out with exit status 1: /root/.local/share/virtualenvs/app-4PlAip0Q/bin/python -u -c \\'import sys, setuptools, tokenize; sys.argv[0] = \\'\"\\'\"\\'/tmp/pip-install-4ewf1g71/confluent-kafka/setup.py\\'\"\\'\"\\'; __file__=\\'\"\\'\"\\'/tmp/pip-install-4ewf1g71/confluent-kafka/setup.py\\'\"\\'\"\\';f=getattr(tokenize, \\'\"\\'\"\\'open\\'\"\\'\"\\', open)(__file__);code=f.read().replace(\\'\"\\'\"\\'\\\\r\\\\n\\'\"\\'\"\\', \\'\"\\'\"\\'\\\\n\\'\"\\'\"\\');f.close();exec(compile(code, __file__, \\'\"\\'\"\\'exec\\'\"\\'\"\\'))\\' install --record /tmp/pip-record-p7gfnkl8/install-record.txt --single-version-externally-managed --compile --install-headers /root/.local/share/virtualenvs/app-4PlAip0Q/include/site/python3.6/confluent-kafka Check the logs for full command output.']\r\nERROR: ERROR: Package installation failed...\r\n  \u2624  \u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589 0/1 \u2014 00:00:02\r\n\r\n```\r\n\r\n\r\nOutput\r\n=========\r\n```\r\n```\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/644", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/644/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/644/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/644/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/644", "id": 471629198, "node_id": "MDU6SXNzdWU0NzE2MjkxOTg=", "number": 644, "title": "Admin API\u2019s NewPartitions missing replication factor", "user": {"login": "polarctis", "id": 37503589, "node_id": "MDQ6VXNlcjM3NTAzNTg5", "avatar_url": "https://avatars3.githubusercontent.com/u/37503589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polarctis", "html_url": "https://github.com/polarctis", "followers_url": "https://api.github.com/users/polarctis/followers", "following_url": "https://api.github.com/users/polarctis/following{/other_user}", "gists_url": "https://api.github.com/users/polarctis/gists{/gist_id}", "starred_url": "https://api.github.com/users/polarctis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polarctis/subscriptions", "organizations_url": "https://api.github.com/users/polarctis/orgs", "repos_url": "https://api.github.com/users/polarctis/repos", "events_url": "https://api.github.com/users/polarctis/events{/privacy}", "received_events_url": "https://api.github.com/users/polarctis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948251, "node_id": "MDU6TGFiZWwzNTc5NDgyNTE=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 932257743, "node_id": "MDU6TGFiZWw5MzIyNTc3NDM=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/docs", "name": "docs", "color": "c5def5", "default": false, "description": "Improve docs"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-07-23T11:01:34Z", "updated_at": "2019-08-22T06:35:58Z", "closed_at": "2019-08-22T06:35:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nThe documentation of Admin API for NewPartitions has replication factor in it, but the implementation lacks it.\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/642", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/642/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/642/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/642/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/642", "id": 471550640, "node_id": "MDU6SXNzdWU0NzE1NTA2NDA=", "number": 642, "title": "Not able to Identify the producer in code.", "user": {"login": "sp1rs", "id": 2691397, "node_id": "MDQ6VXNlcjI2OTEzOTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2691397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sp1rs", "html_url": "https://github.com/sp1rs", "followers_url": "https://api.github.com/users/sp1rs/followers", "following_url": "https://api.github.com/users/sp1rs/following{/other_user}", "gists_url": "https://api.github.com/users/sp1rs/gists{/gist_id}", "starred_url": "https://api.github.com/users/sp1rs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sp1rs/subscriptions", "organizations_url": "https://api.github.com/users/sp1rs/orgs", "repos_url": "https://api.github.com/users/sp1rs/repos", "events_url": "https://api.github.com/users/sp1rs/events{/privacy}", "received_events_url": "https://api.github.com/users/sp1rs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-23T08:18:40Z", "updated_at": "2019-07-23T15:50:38Z", "closed_at": "2019-07-23T15:50:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI have initialized the broker but not able to identify the broker object in the `if` statement.\r\nVersion: 1.1.0\r\npython: 3.6\r\n\r\n\r\nHow to reproduce\r\n================\r\n```\r\nclass TransactionNotificationTracker:\r\n    \"\"\"For logging the transaction data to Kafka\"\"\"\r\n    broker = None\r\n\r\n    @staticmethod\r\n    def transact_notification_tracker(x: str) -> None:\r\n        \"\"\"Push transaction event to Kafka.\"\"\"\r\n        if not TransactionNotificationTracker.broker:\r\n            TransactionNotificationTracker.broker = Producer({'bootstrap.servers': settings.KAFKA_SERVER_HOST})\r\n        if TransactionNotificationTracker.broker and x:\r\n            try:\r\n                # data\r\n                data = x\r\n                TransactionNotificationTracker.broker.produce('transactions_tracker', data.encode('utf-8'))\r\n                TransactionNotificationTracker.broker.flush()\r\n                logger.log('info', ['TransactionNotificationTracker', 'transactions_tracker',\r\n                                    'transaction push_to_kafka', data])\r\n            except Exception as e:\r\n                logger.log('critical', ['TransactionNotificationTracker', 'transactions_tracker',\r\n                                        'Exception in push_to_kafka', data, e])\r\n        else:\r\n            logger.log('critical', ['TransactionNotificationTracker', 'transactions_tracker_not_initialized'])\r\n\r\n```\r\n\r\nThe log which I am getting is `transactions_tracker_not_initialized`.\r\nI tried running it on shell\r\n```\r\n>>> TransactionNotificationTracker.broker\r\n<cimpl.Producer object at 0x7f48605d27b8>\r\n>>> if TransactionNotificationTracker.broker:\r\n...    print(\"SDF\")\r\n... \r\n>>> if not TransactionNotificationTracker.broker:\r\n...    print(\"SDF\")\r\n... \r\nSDF\r\n\r\n```\r\n\r\nSomehow the `if` check is failing. Any idea.?\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [x] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [x] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/639", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/639/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/639/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/639/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/639", "id": 468867236, "node_id": "MDU6SXNzdWU0Njg4NjcyMzY=", "number": 639, "title": "confluent_kafka.Message is not user-instantiable - how to test/mock?", "user": {"login": "joefromct", "id": 3342707, "node_id": "MDQ6VXNlcjMzNDI3MDc=", "avatar_url": "https://avatars2.githubusercontent.com/u/3342707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joefromct", "html_url": "https://github.com/joefromct", "followers_url": "https://api.github.com/users/joefromct/followers", "following_url": "https://api.github.com/users/joefromct/following{/other_user}", "gists_url": "https://api.github.com/users/joefromct/gists{/gist_id}", "starred_url": "https://api.github.com/users/joefromct/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joefromct/subscriptions", "organizations_url": "https://api.github.com/users/joefromct/orgs", "repos_url": "https://api.github.com/users/joefromct/repos", "events_url": "https://api.github.com/users/joefromct/events{/privacy}", "received_events_url": "https://api.github.com/users/joefromct/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-16T21:14:46Z", "updated_at": "2019-10-24T02:31:33Z", "closed_at": "2019-10-24T02:30:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nI want to test a function consuming kafka messages and I'm looking for a good way to mock `confluent_kafka.Message` however (as per the docs) [is not user-instantiatable](https://docs.confluent.io/current/clients/confluent-kafka-python/index.html#confluent_kafka.Message).\r\n\r\nI was hoping to get some clues from the test files for however even the consumer test states [these wont really do anything since there is no broker configured.](https://github.com/confluentinc/confluent-kafka-python/blob/master/tests/test_Consumer.py#L10)\r\n\r\nCurious how are folks doing this when building/testing data processing pipelinse...  maybe it's just not possible.\r\n\r\nAny tips appreciated, and apologies if this isn't the right place to post questions such as this.\r\n\r\n \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/637", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/637/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/637/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/637/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/637", "id": 467380512, "node_id": "MDU6SXNzdWU0NjczODA1MTI=", "number": 637, "title": "Producer cannot be pickled ", "user": {"login": "remiadon", "id": 2931080, "node_id": "MDQ6VXNlcjI5MzEwODA=", "avatar_url": "https://avatars2.githubusercontent.com/u/2931080?v=4", "gravatar_id": "", "url": "https://api.github.com/users/remiadon", "html_url": "https://github.com/remiadon", "followers_url": "https://api.github.com/users/remiadon/followers", "following_url": "https://api.github.com/users/remiadon/following{/other_user}", "gists_url": "https://api.github.com/users/remiadon/gists{/gist_id}", "starred_url": "https://api.github.com/users/remiadon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/remiadon/subscriptions", "organizations_url": "https://api.github.com/users/remiadon/orgs", "repos_url": "https://api.github.com/users/remiadon/repos", "events_url": "https://api.github.com/users/remiadon/events{/privacy}", "received_events_url": "https://api.github.com/users/remiadon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-12T12:10:07Z", "updated_at": "2019-07-21T13:21:52Z", "closed_at": "2019-07-21T13:21:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nMy use case is the following:\r\nI want to be able to process some data using the PySpark.streaming API, manipulating `DStreams` objects\r\nAt the end I want to \"sink\" the results into Kafka, using an instance of `confluent_kafka.Producer`\r\n\r\n\r\nHow to reproduce\r\n================\r\nHere is the pseudo-code :\r\n```python\r\nfrom pyspark.streaming.kafka import KafkaUtils\r\nimport pyspark\r\nimport pyspark.streaming\r\nfrom confluent_kafka import Producer\r\n\r\nsc = pyspark.SparkContext()\r\npoll_interval = 10\r\nssc = pyspark.streaming.StreamingContext(sc, poll_interval)\r\n\r\nkafka_conf = {\"bootstrap.servers\": \"localhost:9092,\"}\r\nstream = KafkaUtils.createDirectStream(ssc, ['kafka-test'], kafka_conf)\r\nproducer = Producer(kafka_conf)\r\n\r\ndef my_produce(e):\r\n    producer.poll(0)\r\n    producer.produce(key=e[0], value=e[1], topic='kafka-test-output')\r\n\r\nstream.foreachRDD(lambda rdd: rdd.foreach(my_produce))\r\nstream.pprint()\r\nssc.start()\r\n``` \r\n\r\nI get an error saying the producer cannot be pickled via `cloudpickle` : \r\n* `TypeError: can't pickle cimpl.Producer objects`\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n - [X] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): ('1.0.1', 16777472) and ('1.0.1', 16777727)\r\n - [X] Apache Kafka broker version: 1.0.0\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/633", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/633/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/633/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/633/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/633", "id": 465089371, "node_id": "MDU6SXNzdWU0NjUwODkzNzE=", "number": 633, "title": "How to use example applications of confluent-kafka-python", "user": {"login": "vikramindian", "id": 22010146, "node_id": "MDQ6VXNlcjIyMDEwMTQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/22010146?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vikramindian", "html_url": "https://github.com/vikramindian", "followers_url": "https://api.github.com/users/vikramindian/followers", "following_url": "https://api.github.com/users/vikramindian/following{/other_user}", "gists_url": "https://api.github.com/users/vikramindian/gists{/gist_id}", "starred_url": "https://api.github.com/users/vikramindian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vikramindian/subscriptions", "organizations_url": "https://api.github.com/users/vikramindian/orgs", "repos_url": "https://api.github.com/users/vikramindian/repos", "events_url": "https://api.github.com/users/vikramindian/events{/privacy}", "received_events_url": "https://api.github.com/users/vikramindian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-08T07:04:50Z", "updated_at": "2019-07-18T15:41:34Z", "closed_at": "2019-07-18T15:41:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nHey,\r\n\r\nI'am new to this. Can you prescribe the step by step process on how to run the example applications (specifically `adminapi.py`).\r\n\r\nI have llibrdkafka installed in `/u/markiv/librdkafka` and confluent-kafka-python in `/u/markiv/confluent-kafka-python/`\r\n\r\n\r\nI tried running `python setup.py build` inside confluent-kafka-python directory, but it says (below)\r\n\r\n> In file included from confluent_kafka/src/confluent_kafka.c:17:0:\r\n> confluent_kafka/src/confluent_kafka.h:22:10: fatal error: librdkafka/rdkafka.h: No such file or directory\r\n>  #include <librdkafka/rdkafka.h>\r\n>           ^~~~~~~~~~~~~~~~~~~~~~\r\n> compilation terminated.\r\n\r\nHow to link librdkafka to confluent-kafka-python?\r\n\r\nThanks in advance.\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/632", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/632/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/632/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/632/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/632", "id": 460768829, "node_id": "MDU6SXNzdWU0NjA3Njg4Mjk=", "number": 632, "title": "Supress Retrying MetadataRequest logs", "user": {"login": "aakashres", "id": 10377544, "node_id": "MDQ6VXNlcjEwMzc3NTQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/10377544?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aakashres", "html_url": "https://github.com/aakashres", "followers_url": "https://api.github.com/users/aakashres/followers", "following_url": "https://api.github.com/users/aakashres/following{/other_user}", "gists_url": "https://api.github.com/users/aakashres/gists{/gist_id}", "starred_url": "https://api.github.com/users/aakashres/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aakashres/subscriptions", "organizations_url": "https://api.github.com/users/aakashres/orgs", "repos_url": "https://api.github.com/users/aakashres/repos", "events_url": "https://api.github.com/users/aakashres/events{/privacy}", "received_events_url": "https://api.github.com/users/aakashres/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-06-26T05:34:04Z", "updated_at": "2019-11-04T08:53:41Z", "closed_at": "2019-06-28T07:36:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI just want some logs to be supressed completly.\r\n\r\n2019-04-25_11:44:58.92622 %4\\|1556192698.926\\|REQTMOUT\\|lpc_kafka_producer#consumer-1\\| [thrd:10.48.207.5:9094/bootstrap]: 10.48.207.5:9094/bootstrap: Timed out 0 in-flight, 2 retry-queued, 0 out-queue, 0 partially-sent requests\r\n--\r\n\u00a0\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\nNA\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version(0.11.6)` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{\r\n            'session.timeout.ms': 6000,\r\n            'enable.auto.commit': False,\r\n            'default.topic.config': {'auto.offset.reset': 'earliest'},\r\n        }`\r\n - [ ] Operating system: Linux Ububtu\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/631", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/631/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/631/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/631/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/631", "id": 460586392, "node_id": "MDU6SXNzdWU0NjA1ODYzOTI=", "number": 631, "title": "Bundled with wheel libz 1.2.3 contains known security vulnerabilities", "user": {"login": "felidadae", "id": 3729236, "node_id": "MDQ6VXNlcjM3MjkyMzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3729236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/felidadae", "html_url": "https://github.com/felidadae", "followers_url": "https://api.github.com/users/felidadae/followers", "following_url": "https://api.github.com/users/felidadae/following{/other_user}", "gists_url": "https://api.github.com/users/felidadae/gists{/gist_id}", "starred_url": "https://api.github.com/users/felidadae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/felidadae/subscriptions", "organizations_url": "https://api.github.com/users/felidadae/orgs", "repos_url": "https://api.github.com/users/felidadae/repos", "events_url": "https://api.github.com/users/felidadae/events{/privacy}", "received_events_url": "https://api.github.com/users/felidadae/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-25T18:55:33Z", "updated_at": "2019-07-19T11:23:03Z", "closed_at": "2019-07-19T11:23:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nThere are security vulnerabilities in binary library bundled in wheel package for linux of confluent-kafka-python 1.0.1 (the newest version) kept in PyPI. The library is libz: libz-a147dcb0.so.1.2.3.\r\nThat version (1.2.3) of libz contains security holes:\r\n- CVE-2016-9841\r\n- CVE-2016-9843\r\n- CVE-2016-9840\r\n- CVE-2016-9842\r\n\r\nThere were fixed in 1.2.10.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/630", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/630/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/630/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/630/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/630", "id": 460365432, "node_id": "MDU6SXNzdWU0NjAzNjU0MzI=", "number": 630, "title": "kafka consumer group stopped receiving messages", "user": {"login": "Kashyap23", "id": 41246906, "node_id": "MDQ6VXNlcjQxMjQ2OTA2", "avatar_url": "https://avatars2.githubusercontent.com/u/41246906?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kashyap23", "html_url": "https://github.com/Kashyap23", "followers_url": "https://api.github.com/users/Kashyap23/followers", "following_url": "https://api.github.com/users/Kashyap23/following{/other_user}", "gists_url": "https://api.github.com/users/Kashyap23/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kashyap23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kashyap23/subscriptions", "organizations_url": "https://api.github.com/users/Kashyap23/orgs", "repos_url": "https://api.github.com/users/Kashyap23/repos", "events_url": "https://api.github.com/users/Kashyap23/events{/privacy}", "received_events_url": "https://api.github.com/users/Kashyap23/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-25T11:22:55Z", "updated_at": "2019-07-18T17:35:18Z", "closed_at": "2019-07-18T17:35:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWe have a single kafka cluster with 3 brokers. We currently have 4 consumer groups receiving messages for the topics matching the regex similar to this\r\n\r\n```\r\n^.*_topic_updates$\r\n```\r\n\r\nEach consumer group has only one consumer running in it. We are launching dedicated threads for each of these consumers. In these 4 consumers sometimes 2 or even 3 consumers stop receiving messages after a period of time. The servers in which  these consumer's are running on are on heavy load. We thought this could be the cause of the issue for consumer not receiving messages. But even when the server is not on full load we are not able to consume messages.\r\n\r\nBut after a manual restart of the service to which the consumers are integrated, we start getting messages properly. After sometime the same issue is repeating. \r\n\r\nWe are facing this issue in production.\r\n\r\nConsumer configuration\r\n```\r\nc_obj = Consumer({\r\n            'bootstrap.servers': 'server1:port1,server2:port2,server3:port3',\r\n            'enable.auto.commit' : True,\r\n            'group.id': group_id,\r\n            'auto.offset.reset': 'earliest',\r\n            'max.partition.fetch.bytes' : 524288000\r\n        })\r\n\r\n```\r\n\r\nNumber of Partitions for each topic = 1\r\n\r\nHow to reproduce\r\n================\r\nRun multiple consumers groups with each of the server on heavy load.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):  confluent-kafka==0.11.6\r\n - [x] Apache Kafka broker version: confluent-platform-oss-2.11\r\n - [ ] Client configuration: `{...}` \r\n - [x] Operating system: Ubuntu 18.04.2 LTS\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [x] Critical issue \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/629", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/629/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/629/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/629/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/629", "id": 459788963, "node_id": "MDU6SXNzdWU0NTk3ODg5NjM=", "number": 629, "title": "Kafka consumer gets stuck after exceeding max.poll.interval.ms", "user": {"login": "im-abeer", "id": 44157375, "node_id": "MDQ6VXNlcjQ0MTU3Mzc1", "avatar_url": "https://avatars0.githubusercontent.com/u/44157375?v=4", "gravatar_id": "", "url": "https://api.github.com/users/im-abeer", "html_url": "https://github.com/im-abeer", "followers_url": "https://api.github.com/users/im-abeer/followers", "following_url": "https://api.github.com/users/im-abeer/following{/other_user}", "gists_url": "https://api.github.com/users/im-abeer/gists{/gist_id}", "starred_url": "https://api.github.com/users/im-abeer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/im-abeer/subscriptions", "organizations_url": "https://api.github.com/users/im-abeer/orgs", "repos_url": "https://api.github.com/users/im-abeer/repos", "events_url": "https://api.github.com/users/im-abeer/events{/privacy}", "received_events_url": "https://api.github.com/users/im-abeer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-06-24T09:34:39Z", "updated_at": "2019-06-24T09:40:16Z", "closed_at": "2019-06-24T09:38:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Created in the wrong repository by mistake\r\nRef: https://github.com/confluentinc/confluent-kafka-go/issues/344", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/628", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/628/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/628/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/628/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/628", "id": 458334892, "node_id": "MDU6SXNzdWU0NTgzMzQ4OTI=", "number": 628, "title": "Periodic UNKNOWN_MEMBER_ID error when resetting offsets", "user": {"login": "geoff-va", "id": 11033703, "node_id": "MDQ6VXNlcjExMDMzNzAz", "avatar_url": "https://avatars3.githubusercontent.com/u/11033703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/geoff-va", "html_url": "https://github.com/geoff-va", "followers_url": "https://api.github.com/users/geoff-va/followers", "following_url": "https://api.github.com/users/geoff-va/following{/other_user}", "gists_url": "https://api.github.com/users/geoff-va/gists{/gist_id}", "starred_url": "https://api.github.com/users/geoff-va/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/geoff-va/subscriptions", "organizations_url": "https://api.github.com/users/geoff-va/orgs", "repos_url": "https://api.github.com/users/geoff-va/repos", "events_url": "https://api.github.com/users/geoff-va/events{/privacy}", "received_events_url": "https://api.github.com/users/geoff-va/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-20T04:10:34Z", "updated_at": "2019-06-24T16:45:26Z", "closed_at": "2019-06-20T18:16:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI periodically get an `UNKNOWN_MEMBER_ID` error when trying to reset the offsets to the beginning for a consumer group. I've read other posts about session timeouts (all timeouts are at defaults), but it looks like from the log timestamps everything is happening ms apart, so that doesn't seem to be it? I tried throwing a 20s pause before committing (to see if I could trigger a timeout, think i saw 10s was the default?), but that never caused a problem. Repeating the call seems to successfully reset the offsets.\r\n\r\nI'm using the following code to achieve this:\r\n```python\r\n    def reset_offsets_to_beginning(self):\r\n        data = self.consumer.list_topics()\r\n        original_tps = []\r\n        new_tps = []\r\n\r\n        for t in self.topics:\r\n            partitions = data.topics[t].partitions.keys()\r\n            for p in partitions:\r\n                cur_tp = TopicPartition(t, p)\r\n                original_tps.append(cur_tp)\r\n                low, high = self.consumer.get_watermark_offsets(cur_tp)\r\n                new_tp = TopicPartition(t, p, low)\r\n                new_tps.append(new_tp)\r\n\r\n        log.info(\r\n            \"Original offsets for group %s: %s\",\r\n            self.group_id, self.consumer.committed(original_tps))\r\n\r\n        self.consumer.commit(offsets=new_tps, async=False)\r\n\r\n        log.info(\r\n            \"New offsets for group %s: %s\",\r\n            self.group_id, self.consumer.committed(new_tps))\r\n```\r\n\r\nThe error occurs when calling `self.consumer.commit`.\r\n\r\nHow to reproduce\r\n================\r\nUnfortunately I haven't found a reliable way to reproduce this and I can't be sure of what's causing it.\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [X] Apache Kafka broker version:\r\n    - kafka_2.12-1.0.0.\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): \r\n    - `confluent_kafka.version(): ('0.11.5', 722176)`\r\n    - `confluent_kafka.libversion(): ('0.11.5', 722431)`\r\n - [X] Client configuration: `{...}`\r\n```python\r\n        self.config = {\r\n            'group.id': group_id,\r\n            'bootstrap.servers': ','.join(server_ips),\r\n            'ssl.ca.location': self.root_ca,\r\n            'sasl.mechanisms': 'PLAIN',\r\n            'sasl.username': username,\r\n            'sasl.password': password,\r\n            'security.protocol': 'SASL_SSL',\r\n            'enable.auto.commit': False,\r\n            'auto.offset.reset': 'beginning'  # If no offset for group, start here\r\n        }\r\n```\r\n - [X] Operating system:\r\n    - Ubuntu 16.04.5 LTS\r\n - [X] Provide client logs (with `'debug': '..'` as necessary)\r\n```shell\r\n2019-06-20 03:09:08,720 [INFO] Kafka Consumer Successfully Created\r\n2019-06-20 03:09:08,799 [INFO] Original offsets for group my-consumer-group: [TopicPartition{topic=topic_name,partition=0,offset=0,error=None}, TopicPartition{topic=topic_name,partition=1,offset=0,error=None}]\r\n[2019-06-20 03:09:08,801: ERROR/ForkPoolWorker-1] Task reset_kafka_offsets[7dbfb5d5-6a15-4682-93f1-8358d08e490f] raised unexpected: KafkaException('KafkaError{code=UNKNOWN_MEMBER_ID,val=25,str=\"Commit failed: Broker: Unknown member\"}',)\r\n```\r\n - [X] Provide broker log excerpts\r\n\r\nThese seem to be the only relevant logs I can find for the broker? Not clear to me what lines like `Member rdkafka-... group <group_name> has failed ...` means?\r\n\r\n```shell\r\n[2019-06-19 23:08:32,130] INFO [GroupCoordinator 1]: Stabilized group my-consumer-group generation 16 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)                                                                 \r\n[2019-06-19 23:08:32,131] INFO [GroupCoordinator 1]: Assignment received from leader for group my-consumer-group for generation 16 (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:08:47,488] INFO [GroupCoordinator 1]: Preparing to rebalance group my-consumer-group with old generation 16 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:08:47,488] INFO [GroupCoordinator 1]: Group my-consumer-group with generation 17 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:08:47,902] INFO [GroupCoordinator 1]: Preparing to rebalance group my-consumer-group with old generation 17 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:08:47,903] INFO [GroupCoordinator 1]: Stabilized group my-consumer-group generation 18 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:08:47,904] INFO [GroupCoordinator 1]: Assignment received from leader for group my-consumer-group for generation 18 (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:09:17,906] INFO [GroupCoordinator 1]: Member rdkafka-ce7d3d86-c984-4442-9393-5a9d292e6c54 in group my-consumer-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:09:17,906] INFO [GroupCoordinator 1]: Preparing to rebalance group my-consumer-group with old generation 18 (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)\r\n[2019-06-19 23:09:17,906] INFO [GroupCoordinator 1]: Group my-consumer-group with generation 19 is now empty (__consumer_offsets-13) (kafka.coordinator.group.GroupCoordinator)\r\n```\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/623", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/623/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/623/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/623/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/623", "id": 456721161, "node_id": "MDU6SXNzdWU0NTY3MjExNjE=", "number": 623, "title": "consumer can't get data", "user": {"login": "jianglijie", "id": 19518364, "node_id": "MDQ6VXNlcjE5NTE4MzY0", "avatar_url": "https://avatars0.githubusercontent.com/u/19518364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jianglijie", "html_url": "https://github.com/jianglijie", "followers_url": "https://api.github.com/users/jianglijie/followers", "following_url": "https://api.github.com/users/jianglijie/following{/other_user}", "gists_url": "https://api.github.com/users/jianglijie/gists{/gist_id}", "starred_url": "https://api.github.com/users/jianglijie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jianglijie/subscriptions", "organizations_url": "https://api.github.com/users/jianglijie/orgs", "repos_url": "https://api.github.com/users/jianglijie/repos", "events_url": "https://api.github.com/users/jianglijie/events{/privacy}", "received_events_url": "https://api.github.com/users/jianglijie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-17T03:03:20Z", "updated_at": "2019-06-18T06:35:42Z", "closed_at": "2019-06-18T06:35:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I installed kafka and zookeeper in docker and run example. I can see the producer print message and message in kafka. But consumer never deal messages. poll always get none. \r\nhow can consumer get data?\r\n\r\n```\r\nfrom confluent_kafka import Producer\r\n\r\n\r\np = Producer({'bootstrap.servers': 'localhost:9092'})\r\n\r\ndef delivery_report(err, msg):\r\n    \"\"\" Called once for each message produced to indicate delivery result.\r\n        Triggered by poll() or flush(). \"\"\"\r\n    if err is not None:\r\n        print('Message delivery failed: {}'.format(err))\r\n    else:\r\n        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\r\n\r\nfor data in some_data_source:\r\n    # Trigger any available delivery report callbacks from previous produce() calls\r\n    p.poll(0)\r\n\r\n    # Asynchronously produce a message, the delivery report callback\r\n    # will be triggered from poll() above, or flush() below, when the message has\r\n    # been successfully delivered or failed permanently.\r\n    p.produce('mytopic', data.encode('utf-8'), callback=delivery_report)\r\n\r\n# Wait for any outstanding messages to be delivered and delivery report\r\n# callbacks to be triggered.\r\np.flush()\r\n\r\n\r\nfrom confluent_kafka import Consumer, KafkaError\r\n\r\n\r\nc = Consumer({\r\n    'bootstrap.servers': 'localhost:9092',\r\n    'group.id': 'mygroup',\r\n    'auto.offset.reset': 'earliest'\r\n})\r\n\r\nc.subscribe(['mytopic'])\r\n\r\nwhile True:\r\n    msg = c.poll(1.0)\r\n\r\n    if msg is None:\r\n        continue\r\n    if msg.error():\r\n        print(\"Consumer error: {}\".format(msg.error()))\r\n        continue\r\n\r\n    print('Received message: {}'.format(msg.value().decode('utf-8')))\r\n\r\nc.close()\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/621", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/621/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/621/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/621/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/621", "id": 455699542, "node_id": "MDU6SXNzdWU0NTU2OTk1NDI=", "number": 621, "title": "Missing source package for 1.0.1", "user": {"login": "asyd", "id": 334868, "node_id": "MDQ6VXNlcjMzNDg2OA==", "avatar_url": "https://avatars0.githubusercontent.com/u/334868?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asyd", "html_url": "https://github.com/asyd", "followers_url": "https://api.github.com/users/asyd/followers", "following_url": "https://api.github.com/users/asyd/following{/other_user}", "gists_url": "https://api.github.com/users/asyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/asyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asyd/subscriptions", "organizations_url": "https://api.github.com/users/asyd/orgs", "repos_url": "https://api.github.com/users/asyd/repos", "events_url": "https://api.github.com/users/asyd/events{/privacy}", "received_events_url": "https://api.github.com/users/asyd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rnpridgeon", "id": 5313961, "node_id": "MDQ6VXNlcjUzMTM5NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5313961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rnpridgeon", "html_url": "https://github.com/rnpridgeon", "followers_url": "https://api.github.com/users/rnpridgeon/followers", "following_url": "https://api.github.com/users/rnpridgeon/following{/other_user}", "gists_url": "https://api.github.com/users/rnpridgeon/gists{/gist_id}", "starred_url": "https://api.github.com/users/rnpridgeon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rnpridgeon/subscriptions", "organizations_url": "https://api.github.com/users/rnpridgeon/orgs", "repos_url": "https://api.github.com/users/rnpridgeon/repos", "events_url": "https://api.github.com/users/rnpridgeon/events{/privacy}", "received_events_url": "https://api.github.com/users/rnpridgeon/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rnpridgeon", "id": 5313961, "node_id": "MDQ6VXNlcjUzMTM5NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5313961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rnpridgeon", "html_url": "https://github.com/rnpridgeon", "followers_url": "https://api.github.com/users/rnpridgeon/followers", "following_url": "https://api.github.com/users/rnpridgeon/following{/other_user}", "gists_url": "https://api.github.com/users/rnpridgeon/gists{/gist_id}", "starred_url": "https://api.github.com/users/rnpridgeon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rnpridgeon/subscriptions", "organizations_url": "https://api.github.com/users/rnpridgeon/orgs", "repos_url": "https://api.github.com/users/rnpridgeon/repos", "events_url": "https://api.github.com/users/rnpridgeon/events{/privacy}", "received_events_url": "https://api.github.com/users/rnpridgeon/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2019-06-13T11:59:35Z", "updated_at": "2019-06-19T08:17:35Z", "closed_at": "2019-06-19T08:17:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nIt's not possible to install confluent-kafka 1.0.1 from source since there is .tar.gz packages on pypi. Thereforce it's impossible to install confluent-kafka on alpine based system for examples.\r\n\r\n(Same issue than #568)\r\n\r\nHow to reproduce\r\n================\r\n\r\n```\r\npython3 -m venv test\r\n./test/bin/pip download --no-binary=:all: --no-deps confluent-kafka==1.0.1\r\nCollecting confluent-kafka==1.0.1\r\n  Could not find a version that satisfies the requirement confluent-kafka==1.0.1 (from versions: 0.9.1.1, 0.9.1.2, 0.9.2, 0.9.4, 0.11.0, 0.11.4, 0.11.5, 0.11.6, 1.0.0)\r\nNo matching distribution found for confluent-kafka==1.0.1\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/618", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/618/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/618/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/618/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/618", "id": 455165685, "node_id": "MDU6SXNzdWU0NTUxNjU2ODU=", "number": 618, "title": "consumer.poll() on empty topic returns MAX_POLL_EXCEEDED after max.poll.interval.ms", "user": {"login": "danmilon", "id": 1139595, "node_id": "MDQ6VXNlcjExMzk1OTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1139595?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danmilon", "html_url": "https://github.com/danmilon", "followers_url": "https://api.github.com/users/danmilon/followers", "following_url": "https://api.github.com/users/danmilon/following{/other_user}", "gists_url": "https://api.github.com/users/danmilon/gists{/gist_id}", "starred_url": "https://api.github.com/users/danmilon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danmilon/subscriptions", "organizations_url": "https://api.github.com/users/danmilon/orgs", "repos_url": "https://api.github.com/users/danmilon/repos", "events_url": "https://api.github.com/users/danmilon/events{/privacy}", "received_events_url": "https://api.github.com/users/danmilon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948251, "node_id": "MDU6TGFiZWwzNTc5NDgyNTE=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}, {"id": 407409076, "node_id": "MDU6TGFiZWw0MDc0MDkwNzY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/librdkafka", "name": "librdkafka", "color": "f9d0c4", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-12T11:44:37Z", "updated_at": "2019-07-18T20:16:33Z", "closed_at": "2019-07-18T20:16:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\nWhen subscribed to an empty topic, `consumer.poll(timeout=None)` returns, after `max.poll.interval.ms`, a message with `msg.error()`:\r\n```\r\nKafkaError{code=_MAX_POLL_EXCEEDED,val=-147,str=\"Application maximum poll interval (10000ms) exceeded by 15ms\"}\r\n```\r\n\r\nExpected Behavior\r\n================\r\n\r\nThe docs mention that `max.poll.interval.ms` is the maximum interval _between_ `poll()` calls, but clearly in the following script, it raises while _inside_ `poll()`, not between `poll()` calls.\r\n```\r\nMaximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member.\r\n```\r\n\r\nHow to reproduce\r\n================\r\n\r\nRun the following script by pointing it to an empty topic and wait for 10 seconds:\r\n```python\r\nimport sys\r\nimport confluent_kafka as kafka\r\n\r\nconsumer = kafka.Consumer({\r\n    \"bootstrap.servers\": sys.argv[1],\r\n    \"group.id\": \"group_id\",\r\n    \"enable.auto.commit\": \"false\",\r\n    \"max.poll.interval.ms\": \"10000\",\r\n})\r\n\r\nconsumer.subscribe([sys.argv[2]])\r\n\r\nprint(\"poll: start\")\r\nmsg = consumer.poll()\r\nprint(\"poll: end\")\r\nif msg.error():\r\n    raise msg.error()\r\nprint(\"end\")\r\n\r\n```\r\nCall it as `python reproduce.py <bootstrap-server> <empty-topic-name>```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): \r\n```\r\n>>> kafka.version()\r\n('1.0.0', 1048576)\r\n>>> kafka.libversion()\r\n('1.0.0', 16777471)\r\n```\r\n - [x] Apache Kafka broker version: `2.1.0`\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: ArchLinux \r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/617", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/617/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/617/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/617/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/617", "id": 454404319, "node_id": "MDU6SXNzdWU0NTQ0MDQzMTk=", "number": 617, "title": "Kafka deploy on Google Cloud - cannot create topic", "user": {"login": "dkajtoch", "id": 32985207, "node_id": "MDQ6VXNlcjMyOTg1MjA3", "avatar_url": "https://avatars2.githubusercontent.com/u/32985207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dkajtoch", "html_url": "https://github.com/dkajtoch", "followers_url": "https://api.github.com/users/dkajtoch/followers", "following_url": "https://api.github.com/users/dkajtoch/following{/other_user}", "gists_url": "https://api.github.com/users/dkajtoch/gists{/gist_id}", "starred_url": "https://api.github.com/users/dkajtoch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dkajtoch/subscriptions", "organizations_url": "https://api.github.com/users/dkajtoch/orgs", "repos_url": "https://api.github.com/users/dkajtoch/repos", "events_url": "https://api.github.com/users/dkajtoch/events{/privacy}", "received_events_url": "https://api.github.com/users/dkajtoch/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-10T22:41:49Z", "updated_at": "2019-07-18T20:15:17Z", "closed_at": "2019-07-18T20:15:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI deployed kafka on google cloud compute using prepared image: https://console.cloud.google.com/marketplace/details/click-to-deploy-images/kafka?q=kafka&id=f19a0f63-fc57-47fd-9d94-8d5ca6af935e\r\n\r\nKnow, I want to connect externally (from outside google cloud) to kafka and create topic. The problem is it does not work using AdminClient. I can create AdminClient (code below) and list topics (I created a few locally using kafka cli), but it hangs on create_topics. Any idea what I should change? \r\nI opened firewall for tcp connection on port 9092 (all ip addresses).\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n# this piece works\r\nfrom confluent_kafka.admin import AdminClient, NewTopic\r\n\r\ncli = AdminClient({'bootstrap.servers': 'gcloud_external_ip:9092'})\r\n\r\ncli.list_topics().topics\r\n\r\n# this piece does not work\r\nfrom confluent_kafka.admin import AdminClient, NewTopic\r\nimport logging\r\n\r\ncli = AdminClient({'bootstrap.servers': 'gcloud_external_ip:9092'})\r\n\r\ntopics = []\r\ntopics.append(\r\n  NewTopic('queries', num_partitions=2, replication_factor=1)\r\n)\r\n\r\nfs = cli.create_topics(topics, request_timeout=10.0)\r\n\r\nfor topic, f in fs.items():\r\n    try:\r\n        f.result()  # The result itself is None\r\n        print(\"Topic {} created\".format(topic))\r\n    except Exception as e:\r\n        print(\"Failed to create topic {}: {}\".format(topic, e))\r\n\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`): 1.0.0 and 1.0.0\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [x] Operating system: Debian 9.9\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/613", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/613/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/613/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/613/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/613", "id": 450961499, "node_id": "MDU6SXNzdWU0NTA5NjE0OTk=", "number": 613, "title": "QUESTION: Way to programmatically determine if there's under replicated partitions?", "user": {"login": "minddrive", "id": 2998978, "node_id": "MDQ6VXNlcjI5OTg5Nzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/2998978?v=4", "gravatar_id": "", "url": "https://api.github.com/users/minddrive", "html_url": "https://github.com/minddrive", "followers_url": "https://api.github.com/users/minddrive/followers", "following_url": "https://api.github.com/users/minddrive/following{/other_user}", "gists_url": "https://api.github.com/users/minddrive/gists{/gist_id}", "starred_url": "https://api.github.com/users/minddrive/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/minddrive/subscriptions", "organizations_url": "https://api.github.com/users/minddrive/orgs", "repos_url": "https://api.github.com/users/minddrive/repos", "events_url": "https://api.github.com/users/minddrive/events{/privacy}", "received_events_url": "https://api.github.com/users/minddrive/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-05-31T20:06:35Z", "updated_at": "2019-06-25T22:39:34Z", "closed_at": "2019-06-25T22:39:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Given that the 'kafka-topic.sh' script has an '--under-replicated-partitions' option (usually used in combination with the '--describe' option), I was wondering if there was a way to programmatically do this with confluent-kafka?  I've gone through the docs a fair amount and done some digging on the web, but haven't been able to determine this.  Might someone be able to answer this definitively?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/607", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/607/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/607/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/607/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/607", "id": 449718234, "node_id": "MDU6SXNzdWU0NDk3MTgyMzQ=", "number": 607, "title": "sending message headers seems not to work from macOS", "user": {"login": "kontrafiktion", "id": 198502, "node_id": "MDQ6VXNlcjE5ODUwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/198502?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kontrafiktion", "html_url": "https://github.com/kontrafiktion", "followers_url": "https://api.github.com/users/kontrafiktion/followers", "following_url": "https://api.github.com/users/kontrafiktion/following{/other_user}", "gists_url": "https://api.github.com/users/kontrafiktion/gists{/gist_id}", "starred_url": "https://api.github.com/users/kontrafiktion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kontrafiktion/subscriptions", "organizations_url": "https://api.github.com/users/kontrafiktion/orgs", "repos_url": "https://api.github.com/users/kontrafiktion/repos", "events_url": "https://api.github.com/users/kontrafiktion/events{/privacy}", "received_events_url": "https://api.github.com/users/kontrafiktion/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-05-29T10:11:12Z", "updated_at": "2019-06-04T11:39:41Z", "closed_at": "2019-06-04T11:31:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nheaders sent from macOS are not available in a Java consumer on Linux. But when using the exact same producer to send messages from Linux, the headers are available in the Java consumer.\r\n\r\nHow to reproduce\r\n================\r\n\r\nProduce a message with headers from macOS and consume it (on Linux): no headers\r\nUse the exact same producer code on Linux and consume it (on Linux): headers are there\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()` ('0.11.6', 722432) ('0.11.6', 722687)\r\n - [ x Apache Kafka broker version: 0.9.x\r\n - [x] Client configuration: \r\n```  'plugin.library.paths': 'monitoring-interceptor'\r\n  'auto.offset.reset': 'earliest'\r\n  'enable.auto.offset.store': false\r\n  'api.version.request': false\r\n```\r\n - [x] Operating system: producer on macOS 10.13\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts: --\r\n - [ ] Critical issue: No\r\n\r\n----\r\nWe have looked into Producer.c and think that RD_KAFKA_V_HEADERS is set, because otherwise it looks like there should be an error message.\r\nWe have looked at integration_test.py but do not understand how to run it, but it looks like there is a test case verifying that headers are received.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/606", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/606/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/606/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/606/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/606", "id": 449688095, "node_id": "MDU6SXNzdWU0NDk2ODgwOTU=", "number": 606, "title": "Producer message headers on macOS", "user": {"login": "kontrafiktion", "id": 198502, "node_id": "MDQ6VXNlcjE5ODUwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/198502?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kontrafiktion", "html_url": "https://github.com/kontrafiktion", "followers_url": "https://api.github.com/users/kontrafiktion/followers", "following_url": "https://api.github.com/users/kontrafiktion/following{/other_user}", "gists_url": "https://api.github.com/users/kontrafiktion/gists{/gist_id}", "starred_url": "https://api.github.com/users/kontrafiktion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kontrafiktion/subscriptions", "organizations_url": "https://api.github.com/users/kontrafiktion/orgs", "repos_url": "https://api.github.com/users/kontrafiktion/repos", "events_url": "https://api.github.com/users/kontrafiktion/events{/privacy}", "received_events_url": "https://api.github.com/users/kontrafiktion/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-05-29T09:11:00Z", "updated_at": "2019-05-29T11:29:28Z", "closed_at": "2019-05-29T11:29:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/605", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/605/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/605/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/605/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/605", "id": 449166144, "node_id": "MDU6SXNzdWU0NDkxNjYxNDQ=", "number": 605, "title": "consumer received PARTITION_EOF error while there is data to be consumed in kafka", "user": {"login": "zealot-shin", "id": 12884162, "node_id": "MDQ6VXNlcjEyODg0MTYy", "avatar_url": "https://avatars1.githubusercontent.com/u/12884162?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zealot-shin", "html_url": "https://github.com/zealot-shin", "followers_url": "https://api.github.com/users/zealot-shin/followers", "following_url": "https://api.github.com/users/zealot-shin/following{/other_user}", "gists_url": "https://api.github.com/users/zealot-shin/gists{/gist_id}", "starred_url": "https://api.github.com/users/zealot-shin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zealot-shin/subscriptions", "organizations_url": "https://api.github.com/users/zealot-shin/orgs", "repos_url": "https://api.github.com/users/zealot-shin/repos", "events_url": "https://api.github.com/users/zealot-shin/events{/privacy}", "received_events_url": "https://api.github.com/users/zealot-shin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-05-28T09:38:12Z", "updated_at": "2019-06-01T05:32:00Z", "closed_at": "2019-06-01T05:32:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nI used confluent_kafka.Consumer to initiate a kafka consumer, before reached the end of the specified partition of topic while in the loop of client.poll(2), all things ran normal.\r\nbut when the consumer met KafkaError._PARTITION_EOF, the consumer became weird.\r\npoll() method constantly returned partition_eof error, however, some data can be consumed, while more and more data accumulated in kafka. \r\nAs a result , consumer can never catch up with the newest data in kafka anymore.\r\n\r\nHow to reproduce\r\n================\r\nrestart my program.\r\nwait until the consumer receive partition_eof error.\r\ndelay  appears again.\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\nconfluent_kafka.version():\r\n('0.11.0', 720896)\r\nconfluent_kafka.libversion():\r\n('0.11.4', 722175)\r\n - [ ] Apache Kafka broker version:  2.12-0.10.2.1\r\n - [ ] Client configuration: `{...}` \r\nconf  {\r\n'bootstrap.servers': self.args[\"bootstrap.servers\"],\r\n            'client.id': self.args[\"client.id\"],\r\n            'group.id': self.args[\"group.id\"],\r\n            'session.timeout.ms': 20000,\r\n            'auto.commit.interval.ms': 5000,\r\n            \"on_commit\": self.on_commit,\r\n            'default.topic.config': {\r\n                'auto.offset.reset': self.args[\"offset.reset\"],\r\n                'enable.auto.commit': \"true\",\r\n                'offset.store.method': 'broker',\r\n                'offset.store.sync.interval.ms': 1000,\r\n                'request.timeout.ms': 5000}\r\n}\r\n - [ ] Operating system: Red Hat 7.4(Maipo)\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n......\r\n[DEBUG] KafkaError{code=_PARTITION_EOF, val=-191,str=\"Broker: No more messages\"}\r\nTOPIC [0] at offset 27574281\r\n......\r\n\r\nhowever , the real bigest offset is like 27581234, always bigger than consumer gets.\r\nand as time goes by, the difference also increase. Thus data delay appears.\r\n\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\nunreasonable partition_eof error caused data delay\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/604", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/604/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/604/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/604/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/604", "id": 448041847, "node_id": "MDU6SXNzdWU0NDgwNDE4NDc=", "number": 604, "title": "How to config sasl_plain related properties in consumer", "user": {"login": "clairexiayanping", "id": 12840794, "node_id": "MDQ6VXNlcjEyODQwNzk0", "avatar_url": "https://avatars0.githubusercontent.com/u/12840794?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clairexiayanping", "html_url": "https://github.com/clairexiayanping", "followers_url": "https://api.github.com/users/clairexiayanping/followers", "following_url": "https://api.github.com/users/clairexiayanping/following{/other_user}", "gists_url": "https://api.github.com/users/clairexiayanping/gists{/gist_id}", "starred_url": "https://api.github.com/users/clairexiayanping/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clairexiayanping/subscriptions", "organizations_url": "https://api.github.com/users/clairexiayanping/orgs", "repos_url": "https://api.github.com/users/clairexiayanping/repos", "events_url": "https://api.github.com/users/clairexiayanping/events{/privacy}", "received_events_url": "https://api.github.com/users/clairexiayanping/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-05-24T08:26:21Z", "updated_at": "2019-05-24T09:02:06Z", "closed_at": "2019-05-24T09:02:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nc = Consumer({\r\n    'bootstrap.servers': mybroker,\r\n    'group.id': 'mygroup',\r\n    'client.id': 'python_client',\r\n    'sasl_mechanism' : 'PLAIN',\r\n    'security_protocol' : 'SASL_PLAINTEXT',\r\n    'sasl_plain_username' : 'xxx',\r\n    'sasl_plain_password' : 'xxx',\r\n    'default.topic.config': {\r\n    'auto.offset.reset': 'smallest'\r\n    }\r\n})\r\n\r\nThe consumer config as above. When I request, got error:\r\ncimpl.KafkaException: KafkaError{code=_INVALID_ARG,val=-186,str=\"No such configuration property: \"sasl_mechanism\"\"}\r\n\r\nHow to config sasl_plain related properties in consumer.\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [ ] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/602", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/602/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/602/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/602/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/602", "id": 446614837, "node_id": "MDU6SXNzdWU0NDY2MTQ4Mzc=", "number": 602, "title": "Problem committing offset to topic partitions", "user": {"login": "tt293", "id": 3470893, "node_id": "MDQ6VXNlcjM0NzA4OTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3470893?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tt293", "html_url": "https://github.com/tt293", "followers_url": "https://api.github.com/users/tt293/followers", "following_url": "https://api.github.com/users/tt293/following{/other_user}", "gists_url": "https://api.github.com/users/tt293/gists{/gist_id}", "starred_url": "https://api.github.com/users/tt293/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tt293/subscriptions", "organizations_url": "https://api.github.com/users/tt293/orgs", "repos_url": "https://api.github.com/users/tt293/repos", "events_url": "https://api.github.com/users/tt293/events{/privacy}", "received_events_url": "https://api.github.com/users/tt293/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 357948256, "node_id": "MDU6TGFiZWwzNTc5NDgyNTY=", "url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-21T13:19:14Z", "updated_at": "2019-06-10T09:14:32Z", "closed_at": "2019-06-10T09:14:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nconfluent-kafka and librdkafka version: ('0.11.6', 722432)\r\nOperating system: RHEL 7\r\n\r\nI am trying to commit a timestamp-based offset to a topic that was created using 5 partitions, using the following python code:\r\n\r\n```\r\ndef commit_timestamp(consumer, partitions):\r\n    print(f'Entering on_assign callback, number of partitions: {len(partitions)} of type {type(partitions[0])}')\r\n\r\n    start_of_day = dt.datetime.now().replace(hour=0, minute=0, second=0)\r\n    offset_time = start_of_day.timestamp() * 1000\r\n\r\n    for x in partitions:\r\n        x.offset = offset_time\r\n\r\n    consumer.assign(consumer.offsets_for_times(partitions))\r\n    module_logger.info(f'Commited timestamp {start_of_day} to consumer.')\r\n\r\n\r\navroParamsConsumer = AvroConsumer({\r\n        \"bootstrap.servers\": \"*1u.*:9093,*2u.*:9093,*3u.*:9093,*4u.*:9093,*5u.*:9093,*6u.*:9093\",\r\n        \"security.protocol\": \"ssl\",\r\n        \"debug\": \"topic,cgrp\",\r\n        \"group.id\": \"flask_server_params\",\r\n        \"auto.offset.reset\": \"earliest\",\r\n        \"ssl.ca.location\": \"***\",\r\n        \"schema.registry.url\": \"https://*-uat.*:9081\",\r\n        \"schema.registry.ssl.ca.location\": \"***\",\r\n        \"session.timeout.ms\": 6000\r\n    })\r\navroParamsConsumer.subscribe(['TOPICNAME'], on_assign=commit_timestamp)\r\n```\r\n\r\nHowever...when running this code (with a poll loop afterwards), the callback, when it eventually gets called (is there a way to force immediate execution of the callback?), receieves only 2 partitions! A short time later, the log shows \"Topic TOPIC_NAME partition count changed from 0 to 5\"\r\n\r\n```\r\n%7|1558444009.426|METADATA|rdkafka#consumer-1| [thrd:main]: ssl://*5u.*:9093/5: 1/1 requested topic(s) seen in metadata\r\n%7|1558444009.426|SUBSCRIPTION|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\": effective subscription list changed from 0 to 1 topic(s):\r\n%7|1558444009.426|SUBSCRIPTION|rdkafka#consumer-1| [thrd:main]:  Topic TOPIC_NAME with 5 partition(s)\r\n%7|1558444009.426|REJOIN|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\": subscription updated from metadata change: rejoining group\r\n%7|1558444009.426|GRPLEADER|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\": resetting group leader info: Group rejoin\r\n%7|1558444009.426|REJOIN|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\" rejoining in join-state init without an assignment\r\n%7|1558444009.426|JOIN|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\": join with 1 (1) subscribed topic(s)\r\n%7|1558444009.426|CGRPMETADATA|rdkafka#consumer-1| [thrd:main]: consumer join: metadata for subscription is up to date (0ms old)\r\n%7|1558444009.426|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\" changed join state init -> wait-join (v1, state up)\r\n%7|1558444009.426|METADATA|rdkafka#consumer-1| [thrd:main]: ssl://*6u.*:9093/6: 1/1 requested topic(s) seen in metadata\r\n%7|1558444009.891|CGRPOP|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\" received op GET_ASSIGNMENT (v0) in state up (join state wait-join, v1 vs 0)\r\n%7|1558444010.227|JOINGROUP|rdkafka#consumer-1| [thrd:main]: JoinGroup response: GenerationId 1820, Protocol range, LeaderId rdkafka-4e9d010c-e310-4f61-afa8-b96b3fb2046c, my MemberId rdkafka-b004030b-1461-4bc1-9c3b-96a16d31aecb, 0 members in group: (no error)\r\n%7|1558444010.227|MEMBERID|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\": updating member id \"\" -> \"rdkafka-b004030b-1461-4bc1-9c3b-96a16d31aecb\"\r\n%7|1558444010.227|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\" changed join state wait-join -> wait-sync (v1, state up)\r\n%7|1558444010.508|SYNCGROUP|rdkafka#consumer-1| [thrd:main]: SyncGroup response: Success (55 bytes of MemberState data)\r\n%7|1558444010.508|ASSIGN|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\": delegating assign of 2 partition(s) to application rebalance callback on queue rd_kafka_cgrp_new: new assignment\r\n%7|1558444010.508|CGRPJOINSTATE|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\" changed join state wait-sync -> wait-assign-rebalance_cb (v1, state up)\r\n%7|1558444010.508|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: ssl://*1u.*:9093/1: Heartbeat for group \"flask_server_params\" generation id 1820\r\n%7|1558444011.890|HEARTBEAT|rdkafka#consumer-1| [thrd:main]: ssl://*1u.*:9093/1: Heartbeat for group \"flask_server_params\" generation id 1820\r\nEntering on_assign callback, number of partitions: 2 of type <class 'cimpl.TopicPartition'>\r\n%7|1558444012.394|OFFSET|rdkafka#consumer-1| [thrd:app]: ssl://*4u.*:9093/4: OffsetRequest (v1, opv 0) for 1 topic(s) and 1 partition(s)\r\n%7|1558444012.394|OFFSET|rdkafka#consumer-1| [thrd:app]: ssl://*2u.*:9093/2: OffsetRequest (v1, opv 0) for 1 topic(s) and 1 partition(s)\r\n%7|1558444012.430|CGRPOP|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\" received op ASSIGN (v0) in state up (join state wait-assign-rebalance_cb, v1 vs 0)\r\n%7|1558444012.430|ASSIGN|rdkafka#consumer-1| [thrd:main]: Group \"flask_server_params\": new assignment of 2 partition(s) in join state wait-assign-rebalance_cb\r\n%7|1558444012.430|TOPIC|rdkafka#consumer-1| [thrd:main]: New local topic: TOPIC_NAME\r\n%7|1558444012.430|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW TOPIC_NAME [-1] 0x7fd7f4003000 (at rd_kafka_topic_new0:362)\r\n%7|1558444012.430|STATE|rdkafka#consumer-1| [thrd:main]: Topic TOPIC_NAME changed state unknown -> exists\r\n%7|1558444012.430|PARTCNT|rdkafka#consumer-1| [thrd:main]: Topic TOPIC_NAME partition count changed from 0 to 5\r\n%7|1558444012.430|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW TOPIC_NAME [0] 0x7fd7f40034b0 (at rd_kafka_topic_partition_cnt_update:589)\r\n%7|1558444012.430|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW TOPIC_NAME [1] 0x7fd7f4003ab0 (at rd_kafka_topic_partition_cnt_update:589)\r\n%7|1558444012.430|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW TOPIC_NAME [2] 0x7fd7f4003de0 (at rd_kafka_topic_partition_cnt_update:589)\r\n%7|1558444012.430|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW TOPIC_NAME [3] 0x7fd7f4004110 (at rd_kafka_topic_partition_cnt_update:589)\r\n%7|1558444012.430|TOPPARNEW|rdkafka#consumer-1| [thrd:main]: NEW TOPIC_NAME [4] 0x7fd7f4004440 (at rd_kafka_topic_partition_cnt_update:589)\r\n%7|1558444012.430|METADATA|rdkafka#consumer-1| [thrd:main]:   Topic TOPIC_NAME partition 0 Leader 6\r\n%7|1558444012.430|BRKDELGT|rdkafka#consumer-1| [thrd:main]: TOPIC_NAME [0]: delegate to broker ssl://*6u.*:9093/6 (rktp 0x7fd7f40034b0, term 0, ref 2, remove 0)\r\n%7|1558444012.430|BRKDELGT|rdkafka#consumer-1| [thrd:main]: TOPIC_NAME [0]: broker ssl://*6u.*:9093/6 is now leader for partition with 0 messages (0 bytes) queued\r\n%7|1558444012.430|BRKMIGR|rdkafka#consumer-1| [thrd:main]: Migrating topic TOPIC_NAME [0] 0x7fd7f40034b0 from (none) to ssl://*6u.*:9093/6 (sending PARTITION_JOIN to ssl://*6u.*:9093/6)\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/600", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/600/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/600/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/600/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/600", "id": 444546136, "node_id": "MDU6SXNzdWU0NDQ1NDYxMzY=", "number": 600, "title": "No module named avro", "user": {"login": "tulasigss", "id": 50673993, "node_id": "MDQ6VXNlcjUwNjczOTkz", "avatar_url": "https://avatars2.githubusercontent.com/u/50673993?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tulasigss", "html_url": "https://github.com/tulasigss", "followers_url": "https://api.github.com/users/tulasigss/followers", "following_url": "https://api.github.com/users/tulasigss/following{/other_user}", "gists_url": "https://api.github.com/users/tulasigss/gists{/gist_id}", "starred_url": "https://api.github.com/users/tulasigss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tulasigss/subscriptions", "organizations_url": "https://api.github.com/users/tulasigss/orgs", "repos_url": "https://api.github.com/users/tulasigss/repos", "events_url": "https://api.github.com/users/tulasigss/events{/privacy}", "received_events_url": "https://api.github.com/users/tulasigss/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-05-15T16:59:05Z", "updated_at": "2019-05-23T06:40:45Z", "closed_at": "2019-05-23T06:40:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\n**i am a new bee for confluent kafka and the following error pops up at this line in python code as below: not sure what is missing**\r\nfrom confluent_kafka.avro import AvroProducer, CachedSchemaRegistryClient\r\nerror: \r\nTraceback (most recent call last):\r\n  File \"/Producers/ConfluentPRoducer.py\", line 3, in <module>\r\n    from confluent_kafka.avro.serializer.message_serializer import MessageSerializer as AvroSerde\r\n  File \"\\FirstKafkaApp\\venv\\lib\\site-packages\\confluent_kafka\\avro\\__init__.py\", line 13, in <module>\r\n    from confluent_kafka.avro.serializer.message_serializer import MessageSerializer\r\n  File \"\\FirstKafkaApp\\venv\\lib\\site-packages\\confluent_kafka\\avro\\serializer\\message_serializer.py\", line 28, in <module>\r\n    import avro\r\nModuleNotFoundError: No module named 'avro'\r\n\r\n**-Ran the following pip and requirements are already satisfied** \r\nC:\\Windows\\System32>pip install confluent-kafka[avro]\r\nRequirement already satisfied: confluent-kafka[avro] in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\r\nRequirement already satisfied: requests; extra == \"avro\" in c:\\programdata\\anaconda3\\lib\\site-packages (from confluent-kafka[avro]) (2.18.4)\r\nCollecting fastavro; extra == \"avro\" (from confluent-kafka[avro])\r\n  Downloading https://files.pythonhosted.org/packages/66/9d/18dee49e551e1c92c6f7d792ed552f232eb2d707ef0829cec37e719e682f/fastavro-0.21.23-cp36-cp36m-win_amd64.whl (306kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 730kB/s\r\nRequirement already satisfied: avro-python3; extra == \"avro\" in c:\\programdata\\anaconda3\\lib\\site-packages (from confluent-kafka[avro]) (1.8.2)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests; extra == \"avro\"->confluent-kafka[avro]) (3.0.4)\r\nRequirement already satisfied: idna<2.7,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests; extra == \"avro\"->confluent-kafka[avro]) (2.6)\r\nRequirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests; extra == \"avro\"->confluent-kafka[avro]) (1.22)\r\nRequirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests; extra == \"avro\"->confluent-kafka[avro]) (2018.4.16)\r\ndistributed 1.21.8 requires msgpack, which is not installed.\r\nInstalling collected packages: fastavro\r\n  The script fastavro.exe is installed in 'C:\\ProgramData\\Anaconda3\\Scripts' which is not on PATH.\r\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\nSuccessfully installed fastavro-0.21.23\r\nYou are using pip version 10.0.1, however version 19.1.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' command.\r\n\r\nHow to reproduce\r\n================\r\n\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\nusing Confluent 5.2\r\nwas able to publish messages using confluent-kafka-python libraries\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/599", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/599/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/599/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/599/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/599", "id": 443401085, "node_id": "MDU6SXNzdWU0NDM0MDEwODU=", "number": 599, "title": "No data from poll() after ssl activation in AvroConsumer", "user": {"login": "robooo", "id": 7584540, "node_id": "MDQ6VXNlcjc1ODQ1NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7584540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robooo", "html_url": "https://github.com/robooo", "followers_url": "https://api.github.com/users/robooo/followers", "following_url": "https://api.github.com/users/robooo/following{/other_user}", "gists_url": "https://api.github.com/users/robooo/gists{/gist_id}", "starred_url": "https://api.github.com/users/robooo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robooo/subscriptions", "organizations_url": "https://api.github.com/users/robooo/orgs", "repos_url": "https://api.github.com/users/robooo/repos", "events_url": "https://api.github.com/users/robooo/events{/privacy}", "received_events_url": "https://api.github.com/users/robooo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-13T13:33:01Z", "updated_at": "2019-05-29T08:40:03Z", "closed_at": "2019-05-29T08:40:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nWe activated ssl in our docker swarm solution for kafka it works with ./kafka-console-consumer.sh, \r\n\r\n```\r\n./kafka-console-consumer.sh --bootstrap-server XXX.XXX.XXX.XXX:9092 --topic my_topic_with_data --from-beginning --consumer.config ../config/my.properties\r\n```\r\n\r\nmy.properties\r\n```\r\nsecurity.protocol=SSL\r\nssl.truststore.location=/path/to/our.truststore.jks\r\nssl.truststore.password=password\r\nssl.keystore.location=/path/to/location.keystore.jks\r\nssl.keystore.password=password\r\nssl.key.password=password\r\n```\r\n\r\nI've updated my consumer with ssl attributes. After these changes I'm getting always no data.\r\nKafka in our swarm has version = kafka:1.0.1-2\r\nschema-registry:1.0.1 \r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\n\r\n```\r\nfrom confluent_kafka.avro import AvroConsumer\r\nfrom confluent_kafka.avro.serializer import SerializerError\r\nimport time\r\n\r\nconsumer = AvroConsumer({\r\n    'bootstrap.servers': '{}:{}'.format(\"MY_IP\", \"9092\"),\r\n    'group.id': \"random_id\",\r\n    'schema.registry.url': \"https://schemaregistry.domain.io\",\r\n    'enable.auto.commit': True,\r\n    'security.protocol': 'ssl',\r\n    'ssl.ca.location': '/path/to/caprot',\r\n    'ssl.certificate.location': '/path/to/cakafka',\r\n    'ssl.key.location': '/path/to/kafkakey',\r\n    'default.topic.config': {\r\n        'auto.offset.reset': \"earliest\"\r\n    }\r\n})\r\n\r\nconsumer.subscribe(['my_topic_with_data'])\r\n\r\n\r\nwhile True:\r\n    try:\r\n        msg = consumer.poll(timeout=5)\r\n    except SerializerError as e:\r\n        print('Message deserialization failed for {}: {}'.format(msg, e))\r\n        break\r\n\r\n    if msg is None:\r\n        time.sleep(5)\r\n        continue\r\n\r\n    if msg.error():\r\n        if msg.error().code():\r\n            continue\r\n        else:\r\n            print(msg.error())\r\n            break\r\n\r\n    print(msg.value())\r\n```\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version()` and `confluent_kafka.libversion()`):'1.0.0', 1048576\r\n - [ ] Apache Kafka broker version:\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/593", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/593/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/593/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/593/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/593", "id": 440522248, "node_id": "MDU6SXNzdWU0NDA1MjIyNDg=", "number": 593, "title": "KafkaException: KafkaError{code=_TIMED_OUT,val=-185,str=\"Timed out waiting for controller\"}", "user": {"login": "hatcherfang", "id": 20816653, "node_id": "MDQ6VXNlcjIwODE2NjUz", "avatar_url": "https://avatars2.githubusercontent.com/u/20816653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hatcherfang", "html_url": "https://github.com/hatcherfang", "followers_url": "https://api.github.com/users/hatcherfang/followers", "following_url": "https://api.github.com/users/hatcherfang/following{/other_user}", "gists_url": "https://api.github.com/users/hatcherfang/gists{/gist_id}", "starred_url": "https://api.github.com/users/hatcherfang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hatcherfang/subscriptions", "organizations_url": "https://api.github.com/users/hatcherfang/orgs", "repos_url": "https://api.github.com/users/hatcherfang/repos", "events_url": "https://api.github.com/users/hatcherfang/events{/privacy}", "received_events_url": "https://api.github.com/users/hatcherfang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-05-06T02:42:42Z", "updated_at": "2019-07-18T19:20:32Z", "closed_at": "2019-07-18T19:20:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nKafkaException: KafkaError{code=_TIMED_OUT,val=-185,str=\"Timed out waiting for controller\"}\r\n\r\n\r\n\r\nHow to reproduce\r\n================\r\nfrom confluent_kafka.admin import AdminClient\r\nself.admin = AdminClient({'bootstrap.servers': BOOTSTRAP_SERVERS})\r\nfs = self.admin.create_topics(new_topics)\r\nfor topic, f in fs.items():\r\n    f.result()\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - [x] confluent-kafka-python and librdkafka version (`confluent_kafka.version(1.0.0)` and `confluent_kafka.libversion(1.0.0)`):\r\n - [x] Apache Kafka broker version: v1.0.1\r\n - [ ] Client configuration: `{...}`\r\n - [ ] Operating system:\r\n - [ ] Provide client logs (with `'debug': '..'` as necessary)\r\n - [ ] Provide broker log excerpts\r\n - [ ] Critical issue\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/592", "repository_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python", "labels_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/592/labels{/name}", "comments_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/592/comments", "events_url": "https://api.github.com/repos/confluentinc/confluent-kafka-python/issues/592/events", "html_url": "https://github.com/confluentinc/confluent-kafka-python/issues/592", "id": 440435243, "node_id": "MDU6SXNzdWU0NDA0MzUyNDM=", "number": 592, "title": "no module named concurrent.futures", "user": {"login": "adjkldd", "id": 4162413, "node_id": "MDQ6VXNlcjQxNjI0MTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/4162413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adjkldd", "html_url": "https://github.com/adjkldd", "followers_url": "https://api.github.com/users/adjkldd/followers", "following_url": "https://api.github.com/users/adjkldd/following{/other_user}", "gists_url": "https://api.github.com/users/adjkldd/gists{/gist_id}", "starred_url": "https://api.github.com/users/adjkldd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adjkldd/subscriptions", "organizations_url": "https://api.github.com/users/adjkldd/orgs", "repos_url": "https://api.github.com/users/adjkldd/repos", "events_url": "https://api.github.com/users/adjkldd/events{/privacy}", "received_events_url": "https://api.github.com/users/adjkldd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-05T11:44:34Z", "updated_at": "2019-07-18T19:07:36Z", "closed_at": "2019-07-18T19:07:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Description\r\n===========\r\nwhen using [adminapi](https://github.com/confluentinc/confluent-kafka-python/blob/master/examples/adminapi.py) to create partitions, python2.7 complains :\r\n\r\n```\r\nImportError: No module named concurrent.futures\r\n```  \r\n\r\nHow to reproduce\r\n================\r\nrun the example code. Google tells me concurrent.futures is a python3.2 module. However, I am using confluent_kafka-1.0.0-py2.7-linux-x86_64.whl\r\n\r\n\r\n\r\nChecklist\r\n=========\r\nPlease provide the following information:\r\n\r\n - python version 2.7\r\n- confluent_kafka version (1.0.0, 1048576)\r\n- librdkafka version 1.0.0\r\n\r\n", "performed_via_github_app": null, "score": 1.0}]}