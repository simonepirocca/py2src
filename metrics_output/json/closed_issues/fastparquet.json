{"total_count": 265, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/dask/fastparquet/issues/513", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/513/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/513/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/513/events", "html_url": "https://github.com/dask/fastparquet/issues/513", "id": 676442303, "node_id": "MDU6SXNzdWU2NzY0NDIzMDM=", "number": 513, "title": "Segmentation fault (core dumped) when run to_pandas", "user": {"login": "ajing", "id": 1566338, "node_id": "MDQ6VXNlcjE1NjYzMzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1566338?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajing", "html_url": "https://github.com/ajing", "followers_url": "https://api.github.com/users/ajing/followers", "following_url": "https://api.github.com/users/ajing/following{/other_user}", "gists_url": "https://api.github.com/users/ajing/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajing/subscriptions", "organizations_url": "https://api.github.com/users/ajing/orgs", "repos_url": "https://api.github.com/users/ajing/repos", "events_url": "https://api.github.com/users/ajing/events{/privacy}", "received_events_url": "https://api.github.com/users/ajing/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-10T22:00:24Z", "updated_at": "2020-08-11T17:47:59Z", "closed_at": "2020-08-11T17:47:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**:\r\n\r\nSegmentation fault\r\n\r\n**What you expected to happen**:\r\n\r\nreturn a pandas dataframe\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport fastparquet\r\npf = fastparquet.ParquetFile('/tmp/tmpu3ivvojp/part-00009-4c3b4a78-98b6-4bca-8591-ed5bd82a1f25-c000.snappy.parquet')\r\npf.to_pandas()\r\n```\r\n\r\n**Anything else we need to know?**:\r\n\r\nThe file size is 85M\r\n\r\n**Environment**:\r\n\r\n- Dask version:0.19.1\r\n- Python version:3.7\r\n- Operating System:Ubuntu\r\n- Install method (conda, pip, source):conda\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/512", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/512/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/512/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/512/events", "html_url": "https://github.com/dask/fastparquet/issues/512", "id": 672284500, "node_id": "MDU6SXNzdWU2NzIyODQ1MDA=", "number": 512, "title": "IndexError: boolean index did not match indexed array along dimension 0; dimension is 50 but corresponding boolean dimension is 10770", "user": {"login": "Ritesh-pandey-crypto", "id": 62175324, "node_id": "MDQ6VXNlcjYyMTc1MzI0", "avatar_url": "https://avatars0.githubusercontent.com/u/62175324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ritesh-pandey-crypto", "html_url": "https://github.com/Ritesh-pandey-crypto", "followers_url": "https://api.github.com/users/Ritesh-pandey-crypto/followers", "following_url": "https://api.github.com/users/Ritesh-pandey-crypto/following{/other_user}", "gists_url": "https://api.github.com/users/Ritesh-pandey-crypto/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ritesh-pandey-crypto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ritesh-pandey-crypto/subscriptions", "organizations_url": "https://api.github.com/users/Ritesh-pandey-crypto/orgs", "repos_url": "https://api.github.com/users/Ritesh-pandey-crypto/repos", "events_url": "https://api.github.com/users/Ritesh-pandey-crypto/events{/privacy}", "received_events_url": "https://api.github.com/users/Ritesh-pandey-crypto/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-08-03T19:12:25Z", "updated_at": "2020-08-07T19:37:19Z", "closed_at": "2020-08-07T19:37:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "train_idx = np.arange(metadata.shape[0]) % 9 != 0\r\ntest_idx = np.arange(metadata.shape[0]) % 9 == 0\r\nprint(metadata.shape)\r\nprint(train_idx.shape)\r\nprint(test_idx.shape)\r\nOutput: (10770,)\r\n             (10770,)\r\n             (10770,)\r\n\r\nembeddings = np.asarray(embeddings)\r\n\r\n X_train = embeddings[train_idx]\r\nX_test = embeddings[test_idx]", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/510", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/510/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/510/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/510/events", "html_url": "https://github.com/dask/fastparquet/issues/510", "id": 664500303, "node_id": "MDU6SXNzdWU2NjQ1MDAzMDM=", "number": 510, "title": "Writing parquet files from pandas with the hive format fails", "user": {"login": "jendrikjoe", "id": 9658781, "node_id": "MDQ6VXNlcjk2NTg3ODE=", "avatar_url": "https://avatars0.githubusercontent.com/u/9658781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jendrikjoe", "html_url": "https://github.com/jendrikjoe", "followers_url": "https://api.github.com/users/jendrikjoe/followers", "following_url": "https://api.github.com/users/jendrikjoe/following{/other_user}", "gists_url": "https://api.github.com/users/jendrikjoe/gists{/gist_id}", "starred_url": "https://api.github.com/users/jendrikjoe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jendrikjoe/subscriptions", "organizations_url": "https://api.github.com/users/jendrikjoe/orgs", "repos_url": "https://api.github.com/users/jendrikjoe/repos", "events_url": "https://api.github.com/users/jendrikjoe/events{/privacy}", "received_events_url": "https://api.github.com/users/jendrikjoe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-23T14:03:00Z", "updated_at": "2020-07-23T15:00:57Z", "closed_at": "2020-07-23T15:00:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**What happened**:\r\nWhen trying to write a pandas dataframe to a parquet file with the hive file schema the following error occurs: `TypeError: argument of type 'NoneType' is not iterable`\r\n\r\n**What you expected to happen**:\r\nI can simply write the dataframe to disk.\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\nimport pandas as pd\r\ncolumn = \"data\"\r\ndf = pd.DataFrame(columns=[column], data=[(\"42\",), (\"\",), (\"0\",), (\"1\",), (\"0.0\",)],)\r\ndf.index.name = \"index\"\r\ndf.to_parquet(\"test.par\", file_scheme=\"hive\", row_group_offsets=[0, 2, 4], engine=\"fastparquet\")\r\n```\r\n\r\n**Anything else we need to know?**:\r\nPR to fix this will be there in some moments.\r\n\r\n**Environment**:\r\n\r\n- Dask version: master hash: 5544556\r\n- Python version: 3.8.2\r\n- Operating System: MacOS\r\n- Install method (conda, pip, source): source via pipenv\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/505", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/505/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/505/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/505/events", "html_url": "https://github.com/dask/fastparquet/issues/505", "id": 634526929, "node_id": "MDU6SXNzdWU2MzQ1MjY5Mjk=", "number": 505, "title": "Error in converting byte array to list of floats using dask dataframe", "user": {"login": "LeenaShekhar", "id": 12227436, "node_id": "MDQ6VXNlcjEyMjI3NDM2", "avatar_url": "https://avatars1.githubusercontent.com/u/12227436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LeenaShekhar", "html_url": "https://github.com/LeenaShekhar", "followers_url": "https://api.github.com/users/LeenaShekhar/followers", "following_url": "https://api.github.com/users/LeenaShekhar/following{/other_user}", "gists_url": "https://api.github.com/users/LeenaShekhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/LeenaShekhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LeenaShekhar/subscriptions", "organizations_url": "https://api.github.com/users/LeenaShekhar/orgs", "repos_url": "https://api.github.com/users/LeenaShekhar/repos", "events_url": "https://api.github.com/users/LeenaShekhar/events{/privacy}", "received_events_url": "https://api.github.com/users/LeenaShekhar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-06-05T18:48:47Z", "updated_at": "2020-06-08T17:35:52Z", "closed_at": "2020-06-08T17:35:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "<!-- Please include a self-contained copy-pastable example that generates the issue if possible.\r\n\r\nPlease be concise with code posted. See guidelines below on how to provide a good bug report:\r\n\r\n- Craft Minimal Bug Reports http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports\r\n- Minimal Complete Verifiable Examples https://stackoverflow.com/help/mcve\r\n\r\nBug reports that follow these guidelines are easier to diagnose, and so are often handled much more quickly.\r\n-->\r\n\r\n**What happened**: byte array is messed up when reading a parquet file using dask dataframe but not when directly using pandas dataframe. Last byte is missing in case of dask (below) but not pandas.\r\n\r\n**What you expected to happen**: Suggest if I am doing something wrong or if this is a bug fix\r\n\r\n**Minimal Complete Verifiable Example**:\r\n\r\n```python\r\n# Pandas dataframe\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: df = pd.read_parquet(\"Testing.parquet\")\r\n\r\nIn [8]: df.ByteArray[0]\r\nOut[8]: b\"\\x1a\\xfb\\xf2>\\x92\\x06\\xdf\\xbe\\xeb\\xe3\\x99\\xbe\\xf1\\xd4\\x13\\xbe\\xed\\r~>\\xe9\\x9d\\n\\xbd.\\xab0\\xbdr\\xa4\\x93\\xbd\\x96B\\xa8>\\xfe\\x7f\\x00?\\x9c\\xf9\\x9d\\xbe\\t\\xa7\\xa5=\\x13a\\x93>\\xc8^??\\x0c\\x1e\\xae>\\xd0\\x80:=\\xdf\\xbf)>\\xd8~\\xf2\\xbdy\\xc9\\xbf\\xbd\\x88\\x81\\x02?6\\x01f\\xbe\\xde\\xb0\\xad\\xbe\\x12\\x12\\xd9\\xbe\\xdc\\xb8u>pB\\x89>h<q\\xbe3\\xc0\\xed\\xbe!\\x8f\\xf0>\\xdfN\\xba>W\\xeb\\xc4<\\xfa\\xb7K\\xbc\\xfaG\\x07?]\\xc5\\xb2>FC\\x06=\\xe0\\xf59\\xbe\\x1a\\x18\\x99\\xbe\\xf4\\xf8m>\\x02g9>\\xc7\\xbb\\x83=\\xc6\\xbf?\\xbe\\xf8P\\x82\\xbeA\\xf4\\xe4>\\x0b\\x98\\xf0\\xbeM\\xf4-\\xbf\\x9b=@>h\\x1f\\x93>\\x85]\\x04>N|u=\\xed\\xf3(>\\xaaf&\\xbe<\\xf8\\r\\xbf\\x83\\xbeD\\xbe\\xfa\\x9b\\xd0\\xbe\\x8e\\\\\\x1b?UP\\xa1>\\xd3N}\\xbe\\xb8\\xcb\\x9e=\\xe4g\\x8b>Y\\xc40\\xbe1\\x97\\xfc\\xbe\\x1aNy>\\xd8\\x9c\\x03=Y\\xc2\\xfa\\xbe4G\\xce\\xbe\\xa8\\x19\\x8a>#0\\xd6\\xbe\\xcf\\xc0(\\xbe\\x8f\\x17\\xb2\\xbd\\xf9\\xf3\\xad=bi\\xa0=\\x9d\\xf4\\x06\\xbf+\\x16\\x87>r\\x8by\\xbc\\xd9vZ\\xbeu9\\x95\\xbeJ^m>\\xafC\\xa5>\\xf1\\xa1t>\\xa6E\\x95>\\xcf\\xa1\\xcc\\xbd*\\xc6I>\\x95\\x9d\\xfe\\xbdC\\xac\\xfe\\xba\\xce5,\\xbe\\xe8\\xa0\\xe3>\\x8bRB>\\xefoP=<K\\xe0\\xbe\\xbcy\\xba>\\xd1\\x05\\xed\\xbe\\xf7\\x94\\xec>\\x93\\x8b\\xa1\\xbe\\xec\\xc1t>F[\\xdd\\xbe\\x15\\x1e\\x04\\xbe\\xc9\\x8f\\x98\\xbd\\x03'\\xdb:\\x8d{s\\xbc\\xfc\\xc4\\x81\\xbc\\x87\\xc0\\x81\\xbe\\xb1Op\\xbeX\\xfe\\\\\\xbeM\\x10\\x95\\xbd\\xbd\\xe4/>\\xae\\xbb\\xb9>\\t\\xa5\\x7f\\xbe\\xf5\\x0e/?\\xaf\\x94\\x8d>\\xca\\x87\\xe0>\\xcd\\xadP\\xbe5\\xeb\\xb4\\xbe\\xe4\\x13r\\xbd%\\xce*>\\xaf\\x98\\xd9\\xbe\\x86u\\x83\\xbd\\xa6\\r\\x87=\\xf7\\x05\\x84\\xbez\\x1aP\\xbe\\xef\\x91\\r=JE\\x1f\\xbf\\xe6\\xadJ\\xbf\\xf5\\x0f2\\xbe\\xcbK\\x1e>\\x92\\xcc\\xaa\\xbe\\xb0\\xe4\\xea=lz\\x88\\xbe\\x81\\xce\\xa4=\\x00\\x00\\x00\\x00\"\r\n\r\n# Dask dataframe\r\nIn [6]: import dask.dataframe as dd\r\n\r\nIn [10]: dd = dd.read_parquet(\"Testing.parquet\")\r\nIn [13]: dd.compute().ByteArray[0]\r\nOut[13]: b\"\\x1a\\xfb\\xf2>\\x92\\x06\\xdf\\xbe\\xeb\\xe3\\x99\\xbe\\xf1\\xd4\\x13\\xbe\\xed\\r~>\\xe9\\x9d\\n\\xbd.\\xab0\\xbdr\\xa4\\x93\\xbd\\x96B\\xa8>\\xfe\\x7f\\x00?\\x9c\\xf9\\x9d\\xbe\\t\\xa7\\xa5=\\x13a\\x93>\\xc8^??\\x0c\\x1e\\xae>\\xd0\\x80:=\\xdf\\xbf)>\\xd8~\\xf2\\xbdy\\xc9\\xbf\\xbd\\x88\\x81\\x02?6\\x01f\\xbe\\xde\\xb0\\xad\\xbe\\x12\\x12\\xd9\\xbe\\xdc\\xb8u>pB\\x89>h<q\\xbe3\\xc0\\xed\\xbe!\\x8f\\xf0>\\xdfN\\xba>W\\xeb\\xc4<\\xfa\\xb7K\\xbc\\xfaG\\x07?]\\xc5\\xb2>FC\\x06=\\xe0\\xf59\\xbe\\x1a\\x18\\x99\\xbe\\xf4\\xf8m>\\x02g9>\\xc7\\xbb\\x83=\\xc6\\xbf?\\xbe\\xf8P\\x82\\xbeA\\xf4\\xe4>\\x0b\\x98\\xf0\\xbeM\\xf4-\\xbf\\x9b=@>h\\x1f\\x93>\\x85]\\x04>N|u=\\xed\\xf3(>\\xaaf&\\xbe<\\xf8\\r\\xbf\\x83\\xbeD\\xbe\\xfa\\x9b\\xd0\\xbe\\x8e\\\\\\x1b?UP\\xa1>\\xd3N}\\xbe\\xb8\\xcb\\x9e=\\xe4g\\x8b>Y\\xc40\\xbe1\\x97\\xfc\\xbe\\x1aNy>\\xd8\\x9c\\x03=Y\\xc2\\xfa\\xbe4G\\xce\\xbe\\xa8\\x19\\x8a>#0\\xd6\\xbe\\xcf\\xc0(\\xbe\\x8f\\x17\\xb2\\xbd\\xf9\\xf3\\xad=bi\\xa0=\\x9d\\xf4\\x06\\xbf+\\x16\\x87>r\\x8by\\xbc\\xd9vZ\\xbeu9\\x95\\xbeJ^m>\\xafC\\xa5>\\xf1\\xa1t>\\xa6E\\x95>\\xcf\\xa1\\xcc\\xbd*\\xc6I>\\x95\\x9d\\xfe\\xbdC\\xac\\xfe\\xba\\xce5,\\xbe\\xe8\\xa0\\xe3>\\x8bRB>\\xefoP=<K\\xe0\\xbe\\xbcy\\xba>\\xd1\\x05\\xed\\xbe\\xf7\\x94\\xec>\\x93\\x8b\\xa1\\xbe\\xec\\xc1t>F[\\xdd\\xbe\\x15\\x1e\\x04\\xbe\\xc9\\x8f\\x98\\xbd\\x03'\\xdb:\\x8d{s\\xbc\\xfc\\xc4\\x81\\xbc\\x87\\xc0\\x81\\xbe\\xb1Op\\xbeX\\xfe\\\\\\xbeM\\x10\\x95\\xbd\\xbd\\xe4/>\\xae\\xbb\\xb9>\\t\\xa5\\x7f\\xbe\\xf5\\x0e/?\\xaf\\x94\\x8d>\\xca\\x87\\xe0>\\xcd\\xadP\\xbe5\\xeb\\xb4\\xbe\\xe4\\x13r\\xbd%\\xce*>\\xaf\\x98\\xd9\\xbe\\x86u\\x83\\xbd\\xa6\\r\\x87=\\xf7\\x05\\x84\\xbez\\x1aP\\xbe\\xef\\x91\\r=JE\\x1f\\xbf\\xe6\\xadJ\\xbf\\xf5\\x0f2\\xbe\\xcbK\\x1e>\\x92\\xcc\\xaa\\xbe\\xb0\\xe4\\xea=lz\\x88\\xbe\\x81\\xce\\xa4=\"\r\n\r\n```\r\n\r\n**Anything else we need to know?**: \r\n\r\n**Environment**:\r\n\r\n- Dask version: 2.1.0\r\n- Python version: 3.7.3\r\n- Operating System: Linux\r\n- Install method (conda, pip, source): conda most probably\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/504", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/504/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/504/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/504/events", "html_url": "https://github.com/dask/fastparquet/issues/504", "id": 627155098, "node_id": "MDU6SXNzdWU2MjcxNTUwOTg=", "number": 504, "title": "fastparquet silently strips NULL bytes off the end of binary arrays", "user": {"login": "adq", "id": 322473, "node_id": "MDQ6VXNlcjMyMjQ3Mw==", "avatar_url": "https://avatars0.githubusercontent.com/u/322473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adq", "html_url": "https://github.com/adq", "followers_url": "https://api.github.com/users/adq/followers", "following_url": "https://api.github.com/users/adq/following{/other_user}", "gists_url": "https://api.github.com/users/adq/gists{/gist_id}", "starred_url": "https://api.github.com/users/adq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adq/subscriptions", "organizations_url": "https://api.github.com/users/adq/orgs", "repos_url": "https://api.github.com/users/adq/repos", "events_url": "https://api.github.com/users/adq/events{/privacy}", "received_events_url": "https://api.github.com/users/adq/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-29T10:08:05Z", "updated_at": "2020-07-16T13:48:03Z", "closed_at": "2020-07-16T13:48:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Environment\r\nfastparquet              0.3.2      \r\nnumpy                    1.18.2     \r\npython37 3.7.7-2\r\nOn arch linux.\r\n\r\n### Issue\r\nI was trying to read a parquet file with binary encoded data in it. Some of the binary encoded data would be \"randomly\" corrupt. Investigation and comparison with pyarrow showed that fastparquet is silently stripping NUL (\\0) bytes from the end of dictionary encoded binary data arrays.\r\n\r\nie the value should be b'abcde\\0'. But fastparquet returns b'abcde'.\r\n\r\ndigging into the library code shows that this is caused in core.py/read_col() where it decodes dictionary pages (two places in the code). Specifically, you do:\r\n\r\n```\r\n    if ph.type == parquet_thrift.PageType.DICTIONARY_PAGE:\r\n        dic = np.array(read_dictionary_page(infile, schema_helper, ph, cmd))\r\n```\r\n\r\nthat call to np.array causes the stripping. It appears to be a \"feature\" of np.array.\r\n\r\nConfirming this with a simple example in a python shell:\r\n\r\n```\r\n>>> import numpy\r\n>>> a = [b'abcd\\0']\r\n>>> numpy.array(a)\r\narray([b'abcd'], dtype='|S5')\r\n```\r\n\r\n### Possible fix\r\nI can obviously simply drop the np.array() calls, but I don't know what consequences this would have on performance etc.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/501", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/501/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/501/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/501/events", "html_url": "https://github.com/dask/fastparquet/issues/501", "id": 618408133, "node_id": "MDU6SXNzdWU2MTg0MDgxMzM=", "number": 501, "title": "Missing dependency on `packaging` for 0.4.0 release", "user": {"login": "nairb774", "id": 128795, "node_id": "MDQ6VXNlcjEyODc5NQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/128795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairb774", "html_url": "https://github.com/nairb774", "followers_url": "https://api.github.com/users/nairb774/followers", "following_url": "https://api.github.com/users/nairb774/following{/other_user}", "gists_url": "https://api.github.com/users/nairb774/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairb774/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairb774/subscriptions", "organizations_url": "https://api.github.com/users/nairb774/orgs", "repos_url": "https://api.github.com/users/nairb774/repos", "events_url": "https://api.github.com/users/nairb774/events{/privacy}", "received_events_url": "https://api.github.com/users/nairb774/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-14T17:23:04Z", "updated_at": "2020-07-16T13:50:28Z", "closed_at": "2020-05-15T14:06:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "setup.py was not updated to add this dependency. I'm seeing `ModuleNotFoundError: No module named 'packaging'` on a minimal venv. It looks like the import for `packaging` was added in https://github.com/dask/fastparquet/pull/496.\r\n\r\nHow to reproduce:\r\n\r\n```\r\n$ python3 -m venv fastparquet\r\n\r\n$ fastparquet/bin/pip install --upgrade pip\r\nCollecting pip\r\n  Using cached https://files.pythonhosted.org/packages/54/2e/df11ea7e23e7e761d484ed3740285a34e38548cf2bad2bed3dd5768ec8b9/pip-20.1-py2.py3-none-any.whl\r\nInstalling collected packages: pip\r\n  Found existing installation: pip 19.2.3\r\n    Uninstalling pip-19.2.3:\r\n      Successfully uninstalled pip-19.2.3\r\nSuccessfully installed pip-20.1\r\n\r\n$ fastparquet/bin/pip install fastparquet\r\nCollecting fastparquet\r\n  Downloading fastparquet-0.4.0.tar.gz (152 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 152 kB 1.9 MB/s\r\nCollecting pandas>=0.19\r\n  Using cached pandas-1.0.3-cp37-cp37m-macosx_10_9_x86_64.whl (10.0 MB)\r\nCollecting numba>=0.28\r\n  Downloading numba-0.49.1-cp37-cp37m-macosx_10_14_x86_64.whl (2.1 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.1 MB 7.3 MB/s\r\nCollecting numpy>=1.11\r\n  Using cached numpy-1.18.4-cp37-cp37m-macosx_10_9_x86_64.whl (15.1 MB)\r\nCollecting thrift>=0.11.0\r\n  Using cached thrift-0.13.0.tar.gz (59 kB)\r\nCollecting six\r\n  Using cached six-1.14.0-py2.py3-none-any.whl (10 kB)\r\nCollecting pytz>=2017.2\r\n  Using cached pytz-2020.1-py2.py3-none-any.whl (510 kB)\r\nCollecting python-dateutil>=2.6.1\r\n  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\r\nCollecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\r\n  Downloading llvmlite-0.32.1-cp37-cp37m-macosx_10_9_x86_64.whl (15.9 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.9 MB 777 kB/s\r\nRequirement already satisfied: setuptools in ./fastparquet/lib/python3.7/site-packages (from numba>=0.28->fastparquet) (41.2.0)\r\nCould not build wheels for fastparquet, since package 'wheel' is not installed.\r\nCould not build wheels for thrift, since package 'wheel' is not installed.\r\nCould not build wheels for setuptools, since package 'wheel' is not installed.\r\nInstalling collected packages: numpy, pytz, six, python-dateutil, pandas, llvmlite, numba, thrift, fastparquet\r\n    Running setup.py install for thrift ... done\r\n    Running setup.py install for fastparquet ... done\r\nSuccessfully installed fastparquet-0.4.0 llvmlite-0.32.1 numba-0.49.1 numpy-1.18.4 pandas-1.0.3 python-dateutil-2.8.1 pytz-2020.1 six-1.14.0 thrift-0.13.0\r\n\r\n$ fastparquet/bin/python -c 'import fastparquet'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"fastparquet/lib/python3.7/site-packages/fastparquet/__init__.py\", line 5, in <module>\r\n    from .core import read_thrift\r\n  File \"fastparquet/lib/python3.7/site-packages/fastparquet/core.py\", line 9, in <module>\r\n    from . import encoding\r\n  File \"fastparquet/lib/python3.7/site-packages/fastparquet/encoding.py\", line 12, in <module>\r\n    from packaging import version\r\nModuleNotFoundError: No module named 'packaging'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/499", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/499/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/499/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/499/events", "html_url": "https://github.com/dask/fastparquet/issues/499", "id": 612662914, "node_id": "MDU6SXNzdWU2MTI2NjI5MTQ=", "number": 499, "title": "fast parquet write with multiprocessing.process in python stuck in AWS lambda", "user": {"login": "nayendur", "id": 43058456, "node_id": "MDQ6VXNlcjQzMDU4NDU2", "avatar_url": "https://avatars0.githubusercontent.com/u/43058456?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nayendur", "html_url": "https://github.com/nayendur", "followers_url": "https://api.github.com/users/nayendur/followers", "following_url": "https://api.github.com/users/nayendur/following{/other_user}", "gists_url": "https://api.github.com/users/nayendur/gists{/gist_id}", "starred_url": "https://api.github.com/users/nayendur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nayendur/subscriptions", "organizations_url": "https://api.github.com/users/nayendur/orgs", "repos_url": "https://api.github.com/users/nayendur/repos", "events_url": "https://api.github.com/users/nayendur/events{/privacy}", "received_events_url": "https://api.github.com/users/nayendur/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-05T14:49:07Z", "updated_at": "2020-05-06T05:21:54Z", "closed_at": "2020-05-06T05:21:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using fastparquet write function which takes panda's data frame to convert to parquet files and then write to S3 from different python process using multiprocessing process module. I am providing s3fs.open handle to write function. sample code as follows\r\n\r\nhttps://gist.github.com/nayendur/c242bdf7ccd12fa78097c2d055e9b73f\r\n\r\nIn aws lambda logs, I can able to see the logs while starting the process. I can able to see the logs inside the function as well. However, process getting stuck while invoking write function.\r\n\r\nIs fastparquet capable of writing from multiple process? Reason for considering multiprocessing is to speed up the writing to S3.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/495", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/495/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/495/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/495/events", "html_url": "https://github.com/dask/fastparquet/issues/495", "id": 605690673, "node_id": "MDU6SXNzdWU2MDU2OTA2NzM=", "number": 495, "title": "Update numba jitclass imports", "user": {"login": "luk-f-a", "id": 17914484, "node_id": "MDQ6VXNlcjE3OTE0NDg0", "avatar_url": "https://avatars1.githubusercontent.com/u/17914484?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luk-f-a", "html_url": "https://github.com/luk-f-a", "followers_url": "https://api.github.com/users/luk-f-a/followers", "following_url": "https://api.github.com/users/luk-f-a/following{/other_user}", "gists_url": "https://api.github.com/users/luk-f-a/gists{/gist_id}", "starred_url": "https://api.github.com/users/luk-f-a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luk-f-a/subscriptions", "organizations_url": "https://api.github.com/users/luk-f-a/orgs", "repos_url": "https://api.github.com/users/luk-f-a/repos", "events_url": "https://api.github.com/users/luk-f-a/events{/privacy}", "received_events_url": "https://api.github.com/users/luk-f-a/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2020-04-23T16:51:38Z", "updated_at": "2020-06-25T17:30:55Z", "closed_at": "2020-05-09T13:26:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! Thanks a lot for this package.\r\n\r\nI'd like to raise the issue that Numba has changed the location of `jitclass`. In the current version (0.49) we are getting a bunch of DeprecationWarnings, and from 0.50 this imports will fail. The new location is numba.experimental.jitclass.\r\n\r\n```\r\nC:\\Users\\\\miniconda3\\envs\\litt-conda\\lib\\site-packages\\fastparquet\\encoding.py:222: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\r\n  Numpy8 = numba.jitclass(spec8)(NumpyIO)\r\nC:\\Users\\\\miniconda3\\envs\\litt-conda\\lib\\site-packages\\fastparquet\\encoding.py:224: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\r\n  Numpy32 = numba.jitclass(spec32)(NumpyIO)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/489", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/489/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/489/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/489/events", "html_url": "https://github.com/dask/fastparquet/issues/489", "id": 583935255, "node_id": "MDU6SXNzdWU1ODM5MzUyNTU=", "number": 489, "title": "my_nan in core.read_col", "user": {"login": "jbrockmendel", "id": 8078968, "node_id": "MDQ6VXNlcjgwNzg5Njg=", "avatar_url": "https://avatars1.githubusercontent.com/u/8078968?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbrockmendel", "html_url": "https://github.com/jbrockmendel", "followers_url": "https://api.github.com/users/jbrockmendel/followers", "following_url": "https://api.github.com/users/jbrockmendel/following{/other_user}", "gists_url": "https://api.github.com/users/jbrockmendel/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbrockmendel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbrockmendel/subscriptions", "organizations_url": "https://api.github.com/users/jbrockmendel/orgs", "repos_url": "https://api.github.com/users/jbrockmendel/repos", "events_url": "https://api.github.com/users/jbrockmendel/events{/privacy}", "received_events_url": "https://api.github.com/users/jbrockmendel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-18T18:50:56Z", "updated_at": "2020-03-19T13:21:43Z", "closed_at": "2020-03-19T13:21:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "In experimenting with a refactor in pandas internals (using DatetimeArray/TimedeltaArray in a couple of places instead of ndarray) I stumbled on this in `fp.core.readcol`:\r\n\r\n```\r\n        elif assign.dtype.kind in [\"M\", 'm']:\r\n            my_nan = -9223372036854775808  # int64 version of NaT\r\n```\r\n\r\nSetting `my_nan` into an `np.ndarray` is allowed, but setting it into a pandas `DatetimeArray` is not, so one of our downstream tests failed here.\r\n\r\nInstead of the integer value, would it be viable to use `my_nan = assign.dtype.type(\"NaT\")` here?  That fixes the issue for me locally.\r\n\r\nIf this is acceptable, I'll open a PR.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/486", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/486/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/486/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/486/events", "html_url": "https://github.com/dask/fastparquet/issues/486", "id": 574627827, "node_id": "MDU6SXNzdWU1NzQ2Mjc4Mjc=", "number": 486, "title": "Stale documentation for fasparquet.write", "user": {"login": "eadanfahey", "id": 6443106, "node_id": "MDQ6VXNlcjY0NDMxMDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/6443106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eadanfahey", "html_url": "https://github.com/eadanfahey", "followers_url": "https://api.github.com/users/eadanfahey/followers", "following_url": "https://api.github.com/users/eadanfahey/following{/other_user}", "gists_url": "https://api.github.com/users/eadanfahey/gists{/gist_id}", "starred_url": "https://api.github.com/users/eadanfahey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eadanfahey/subscriptions", "organizations_url": "https://api.github.com/users/eadanfahey/orgs", "repos_url": "https://api.github.com/users/eadanfahey/repos", "events_url": "https://api.github.com/users/eadanfahey/events{/privacy}", "received_events_url": "https://api.github.com/users/eadanfahey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-03T12:16:34Z", "updated_at": "2020-03-03T19:10:43Z", "closed_at": "2020-03-03T19:10:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "The documentation for the `compression` argument to [`fastparquet.write`](https://fastparquet.readthedocs.io/en/latest/api.html#fastparquet.write) is out of date, and the provided example produces the error:\r\n\r\n```\r\nFile \"~/.local/lib/python3.6/site-packages/fastparquet/compression.py\", line 69, in lz4_compress\r\n    return lz4.block.compress(data, **kwargs)\r\nTypeError: 'compression_level' is an invalid keyword argument for this function\r\n```\r\n\r\nThe error is because the API to `python-lz4` library [has changed](https://python-lz4.readthedocs.io/en/stable/lz4.block.html#lz4.block.compress) (`compression_level` and `content_checksum` are no longer arguments of `lz4.block.compress`). A more up-to-date example could be:\r\n\r\n```\r\n{\r\n    col1: {\r\n        \"type\": \"LZ4\",\r\n        \"args\": {\r\n            \"mode\": \"high_compression\",\r\n            \"compression\": 9\r\n         }\r\n    },\r\n    col2: {\r\n        \"type\": \"SNAPPY\",\r\n        \"args\": None\r\n    }\r\n    \"_default\": {\r\n        \"type\": \"GZIP\",\r\n        \"args\": None\r\n    }\r\n}\r\n```\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/480", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/480/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/480/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/480/events", "html_url": "https://github.com/dask/fastparquet/issues/480", "id": 559973572, "node_id": "MDU6SXNzdWU1NTk5NzM1NzI=", "number": 480, "title": "Update file metadata num_rows", "user": {"login": "martindurant", "id": 6042212, "node_id": "MDQ6VXNlcjYwNDIyMTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6042212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martindurant", "html_url": "https://github.com/martindurant", "followers_url": "https://api.github.com/users/martindurant/followers", "following_url": "https://api.github.com/users/martindurant/following{/other_user}", "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}", "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions", "organizations_url": "https://api.github.com/users/martindurant/orgs", "repos_url": "https://api.github.com/users/martindurant/repos", "events_url": "https://api.github.com/users/martindurant/events{/privacy}", "received_events_url": "https://api.github.com/users/martindurant/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-04T20:56:06Z", "updated_at": "2020-02-07T15:25:17Z", "closed_at": "2020-02-07T15:25:17Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "cc https://stackoverflow.com/questions/60010395/impala-fails-at-creating-partitioned-table-because-of-corrupted-parquet-file/60010874", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/478", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/478/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/478/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/478/events", "html_url": "https://github.com/dask/fastparquet/issues/478", "id": 554578003, "node_id": "MDU6SXNzdWU1NTQ1NzgwMDM=", "number": 478, "title": "Index KeyError when appending", "user": {"login": "gvelchuru", "id": 1124570, "node_id": "MDQ6VXNlcjExMjQ1NzA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1124570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gvelchuru", "html_url": "https://github.com/gvelchuru", "followers_url": "https://api.github.com/users/gvelchuru/followers", "following_url": "https://api.github.com/users/gvelchuru/following{/other_user}", "gists_url": "https://api.github.com/users/gvelchuru/gists{/gist_id}", "starred_url": "https://api.github.com/users/gvelchuru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gvelchuru/subscriptions", "organizations_url": "https://api.github.com/users/gvelchuru/orgs", "repos_url": "https://api.github.com/users/gvelchuru/repos", "events_url": "https://api.github.com/users/gvelchuru/events{/privacy}", "received_events_url": "https://api.github.com/users/gvelchuru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-24T07:08:19Z", "updated_at": "2020-01-24T20:59:22Z", "closed_at": "2020-01-24T20:59:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I get the following error when appending a DataFrame (after checking all columns, etc. are the same)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 113, in <module>\r\n    HOLDINGS = ta.get_data()\r\n  File \"/home/ubuntu/theta_gang/daily_price/price_module.py\", line 196, in get_data\r\n    compute=True,\r\n  File \"/home/ubuntu/miniconda3/envs/dev/lib/python3.7/site-packages/dask/dataframe/core.py\", line 3751, in to_parquet\r\n    return to_parquet(self, path, *args, **kwargs)\r\n  File \"/home/ubuntu/miniconda3/envs/dev/lib/python3.7/site-packages/dask/dataframe/io/parquet/core.py\", line 424, in to_parquet\r\nholdings queue worker exited\r\n    **kwargs_pass\r\n  File \"/home/ubuntu/miniconda3/envs/dev/lib/python3.7/site-packages/dask/dataframe/io/parquet/fastparquet.py\", line 432, in initialize_write\r\n    old_end = minmax[index_cols[0]][\"max\"][-1]\r\nKeyError: 'index'\r\n```\r\n\r\nThe offending line is here:\r\n```      \r\nself.dask_frame.to_parquet(\r\n              \"s3://tmp-th\" if self.dry_run else self.datapath,\r\n              append=not self.dry_run,\r\n              compute=True,\r\n          )\r\n```\r\nAny idea why this is?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/473", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/473/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/473/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/473/events", "html_url": "https://github.com/dask/fastparquet/issues/473", "id": 548602999, "node_id": "MDU6SXNzdWU1NDg2MDI5OTk=", "number": 473, "title": "fastparquet 0.3.2 RuntimeError: Ran out of input", "user": {"login": "apiszcz", "id": 1430861, "node_id": "MDQ6VXNlcjE0MzA4NjE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1430861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apiszcz", "html_url": "https://github.com/apiszcz", "followers_url": "https://api.github.com/users/apiszcz/followers", "following_url": "https://api.github.com/users/apiszcz/following{/other_user}", "gists_url": "https://api.github.com/users/apiszcz/gists{/gist_id}", "starred_url": "https://api.github.com/users/apiszcz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apiszcz/subscriptions", "organizations_url": "https://api.github.com/users/apiszcz/orgs", "repos_url": "https://api.github.com/users/apiszcz/repos", "events_url": "https://api.github.com/users/apiszcz/events{/privacy}", "received_events_url": "https://api.github.com/users/apiszcz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-12T17:45:57Z", "updated_at": "2020-01-13T18:04:47Z", "closed_at": "2020-01-13T18:04:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Any thoughts or advice on the following stack trace appreciated.\r\nThis occurs after hours of and hundreds of reads.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"lib\\_testp\\testp.py\", line 1626, in <module>\r\n    main()\r\n  File \"lib\\_testp\\testp.py\", line 1616, in main\r\n    ost.run(oc, ol, ou, modes, src)\r\n  File \"lib\\_testp\\testp.py\", line 1398, in run\r\n    self.track(oc, ol, ou, modes, src)\r\n  File \"lib\\_testp\\testp.py\", line 724, in track\r\n    df,t_pf = get_parquet_filtered()\r\n  File \"lib\\_testp\\testp.py\", line 850, in track_get_parquet_filtered\r\n    ('data', '<=', 100),\r\n  File \"C:\\user1\\lib\\fastparquet\\api.py\", line 430, in to_pandas\r\n    index=index, assign=parts)\r\n  File \"C:\\user1\\lib\\fastparquet\\api.py\", line 259, in read_row_group\r\n    scheme=self.file_scheme)\r\n  File \"C:\\user1\\lib\\fastparquet\\core.py\", line 354, in read_row_group\r\n    cats, selfmade, assign=assign)\r\n  File \"C:\\user1\\lib\\fastparquet\\core.py\", line 331, in read_row_group_arrays\r\n    catdef=out.get(name+'-catdef', None))\r\n  File \"C:\\user1\\lib\\fastparquet\\core.py\", line 245, in read_col\r\n    skip_nulls, selfmade=selfmade)\r\n  File \"C:\\user1\\lib\\fastparquet\\core.py\", line 118, in read_data_page\r\n    width=width)\r\n  File \"C:\\user1\\lib\\fastparquet\\encoding.py\", line 56, in read_plain\r\n    return np.array(unpack_byte_array(raw_bytes, count), dtype='O')\r\n  File \"fastparquet\\speedups.pyx\", line 163, in fastparquet.speedups.unpack_byte_array\r\nRuntimeError: Ran out of input\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/472", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/472/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/472/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/472/events", "html_url": "https://github.com/dask/fastparquet/issues/472", "id": 546598015, "node_id": "MDU6SXNzdWU1NDY1OTgwMTU=", "number": 472, "title": "ParquetFile(file) NotADirectoryError: error[20] Not a directory: '/xxx/xxx/xx.parquet/ _metadata'", "user": {"login": "Wercurial", "id": 43528116, "node_id": "MDQ6VXNlcjQzNTI4MTE2", "avatar_url": "https://avatars0.githubusercontent.com/u/43528116?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wercurial", "html_url": "https://github.com/Wercurial", "followers_url": "https://api.github.com/users/Wercurial/followers", "following_url": "https://api.github.com/users/Wercurial/following{/other_user}", "gists_url": "https://api.github.com/users/Wercurial/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wercurial/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wercurial/subscriptions", "organizations_url": "https://api.github.com/users/Wercurial/orgs", "repos_url": "https://api.github.com/users/Wercurial/repos", "events_url": "https://api.github.com/users/Wercurial/events{/privacy}", "received_events_url": "https://api.github.com/users/Wercurial/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-01-08T01:40:04Z", "updated_at": "2020-01-08T06:20:34Z", "closed_at": "2020-01-08T06:20:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, let me ask you a question. \r\nsystem: centos 6.6, fastparquet 0.1.5, anaconda python 3.6.3\r\nI use pandas.to_parquet() to split a small parquet file from a large parquet file. Reading the small parquet file with fastparquet.ParquetFile (file) will report the error as the topic.\r\n but I will merge 3600 such small parquet files into a large parquet file, and then read it with fastparquet.ParquetFile (file). Excuse me ,Do you know why?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/470", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/470/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/470/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/470/events", "html_url": "https://github.com/dask/fastparquet/issues/470", "id": 537882671, "node_id": "MDU6SXNzdWU1Mzc4ODI2NzE=", "number": 470, "title": "Importing from pandas.core.index is deprecated", "user": {"login": "jorisvandenbossche", "id": 1020496, "node_id": "MDQ6VXNlcjEwMjA0OTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorisvandenbossche", "html_url": "https://github.com/jorisvandenbossche", "followers_url": "https://api.github.com/users/jorisvandenbossche/followers", "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}", "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions", "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs", "repos_url": "https://api.github.com/users/jorisvandenbossche/repos", "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}", "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-14T08:06:17Z", "updated_at": "2020-01-14T12:17:30Z", "closed_at": "2020-01-14T12:17:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "The imports here (and maybe in other places as well):\r\n\r\nhttps://github.com/dask/fastparquet/blob/8767c2f94975aed8d052a9d0ad6f5e4e630a6f12/fastparquet/dataframe.py#L5\r\n\r\nis deprecated now in pandas master (and it has also been private before). Those imports are simply available from the top-level namespace, so no need to import them from there.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/468", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/468/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/468/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/468/events", "html_url": "https://github.com/dask/fastparquet/issues/468", "id": 526893274, "node_id": "MDU6SXNzdWU1MjY4OTMyNzQ=", "number": 468, "title": "Release wheels for Python 3.8", "user": {"login": "tswast", "id": 247555, "node_id": "MDQ6VXNlcjI0NzU1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/247555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tswast", "html_url": "https://github.com/tswast", "followers_url": "https://api.github.com/users/tswast/followers", "following_url": "https://api.github.com/users/tswast/following{/other_user}", "gists_url": "https://api.github.com/users/tswast/gists{/gist_id}", "starred_url": "https://api.github.com/users/tswast/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tswast/subscriptions", "organizations_url": "https://api.github.com/users/tswast/orgs", "repos_url": "https://api.github.com/users/tswast/repos", "events_url": "https://api.github.com/users/tswast/events{/privacy}", "received_events_url": "https://api.github.com/users/tswast/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-21T23:12:23Z", "updated_at": "2020-02-18T21:20:20Z", "closed_at": "2020-02-18T21:20:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "When trying to install `fastparquet` in Python 3.8, I get the following error:\r\n\r\n```\r\n$ pip install fastparquet\r\nProcessing /Users/swast/Library/Caches/pip/wheels/b9/36/13/01416a760ddcab0eb8281ec9c9ffcbed945c9b831647c8b904/fastparquet-0.3.2-cp38-cp38-macosx_10_9_x86_64.whl\r\nProcessing /Users/swast/Library/Caches/pip/wheels/f7/e4/ac/860af9e439e40f5beccba61955fdaa04ee329e79f33d644d5c/numba-0.46.0-cp38-cp38-macosx_10_9_x86_64.whl\r\nRequirement already satisfied: six in ./.nox/unit-3-8/lib/python3.8/site-packages (from fastparquet) (1.13.0)\r\nRequirement already satisfied: pandas>=0.19 in ./.nox/unit-3-8/lib/python3.8/site-packages (from fastparquet) (0.25.3)\r\nProcessing /Users/swast/Library/Caches/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e/thrift-0.13.0-cp38-cp38-macosx_10_9_x86_64.whl\r\nRequirement already satisfied: numpy>=1.11 in ./.nox/unit-3-8/lib/python3.8/site-packages (from fastparquet) (1.17.4)\r\nCollecting llvmlite>=0.30.0dev0\r\n  Using cached https://files.pythonhosted.org/packages/8b/b0/df26861e6ce2fc91c8bb93ea808fa2e631ee8a29fc4c3bc96626b78dae74/llvmlite-0.30.0.tar.gz\r\nRequirement already satisfied: pytz>=2017.2 in ./.nox/unit-3-8/lib/python3.8/site-packages (from pandas>=0.19->fastparquet) (2019.3)\r\nRequirement already satisfied: python-dateutil>=2.6.1 in ./.nox/unit-3-8/lib/python3.8/site-packages (from pandas>=0.19->fastparquet) (2.8.1)\r\nBuilding wheels for collected packages: llvmlite\r\n  Building wheel for llvmlite (setup.py) ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-wheel-61sg6l15 --python-tag cp38\r\n       cwd: /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/\r\n  Complete output (26 lines):\r\n  running bdist_wheel\r\n  /Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/python3.8 /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\r\n  LLVM version... Traceback (most recent call last):\r\n    File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 105, in main_posix\r\n      out = subprocess.check_output([llvm_config, '--version'])\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 411, in check_output\r\n      return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 489, in run\r\n      with Popen(*popenargs, **kwargs) as process:\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 854, in __init__\r\n      self._execute_child(args, executable, preexec_fn, close_fds,\r\n    File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 1702, in _execute_child\r\n      raise child_exception_type(errno_num, err_msg, err_filename)\r\n  FileNotFoundError: [Errno 2] No such file or directory: 'llvm-config'\r\n  \r\n  During handling of the above exception, another exception occurred:\r\n  \r\n  Traceback (most recent call last):\r\n    File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 168, in <module>\r\n      main()\r\n    File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 162, in main\r\n      main_posix('osx', '.dylib')\r\n    File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 107, in main_posix\r\n      raise RuntimeError(\"%s failed executing, please point LLVM_CONFIG \"\r\n  RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config\r\n  error: command '/Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/python3.8' failed with exit status 1\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for llvmlite\r\n  Running setup.py clean for llvmlite\r\nFailed to build llvmlite\r\nInstalling collected packages: llvmlite, numba, thrift, fastparquet\r\n    Running setup.py install for llvmlite ... error\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-record-pgd56hvj/install-record.txt --single-version-externally-managed --compile --install-headers /Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/../include/site/python3.8/llvmlite\r\n         cwd: /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/\r\n    Complete output (29 lines):\r\n    running install\r\n    running build\r\n    got version from file /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/llvmlite/_version.py {'version': '0.30.0', 'full': '3cad106e2f727bc400f1b426ce306b1e30e34b72'}\r\n    running build_ext\r\n    /Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/python3.8 /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\r\n    LLVM version... Traceback (most recent call last):\r\n      File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 105, in main_posix\r\n        out = subprocess.check_output([llvm_config, '--version'])\r\n      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 411, in check_output\r\n        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 489, in run\r\n        with Popen(*popenargs, **kwargs) as process:\r\n      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 854, in __init__\r\n        self._execute_child(args, executable, preexec_fn, close_fds,\r\n      File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 1702, in _execute_child\r\n        raise child_exception_type(errno_num, err_msg, err_filename)\r\n    FileNotFoundError: [Errno 2] No such file or directory: 'llvm-config'\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 168, in <module>\r\n        main()\r\n      File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 162, in main\r\n        main_posix('osx', '.dylib')\r\n      File \"/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/ffi/build.py\", line 107, in main_posix\r\n        raise RuntimeError(\"%s failed executing, please point LLVM_CONFIG \"\r\n    RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config\r\n    error: command '/Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/python3.8' failed with exit status 1\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/python3.8 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-install-yjstph0o/llvmlite/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/sm/phwfkhts38114zggj4_l13pc006gf0/T/pip-record-pgd56hvj/install-record.txt --single-version-externally-managed --compile --install-headers /Users/swast/src/google-cloud-python/bigquery/.nox/unit-3-8/bin/../include/site/python3.8/llvmlite Check the logs for full command output.\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/467", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/467/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/467/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/467/events", "html_url": "https://github.com/dask/fastparquet/issues/467", "id": 523607721, "node_id": "MDU6SXNzdWU1MjM2MDc3MjE=", "number": 467, "title": "KeyError: 0 - Works in Pandas, not in Dask", "user": {"login": "oeyvindds", "id": 13333063, "node_id": "MDQ6VXNlcjEzMzMzMDYz", "avatar_url": "https://avatars2.githubusercontent.com/u/13333063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oeyvindds", "html_url": "https://github.com/oeyvindds", "followers_url": "https://api.github.com/users/oeyvindds/followers", "following_url": "https://api.github.com/users/oeyvindds/following{/other_user}", "gists_url": "https://api.github.com/users/oeyvindds/gists{/gist_id}", "starred_url": "https://api.github.com/users/oeyvindds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oeyvindds/subscriptions", "organizations_url": "https://api.github.com/users/oeyvindds/orgs", "repos_url": "https://api.github.com/users/oeyvindds/repos", "events_url": "https://api.github.com/users/oeyvindds/events{/privacy}", "received_events_url": "https://api.github.com/users/oeyvindds/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-15T17:53:47Z", "updated_at": "2019-11-15T18:05:22Z", "closed_at": "2019-11-15T17:59:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is the exact input. It works fine in Pandas, but not in Dask:\r\n\r\nfrom fastparquet import write\r\ndf = dsk.compute()\r\n\r\ndf.to_parquet('data/yelp_subset.parq', compression='gzip')\r\n\r\nwrite('data/yelp_subset.parq',dsk, compression='gzip')\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/oeyvind/.local/share/virtualenvs/dds-qlJdqjIA/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 2897, in get_loc\r\n    return self._engine.get_loc(key)\r\n  File \"pandas/_libs/index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\nKeyError: 0\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/Users/oeyvind/.local/share/virtualenvs/dds-qlJdqjIA/lib/python3.7/site-packages/fastparquet/writer.py\", line 849, in write\r\n    cols = set(data)\r\n  File \"/Users/oeyvind/.local/share/virtualenvs/dds-qlJdqjIA/lib/python3.7/site-packages/dask/dataframe/core.py\", line 3234, in __getitem__\r\n    meta = self._meta[_extract_meta(key)]\r\n  File \"/Users/oeyvind/.local/share/virtualenvs/dds-qlJdqjIA/lib/python3.7/site-packages/pandas/core/frame.py\", line 2995, in __getitem__\r\n    indexer = self.columns.get_loc(key)\r\n  File \"/Users/oeyvind/.local/share/virtualenvs/dds-qlJdqjIA/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 2899, in get_loc\r\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\r\n  File \"pandas/_libs/index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\nKeyError: 0\r\n\r\nDoes anyone have any idea about what have gone wrong here? The file is about 70 mb. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/466", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/466/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/466/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/466/events", "html_url": "https://github.com/dask/fastparquet/issues/466", "id": 507684153, "node_id": "MDU6SXNzdWU1MDc2ODQxNTM=", "number": 466, "title": "Skip bad lines when writing dataframe", "user": {"login": "matanpel", "id": 44837161, "node_id": "MDQ6VXNlcjQ0ODM3MTYx", "avatar_url": "https://avatars0.githubusercontent.com/u/44837161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matanpel", "html_url": "https://github.com/matanpel", "followers_url": "https://api.github.com/users/matanpel/followers", "following_url": "https://api.github.com/users/matanpel/following{/other_user}", "gists_url": "https://api.github.com/users/matanpel/gists{/gist_id}", "starred_url": "https://api.github.com/users/matanpel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matanpel/subscriptions", "organizations_url": "https://api.github.com/users/matanpel/orgs", "repos_url": "https://api.github.com/users/matanpel/repos", "events_url": "https://api.github.com/users/matanpel/events{/privacy}", "received_events_url": "https://api.github.com/users/matanpel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-16T08:04:07Z", "updated_at": "2019-10-16T14:30:43Z", "closed_at": "2019-10-16T14:30:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "When writing a pandas dataframe to parquet with fastparquet, if a line has a value that does not correspond with the column's data type, the operation fails.\r\n\r\nIt would be great to have an option to skip bad lines, like pandas df.to_csv() has the error_bad_lines=False option.\r\n\r\nCode:\r\n```\r\ndf.to_parquet(\r\n            file_path,\r\n            engine='fastparquet',\r\n            compression='gzip',\r\n            index=False,\r\n            times='int96'\r\n        )\r\n```\r\n\r\nError:\r\n\r\n> Can't infer object conversion type: 0    9030-09-11 00:00:00", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/462", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/462/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/462/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/462/events", "html_url": "https://github.com/dask/fastparquet/issues/462", "id": 491102334, "node_id": "MDU6SXNzdWU0OTExMDIzMzQ=", "number": 462, "title": "Use \"field_name\" in pandas metadata for consistency with pyarrow", "user": {"login": "jorisvandenbossche", "id": 1020496, "node_id": "MDQ6VXNlcjEwMjA0OTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorisvandenbossche", "html_url": "https://github.com/jorisvandenbossche", "followers_url": "https://api.github.com/users/jorisvandenbossche/followers", "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}", "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions", "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs", "repos_url": "https://api.github.com/users/jorisvandenbossche/repos", "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}", "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-09T13:39:04Z", "updated_at": "2019-09-11T13:57:52Z", "closed_at": "2019-09-11T13:57:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "From the original report in pandas (https://github.com/pandas-dev/pandas/issues/28252) about a file written with fastparquet that cannot be read with pyarrow, I fixed this in the pyarrow metadata compatibility handling (https://issues.apache.org/jira/browse/ARROW-6492, https://github.com/apache/arrow/pull/5331).\r\n\r\nAlthough this particular issues will thus be fixed in the next pyarrow release, it would still be good that both libraries write consistent pandas metadata. \r\n\r\nThe main difference that caused the above bug was the `\"field_name\"` key in the columns descriptions (which fastparquet does not write, but pyarrow does). \r\nWould you be open to write the `\"field_name\"` key as well (which would be identical to `\"name\"` in all cases supported by fastparquet I think)? This would also follow was is currently written in the spec (https://dev.pandas.io/development/developer.html#storing-pandas-dataframe-objects-in-apache-parquet-format)\r\n\r\nAn example where pyarrow uses the fact of having both is non-default index without name: fastparquet gives this a default generated \"index\" name, so the written parquet file comes back with this name. In pyarrow the `\"field_name\"` is set to the generated name used in the parquet file, but `\"name\"` is still `None`, so on roundtripping pyarrow can \"preserve\" the fact that the index has no name.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/460", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/460/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/460/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/460/events", "html_url": "https://github.com/dask/fastparquet/issues/460", "id": 486716026, "node_id": "MDU6SXNzdWU0ODY3MTYwMjY=", "number": 460, "title": "Latest version of fastparquet/s3fs/fsspec incompatible with dask/distributed 2.1.0.", "user": {"login": "birdsarah", "id": 1796208, "node_id": "MDQ6VXNlcjE3OTYyMDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1796208?v=4", "gravatar_id": "", "url": "https://api.github.com/users/birdsarah", "html_url": "https://github.com/birdsarah", "followers_url": "https://api.github.com/users/birdsarah/followers", "following_url": "https://api.github.com/users/birdsarah/following{/other_user}", "gists_url": "https://api.github.com/users/birdsarah/gists{/gist_id}", "starred_url": "https://api.github.com/users/birdsarah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/birdsarah/subscriptions", "organizations_url": "https://api.github.com/users/birdsarah/orgs", "repos_url": "https://api.github.com/users/birdsarah/repos", "events_url": "https://api.github.com/users/birdsarah/events{/privacy}", "received_events_url": "https://api.github.com/users/birdsarah/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-08-29T03:30:31Z", "updated_at": "2019-08-29T04:45:46Z", "closed_at": "2019-08-29T04:45:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Latest version of fastparquet/s3fs/fsspec incompatible with dask/distributed 2.1.0.\r\n\r\nWhen running dask/distributed pinned to 2.1.0 with latest version of s3fs and fastparquet \r\ninstalled from conda-forge to give the env detailed: https://gist.github.com/birdsarah/a1eebbe5e5320f3e613cfc7277afe0a2\r\n\r\nI get the following error when trying to write_parquet\r\n\r\n```\r\n08/29/2019 03:23:08 AM UTC expected list of bytes\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-12-b0b8e2c0fe2e>\", line 15, in <module>\r\n    storage_options={'s3_additional_kwargs': sse_args},\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/dask/dataframe/core.py\", line 3618, in to_parquet\r\n    return to_parquet(self, path, *args, **kwargs)\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/dask/dataframe/io/parquet.py\", line 1496, in to_parquet\r\n    out.compute()\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/dask/base.py\", line 175, in compute\r\n    (result,) = compute(self, traverse=False, **kwargs)\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/dask/base.py\", line 446, in compute\r\n    results = schedule(dsk, keys, **kwargs)\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/distributed/client.py\", line 2527, in get\r\n    results = self.gather(packed, asynchronous=asynchronous, direct=direct)\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/distributed/client.py\", line 1823, in gather\r\n    asynchronous=asynchronous,\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/distributed/client.py\", line 763, in sync\r\n    self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/distributed/utils.py\", line 332, in sync\r\n    six.reraise(*error[0])\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/distributed/utils.py\", line 317, in f\r\n    result[0] = yield future\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\r\n    value = future.result()\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\r\n    yielded = self.gen.throw(*exc_info)  # type: ignore\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/distributed/client.py\", line 1680, in _gather\r\n    six.reraise(type(exception), exception, traceback)\r\n  File \"/mnt/miniconda/envs/jestr-etl/lib/python3.7/site-packages/six.py\", line 692, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1566335097751_3811/container_1566335097751_3811_01_000029/environment/lib/python3.7/site-packages/dask/dataframe/io/parquet.py\", line 668, in _write_partition_fastparquet\r\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1566335097751_3811/container_1566335097751_3811_01_000029/environment/lib/python3.7/site-packages/fastparquet/writer.py\", line 631, in make_part_file\r\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1566335097751_3811/container_1566335097751_3811_01_000029/environment/lib/python3.7/site-packages/fastparquet/writer.py\", line 619, in make_row_group\r\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1566335097751_3811/container_1566335097751_3811_01_000029/environment/lib/python3.7/site-packages/fastparquet/writer.py\", line 513, in write_column\r\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1566335097751_3811/container_1566335097751_3811_01_000029/environment/lib/python3.7/site-packages/fastparquet/writer.py\", line 254, in encode_plain\r\n  File \"fastparquet/speedups.pyx\", line 112, in fastparquet.speedups.pack_byte_array\r\n    raise TypeError(\"expected list of bytes\")\r\nTypeError: expected list of bytes\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/457", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/457/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/457/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/457/events", "html_url": "https://github.com/dask/fastparquet/issues/457", "id": 481479973, "node_id": "MDU6SXNzdWU0ODE0Nzk5NzM=", "number": 457, "title": "AttributeError When Read certain columns from ParquetFile", "user": {"login": "yushizhao", "id": 23001757, "node_id": "MDQ6VXNlcjIzMDAxNzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/23001757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yushizhao", "html_url": "https://github.com/yushizhao", "followers_url": "https://api.github.com/users/yushizhao/followers", "following_url": "https://api.github.com/users/yushizhao/following{/other_user}", "gists_url": "https://api.github.com/users/yushizhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/yushizhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yushizhao/subscriptions", "organizations_url": "https://api.github.com/users/yushizhao/orgs", "repos_url": "https://api.github.com/users/yushizhao/repos", "events_url": "https://api.github.com/users/yushizhao/events{/privacy}", "received_events_url": "https://api.github.com/users/yushizhao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-08-16T07:24:25Z", "updated_at": "2019-08-21T05:54:55Z", "closed_at": "2019-08-21T05:54:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-19-2124c416cd8c> in <module>\r\n----> 1 first = next(iter(pf.iter_row_groups(columns=[\"BidUpdate\"])))\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/api.py in iter_row_groups(self, columns, categories, filters, index)\r\n    359                                                   categories, index)\r\n    360                     self.read_row_group(rg, columns, categories, infile=f,\r\n--> 361                                         index=index, assign=views)\r\n    362                     yield df\r\n    363         else:\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/api.py in read_row_group(self, rg, columns, categories, infile, index, assign)\r\n    257                 infile, rg, columns, categories, self.schema, self.cats,\r\n    258                 self.selfmade, index=index, assign=assign,\r\n--> 259                 scheme=self.file_scheme)\r\n    260         if ret:\r\n    261             return df\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/core.py in read_row_group(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme)\r\n    352         raise RuntimeError('Going with pre-allocation!')\r\n    353     read_row_group_arrays(file, rg, columns, categories, schema_helper,\r\n--> 354                           cats, selfmade, assign=assign)\r\n    355 \r\n    356     for cat in cats:\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/core.py in read_row_group_arrays(file, rg, columns, categories, schema_helper, cats, selfmade, assign)\r\n    329         read_col(column, schema_helper, file, use_cat=name+'-catdef' in out,\r\n    330                  selfmade=selfmade, assign=out[name],\r\n--> 331                  catdef=out.get(name+'-catdef', None))\r\n    332 \r\n    333         if _is_map_like(schema_helper, column):\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/core.py in read_col(column, schema_helper, infile, use_cat, grab_dict, selfmade, assign, catdef)\r\n    261                         parquet_thrift.FieldRepetitionType.REQUIRED)\r\n    262             row_idx = 1 + encoding._assemble_objects(assign, defi, rep, val, dic, d,\r\n--> 263                                              null, null_val, max_defi, row_idx)\r\n    264         elif defi is not None:\r\n    265             max_defi = schema_helper.max_definition_level(cmd.path_in_schema)\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/encoding.py in _assemble_objects(assign, defi, rep, val, dic, d, null, null_val, max_defi, prev_i)\r\n    286         assign[i] = None if have_null else part\r\n    287     else: # can only happen if the only elements in this page are the continuation of the last row from previous page\r\n--> 288         assign[i - 1].extend(part)\r\n    289     return i\r\n    290 \r\n\r\nAttributeError: 'NoneType' object has no attribute 'extend'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/456", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/456/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/456/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/456/events", "html_url": "https://github.com/dask/fastparquet/issues/456", "id": 481089566, "node_id": "MDU6SXNzdWU0ODEwODk1NjY=", "number": 456, "title": "Read _metadata file", "user": {"login": "marchinidavide", "id": 37088541, "node_id": "MDQ6VXNlcjM3MDg4NTQx", "avatar_url": "https://avatars3.githubusercontent.com/u/37088541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marchinidavide", "html_url": "https://github.com/marchinidavide", "followers_url": "https://api.github.com/users/marchinidavide/followers", "following_url": "https://api.github.com/users/marchinidavide/following{/other_user}", "gists_url": "https://api.github.com/users/marchinidavide/gists{/gist_id}", "starred_url": "https://api.github.com/users/marchinidavide/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marchinidavide/subscriptions", "organizations_url": "https://api.github.com/users/marchinidavide/orgs", "repos_url": "https://api.github.com/users/marchinidavide/repos", "events_url": "https://api.github.com/users/marchinidavide/events{/privacy}", "received_events_url": "https://api.github.com/users/marchinidavide/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-08-15T10:24:40Z", "updated_at": "2019-08-15T14:43:32Z", "closed_at": "2019-08-15T14:43:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi everyone!\r\n\r\nAs title, how can I read the `_metadata` file created when running a `dd.to_parquet()`?\r\nAlso, how can I modify it?\r\nMaybe a a dumb question, please point me to the right docs in that case!\r\n\r\nBest, Davide", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/450", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/450/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/450/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/450/events", "html_url": "https://github.com/dask/fastparquet/issues/450", "id": 469868937, "node_id": "MDU6SXNzdWU0Njk4Njg5Mzc=", "number": 450, "title": "Fastparquet crashes in official Python 3.7.4 docker image", "user": {"login": "philipp-block", "id": 24493918, "node_id": "MDQ6VXNlcjI0NDkzOTE4", "avatar_url": "https://avatars0.githubusercontent.com/u/24493918?v=4", "gravatar_id": "", "url": "https://api.github.com/users/philipp-block", "html_url": "https://github.com/philipp-block", "followers_url": "https://api.github.com/users/philipp-block/followers", "following_url": "https://api.github.com/users/philipp-block/following{/other_user}", "gists_url": "https://api.github.com/users/philipp-block/gists{/gist_id}", "starred_url": "https://api.github.com/users/philipp-block/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/philipp-block/subscriptions", "organizations_url": "https://api.github.com/users/philipp-block/orgs", "repos_url": "https://api.github.com/users/philipp-block/repos", "events_url": "https://api.github.com/users/philipp-block/events{/privacy}", "received_events_url": "https://api.github.com/users/philipp-block/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-18T16:29:06Z", "updated_at": "2019-07-23T19:17:50Z", "closed_at": "2019-07-23T19:17:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using fastparquet in the official Python 3.7.4 docker image, the Python process exits with a segmentation fault and non-zero exit code (but after successfully executing the code).\r\n\r\nA reproducible example can be found in https://github.com/philipp-block/fastparquet-py374-crash . The example uses pandas, but the same behavior happens when using Dask.\r\n\r\nThe segfault doesn't happen when using python3.6 or python3.7.3 docker images.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/448", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/448/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/448/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/448/events", "html_url": "https://github.com/dask/fastparquet/issues/448", "id": 466640768, "node_id": "MDU6SXNzdWU0NjY2NDA3Njg=", "number": 448, "title": "fastparquet \"write\" fails to import in macos environment", "user": {"login": "arnoldad", "id": 5604896, "node_id": "MDQ6VXNlcjU2MDQ4OTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/5604896?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arnoldad", "html_url": "https://github.com/arnoldad", "followers_url": "https://api.github.com/users/arnoldad/followers", "following_url": "https://api.github.com/users/arnoldad/following{/other_user}", "gists_url": "https://api.github.com/users/arnoldad/gists{/gist_id}", "starred_url": "https://api.github.com/users/arnoldad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arnoldad/subscriptions", "organizations_url": "https://api.github.com/users/arnoldad/orgs", "repos_url": "https://api.github.com/users/arnoldad/repos", "events_url": "https://api.github.com/users/arnoldad/events{/privacy}", "received_events_url": "https://api.github.com/users/arnoldad/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-11T03:28:27Z", "updated_at": "2019-07-11T03:48:10Z", "closed_at": "2019-07-11T03:46:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Environment:\r\nMacOS Mojave, Python 3.7\r\n\r\nSteps to reproduce from system python install OR via conda virtualenv:\r\n\r\n`from fastparquet import write`\r\n\r\n\r\nException:\r\n```\r\nAssertion failed: (PassInf && \"Expected all immutable passes to be initialized\"), function addImmutablePass, file /opt/concourse/worker/volumes/live/d296f8df-3e99-45ac-67f1-8ec44282abf4/volume/llvmdev_1546571852224/work/lib/IR/LegacyPassManager.cpp, line 812.\r\nAbort trap: 6\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/447", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/447/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/447/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/447/events", "html_url": "https://github.com/dask/fastparquet/issues/447", "id": 464857745, "node_id": "MDU6SXNzdWU0NjQ4NTc3NDU=", "number": 447, "title": "Local variable 'tz' referenced before assignment", "user": {"login": "Japkeerat", "id": 39667771, "node_id": "MDQ6VXNlcjM5NjY3Nzcx", "avatar_url": "https://avatars3.githubusercontent.com/u/39667771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Japkeerat", "html_url": "https://github.com/Japkeerat", "followers_url": "https://api.github.com/users/Japkeerat/followers", "following_url": "https://api.github.com/users/Japkeerat/following{/other_user}", "gists_url": "https://api.github.com/users/Japkeerat/gists{/gist_id}", "starred_url": "https://api.github.com/users/Japkeerat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Japkeerat/subscriptions", "organizations_url": "https://api.github.com/users/Japkeerat/orgs", "repos_url": "https://api.github.com/users/Japkeerat/repos", "events_url": "https://api.github.com/users/Japkeerat/events{/privacy}", "received_events_url": "https://api.github.com/users/Japkeerat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-06T14:08:14Z", "updated_at": "2019-07-08T20:14:45Z", "closed_at": "2019-07-08T20:14:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "This error arises when trying to load a parquet file that doesn't exist.\r\n\r\nInstalled Fastparquet using Conda today.\r\nFastparquet version: 0.3.0\r\nConda version: 4.7.5\r\nPython version: 3.7.3\r\n\r\nError StackTrace:\r\n\r\n```\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-20-4b86cd248646> in <module>\r\n      1 from fastparquet import ParquetFile\r\n      2 # weather = pd.read_parquet(all_weather_files[0])\r\n----> 3 pf = ParquetFile(all_weather_files[0])\r\n\r\n~\\Anaconda3\\lib\\site-packages\\fastparquet\\api.py in __init__(self, fn, verify, open_with, root, sep)\r\n    114                 self.fn = join_path(fn)\r\n    115                 with open_with(fn, 'rb') as f:\r\n--> 116                     self._parse_header(f, verify)\r\n    117         self.open = open_with\r\n    118         self.sep = sep\r\n\r\n~\\Anaconda3\\lib\\site-packages\\fastparquet\\api.py in _parse_header(self, f, verify)\r\n    138         self.head_size = head_size\r\n    139         self.fmd = fmd\r\n--> 140         self._set_attrs()\r\n    141 \r\n    142     def _set_attrs(self):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\fastparquet\\api.py in _set_attrs(self)\r\n    159         self.file_scheme = get_file_scheme(files)\r\n    160         self._read_partitions()\r\n--> 161         self._dtypes()\r\n    162 \r\n    163     @property\r\n\r\n~\\Anaconda3\\lib\\site-packages\\fastparquet\\api.py in _dtypes(self, categories)\r\n    523                         dtype[col] = np.dtype('f8')\r\n    524             elif dt.kind == \"M\":\r\n--> 525                 if tz.get(col, False):\r\n    526                     dtype[col] = pd.Series([], dtype='M8[ns]'\r\n    527                                            ).dt.tz_localize(tz[col]).dtype\r\n\r\nUnboundLocalError: local variable 'tz' referenced before assignment\r\n```\r\n\r\nI followed the Stacktrace on the api.py file on Github and it seems the issue is actually resolved but not yet published with Conda. However, I would like the person actively maintaining the project to look into it and not take my word for it.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/445", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/445/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/445/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/445/events", "html_url": "https://github.com/dask/fastparquet/issues/445", "id": 462901026, "node_id": "MDU6SXNzdWU0NjI5MDEwMjY=", "number": 445, "title": "Pip installing fastparquet on Linux requests a pre-release version of numpy", "user": {"login": "tswast", "id": 247555, "node_id": "MDQ6VXNlcjI0NzU1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/247555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tswast", "html_url": "https://github.com/tswast", "followers_url": "https://api.github.com/users/tswast/followers", "following_url": "https://api.github.com/users/tswast/following{/other_user}", "gists_url": "https://api.github.com/users/tswast/gists{/gist_id}", "starred_url": "https://api.github.com/users/tswast/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tswast/subscriptions", "organizations_url": "https://api.github.com/users/tswast/orgs", "repos_url": "https://api.github.com/users/tswast/repos", "events_url": "https://api.github.com/users/tswast/events{/privacy}", "received_events_url": "https://api.github.com/users/tswast/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-07-01T20:38:38Z", "updated_at": "2019-07-11T15:14:08Z", "closed_at": "2019-07-03T00:14:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "We're getting failure to install `fastparquet` on Python 2.7 due to a dependency on a pre-release version of numpy. https://github.com/googleapis/google-cloud-python/issues/8549\r\n\r\nThis is reproducible by running `pip install fastparquet` on a Python 2.7 virtualenv.\r\n\r\n```\r\n$ pip install fastparquet\r\nDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintain[40/550]\r\n that date. A future version of pip will drop support for Python 2.7.\r\nCollecting fastparquet\r\n  Using cached https://files.pythonhosted.org/packages/85/b9/dc59386bc5824f86c640e7178fc78986f0c81763b924b2e37337ffb6a563/fastparquet-0.3.1.tar\r\n.gz\r\n    ERROR: Complete output from command python setup.py egg_info:\r\n    ERROR: Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-install-QzepJr/fastparquet/setup.py\", line 98, in <module>\r\n        **extra\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/__init__.py\", line 144, in setup\r\n        _install_setup_requires(attrs)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/__init__.py\", line 139, in _install_setup_require\r\ns\r\n        dist.fetch_build_eggs(dist.setup_requires)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/dist.py\", line 717, in fetch_build_eggs\r\n        replace_conflicting=True,\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 866, in resolve\r\n        replace_conflicting=replace_conflicting\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1146, in best_match\r\n        return self.obtain(req, installer)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1158, in obtain\r\n        return installer(requirement)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/dist.py\", line 784, in fetch_build_egg\r\n        return cmd.easy_install(req)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 679, in easy_insta\r\nll\r\n        return self.install_item(spec, dist.location, tmpdir, deps)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 705, in install_it\r\nem\r\n        dists = self.install_eggs(spec, download, tmpdir)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 890, in install_eg\r\ngs\r\n        return self.build_and_install(setup_script, setup_base)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1158, in build_and\r\n_install\r\n        self.run_setup(setup_script, setup_base, args)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1144, in run_setup\r\n        run_setup(setup_script, args)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 253, in run_setup\r\n        raise\r\n      File \"/usr/lib/python2.7/contextlib.py\", line 35, in __exit__\r\n        self.gen.throw(type, value, traceback)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 195, in setup_context\r\n        yield\r\n      File \"/usr/lib/python2.7/contextlib.py\", line 35, in __exit__\r\n        self.gen.throw(type, value, traceback)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 166, in save_modules\r\n        saved_exc.resume()\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 141, in resume\r\n        six.reraise(type, exc, self._tb)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 154, in save_modules\r\n        yield saved\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 195, in setup_context\r\n        yield\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 250, in run_setup\r\n        _execfile(setup_script, ns)\r\n      File \"/usr/local/google/home/swast/venv/bq/local/lib/python2.7/site-packages/setuptools/sandbox.py\", line 45, in _execfile\r\n        exec(code, globals, locals)\r\n      File \"/tmp/easy_install-jVnDVe/numpy-1.17.0rc1/setup.py\", line 31, in <module>\r\n        modules_to_build = {\r\n    RuntimeError: Python version >= 3.5 required.\r\n    ----------------------------------------\r\nERROR: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-QzepJr/fastparquet/\r\n```\r\n\r\nThis also affects Python 3, in that a pre-release version of numpy is still installed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/443", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/443/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/443/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/443/events", "html_url": "https://github.com/dask/fastparquet/issues/443", "id": 458249473, "node_id": "MDU6SXNzdWU0NTgyNDk0NzM=", "number": 443, "title": "Missing data when parsing Parquet files containing maps with complex elements", "user": {"login": "cmenguy", "id": 1641931, "node_id": "MDQ6VXNlcjE2NDE5MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1641931?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cmenguy", "html_url": "https://github.com/cmenguy", "followers_url": "https://api.github.com/users/cmenguy/followers", "following_url": "https://api.github.com/users/cmenguy/following{/other_user}", "gists_url": "https://api.github.com/users/cmenguy/gists{/gist_id}", "starred_url": "https://api.github.com/users/cmenguy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cmenguy/subscriptions", "organizations_url": "https://api.github.com/users/cmenguy/orgs", "repos_url": "https://api.github.com/users/cmenguy/repos", "events_url": "https://api.github.com/users/cmenguy/events{/privacy}", "received_events_url": "https://api.github.com/users/cmenguy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-06-19T22:13:28Z", "updated_at": "2019-06-21T18:39:38Z", "closed_at": "2019-06-21T18:39:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "For example I have some Parquet files where one of the column has the following schema (in Spark):\r\n\r\n```\r\n |-- myNestedMap: map (nullable = true)\r\n |    |-- key: string\r\n |    |-- value: map (valueContainsNull = true)\r\n |    |    |-- key: string\r\n |    |    |-- value: struct (valueContainsNull = true)\r\n |    |    |    |-- field1: timestamp (nullable = true)\r\n |    |    |    |-- payload: struct (nullable = true)\r\n |    |    |    |    |-- payloadField1: double (nullable = true)\r\n |    |    |    |    |-- payloadField2: string (nullable = true)\r\n```\r\n\r\nAlthough this is parsing without errors with `fastparquet`, it seems like this column is missing some data and shows as simply an array containing just the keys. The entire values are gone.\r\n\r\nFile attached which exhibits this behavior: \r\n\r\n[nestedMapClean.zip](https://github.com/dask/fastparquet/files/3308059/nestedMapClean.zip)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/442", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/442/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/442/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/442/events", "html_url": "https://github.com/dask/fastparquet/issues/442", "id": 457087914, "node_id": "MDU6SXNzdWU0NTcwODc5MTQ=", "number": 442, "title": "Deprecated dtype argument in Block.make_block_same_class", "user": {"login": "TomAugspurger", "id": 1312546, "node_id": "MDQ6VXNlcjEzMTI1NDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1312546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TomAugspurger", "html_url": "https://github.com/TomAugspurger", "followers_url": "https://api.github.com/users/TomAugspurger/followers", "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}", "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}", "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions", "organizations_url": "https://api.github.com/users/TomAugspurger/orgs", "repos_url": "https://api.github.com/users/TomAugspurger/repos", "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}", "received_events_url": "https://api.github.com/users/TomAugspurger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-06-17T18:32:25Z", "updated_at": "2019-07-02T13:55:46Z", "closed_at": "2019-07-02T12:56:56Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Just FYI, \r\n\r\n\r\n```python\r\nIn [1]: import warnings\r\nIn [2]: import fastparquet\r\nIn [3]: warnings.simplefilter('always', DeprecationWarning)\r\nIn [6]: import pandas as pd\r\nIn [7]: fastparquet.dataframe.empty([pd.DatetimeTZDtype(tz=\"UTC\")], 10, cols=['a'], timezones={'a': 'UTC'})\r\n/Users/taugspurger/Envs/dask-dev/lib/python3.7/site-packages/pandas/core/internals/blocks.py:231: DeprecationWarning: dtype argument is deprecated, will be removed in a future release.\r\n  \"in a future release.\", DeprecationWarning)\r\nOut[7]:\r\n(                                    a\r\n 0 2152-09-02 23:52:03.034234880+00:00\r\n 1 2079-08-09 02:21:19.214470707+00:00\r\n 2 1970-01-01 00:00:04.840882181+00:00\r\n 3 1970-01-01 00:00:04.840938992+00:00\r\n 4 1970-01-01 00:00:04.840939184+00:00\r\n 5 1970-01-01 00:00:04.583459448+00:00\r\n 6 1970-01-01 00:00:04.840968304+00:00\r\n 7 1970-01-01 00:00:04.538786008+00:00\r\n 8 1970-01-01 00:00:04.840968560+00:00\r\n 9 1970-01-17 06:56:19.724521968+00:00,\r\n {'a': array(['2152-09-02T23:52:03.034234880', '2079-08-09T02:21:19.214470707',\r\n         '1970-01-01T00:00:04.840882181', '1970-01-01T00:00:04.840938992',\r\n         '1970-01-01T00:00:04.840939184', '1970-01-01T00:00:04.583459448',\r\n         '1970-01-01T00:00:04.840968304', '1970-01-01T00:00:04.538786008',\r\n         '1970-01-01T00:00:04.840968560', '1970-01-17T06:56:19.724521968'],\r\n        dtype='datetime64[ns]')})\r\n\r\n```\r\n\r\nI think something like this should work.\r\n\r\n```python\r\nipdb> pp block.make_block_same_class(type(block.values)(values, dtype=block.values.dtype))\r\nDatetimeTZBlock: slice(0, 1, 1), 1 x 10, dtype: datetime64[ns, UTC]\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/441", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/441/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/441/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/441/events", "html_url": "https://github.com/dask/fastparquet/issues/441", "id": 456436036, "node_id": "MDU6SXNzdWU0NTY0MzYwMzY=", "number": 441, "title": "Pandas Period and including period frequency in pandas metadata", "user": {"login": "alexifm", "id": 47069922, "node_id": "MDQ6VXNlcjQ3MDY5OTIy", "avatar_url": "https://avatars1.githubusercontent.com/u/47069922?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexifm", "html_url": "https://github.com/alexifm", "followers_url": "https://api.github.com/users/alexifm/followers", "following_url": "https://api.github.com/users/alexifm/following{/other_user}", "gists_url": "https://api.github.com/users/alexifm/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexifm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexifm/subscriptions", "organizations_url": "https://api.github.com/users/alexifm/orgs", "repos_url": "https://api.github.com/users/alexifm/repos", "events_url": "https://api.github.com/users/alexifm/events{/privacy}", "received_events_url": "https://api.github.com/users/alexifm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-06-14T20:54:40Z", "updated_at": "2019-06-21T18:39:38Z", "closed_at": "2019-06-21T18:39:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Would it make sense to optionally include a key `period_freq` in the pandas metadata?  This would only get used to call `.to_period(pd_metadata[col]['period_freq'])` if `period_freq` is there.\r\n\r\nRelatedly, what is the reason for not including the partition columns in the pandas metadata?  If I have a Period column that I partition on, it would be nice to convert back to Period using the metadata.   ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/437", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/437/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/437/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/437/events", "html_url": "https://github.com/dask/fastparquet/issues/437", "id": 453735885, "node_id": "MDU6SXNzdWU0NTM3MzU4ODU=", "number": 437, "title": "to_datetime removes _meta.index.name but does not remove the partition index names", "user": {"login": "bolliger32", "id": 4801430, "node_id": "MDQ6VXNlcjQ4MDE0MzA=", "avatar_url": "https://avatars2.githubusercontent.com/u/4801430?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bolliger32", "html_url": "https://github.com/bolliger32", "followers_url": "https://api.github.com/users/bolliger32/followers", "following_url": "https://api.github.com/users/bolliger32/following{/other_user}", "gists_url": "https://api.github.com/users/bolliger32/gists{/gist_id}", "starred_url": "https://api.github.com/users/bolliger32/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bolliger32/subscriptions", "organizations_url": "https://api.github.com/users/bolliger32/orgs", "repos_url": "https://api.github.com/users/bolliger32/repos", "events_url": "https://api.github.com/users/bolliger32/events{/privacy}", "received_events_url": "https://api.github.com/users/bolliger32/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-08T00:27:07Z", "updated_at": "2019-06-08T00:43:27Z", "closed_at": "2019-06-08T00:43:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "This problem was noticed when trying to call `to_parquet` and getting a columns do not match the metadata `ValueError`\r\n\r\n\r\nExample:\r\n-------\r\n\r\nin.csv:\r\n```ix,dates\r\n0,1900-01-01T00:00:00.000000000\r\n1,2019-01-07T00:00:00.000000000\r\n2,2012-04-18T00:00:00.000000000\r\n3,2004-07-01T00:00:00.000000000\r\n4,2010-05-12T00:00:00.000000000\r\n```\r\n\r\n```\r\nimport dask.dataframe as ddf\r\nthis_ddf = ddf.from_pandas(pd.read_csv('in.csv', index_col=0),npartitions=1)\r\nthis_ddf.dates = ddf.to_datetime(this_ddf.dates, format='%Y-%m-%dT%H:%M:%S')\r\nprint(this_ddf._meta.index.name, this_ddf.head().index.name)\r\n```\r\n\r\nThe printed value is: `(None, 'ix')`. Expected result: `('ix', 'ix')`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/436", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/436/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/436/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/436/events", "html_url": "https://github.com/dask/fastparquet/issues/436", "id": 452654774, "node_id": "MDU6SXNzdWU0NTI2NTQ3NzQ=", "number": 436, "title": "Pandas 025 has made RangeIndex start, stop & step public and dropped the like-named internal atiributes", "user": {"login": "topper-123", "id": 26364415, "node_id": "MDQ6VXNlcjI2MzY0NDE1", "avatar_url": "https://avatars1.githubusercontent.com/u/26364415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/topper-123", "html_url": "https://github.com/topper-123", "followers_url": "https://api.github.com/users/topper-123/followers", "following_url": "https://api.github.com/users/topper-123/following{/other_user}", "gists_url": "https://api.github.com/users/topper-123/gists{/gist_id}", "starred_url": "https://api.github.com/users/topper-123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/topper-123/subscriptions", "organizations_url": "https://api.github.com/users/topper-123/orgs", "repos_url": "https://api.github.com/users/topper-123/repos", "events_url": "https://api.github.com/users/topper-123/events{/privacy}", "received_events_url": "https://api.github.com/users/topper-123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-06-05T18:35:42Z", "updated_at": "2019-12-11T14:21:35Z", "closed_at": "2019-07-30T14:48:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nPandas has in master refactored ``RangeIndex`` to use a single python ``range`` in a internal ``._range`` attribute, rather than three separate ``_start``, ``_stop`` & ``_step`` integer attributes. Those have been deprecated, emitting a ``DeprecationWarning``.\r\n\r\nPublic-facing ``start``, ``stop`` & ``step`` attributes have been added to ``RangeIndex``, and they replace the private ones. \r\n\r\nIf I understand it correctly, there has been some discussion with pandas maintainers about fastparquet could use those private attributes. If this change causes too much trouble, please let me know.\r\n\r\nSee https://github.com/pandas-dev/pandas/pull/26581 for the relevant PR.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/434", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/434/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/434/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/434/events", "html_url": "https://github.com/dask/fastparquet/issues/434", "id": 445601198, "node_id": "MDU6SXNzdWU0NDU2MDExOTg=", "number": 434, "title": "\"Non-categorical multi-index is likely brittle\"", "user": {"login": "Dr-Irv", "id": 15113894, "node_id": "MDQ6VXNlcjE1MTEzODk0", "avatar_url": "https://avatars0.githubusercontent.com/u/15113894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dr-Irv", "html_url": "https://github.com/Dr-Irv", "followers_url": "https://api.github.com/users/Dr-Irv/followers", "following_url": "https://api.github.com/users/Dr-Irv/following{/other_user}", "gists_url": "https://api.github.com/users/Dr-Irv/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dr-Irv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dr-Irv/subscriptions", "organizations_url": "https://api.github.com/users/Dr-Irv/orgs", "repos_url": "https://api.github.com/users/Dr-Irv/repos", "events_url": "https://api.github.com/users/Dr-Irv/events{/privacy}", "received_events_url": "https://api.github.com/users/Dr-Irv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-05-17T20:15:03Z", "updated_at": "2019-05-17T20:56:34Z", "closed_at": "2019-05-17T20:56:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "We are switching from HDF files to Parquet files, and got this warning:\r\n\"Non-categorical multi-index is likely brittle\"\r\n\r\nNow we are wondering if we shouldn't have switched.  I'm thinking this warning was from the time before fastparquet had support for `MultiIndex` column names.\r\n\r\nCan anyone indicate what may still be brittle?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/433", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/433/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/433/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/433/events", "html_url": "https://github.com/dask/fastparquet/issues/433", "id": 444743711, "node_id": "MDU6SXNzdWU0NDQ3NDM3MTE=", "number": 433, "title": "Dataframes with datetime indexes and timezone can be written but not read - TypeError: data type not understood", "user": {"login": "LorisMarini", "id": 18545132, "node_id": "MDQ6VXNlcjE4NTQ1MTMy", "avatar_url": "https://avatars0.githubusercontent.com/u/18545132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LorisMarini", "html_url": "https://github.com/LorisMarini", "followers_url": "https://api.github.com/users/LorisMarini/followers", "following_url": "https://api.github.com/users/LorisMarini/following{/other_user}", "gists_url": "https://api.github.com/users/LorisMarini/gists{/gist_id}", "starred_url": "https://api.github.com/users/LorisMarini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LorisMarini/subscriptions", "organizations_url": "https://api.github.com/users/LorisMarini/orgs", "repos_url": "https://api.github.com/users/LorisMarini/repos", "events_url": "https://api.github.com/users/LorisMarini/events{/privacy}", "received_events_url": "https://api.github.com/users/LorisMarini/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-05-16T04:01:54Z", "updated_at": "2019-06-13T15:01:19Z", "closed_at": "2019-06-13T15:01:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Description\r\nThis bug concerns pandas dataframes which contain Datetime data with timezone information. When such data is part of a dataframe column, both writing and reading to/from a parquet file succeed. When the same data is part of the frame index, writing succeeds but reading fails. Note that if the datetime array is timezone insensitive (e.g. pd.date_range(start=\"2019-01-01\", end=\"2019-05-01\", freq=\"M\", tz=None)), everything works fine. \r\n\r\n## Packages versions\r\npandas=0.24.2\r\nfastparquet=0.3.1 (conda-forge)\r\n\r\n## How to reproduce the error\r\n\r\n\\# Prepare dataframe\r\ndata_array = np.empty((4,3))\r\ndata_array[:] = np.nan\r\nindex = pd.DatetimeIndex(pd.date_range(start=\"2019-01-01\", end=\"2019-05-01\", freq=\"M\", tz=\"UTC\"))                       \r\ndf = pd.DataFrame(index=index, data=data_array, columns=[\"A\", \"B\", \"C\"])\r\n\r\n\\# Move Datetime data to the columns\r\nA = df.reset_index()\r\n\r\n\\# Keep Datetime on the index\r\nB = df\r\n\r\n\\# Save and read back A\r\npath=\"/tmp/test_pkl_to_par.parquet\"\r\n\r\n\\# Write and read back A (works fine)\r\nA.to_parquet(path, engine=\"fastparquet\", compression='snappy', file_scheme='simple')\r\n_ = pd.read_parquet(path, engine='fastparquet', columns=None)\r\nprint(\"Save and read back A: SUCCESS.\")\r\n\r\n\\# Write and read back B (Raises error)\r\nB.to_parquet(path, engine=\"fastparquet\", compression='snappy', file_scheme='simple')\r\n_ = pd.read_parquet(path, engine='fastparquet', columns=None)\r\n\r\n## Output \r\n\r\n```\r\nSave and read back A: SUCCESS.\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-105-332bf3f633c3> in <module>()\r\n     16 # Save and read back B\r\n     17 B.to_parquet(path, engine=\"fastparquet\", compression='snappy', file_scheme='simple')\r\n---> 18 pd.read_parquet(path, engine='fastparquet', columns=None)\r\n\r\n/opt/conda/envs/myenv/lib/python3.6/site-packages/pandas/io/parquet.py in read_parquet(path, engine, columns, **kwargs)\r\n    280 \r\n    281     impl = get_engine(engine)\r\n--> 282     return impl.read(path, columns=columns, **kwargs)\r\n\r\n/opt/conda/envs/myenv/lib/python3.6/site-packages/pandas/io/parquet.py in read(self, path, columns, **kwargs)\r\n    209             parquet_file = self.api.ParquetFile(path)\r\n    210 \r\n--> 211         return parquet_file.to_pandas(columns=columns, **kwargs)\r\n    212 \r\n    213 \r\n\r\n/opt/conda/envs/myenv/lib/python3.6/site-packages/fastparquet/api.py in to_pandas(self, columns, categories, filters, index)\r\n    416             columns += [i for i in index if i not in columns]\r\n    417         check_column_names(self.columns + list(self.cats), columns, categories)\r\n--> 418         df, views = self.pre_allocate(size, columns, categories, index)\r\n    419         start = 0\r\n    420         if self.file_scheme == 'simple':\r\n\r\n/opt/conda/envs/myenv/lib/python3.6/site-packages/fastparquet/api.py in pre_allocate(self, size, columns, categories, index)\r\n    440         categories = self.check_categories(categories)\r\n    441         return _pre_allocate(size, columns, categories, index, self.cats,\r\n--> 442                              self._dtypes(categories), self.tz)\r\n    443 \r\n    444     @property\r\n\r\n/opt/conda/envs/myenv/lib/python3.6/site-packages/fastparquet/api.py in _pre_allocate(size, columns, categories, index, cs, dt, tz)\r\n    555     dtypes.extend(['category'] * len(cs))\r\n    556     df, views = dataframe.empty(dtypes, size, cols=cols, index_names=index,\r\n--> 557                                 index_types=index_types, cats=cats, timezones=tz)\r\n    558     return df, views\r\n    559 \r\n\r\n/opt/conda/envs/myenv/lib/python3.6/site-packages/fastparquet/dataframe.py in empty(types, size, cats, cols, index_types, index_names, timezones)\r\n    114             views[col+'-catdef'] = index._data\r\n    115         else:\r\n--> 116             d = np.empty(size, dtype=t)\r\n    117             index = Index(d)\r\n    118             views[col] = index.values\r\n\r\nTypeError: data type not understood\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/432", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/432/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/432/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/432/events", "html_url": "https://github.com/dask/fastparquet/issues/432", "id": 444096670, "node_id": "MDU6SXNzdWU0NDQwOTY2NzA=", "number": 432, "title": "Use times=int96 when writing parquet for Redshift Spectrum", "user": {"login": "sburns", "id": 440820, "node_id": "MDQ6VXNlcjQ0MDgyMA==", "avatar_url": "https://avatars1.githubusercontent.com/u/440820?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sburns", "html_url": "https://github.com/sburns", "followers_url": "https://api.github.com/users/sburns/followers", "following_url": "https://api.github.com/users/sburns/following{/other_user}", "gists_url": "https://api.github.com/users/sburns/gists{/gist_id}", "starred_url": "https://api.github.com/users/sburns/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sburns/subscriptions", "organizations_url": "https://api.github.com/users/sburns/orgs", "repos_url": "https://api.github.com/users/sburns/repos", "events_url": "https://api.github.com/users/sburns/events{/privacy}", "received_events_url": "https://api.github.com/users/sburns/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-14T19:56:15Z", "updated_at": "2019-05-14T19:56:41Z", "closed_at": "2019-05-14T19:56:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "\ud83d\udc4b thanks for a wonderful library.\r\n\r\nI'm opening this issue mostly for the Google juice to help people in the future and will instantly close this.\r\n\r\nWhen you're using dask+fastparquet to write parquet files for use in Redshift Spectrum, you should use the `times='int96'` argument. I can't say for sure but it seems like Spectrum is using Spark underneath the hood. If you use the default `int64` argument, all of your timestamps will read by Spectrum as `2262-04-09 23:47:16.854776`.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/429", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/429/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/429/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/429/events", "html_url": "https://github.com/dask/fastparquet/issues/429", "id": 442267943, "node_id": "MDU6SXNzdWU0NDIyNjc5NDM=", "number": 429, "title": "Fast parquet strips `_metadata` from anywhere in path during S3 read", "user": {"login": "jperkelens", "id": 176702, "node_id": "MDQ6VXNlcjE3NjcwMg==", "avatar_url": "https://avatars1.githubusercontent.com/u/176702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jperkelens", "html_url": "https://github.com/jperkelens", "followers_url": "https://api.github.com/users/jperkelens/followers", "following_url": "https://api.github.com/users/jperkelens/following{/other_user}", "gists_url": "https://api.github.com/users/jperkelens/gists{/gist_id}", "starred_url": "https://api.github.com/users/jperkelens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jperkelens/subscriptions", "organizations_url": "https://api.github.com/users/jperkelens/orgs", "repos_url": "https://api.github.com/users/jperkelens/repos", "events_url": "https://api.github.com/users/jperkelens/events{/privacy}", "received_events_url": "https://api.github.com/users/jperkelens/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-09T14:30:38Z", "updated_at": "2019-05-09T15:12:35Z", "closed_at": "2019-05-09T15:12:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "Not sure if an S3 only bug or not but the following code: \r\n```python\r\npath = f\"dataeng-data-test/testing/data/products/sku_metadata/*.parquet\"\r\ns3 = s3fs.S3FileSystem()\r\nfs = s3fs.core.S3FileSystem()\r\nall_paths_from_s3 = fs.glob(path=path)\r\n \t \r\ncustom_open = s3.open\r\nfp_obj = fp.ParquetFile(all_paths_from_s3, open_with=custom_open)\r\nactual = fp_obj.to_pandas()\r\n```\r\nresults in the following error:\r\n```sh\r\nFileNotFoundError: dataeng-data-test/testing/data/products/sku/<partitions>/<filename>.parquet\r\n```\r\n\r\nNote the path descrepancy in the directory called `sku_metadata`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/426", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/426/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/426/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/426/events", "html_url": "https://github.com/dask/fastparquet/issues/426", "id": 440190477, "node_id": "MDU6SXNzdWU0NDAxOTA0Nzc=", "number": 426, "title": "List index out of range on some nested Parquet files", "user": {"login": "cmenguy", "id": 1641931, "node_id": "MDQ6VXNlcjE2NDE5MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1641931?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cmenguy", "html_url": "https://github.com/cmenguy", "followers_url": "https://api.github.com/users/cmenguy/followers", "following_url": "https://api.github.com/users/cmenguy/following{/other_user}", "gists_url": "https://api.github.com/users/cmenguy/gists{/gist_id}", "starred_url": "https://api.github.com/users/cmenguy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cmenguy/subscriptions", "organizations_url": "https://api.github.com/users/cmenguy/orgs", "repos_url": "https://api.github.com/users/cmenguy/repos", "events_url": "https://api.github.com/users/cmenguy/events{/privacy}", "received_events_url": "https://api.github.com/users/cmenguy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-05-03T19:34:18Z", "updated_at": "2019-05-08T16:06:28Z", "closed_at": "2019-05-08T16:06:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "This seems like the same error as #352 \r\nMy Parquet file is nested with only simple types.\r\nThis error does not happen on all nested Parquet files, just this particular one.\r\n\r\nHowever, I also want to point out that I am able to read this file correctly with fastparquet 0.1.4 (it returns a dict instead of individual columns which is still not ideal but at least workable, as opposed to currently having it just error out), but it started breaking after that. This issue was closed as not supported, but it appears it was supported in earlier versions, so I am not sure if that makes sense.\r\n\r\nAlso same file can be read fine in pyarrow, so I think something is up. I am looking at the code now to see if I can fix it, but in the meantime if you have any ideas let me know.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/site-packages/pandas/io/parquet.py\", line 288, in read_parquet\r\n    return impl.read(path, columns=columns, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/pandas/io/parquet.py\", line 232, in read\r\n    parquet_file = self.api.ParquetFile(path)\r\n  File \"/usr/local/lib/python2.7/site-packages/fastparquet/api.py\", line 116, in __init__\r\n    self._parse_header(f, verify)\r\n  File \"/usr/local/lib/python2.7/site-packages/fastparquet/api.py\", line 140, in _parse_header\r\n    self._set_attrs()\r\n  File \"/usr/local/lib/python2.7/site-packages/fastparquet/api.py\", line 157, in _set_attrs\r\n    self._dtypes()\r\n  File \"/usr/local/lib/python2.7/site-packages/fastparquet/api.py\", line 505, in _dtypes\r\n    chunk = rg.columns[i]\r\nIndexError: list index out of range\r\n```\r\n\r\n[tempFile.parquet.zip](https://github.com/dask/fastparquet/files/3143366/tempFile.parquet.zip)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/424", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/424/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/424/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/424/events", "html_url": "https://github.com/dask/fastparquet/issues/424", "id": 436187050, "node_id": "MDU6SXNzdWU0MzYxODcwNTA=", "number": 424, "title": "Handling of dateutil timezones", "user": {"login": "jorisvandenbossche", "id": 1020496, "node_id": "MDQ6VXNlcjEwMjA0OTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorisvandenbossche", "html_url": "https://github.com/jorisvandenbossche", "followers_url": "https://api.github.com/users/jorisvandenbossche/followers", "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}", "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions", "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs", "repos_url": "https://api.github.com/users/jorisvandenbossche/repos", "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}", "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-04-23T13:26:14Z", "updated_at": "2019-05-07T15:15:43Z", "closed_at": "2019-05-07T14:59:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "From https://github.com/pandas-dev/pandas/issues/25423\r\n\r\nfastparquet converts the `tz` to a string and writes that into the metadata:\r\n\r\nhttps://github.com/dask/fastparquet/blob/c0c8a442a0476ee809e3c70f2488fce8d0e6798c/fastparquet/util.py#L236-L237\r\n\r\nHowever, this \"fails\" (gives you a result that cannot be read back), if there is a `dateutil.tz.tzutc` timezone, since this converts to `'tzutc()'`, which will fail parsing:\r\n\r\n```\r\nIn [78]: from dateutil.tz import tzutc\r\n\r\nIn [79]: tz = tzutc()\r\n\r\nIn [80]: str(tz)\r\nOut[80]: 'tzutc()'\r\n```\r\n\r\nFor that reason, you get the error \"UnknownTimeZoneError: 'tzutc()'\" when reading the parquet file, as reported in https://github.com/pandas-dev/pandas/issues/25423\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/421", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/421/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/421/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/421/events", "html_url": "https://github.com/dask/fastparquet/issues/421", "id": 435637954, "node_id": "MDU6SXNzdWU0MzU2Mzc5NTQ=", "number": 421, "title": "Undefined name 'out' in columns.py", "user": {"login": "cclauss", "id": 3709715, "node_id": "MDQ6VXNlcjM3MDk3MTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cclauss", "html_url": "https://github.com/cclauss", "followers_url": "https://api.github.com/users/cclauss/followers", "following_url": "https://api.github.com/users/cclauss/following{/other_user}", "gists_url": "https://api.github.com/users/cclauss/gists{/gist_id}", "starred_url": "https://api.github.com/users/cclauss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cclauss/subscriptions", "organizations_url": "https://api.github.com/users/cclauss/orgs", "repos_url": "https://api.github.com/users/cclauss/repos", "events_url": "https://api.github.com/users/cclauss/events{/privacy}", "received_events_url": "https://api.github.com/users/cclauss/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-22T07:54:36Z", "updated_at": "2019-04-22T14:19:32Z", "closed_at": "2019-04-22T14:19:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "[flake8](http://flake8.pycqa.org) testing of https://github.com/dask/fastparquet on Python 3.7.1\r\n\r\n$ __flake8 . --count --select=E9,F63,F72,F82 --show-source --statistics__\r\n```\r\n./fastparquet/util.py:72:51: F821 undefined name 'unicode'\r\n        return s.encode('utf-8') if isinstance(s, unicode) else s\r\n                                                  ^\r\n./fastparquet/util.py:102:12: F821 undefined name 'buffer'\r\n    return buffer(raw_bytes) if PY2 else memoryview(raw_bytes)\r\n           ^\r\n./fastparquet/writer.py:220:36: F821 undefined name 'unicode'\r\n    elif PY2 and all(isinstance(i, unicode) for i in head):\r\n                                   ^\r\n./fastparquet/writer.py:344:22: F821 undefined name 'wnd'\r\n        write_length(wnd - start, o)\r\n                     ^\r\n./fastparquet/benchmarks/columns.py:144:23: F821 undefined name 'out'\r\n    df = pd.DataFrame(out, columns=('type', 'nvalid', 'op', 'time'))\r\n                      ^\r\n5     F821 undefined name 'out'\r\n5\r\n```\r\n__E901,E999,F821,F822,F823__ are the \"_showstopper_\" [flake8](http://flake8.pycqa.org) issues that can halt the runtime with a SyntaxError, NameError, etc. These 5 are different from most other flake8 issues which are merely \"style violations\" -- useful for readability but they do not effect runtime safety.\r\n* F821: undefined name `name`\r\n* F822: undefined name `name` in `__all__`\r\n* F823: local variable name referenced before assignment\r\n* E901: SyntaxError or IndentationError\r\n* E999: SyntaxError -- failed to compile a file into an Abstract Syntax Tree\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/420", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/420/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/420/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/420/events", "html_url": "https://github.com/dask/fastparquet/issues/420", "id": 433474890, "node_id": "MDU6SXNzdWU0MzM0NzQ4OTA=", "number": 420, "title": "schema.SchemaHelper.__eq__ is object default method", "user": {"login": "thomasbkahn", "id": 8884155, "node_id": "MDQ6VXNlcjg4ODQxNTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8884155?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasbkahn", "html_url": "https://github.com/thomasbkahn", "followers_url": "https://api.github.com/users/thomasbkahn/followers", "following_url": "https://api.github.com/users/thomasbkahn/following{/other_user}", "gists_url": "https://api.github.com/users/thomasbkahn/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasbkahn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasbkahn/subscriptions", "organizations_url": "https://api.github.com/users/thomasbkahn/orgs", "repos_url": "https://api.github.com/users/thomasbkahn/repos", "events_url": "https://api.github.com/users/thomasbkahn/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasbkahn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-04-15T20:56:57Z", "updated_at": "2019-04-23T18:26:56Z", "closed_at": "2019-04-23T18:26:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "The `schema.SchemaHelper.__eq__` method is not explicitly implemented, and therefore inherits the default `__eq__` method from `object`. This evaluation is whether two instances are the same, but not (necessarily, and in this case) whether the _contents_ are identical. This means that comparing separate files written with the same schema may not return expected results when using `==` (how I found this issue).\r\n\r\nA quick fix for this could just be adding:\r\n```python\r\ndef __eq__(self, other):\r\n    return self.schema_elements == other.schema_elements\r\n```\r\n\r\nI can submit a PR if there's interest.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/419", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/419/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/419/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/419/events", "html_url": "https://github.com/dask/fastparquet/issues/419", "id": 431894299, "node_id": "MDU6SXNzdWU0MzE4OTQyOTk=", "number": 419, "title": "Reading array of string does not work as expected", "user": {"login": "victornoel", "id": 160975, "node_id": "MDQ6VXNlcjE2MDk3NQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/160975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/victornoel", "html_url": "https://github.com/victornoel", "followers_url": "https://api.github.com/users/victornoel/followers", "following_url": "https://api.github.com/users/victornoel/following{/other_user}", "gists_url": "https://api.github.com/users/victornoel/gists{/gist_id}", "starred_url": "https://api.github.com/users/victornoel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/victornoel/subscriptions", "organizations_url": "https://api.github.com/users/victornoel/orgs", "repos_url": "https://api.github.com/users/victornoel/repos", "events_url": "https://api.github.com/users/victornoel/events{/privacy}", "received_events_url": "https://api.github.com/users/victornoel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-04-11T08:27:58Z", "updated_at": "2019-04-11T16:01:10Z", "closed_at": "2019-04-11T15:59:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nAccording to the documentation, fastparquet should be able to read arrays of string (https://fastparquet.readthedocs.io/en/latest/details.html#reading-nested-schema).\r\n\r\nActually, I\u00a0have been reading map of string without any problem. But for a reason I\u00a0don't understand, reading an array of string always give me a `None` value.\r\n\r\nI am using pandas 0.24.2 and fastparquet 0.3.0.\r\n\r\nI use the following code to read it:\r\n```py\r\nmeta = pd.read_parquet('/home/jovyan/work/meta.parquet')\r\n```\r\n\r\nThe parquet files are written using parquet-avro 1.10.1 in Java.\r\n\r\nThe avro schema is as follows:\r\n```json\r\n{\"namespace\": \"ns\",\r\n \"type\": \"record\",\r\n \"name\": \"Log\",\r\n \"fields\": [\r\n     {\"name\": \"id\", \"type\": \"string\"},\r\n     {\"name\": \"step\", \"type\": \"int\"},\r\n     {\"name\": \"product_ids\", \"type\": {\"type\": \"array\", \"items\": \"string\"}},\r\n     {\"name\": \"histo_val\", \"type\": \"double\"},\r\n     {\"name\": \"impacted_histo_ca\", \"type\": [\"null\", \"double\"]},\r\n     {\"name\": \"impacted_nb_clients\", \"type\": [\"null\", \"int\"]},\r\n     {\"name\": \"impacted_volume\", \"type\": [\"null\", \"double\"]}\r\n ]\r\n}\r\n```\r\n\r\nAnd the problematic one is `product_ids`.\r\n\r\nFor the record, here is a schema with a map that can be read without problems by fastparquet:\r\n```json\r\n{\"namespace\": \"ns\",\r\n \"type\": \"record\",\r\n \"name\": \"Log\",\r\n \"fields\": [\r\n     {\"name\": \"id\", \"type\": \"string\"},\r\n     {\"name\": \"step\", \"type\": \"int\", \"unsigned\": true},\r\n     {\"name\": \"criticality\", \"type\":\r\n     \t[\"null\", {\"name\": \"CriticalityLog\", \"type\": \"record\", \"fields\": [\r\n     \t\t{\"name\": \"dissatisfaction\", \"type\": \"double\"},\r\n     \t\t{\"name\": \"importance\", \"type\": \"double\"}\r\n     \t]}]\r\n     },\r\n     {\"name\": \"variable\", \"type\":\r\n     \t{\"name\": \"DomainEntityLog\", \"type\": \"record\", \"fields\": [\r\n     \t\t{\"name\": \"value\", \"type\": \"double\"},\r\n     \t\t{\"name\": \"min\", \"type\": \"double\"},\r\n     \t\t{\"name\": \"max\", \"type\": \"double\"}\r\n     \t]}\r\n     },\r\n     {\"name\": \"meta\", \"type\": {\"type\": \"map\", \"values\": \"string\"}}\r\n ]\r\n}\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/417", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/417/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/417/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/417/events", "html_url": "https://github.com/dask/fastparquet/issues/417", "id": 430329024, "node_id": "MDU6SXNzdWU0MzAzMjkwMjQ=", "number": 417, "title": "pd.read_parquet engine='fastparquet' corrupting FIXED_LEN_BYTE_ARRAY, DECIMAL inputs", "user": {"login": "telferm57", "id": 16674375, "node_id": "MDQ6VXNlcjE2Njc0Mzc1", "avatar_url": "https://avatars1.githubusercontent.com/u/16674375?v=4", "gravatar_id": "", "url": "https://api.github.com/users/telferm57", "html_url": "https://github.com/telferm57", "followers_url": "https://api.github.com/users/telferm57/followers", "following_url": "https://api.github.com/users/telferm57/following{/other_user}", "gists_url": "https://api.github.com/users/telferm57/gists{/gist_id}", "starred_url": "https://api.github.com/users/telferm57/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/telferm57/subscriptions", "organizations_url": "https://api.github.com/users/telferm57/orgs", "repos_url": "https://api.github.com/users/telferm57/repos", "events_url": "https://api.github.com/users/telferm57/events{/privacy}", "received_events_url": "https://api.github.com/users/telferm57/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2019-04-08T08:49:37Z", "updated_at": "2019-04-10T14:15:09Z", "closed_at": "2019-04-09T23:01:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "When reading a file  generated by parquet-dotnet that contains decimals, using pandas read_parquet engine='fastparquet', the numeric values change to seemingly unrelated numbers, e.g.\r\n 44.4520 -> 0.000678\r\n\r\nWhen reading the same file using pyarrow, there is no problem. \r\n\r\ninput file and \r\n\r\nscript attached\r\n[parquet_test_1.zip](https://github.com/dask/fastparquet/files/3053442/parquet_test_1.zip)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/414", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/414/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/414/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/414/events", "html_url": "https://github.com/dask/fastparquet/issues/414", "id": 427976334, "node_id": "MDU6SXNzdWU0Mjc5NzYzMzQ=", "number": 414, "title": "Cannot read pyarrow RangeIndex", "user": {"login": "bchu", "id": 2050208, "node_id": "MDQ6VXNlcjIwNTAyMDg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2050208?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bchu", "html_url": "https://github.com/bchu", "followers_url": "https://api.github.com/users/bchu/followers", "following_url": "https://api.github.com/users/bchu/following{/other_user}", "gists_url": "https://api.github.com/users/bchu/gists{/gist_id}", "starred_url": "https://api.github.com/users/bchu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bchu/subscriptions", "organizations_url": "https://api.github.com/users/bchu/orgs", "repos_url": "https://api.github.com/users/bchu/repos", "events_url": "https://api.github.com/users/bchu/events{/privacy}", "received_events_url": "https://api.github.com/users/bchu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-04-02T01:05:36Z", "updated_at": "2019-06-30T13:48:54Z", "closed_at": "2019-06-06T16:35:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\ndf = pd.DataFrame([1,2,3], columns=['a'])\r\ndf.to_parquet('tmp.parquet', engine='pyarrow')\r\npd.read_parquet('tmp.parquet', engine='fastparquet')\r\n```\r\n\r\nRaises the exception\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-42-d993694086f8> in <module>\r\n      1 df = pd.DataFrame([1,2,3], columns=['a'])\r\n      2 df.to_parquet('tmp.parquet', engine='pyarrow')\r\n----> 3 pd.read_parquet('tmp.parquet', engine='fastparquet')\r\n\r\n~/.pyenv/versions/3.6.2/envs/general/lib/python3.6/site-packages/pandas/io/parquet.py in read_parquet(path, engine, columns, **kwargs)\r\n    280 \r\n    281     impl = get_engine(engine)\r\n--> 282     return impl.read(path, columns=columns, **kwargs)\r\n\r\n~/.pyenv/versions/3.6.2/envs/general/lib/python3.6/site-packages/pandas/io/parquet.py in read(self, path, columns, **kwargs)\r\n    209             parquet_file = self.api.ParquetFile(path)\r\n    210 \r\n--> 211         return parquet_file.to_pandas(columns=columns, **kwargs)\r\n    212 \r\n    213 \r\n\r\n~/.pyenv/versions/3.6.2/envs/general/lib/python3.6/site-packages/fastparquet/api.py in to_pandas(self, columns, categories, filters, index)\r\n    419         if index:\r\n    420             columns += [i for i in index if i not in columns]\r\n--> 421         check_column_names(self.columns + list(self.cats), columns, categories)\r\n    422         df, views = self.pre_allocate(size, columns, categories, index)\r\n    423         start = 0\r\n\r\n~/.pyenv/versions/3.6.2/envs/general/lib/python3.6/site-packages/fastparquet/util.py in check_column_names(columns, *args)\r\n     90     for arg in args:\r\n     91         if isinstance(arg, (tuple, list)):\r\n---> 92             if set(arg) - set(columns):\r\n     93                 raise ValueError(\"Column name not in list.\\n\"\r\n     94                                  \"Requested %s\\n\"\r\n\r\nTypeError: unhashable type: 'dict'\r\n```\r\n\r\nThis is most likely the result of: https://github.com/pandas-dev/pandas/issues/25672 and https://github.com/apache/arrow/pull/3868", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/412", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/412/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/412/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/412/events", "html_url": "https://github.com/dask/fastparquet/issues/412", "id": 427600323, "node_id": "MDU6SXNzdWU0Mjc2MDAzMjM=", "number": 412, "title": "tz is undefined in one code path in api.py", "user": {"login": "cottrell", "id": 223276, "node_id": "MDQ6VXNlcjIyMzI3Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/223276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cottrell", "html_url": "https://github.com/cottrell", "followers_url": "https://api.github.com/users/cottrell/followers", "following_url": "https://api.github.com/users/cottrell/following{/other_user}", "gists_url": "https://api.github.com/users/cottrell/gists{/gist_id}", "starred_url": "https://api.github.com/users/cottrell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cottrell/subscriptions", "organizations_url": "https://api.github.com/users/cottrell/orgs", "repos_url": "https://api.github.com/users/cottrell/repos", "events_url": "https://api.github.com/users/cottrell/events{/privacy}", "received_events_url": "https://api.github.com/users/cottrell/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-01T09:26:32Z", "updated_at": "2019-04-01T14:46:34Z", "closed_at": "2019-04-01T14:46:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I think the recent refactor broke the one part of the code path where 'tz' is undefined:\r\n\r\nhttps://github.com/dask/fastparquet/blob/master/fastparquet/api.py#L525\r\n\r\nI suspect it should be self.tz but probably @martindurant chimes in.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/409", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/409/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/409/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/409/events", "html_url": "https://github.com/dask/fastparquet/issues/409", "id": 423522007, "node_id": "MDU6SXNzdWU0MjM1MjIwMDc=", "number": 409, "title": "column index name(s) not persisted on save or load", "user": {"login": "timothydmorton", "id": 1895387, "node_id": "MDQ6VXNlcjE4OTUzODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1895387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timothydmorton", "html_url": "https://github.com/timothydmorton", "followers_url": "https://api.github.com/users/timothydmorton/followers", "following_url": "https://api.github.com/users/timothydmorton/following{/other_user}", "gists_url": "https://api.github.com/users/timothydmorton/gists{/gist_id}", "starred_url": "https://api.github.com/users/timothydmorton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timothydmorton/subscriptions", "organizations_url": "https://api.github.com/users/timothydmorton/orgs", "repos_url": "https://api.github.com/users/timothydmorton/repos", "events_url": "https://api.github.com/users/timothydmorton/events{/privacy}", "received_events_url": "https://api.github.com/users/timothydmorton/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-03-20T23:53:37Z", "updated_at": "2019-06-06T16:35:12Z", "closed_at": "2019-06-06T16:35:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "fastparquet does not persist column index name information, and appears to not support multiindexing?\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom fastparquet import write, ParquetFile\r\nfrom pandas.util.testing import assert_frame_equal\r\n\r\ncolumn_index = pd.Index(['a', 'b'], name='column')\r\ndf = pd.DataFrame(np.random.random((4, 2)), columns=column_index)\r\n\r\ndef test_roundtrip_fastparquet(df):    \r\n    print(df.columns)\r\n    write('test.parq', df)\r\n    pf = ParquetFile('test.parq')\r\n    df2 = pf.to_pandas()\r\n    print(df2.columns)\r\n    assert_frame_equal(df, df2)\r\n    \r\ntest_roundtrip_fastparquet(df)\r\n```\r\nrunning this gives:\r\n```\r\nIndex(['a', 'b'], dtype='object', name='column')\r\nIndex(['a', 'b'], dtype='object')\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-57-3692bf07de0a> in <module>\r\n     15     assert_frame_equal(df, df2)\r\n     16 \r\n---> 17 test_roundtrip_fastparquet(df)\r\n...etc\r\n```\r\nIn contrast, pyarrow works:\r\n```python\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\ndef test_roundtrip_pyarrow(df):\r\n    print(df.columns)\r\n    table = pa.Table.from_pandas(df)\r\n    pq.write_table(table, 'test2.parq')\r\n    df2 = pq.ParquetFile('test2.parq').read(use_pandas_metadata=True).to_pandas()\r\n    print(df2.columns)\r\n    assert_frame_equal(df, df2)\r\n    \r\ntest_roundtrip_pyarrow(df)\r\n```\r\nGives \r\n```\r\nIndex(['a', 'b'], dtype='object', name='column')\r\nIndex(['a', 'b'], dtype='object', name='column')\r\n```\r\nA multi-level column index fails:\r\n```python\r\ncolumn_index_multi = pd.MultiIndex.from_tuples([('a', 'b'), ('a', 'c')], names=['level1', 'level2'])\r\ndf_multi = pd.DataFrame(np.random.random((4,2)), columns=column_index_multi)\r\ntest_roundtrip_fastparquet(df_multi)\r\n```\r\ngives\r\n```\r\nMultiIndex(levels=[['a'], ['b', 'c']],\r\n           labels=[[0, 0], [0, 1]],\r\n           names=['level1', 'level2'])\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-63-66bcf22cf107> in <module>\r\n      1 column_index_multi = pd.MultiIndex.from_tuples([('a', 'b'), ('a', 'c')], names=['level1', 'level2'])\r\n      2 df_multi = pd.DataFrame(np.random.random((4,2)), columns=column_index_multi)\r\n----> 3 test_roundtrip_fastparquet(df_multi)\r\n\r\n<ipython-input-57-3692bf07de0a> in test_roundtrip_fastparquet(df)\r\n      9 def test_roundtrip_fastparquet(df):\r\n     10     print(df.columns)\r\n---> 11     write('test.parq', df)\r\n     12     pf = ParquetFile('test.parq')\r\n     13     df2 = pf.to_pandas()\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/writer.py in write(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times)\r\n    842     fmd = make_metadata(data, has_nulls=has_nulls, ignore_columns=ignore,\r\n    843                         fixed_text=fixed_text, object_encoding=object_encoding,\r\n--> 844                         times=times, index_cols=index_cols)\r\n    845 \r\n    846     if file_scheme == 'simple':\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/writer.py in make_metadata(data, has_nulls, ignore_columns, fixed_text, object_encoding, times, index_cols)\r\n    666             continue\r\n    667         pandas_metadata['columns'].append(\r\n--> 668             get_column_metadata(data[column], column))\r\n    669         oencoding = (object_encoding if isinstance(object_encoding, STR_TYPE)\r\n    670                      else object_encoding.get(column, None))\r\n\r\n~/.local/lib/python3.6/site-packages/fastparquet/util.py in get_column_metadata(column, name)\r\n    221         raise TypeError(\r\n    222             'Column name must be a string. Got column {} of type {}'.format(\r\n--> 223                 name, type(name).__name__\r\n    224             )\r\n    225         )\r\n\r\nTypeError: Column name must be a string. Got column ('a', 'b') of type tuple\r\n```\r\nWhereas it works for pyarrow.  \r\n\r\nI had switched to pyarrow for my current purposes I think mostly because of the lack of multi-level column index support, but then pyarrow is having problems loading large files that fastparquet *does* load, except for this issue with the column index name.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/408", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/408/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/408/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/408/events", "html_url": "https://github.com/dask/fastparquet/issues/408", "id": 414868477, "node_id": "MDU6SXNzdWU0MTQ4Njg0Nzc=", "number": 408, "title": "FR: Accept a file-like object in addition to a path in `fastparquet.write`", "user": {"login": "tswast", "id": 247555, "node_id": "MDQ6VXNlcjI0NzU1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/247555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tswast", "html_url": "https://github.com/tswast", "followers_url": "https://api.github.com/users/tswast/followers", "following_url": "https://api.github.com/users/tswast/following{/other_user}", "gists_url": "https://api.github.com/users/tswast/gists{/gist_id}", "starred_url": "https://api.github.com/users/tswast/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tswast/subscriptions", "organizations_url": "https://api.github.com/users/tswast/orgs", "repos_url": "https://api.github.com/users/tswast/repos", "events_url": "https://api.github.com/users/tswast/events{/privacy}", "received_events_url": "https://api.github.com/users/tswast/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-02-26T23:21:57Z", "updated_at": "2019-04-25T14:32:55Z", "closed_at": "2019-04-25T14:32:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "pyarrow accepts either a filename or a file-like object when writing a DataFrame. It would be very helpful if fastparquet also does this. I found https://github.com/dask/fastparquet/issues/215 which adds a special in-memory path name, but that seems specific to the dask-ecosystem.\r\n\r\nCurrently `google-cloud-bigquery` requires `pyarrow` rather than `fastparquet` to upload a DataFrame to BigQuery due to this issue.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/402", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/402/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/402/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/402/events", "html_url": "https://github.com/dask/fastparquet/issues/402", "id": 409099738, "node_id": "MDU6SXNzdWU0MDkwOTk3Mzg=", "number": 402, "title": "Converting CSV to Parquet. RuntimeError: Compression 'snappy' not available", "user": {"login": "hmajumdar", "id": 4563760, "node_id": "MDQ6VXNlcjQ1NjM3NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/4563760?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hmajumdar", "html_url": "https://github.com/hmajumdar", "followers_url": "https://api.github.com/users/hmajumdar/followers", "following_url": "https://api.github.com/users/hmajumdar/following{/other_user}", "gists_url": "https://api.github.com/users/hmajumdar/gists{/gist_id}", "starred_url": "https://api.github.com/users/hmajumdar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hmajumdar/subscriptions", "organizations_url": "https://api.github.com/users/hmajumdar/orgs", "repos_url": "https://api.github.com/users/hmajumdar/repos", "events_url": "https://api.github.com/users/hmajumdar/events{/privacy}", "received_events_url": "https://api.github.com/users/hmajumdar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-02-12T03:11:29Z", "updated_at": "2019-06-06T16:35:12Z", "closed_at": "2019-06-06T16:35:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using Python 3.6 interpreter in my PyCharm venv, and trying to convert a CSV to Parquet.\r\n\r\n```\r\nimport pandas as pd    \r\ndf = pd.read_csv('/parquet/drivers.csv')\r\ndf.to_parquet('output.parquet', engine='fastparquet')\r\n```\r\n\r\nI get this **error**\r\n\r\n`/Users/python parquet/venv/lib/python3.6/site-packages/fastparquet/compression.py\", line 131, in compress_data (algorithm, sorted(compressions))) RuntimeError: Compression 'snappy' not available. Options: ['GZIP', 'UNCOMPRESSED']`\r\n\r\nI Installed **python-snappy 0.5.3** but still getting the same error? Do I need to install any other library? If I use **PyArrow 0.12.0** engine instead, I don't see this error", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/400", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/400/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/400/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/400/events", "html_url": "https://github.com/dask/fastparquet/issues/400", "id": 408418346, "node_id": "MDU6SXNzdWU0MDg0MTgzNDY=", "number": 400, "title": "Possible improvements to in operator", "user": {"login": "pablojim", "id": 683731, "node_id": "MDQ6VXNlcjY4MzczMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/683731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pablojim", "html_url": "https://github.com/pablojim", "followers_url": "https://api.github.com/users/pablojim/followers", "following_url": "https://api.github.com/users/pablojim/following{/other_user}", "gists_url": "https://api.github.com/users/pablojim/gists{/gist_id}", "starred_url": "https://api.github.com/users/pablojim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pablojim/subscriptions", "organizations_url": "https://api.github.com/users/pablojim/orgs", "repos_url": "https://api.github.com/users/pablojim/repos", "events_url": "https://api.github.com/users/pablojim/events{/privacy}", "received_events_url": "https://api.github.com/users/pablojim/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-02-09T10:19:35Z", "updated_at": "2019-02-27T08:28:04Z", "closed_at": "2019-02-27T08:28:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "The current \"in\" operator filter implementation can return more row groups than necessary from the existing row group statistics.\r\n\r\ne.g:\r\n```\r\ndf = pd.DataFrame({'a': range(100),'b': np.random.rand(100)})\r\nwrite(parquet_path, df, row_group_offsets=10)\r\n\r\nres = list(pf.iter_row_groups(filters=[('a', 'in', [9])]))\r\nlen(res) # 1 group\r\n\r\nres = list(pf.iter_row_groups(filters=[('a', 'in', [99])]))\r\nlen(res) # 1 group\r\n\r\nres = list(pf.iter_row_groups(filters=[('a', 'in', [9, 99])]))\r\nlen(res) # 10 groups returned\r\n```\r\n\r\nThis is due to max/min handling here: https://github.com/dask/fastparquet/blob/master/fastparquet/api.py#L795\r\n\r\nIs this desired behaviour? maybe for performance reasons?\r\n\r\nWould a PR that instead considers each value in the set separately be considered?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/399", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/399/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/399/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/399/events", "html_url": "https://github.com/dask/fastparquet/issues/399", "id": 407039071, "node_id": "MDU6SXNzdWU0MDcwMzkwNzE=", "number": 399, "title": "Reading multi-index is broken in pandas>0.24", "user": {"login": "limx0", "id": 4816153, "node_id": "MDQ6VXNlcjQ4MTYxNTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/4816153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/limx0", "html_url": "https://github.com/limx0", "followers_url": "https://api.github.com/users/limx0/followers", "following_url": "https://api.github.com/users/limx0/following{/other_user}", "gists_url": "https://api.github.com/users/limx0/gists{/gist_id}", "starred_url": "https://api.github.com/users/limx0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/limx0/subscriptions", "organizations_url": "https://api.github.com/users/limx0/orgs", "repos_url": "https://api.github.com/users/limx0/repos", "events_url": "https://api.github.com/users/limx0/events{/privacy}", "received_events_url": "https://api.github.com/users/limx0/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-02-06T00:52:53Z", "updated_at": "2019-02-06T04:02:08Z", "closed_at": "2019-02-06T04:02:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\nimport fastparquet\r\nimport pandas as pd\r\nfastparquet.__version__, pd.__version__\r\n('0.2.1', '0.24.0')\r\n```\r\n\r\n```python\r\nfilename = 'sample.parquet'\r\nshape = (100, 10)\r\n\r\ndf = pd.DataFrame(\r\n    data=pd.np.random.random(shape),\r\n    index=pd.DatetimeIndex(pd.date_range('2018-01-01', periods=shape[0]), name='date'),\r\n    columns=list(map(str, range(shape[1])))\r\n)\r\n\r\nmulti_index = pd.concat([sample], keys=['a'], names=['type'])\r\n\r\nwrite(filename, multi_index)\r\npf = ParquetFile(filename)\r\npf.to_pandas()\r\n```\r\n\r\nraises \r\n\r\n`ValueError: Shape of passed values is (100, 10), indices imply (0, 10)`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/398", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/398/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/398/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/398/events", "html_url": "https://github.com/dask/fastparquet/issues/398", "id": 406842556, "node_id": "MDU6SXNzdWU0MDY4NDI1NTY=", "number": 398, "title": "Papermill import breaks fastparquet writer", "user": {"login": "bartaelterman", "id": 2742677, "node_id": "MDQ6VXNlcjI3NDI2Nzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2742677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bartaelterman", "html_url": "https://github.com/bartaelterman", "followers_url": "https://api.github.com/users/bartaelterman/followers", "following_url": "https://api.github.com/users/bartaelterman/following{/other_user}", "gists_url": "https://api.github.com/users/bartaelterman/gists{/gist_id}", "starred_url": "https://api.github.com/users/bartaelterman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bartaelterman/subscriptions", "organizations_url": "https://api.github.com/users/bartaelterman/orgs", "repos_url": "https://api.github.com/users/bartaelterman/repos", "events_url": "https://api.github.com/users/bartaelterman/events{/privacy}", "received_events_url": "https://api.github.com/users/bartaelterman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-02-05T15:40:53Z", "updated_at": "2019-06-06T16:36:17Z", "closed_at": "2019-06-06T16:36:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI am experiencing issues with writing data to parquet (with dask and fastparquet) when I import papermill in my notebook. The issue is the result of the combination of a couple of packages so I'll be cross posting this issue on the different repositories.\r\n- fastparquet\r\n- [papermill](https://github.com/nteract/papermill/issues/304)\r\n- [dask-adlfs](https://github.com/eriklangenborg-rs/dask-adlfs) (unfortunately this repo has no issues)\r\n\r\nHere is a code sample to reproduce the issue:\r\n\r\n```\r\nfrom random import randint\r\nimport papermill as pm\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n\r\n# a function to generate a sizeable dataframe\r\ndef get_large_df(cols, rows):\r\n    data = {}\r\n    for c in range(cols):\r\n        data['rand{}'.format(c)] = [randint(0, 1000000) for x in range(rows)]\r\n    return pd.DataFrame(data=data)\r\n\r\n# Write the data frame to Azure Data Lake\r\ndf = get_large_df(10, 1_000)\r\nddf = dd.from_pandas(df, npartitions=4)\r\nddf.reset_index()\r\nddf.to_parquet('adl://my_container/test_parquet', engine='fastparquet', storage_options=STORAGE_OPTIONS)\r\n\r\n# Read the data frame\r\ndd.read_parquet('adl://prodstate/mh-webops/mh-webops/test/btan03/test_parquet', engine='fastparquet', storage_options=STORAGE_OPTIONS)\r\n```\r\n\r\nReading the dataframe fails with the following exception:\r\n\r\n```\r\nException FileNotFoundError('test_parquet/_metadata/_metadata',)\r\n```\r\n\r\nIt is unclear to me why dask looks for a `_metadata/_metadata` file because that does not exist. Now here is the funny thing:\r\n\r\n- Simply removing the `import papermill as pm` resolves the issue,\r\n- Writing to a local file instead of one on Azure works as expected,\r\n- Writing and reading with `pyarrow` works (although I have to set `partition_on` when writing, which I didn't expect)\r\n\r\nIt is unclear to me which one of the packages is not playing nice. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/397", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/397/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/397/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/397/events", "html_url": "https://github.com/dask/fastparquet/issues/397", "id": 406113971, "node_id": "MDU6SXNzdWU0MDYxMTM5NzE=", "number": 397, "title": "Discussion: GSOC?", "user": {"login": "klahnakoski", "id": 2334429, "node_id": "MDQ6VXNlcjIzMzQ0Mjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/2334429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/klahnakoski", "html_url": "https://github.com/klahnakoski", "followers_url": "https://api.github.com/users/klahnakoski/followers", "following_url": "https://api.github.com/users/klahnakoski/following{/other_user}", "gists_url": "https://api.github.com/users/klahnakoski/gists{/gist_id}", "starred_url": "https://api.github.com/users/klahnakoski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/klahnakoski/subscriptions", "organizations_url": "https://api.github.com/users/klahnakoski/orgs", "repos_url": "https://api.github.com/users/klahnakoski/repos", "events_url": "https://api.github.com/users/klahnakoski/events{/privacy}", "received_events_url": "https://api.github.com/users/klahnakoski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-02-03T18:08:40Z", "updated_at": "2019-04-16T18:15:57Z", "closed_at": "2019-04-16T18:15:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am looking to mentor some work on fastparquet. I intend for the coding to proceed on a fork, but I am wondering how much (or how little) you (the project owner) would like to be involved:\r\n\r\n* You can be hands off (default) - we will work off a fork, and submit a series of PRs near the end of the summer.\r\n* You can provide reviews (1 or 2 hours per week) - we can work on a fastparquet branch, and you get to direct the work in progress.\r\n* You can help mentor (4hours per week) - and guide the student on a near-daily basis.\r\n\r\nIn all cases, I will give you updates on the project, I will be around to guide the student, and ensure the administrative stuff is done. **IF** this happens, then the student will be working, full time from about May to end of August.\r\n\r\nThe project has been added here: https://wiki.mozilla.org/Community:SummerOfCode19:Brainstorming#2019_Proposed_Project_List", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/395", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/395/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/395/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/395/events", "html_url": "https://github.com/dask/fastparquet/issues/395", "id": 405712132, "node_id": "MDU6SXNzdWU0MDU3MTIxMzI=", "number": 395, "title": "The test test_partition_filters_specialstrings is failing on Windows", "user": {"login": "Dimplexion", "id": 772418, "node_id": "MDQ6VXNlcjc3MjQxOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/772418?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dimplexion", "html_url": "https://github.com/Dimplexion", "followers_url": "https://api.github.com/users/Dimplexion/followers", "following_url": "https://api.github.com/users/Dimplexion/following{/other_user}", "gists_url": "https://api.github.com/users/Dimplexion/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dimplexion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dimplexion/subscriptions", "organizations_url": "https://api.github.com/users/Dimplexion/orgs", "repos_url": "https://api.github.com/users/Dimplexion/repos", "events_url": "https://api.github.com/users/Dimplexion/events{/privacy}", "received_events_url": "https://api.github.com/users/Dimplexion/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-02-01T13:56:41Z", "updated_at": "2019-02-14T16:21:38Z", "closed_at": "2019-02-14T16:21:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "The test `fastparquet/test/test_partition_filters_specialstrings.py` is failing when the data is partitioned by `symbols` column and it's saved into a hive format. The column contains value `VIX*` which includes the special character `*` which is not allowed on Windows directory names. I'm not quite sure how to fix this one. One option would be to encode the character on Windows but it feels a little hacky unless it's done on all platforms.\r\n\r\nHere is the output from the test for reference:\r\n```\r\nfastparquet/test/test_partition_filters_specialstrings.py::test_frame_write_read_verify[input_symbols3-504-hive-2-partitions3-filters3] FAILED                                                                                        [ 77%]\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n\r\ntempdir = 'C:\\\\Users\\\\Janne\\\\AppData\\\\Local\\\\Temp\\\\tmpf5ztngs4', input_symbols = ['VIX*', 'SPY', 'VIX'], input_days = 504, file_scheme = 'hive', input_columns = 2, partitions = ['symbol', 'year'], filters = [('symbol', '==', 'SPY')]\r\n\r\n    @pytest.mark.parametrize('input_symbols,input_days,file_scheme,input_columns,'\r\n                             'partitions,filters',\r\n                             [\r\n                                 (['NOW', 'SPY', 'VIX'], 2 * 252, 'hive', 2,\r\n                                  ['symbol', 'year'], [('symbol', '==', 'SPY')]),\r\n                                 (['now', 'SPY', 'VIX'], 2 * 252, 'hive', 2,\r\n                                  ['symbol', 'year'], [('symbol', '==', 'SPY')]),\r\n                                 (['TODAY', 'SPY', 'VIX'], 2 * 252, 'hive', 2,\r\n                                  ['symbol', 'year'], [('symbol', '==', 'SPY')]),\r\n                                 (['VIX*', 'SPY', 'VIX'], 2 * 252, 'hive', 2,\r\n                                  ['symbol', 'year'], [('symbol', '==', 'SPY')]),\r\n                                 (['QQQ*', 'SPY', 'VIX'], 2 * 252, 'hive', 2,\r\n                                  ['symbol', 'year'], [('symbol', '==', 'SPY')]),\r\n                                 (['QQQ!', 'SPY', 'VIX'], 2 * 252, 'hive', 2,\r\n                                  ['symbol', 'year'], [('symbol', '==', 'SPY')]),\r\n                                 (['Q%QQ', 'SPY', 'VIX'], 2 * 252, 'hive', 2,\r\n                                  ['symbol', 'year'], [('symbol', '==', 'SPY')]),\r\n                                 (['NOW', 'SPY', 'VIX'], 10, 'hive', 2,\r\n                                  ['symbol', 'dtTrade'], [('symbol', '==', 'SPY')]),\r\n                                 (['NOW', 'SPY', 'VIX'], 10, 'hive', 2,\r\n                                  ['symbol', 'dtTrade'],\r\n                                  [('dtTrade', '==',\r\n                                    '2005-01-02T00:00:00.000000000')]),\r\n                                 (['NOW', 'SPY', 'VIX'], 10, 'hive', 2,\r\n                                  ['symbol', 'dtTrade'],\r\n                                  [('dtTrade', '==',\r\n                                    Timestamp('2005-01-01 00:00:00'))]),\r\n                             ]\r\n                             )\r\n    def test_frame_write_read_verify(tempdir, input_symbols, input_days,\r\n                                     file_scheme,\r\n                                     input_columns, partitions, filters):\r\n        # Generate Temp Director for parquet Files\r\n        fdir = str(tempdir)\r\n        fname = os.path.join(fdir, 'test')\r\n\r\n        # Generate Test Input Frame\r\n        input_df = frame_symbol_dtTrade_type_strike(days=input_days,\r\n                                                    symbols=input_symbols,\r\n                                                    numbercolumns=input_columns)\r\n        input_df.reset_index(inplace=True)\r\n        write(fname, input_df, partition_on=partitions, file_scheme=file_scheme,\r\n>             compression='SNAPPY')\r\n\r\nfastparquet\\test\\test_partition_filters_specialstrings.py:76:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nfastparquet\\writer.py:877: in write\r\n    with_field=file_scheme == 'hive'\r\nfastparquet\\writer.py:941: in partition_on_columns\r\n    mkdirs(join_path(root_path, path))\r\nfastparquet\\util.py:34: in default_mkdirs\r\n    os.makedirs(f, exist_ok=True)\r\nc:\\python\\anaconda3\\lib\\os.py:210: in makedirs\r\n    makedirs(head, mode, exist_ok)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nname = 'C:/Users/Janne/AppData/Local/Temp/tmpf5ztngs4/test/symbol=VIX*', mode = 511, exist_ok = True\r\n\r\n    def makedirs(name, mode=0o777, exist_ok=False):\r\n        \"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\r\n\r\n        Super-mkdir; create a leaf directory and all intermediate ones.  Works like\r\n        mkdir, except that any intermediate path segment (not just the rightmost)\r\n        will be created if it does not exist. If the target directory already\r\n        exists, raise an OSError if exist_ok is False. Otherwise no exception is\r\n        raised.  This is recursive.\r\n\r\n        \"\"\"\r\n        head, tail = path.split(name)\r\n        if not tail:\r\n            head, tail = path.split(head)\r\n        if head and tail and not path.exists(head):\r\n            try:\r\n                makedirs(head, mode, exist_ok)\r\n            except FileExistsError:\r\n                # Defeats race condition when another thread created the path\r\n                pass\r\n            cdir = curdir\r\n            if isinstance(tail, bytes):\r\n                cdir = bytes(curdir, 'ASCII')\r\n            if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists\r\n                return\r\n        try:\r\n>           mkdir(name, mode)\r\nE           OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'C:/Users/Janne/AppData/Local/Temp/tmpf5ztngs4/test/symbol=VIX*'\r\n\r\nc:\\python\\anaconda3\\lib\\os.py:220: OSError```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/391", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/391/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/391/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/391/events", "html_url": "https://github.com/dask/fastparquet/issues/391", "id": 391470066, "node_id": "MDU6SXNzdWUzOTE0NzAwNjY=", "number": 391, "title": "Release", "user": {"login": "TomAugspurger", "id": 1312546, "node_id": "MDQ6VXNlcjEzMTI1NDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1312546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TomAugspurger", "html_url": "https://github.com/TomAugspurger", "followers_url": "https://api.github.com/users/TomAugspurger/followers", "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}", "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}", "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions", "organizations_url": "https://api.github.com/users/TomAugspurger/orgs", "repos_url": "https://api.github.com/users/TomAugspurger/repos", "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}", "received_events_url": "https://api.github.com/users/TomAugspurger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-12-16T13:59:38Z", "updated_at": "2018-12-18T21:41:48Z", "closed_at": "2018-12-18T21:39:23Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "I'm planning to do a 0.2.1 release off of master tomorrow if there are no objections.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/389", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/389/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/389/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/389/events", "html_url": "https://github.com/dask/fastparquet/issues/389", "id": 390316619, "node_id": "MDU6SXNzdWUzOTAzMTY2MTk=", "number": 389, "title": "dependency on numpy is missing from setup.py", "user": {"login": "simonvanderveldt", "id": 204286, "node_id": "MDQ6VXNlcjIwNDI4Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/204286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simonvanderveldt", "html_url": "https://github.com/simonvanderveldt", "followers_url": "https://api.github.com/users/simonvanderveldt/followers", "following_url": "https://api.github.com/users/simonvanderveldt/following{/other_user}", "gists_url": "https://api.github.com/users/simonvanderveldt/gists{/gist_id}", "starred_url": "https://api.github.com/users/simonvanderveldt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simonvanderveldt/subscriptions", "organizations_url": "https://api.github.com/users/simonvanderveldt/orgs", "repos_url": "https://api.github.com/users/simonvanderveldt/repos", "events_url": "https://api.github.com/users/simonvanderveldt/events{/privacy}", "received_events_url": "https://api.github.com/users/simonvanderveldt/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-12-12T16:53:32Z", "updated_at": "2019-02-20T15:17:34Z", "closed_at": "2019-02-20T15:17:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Numpy is missing in the list of dependencies in `setup.py` causing a installation to fail when numpy isn't installed before.\r\nEven if installation were possible running code which uses fastparquet would of course fail when numpy wouldn't be installed as well :)\r\nIs there a reason the dependency on numpy missing?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/388", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/388/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/388/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/388/events", "html_url": "https://github.com/dask/fastparquet/issues/388", "id": 387481020, "node_id": "MDU6SXNzdWUzODc0ODEwMjA=", "number": 388, "title": "Pandas DatetimeArray refactor", "user": {"login": "TomAugspurger", "id": 1312546, "node_id": "MDQ6VXNlcjEzMTI1NDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1312546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TomAugspurger", "html_url": "https://github.com/TomAugspurger", "followers_url": "https://api.github.com/users/TomAugspurger/followers", "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}", "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}", "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions", "organizations_url": "https://api.github.com/users/TomAugspurger/orgs", "repos_url": "https://api.github.com/users/TomAugspurger/repos", "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}", "received_events_url": "https://api.github.com/users/TomAugspurger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-12-04T21:10:29Z", "updated_at": "2018-12-14T12:11:49Z", "closed_at": "2018-12-14T12:11:49Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "FYI, https://github.com/pandas-dev/pandas/pull/24024 is going to break parts of fastparquet.\r\n\r\nrunning `pytest pandas/tests/io/test_parquet.py::TestParquetFastParquet::test_basic -x --pdb`\r\n\r\n<details>\r\n\r\n```pytb\r\npandas/tests/io/test_parquet.py F\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n\r\nself = <pandas.tests.io.test_parquet.TestParquetFastParquet object at 0x10d124048>, fp = 'fastparquet'\r\ndf_full =   string string_with_nan string_with_none   bytes unicode  int    ...     float_with_nan   bool   datetime  datetime_w... ...                3.0   True 2013-01-03         2013-01-03 2013-01-03 00:00:00-05:00    3 days\r\n\r\n[3 rows x 14 columns]\r\n\r\n    def test_basic(self, fp, df_full):\r\n        df = df_full\r\n\r\n        # additional supported types for fastparquet\r\n        if LooseVersion(fastparquet.__version__) >= LooseVersion('0.1.4'):\r\n            df['datetime_tz'] = pd.date_range('20130101', periods=3,\r\n                                              tz='US/Eastern')\r\n        df['timedelta'] = pd.timedelta_range('1 day', periods=3)\r\n>       check_round_trip(df, fp)\r\n\r\npandas/tests/io/test_parquet.py:482:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npandas/tests/io/test_parquet.py:152: in check_round_trip\r\n    compare(repeat)\r\npandas/tests/io/test_parquet.py:146: in compare\r\n    actual = read_parquet(path, **read_kwargs)\r\npandas/io/parquet.py:282: in read_parquet\r\n    return impl.read(path, columns=columns, **kwargs)\r\npandas/io/parquet.py:211: in read\r\n    return parquet_file.to_pandas(columns=columns, **kwargs)\r\n../../Envs/pandas-dev/lib/python3.7/site-packages/fastparquet/api.py:422: in to_pandas\r\n    df, views = self.pre_allocate(size, columns, categories, index)\r\n../../Envs/pandas-dev/lib/python3.7/site-packages/fastparquet/api.py:451: in pre_allocate\r\n    self._dtypes(categories), tz)\r\n../../Envs/pandas-dev/lib/python3.7/site-packages/fastparquet/api.py:554: in _pre_allocate\r\n    index_types=index_types, cats=cats, timezones=tz)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ntypes = [dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('int64'), ...], size = 3, cats = {}\r\ncols = ['string', 'string_with_nan', 'string_with_none', 'bytes', 'unicode', 'int', ...], index_types = [], index_names = []\r\ntimezones = {'datetime_tz': 'US/Eastern'}\r\n\r\n    def empty(types, size, cats=None, cols=None, index_types=None, index_names=None,\r\n              timezones=None):\r\n        \"\"\"\r\n        Create empty DataFrame to assign into\r\n\r\n        In the simplest case, will return a Pandas dataframe of the given size,\r\n        with columns of the given names and types. The second return value `views`\r\n        is a dictionary of numpy arrays into which you can assign values that\r\n        show up in the dataframe.\r\n\r\n        For categorical columns, you get two views to assign into: if the\r\n        column name is \"col\", you get both \"col\" (the category codes) and\r\n        \"col-catdef\" (the category labels).\r\n\r\n        For a single categorical index, you should use the `.set_categories`\r\n        method of the appropriate \"-catdef\" columns, passing an Index of values\r\n\r\n        ``views['index-catdef'].set_categories(pd.Index(newvalues), fastpath=True)``\r\n\r\n        Multi-indexes work a lot like categoricals, even if the types of each\r\n        index are not themselves categories, and will also have \"-catdef\" entries\r\n        in the views. However, these will be Dummy instances, providing only a\r\n        ``.set_categories`` method, to be used as above.\r\n\r\n        Parameters\r\n        ----------\r\n        types: like np record structure, 'i4,u2,f4,f2,f4,M8,m8', or using tuples\r\n            applies to non-categorical columns. If there are only categorical\r\n            columns, an empty string of None will do.\r\n        size: int\r\n            Number of rows to allocate\r\n        cats: dict {col: labels}\r\n            Location and labels for categorical columns, e.g., {1: ['mary', 'mo]}\r\n            will create column index 1 (inserted amongst the numerical columns)\r\n            with two possible values. If labels is an integers, `{'col': 5}`,\r\n            will generate temporary labels using range. If None, or column name\r\n            is missing, will assume 16-bit integers (a reasonable default).\r\n        cols: list of labels\r\n            assigned column names, including categorical ones.\r\n        index_types: list of str\r\n            For one of more index columns, make them have this type. See general\r\n            description, above, for caveats about multi-indexing. If None, the\r\n            index will be the default RangeIndex.\r\n        index_names: list of str\r\n            Names of the index column(s), if using\r\n        timezones: dict {col: timezone_str}\r\n            for timestamp type columns, apply this timezone to the pandas series;\r\n            the numpy view will be UTC.\r\n\r\n        Returns\r\n        -------\r\n        - dataframe with correct shape and data-types\r\n        - list of numpy views, in order, of the columns of the dataframe. Assign\r\n            to this.\r\n        \"\"\"\r\n        views = {}\r\n        timezones = timezones or {}\r\n\r\n        if isinstance(types, STR_TYPE):\r\n            types = types.split(',')\r\n        cols = cols if cols is not None else range(len(types))\r\n\r\n        def cat(col):\r\n            if cats is None or col not in cats:\r\n                return RangeIndex(0, 2**14)\r\n            elif isinstance(cats[col], int):\r\n                return RangeIndex(0, cats[col])\r\n            else:  # explicit labels list\r\n                return cats[col]\r\n\r\n        df = OrderedDict()\r\n        for t, col in zip(types, cols):\r\n            if str(t) == 'category':\r\n                df[six.text_type(col)] = Categorical([], categories=cat(col),\r\n                                                     fastpath=True)\r\n            else:\r\n                d = np.empty(0, dtype=t)\r\n                if d.dtype.kind == \"M\" and six.text_type(col) in timezones:\r\n                    d = Series(d).dt.tz_localize(timezones[six.text_type(col)])\r\n                df[six.text_type(col)] = d\r\n\r\n        df = DataFrame(df)\r\n        if not index_types:\r\n            index = RangeIndex(size)\r\n        elif len(index_types) == 1:\r\n            t, col = index_types[0], index_names[0]\r\n            if col is None:\r\n                raise ValueError('If using an index, must give an index name')\r\n            if str(t) == 'category':\r\n                c = Categorical([], categories=cat(col), fastpath=True)\r\n                vals = np.zeros(size, dtype=c.codes.dtype)\r\n                index = CategoricalIndex(c)\r\n                index._data._codes = vals\r\n                views[col] = vals\r\n                views[col+'-catdef'] = index._data\r\n            else:\r\n                d = np.empty(size, dtype=t)\r\n                index = Index(d)\r\n                views[col] = index.values\r\n        else:\r\n            index = MultiIndex([[]], [[]])\r\n            # index = MultiIndex.from_arrays(indexes)\r\n            index._levels = list()\r\n            index._labels = list()\r\n            for i, col in enumerate(index_names):\r\n                index._levels.append(Index([None]))\r\n\r\n                def set_cats(values, i=i, col=col, **kwargs):\r\n                    values.name = col\r\n                    if index._levels[i][0] is None:\r\n                        index._levels[i] = values\r\n                    elif not index._levels[i].equals(values):\r\n                        raise RuntimeError(\"Different dictionaries encountered\"\r\n                                           \" while building categorical\")\r\n\r\n                x = Dummy()\r\n                x._set_categories = set_cats\r\n\r\n                d = np.zeros(size, dtype=int)\r\n                index._labels.append(d)\r\n                views[col] = d\r\n                views[col+'-catdef'] = x\r\n\r\n        axes = [df._data.axes[0], index]\r\n\r\n        # allocate and create blocks\r\n        blocks = []\r\n        for block in df._data.blocks:\r\n            if block.is_categorical:\r\n                categories = block.values.categories\r\n                code = np.zeros(shape=size, dtype=block.values.codes.dtype)\r\n                values = Categorical(values=code, categories=categories,\r\n                                     fastpath=True)\r\n                new_block = block.make_block_same_class(values=values)\r\n            elif getattr(block.dtype, 'tz', None):\r\n                new_shape = (size, )\r\n>               values = np.empty(shape=new_shape, dtype=block.values.values.dtype)\r\nE               AttributeError: 'DatetimeArrayMixin' object has no attribute 'values'\r\n\r\n../../Envs/pandas-dev/lib/python3.7/site-packages/fastparquet/dataframe.py:152: AttributeError\r\n```\r\n\r\n</details>\r\n\r\nFor a datetime column, `DatetimeTZBlock.values`  is now a `DatetimeArray` (pandas extension array). I haven't looked closely at what fastparquet is doing, but it's possible that\r\n\r\n```python\r\nblock.make_block_same_class(np.empty(new_shape, 'M8[ns]'), dtype=block.values.dtype)\r\n```\r\n\r\nwill work for old and new pandas. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/386", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/386/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/386/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/386/events", "html_url": "https://github.com/dask/fastparquet/issues/386", "id": 385578451, "node_id": "MDU6SXNzdWUzODU1Nzg0NTE=", "number": 386, "title": "Close a file?", "user": {"login": "jkleint", "id": 931505, "node_id": "MDQ6VXNlcjkzMTUwNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/931505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jkleint", "html_url": "https://github.com/jkleint", "followers_url": "https://api.github.com/users/jkleint/followers", "following_url": "https://api.github.com/users/jkleint/following{/other_user}", "gists_url": "https://api.github.com/users/jkleint/gists{/gist_id}", "starred_url": "https://api.github.com/users/jkleint/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jkleint/subscriptions", "organizations_url": "https://api.github.com/users/jkleint/orgs", "repos_url": "https://api.github.com/users/jkleint/repos", "events_url": "https://api.github.com/users/jkleint/events{/privacy}", "received_events_url": "https://api.github.com/users/jkleint/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-29T05:41:08Z", "updated_at": "2018-12-06T22:32:01Z", "closed_at": "2018-12-06T22:32:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello and thanks for such a great library.\r\n\r\nDumb question... it seems there's no way to close a ParquetFile?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/383", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/383/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/383/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/383/events", "html_url": "https://github.com/dask/fastparquet/issues/383", "id": 379854543, "node_id": "MDU6SXNzdWUzNzk4NTQ1NDM=", "number": 383, "title": "ParquetFile.to_pandas changes global `cols` object", "user": {"login": "kylebarron", "id": 15164633, "node_id": "MDQ6VXNlcjE1MTY0NjMz", "avatar_url": "https://avatars3.githubusercontent.com/u/15164633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kylebarron", "html_url": "https://github.com/kylebarron", "followers_url": "https://api.github.com/users/kylebarron/followers", "following_url": "https://api.github.com/users/kylebarron/following{/other_user}", "gists_url": "https://api.github.com/users/kylebarron/gists{/gist_id}", "starred_url": "https://api.github.com/users/kylebarron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kylebarron/subscriptions", "organizations_url": "https://api.github.com/users/kylebarron/orgs", "repos_url": "https://api.github.com/users/kylebarron/repos", "events_url": "https://api.github.com/users/kylebarron/events{/privacy}", "received_events_url": "https://api.github.com/users/kylebarron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-12T16:27:52Z", "updated_at": "2018-11-12T21:50:57Z", "closed_at": "2018-11-12T21:50:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tend to name the list of columns that I want to import `cols`. To my surprise, after reading a file with `to_pandas`, the `cols` list is mutated. I assume this is a bug, that the reader shouldn't mutate an object in the global scope?\r\n\r\n```py\r\n>>> import pandas as pd\r\n>>> import fastparquet as fp\r\n>>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\r\n>>> df.to_parquet('test.parquet')\r\n>>> cols = ['a']\r\n>>> pf = fp.ParquetFile('test.parquet')\r\n>>> df = pf.to_pandas(columns=cols)\r\n>>> cols\r\n['a', '__index_level_0__']\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/381", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/381/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/381/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/381/events", "html_url": "https://github.com/dask/fastparquet/issues/381", "id": 378503165, "node_id": "MDU6SXNzdWUzNzg1MDMxNjU=", "number": 381, "title": "ZSTD implemented wrong if `zstd` is installed", "user": {"login": "dargueta", "id": 620513, "node_id": "MDQ6VXNlcjYyMDUxMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/620513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dargueta", "html_url": "https://github.com/dargueta", "followers_url": "https://api.github.com/users/dargueta/followers", "following_url": "https://api.github.com/users/dargueta/following{/other_user}", "gists_url": "https://api.github.com/users/dargueta/gists{/gist_id}", "starred_url": "https://api.github.com/users/dargueta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dargueta/subscriptions", "organizations_url": "https://api.github.com/users/dargueta/orgs", "repos_url": "https://api.github.com/users/dargueta/repos", "events_url": "https://api.github.com/users/dargueta/events{/privacy}", "received_events_url": "https://api.github.com/users/dargueta/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2018-11-07T22:49:05Z", "updated_at": "2020-04-02T13:06:27Z", "closed_at": "2020-04-02T13:06:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "When calling `write()` with `compression='zstd'`, if you have [`zstd`](https://pypi.org/project/zstd/) installed instead of `zstandard` you get the following exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./load_logs.py\", line 80, in <module>\r\n    load_files()\r\n  File \"./load_logs.py\", line 77, in load_files\r\n    compression='zstd', engine='fastparquet', has_nulls=False)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/core/frame.py\", line 1945, in to_parquet\r\n    compression=compression, **kwargs)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/io/parquet.py\", line 257, in to_parquet\r\n    return impl.write(df, path, compression=compression, **kwargs)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/io/parquet.py\", line 218, in write\r\n    compression=compression, **kwargs)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/fastparquet/writer.py\", line 848, in write\r\n    compression, open_with, has_nulls, append)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/fastparquet/writer.py\", line 717, in write_simple\r\n    compression=compression)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/fastparquet/writer.py\", line 614, in make_row_group\r\n    compression=comp)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/fastparquet/writer.py\", line 536, in write_column\r\n    bdata = compress_data(bdata, compression)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/fastparquet/compression.py\", line 133, in compress_data\r\n    return compressions[algorithm.upper()](data)\r\n  File \"/Users/dargueta/.pyenv/versions/3.7.0/lib/python3.7/site-packages/fastparquet/compression.py\", line 94, in zstd_compress\r\n    cctx = zstd.ZstdCompressor(**kwargs)\r\nAttributeError: module 'zstd' has no attribute 'ZstdCompressor'\r\n```\r\n\r\nUsing `zstd` instead of `zstandard` *is* supported in theory, but the bug appears to be a copy-paste error. The compression and decompression functions can be simplified to something like the following:\r\n\r\n```py\r\ndef zstd_compress(data, compression_level=None):\r\n    if compression_level is not None:\r\n        return zstd.compress(data, compression_level)\r\n    return zstd.compress(data)\r\ndef zstd_decompress(data):\r\n    return zstd.decompress(data)\r\n```\r\n\r\nEnvironment:\r\n* Python 3.7.0\r\n* MacOS High Sierra\r\n* Packages:\r\n\r\n```\r\nfastparquet==0.1.6\r\nzstd==1.3.5.1\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/376", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/376/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/376/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/376/events", "html_url": "https://github.com/dask/fastparquet/issues/376", "id": 360689852, "node_id": "MDU6SXNzdWUzNjA2ODk4NTI=", "number": 376, "title": "Reading a column that doesn't exist works!", "user": {"login": "birdsarah", "id": 1796208, "node_id": "MDQ6VXNlcjE3OTYyMDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1796208?v=4", "gravatar_id": "", "url": "https://api.github.com/users/birdsarah", "html_url": "https://github.com/birdsarah", "followers_url": "https://api.github.com/users/birdsarah/followers", "following_url": "https://api.github.com/users/birdsarah/following{/other_user}", "gists_url": "https://api.github.com/users/birdsarah/gists{/gist_id}", "starred_url": "https://api.github.com/users/birdsarah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/birdsarah/subscriptions", "organizations_url": "https://api.github.com/users/birdsarah/orgs", "repos_url": "https://api.github.com/users/birdsarah/repos", "events_url": "https://api.github.com/users/birdsarah/events{/privacy}", "received_events_url": "https://api.github.com/users/birdsarah/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-16T23:31:37Z", "updated_at": "2018-09-18T12:44:37Z", "closed_at": "2018-09-18T12:44:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "I spent a long time today thinking that my data was corrupt, until I realized that I had a typo in my column name.\r\n\r\n```\r\ndf = dd.read_parquet(sample_file, columns=['b', 'a', 'nonsense'], engine='fastparquet')\r\n```\r\n\r\nThis code works, and builds a nonsense column full of very large and very small numbers.\r\n\r\nIf I use pyarrow, it at least fails, although the error is somewhat inscrutable\r\n\r\n```\r\nKeyError: \"['nonsense'] not in index\"\r\n```\r\n\r\nIf I attempt similar, albeit this example is from read_csv on pandas, I get:\r\n\r\n```\r\nValueError: Usecols do not match columns, columns expected but not found: ['nonsense']\r\n```\r\n\r\nI most certainly expected:\r\n\r\n1. the read to fail\r\n1. ideally error message like the last one", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/374", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/374/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/374/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/374/events", "html_url": "https://github.com/dask/fastparquet/issues/374", "id": 359504707, "node_id": "MDU6SXNzdWUzNTk1MDQ3MDc=", "number": 374, "title": "Remaining differences with pyarrow", "user": {"login": "martindurant", "id": 6042212, "node_id": "MDQ6VXNlcjYwNDIyMTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6042212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martindurant", "html_url": "https://github.com/martindurant", "followers_url": "https://api.github.com/users/martindurant/followers", "following_url": "https://api.github.com/users/martindurant/following{/other_user}", "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}", "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions", "organizations_url": "https://api.github.com/users/martindurant/orgs", "repos_url": "https://api.github.com/users/martindurant/repos", "events_url": "https://api.github.com/users/martindurant/events{/privacy}", "received_events_url": "https://api.github.com/users/martindurant/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-09-12T14:22:18Z", "updated_at": "2019-06-06T16:37:14Z", "closed_at": "2019-06-06T16:37:14Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "This is a list of functionality that I believe fastparquet implements but pyarrow does not. It should be considered for making a set of JIRAs in arrow, should there be will to implement any of them (@wesm). Please help by adding to this list anything I have missed or noting things that already do also exist in pyarrow.\r\n\r\nThe list is in no particular order.\r\n\r\n- JSON and BSON encoding\r\n- reading of array and map types\r\n- directory-based partitioning in \"drill\" style\r\n- direct reading and writing of categoricals\r\n- use of metadata files to avoid pre-scan of all data files\r\n- filtering on index or specific columns which have chunk-level max/min information\r\n- column-level choice of object encoding and optional/required\r\n- parameters passed to compression backends\r\n- arbitrary row-group splitting\r\n- fixed-length string and bytes types\r\n- 8- and 12-bytes times\r\n- (new) read datasets without scanning the files at all in the client", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/372", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/372/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/372/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/372/events", "html_url": "https://github.com/dask/fastparquet/issues/372", "id": 357707968, "node_id": "MDU6SXNzdWUzNTc3MDc5Njg=", "number": 372, "title": "ARRAY<string> data type with fastparquet?", "user": {"login": "alexwbai", "id": 30630653, "node_id": "MDQ6VXNlcjMwNjMwNjUz", "avatar_url": "https://avatars3.githubusercontent.com/u/30630653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexwbai", "html_url": "https://github.com/alexwbai", "followers_url": "https://api.github.com/users/alexwbai/followers", "following_url": "https://api.github.com/users/alexwbai/following{/other_user}", "gists_url": "https://api.github.com/users/alexwbai/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexwbai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexwbai/subscriptions", "organizations_url": "https://api.github.com/users/alexwbai/orgs", "repos_url": "https://api.github.com/users/alexwbai/repos", "events_url": "https://api.github.com/users/alexwbai/events{/privacy}", "received_events_url": "https://api.github.com/users/alexwbai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-06T15:24:08Z", "updated_at": "2018-09-06T15:33:38Z", "closed_at": "2018-09-06T15:33:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey quick question for you smart people-\r\n\r\nI have an AWS pipeline that outputs data to S3 in parquet format.  I've defined some DDL (using AWS Athena) which maps a column in the parquet data as an ARRAY<string> datatype.  The data for this column looks like this: [string,string,string].  This part works fine.\r\n\r\nI need to manually add some additional parquet data to this S3 bucket outside of the pipeline.  I have decided to use fastparquet to do it and I seem to be running into an issue where after saving the parquet file with fastparquet my column of [string,string,string] is not being read as ARRAY<string>.  If I change the DDL to just STRING it works.\r\n\r\nIs there any trick to convert a column with data like this ['string1','string2','string3'] to be able to be discovered as an ARRAY<STRING> datatype when saved with fastparquet?  Or is this more of a pandas issue?  Thanks for any help!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/370", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/370/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/370/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/370/events", "html_url": "https://github.com/dask/fastparquet/issues/370", "id": 356198617, "node_id": "MDU6SXNzdWUzNTYxOTg2MTc=", "number": 370, "title": "`fastparquet.api.sorted_partitioned_columns()` does not account for `filters`", "user": {"login": "andrethrill", "id": 25300892, "node_id": "MDQ6VXNlcjI1MzAwODky", "avatar_url": "https://avatars1.githubusercontent.com/u/25300892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrethrill", "html_url": "https://github.com/andrethrill", "followers_url": "https://api.github.com/users/andrethrill/followers", "following_url": "https://api.github.com/users/andrethrill/following{/other_user}", "gists_url": "https://api.github.com/users/andrethrill/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrethrill/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrethrill/subscriptions", "organizations_url": "https://api.github.com/users/andrethrill/orgs", "repos_url": "https://api.github.com/users/andrethrill/repos", "events_url": "https://api.github.com/users/andrethrill/events{/privacy}", "received_events_url": "https://api.github.com/users/andrethrill/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-01T12:24:33Z", "updated_at": "2018-09-04T14:01:16Z", "closed_at": "2018-09-04T14:01:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "### TLDR\r\n\r\n`fastparquet.api.sorted_partitioned_columns()` currently checks the entire parquet file without considering the possibility of `filters`. \r\n\r\n*I'm uploading a PR in a few moments that fixes this.*\r\n\r\nThis, in turn, is useful for https://github.com/dask/dask/issues/3930.\r\n\r\n\r\n### Imports\r\n\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\nimport fastparquet\r\nfrom fastparquet import ParquetFile\r\n```\r\n\r\n### Generate dummy data. Save to a partitioned parquet file\r\n\r\n\r\n```python\r\ndef generate_df(size, id_val=None):\r\n    df = pd.DataFrame({'unique' : np.arange(0, size),\r\n                       'dates' : pd.date_range('2015-01-01',\r\n                                             periods=size,\r\n                                             freq='1min',\r\n                                             name='dates')})\r\n    if id_val is not None:\r\n        df['id']=id_val\r\n    return df\r\n```\r\n\r\n\r\n```python\r\ndf_list = [generate_df(4, id_val = id_val) for id_val in ['id1','id2'] ]\r\ndf_pd = pd.concat(df_list).sort_values('dates').set_index('dates')\r\ndf_pd\r\n```\r\n\r\n\r\n\r\n\r\n<div>\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>unique</th>\r\n      <th>id</th>\r\n    </tr>\r\n    <tr>\r\n      <th>dates</th>\r\n      <th></th>\r\n      <th></th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>2015-01-01 00:00:00</th>\r\n      <td>0</td>\r\n      <td>id1</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2015-01-01 00:00:00</th>\r\n      <td>0</td>\r\n      <td>id2</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2015-01-01 00:01:00</th>\r\n      <td>1</td>\r\n      <td>id1</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2015-01-01 00:01:00</th>\r\n      <td>1</td>\r\n      <td>id2</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2015-01-01 00:02:00</th>\r\n      <td>2</td>\r\n      <td>id1</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2015-01-01 00:02:00</th>\r\n      <td>2</td>\r\n      <td>id2</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2015-01-01 00:03:00</th>\r\n      <td>3</td>\r\n      <td>id1</td>\r\n    </tr>\r\n    <tr>\r\n      <th>2015-01-01 00:03:00</th>\r\n      <td>3</td>\r\n      <td>id2</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n</div>\r\n\r\n\r\n\r\n\r\n```python\r\ndf = dd.from_pandas(df_pd, npartitions=2)\r\n```\r\n\r\n\r\n```python\r\n# The divisions\r\ndf.divisions\r\n```\r\n\r\n\r\n\r\n\r\n    (Timestamp('2015-01-01 00:00:00'),\r\n     Timestamp('2015-01-01 00:02:00'),\r\n     Timestamp('2015-01-01 00:03:00'))\r\n\r\n\r\n\r\n\r\n```python\r\n# 2 partitions as expected\r\nprint('partitions: ' + str(df.npartitions))\r\n```\r\n\r\n    partitions: 2\r\n\r\n\r\n\r\n```python\r\ndf.to_parquet('./data.parquet', compression='snappy',\r\n              engine='fastparquet', partition_on=['id'])\r\n```\r\n\r\n### Load ParquetFile\r\n\r\n\r\n```python\r\npq_f = ParquetFile('./data.parquet')\r\n```\r\n\r\n\r\n```python\r\n# the group files\r\npq_f.group_files\r\n```\r\n\r\n\r\n\r\n\r\n    {0: {'id=id1/part.0.parquet'},\r\n     1: {'id=id2/part.0.parquet'},\r\n     2: {'id=id1/part.1.parquet'},\r\n     3: {'id=id2/part.1.parquet'}}\r\n\r\n\r\n\r\n### The output of sorted_partitioned_columns()\r\n\r\nAssume I want to filter the parquet file only based on the following filters:\r\n\r\n\r\n```python\r\nfilters = [('id', '==', 'id1')]\r\n```\r\n\r\nAt the moment, `sorted_partitioned_columns()` does not take `filters` into consideration and, as expected, outputs an empty `dict()`:\r\n\r\n\r\n```python\r\nfastparquet.api.sorted_partitioned_columns(pq_f)\r\n```\r\n\r\n\r\n\r\n\r\n    {}\r\n\r\n\r\n\r\nI'm soon creating a PR that will allow to take into consideration `filters` when checking for sorted columns partition-by-partition:\r\n\r\n\r\n```python\r\noutput = fastparquet.api.sorted_partitioned_columns(pq_f, filters=filters)\r\noutput\r\n```\r\n\r\n\r\n\r\n\r\n    {'dates': {'min': [numpy.datetime64('2015-01-01T00:00:00.000000000'),\r\n       numpy.datetime64('2015-01-01T00:02:00.000000000')],\r\n      'max': [numpy.datetime64('2015-01-01T00:01:00.000000000'),\r\n       numpy.datetime64('2015-01-01T00:03:00.000000000')]},\r\n     'unique': {'min': [0, 2], 'max': [1, 3]}}\r\n\r\n\r\n\r\nThis, in turn, is useful for https://github.com/dask/dask/issues/3930.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/366", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/366/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/366/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/366/events", "html_url": "https://github.com/dask/fastparquet/issues/366", "id": 354329499, "node_id": "MDU6SXNzdWUzNTQzMjk0OTk=", "number": 366, "title": "RuntimeError: Decompression 'SNAPPY' not available.  Options: ['GZIP', 'UNCOMPRESSED']", "user": {"login": "lqueryvg", "id": 5748327, "node_id": "MDQ6VXNlcjU3NDgzMjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5748327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lqueryvg", "html_url": "https://github.com/lqueryvg", "followers_url": "https://api.github.com/users/lqueryvg/followers", "following_url": "https://api.github.com/users/lqueryvg/following{/other_user}", "gists_url": "https://api.github.com/users/lqueryvg/gists{/gist_id}", "starred_url": "https://api.github.com/users/lqueryvg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lqueryvg/subscriptions", "organizations_url": "https://api.github.com/users/lqueryvg/orgs", "repos_url": "https://api.github.com/users/lqueryvg/repos", "events_url": "https://api.github.com/users/lqueryvg/events{/privacy}", "received_events_url": "https://api.github.com/users/lqueryvg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2018-08-27T13:56:05Z", "updated_at": "2020-07-13T14:59:11Z", "closed_at": "2018-08-27T18:58:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Output:\r\n```\r\nRuntimeError: Decompression 'SNAPPY' not available.  Options: ['GZIP', 'UNCOMPRESSED']\r\n```\r\n\r\nCode:\r\n```\r\nfrom fastparquet import ParquetFile\r\n\r\nfilename = 'somefile.parquet'\r\npf = ParquetFile(filename)\r\n```\r\n\r\nEnvironment:\r\n```\r\n$ python -V; pip list | grep -e fastparquet -e snapp\r\nPython 3.6.5\r\nfastparquet      0.1.6\r\npython-snappy    0.5.3\r\n```\r\n\r\nI've tried install snappy instead of python-snappy. Still no joy because with these installed...\r\n\r\n```\r\npython-snappy    0.5.3\r\nsnappy           2.6.1\r\nsnappy-manifolds 1.0\r\n```\r\n\r\nI get the error:\r\n\r\n```\r\nAttributeError: module 'snappy' has no attribute 'compress'\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/365", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/365/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/365/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/365/events", "html_url": "https://github.com/dask/fastparquet/issues/365", "id": 353801396, "node_id": "MDU6SXNzdWUzNTM4MDEzOTY=", "number": 365, "title": "Support pathlib Path inputs", "user": {"login": "kylebarron", "id": 15164633, "node_id": "MDQ6VXNlcjE1MTY0NjMz", "avatar_url": "https://avatars3.githubusercontent.com/u/15164633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kylebarron", "html_url": "https://github.com/kylebarron", "followers_url": "https://api.github.com/users/kylebarron/followers", "following_url": "https://api.github.com/users/kylebarron/following{/other_user}", "gists_url": "https://api.github.com/users/kylebarron/gists{/gist_id}", "starred_url": "https://api.github.com/users/kylebarron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kylebarron/subscriptions", "organizations_url": "https://api.github.com/users/kylebarron/orgs", "repos_url": "https://api.github.com/users/kylebarron/repos", "events_url": "https://api.github.com/users/kylebarron/events{/privacy}", "received_events_url": "https://api.github.com/users/kylebarron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-24T14:14:41Z", "updated_at": "2018-11-26T14:30:33Z", "closed_at": "2018-11-26T14:30:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently when I do\r\n```py\r\nimport fastparquet as fp\r\nfrom pathlib import Path\r\nfp.ParquetFile(Path('/path/to/file.parquet'))\r\n```\r\nI get a `TypeError`\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-56-691671112f6c> in <module>()\r\n----> 1 pf = fp.ParquetFile((self.data_dir / 'raw' / 'pos' / f'pos{self.year}.parquet'))\r\n\r\n~/disk/age-discontinuities/local/miniconda/envs/age-discont/lib/python3.6/site-packages/fastparquet/api.py in __init__(self, fn, verify, open_with, root, sep)\r\n     90         else:\r\n     91             try:\r\n---> 92                 fn2 = join_path(fn, '_metadata')\r\n     93                 self.fn = fn2\r\n     94                 with open_with(fn2, 'rb') as f:\r\n\r\n~/disk/age-discontinuities/local/miniconda/envs/age-discont/lib/python3.6/site-packages/fastparquet/util.py in join_path(*path)\r\n    305     abs_prefix = ''\r\n    306     if path and path[0]:\r\n--> 307         if path[0][0] == '/':\r\n    308             abs_prefix = '/'\r\n    309             path = list(path)\r\n\r\nTypeError: 'PosixPath' object does not support indexing\r\n```\r\n\r\nAre there any plans to support pathlib inputs?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/364", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/364/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/364/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/364/events", "html_url": "https://github.com/dask/fastparquet/issues/364", "id": 353427744, "node_id": "MDU6SXNzdWUzNTM0Mjc3NDQ=", "number": 364, "title": "Compatibilty between pyarrow partitions and fastparquet", "user": {"login": "barrachri", "id": 4802083, "node_id": "MDQ6VXNlcjQ4MDIwODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/4802083?v=4", "gravatar_id": "", "url": "https://api.github.com/users/barrachri", "html_url": "https://github.com/barrachri", "followers_url": "https://api.github.com/users/barrachri/followers", "following_url": "https://api.github.com/users/barrachri/following{/other_user}", "gists_url": "https://api.github.com/users/barrachri/gists{/gist_id}", "starred_url": "https://api.github.com/users/barrachri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/barrachri/subscriptions", "organizations_url": "https://api.github.com/users/barrachri/orgs", "repos_url": "https://api.github.com/users/barrachri/repos", "events_url": "https://api.github.com/users/barrachri/events{/privacy}", "received_events_url": "https://api.github.com/users/barrachri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-23T15:10:52Z", "updated_at": "2018-08-23T15:30:31Z", "closed_at": "2018-08-23T15:30:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I was playing with fastarrow and pyarrow and I was expecting this to work\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow.parquet as pq\r\nimport pyarrow as pa\r\nfrom fastparquet import ParquetFile\r\n\r\n# create a df\r\ndf = pd.DataFrame({'one': [-1, np.nan, 2.5], 'two': ['foo', 'bar', 'baz'], 'three': [True, False, True]})\r\n\r\n# save it using pyarrow\r\ntable = pa.Table.from_pandas(df)\r\npq.write_to_dataset(table, 'example_pyarrow', partition_cols=['three'])\r\n\r\n# reading it using fastparquet\r\nParquetFile('example_pyarrow').to_pandas()\r\n\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n~/miniconda3/envs/pyarrow_fastparquet/lib/python3.6/site-packages/fastparquet/api.py in __init__(self, fn, verify, open_with, root, sep)\r\n    109                 self.fn = fn2\r\n--> 110                 with open_with(fn2, 'rb') as f:\r\n    111                     self._parse_header(f, verify)\r\n\r\n~/miniconda3/envs/pyarrow_fastparquet/lib/python3.6/site-packages/fastparquet/util.py in default_open(f, mode)\r\n     37 def default_open(f, mode='rb'):\r\n---> 38     return open(f, mode)\r\n     39 \r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'example_pyarrow/_metadata'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nIsADirectoryError                         Traceback (most recent call last)\r\n<ipython-input-13-8484e4004a14> in <module>()\r\n      2 table = pa.Table.from_pandas(df)\r\n      3 pq.write_to_dataset(table, 'test', partition_cols=['three'])\r\n----> 4 ParquetFile('example_pyarrow').to_pandas()\r\n\r\n~/miniconda3/envs/pyarrow_fastparquet/lib/python3.6/site-packages/fastparquet/api.py in __init__(self, fn, verify, open_with, root, sep)\r\n    113             except (IOError, OSError):\r\n    114                 self.fn = join_path(fn)\r\n--> 115                 with open_with(fn, 'rb') as f:\r\n    116                     self._parse_header(f, verify)\r\n    117         self.open = open_with\r\n\r\n~/miniconda3/envs/pyarrow_fastparquet/lib/python3.6/site-packages/fastparquet/util.py in default_open(f, mode)\r\n     36 \r\n     37 def default_open(f, mode='rb'):\r\n---> 38     return open(f, mode)\r\n     39 \r\n     40 \r\n\r\nIsADirectoryError: [Errno 21] Is a directory: 'example_pyarrow'\r\n```\r\nTo summarize: \r\n- reading fastparquet partitions with fastparquet \u2705\r\n- reading fastparquet partitions with pyarrow \u2705\r\n- reading pyarrow partitions with fastparquet \ud83d\uded1\r\n- reading pyarrow partitions with pyarrow \u2705\r\n\r\nIs the pyarrow format wrong?\r\n\r\nIt doesn't seem to use the hive format but just separting the dataframes into different files.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/362", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/362/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/362/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/362/events", "html_url": "https://github.com/dask/fastparquet/issues/362", "id": 352381336, "node_id": "MDU6SXNzdWUzNTIzODEzMzY=", "number": 362, "title": "Appetite for PR to accommodate `filters` with strings and `in` operator?", "user": {"login": "CharlesHe16", "id": 23063929, "node_id": "MDQ6VXNlcjIzMDYzOTI5", "avatar_url": "https://avatars2.githubusercontent.com/u/23063929?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CharlesHe16", "html_url": "https://github.com/CharlesHe16", "followers_url": "https://api.github.com/users/CharlesHe16/followers", "following_url": "https://api.github.com/users/CharlesHe16/following{/other_user}", "gists_url": "https://api.github.com/users/CharlesHe16/gists{/gist_id}", "starred_url": "https://api.github.com/users/CharlesHe16/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CharlesHe16/subscriptions", "organizations_url": "https://api.github.com/users/CharlesHe16/orgs", "repos_url": "https://api.github.com/users/CharlesHe16/repos", "events_url": "https://api.github.com/users/CharlesHe16/events{/privacy}", "received_events_url": "https://api.github.com/users/CharlesHe16/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-08-21T04:27:08Z", "updated_at": "2018-11-26T14:30:33Z", "closed_at": "2018-11-26T14:30:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using `ParquetFile()` or `read_parquet()`, there is a use case for `filters=` with sets of strings as the values in the `filters`, particularly using the `in` or equality operators. For example:\r\n\r\n```python\r\nfilters = [('planet','in','jedha'), ('planet','in','scarif')]\r\n```\r\n\r\nThis isn't currently accommodated. Instead, as seen in `fastparquet.api.filter_val()`, there is a lexicographic comparison involving `max`/`min` operators. (There is also no warning for this behavior, and this use of filters is not proscribed by the docs.)\r\n\r\nCan I make a PR to modify `filter_val()` for this use case?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/361", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/361/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/361/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/361/events", "html_url": "https://github.com/dask/fastparquet/issues/361", "id": 352307117, "node_id": "MDU6SXNzdWUzNTIzMDcxMTc=", "number": 361, "title": "IndexError when loading zero-column DataFrame", "user": {"login": "adamhooper", "id": 10812, "node_id": "MDQ6VXNlcjEwODEy", "avatar_url": "https://avatars3.githubusercontent.com/u/10812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamhooper", "html_url": "https://github.com/adamhooper", "followers_url": "https://api.github.com/users/adamhooper/followers", "following_url": "https://api.github.com/users/adamhooper/following{/other_user}", "gists_url": "https://api.github.com/users/adamhooper/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamhooper/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamhooper/subscriptions", "organizations_url": "https://api.github.com/users/adamhooper/orgs", "repos_url": "https://api.github.com/users/adamhooper/repos", "events_url": "https://api.github.com/users/adamhooper/events{/privacy}", "received_events_url": "https://api.github.com/users/adamhooper/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-20T21:42:47Z", "updated_at": "2018-08-21T15:53:52Z", "closed_at": "2018-08-21T15:53:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "fastparquet can save a zero-column DataFrame, but it can't load it. Here's the test case:\r\n\r\n```\r\n>>> import fastparquet\r\n>>> import pandas\r\n>>> fastparquet.write('empty.parq', pandas.DataFrame({'A': ['a', 'b',' c']})[[]])\r\n>>> fastparquet.ParquetFile('empty.parq')\r\nTraceback (most recent call last):\r\n  File \"/root/.local/share/virtualenvs/app-4PlAip0Q/lib/python3.6/site-packages/fastparquet/api.py\", line 110, in __init__\r\n    with open_with(fn2, 'rb') as f:\r\n  File \"/root/.local/share/virtualenvs/app-4PlAip0Q/lib/python3.6/site-packages/fastparquet/util.py\", line 38, in default_open\r\n    return open(f, mode)\r\nNotADirectoryError: [Errno 20] Not a directory: 'empty.parq/_metadata'\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/.local/share/virtualenvs/app-4PlAip0Q/lib/python3.6/site-packages/fastparquet/api.py\", line 116, in __init__\r\n    self._parse_header(f, verify)\r\n  File \"/root/.local/share/virtualenvs/app-4PlAip0Q/lib/python3.6/site-packages/fastparquet/api.py\", line 140, in _parse_header\r\n    self._set_attrs()\r\n  File \"/root/.local/share/virtualenvs/app-4PlAip0Q/lib/python3.6/site-packages/fastparquet/api.py\", line 157, in _set_attrs\r\n    for rg in self.row_groups])\r\n  File \"/root/.local/share/virtualenvs/app-4PlAip0Q/lib/python3.6/site-packages/fastparquet/api.py\", line 157, in <listcomp>\r\n    for rg in self.row_groups])\r\nIndexError: list index out of range\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/360", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/360/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/360/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/360/events", "html_url": "https://github.com/dask/fastparquet/issues/360", "id": 351578196, "node_id": "MDU6SXNzdWUzNTE1NzgxOTY=", "number": 360, "title": "Would it be worth making this method accept both filepath and BytesIO/StringIO object?", "user": {"login": "aaronfowles", "id": 3754959, "node_id": "MDQ6VXNlcjM3NTQ5NTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3754959?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaronfowles", "html_url": "https://github.com/aaronfowles", "followers_url": "https://api.github.com/users/aaronfowles/followers", "following_url": "https://api.github.com/users/aaronfowles/following{/other_user}", "gists_url": "https://api.github.com/users/aaronfowles/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaronfowles/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaronfowles/subscriptions", "organizations_url": "https://api.github.com/users/aaronfowles/orgs", "repos_url": "https://api.github.com/users/aaronfowles/repos", "events_url": "https://api.github.com/users/aaronfowles/events{/privacy}", "received_events_url": "https://api.github.com/users/aaronfowles/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-08-17T12:39:21Z", "updated_at": "2019-09-23T16:00:10Z", "closed_at": "2018-11-26T14:30:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "When calling the `to_parquet` method in pandas, the api is not consistent when using fastparquet engine. For example, `to_csv` accepts either a filepath or a buffer. So does `to_parquet` when using the pyarrow engine. However, calling `to_parquet` when using the fastparquet engine will error when passed a buffer rather than filepath.\r\n\r\nI think it would be good to adhere to the polymorphic api exposed by the other `to_<format>` methods and allow both filepath and buffer to be passed to `to_parquet`.\r\n\r\nhttps://github.com/dask/fastparquet/blob/5b4f30a0a7fe90cc61ca5119b8a65ca614aa0a45/fastparquet/writer.py#L735\r\n\r\nI am happy to submit a PR for this unless anyone has an objection?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/359", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/359/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/359/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/359/events", "html_url": "https://github.com/dask/fastparquet/issues/359", "id": 351252923, "node_id": "MDU6SXNzdWUzNTEyNTI5MjM=", "number": 359, "title": "Performance issues on Python 3.6?", "user": {"login": "andrethrill", "id": 25300892, "node_id": "MDQ6VXNlcjI1MzAwODky", "avatar_url": "https://avatars1.githubusercontent.com/u/25300892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrethrill", "html_url": "https://github.com/andrethrill", "followers_url": "https://api.github.com/users/andrethrill/followers", "following_url": "https://api.github.com/users/andrethrill/following{/other_user}", "gists_url": "https://api.github.com/users/andrethrill/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrethrill/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrethrill/subscriptions", "organizations_url": "https://api.github.com/users/andrethrill/orgs", "repos_url": "https://api.github.com/users/andrethrill/repos", "events_url": "https://api.github.com/users/andrethrill/events{/privacy}", "received_events_url": "https://api.github.com/users/andrethrill/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-08-16T15:21:18Z", "updated_at": "2018-08-16T16:52:37Z", "closed_at": "2018-08-16T16:52:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a (big) partitioned parquet file which initializes relatively quickly on Python 3.5 and 3.7, but very slowly on Python 3.6:\r\n\r\n```python\r\n# python --version\r\n# Python 3.5.2\r\n    \r\n# pip freeze | grep fastparquet\r\n# fastparquet==0.1.5\r\n\r\n# pip freeze | grep thrift\r\n# thrift==0.11.0\r\n    \r\nimport cProfile\r\nimport fastparquet\r\ncProfile.run(\"fastparquet.ParquetFile('my_parquet_file')\", sort=2)\r\n```\r\n```\r\n         1193523 function calls (1193359 primitive calls) in 2.797 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000    2.798    2.798 {built-in method builtins.exec}\r\n        1    0.146    0.146    2.798    2.798 <string>:1(<module>)\r\n        1    0.000    0.000    2.652    2.652 api.py:79(__init__)\r\n        1    0.000    0.000    2.652    2.652 api.py:104(_parse_header)\r\n        1    0.000    0.000    1.778    1.778 thrift_structures.py:10(read_thrift)\r\n        1    0.000    0.000    1.778    1.778 ttypes.py:1924(read)\r\n        1    1.526    1.526    1.778    1.778 {built-in method thrift.protocol.fastbinary.decode_compact}\r\n        1    0.366    0.366    0.874    0.874 api.py:126(_set_attrs)\r\n        1    0.147    0.147    0.416    0.416 api.py:161(_read_partitions)\r\n   115920    0.175    0.000    0.175    0.000 {method 'findall' of '_sre.SRE_Pattern' objects}\r\n   106260    0.085    0.000    0.085    0.000 ttypes.py:1357(__init__)\r\n   106260    0.061    0.000    0.061    0.000 ttypes.py:293(__init__)\r\n        1    0.000    0.000    0.056    0.056 api.py:178(<listcomp>)\r\n      123    0.001    0.000    0.056    0.000 util.py:40(val_to_num)\r\n   106260    0.051    0.000    0.051    0.000 ttypes.py:1258(__init__)\r\n      123    0.001    0.000    0.048    0.000 datetimes.py:106(to_datetime)\r\n      123    0.001    0.000    0.044    0.000 datetimes.py:276(_convert_listlike)\r\n      123    0.009    0.000    0.039    0.000 {built-in method pandas._libs.tslib.array_to_datetime}\r\n        1    0.003    0.003    0.038    0.038 util.py:250(get_file_scheme)\r\n\r\n\r\n......\r\n......\r\n```\r\n\r\n```python\r\n# python --version\r\n# Python 3.6.6\r\n\r\n# pip freeze | grep fastparquet\r\n# fastparquet==0.1.5\r\n\r\n# pip freeze | grep thrift\r\n# thrift==0.11.0\r\n\r\nimport cProfile\r\nimport fastparquet\r\ncProfile.run(\"fastparquet.ParquetFile('my_parquet_file')\", sort=2)\r\n```\r\n```\r\n         66274724 function calls (66274466 primitive calls) in 31.781 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000   31.783   31.783 {built-in method builtins.exec}\r\n        1    0.124    0.124   31.783   31.783 <string>:1(<module>)\r\n        1    0.000    0.000   31.658   31.658 api.py:79(__init__)\r\n        1    0.000    0.000   31.658   31.658 api.py:104(_parse_header)\r\n        1    0.000    0.000   31.122   31.122 thrift_structures.py:10(read_thrift)\r\n        1    0.019    0.019   31.122   31.122 ttypes.py:1924(read)\r\n     9660    0.386    0.000   31.096    0.003 ttypes.py:1682(read)\r\n   106260    0.723    0.000   30.250    0.000 ttypes.py:1596(read)\r\n   106260    2.893    0.000   25.702    0.000 ttypes.py:1372(read)\r\n  1932051    2.055    0.000   12.361    0.000 TCompactProtocol.py:40(nested)\r\n  2588948    2.935    0.000    8.866    0.000 TCompactProtocol.py:278(readFieldBegin)\r\n  6428382    4.020    0.000    8.781    0.000 TTransport.py:56(readAll)\r\n  1506996    0.942    0.000    7.557    0.000 TCompactProtocol.py:315(__readZigZag)\r\n  1932052    0.681    0.000    7.352    0.000 TCompactProtocol.py:312(__readVarint)\r\n  1932052    2.360    0.000    6.671    0.000 TCompactProtocol.py:72(readVarint)\r\n  3023651    1.706    0.000    6.405    0.000 TCompactProtocol.py:304(__readUByte)\r\n  6428513    2.927    0.000    4.325    0.000 TTransport.py:158(read)\r\n   106260    0.652    0.000    4.299    0.000 ttypes.py:301(read)\r\n   106260    0.598    0.000    3.677    0.000 ttypes.py:1263(read)\r\n\r\n\r\n......\r\n......\r\n```\r\n\r\n```python\r\n# python --version\r\n#\u00a0Python 3.7.0\r\n    \r\n# pip freeze | grep fastparquet\r\n# fastparquet==0.1.5\r\n\r\n# pip freeze | grep thrift\r\n# thrift==0.11.0\r\n    \r\nimport cProfile\r\nimport fastparquet\r\ncProfile.run(\"fastparquet.ParquetFile('my_parquet_file')\", sort=2)\r\n```\r\n```\r\n         1190690 function calls (1190679 primitive calls) in 2.527 seconds\r\n\r\n   Ordered by: cumulative time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n        1    0.000    0.000    2.528    2.528 {built-in method builtins.exec}\r\n        1    0.107    0.107    2.528    2.528 <string>:1(<module>)\r\n        1    0.000    0.000    2.422    2.422 api.py:79(__init__)\r\n        1    0.000    0.000    2.421    2.421 api.py:104(_parse_header)\r\n        1    0.000    0.000    1.869    1.869 thrift_structures.py:10(read_thrift)\r\n        1    0.000    0.000    1.868    1.868 ttypes.py:1924(read)\r\n        1    1.292    1.292    1.868    1.868 {built-in method thrift.protocol.fastbinary.decode_compact}\r\n        1    0.078    0.078    0.552    0.552 api.py:126(_set_attrs)\r\n        1    0.129    0.129    0.382    0.382 api.py:161(_read_partitions)\r\n   106260    0.263    0.000    0.263    0.000 ttypes.py:293(__init__)\r\n   106260    0.174    0.000    0.174    0.000 ttypes.py:1258(__init__)\r\n   115920    0.161    0.000    0.161    0.000 {method 'findall' of 're.Pattern' objects}\r\n   106260    0.095    0.000    0.095    0.000 ttypes.py:1357(__init__)\r\n        1    0.000    0.000    0.050    0.050 api.py:178(<listcomp>)\r\n      123    0.001    0.000    0.050    0.000 util.py:40(val_to_num)\r\n      123    0.001    0.000    0.043    0.000 datetimes.py:106(to_datetime)\r\n      123    0.001    0.000    0.040    0.000 datetimes.py:276(_convert_listlike)\r\n      123    0.009    0.000    0.035    0.000 {built-in method pandas._libs.tslib.array_to_datetime}\r\n        1    0.003    0.003    0.033    0.033 util.py:250(get_file_scheme)\r\n......\r\n......\r\n```\r\n\r\nIt takes like 5 times more (when not profiling) to initialize the ParquetFile when using Python 3.6.\r\n\r\nIs this a known issue? Any ideas on how to solve it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/354", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/354/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/354/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/354/events", "html_url": "https://github.com/dask/fastparquet/issues/354", "id": 348001200, "node_id": "MDU6SXNzdWUzNDgwMDEyMDA=", "number": 354, "title": "allow_empty parameter has been deleted in python-zstandard 0.9", "user": {"login": "thrasibule", "id": 494248, "node_id": "MDQ6VXNlcjQ5NDI0OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/494248?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thrasibule", "html_url": "https://github.com/thrasibule", "followers_url": "https://api.github.com/users/thrasibule/followers", "following_url": "https://api.github.com/users/thrasibule/following{/other_user}", "gists_url": "https://api.github.com/users/thrasibule/gists{/gist_id}", "starred_url": "https://api.github.com/users/thrasibule/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thrasibule/subscriptions", "organizations_url": "https://api.github.com/users/thrasibule/orgs", "repos_url": "https://api.github.com/users/thrasibule/repos", "events_url": "https://api.github.com/users/thrasibule/events{/privacy}", "received_events_url": "https://api.github.com/users/thrasibule/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-06T16:51:04Z", "updated_at": "2018-08-06T18:38:11Z", "closed_at": "2018-08-06T18:38:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "See release note https://github.com/indygreg/python-zstandard/blob/d530bc7689c07bad67161d1d679dc57275f83e71/NEWS.rst. which causes this line to break: https://github.com/dask/fastparquet/blob/master/fastparquet/compression.py#L95\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/352", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/352/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/352/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/352/events", "html_url": "https://github.com/dask/fastparquet/issues/352", "id": 342840968, "node_id": "MDU6SXNzdWUzNDI4NDA5Njg=", "number": 352, "title": "IndeError: list index out of range when opening a parquet file saved by pySpark because there is no _metadata folder", "user": {"login": "IceS2", "id": 4912399, "node_id": "MDQ6VXNlcjQ5MTIzOTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/4912399?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IceS2", "html_url": "https://github.com/IceS2", "followers_url": "https://api.github.com/users/IceS2/followers", "following_url": "https://api.github.com/users/IceS2/following{/other_user}", "gists_url": "https://api.github.com/users/IceS2/gists{/gist_id}", "starred_url": "https://api.github.com/users/IceS2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IceS2/subscriptions", "organizations_url": "https://api.github.com/users/IceS2/orgs", "repos_url": "https://api.github.com/users/IceS2/repos", "events_url": "https://api.github.com/users/IceS2/events{/privacy}", "received_events_url": "https://api.github.com/users/IceS2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 19, "created_at": "2018-07-19T18:30:40Z", "updated_at": "2018-07-20T18:45:24Z", "closed_at": "2018-07-20T18:31:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I saved a pySpark dataframe to a parquet file trying to reproduce a error I encountered on nested parquet files where I could load them with fastparquet but couldn't transform them into a pandas DataFrame because `ValueError` would rise because there was a Null value in an integer column.\r\n\r\nTrying to load the dataframe now is raising `IndexError: list index out of range`\r\n\r\n```>>> df.show()\r\n+------------+\r\n|        body|\r\n+------------+\r\n| [32189, 10]|\r\n|[32196, 100]|\r\n|[32197, 100]|\r\n|[32198, 100]|\r\n|[32192, 100]|\r\n|[32193, 100]|\r\n| [32187, 10]|\r\n|[32191, 100]|\r\n|    [32210,]|\r\n|[32212, 123]|\r\n|[32213, 100]|\r\n|[32152, 100]|\r\n| [32148, 10]|\r\n| [32178, 10]|\r\n| [32176, 10]|\r\n| [32179, 10]|\r\n|[32196, 100]|\r\n|[32197, 100]|\r\n|[32198, 100]|\r\n| [32205, 10]|\r\n+------------+\r\nonly showing top 20 rows\r\n\r\n>>> df.write.parquet('test.parquet', mode='overwrite')\r\n>>> df.printSchema()\r\nroot\r\n |-- body: struct (nullable = true)\r\n |    |-- id: integer (nullable = true)\r\n |    |-- candidates_requested: integer (nullable = true)\r\n```\r\n\r\n```In [1]: import fastparquet as fp\r\n   ...: pq = fp.ParquetFile('part-00000-c0834688-6375-4c0f-bc44-a2f5b3d0\r\n   ...: bc4b-c000.snappy.parquet')\r\n   ...: df = pq.to_pandas()\r\n   ...: \r\n   ...: \r\n------------------------------------------------------------------------\r\nNotADirectoryError                     Traceback (most recent call last)\r\n~/.virtualenvs/default/lib/python3.6/site-packages/fastparquet/api.py in __init__(self, fn, verify, open_with, root, sep)\r\n     93                 self.fn = fn2\r\n---> 94                 with open_with(fn2, 'rb') as f:\r\n     95                     self._parse_header(f, verify)\r\n\r\n~/.virtualenvs/default/lib/python3.6/site-packages/fastparquet/util.py in default_open(f, mode)\r\n     36 def default_open(f, mode='rb'):\r\n---> 37     return open(f, mode)\r\n     38 \r\n\r\nNotADirectoryError: [Errno 20] Not a directory: 'part-00000-c0834688-6375-4c0f-bc44-a2f5b3d0bc4b-c000.snappy.parquet/_metadata'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nIndexError                             Traceback (most recent call last)\r\n<ipython-input-1-c3650c67f022> in <module>()\r\n      1 import fastparquet as fp\r\n----> 2 pq = fp.ParquetFile('part-00000-c0834688-6375-4c0f-bc44-a2f5b3d0bc4b-c000.snappy.parquet')\r\n      3 df = pq.to_pandas()\r\n\r\n~/.virtualenvs/default/lib/python3.6/site-packages/fastparquet/api.py in __init__(self, fn, verify, open_with, root, sep)\r\n     98                 self.fn = join_path(fn)\r\n     99                 with open_with(fn, 'rb') as f:\r\n--> 100                     self._parse_header(f, verify)\r\n    101         self.open = open_with\r\n    102         self.sep = sep\r\n\r\n~/.virtualenvs/default/lib/python3.6/site-packages/fastparquet/api.py in _parse_header(self, f, verify)\r\n    122         self.head_size = head_size\r\n    123         self.fmd = fmd\r\n--> 124         self._set_attrs()\r\n    125 \r\n    126     def _set_attrs(self):\r\n\r\n~/.virtualenvs/default/lib/python3.6/site-packages/fastparquet/api.py in _set_attrs(self)\r\n    141                                            for rg in self.row_groups])\r\n    142         self._read_partitions()\r\n--> 143         self._dtypes()\r\n    144 \r\n    145     @ property\r\n\r\n~/.virtualenvs/default/lib/python3.6/site-packages/fastparquet/api.py in _dtypes(self, categories)\r\n    458                 num_nulls = 0\r\n    459                 for rg in self.row_groups:\r\n--> 460                     chunk = rg.columns[i]\r\n    461                     if chunk.meta_data.statistics is None:\r\n    462                         num_nulls = True\r\n\r\nIndexError: list index out of range\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/350", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/350/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/350/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/350/events", "html_url": "https://github.com/dask/fastparquet/issues/350", "id": 342480211, "node_id": "MDU6SXNzdWUzNDI0ODAyMTE=", "number": 350, "title": "Can't write DataFrame as parquet on S3", "user": {"login": "airanzad", "id": 10700179, "node_id": "MDQ6VXNlcjEwNzAwMTc5", "avatar_url": "https://avatars3.githubusercontent.com/u/10700179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/airanzad", "html_url": "https://github.com/airanzad", "followers_url": "https://api.github.com/users/airanzad/followers", "following_url": "https://api.github.com/users/airanzad/following{/other_user}", "gists_url": "https://api.github.com/users/airanzad/gists{/gist_id}", "starred_url": "https://api.github.com/users/airanzad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/airanzad/subscriptions", "organizations_url": "https://api.github.com/users/airanzad/orgs", "repos_url": "https://api.github.com/users/airanzad/repos", "events_url": "https://api.github.com/users/airanzad/events{/privacy}", "received_events_url": "https://api.github.com/users/airanzad/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-07-18T20:29:22Z", "updated_at": "2019-07-23T14:17:43Z", "closed_at": "2018-07-20T14:30:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a simple code to create a DataFrame and save it as Parquet on S3. \r\n\r\n```\r\nimport pandas as pd\r\nimport s3fs\r\nimport fastparquet as fp\r\ns3 = s3fs.S3FileSystem()\r\nmyopen = s3.open\r\nnop = lambda *args, **kwargs: None\r\n\r\nfileContents = \"\"\"\r\n{\r\n\"Results\": [{\r\n\"Test\": {\r\n\"Address\": \"[nil]\",\r\n\"Type\": \"PowerON\",\r\n\"TestStartTime\": \"02/02/2018 15:58:54\",\r\n\"TestStopTime\": \"02/02/2018 15:59:20\"\r\n}]\r\n}\r\n\"\"\"\r\n\r\ndef writeDataForYear():\r\n    year = 2018\r\n    for x in range(1, 365):\r\n        day = x \r\n        for y in range(1, 5):\r\n            resId = str(year) + \"_\" + str(day) + \"_\" + str(y)\r\n            data = {'SerialNumber': ['SN'], 'Year': [year], 'Day': [day], 'RawData': [fileContents]}\r\n            data_frame = pd.DataFrame(data)        \r\n            fp.write('bucket_name/fastparquet.parq', data_frame, partition_on = ['Day', 'Year'], open_with = myopen, mkdirs = nop, append = True)\r\n    pf = fp.ParquetFile(outputFile)\r\n    data_frame2 = pf.to_pandas(['Day', 'Year', 'SerialNumber'], filters=[('Day', 'in', [1, 100, 110,199])])\r\n    print(data_frame2)\r\n```\r\n\r\n\r\nBut it returns error:\r\n\r\n> /var/task/fastparquet/writer.py:217: FutureWarning: Method .valid will be removed in a future version. Use .dropna instead.\r\n  head = data[:10] if isinstance(data, pd.Index) else data.valid()[:10]\r\nbucket_name/fastparquet.parq: FileNotFoundError\r\nTraceback (most recent call last):\r\n  File \"/var/task/writeParquet.py\", line 3429, in handler\r\n    writeDataForYear(2018)\r\n  File \"/var/task/writeParquet.py\", line 3420, in writeDataForYear\r\n    fp.write('bucket_name/fastparquet.parq', data_frame, partition_on = ['Day', 'Year'], open_with = myopen, mkdirs = nop, append = True)\r\n  File \"/var/task/fastparquet/writer.py\", line 848, in write\r\n    compression, open_with, has_nulls, append)\r\n  File \"/var/task/fastparquet/writer.py\", line 698, in write_simple\r\n    pf = api.ParquetFile(fn, open_with=open_with)\r\n  File \"/var/task/fastparquet/api.py\", line 99, in __init__\r\n    with open_with(fn, 'rb') as f:\r\n  File \"/var/task/s3fs/core.py\", line 315, in open\r\n    s3_additional_kwargs=kw)\r\n  File \"/var/task/s3fs/core.py\", line 1102, in __init__\r\n    info = self.info()\r\n  File \"/var/task/s3fs/core.py\", line 1120, in info\r\n    refresh=refresh, **kwargs)\r\n  File \"/var/task/s3fs/core.py\", line 455, in info\r\n    raise FileNotFoundError(path)\r\nFileNotFoundError: bucket_name/fastparquet.parq\r\n\r\nI can't figure out what the problem is. Looks like it is a bug.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/349", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/349/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/349/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/349/events", "html_url": "https://github.com/dask/fastparquet/issues/349", "id": 342444644, "node_id": "MDU6SXNzdWUzNDI0NDQ2NDQ=", "number": 349, "title": "MultiIndex.levels become duplicated and fail indexing when using fastparquet", "user": {"login": "0x0L", "id": 3621629, "node_id": "MDQ6VXNlcjM2MjE2Mjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/3621629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0x0L", "html_url": "https://github.com/0x0L", "followers_url": "https://api.github.com/users/0x0L/followers", "following_url": "https://api.github.com/users/0x0L/following{/other_user}", "gists_url": "https://api.github.com/users/0x0L/gists{/gist_id}", "starred_url": "https://api.github.com/users/0x0L/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0x0L/subscriptions", "organizations_url": "https://api.github.com/users/0x0L/orgs", "repos_url": "https://api.github.com/users/0x0L/repos", "events_url": "https://api.github.com/users/0x0L/events{/privacy}", "received_events_url": "https://api.github.com/users/0x0L/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 32, "created_at": "2018-07-18T18:37:41Z", "updated_at": "2018-08-08T14:10:19Z", "closed_at": "2018-08-08T14:10:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "(from https://github.com/pandas-dev/pandas/issues/21949)\r\n\r\n```python\r\ndf = pd.DataFrame(list(range(9)), pd.MultiIndex.from_product([['a', 'b', 'c'], [1, 2, 3]]), ['col'])\r\ndf.index\r\n# MultiIndex(levels=[['a', 'b', 'c'], [1, 2, 3]],\r\n#            labels=[[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]])\r\ndf.to_parquet('test.parquet', engine='fastparquet')\r\ndf1 = pd.read_parquet('test.parquet', engine='fastparquet')\r\ndf1.index\r\n# MultiIndex(levels=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'c'], [1, 2, 3, 1, 2, 3, 1, 2, 3]],\r\n#            labels=[[0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8]],\r\n#            names=['level_0', 'level_1'])\r\n# \r\ndf.loc['a', 1]\r\n# col    0\r\n# Name: (a, 1), dtype: int64\r\ndf1.loc['a', 1]\r\n# Traceback (most recent call last):\r\n...\r\n# TypeError: cannot do label indexing on <class 'pandas.core.indexes.base.Index'> with these indexers [1] of <class 'int'>\r\n```\r\n\r\nThe index can be fixed with\r\n```\r\ndf1.reset_index().set_index(['level_0', 'level_1'])\r\n```\r\n\r\n@martindurant\r\nGood thing you didn't release a new version last week :)\r\n\r\nI can see two ways to fix this:\r\n\r\n* simple one: remove all index logic form `empty` just treat indexes as regular columns and add a call to `set_index` in each and every single methods returning a dataframe\r\n\r\n* less simple one: use a class for the view (instead of a plain numpy array) that would take care of updating the multi-index correctly. I have no idea how this would impact the performances\r\n\r\nWhat do you think ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/348", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/348/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/348/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/348/events", "html_url": "https://github.com/dask/fastparquet/issues/348", "id": 342408288, "node_id": "MDU6SXNzdWUzNDI0MDgyODg=", "number": 348, "title": "Error while saving a big pandas dataframe", "user": {"login": "sbatururimi", "id": 1911418, "node_id": "MDQ6VXNlcjE5MTE0MTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1911418?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbatururimi", "html_url": "https://github.com/sbatururimi", "followers_url": "https://api.github.com/users/sbatururimi/followers", "following_url": "https://api.github.com/users/sbatururimi/following{/other_user}", "gists_url": "https://api.github.com/users/sbatururimi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbatururimi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbatururimi/subscriptions", "organizations_url": "https://api.github.com/users/sbatururimi/orgs", "repos_url": "https://api.github.com/users/sbatururimi/repos", "events_url": "https://api.github.com/users/sbatururimi/events{/privacy}", "received_events_url": "https://api.github.com/users/sbatururimi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-18T16:44:20Z", "updated_at": "2018-07-19T12:47:11Z", "closed_at": "2018-07-19T10:42:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to save a dataframe to disk\r\n```\r\nwrite(_filename_with_total_num_logs_sorted, df_sorted_by_logs_num, compression=\"gzip\", \r\n         row_group_offsets=20000000 )\r\n```\r\n\r\nreturns \r\n```\r\nOverflowError: int out of range\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/346", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/346/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/346/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/346/events", "html_url": "https://github.com/dask/fastparquet/issues/346", "id": 340202343, "node_id": "MDU6SXNzdWUzNDAyMDIzNDM=", "number": 346, "title": "Keeping original dtypes when a file is partitioned by some columns", "user": {"login": "andrethrill", "id": 25300892, "node_id": "MDQ6VXNlcjI1MzAwODky", "avatar_url": "https://avatars1.githubusercontent.com/u/25300892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrethrill", "html_url": "https://github.com/andrethrill", "followers_url": "https://api.github.com/users/andrethrill/followers", "following_url": "https://api.github.com/users/andrethrill/following{/other_user}", "gists_url": "https://api.github.com/users/andrethrill/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrethrill/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrethrill/subscriptions", "organizations_url": "https://api.github.com/users/andrethrill/orgs", "repos_url": "https://api.github.com/users/andrethrill/repos", "events_url": "https://api.github.com/users/andrethrill/events{/privacy}", "received_events_url": "https://api.github.com/users/andrethrill/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-07-11T11:30:48Z", "updated_at": "2018-07-12T16:20:36Z", "closed_at": "2018-07-12T16:20:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n**tldr**: if a save a parquet partitioned by some column, when I read back that same parquet, the dtype of the column used for partitioning is not kept: \r\n\r\nOriginal df dtypes:\r\n```\r\nints                  int64\r\nfloats              float64\r\nbools                 int64\r\nfake_categorical      int64\r\ndtype: object\r\n```\r\n\r\nDf dtypes read from disk:\r\n```\r\nints                   int64\r\nfloats               float64\r\nbools                  int64\r\nfake_categorical    category\r\ndtype: object\r\n```\r\n\r\nIs this the expected behaviour? Can the original dtypes be kept?\r\n\r\nThe code to reproduce this follows below:\r\n\r\n# Imports\r\n\r\n\r\n```python\r\nimport dask.dataframe as dd\r\nimport fastparquet as fp\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\nimport shutil\r\n```\r\n\r\n# Aux function\r\n\r\n\r\n```python\r\ndef generate_df(size):\r\n    d = {\r\n        'dates' : pd.date_range('1980-01-01', periods=size, freq='1T'),\r\n        'ints' : np.random.randint(0, size, size=size),\r\n        'floats' : np.random.randn(size),\r\n        'bools' : np.random.choice([0, 1], size=size),\r\n        'fake_categorical' : np.random.choice([10, 20, 30, 40, 50], size=size) ,\r\n\r\n    }\r\n\r\n    df = pd.DataFrame(d)\r\n    df.index = df.dates\r\n    df.index.name = 'dates'\r\n    df = df.drop('dates', axis=1)\r\n\r\n    return df\r\n```\r\n\r\n\r\n# generate DF\r\n\r\n\r\n```python\r\ndf = generate_df(1000)\r\n```\r\n\r\n\r\n```python\r\ndf.head()\r\n```\r\n\r\n\r\n\r\n\r\n\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>ints</th>\r\n      <th>floats</th>\r\n      <th>bools</th>\r\n      <th>fake_categorical</th>\r\n    </tr>\r\n    <tr>\r\n      <th>dates</th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>1980-01-01 00:00:00</th>\r\n      <td>722</td>\r\n      <td>2.081084</td>\r\n      <td>1</td>\r\n      <td>20</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:01:00</th>\r\n      <td>526</td>\r\n      <td>0.330207</td>\r\n      <td>1</td>\r\n      <td>50</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:02:00</th>\r\n      <td>627</td>\r\n      <td>-0.875390</td>\r\n      <td>0</td>\r\n      <td>30</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:03:00</th>\r\n      <td>217</td>\r\n      <td>0.997309</td>\r\n      <td>1</td>\r\n      <td>20</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:04:00</th>\r\n      <td>258</td>\r\n      <td>0.293013</td>\r\n      <td>0</td>\r\n      <td>40</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n</div>\r\n\r\n\r\n\r\n\r\n```python\r\ndf.shape\r\n```\r\n\r\n\r\n\r\n\r\n    (1000, 4)\r\n\r\n\r\n\r\n# Original df dtypes:\r\n\r\n\r\n```python\r\ndf.dtypes\r\n```\r\n\r\n\r\n\r\n\r\n    ints                  int64\r\n    floats              float64\r\n    bools                 int64\r\n    fake_categorical      int64\r\n    dtype: object\r\n\r\n\r\n\r\n# save to parquet\r\n\r\n\r\n```python\r\nfp.write('original_df.parquet', df, compression='snappy', file_scheme='hive', partition_on=['fake_categorical'])\r\n```\r\n\r\n# Create a copy\r\n\r\n\r\n```python\r\n!mkdir copied_df.parquet/\r\n!cp -r original_df.parquet/* copied_df.parquet/\r\n```\r\n\r\n# Load copied parquet\r\n\r\n\r\n```python\r\nfpFile = fp.ParquetFile('copied_df.parquet/')\r\n```\r\n\r\n\r\n```python\r\ndf_new = fpFile.to_pandas(filters=[('fake_categorical','==',30)])\r\n```\r\n\r\n\r\n```python\r\ndf_new.head()\r\n```\r\n\r\n\r\n\r\n\r\n<div>\r\n<style scoped>\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n</style>\r\n<table border=\"1\" class=\"dataframe\">\r\n  <thead>\r\n    <tr style=\"text-align: right;\">\r\n      <th></th>\r\n      <th>ints</th>\r\n      <th>floats</th>\r\n      <th>bools</th>\r\n      <th>fake_categorical</th>\r\n    </tr>\r\n    <tr>\r\n      <th>dates</th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n      <th></th>\r\n    </tr>\r\n  </thead>\r\n  <tbody>\r\n    <tr>\r\n      <th>1980-01-01 00:02:00</th>\r\n      <td>627</td>\r\n      <td>-0.875390</td>\r\n      <td>0</td>\r\n      <td>30</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:07:00</th>\r\n      <td>493</td>\r\n      <td>1.213180</td>\r\n      <td>1</td>\r\n      <td>30</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:08:00</th>\r\n      <td>90</td>\r\n      <td>1.400426</td>\r\n      <td>1</td>\r\n      <td>30</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:09:00</th>\r\n      <td>850</td>\r\n      <td>-1.076183</td>\r\n      <td>1</td>\r\n      <td>30</td>\r\n    </tr>\r\n    <tr>\r\n      <th>1980-01-01 00:11:00</th>\r\n      <td>399</td>\r\n      <td>-0.979044</td>\r\n      <td>0</td>\r\n      <td>30</td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n</div>\r\n\r\n\r\n\r\n\r\n```python\r\ndf_new.shape\r\n```\r\n\r\n\r\n\r\n\r\n    (177, 4)\r\n\r\n\r\n\r\n# Df dtypes read from disk:\r\n\r\n\r\n```python\r\ndf_new.dtypes\r\n```\r\n\r\n\r\n\r\n\r\n    ints                   int64\r\n    floats               float64\r\n    bools                  int64\r\n    fake_categorical    category\r\n    dtype: object\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/344", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/344/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/344/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/344/events", "html_url": "https://github.com/dask/fastparquet/issues/344", "id": 339165956, "node_id": "MDU6SXNzdWUzMzkxNjU5NTY=", "number": 344, "title": "Don't default to gzip level 9", "user": {"login": "seibert", "id": 425352, "node_id": "MDQ6VXNlcjQyNTM1Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/425352?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seibert", "html_url": "https://github.com/seibert", "followers_url": "https://api.github.com/users/seibert/followers", "following_url": "https://api.github.com/users/seibert/following{/other_user}", "gists_url": "https://api.github.com/users/seibert/gists{/gist_id}", "starred_url": "https://api.github.com/users/seibert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seibert/subscriptions", "organizations_url": "https://api.github.com/users/seibert/orgs", "repos_url": "https://api.github.com/users/seibert/repos", "events_url": "https://api.github.com/users/seibert/events{/privacy}", "received_events_url": "https://api.github.com/users/seibert/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-07T18:32:46Z", "updated_at": "2018-08-16T19:20:27Z", "closed_at": "2018-08-16T19:20:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Wondering why GZIP compression in fastparquet was so slow, I noticed that the gzip compression level is defaulted to level 9:\r\n\r\nhttps://github.com/dask/fastparquet/blob/master/fastparquet/compression.py#L16-L39\r\n\r\nGzip compression is extremely speed inefficient at level 9, and defaulting to this makes gzip very impractical.  A quick example from some parquet data I'm writing (\"create\" includes some constant time spent loading the data in memory + time for fastparquet to write):\r\n\r\n* gzip 1: create time = 64.46 sec, output size = 69 MB\r\n* gzip 6: create time =  73.94 sec, output size = 59 MB\r\n* gzip 9: create time =  229.48 sec, output size = 58 MB\r\n\r\nThe extra 2.5 minutes spent going from level 6 to level 9 is basically wasted in this case.  Setting the default compression level to 6 (matching the gzip command line utility) is probably the best option.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/343", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/343/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/343/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/343/events", "html_url": "https://github.com/dask/fastparquet/issues/343", "id": 338058454, "node_id": "MDU6SXNzdWUzMzgwNTg0NTQ=", "number": 343, "title": "Question - Save custom metadata information", "user": {"login": "guilhermecgs", "id": 8465337, "node_id": "MDQ6VXNlcjg0NjUzMzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8465337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guilhermecgs", "html_url": "https://github.com/guilhermecgs", "followers_url": "https://api.github.com/users/guilhermecgs/followers", "following_url": "https://api.github.com/users/guilhermecgs/following{/other_user}", "gists_url": "https://api.github.com/users/guilhermecgs/gists{/gist_id}", "starred_url": "https://api.github.com/users/guilhermecgs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guilhermecgs/subscriptions", "organizations_url": "https://api.github.com/users/guilhermecgs/orgs", "repos_url": "https://api.github.com/users/guilhermecgs/repos", "events_url": "https://api.github.com/users/guilhermecgs/events{/privacy}", "received_events_url": "https://api.github.com/users/guilhermecgs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-07-03T21:21:04Z", "updated_at": "2019-07-07T20:53:11Z", "closed_at": "2018-11-26T14:30:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Folks,\r\n\r\nI have a pandas dataframe that I want to save in a single parquet file.\r\n\r\nThis dataframe have some custom attributes/metadata, like ID, address, name, last name, age, that are specific to my application .\r\n\r\nIs there a clever way to persist these information alongside with the actual data?\r\n\r\nDo I need to overwrite some method to do this?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/341", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/341/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/341/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/341/events", "html_url": "https://github.com/dask/fastparquet/issues/341", "id": 332089875, "node_id": "MDU6SXNzdWUzMzIwODk4NzU=", "number": 341, "title": "Python 2.7 Unicode Error", "user": {"login": "bschreck", "id": 3237925, "node_id": "MDQ6VXNlcjMyMzc5MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3237925?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bschreck", "html_url": "https://github.com/bschreck", "followers_url": "https://api.github.com/users/bschreck/followers", "following_url": "https://api.github.com/users/bschreck/following{/other_user}", "gists_url": "https://api.github.com/users/bschreck/gists{/gist_id}", "starred_url": "https://api.github.com/users/bschreck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bschreck/subscriptions", "organizations_url": "https://api.github.com/users/bschreck/orgs", "repos_url": "https://api.github.com/users/bschreck/repos", "events_url": "https://api.github.com/users/bschreck/events{/privacy}", "received_events_url": "https://api.github.com/users/bschreck/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-13T17:23:31Z", "updated_at": "2018-06-13T19:09:22Z", "closed_at": "2018-06-13T19:09:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a dataframe with a column name that contains a non-ascii character (the accented \u00e9). When I try to save to a parquet file and then read from that file, fastparquet attempts to cast the column to an ascii string. This fails with a UnicodeEncodeError.\r\n\r\n```\r\nfrom fastparquet import ParquetFile, write\r\ndf = pd.DataFrame({u\"r\u00e9gion\": [1,2,3]})\r\nwrite(\"test.parq\", df)\r\npf = ParquetFile(\"test.parq\")\r\ndf = pf.to_pandas()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnicodeEncodeError                        Traceback (most recent call last)\r\n<ipython-input-10-e44c9491a416> in <module>()\r\n----> 1 df = pf.to_pandas()\r\n\r\n/Users/bschreck/miniconda3/envs/py2default/lib/python2.7/site-packages/fastparquet/api.pyc in to_pandas(self, columns, categories, filters, index)\r\n    386             columns.append(index)\r\n    387         check_column_names(self.columns + list(self.cats), columns, categories)\r\n--> 388         df, views = self.pre_allocate(size, columns, categories, index)\r\n    389         start = 0\r\n    390         if self.file_scheme == 'simple':\r\n\r\n/Users/bschreck/miniconda3/envs/py2default/lib/python2.7/site-packages/fastparquet/api.pyc in pre_allocate(self, size, columns, categories, index)\r\n    416                   if (c.get('metadata', {}) or {}).get('timezone', None)}\r\n    417         return _pre_allocate(size, columns, categories, index, self.cats,\r\n--> 418                              self._dtypes(categories), tz)\r\n    419\r\n    420     @property\r\n\r\n/Users/bschreck/miniconda3/envs/py2default/lib/python2.7/site-packages/fastparquet/api.pyc in _pre_allocate(size, columns, categories, index, cs, dt, tz)\r\n    507     dtypes.extend(['category'] * len(cs))\r\n    508     df, views = dataframe.empty(dtypes, size, cols=cols, index_name=index,\r\n--> 509                                 index_type=index_type, cats=cats, timezones=tz)\r\n    510     if index and re.match(r'__index_level_\\d+__', index):\r\n    511         df.index.name = None\r\n\r\n/Users/bschreck/miniconda3/envs/py2default/lib/python2.7/site-packages/fastparquet/dataframe.py in empty(types, size, cats, cols, index_type, index_name, timezones)\r\n     62             if d.dtype.kind == \"M\" and str(col) in timezones:\r\n     63                 d = Series(d).dt.tz_localize(timezones[str(col)])\r\n---> 64             df[str(col)] = d\r\n     65     df = DataFrame(df)\r\n     66\r\n\r\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\xe9' in position 1: ordinal not in range(128)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/339", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/339/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/339/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/339/events", "html_url": "https://github.com/dask/fastparquet/issues/339", "id": 327941059, "node_id": "MDU6SXNzdWUzMjc5NDEwNTk=", "number": 339, "title": "Error loading empty parquet dataframe", "user": {"login": "npezolano", "id": 1518637, "node_id": "MDQ6VXNlcjE1MTg2Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1518637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/npezolano", "html_url": "https://github.com/npezolano", "followers_url": "https://api.github.com/users/npezolano/followers", "following_url": "https://api.github.com/users/npezolano/following{/other_user}", "gists_url": "https://api.github.com/users/npezolano/gists{/gist_id}", "starred_url": "https://api.github.com/users/npezolano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/npezolano/subscriptions", "organizations_url": "https://api.github.com/users/npezolano/orgs", "repos_url": "https://api.github.com/users/npezolano/repos", "events_url": "https://api.github.com/users/npezolano/events{/privacy}", "received_events_url": "https://api.github.com/users/npezolano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-31T00:01:22Z", "updated_at": "2018-06-05T14:11:51Z", "closed_at": "2018-06-05T14:11:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Fastparquet throws an error when loading an empty parquet pandas dataframe, this is different behaviour from pyarrow. Please see the example below:\r\n\r\n```\r\nimport pandas as pd\r\nimport fastparquet as fp\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\ndf = pd.DataFrame(columns=['a', 'b', 'c'])\r\ntable = pa.Table.from_pandas(df)\r\npq.write_table(table, 'test.parquet')\r\n\r\n#Reads correctly\r\ntable2 = pq.read_table('test.parquet')\r\ndf2 = table2.to_pandas()\r\n\r\n#Throws an error\r\nfp.ParquetFile('test.parquet')\r\n```\r\n\r\nThe error:\r\n\r\n----> 1 fp.ParquetFile('test.parquet')\r\n\r\n```\r\n/home/n/anaconda2/lib/python2.7/site-packages/fastparquet/api.pyc in __init__(self, fn, verify, open_with, root, sep)\r\n     98                 self.fn = join_path(fn)\r\n     99                 with open_with(fn, 'rb') as f:\r\n--> 100                     self._parse_header(f, verify)\r\n    101         self.open = open_with\r\n    102         self.sep = sep\r\n\r\n/home/n/anaconda2/lib/python2.7/site-packages/fastparquet/api.pyc in _parse_header(self, f, verify)\r\n    122         self.head_size = head_size\r\n    123         self.fmd = fmd\r\n--> 124         self._set_attrs()\r\n    125 \r\n    126     def _set_attrs(self):\r\n\r\n/home/n/anaconda2/lib/python2.7/site-packages/fastparquet/api.pyc in _set_attrs(self)\r\n    136             for chunk in rg.columns:\r\n    137                 self.group_files.setdefault(i, set()).add(chunk.file_path)\r\n--> 138         self.schema = schema.SchemaHelper(self._schema)\r\n    139         self.selfmade = self.created_by.split(' ', 1)[0] == \"fastparquet-python\"\r\n    140         self.file_scheme = get_file_scheme([rg.columns[0].file_path\r\n\r\n/home/n/anaconda2/lib/python2.7/site-packages/fastparquet/schema.pyc in __init__(self, schema_elements)\r\n     78             [(se.name, se) for se in schema_elements])\r\n     79         schema_tree(schema_elements)\r\n---> 80         self.text = schema_to_text(self.schema_elements[0])\r\n     81         flatten(self.root, self.root)\r\n     82 \r\n\r\n/home/n/anaconda2/lib/python2.7/site-packages/fastparquet/schema.pyc in schema_to_text(root, indent)\r\n     45             if i == len(root.children) - 1:\r\n     46                 indent[-1] = '  '\r\n---> 47             text += '\\n' + schema_to_text(child, indent)\r\n     48     indent.pop()\r\n     49     return text\r\n\r\n/home/n/anaconda2/lib/python2.7/site-packages/fastparquet/schema.pyc in schema_to_text(root, indent)\r\n     34     if root.converted_type is not None:\r\n     35         parts.append(parquet_thrift.ConvertedType._VALUES_TO_NAMES[\r\n---> 36                          root.converted_type])\r\n     37     if root.repetition_type is not None:\r\n     38         parts.append(parquet_thrift.FieldRepetitionType._VALUES_TO_NAMES[\r\n\r\nKeyError: 24\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/338", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/338/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/338/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/338/events", "html_url": "https://github.com/dask/fastparquet/issues/338", "id": 324864701, "node_id": "MDU6SXNzdWUzMjQ4NjQ3MDE=", "number": 338, "title": "Int out of range error even if all columns are string", "user": {"login": "shantanuo", "id": 222800, "node_id": "MDQ6VXNlcjIyMjgwMA==", "avatar_url": "https://avatars2.githubusercontent.com/u/222800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shantanuo", "html_url": "https://github.com/shantanuo", "followers_url": "https://api.github.com/users/shantanuo/followers", "following_url": "https://api.github.com/users/shantanuo/following{/other_user}", "gists_url": "https://api.github.com/users/shantanuo/gists{/gist_id}", "starred_url": "https://api.github.com/users/shantanuo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shantanuo/subscriptions", "organizations_url": "https://api.github.com/users/shantanuo/orgs", "repos_url": "https://api.github.com/users/shantanuo/repos", "events_url": "https://api.github.com/users/shantanuo/events{/privacy}", "received_events_url": "https://api.github.com/users/shantanuo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-21T10:23:30Z", "updated_at": "2018-05-23T13:01:35Z", "closed_at": "2018-05-23T13:01:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "The same command works for 1 million, but fails when I choose entire dataframe.\r\n\r\n> write('urls_1stFeb2018_31stMay2018z2.parq', df.head(100000000), object_encoding = 'utf8' )\r\n\r\nI have checked that all columns are \"object\" and I tried to use object_encoding.\r\nI am still getting error: \"OverflowError: int out of range\"\r\nI am using latest pandas release 0.23\r\n\r\n```\r\n\r\nOverflowError                             Traceback (most recent call last)\r\n<ipython-input-49-90c34a121eba> in <module>()\r\n----> 1 write('urls_1stFeb2018_31stMay2018z2.parq', df.head(100000000), object_encoding = 'utf8' )\r\n\r\n/opt/conda/lib/python3.6/site-packages/fastparquet/writer.py in write(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times)\r\n    846     if file_scheme == 'simple':\r\n    847         write_simple(filename, data, fmd, row_group_offsets,\r\n--> 848                      compression, open_with, has_nulls, append)\r\n    849     elif file_scheme in ['hive', 'drill']:\r\n    850         if append:\r\n\r\n/opt/conda/lib/python3.6/site-packages/fastparquet/writer.py in write_simple(fn, data, fmd, row_group_offsets, compression, open_with, has_nulls, append)\r\n    715                    else None)\r\n    716             rg = make_row_group(f, data[start:end], fmd.schema,\r\n--> 717                                 compression=compression)\r\n    718             if rg is not None:\r\n    719                 fmd.row_groups.append(rg)\r\n\r\n/opt/conda/lib/python3.6/site-packages/fastparquet/writer.py in make_row_group(f, data, schema, compression)\r\n    612                 comp = compression\r\n    613             chunk = write_column(f, data[column.name], column,\r\n--> 614                                  compression=comp)\r\n    615             rg.columns.append(chunk)\r\n    616     rg.total_byte_size = sum([c.meta_data.total_uncompressed_size for c in\r\n\r\n/opt/conda/lib/python3.6/site-packages/fastparquet/writer.py in write_column(f, data, selement, compression)\r\n    545                                    data_page_header=dph, crc=None)\r\n    546 \r\n--> 547     write_thrift(f, ph)\r\n    548     f.write(bdata)\r\n    549 \r\n\r\n/opt/conda/lib/python3.6/site-packages/fastparquet/thrift_structures.py in write_thrift(fobj, thrift)\r\n     49     pout = TCompactProtocol(fobj)\r\n     50     try:\r\n---> 51         thrift.write(pout)\r\n     52         fail = False\r\n     53     except TProtocolException as e:\r\n\r\n/opt/conda/lib/python3.6/site-packages/fastparquet/parquet_thrift/parquet/ttypes.py in write(self, oprot)\r\n   1028     def write(self, oprot):\r\n   1029         if oprot._fast_encode is not None and self.thrift_spec is not None:\r\n-> 1030             oprot.trans.write(oprot._fast_encode(self, [self.__class__, self.thrift_spec]))\r\n   1031             return\r\n   1032         oprot.writeStructBegin('PageHeader')\r\n\r\nOverflowError: int out of range\r\n\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/337", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/337/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/337/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/337/events", "html_url": "https://github.com/dask/fastparquet/issues/337", "id": 324496383, "node_id": "MDU6SXNzdWUzMjQ0OTYzODM=", "number": 337, "title": "ENH: parallel loading", "user": {"login": "martindurant", "id": 6042212, "node_id": "MDQ6VXNlcjYwNDIyMTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6042212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martindurant", "html_url": "https://github.com/martindurant", "followers_url": "https://api.github.com/users/martindurant/followers", "following_url": "https://api.github.com/users/martindurant/following{/other_user}", "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}", "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions", "organizations_url": "https://api.github.com/users/martindurant/orgs", "repos_url": "https://api.github.com/users/martindurant/repos", "events_url": "https://api.github.com/users/martindurant/events{/privacy}", "received_events_url": "https://api.github.com/users/martindurant/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-05-18T17:42:53Z", "updated_at": "2018-05-19T17:10:15Z", "closed_at": "2018-05-19T17:10:15Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Each row-group is loaded independently, so once the memory is allocated, could easily parallelise with threads (nb: much of the actual computation is numpy, cython and numba, should not hold GIL).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/335", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/335/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/335/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/335/events", "html_url": "https://github.com/dask/fastparquet/issues/335", "id": 323668478, "node_id": "MDU6SXNzdWUzMjM2Njg0Nzg=", "number": 335, "title": "Hive-partitioned parquet files are broken", "user": {"login": "Spacerat", "id": 141427, "node_id": "MDQ6VXNlcjE0MTQyNw==", "avatar_url": "https://avatars3.githubusercontent.com/u/141427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Spacerat", "html_url": "https://github.com/Spacerat", "followers_url": "https://api.github.com/users/Spacerat/followers", "following_url": "https://api.github.com/users/Spacerat/following{/other_user}", "gists_url": "https://api.github.com/users/Spacerat/gists{/gist_id}", "starred_url": "https://api.github.com/users/Spacerat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Spacerat/subscriptions", "organizations_url": "https://api.github.com/users/Spacerat/orgs", "repos_url": "https://api.github.com/users/Spacerat/repos", "events_url": "https://api.github.com/users/Spacerat/events{/privacy}", "received_events_url": "https://api.github.com/users/Spacerat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-05-16T15:18:49Z", "updated_at": "2018-05-18T17:43:18Z", "closed_at": "2018-05-18T17:43:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Due to this line (I think!): https://github.com/dask/fastparquet/blob/master/fastparquet/core.py#L347\r\n\r\nThe following code:\r\n\r\n```py\r\nimport fastparquet\r\nimport pandas as pd\r\n\r\n\r\nfastparquet.write('test.parquet', pd.DataFrame({\r\n    'literal': ['40+2', '1e-10', '\"5\"', \"2018-10-09\", \"2018-10-10\"],\r\n    'idx': [1, 2, 3, 4, 5]\r\n}), partition_on=['literal'], file_scheme='hive')\r\n\r\nfastparquet.ParquetFile('test.parquet').to_pandas()\r\n```\r\n\r\nproduces the following output\r\n\r\n<img width=\"187\" alt=\"screen shot 2018-05-15 at 9 56 13 pm\" src=\"https://user-images.githubusercontent.com/141427/40097402-cdab11e4-588a-11e8-884e-3665d6c50b59.png\">\r\n\r\nI can see that this has been done so that the after the dataframe has been read back in, the underlying values of the categorical column generated for the partition key are (hopefully likely to be) the type you expect. \r\n\r\nSince the column ends up as a categorical anyway, does it really matter if it\u2019s backed by strings or integers/dates/etc? Or: since the data doesn\u2019t actually exist as typed columns in the parquet files themselves, does it make sense to try to simulate that?\r\n\r\nMy answers to the above two questions would be no, but if you disagree then I think the `val_to_num` function needs to be made a bit more robust; e.g. by parsing things as dates first and parsing literals in a more deliberate way (or at the very least, white or blacklisting certain characters before parsing)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/334", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/334/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/334/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/334/events", "html_url": "https://github.com/dask/fastparquet/issues/334", "id": 321539025, "node_id": "MDU6SXNzdWUzMjE1MzkwMjU=", "number": 334, "title": "writing partitioned parquet files to s3 via AWS lambda gives a FileNotFoundError", "user": {"login": "vc019", "id": 22635891, "node_id": "MDQ6VXNlcjIyNjM1ODkx", "avatar_url": "https://avatars3.githubusercontent.com/u/22635891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vc019", "html_url": "https://github.com/vc019", "followers_url": "https://api.github.com/users/vc019/followers", "following_url": "https://api.github.com/users/vc019/following{/other_user}", "gists_url": "https://api.github.com/users/vc019/gists{/gist_id}", "starred_url": "https://api.github.com/users/vc019/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vc019/subscriptions", "organizations_url": "https://api.github.com/users/vc019/orgs", "repos_url": "https://api.github.com/users/vc019/repos", "events_url": "https://api.github.com/users/vc019/events{/privacy}", "received_events_url": "https://api.github.com/users/vc019/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-05-09T11:50:00Z", "updated_at": "2018-05-09T15:15:24Z", "closed_at": "2018-05-09T15:15:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI am getting an error when I am trying to write partitioned parquet files when the logic is enclosed in AWS Lambda. The same implmentation works fine on a local or an ec2 instance. Below are the error logs.  I am yet to look at writer.py. But my guess is that it is trying to create a local file structure before pushing the files to s3. For now I will leave it to experts . :)\r\n\r\nThanks\r\n\r\nV\r\n\r\n{\r\n  \"stackTrace\": [\r\n    [\r\n      \"/var/task/lambda.py\",\r\n      18,\r\n      \"lambda_handler\",\r\n      \"write('s3://vipin-parquet/orders/', df, open_with=fh, partition_on=['year', 'month', 'day'], file_scheme='hive',append=True)\"\r\n    ],\r\n    [\r\n      \"/tmp/pip-install-h1nhEU/fastparquet/fastparquet/writer.py\",\r\n      851,\r\n      \"write\",\r\n      null\r\n    ],\r\n    [\r\n      \"/tmp/pip-install-h1nhEU/fastparquet/fastparquet/api.py\",\r\n      99,\r\n      \"__init__\",\r\n      null\r\n    ],\r\n    [\r\n      \"/tmp/pip-install-ekrfd6/s3fs/s3fs/core.py\",\r\n      315,\r\n      \"open\",\r\n      null\r\n    ],\r\n    [\r\n      \"/tmp/pip-install-ekrfd6/s3fs/s3fs/core.py\",\r\n      1102,\r\n      \"__init__\",\r\n      null\r\n    ],\r\n    [\r\n      \"/tmp/pip-install-ekrfd6/s3fs/s3fs/core.py\",\r\n      1120,\r\n      \"info\",\r\n      null\r\n    ],\r\n    [\r\n      \"/tmp/pip-install-ekrfd6/s3fs/s3fs/core.py\",\r\n      455,\r\n      \"info\",\r\n      null\r\n    ]\r\n  ],\r\n  \"errorType\": \"FileNotFoundError\",\r\n  \"errorMessage\": \"vipin-parquet/orders/\"\r\n}", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/333", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/333/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/333/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/333/events", "html_url": "https://github.com/dask/fastparquet/issues/333", "id": 319802846, "node_id": "MDU6SXNzdWUzMTk4MDI4NDY=", "number": 333, "title": "TIMESTAMP_MILLIS support", "user": {"login": "Spacerat", "id": 141427, "node_id": "MDQ6VXNlcjE0MTQyNw==", "avatar_url": "https://avatars3.githubusercontent.com/u/141427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Spacerat", "html_url": "https://github.com/Spacerat", "followers_url": "https://api.github.com/users/Spacerat/followers", "following_url": "https://api.github.com/users/Spacerat/following{/other_user}", "gists_url": "https://api.github.com/users/Spacerat/gists{/gist_id}", "starred_url": "https://api.github.com/users/Spacerat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Spacerat/subscriptions", "organizations_url": "https://api.github.com/users/Spacerat/orgs", "repos_url": "https://api.github.com/users/Spacerat/repos", "events_url": "https://api.github.com/users/Spacerat/events{/privacy}", "received_events_url": "https://api.github.com/users/Spacerat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-03T06:40:26Z", "updated_at": "2019-06-06T16:37:13Z", "closed_at": "2019-06-06T16:37:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi!\r\n\r\nI'm considering adding support to fastparquet for the Parquet TIMESTAMP_MILLIS type. The motivation is to support Amazon Athena (i.e. Presto), which assumes that parquet int64 timestamps are milliseconds and thus reads fastparquet timestamps as being far in the future.\r\n\r\nIt is actually possible to get round the issue using int96, but given that int96 is supposed to be deprecated, it would be nice to support int64/millis.\r\n\r\nBefore I try it I just want to:\r\na) check with you that you would actually accept the diff, and that you haven't limited writes to MICROS for any specific reason\r\nb) confirm that the approach I'm considering would be correct\r\n\r\nIf you would accept a diff adding MILLIS, then as far as I can tell, the following changes would be required:\r\n- A new option for `times` [here](https://github.com/dask/fastparquet/blob/4b5b3e4e866c924e7f219f7b742215c3a78324a4/fastparquet/writer.py#L132 ) which accepts something like 'int64ms' and outputs a `TIMESTAMP_MILLIS`\r\n- A new condition [here](https://github.com/dask/fastparquet/blob/4b5b3e4e866c924e7f219f7b742215c3a78324a4/fastparquet/writer.py#L198) which checks for `TIMESTAMP_MILLIS` and calls `time_shift` with `factor=1000000`\r\n- Documentation updates & tests\r\n\r\nWhat do you think?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/332", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/332/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/332/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/332/events", "html_url": "https://github.com/dask/fastparquet/issues/332", "id": 319695150, "node_id": "MDU6SXNzdWUzMTk2OTUxNTA=", "number": 332, "title": "pip install fails in python 3.6 virtual environment", "user": {"login": "hugomailhot", "id": 5240492, "node_id": "MDQ6VXNlcjUyNDA0OTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/5240492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hugomailhot", "html_url": "https://github.com/hugomailhot", "followers_url": "https://api.github.com/users/hugomailhot/followers", "following_url": "https://api.github.com/users/hugomailhot/following{/other_user}", "gists_url": "https://api.github.com/users/hugomailhot/gists{/gist_id}", "starred_url": "https://api.github.com/users/hugomailhot/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hugomailhot/subscriptions", "organizations_url": "https://api.github.com/users/hugomailhot/orgs", "repos_url": "https://api.github.com/users/hugomailhot/repos", "events_url": "https://api.github.com/users/hugomailhot/events{/privacy}", "received_events_url": "https://api.github.com/users/hugomailhot/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-02T20:11:33Z", "updated_at": "2018-08-14T13:07:59Z", "closed_at": "2018-05-02T20:51:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Trying to install fastparquet using pipenv and Python 3.6.\r\n\r\n**What I do**\r\n\r\n```\r\n$ pipenv install --python 3.6\r\n$ pipenv shell\r\n(test36-gubVLC04) $ pip install fastparquet\r\n```\r\n\r\n**What I get**\r\n\r\nhttps://pastebin.com/QKvrp3Bp\r\n\r\n**This seems specific to Python 3.6**\r\n\r\nThe following works without any problem\r\n\r\n```\r\n$ pipenv install --python 3.5\r\n$ pipenv shell\r\n(test35-8PQYERv4) $ pip install fastparquet\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/329", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/329/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/329/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/329/events", "html_url": "https://github.com/dask/fastparquet/issues/329", "id": 318401588, "node_id": "MDU6SXNzdWUzMTg0MDE1ODg=", "number": 329, "title": "read parquet from byte array", "user": {"login": "virtualluke", "id": 19656246, "node_id": "MDQ6VXNlcjE5NjU2MjQ2", "avatar_url": "https://avatars2.githubusercontent.com/u/19656246?v=4", "gravatar_id": "", "url": "https://api.github.com/users/virtualluke", "html_url": "https://github.com/virtualluke", "followers_url": "https://api.github.com/users/virtualluke/followers", "following_url": "https://api.github.com/users/virtualluke/following{/other_user}", "gists_url": "https://api.github.com/users/virtualluke/gists{/gist_id}", "starred_url": "https://api.github.com/users/virtualluke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/virtualluke/subscriptions", "organizations_url": "https://api.github.com/users/virtualluke/orgs", "repos_url": "https://api.github.com/users/virtualluke/repos", "events_url": "https://api.github.com/users/virtualluke/events{/privacy}", "received_events_url": "https://api.github.com/users/virtualluke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-04-27T13:03:47Z", "updated_at": "2018-04-27T21:19:16Z", "closed_at": "2018-04-27T14:26:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am in a function receiving a byte array which happens to be a self-contained parquet object from S3 and would like to read it using fastparquet.  I am being passed the data as a byte array.  \r\n\r\nIs there a way to do this?  I see the example using s3fs but I am in a function where the overhead of adding an s3fs call is more than I want since I already have access to the data as a byte array.  I tried reading it as fastparquet.ParquetFile(io.BytesIO(byte_array)) but get the error: \"typeError: '_io.BytesIO' object is not subscriptable\" (when is is checking path[0][0]=='/').\r\n\r\nthoughts?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/327", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/327/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/327/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/327/events", "html_url": "https://github.com/dask/fastparquet/issues/327", "id": 314893566, "node_id": "MDU6SXNzdWUzMTQ4OTM1NjY=", "number": 327, "title": "Appending parquet file from python to s3", "user": {"login": "Jeeva-Ganesan", "id": 28073830, "node_id": "MDQ6VXNlcjI4MDczODMw", "avatar_url": "https://avatars1.githubusercontent.com/u/28073830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jeeva-Ganesan", "html_url": "https://github.com/Jeeva-Ganesan", "followers_url": "https://api.github.com/users/Jeeva-Ganesan/followers", "following_url": "https://api.github.com/users/Jeeva-Ganesan/following{/other_user}", "gists_url": "https://api.github.com/users/Jeeva-Ganesan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jeeva-Ganesan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jeeva-Ganesan/subscriptions", "organizations_url": "https://api.github.com/users/Jeeva-Ganesan/orgs", "repos_url": "https://api.github.com/users/Jeeva-Ganesan/repos", "events_url": "https://api.github.com/users/Jeeva-Ganesan/events{/privacy}", "received_events_url": "https://api.github.com/users/Jeeva-Ganesan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-04-17T03:38:10Z", "updated_at": "2018-04-19T13:42:56Z", "closed_at": "2018-04-19T13:42:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Here is my snippet in spark-shell\r\n\r\n`jdbcDF.write.mode(\"append\").partitionBy(\"date\").parquet(\"s3://bucket/Data/\")`\r\n\r\nProblem description\r\n\r\nNow, i am trying to do the same thing in python with fastparquet. \r\n\r\n```\r\nimport s3fs\r\nfrom fastparquet import write\r\ns3 = s3fs.S3FileSystem()\r\nmyopen = s3.open\r\nwrite('****/20180101.parq', data, compression='GZIP', open_with=myopen)\r\n```\r\n\r\nFirst thing, I tried to save as snappy compression, \r\n`write('****/20180101.snappy.parquet', data, compression='SNAPPY', open_with=myopen)`\r\n\r\nbut got error, \r\n> Compression 'SNAPPY' not available.  Options: ['GZIP', 'UNCOMPRESSED']\r\n\r\nThen, tried to use GZIP, it worked, but not sure how I can append or create partition here. Here is an issue I created in pandas. [https://github.com/pandas-dev/pandas/issues/20638](url)\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/326", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/326/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/326/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/326/events", "html_url": "https://github.com/dask/fastparquet/issues/326", "id": 313004173, "node_id": "MDU6SXNzdWUzMTMwMDQxNzM=", "number": 326, "title": "IndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 3", "user": {"login": "sam-reh-hs", "id": 20913211, "node_id": "MDQ6VXNlcjIwOTEzMjEx", "avatar_url": "https://avatars1.githubusercontent.com/u/20913211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sam-reh-hs", "html_url": "https://github.com/sam-reh-hs", "followers_url": "https://api.github.com/users/sam-reh-hs/followers", "following_url": "https://api.github.com/users/sam-reh-hs/following{/other_user}", "gists_url": "https://api.github.com/users/sam-reh-hs/gists{/gist_id}", "starred_url": "https://api.github.com/users/sam-reh-hs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sam-reh-hs/subscriptions", "organizations_url": "https://api.github.com/users/sam-reh-hs/orgs", "repos_url": "https://api.github.com/users/sam-reh-hs/repos", "events_url": "https://api.github.com/users/sam-reh-hs/events{/privacy}", "received_events_url": "https://api.github.com/users/sam-reh-hs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-04-10T16:43:11Z", "updated_at": "2018-04-12T00:04:17Z", "closed_at": "2018-04-10T20:14:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting an unexpected error when reading in a list of strings:\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-22-8ddd76dd0792> in <module>()\r\n      3 path ='3489803881346866476.parquet'\r\n      4 ## need to have fastparquet and python-snappy installed to make this work.\r\n----> 5 df = pd.read_parquet(path,engine='fastparquet')\r\n\r\n/anaconda2/lib/python2.7/site-packages/pandas/io/parquet.pyc in read_parquet(path, engine, columns, **kwargs)\r\n    255 \r\n    256     impl = get_engine(engine)\r\n--> 257     return impl.read(path, columns=columns, **kwargs)\r\n\r\n/anaconda2/lib/python2.7/site-packages/pandas/io/parquet.pyc in read(self, path, columns, **kwargs)\r\n    203         path, _, _ = get_filepath_or_buffer(path)\r\n    204         parquet_file = self.api.ParquetFile(path)\r\n--> 205         return parquet_file.to_pandas(columns=columns, **kwargs)\r\n    206 \r\n    207 \r\n\r\n/anaconda2/lib/python2.7/site-packages/fastparquet/api.pyc in to_pandas(self, columns, categories, filters, index)\r\n    395                              for (name, v) in views.items()}\r\n    396                     self.read_row_group(rg, columns, categories, infile=f,\r\n--> 397                                         index=index, assign=parts)\r\n    398                     start += rg.num_rows\r\n    399         else:\r\n\r\n/anaconda2/lib/python2.7/site-packages/fastparquet/api.pyc in read_row_group(self, rg, columns, categories, infile, index, assign)\r\n    222                 infile, rg, columns, categories, self.schema, self.cats,\r\n    223                 self.selfmade, index=index, assign=assign,\r\n--> 224                 scheme=self.file_scheme)\r\n    225         if ret:\r\n    226             return df\r\n\r\n/anaconda2/lib/python2.7/site-packages/fastparquet/core.pyc in read_row_group(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme)\r\n    336     \"\"\"\r\n    337     if assign is None:\r\n--> 338         raise RuntimeError('Going with pre-allocation!')\r\n    339     read_row_group_arrays(file, rg, columns, categories, schema_helper,\r\n    340                           cats, selfmade, assign=assign)\r\n\r\n/anaconda2/lib/python2.7/site-packages/fastparquet/core.pyc in read_row_group_arrays(file, rg, columns, categories, schema_helper, cats, selfmade, assign)\r\n    313 \r\n    314         use = name in categories if categories is not None else False\r\n--> 315         read_col(column, schema_helper, file, use_cat=use,\r\n    316                  selfmade=selfmade, assign=out[name],\r\n    317                  catdef=out[name+'-catdef'] if use else None)\r\n\r\n/anaconda2/lib/python2.7/site-packages/fastparquet/core.pyc in read_col(column, schema_helper, infile, use_cat, grab_dict, selfmade, assign, catdef)\r\n    258             max_defi = schema_helper.max_definition_level(cmd.path_in_schema)\r\n    259             part = assign[num:num+len(defi)]\r\n--> 260             part[defi != max_defi] = my_nan\r\n    261             if d and not use_cat:\r\n    262                 part[defi == max_defi] = dic[val]\r\n\r\nIndexError: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 3\r\n```\r\n\r\nRunning:\r\nMac OS X: 10.12.6\r\nPython: 2.7.14\r\nFastParquet: 0.1.5\r\n\r\nHere's the schema I'm attempting to load:\r\n```\r\nmessage Msg {\r\n  optional binary sn_id (UTF8);\r\n  optional binary sn_name (UTF8);\r\n  optional binary sn_type (UTF8);\r\n  optional binary author_id (UTF8);\r\n  optional binary author_name (UTF8);\r\n  optional binary sn_msg_id (UTF8);\r\n  optional binary sn_msg_type (UTF8);\r\n  optional int64 sent_ts;\r\n  optional binary text (UTF8);\r\n  repeated binary tag_ids (UTF8);\r\n  repeated binary tag_names (UTF8);\r\n  repeated binary tag_descriptions (UTF8);\r\n}\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/323", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/323/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/323/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/323/events", "html_url": "https://github.com/dask/fastparquet/issues/323", "id": 309016158, "node_id": "MDU6SXNzdWUzMDkwMTYxNTg=", "number": 323, "title": "fastparquet fails to parse parquet files with column containing a number list", "user": {"login": "marcusklaas", "id": 1255413, "node_id": "MDQ6VXNlcjEyNTU0MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1255413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marcusklaas", "html_url": "https://github.com/marcusklaas", "followers_url": "https://api.github.com/users/marcusklaas/followers", "following_url": "https://api.github.com/users/marcusklaas/following{/other_user}", "gists_url": "https://api.github.com/users/marcusklaas/gists{/gist_id}", "starred_url": "https://api.github.com/users/marcusklaas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marcusklaas/subscriptions", "organizations_url": "https://api.github.com/users/marcusklaas/orgs", "repos_url": "https://api.github.com/users/marcusklaas/repos", "events_url": "https://api.github.com/users/marcusklaas/events{/privacy}", "received_events_url": "https://api.github.com/users/marcusklaas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-27T15:19:55Z", "updated_at": "2018-03-27T15:36:33Z", "closed_at": "2018-03-27T15:35:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Fastparquet seems to have trouble parsing parquet files into a pandas Dataframe when the parquet files have the following schema:\r\n```\r\n- spark_schema:\r\n| - _1: INT32, REQUIRED\r\n  - _2: LIST, OPTIONAL\r\n    - list: REPEATED\r\n      - element: DOUBLE, REQUIRED\r\n```\r\nIt results in the following stacktrace on my machine:\r\n```\r\nTraceback (most recent call last):\r\n  File \"app.py\", line 55, in <module>\r\n    pf = print(ParquetFile(['spark-model/1.snappy.parquet', 'spark-model/2.snappy.parquet', 'spark-model/3.snappy.parquet']).to_pandas(index=False))\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/api.py\", line 405, in to_pandas\r\n    assign=parts)\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/api.py\", line 205, in read_row_group_file\r\n    assign=assign, scheme=self.file_scheme)\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/core.py\", line 284, in read_row_group_file\r\n    scheme=scheme)\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/core.py\", line 334, in read_row_group\r\n    cats, selfmade, assign=assign)\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/core.py\", line 311, in read_row_group_arrays\r\n    catdef=out[name+'-catdef'] if use else None)\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/core.py\", line 235, in read_col\r\n    skip_nulls, selfmade=selfmade)\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/core.py\", line 103, in read_data_page\r\n    repetition_levels = read_rep(io_obj, daph, helper, metadata)\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/core.py\", line 85, in read_rep\r\n    bit_width)[:daph.num_values]\r\n  File \"/home/marcus/anaconda3/lib/python3.6/site-packages/fastparquet/core.py\", line 46, in read_data\r\n    raise NotImplementedError('Encoding %s' % coding)\r\nNotImplementedError: Encoding 4\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/321", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/321/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/321/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/321/events", "html_url": "https://github.com/dask/fastparquet/issues/321", "id": 308498497, "node_id": "MDU6SXNzdWUzMDg0OTg0OTc=", "number": 321, "title": "Spark cannot read files with converted type TIMESTAMP_MICROS", "user": {"login": "bartek-u", "id": 30018174, "node_id": "MDQ6VXNlcjMwMDE4MTc0", "avatar_url": "https://avatars2.githubusercontent.com/u/30018174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bartek-u", "html_url": "https://github.com/bartek-u", "followers_url": "https://api.github.com/users/bartek-u/followers", "following_url": "https://api.github.com/users/bartek-u/following{/other_user}", "gists_url": "https://api.github.com/users/bartek-u/gists{/gist_id}", "starred_url": "https://api.github.com/users/bartek-u/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bartek-u/subscriptions", "organizations_url": "https://api.github.com/users/bartek-u/orgs", "repos_url": "https://api.github.com/users/bartek-u/repos", "events_url": "https://api.github.com/users/bartek-u/events{/privacy}", "received_events_url": "https://api.github.com/users/bartek-u/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-03-26T09:36:33Z", "updated_at": "2018-03-26T12:29:54Z", "closed_at": "2018-03-26T12:29:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nFastparquet produces files with converted type TIMESTAMP_MICROS. This cannot be read in Spark. Can we add support for writing TIMESTAMP_MILLIS, please?\r\n\r\nKind Regards,\r\nBartek", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/319", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/319/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/319/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/319/events", "html_url": "https://github.com/dask/fastparquet/issues/319", "id": 307804954, "node_id": "MDU6SXNzdWUzMDc4MDQ5NTQ=", "number": 319, "title": "manylinux wheel on pypi?", "user": {"login": "erikcw", "id": 113129, "node_id": "MDQ6VXNlcjExMzEyOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/113129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erikcw", "html_url": "https://github.com/erikcw", "followers_url": "https://api.github.com/users/erikcw/followers", "following_url": "https://api.github.com/users/erikcw/following{/other_user}", "gists_url": "https://api.github.com/users/erikcw/gists{/gist_id}", "starred_url": "https://api.github.com/users/erikcw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erikcw/subscriptions", "organizations_url": "https://api.github.com/users/erikcw/orgs", "repos_url": "https://api.github.com/users/erikcw/repos", "events_url": "https://api.github.com/users/erikcw/events{/privacy}", "received_events_url": "https://api.github.com/users/erikcw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-03-22T20:36:02Z", "updated_at": "2018-12-13T10:06:45Z", "closed_at": "2018-11-26T14:30:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm working on porting a project that uses fastparquet over to AWS Lambda (using [Zappa](https://github.com/Miserlou/Zappa)).\r\n\r\nIt's quite difficult to deploy fastparquet in this situation because of the cython compilation step which AWS doesn't support out of the box.  Seems like the community is converging on \"[manylinux](https://github.com/pypa/manylinux)\" wheels for this type of scenario which work quite well.\r\n\r\nI see that there is already a Windows wheel available on pypi -- would it be possible to include manylinux wheels in the release process?\r\n\r\n```\r\n[1521750090013] No module named 'fastparquet.speedups': ModuleNotFoundError\r\nTraceback (most recent call last):\r\n  File \"/var/task/handler.py\", line 509, in lambda_handler\r\n  return LambdaHandler.lambda_handler(event, context)\r\n  File \"/var/task/handler.py\", line 240, in lambda_handler\r\n  return handler.handler(event, context)\r\n  File \"/var/task/handler.py\", line 386, in handler\r\n  app_function = self.import_module_and_get_function(whole_function)\r\n  File \"/var/task/handler.py\", line 231, in import_module_and_get_function\r\n  app_module = importlib.import_module(module)\r\n  File \"/var/lang/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n  return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 655, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\n  File \"/tmp/convert-to-parq/app.py\", line 25, in <module>\r\n  from parquet import write_to_parquet\r\n  File \"/tmp/convert-to-parq/parquet.py\", line 1, in <module>\r\n  import fastparquet\r\n  File \"/tmp/convert-to-parq/fastparquet/__init__.py\", line 8, in <module>\r\n  from .core import read_thrift\r\n  File \"/tmp/convert-to-parq/fastparquet/core.py\", line 13, in <module>\r\n  from . import encoding\r\n  File \"/tmp/convert-to-parq/fastparquet/encoding.py\", line 11, in <module>\r\n  from .speedups import unpack_byte_array\r\nModuleNotFoundError: No module named 'fastparquet.speedups'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/317", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/317/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/317/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/317/events", "html_url": "https://github.com/dask/fastparquet/issues/317", "id": 306953328, "node_id": "MDU6SXNzdWUzMDY5NTMzMjg=", "number": 317, "title": "Slow initialization with large numbers of columns", "user": {"login": "timothydmorton", "id": 1895387, "node_id": "MDQ6VXNlcjE4OTUzODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1895387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timothydmorton", "html_url": "https://github.com/timothydmorton", "followers_url": "https://api.github.com/users/timothydmorton/followers", "following_url": "https://api.github.com/users/timothydmorton/following{/other_user}", "gists_url": "https://api.github.com/users/timothydmorton/gists{/gist_id}", "starred_url": "https://api.github.com/users/timothydmorton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timothydmorton/subscriptions", "organizations_url": "https://api.github.com/users/timothydmorton/orgs", "repos_url": "https://api.github.com/users/timothydmorton/repos", "events_url": "https://api.github.com/users/timothydmorton/events{/privacy}", "received_events_url": "https://api.github.com/users/timothydmorton/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-03-20T16:57:04Z", "updated_at": "2018-03-20T23:59:42Z", "closed_at": "2018-03-20T23:59:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am finding that a particular type of file, containing about 10k columns and written by fastparquet, takes inordinately long to be read by fastparquet, but not so with pyarrow.  As I am unable to reproduce this with synthetic data, I have posted a demo file (only 5 rows, but with the same structure otherwise) [here](http://lsst-web.ncsa.illinois.edu/~tmorton/fastparquet_bug/test_head.parq).\r\n\r\n```\r\nimport fastparquet\r\nfile = 'test_head.parq'\r\n%time pf = fastparquet.ParquetFile(file)\r\nCPU times: user 11.3 s, sys: 20.5 ms, total: 11.3 s\r\nWall time: 11.3 s\r\n```\r\nAnd then, after that, even worse to dump to pandas:\r\n```\r\n%time pf.to_pandas()\r\nCPU times: user 26.5 s, sys: 0 ns, total: 26.5 s\r\nWall time: 26.5 s\r\n```\r\nPyarrow has no problem:\r\n```\r\nimport pyarrow.parquet as pq\r\n%time table = pq.read_table('test_head.parq')\r\nCPU times: user 245 ms, sys: 18.3 ms, total: 263 ms\r\nWall time: 284 ms\r\n```\r\nand\r\n```\r\n%time table.to_pandas()\r\nCPU times: user 149 ms, sys: 26.2 ms, total: 175 ms\r\nWall time: 99.6 ms\r\n```\r\n\r\nIt seems like some book-keeping is taking way too long:\r\n```\r\n%prun pf = fastparquet.ParquetFile(file)\r\n         56934834 function calls (56924628 primitive calls) in 17.071 seconds\r\n\r\n   Ordered by: internal time\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n     5556   12.852    0.002   16.648    0.003 api.py:460(<listcomp>)\r\n 56735158    3.801    0.000    3.801    0.000 {method 'join' of 'str' objects}\r\n  10207/1    0.142    0.000    0.168    0.168 schema.py:28(schema_to_text)\r\n        1    0.097    0.097    0.158    0.158 {built-in method thrift.protocol.fastbinary.decode_compact}\r\n        1    0.022    0.022   16.690   16.690 api.py:448(_dtypes)\r\n    10207    0.015    0.000    0.015    0.000 ttypes.py:422(__init__)\r\n        1    0.013    0.013   17.071   17.071 <string>:1(<module>)\r\n    10206    0.013    0.000    0.013    0.000 ttypes.py:1357(__init__)\r\n        1    0.013    0.013    0.013    0.013 decoder.py:345(raw_decode)\r\n       28    0.012    0.000    0.012    0.000 {method 'read' of '_io.BufferedReader' objects}\r\n    10211    0.012    0.000    0.012    0.000 {built-in method builtins.hasattr}\r\n    10206    0.011    0.000    0.011    0.000 ttypes.py:293(__init__)\r\n        1    0.009    0.009    0.021    0.021 schema.py:52(flatten)\r\n    10206    0.007    0.000    0.007    0.000 {built-in method builtins.getattr}\r\n        1    0.006    0.006    0.006    0.006 schema.py:13(schema_tree)\r\n    40856    0.005    0.000    0.005    0.000 {built-in method builtins.len}\r\n    10206    0.005    0.000    0.005    0.000 ttypes.py:1258(__init__)\r\n    10206    0.005    0.000    0.005    0.000 ttypes.py:1591(__init__)\r\n        1    0.005    0.005   16.897   16.897 api.py:126(_set_attrs)\r\n        1    0.004    0.004    0.007    0.007 api.py:452(<dictcomp>)\r\n    30622    0.004    0.000    0.004    0.000 {method 'append' of 'list' objects}\r\n        1    0.004    0.004    0.004    0.004 schema.py:78(<listcomp>)\r\n        1    0.003    0.003    0.003    0.003 {method 'copy' of 'collections.OrderedDict' objects}\r\netc\r\n```\r\n\r\nThis is using fastparquet version `0.1.4` and pyarrow `0.8.0`.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/314", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/314/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/314/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/314/events", "html_url": "https://github.com/dask/fastparquet/issues/314", "id": 303610618, "node_id": "MDU6SXNzdWUzMDM2MTA2MTg=", "number": 314, "title": "Use LZ4 block compression", "user": {"login": "llchan", "id": 51099, "node_id": "MDQ6VXNlcjUxMDk5", "avatar_url": "https://avatars2.githubusercontent.com/u/51099?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llchan", "html_url": "https://github.com/llchan", "followers_url": "https://api.github.com/users/llchan/followers", "following_url": "https://api.github.com/users/llchan/following{/other_user}", "gists_url": "https://api.github.com/users/llchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/llchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llchan/subscriptions", "organizations_url": "https://api.github.com/users/llchan/orgs", "repos_url": "https://api.github.com/users/llchan/repos", "events_url": "https://api.github.com/users/llchan/events{/privacy}", "received_events_url": "https://api.github.com/users/llchan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2018-03-08T19:56:24Z", "updated_at": "2018-03-19T18:51:46Z", "closed_at": "2018-03-19T18:51:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "From what I can tell from the [parquet-cpp implementation](https://github.com/apache/arrow/blob/master/cpp/src/arrow/util/compression_lz4.cc), LZ4 compression should use the LZ4 block (i.e. un-framed) format.\r\n\r\nAt the moment, getting this to work with `parquet-cpp` is unfortunately a bit more tricky than simply changing the compression/decompression funcs in `fastparquet.compression` to the funcs in `lz4.block`, because it appears that the decompression func fails with corrupted lengths. I have a suspicion this may be a bug on the `parquet-cpp` side of things, maybe writing some padding/garbage bytes after the true payload, but I haven't dug through the code yet.\r\n\r\nFor anyone debugging, I've gotten it to work by passing `page_header.uncompressed_page_size` down, but this makes the code less clean, so hopefully it's not a permanent requirement.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/310", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/310/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/310/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/310/events", "html_url": "https://github.com/dask/fastparquet/issues/310", "id": 300787805, "node_id": "MDU6SXNzdWUzMDA3ODc4MDU=", "number": 310, "title": "Unnecessary print statement when reading file with CategoricalDtype index", "user": {"login": "kylebarron", "id": 15164633, "node_id": "MDQ6VXNlcjE1MTY0NjMz", "avatar_url": "https://avatars3.githubusercontent.com/u/15164633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kylebarron", "html_url": "https://github.com/kylebarron", "followers_url": "https://api.github.com/users/kylebarron/followers", "following_url": "https://api.github.com/users/kylebarron/following{/other_user}", "gists_url": "https://api.github.com/users/kylebarron/gists{/gist_id}", "starred_url": "https://api.github.com/users/kylebarron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kylebarron/subscriptions", "organizations_url": "https://api.github.com/users/kylebarron/orgs", "repos_url": "https://api.github.com/users/kylebarron/repos", "events_url": "https://api.github.com/users/kylebarron/events{/privacy}", "received_events_url": "https://api.github.com/users/kylebarron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-27T20:50:42Z", "updated_at": "2018-02-27T21:15:29Z", "closed_at": "2018-02-27T21:15:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "[This line](https://github.com/dask/fastparquet/blob/master/fastparquet/dataframe.py#L82) is causing an unnecessary line to be printed to console/output every time I read in a file written by pandas and which has a categorical index. Can this be removed?\r\n\r\n```py\r\ndef empty(types, size, cats=None, cols=None, index_type=None, index_name=None,\r\n          timezones=None):\r\n    ...\r\n    if index_type is not None and index_type is not False:\r\n    ...\r\n        if str(index_type) == 'category':\r\n        ...\r\n            print(cats, index_name, c)\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/308", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/308/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/308/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/308/events", "html_url": "https://github.com/dask/fastparquet/issues/308", "id": 299012137, "node_id": "MDU6SXNzdWUyOTkwMTIxMzc=", "number": 308, "title": "Wrong URL in setup.py (github.com/martindurant/fastparquet/)", "user": {"login": "alexmojaki", "id": 3627481, "node_id": "MDQ6VXNlcjM2Mjc0ODE=", "avatar_url": "https://avatars0.githubusercontent.com/u/3627481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexmojaki", "html_url": "https://github.com/alexmojaki", "followers_url": "https://api.github.com/users/alexmojaki/followers", "following_url": "https://api.github.com/users/alexmojaki/following{/other_user}", "gists_url": "https://api.github.com/users/alexmojaki/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexmojaki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexmojaki/subscriptions", "organizations_url": "https://api.github.com/users/alexmojaki/orgs", "repos_url": "https://api.github.com/users/alexmojaki/repos", "events_url": "https://api.github.com/users/alexmojaki/events{/privacy}", "received_events_url": "https://api.github.com/users/alexmojaki/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-21T15:23:19Z", "updated_at": "2018-03-19T18:52:38Z", "closed_at": "2018-03-19T18:52:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "It seems to me that this repo here is where the project lives, but PyPi first pointed me to a different fork. This confused me for a bit.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/305", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/305/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/305/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/305/events", "html_url": "https://github.com/dask/fastparquet/issues/305", "id": 298112357, "node_id": "MDU6SXNzdWUyOTgxMTIzNTc=", "number": 305, "title": "ColumnMetaData.total_(un)compressed_size does not take DictionaryPage into account", "user": {"login": "xhochy", "id": 70274, "node_id": "MDQ6VXNlcjcwMjc0", "avatar_url": "https://avatars2.githubusercontent.com/u/70274?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xhochy", "html_url": "https://github.com/xhochy", "followers_url": "https://api.github.com/users/xhochy/followers", "following_url": "https://api.github.com/users/xhochy/following{/other_user}", "gists_url": "https://api.github.com/users/xhochy/gists{/gist_id}", "starred_url": "https://api.github.com/users/xhochy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xhochy/subscriptions", "organizations_url": "https://api.github.com/users/xhochy/orgs", "repos_url": "https://api.github.com/users/xhochy/repos", "events_url": "https://api.github.com/users/xhochy/events{/privacy}", "received_events_url": "https://api.github.com/users/xhochy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-18T19:50:38Z", "updated_at": "2018-02-20T21:11:46Z", "closed_at": "2018-02-20T21:11:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "`start = f.tell()` is only called after this page is already written. Thus the sizes of `ColumnMetaData` don't include it.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/302", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/302/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/302/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/302/events", "html_url": "https://github.com/dask/fastparquet/issues/302", "id": 296918844, "node_id": "MDU6SXNzdWUyOTY5MTg4NDQ=", "number": 302, "title": "Use of deprecated `fromstring`", "user": {"login": "TomAugspurger", "id": 1312546, "node_id": "MDQ6VXNlcjEzMTI1NDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1312546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TomAugspurger", "html_url": "https://github.com/TomAugspurger", "followers_url": "https://api.github.com/users/TomAugspurger/followers", "following_url": "https://api.github.com/users/TomAugspurger/following{/other_user}", "gists_url": "https://api.github.com/users/TomAugspurger/gists{/gist_id}", "starred_url": "https://api.github.com/users/TomAugspurger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TomAugspurger/subscriptions", "organizations_url": "https://api.github.com/users/TomAugspurger/orgs", "repos_url": "https://api.github.com/users/TomAugspurger/repos", "events_url": "https://api.github.com/users/TomAugspurger/events{/privacy}", "received_events_url": "https://api.github.com/users/TomAugspurger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-13T23:07:35Z", "updated_at": "2018-02-14T15:49:26Z", "closed_at": "2018-02-14T15:30:16Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```python\r\nIn [1]: import pandas as pd\r\n\r\nIn [2]: import fastparquet as fp\r\n\r\nIn [3]: import warnings\r\n\r\nIn [4]: warnings.simplefilter(\"error\", DeprecationWarning)\r\n\r\nIn [5]: df = pd.DataFrame({\"A\": [True, False]})\r\n\r\nIn [6]: fp.write('foo.parq', df)\r\n\r\nIn [7]: fp.ParquetFile('foo.parq').to_pandas()\r\n```\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nDeprecationWarning                        Traceback (most recent call last)\r\n<ipython-input-7-95bfba083c58> in <module>()\r\n----> 1 fp.ParquetFile('foo.parq').to_pandas()\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/api.py in to_pandas(self, columns, categories, filters, index)\r\n    395                              for (name, v) in views.items()}\r\n    396                     self.read_row_group(rg, columns, categories, infile=f,\r\n--> 397                                         index=index, assign=parts)\r\n    398                     start += rg.num_rows\r\n    399         else:\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/api.py in read_row_group(self, rg, columns, categories, infile, index, assign)\r\n    222                 infile, rg, columns, categories, self.schema, self.cats,\r\n    223                 self.selfmade, index=index, assign=assign,\r\n--> 224                 scheme=self.file_scheme)\r\n    225         if ret:\r\n    226             return df\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/core.py in read_row_group(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme)\r\n    332         raise RuntimeError('Going with pre-allocation!')\r\n    333     read_row_group_arrays(file, rg, columns, categories, schema_helper,\r\n--> 334                           cats, selfmade, assign=assign)\r\n    335\r\n    336     for cat in cats:\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/core.py in read_row_group_arrays(file, rg, columns, categories, schema_helper, cats, selfmade, assign)\r\n    309         read_col(column, schema_helper, file, use_cat=use,\r\n    310                  selfmade=selfmade, assign=out[name],\r\n--> 311                  catdef=out[name+'-catdef'] if use else None)\r\n    312\r\n    313         if _is_map_like(schema_helper, column):\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/core.py in read_col(column, schema_helper, infile, use_cat, grab_dict, selfmade, assign, catdef)\r\n    233             skip_nulls = False\r\n    234         defi, rep, val = read_data_page(infile, schema_helper, ph, cmd,\r\n--> 235                                         skip_nulls, selfmade=selfmade)\r\n    236         if rep is not None and assign.dtype.kind != 'O':  # pragma: no cover\r\n    237             # this should never get called\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/core.py in read_data_page(f, helper, header, metadata, skip_nulls, selfmade)\r\n    116                                      metadata.type,\r\n    117                                      int(daph.num_values - num_nulls),\r\n--> 118                                      width=width)\r\n    119     elif daph.encoding in [parquet_thrift.Encoding.PLAIN_DICTIONARY,\r\n    120                            parquet_thrift.Encoding.RLE]:\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/encoding.py in read_plain(raw_bytes, type_, count, width)\r\n     38         return np.frombuffer(byte_buffer(raw_bytes), dtype=dtype, count=count)\r\n     39     if type_ == parquet_thrift.Type.BOOLEAN:\r\n---> 40         return read_plain_boolean(raw_bytes, count)\r\n     41     # variable byte arrays (rare)\r\n     42     try:\r\n\r\n~/miniconda3/envs/pandas-ip/lib/python3.6/site-packages/fastparquet/encoding.py in read_plain_boolean(raw_bytes, count)\r\n     16 def read_plain_boolean(raw_bytes, count):\r\n     17     \"\"\"Read `count` booleans using the plain encoding.\"\"\"\r\n---> 18     return np.unpackbits(np.fromstring(raw_bytes, dtype=np.uint8)).reshape(\r\n     19             (-1, 8))[:, ::-1].ravel().astype(bool)[:count]\r\n     20\r\n\r\nDeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\r\n```\r\n\r\nThis is with 0.1.4", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/301", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/301/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/301/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/301/events", "html_url": "https://github.com/dask/fastparquet/issues/301", "id": 295602340, "node_id": "MDU6SXNzdWUyOTU2MDIzNDA=", "number": 301, "title": "QUESTION : How to handle timeseries data with different and variable frequencies", "user": {"login": "guilhermecgs", "id": 8465337, "node_id": "MDQ6VXNlcjg0NjUzMzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8465337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guilhermecgs", "html_url": "https://github.com/guilhermecgs", "followers_url": "https://api.github.com/users/guilhermecgs/followers", "following_url": "https://api.github.com/users/guilhermecgs/following{/other_user}", "gists_url": "https://api.github.com/users/guilhermecgs/gists{/gist_id}", "starred_url": "https://api.github.com/users/guilhermecgs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guilhermecgs/subscriptions", "organizations_url": "https://api.github.com/users/guilhermecgs/orgs", "repos_url": "https://api.github.com/users/guilhermecgs/repos", "events_url": "https://api.github.com/users/guilhermecgs/events{/privacy}", "received_events_url": "https://api.github.com/users/guilhermecgs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-08T17:35:07Z", "updated_at": "2018-02-09T23:08:52Z", "closed_at": "2018-02-09T23:08:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Folks,\r\n\r\nI am evaluating this library but, before any coding, I would like to ask some basic stuff...\r\nIt would be really nice if you took 2 min to help me....\r\n\r\nMy situation is:  \r\n\r\n- I have a list of hundreds of timeseries parameters\r\n- Each parameter is independent of each other\r\n- Each parameter has a basic frequency (lets say, 10hz) but due to hardware limitations may be saved from 9 to 11 hz (variable frequency)\r\n- Normally, I only need to read a few of there parameters at a time\r\n\r\nMy questions are:\r\n\r\n1. Do you think parquet would be a suitable file format given this brief description?\r\n2. Can parquet (and this library) deal with variable frequency? In other words, does it save a timestamp associated with each value?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/300", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/300/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/300/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/300/events", "html_url": "https://github.com/dask/fastparquet/issues/300", "id": 293912075, "node_id": "MDU6SXNzdWUyOTM5MTIwNzU=", "number": 300, "title": "partition_on_columns breaking change in 0.1.4", "user": {"login": "gorlins", "id": 139286, "node_id": "MDQ6VXNlcjEzOTI4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/139286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gorlins", "html_url": "https://github.com/gorlins", "followers_url": "https://api.github.com/users/gorlins/followers", "following_url": "https://api.github.com/users/gorlins/following{/other_user}", "gists_url": "https://api.github.com/users/gorlins/gists{/gist_id}", "starred_url": "https://api.github.com/users/gorlins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gorlins/subscriptions", "organizations_url": "https://api.github.com/users/gorlins/orgs", "repos_url": "https://api.github.com/users/gorlins/repos", "events_url": "https://api.github.com/users/gorlins/events{/privacy}", "received_events_url": "https://api.github.com/users/gorlins/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-02T14:48:38Z", "updated_at": "2018-02-09T23:08:52Z", "closed_at": "2018-02-09T23:08:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "The addition of the argument ```sep``` in ```fastparquet.writer. partition_on_columns``` breaks Dask compatibility since both are using positional arguments, and are now out of order.  I would suggest using keywords or only adding new positional args to the end to prevent future issues", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/299", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/299/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/299/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/299/events", "html_url": "https://github.com/dask/fastparquet/issues/299", "id": 293364514, "node_id": "MDU6SXNzdWUyOTMzNjQ1MTQ=", "number": 299, "title": "about decimal type with redshift spectrum", "user": {"login": "thomaszdxsn", "id": 23616426, "node_id": "MDQ6VXNlcjIzNjE2NDI2", "avatar_url": "https://avatars1.githubusercontent.com/u/23616426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomaszdxsn", "html_url": "https://github.com/thomaszdxsn", "followers_url": "https://api.github.com/users/thomaszdxsn/followers", "following_url": "https://api.github.com/users/thomaszdxsn/following{/other_user}", "gists_url": "https://api.github.com/users/thomaszdxsn/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomaszdxsn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomaszdxsn/subscriptions", "organizations_url": "https://api.github.com/users/thomaszdxsn/orgs", "repos_url": "https://api.github.com/users/thomaszdxsn/repos", "events_url": "https://api.github.com/users/thomaszdxsn/events{/privacy}", "received_events_url": "https://api.github.com/users/thomaszdxsn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-01T00:42:36Z", "updated_at": "2018-02-09T23:08:52Z", "closed_at": "2018-02-09T23:08:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "hi, i need some help...\r\n\r\nwhen i use fastparquet generate parquet file, and upload s3, use redshift spectrum query it, some problem happen.\r\n\r\nsuch as my table has a column is DECIMAL type, it will raise:\r\n\r\n```\r\nS3 Query Exception (Fetch)\r\n...has an incompatible Parquet schema for column...\r\n```\r\n\r\nand i alter table schema, change column type to FLOAT, can query with parquet, but only 2 point precision.\r\n\r\nand i search related article, find[issue48](https://github.com/dask/fastparquet/issues/48), [fastparquet-doc](http://fastparquet.readthedocs.io/en/latest/details.html#spark-timestamps),i know fastparquet not support DECIMAL, but how to solve my problem, i need redshift query FLOAT full point precision, may be i can store it with VARCHAR type, is it a good idea?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dask/fastparquet/issues/297", "repository_url": "https://api.github.com/repos/dask/fastparquet", "labels_url": "https://api.github.com/repos/dask/fastparquet/issues/297/labels{/name}", "comments_url": "https://api.github.com/repos/dask/fastparquet/issues/297/comments", "events_url": "https://api.github.com/repos/dask/fastparquet/issues/297/events", "html_url": "https://github.com/dask/fastparquet/issues/297", "id": 292210780, "node_id": "MDU6SXNzdWUyOTIyMTA3ODA=", "number": 297, "title": "New release failing on pandas master", "user": {"login": "jorisvandenbossche", "id": 1020496, "node_id": "MDQ6VXNlcjEwMjA0OTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1020496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorisvandenbossche", "html_url": "https://github.com/jorisvandenbossche", "followers_url": "https://api.github.com/users/jorisvandenbossche/followers", "following_url": "https://api.github.com/users/jorisvandenbossche/following{/other_user}", "gists_url": "https://api.github.com/users/jorisvandenbossche/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorisvandenbossche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorisvandenbossche/subscriptions", "organizations_url": "https://api.github.com/users/jorisvandenbossche/orgs", "repos_url": "https://api.github.com/users/jorisvandenbossche/repos", "events_url": "https://api.github.com/users/jorisvandenbossche/events{/privacy}", "received_events_url": "https://api.github.com/users/jorisvandenbossche/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-01-28T14:58:32Z", "updated_at": "2018-02-09T23:08:52Z", "closed_at": "2018-02-09T23:08:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "The new release breaks some of our tests. It seems that we adapted the signature of `make_block_same_class` (in https://github.com/pandas-dev/pandas/pull/19265). Maybe we should also fix this temporarily in pandas by putting it back\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n___________________ TestParquetFastParquet.test_datetime_tz ____________________\r\n[gw0] linux -- Python 3.6.4 /home/travis/miniconda3/envs/pandas/bin/python\r\nself = <pandas.tests.io.test_parquet.TestParquetFastParquet object at 0x7fce4e4d7940>\r\nfp = 'fastparquet'\r\n    def test_datetime_tz(self, fp):\r\n        # doesn't preserve tz\r\n        df = pd.DataFrame({'a': pd.date_range('20130101', periods=3,\r\n                                              tz='US/Eastern')})\r\n    \r\n        # warns on the coercion\r\n        with catch_warnings(record=True):\r\n>           check_round_trip(df, fp, expected=df.astype('datetime64[ns]'))\r\npandas/tests/io/test_parquet.py:478: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\npandas/tests/io/test_parquet.py:158: in check_round_trip\r\n    compare(repeat)\r\npandas/tests/io/test_parquet.py:152: in compare\r\n    actual = read_parquet(path, **read_kwargs)\r\npandas/io/parquet.py:278: in read_parquet\r\n    return impl.read(path, columns=columns, **kwargs)\r\npandas/io/parquet.py:226: in read\r\n    return parquet_file.to_pandas(columns=columns, **kwargs)\r\n../../../miniconda3/envs/pandas/lib/python3.6/site-packages/fastparquet/api.py:388: in to_pandas\r\n    df, views = self.pre_allocate(size, columns, categories, index)\r\n../../../miniconda3/envs/pandas/lib/python3.6/site-packages/fastparquet/api.py:418: in pre_allocate\r\n    self._dtypes(categories), tz)\r\n../../../miniconda3/envs/pandas/lib/python3.6/site-packages/fastparquet/api.py:509: in _pre_allocate\r\n    index_type=index_type, cats=cats, timezones=tz)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntypes = [dtype('<M8[ns]')], size = 3, cats = {}, cols = ['a'], index_type = None\r\nindex_name = None, timezones = {'a': 'US/Eastern'}\r\n    def empty(types, size, cats=None, cols=None, index_type=None, index_name=None,\r\n              timezones=None):\r\n        \"\"\"\r\n        Create empty DataFrame to assign into\r\n    \r\n        Parameters\r\n        ----------\r\n        types: like np record structure, 'i4,u2,f4,f2,f4,M8,m8', or using tuples\r\n            applies to non-categorical columns. If there are only categorical\r\n            columns, an empty string of None will do.\r\n        size: int\r\n            Number of rows to allocate\r\n        cats: dict {col: labels}\r\n            Location and labels for categorical columns, e.g., {1: ['mary', 'mo]}\r\n            will create column index 1 (inserted amongst the numerical columns)\r\n            with two possible values. If labels is an integers, `{'col': 5}`,\r\n            will generate temporary labels using range. If None, or column name\r\n            is missing, will assume 16-bit integers (a reasonable default).\r\n        cols: list of labels\r\n            assigned column names, including categorical ones.\r\n        timezones: dict {col: timezone_str}\r\n            for timestamp type columns, apply this timezone to the pandas series;\r\n            the numpy view will be UTC.\r\n    \r\n        Returns\r\n        -------\r\n        - dataframe with correct shape and data-types\r\n        - list of numpy views, in order, of the columns of the dataframe. Assign\r\n            to this.\r\n        \"\"\"\r\n        df = DataFrame()\r\n        views = {}\r\n        timezones = timezones or {}\r\n    \r\n        cols = cols if cols is not None else range(cols)\r\n        if isinstance(types, STR_TYPE):\r\n            types = types.split(',')\r\n        for t, col in zip(types, cols):\r\n            if str(t) == 'category':\r\n                if cats is None or col not in cats:\r\n                    df[str(col)] = Categorical(\r\n                            [], categories=RangeIndex(0, 2**14),\r\n                            fastpath=True)\r\n                elif isinstance(cats[col], int):\r\n                    df[str(col)] = Categorical(\r\n                            [], categories=RangeIndex(0, cats[col]),\r\n                            fastpath=True)\r\n                else:  # explicit labels list\r\n                    df[str(col)] = Categorical([], categories=cats[col],\r\n                                               fastpath=True)\r\n            else:\r\n                df[str(col)] = np.empty(0, dtype=t)\r\n                if df[str(col)].dtype.kind == \"M\" and str(col) in timezones:\r\n                    df[str(col)] = df[str(col)].dt.tz_localize(timezones[str(col)])\r\n    \r\n        if index_type is not None and index_type is not False:\r\n            if index_name is None:\r\n                raise ValueError('If using an index, must give an index name')\r\n            if str(index_type) == 'category':\r\n                if cats is None or index_name not in cats:\r\n                    c = Categorical(\r\n                            [], categories=RangeIndex(0, 2**14),\r\n                            fastpath=True)\r\n                elif isinstance(cats[index_name], int):\r\n                    c = Categorical(\r\n                            [], categories=RangeIndex(0, cats[index_name]),\r\n                            fastpath=True)\r\n                else:  # explicit labels list\r\n                    c = Categorical([], categories=cats[index_name],\r\n                                    fastpath=True)\r\n                print(cats, index_name, c)\r\n                vals = np.empty(size, dtype=c.codes.dtype)\r\n                index = CategoricalIndex(c)\r\n                index._data._codes = vals\r\n                views[index_name] = vals\r\n            else:\r\n                index = Index(np.empty(size, dtype=index_type))\r\n                views[index_name] = index.values\r\n    \r\n            axes = [df._data.axes[0], index]\r\n        else:\r\n            axes = [df._data.axes[0], RangeIndex(size)]\r\n    \r\n        # allocate and create blocks\r\n        blocks = []\r\n        for block in df._data.blocks:\r\n            if block.is_categorical:\r\n                categories = block.values.categories\r\n                code = np.zeros(shape=size, dtype=block.values.codes.dtype)\r\n                values = Categorical(values=code, categories=categories,\r\n                                     fastpath=True)\r\n                new_block = block.make_block_same_class(values=values)\r\n            elif getattr(block.dtype, 'tz', None):\r\n                new_shape = (size, )\r\n                values = np.empty(shape=new_shape, dtype=block.values.values.dtype)\r\n                new_block = block.make_block_same_class(\r\n>                       values=values, dtype=block.values.dtype)\r\nE               TypeError: make_block_same_class() got an unexpected keyword argument 'dtype'\r\n../../../miniconda3/envs/pandas/lib/python3.6/site-packages/fastparquet/dataframe.py:105: TypeError\r\n```", "performed_via_github_app": null, "score": 1.0}]}