{"total_count": 1107, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2105", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2105/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2105/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2105/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2105", "id": 678866961, "node_id": "MDU6SXNzdWU2Nzg4NjY5NjE=", "number": 2105, "title": "SSL: CERTIFICATE_VERIFY_FAILED and SASL", "user": {"login": "bbkchdhry", "id": 22065325, "node_id": "MDQ6VXNlcjIyMDY1MzI1", "avatar_url": "https://avatars2.githubusercontent.com/u/22065325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bbkchdhry", "html_url": "https://github.com/bbkchdhry", "followers_url": "https://api.github.com/users/bbkchdhry/followers", "following_url": "https://api.github.com/users/bbkchdhry/following{/other_user}", "gists_url": "https://api.github.com/users/bbkchdhry/gists{/gist_id}", "starred_url": "https://api.github.com/users/bbkchdhry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bbkchdhry/subscriptions", "organizations_url": "https://api.github.com/users/bbkchdhry/orgs", "repos_url": "https://api.github.com/users/bbkchdhry/repos", "events_url": "https://api.github.com/users/bbkchdhry/events{/privacy}", "received_events_url": "https://api.github.com/users/bbkchdhry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-14T03:20:42Z", "updated_at": "2020-08-14T09:46:48Z", "closed_at": "2020-08-14T09:46:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Team,\r\n\r\nI am trying to connect to kafka from python client, but no matter what I do, I keep getting Certificate Error. I also tried to consume records from Java Kafka Consumer with same certificates and it is working fine.\r\n\r\nAlso, I am unable to find a solution for kafka GSSAPI with kerberos keytab, there is only a solution for SASL_PLAINTEXT, so can you guys help me out a little. Thank you.\r\n\r\nSystem and Libraries I am using:\r\n- Ubuntu 16 (Linux 4.15.0-112-generic x86_64)\r\n- kafka-python (2.0.1)\r\n\r\nCode:\r\n![image](https://user-images.githubusercontent.com/22065325/90210242-27174900-de0d-11ea-82dc-50dd9f774626.png)\r\n\r\nError:\r\n![image](https://user-images.githubusercontent.com/22065325/90210263-35fdfb80-de0d-11ea-8d58-a44957e459e7.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2098", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2098/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2098/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2098/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2098", "id": 672148945, "node_id": "MDU6SXNzdWU2NzIxNDg5NDU=", "number": 2098, "title": "do_handshake error appearing randomly", "user": {"login": "FrancoisFerrariKpler", "id": 68236294, "node_id": "MDQ6VXNlcjY4MjM2Mjk0", "avatar_url": "https://avatars3.githubusercontent.com/u/68236294?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FrancoisFerrariKpler", "html_url": "https://github.com/FrancoisFerrariKpler", "followers_url": "https://api.github.com/users/FrancoisFerrariKpler/followers", "following_url": "https://api.github.com/users/FrancoisFerrariKpler/following{/other_user}", "gists_url": "https://api.github.com/users/FrancoisFerrariKpler/gists{/gist_id}", "starred_url": "https://api.github.com/users/FrancoisFerrariKpler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FrancoisFerrariKpler/subscriptions", "organizations_url": "https://api.github.com/users/FrancoisFerrariKpler/orgs", "repos_url": "https://api.github.com/users/FrancoisFerrariKpler/repos", "events_url": "https://api.github.com/users/FrancoisFerrariKpler/events{/privacy}", "received_events_url": "https://api.github.com/users/FrancoisFerrariKpler/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-03T15:14:47Z", "updated_at": "2020-08-07T07:42:55Z", "closed_at": "2020-08-07T07:42:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nWe keep having issues while writing messages to Kafka and we don't understand why, this issues do not happen too frequently but they are causing troubles, here is the stack trace for the error we have. Would you have an idea from what this error can come ?\r\n\r\nWe use the version 2.0.0 of the library,\r\n\r\nThank you\r\n\r\nTraceback (most recent call last):\r\nFile \"/home/XXXXXXXX/.virtualenvs/YYYY/lib/python3.7/site-packages/kafka/producer/sender.py\", line 74, in run self.run_once()\r\nFile \"/home/XXXXXXXX/.virtualenvs/YYYY/lib/python3.7/site-packages/kafka/producer/sender.py\", line 160, in run_once self._client.poll(timeout_ms=poll_timeout_ms)\r\nFile \"/home/XXXXXXXX/.virtualenvs/YYYY/lib/python3.7/site-packages/kafka/client_async.py\", line 580, in poll self._maybe_connect(node_id)\r\nFile \"/home/XXXXXXXX/.virtualenvs/YYYY/lib/python3.7/site-packages/kafka/client_async.py\", line 390, in _maybe_connect conn.connect()\r\nFile \"/home/XXXXXXXX/.virtualenvs/YYYY/lib/python3.7/site-packages/kafka/conn.py\", line 426, in connect if self._try_handshake():\r\nFile \"/home/XXXXXXXX/.virtualenvs/YYYY/lib/python3.7/site-packages/kafka/conn.py\", line 505, in _try_handshake self._sock.do_handshake()\r\nFile \"/usr/lib/python3.7/ssl.py\", line 1139, in do_handshake self._sslobj.do_handshake()\r\nOSError: [Errno 0] Error\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2089", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2089/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2089/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2089/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2089", "id": 663609771, "node_id": "MDU6SXNzdWU2NjM2MDk3NzE=", "number": 2089, "title": "Is `kafka-python` 3.8 compatible", "user": {"login": "kornicameister", "id": 1029674, "node_id": "MDQ6VXNlcjEwMjk2NzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1029674?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kornicameister", "html_url": "https://github.com/kornicameister", "followers_url": "https://api.github.com/users/kornicameister/followers", "following_url": "https://api.github.com/users/kornicameister/following{/other_user}", "gists_url": "https://api.github.com/users/kornicameister/gists{/gist_id}", "starred_url": "https://api.github.com/users/kornicameister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kornicameister/subscriptions", "organizations_url": "https://api.github.com/users/kornicameister/orgs", "repos_url": "https://api.github.com/users/kornicameister/repos", "events_url": "https://api.github.com/users/kornicameister/events{/privacy}", "received_events_url": "https://api.github.com/users/kornicameister/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-22T09:21:08Z", "updated_at": "2020-08-10T18:43:28Z", "closed_at": "2020-08-10T18:43:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI did not notice anything suggesting `kafka-python` works well with `3.8`.\r\nCan someone can \"officially\" confirm it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2086", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2086/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2086/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2086/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2086", "id": 660857862, "node_id": "MDU6SXNzdWU2NjA4NTc4NjI=", "number": 2086, "title": "Consumer close() causing issue while gracefully exiting", "user": {"login": "kartheek7895", "id": 14072000, "node_id": "MDQ6VXNlcjE0MDcyMDAw", "avatar_url": "https://avatars1.githubusercontent.com/u/14072000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kartheek7895", "html_url": "https://github.com/kartheek7895", "followers_url": "https://api.github.com/users/kartheek7895/followers", "following_url": "https://api.github.com/users/kartheek7895/following{/other_user}", "gists_url": "https://api.github.com/users/kartheek7895/gists{/gist_id}", "starred_url": "https://api.github.com/users/kartheek7895/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kartheek7895/subscriptions", "organizations_url": "https://api.github.com/users/kartheek7895/orgs", "repos_url": "https://api.github.com/users/kartheek7895/repos", "events_url": "https://api.github.com/users/kartheek7895/events{/privacy}", "received_events_url": "https://api.github.com/users/kartheek7895/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-19T14:09:06Z", "updated_at": "2020-07-20T06:53:37Z", "closed_at": "2020-07-20T06:53:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Call to `consumer.close()` is not exiting when a signal is caught(`SIGINT` or `SIGTERM`). This happens only when there are no messages to consume. Also, the consumer gracefully shutdown when I send 2 `SIGINT` signals. Here is my consumer code: \r\n\r\n```\r\nproperties = dict()\r\nproperties['bootstrap_servers'] = hosts\r\nproperties['group_id'] = consumer_group\r\nproperties['client_id'] = kwargs.get('client_id', \"1\")\r\nproperties['enable_auto_commit'] = kwargs.get('auto_commit', False)\r\nproperties['max_poll_records'] = kwargs.get('no_records', 1)\r\nproperties['max_poll_interval_ms'] = kwargs.get('poll_interval', 15 * 60 * 1000)\r\nproperties['session_timeout_ms'] = kwargs.get('session_timeout', 30 * 1000)\r\nproperties['heartbeat_interval_ms'] = kwargs.get('heartbeat_interval', 5 * 1000)\r\n\r\nconsumer = KafkaConsumer(topic, **properties)\r\n\r\ndef graceful_exit_handler(signum, frame):\r\n    log.info(\"Shutting down\")\r\n    logging.shutdown()\r\n    consumer.close()\r\n    sys.exit(0)\r\n\r\nsignal.signal(signal.SIGINT, graceful_exit_handler)\r\nsignal.signal(signal.SIGTERM, graceful_exit_handler)\r\n\r\nfor msg in consumer:\r\n    try:\r\n        // processing\r\n    except:\r\n        pass\r\n    consumer.commit_async()\r\n\r\n```\r\n\r\nHere is the respective thread stack trace when the first signal was caught  :\r\n```\r\n<_MainThread(MainThread, started 140650904688448)>\r\n  File \"my_consumer.py\", line 62, in <module>\r\n    for msg in consumer:\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/consumer/group.py\", line 1192, in __next__\r\n    return self.next_v2()\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/consumer/group.py\", line 1200, in next_v2\r\n    return next(self._iterator)\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/consumer/group.py\", line 1115, in _message_generator_v2\r\n    record_map = self.poll(timeout_ms=timeout_ms, update_offsets=False)\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/consumer/group.py\", line 654, in poll\r\n    records = self._poll_once(remaining, max_records, update_offsets=update_offsets)\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/consumer/group.py\", line 701, in _poll_once\r\n    self._client.poll(timeout_ms=timeout_ms)\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/client_async.py\", line 600, in poll\r\n    self._poll(timeout / 1000)\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/client_async.py\", line 632, in _poll\r\n    ready = self._selector.select(timeout)\r\n  File \"/usr/lib/python3.7/selectors.py\", line 468, in select\r\n    fd_event_list = self._selector.poll(timeout, max_ev)\r\n  File \"my_consumer.py\", line 56, in graceful_exit_handler\r\n    consumer.close()\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/consumer/group.py\", line 455, in close\r\n    self._coordinator.close(autocommit=autocommit)\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/coordinator/consumer.py\", line 433, in close\r\n    super(ConsumerCoordinator, self).close()\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/coordinator/base.py\", line 762, in close\r\n    self._close_heartbeat_thread()\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/coordinator/base.py\", line 751, in _close_heartbeat_thread\r\n    self._heartbeat_thread.close()\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/coordinator/base.py\", line 925, in close\r\n    with self.coordinator._lock:\r\n  File \"/usr/lib/python3.7/threading.py\", line 241, in __enter__\r\n    return self._lock.__enter__()\r\n\r\n\r\n <HeartbeatThread(my_consumer-heartbeat, started daemon 140650877511424)>\r\n  File \"/usr/lib/python3.7/threading.py\", line 890, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/coordinator/base.py\", line 936, in run\r\n    self._run_once()\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/coordinator/base.py\", line 968, in _run_once\r\n    self.coordinator._client.poll(timeout_ms=0)\r\n  File \"/home/user/project/venv/lib/python3.7/site-packages/kafka/client_async.py\", line 574, in poll\r\n    with self._lock:\r\n\r\n``` \r\nOutput:\r\n```\r\n^C2020-07-19 19:02:26,948-28353 - INFO - Shutting down\r\n2020-07-19 19:02:26,949-28353 - DEBUG - Closing the KafkaConsumer.\r\n^C2020-07-19 19:41:35,275-28353 - INFO - Shutting down\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2077", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2077/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2077/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2077/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2077", "id": 655121397, "node_id": "MDU6SXNzdWU2NTUxMjEzOTc=", "number": 2077, "title": "Consumer client running in to a deadlock", "user": {"login": "vsrini-ns", "id": 54335310, "node_id": "MDQ6VXNlcjU0MzM1MzEw", "avatar_url": "https://avatars0.githubusercontent.com/u/54335310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vsrini-ns", "html_url": "https://github.com/vsrini-ns", "followers_url": "https://api.github.com/users/vsrini-ns/followers", "following_url": "https://api.github.com/users/vsrini-ns/following{/other_user}", "gists_url": "https://api.github.com/users/vsrini-ns/gists{/gist_id}", "starred_url": "https://api.github.com/users/vsrini-ns/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vsrini-ns/subscriptions", "organizations_url": "https://api.github.com/users/vsrini-ns/orgs", "repos_url": "https://api.github.com/users/vsrini-ns/repos", "events_url": "https://api.github.com/users/vsrini-ns/events{/privacy}", "received_events_url": "https://api.github.com/users/vsrini-ns/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-11T03:04:48Z", "updated_at": "2020-07-14T01:41:09Z", "closed_at": "2020-07-11T05:23:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Problem Details**\r\n\r\nWe are seeing a deadlock situation with the Kafka consumer especially when the Group Coordinator is not reachable for a period of time larger than the request timeout. The heartbeat thread and the AsyncClient are waiting for each other to release the lock.\r\n\r\nBased on the analysis, it appears there is a race condition for the deadlock to happen between the heartbeat thread and the Async client especially when the consumer group coordinator is not reachable beyond the maximum request timeout limit. \r\n\r\nA combination of parallel execution of the heartbeat request, broker connection timeout and the cancellation of the any pending in-flight requests when the consumer group coordinator is not reachable could lead us to this situation.\r\n\r\n**Branch:** Kafka-Python 1.4.7\r\n\r\n**Consumer Properties**\r\n```\r\n{'bootstrap_servers': ['kafka01:9092', 'kafka02:9092', 'kafka03:9092', 'kafka04.:9092', 'kafka05:9092', 'kafka06:9092'], 'enable_auto_commit': True, 'session_timeout_ms': 60000, 'max_poll_records': 100, 'group_id': 'deadlock-test'}\r\n```\r\n**From the Logs**\r\n```\r\n1) Consumer is polling continuously \r\n2) Heart Beat is failing (unable to talk to group coordinator) - coordinator is marked dead\r\n3) Heartbeat session expired, marking coordinator dead \t\r\n(00:45:55.498 -0700 to 00:49:56.374 -0700)\r\n4) Auto offset commit failing continuously (NodeNotReadyError), while poll(..) is able to fetch records\r\n   - 00:45:10.520 -0700 to 00:50:03.251 -0700\r\n5) Async client connection is closed after the request timeout is elapsed (timed out after 305000 ms. Closing connection)\r\n   - 00:50:03.253 -0700\r\n```\r\n\r\n**Code Analysis**\r\n```\r\nConsumerGroup.Poll()\t\r\n\t-> Consumer.Poll()  \r\n\t\t-> Activates Heart Beat Thread to send heart beat\t\t\t\t\t\t\tHeartBeat Thread\r\n\t\t-> CommitOffsetAsync                                      \t\t\t\t\t\t\t-> lock()\r\n\t\t\t-> AsyncClient.poll()\t\t\t\t\t\t\t\t\t\t\t\t-> AsyncClient.poll() \r\n\t\t\t\t-> lock()\t\t\t\t\t\t\t\t\t\t\t\t\t-> monitor.lock() (waiting for CommitOffsetAsync to release lock)\r\n\t\t\t\t\t-> _poll()\r\n\t\t\t\t\t\t-> conn.close(error) [ Close All Request Timeout Connections ]\r\n\t\t\t\t\t\t\t-> Iterates In-Flight requests and associated callbacks\r\n\t\t\t\t\t\t\t\t-> Executes Callbacks in loop\r\n\t\t\t\t\t\t\t\t\t-> HandleHeartBeatFailure   \t\t\tHandleHeartBeatFailure()\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t-> lock() --> (waiting for heartbeat thread to release the lock)\r\n\r\n```\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2076", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2076/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2076/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2076/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2076", "id": 654283130, "node_id": "MDU6SXNzdWU2NTQyODMxMzA=", "number": 2076, "title": "Deadlock bug", "user": {"login": "jeffwidman", "id": 483314, "node_id": "MDQ6VXNlcjQ4MzMxNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/483314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffwidman", "html_url": "https://github.com/jeffwidman", "followers_url": "https://api.github.com/users/jeffwidman/followers", "following_url": "https://api.github.com/users/jeffwidman/following{/other_user}", "gists_url": "https://api.github.com/users/jeffwidman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffwidman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffwidman/subscriptions", "organizations_url": "https://api.github.com/users/jeffwidman/orgs", "repos_url": "https://api.github.com/users/jeffwidman/repos", "events_url": "https://api.github.com/users/jeffwidman/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffwidman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-09T19:04:31Z", "updated_at": "2020-07-10T04:12:09Z", "closed_at": "2020-07-10T04:12:09Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "When network connectivity to one of the brokers temporarily flakes, it can occasionally result in a deadlock, from which the consumer can never recover.\r\n\r\nA coworker debugged further and found the problem appears to be a lock contention issue when futures fail due to connection timeouts.\r\n\r\nFrom the below--you can see that Thread A grabs the client lock, then attempts to grab the coordinator lock.  Meanwhile, Thread B grabbed the coordinator lock and then tries to grab the client lock. The result is a deadlock.\r\n\r\n```\r\nThread-A:\r\n\r\nKafkaConsumer[_client]\r\nKafkaConsumer.poll()\r\n- KafkaConsumer._poll_once()\r\n  - ConsumerCoordinator(BaseCoordinator).poll()\r\n    - ConsumerCoordinator(BaseCoordinator)._maybe_auto_commit_offsets_async()\r\n      - ConsumerCoordinator(BaseCoordinator).commit_offsets_async()\r\n        - KafkaClient[_client].poll()\r\n          - [[_client.lock]]\r\n          - KafkaClient[_client]._poll()\r\n            - BrokerConnection.close()\r\n              - Future.failure()\r\n                - Future._call_backs()\r\n                  - ConsumerCoordinator(BaseCoordinator)._failed_request()\r\n                    - Future.failure() // <-- tail recursion\r\n                      - Future._callbacks()\r\n                        - HeartbeatThread._handle_heartbeat_failure()\r\n                          - [[coordinator._lock]]\r\n\r\n\r\nThread-B:\r\n\r\nHeartbeatThread.run()\r\n- HeartbeatThread._run_once()\r\n  - [[coordinator._lock]]\r\n  - KafkaClient[_client].poll()\r\n    - [[_client.lock]]\r\n```\r\n(Many thanks to my unnamed coworker for the excellent diagram of the issue.)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2074", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2074/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2074/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2074/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2074", "id": 652670510, "node_id": "MDU6SXNzdWU2NTI2NzA1MTA=", "number": 2074, "title": "SCRAM-SHA-256 not supported in Kafka-python 2.0.0", "user": {"login": "evgeniyasti", "id": 45977438, "node_id": "MDQ6VXNlcjQ1OTc3NDM4", "avatar_url": "https://avatars3.githubusercontent.com/u/45977438?v=4", "gravatar_id": "", "url": "https://api.github.com/users/evgeniyasti", "html_url": "https://github.com/evgeniyasti", "followers_url": "https://api.github.com/users/evgeniyasti/followers", "following_url": "https://api.github.com/users/evgeniyasti/following{/other_user}", "gists_url": "https://api.github.com/users/evgeniyasti/gists{/gist_id}", "starred_url": "https://api.github.com/users/evgeniyasti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/evgeniyasti/subscriptions", "organizations_url": "https://api.github.com/users/evgeniyasti/orgs", "repos_url": "https://api.github.com/users/evgeniyasti/repos", "events_url": "https://api.github.com/users/evgeniyasti/events{/privacy}", "received_events_url": "https://api.github.com/users/evgeniyasti/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-07T21:41:45Z", "updated_at": "2020-07-08T13:57:59Z", "closed_at": "2020-07-08T13:57:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "I need to use the SCRAM-SHA-256 in my Python program.\r\n\r\nBut it get an error saying it's not supported:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"./perform-load-test.py\", line 154, in <module>\r\n    main(sys.argv[1:])\r\n  File \"./perform-load-test.py\", line 127, in main\r\n    auto_offset_reset='latest')\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/consumer/group.py\", line 355, in __init__\r\n    self._client = KafkaClient(metrics=self._metrics, **self.config)\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/client_async.py\", line 242, in __init__\r\n    self.config['api_version'] = self.check_version(timeout=check_timeout)\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/client_async.py\", line 899, in check_version\r\n    self._maybe_connect(try_node)\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/client_async.py\", line 379, in _maybe_connect\r\n    **self.config)\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/conn.py\", line 254, in __init__\r\n    'security_protocol must be in ' + ', '.join(self.SECURITY_PROTOCOLS))\r\nAssertionError: security_protocol must be in PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL\r\n```\r\nHere is my version of kafka-python:\r\n```\r\nroot@machine00:/# pip3 freeze | grep kafka\r\nkafka-python==2.0.0\r\n```\r\n\r\nHere is how I setup the client:\r\n\r\n```\r\n                consumer = KafkaConsumer('helloworld',\r\n                                         group_id=\"mygroup\",\r\n                                         bootstrap_servers=bootstrap_servers,\r\n                                         ssl_cafile='caroot_dev.cer',\r\n                                         security_protocol='SCRAM-SHA-256',\r\n                                         sasl_plain_username='covid19',\r\n                                         sasl_plain_password='secret',\r\n                                         auto_offset_reset='latest')\r\n\r\n```\r\n\r\nAnd here is my version of Python:\r\n\r\n```\r\nroot@machine00:/# python3 --version\r\nPython 3.6.11\r\n```\r\n\r\nCan someone please tell me how to use SCRAM-SHA-256?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2066", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2066/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2066/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2066/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2066", "id": 636393270, "node_id": "MDU6SXNzdWU2MzYzOTMyNzA=", "number": 2066, "title": "How to yeild CPU if consumer empty", "user": {"login": "evgeniyasti", "id": 45977438, "node_id": "MDQ6VXNlcjQ1OTc3NDM4", "avatar_url": "https://avatars3.githubusercontent.com/u/45977438?v=4", "gravatar_id": "", "url": "https://api.github.com/users/evgeniyasti", "html_url": "https://github.com/evgeniyasti", "followers_url": "https://api.github.com/users/evgeniyasti/followers", "following_url": "https://api.github.com/users/evgeniyasti/following{/other_user}", "gists_url": "https://api.github.com/users/evgeniyasti/gists{/gist_id}", "starred_url": "https://api.github.com/users/evgeniyasti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/evgeniyasti/subscriptions", "organizations_url": "https://api.github.com/users/evgeniyasti/orgs", "repos_url": "https://api.github.com/users/evgeniyasti/repos", "events_url": "https://api.github.com/users/evgeniyasti/events{/privacy}", "received_events_url": "https://api.github.com/users/evgeniyasti/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-06-10T16:36:04Z", "updated_at": "2020-06-12T13:20:58Z", "closed_at": "2020-06-12T13:20:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello.\r\n\r\nI consuming message with\r\n\r\n\r\n    for message in consumer:\r\n       ...\r\n\r\nI need to yield the CPU though if there are no messages in the consumer:\r\n\r\n    for message in consumer or 'default':\r\n       if message == 'default':\r\n           await asyncio.sleep(0)\r\n\r\nSomething along those lines. Basically I need to either consumer or do other stuff on another co-routine.\r\n\r\nHow do I achieve this?\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2065", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2065/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2065/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2065/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2065", "id": 636203663, "node_id": "MDU6SXNzdWU2MzYyMDM2NjM=", "number": 2065, "title": "Kafka consumer process the same message several time if scaled", "user": {"login": "Sabutobi", "id": 12499311, "node_id": "MDQ6VXNlcjEyNDk5MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/12499311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sabutobi", "html_url": "https://github.com/Sabutobi", "followers_url": "https://api.github.com/users/Sabutobi/followers", "following_url": "https://api.github.com/users/Sabutobi/following{/other_user}", "gists_url": "https://api.github.com/users/Sabutobi/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sabutobi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sabutobi/subscriptions", "organizations_url": "https://api.github.com/users/Sabutobi/orgs", "repos_url": "https://api.github.com/users/Sabutobi/repos", "events_url": "https://api.github.com/users/Sabutobi/events{/privacy}", "received_events_url": "https://api.github.com/users/Sabutobi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-06-10T12:26:02Z", "updated_at": "2020-06-15T10:39:43Z", "closed_at": "2020-06-15T10:39:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, all.\r\nMy code:\r\nConsumer settings:\r\n```\r\nfrom kafka import KafkaConsumer\r\nfrom kafka.coordinator.assignors.roundrobin import RoundRobinPartitionAssignor\r\n\r\nconsumer = KafkaConsumer(\r\n    bootstrap_servers=kafka_host + \":\" + kafka_port,\r\n    security_protocol='PLAINTEXT',\r\n    group_id='messages',\r\n    partition_assignment_strategy=[RoundRobinPartitionAssignor],\r\n    auto_offset_reset='latest',\r\n    session_timeout_ms=60000,\r\n    enable_auto_commit=True,\r\n    max_poll_records=100\r\n)\r\n```\r\nConsumer execution:\r\n```consumer.subscribe(config['topic_que']['topic'])\r\nfor msg in consumer:\r\n        message = json.loads(msg.value.decode(\"utf-8\"))\r\n        if message['msg'] == config['some']['message']:\r\n            # do something\r\n            # nothing else there about kafka consumer\r\n```\r\nNot default settings in kafka broker:\r\n```\r\ngroup.max.session.timeout.ms = 70000\r\n```\r\n\r\nProblem:\r\nDone the \r\n``` docker-compose up -d --build --scale my-service=3 ```\r\nIn another word, we have 3 Consumers that using the same group_id\r\nFrom the first looks like all works fine messages are shared between several consumers:\r\n```\r\nmy-service_1  | 10:07:13.902 INFO     __main__:33  --- Message from kafka - some my message\r\nmy-service_2  | 10:07:13.902 INFO     __main__:33  --- Message from kafka - some my message2\r\nmy-service_3  | 10:07:13.902 INFO     __main__:33  --- Message from kafka - some my message3\r\n```\r\nApprox. The number of messages is 100. Time of execution of logic that was activated by messages from 30 sec to 5-6 minutes.\r\nAfter some time of execution I can see:\r\n```\r\nmy-service_1  | 11:09:18.469 INFO     __main__:33  --- Message from kafka - some my message\r\nmy-service_2  | 11:45:39.202 INFO     __main__:33  --- Message from kafka - some my message2\r\nmy-service_3  | 10:37:13.902 INFO     __main__:33  --- Message from kafka - some my message3\r\n```\r\nThat is the same message in the same scaled service. \r\nI've checked for duplicates in kafka-broker-topics - all are **unique**.\r\nIn other words looks like that messages haven't marked as processed. And that services can run infinite time and will process those messages again and again.\r\nNote: if I add `consumer.commit()` in the end.\r\nI'll receive error:\r\n```\r\nCommitFailedError: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max_poll_interval_ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the rebalance timeout with max_poll_interval_ms, or by reducing the maximum size of batches returned in poll() with max_poll_records.\r\n```\r\n\r\nMaybe I've set something wrong? Seeking for some fresh ideas. Thanks a lot for your attention to this issue.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2054", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2054/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2054/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2054/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2054", "id": 623820872, "node_id": "MDU6SXNzdWU2MjM4MjA4NzI=", "number": 2054, "title": "Feature request: get topic partition metadata", "user": {"login": "qinghui-xu", "id": 5479630, "node_id": "MDQ6VXNlcjU0Nzk2MzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5479630?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qinghui-xu", "html_url": "https://github.com/qinghui-xu", "followers_url": "https://api.github.com/users/qinghui-xu/followers", "following_url": "https://api.github.com/users/qinghui-xu/following{/other_user}", "gists_url": "https://api.github.com/users/qinghui-xu/gists{/gist_id}", "starred_url": "https://api.github.com/users/qinghui-xu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qinghui-xu/subscriptions", "organizations_url": "https://api.github.com/users/qinghui-xu/orgs", "repos_url": "https://api.github.com/users/qinghui-xu/repos", "events_url": "https://api.github.com/users/qinghui-xu/events{/privacy}", "received_events_url": "https://api.github.com/users/qinghui-xu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-24T08:36:07Z", "updated_at": "2020-05-24T20:36:36Z", "closed_at": "2020-05-24T20:36:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "We are currently using kafka-python 0.9.5, in which we can get all the topic partition metadata from the public API.\r\nBut it seems this is no more public in the latest version, which blocks us from upgrading the version.\r\nIs it possible to have this feature back? Or is there a reason not to have it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2053", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2053/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2053/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2053/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2053", "id": 621294601, "node_id": "MDU6SXNzdWU2MjEyOTQ2MDE=", "number": 2053, "title": "How to specify TLS version", "user": {"login": "ErikBrewster", "id": 9663737, "node_id": "MDQ6VXNlcjk2NjM3Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/9663737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ErikBrewster", "html_url": "https://github.com/ErikBrewster", "followers_url": "https://api.github.com/users/ErikBrewster/followers", "following_url": "https://api.github.com/users/ErikBrewster/following{/other_user}", "gists_url": "https://api.github.com/users/ErikBrewster/gists{/gist_id}", "starred_url": "https://api.github.com/users/ErikBrewster/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ErikBrewster/subscriptions", "organizations_url": "https://api.github.com/users/ErikBrewster/orgs", "repos_url": "https://api.github.com/users/ErikBrewster/repos", "events_url": "https://api.github.com/users/ErikBrewster/events{/privacy}", "received_events_url": "https://api.github.com/users/ErikBrewster/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-19T21:24:33Z", "updated_at": "2020-06-08T14:05:06Z", "closed_at": "2020-06-05T20:24:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have several devices trying to connect to my brokers. Most of them work, but some are getting rejected and I can see reset packets in wireshark that show it is communicating on TLS1.0 and getting rejected by our server because it only allows TLS1.2. We are deploying in containers, so the environment that the different devices run should be the same, except for the certs and stuff like UUIDs. I don't understand where to set the allowed TLS version in config.\r\n\r\nIf I edit the library and change (line 465, conn.py) from:\r\n            self._ssl_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)  # pylint: disable=no-member\r\nto\r\n            self._ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)  # pylint: disable=no-member\r\n\r\nIt appears to connect ok. It seems like this is something that could be changed in config -- perhaps in ssl_context = ??? when initializing the producer. Any tips on how to specify TLS 1.2 only?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2050", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2050/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2050/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2050/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2050", "id": 619891929, "node_id": "MDU6SXNzdWU2MTk4OTE5Mjk=", "number": 2050, "title": "KafkaTimeoutError:", "user": {"login": "skskcco2o17", "id": 34002271, "node_id": "MDQ6VXNlcjM0MDAyMjcx", "avatar_url": "https://avatars2.githubusercontent.com/u/34002271?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skskcco2o17", "html_url": "https://github.com/skskcco2o17", "followers_url": "https://api.github.com/users/skskcco2o17/followers", "following_url": "https://api.github.com/users/skskcco2o17/following{/other_user}", "gists_url": "https://api.github.com/users/skskcco2o17/gists{/gist_id}", "starred_url": "https://api.github.com/users/skskcco2o17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skskcco2o17/subscriptions", "organizations_url": "https://api.github.com/users/skskcco2o17/orgs", "repos_url": "https://api.github.com/users/skskcco2o17/repos", "events_url": "https://api.github.com/users/skskcco2o17/events{/privacy}", "received_events_url": "https://api.github.com/users/skskcco2o17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-18T04:04:41Z", "updated_at": "2020-05-27T20:31:57Z", "closed_at": "2020-05-27T20:31:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "My code:\r\n\r\n```\r\ndef kafka_producer():\r\n     producer = KafkaProducer(bootstrap_servers='{0}:{1}'.format(HOST, PORT),\r\n                         api_version=(0, 10, 1)\r\n                         )\r\n   try:\r\n       future = producer.send('test', b'hello kafka')\r\n       producer.flush()\r\n       record_metadata = future.get(timeout=10)\r\n       print(record_metadata)\r\nexcept KafkaError as e:\r\n       print(e)    \r\n```\r\n\r\n \r\n \r\nMy Kafka Broker (v 2.4.0) is running in OpenShift container. My Python producer code is similar to the above and that is causing the below issue.\r\n\r\n```\r\nKafkaTimeoutError: Batch for TopicPartition(topic='test', partition=0) containing 1 record(s) expired: 31 seconds have passed since batch creation plus linger time\r\n   File \"C:\\Users\\XXXXXXX\\Anaconda3\\lib\\site-packages\\kafka\\producer\\future.py\", line 65, in get\r\n    raise self.exception # pylint: disable-msg=raising-bad-type\r\n```\r\n\r\nMy kafka-python version is 2.0.1\r\n\r\nI am fighting on the issue for last two days. I am frustrated. Please assist how do you resolve it / why does it occur?\r\n\r\nI have tested using the Kafka sh script to send the topic it's working. So where is the problem in python?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2037", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2037/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2037/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2037/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2037", "id": 598467394, "node_id": "MDU6SXNzdWU1OTg0NjczOTQ=", "number": 2037, "title": "capturing producer/consumer performance metrics", "user": {"login": "shakti-garg", "id": 5525521, "node_id": "MDQ6VXNlcjU1MjU1MjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5525521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shakti-garg", "html_url": "https://github.com/shakti-garg", "followers_url": "https://api.github.com/users/shakti-garg/followers", "following_url": "https://api.github.com/users/shakti-garg/following{/other_user}", "gists_url": "https://api.github.com/users/shakti-garg/gists{/gist_id}", "starred_url": "https://api.github.com/users/shakti-garg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shakti-garg/subscriptions", "organizations_url": "https://api.github.com/users/shakti-garg/orgs", "repos_url": "https://api.github.com/users/shakti-garg/repos", "events_url": "https://api.github.com/users/shakti-garg/events{/privacy}", "received_events_url": "https://api.github.com/users/shakti-garg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-12T11:45:58Z", "updated_at": "2020-04-23T16:24:49Z", "closed_at": "2020-04-18T05:44:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have written couple of producers/consumers but i want to capture their performance metrics in prometheus(or similar monitoring tool) for the sake of monitoring.\r\nI have earlier done this using JMX on java-api for kafka but needs your guidance for this. I do see metrics.py in the code but not sure how to use this.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2034", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2034/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2034/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2034/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2034", "id": 597168842, "node_id": "MDU6SXNzdWU1OTcxNjg4NDI=", "number": 2034, "title": "Pull the information several times, the first time is empty, after several times can get the information", "user": {"login": "ghleilei", "id": 45966474, "node_id": "MDQ6VXNlcjQ1OTY2NDc0", "avatar_url": "https://avatars1.githubusercontent.com/u/45966474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghleilei", "html_url": "https://github.com/ghleilei", "followers_url": "https://api.github.com/users/ghleilei/followers", "following_url": "https://api.github.com/users/ghleilei/following{/other_user}", "gists_url": "https://api.github.com/users/ghleilei/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghleilei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghleilei/subscriptions", "organizations_url": "https://api.github.com/users/ghleilei/orgs", "repos_url": "https://api.github.com/users/ghleilei/repos", "events_url": "https://api.github.com/users/ghleilei/events{/privacy}", "received_events_url": "https://api.github.com/users/ghleilei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-09T10:02:34Z", "updated_at": "2020-04-11T04:56:55Z", "closed_at": "2020-04-11T04:56:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "hi\uff0c@dpkp\r\nRun the consumer code, pull the information several times, get it null the first time, and get it several timeshis is the consumer code. Why?\r\nThis is the consumer code\r\n\r\ndef consumer():\r\n    consumer = KafkaConsumer(kafka_topic,\r\n                             bootstrap_servers=server ,\r\n                             auto_offset_reset='earliest',\r\n                             enable_auto_commit=False, \r\n                             group_id='kafka-test-1',\r\n                             consumer_timeout_ms=10000,           \r\n                             value_deserializer = bytes.decode\r\n                             )\r\n    for _ in range(3):\r\n        datass = consumer.poll(timeout_ms=5,max_records=10) \r\n        print(datass)\r\n        consumer.commit()", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2032", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2032/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2032/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2032/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2032", "id": 596145320, "node_id": "MDU6SXNzdWU1OTYxNDUzMjA=", "number": 2032, "title": "Not able to install python-snappy in python virtual env", "user": {"login": "AnamikaN", "id": 25681678, "node_id": "MDQ6VXNlcjI1NjgxNjc4", "avatar_url": "https://avatars3.githubusercontent.com/u/25681678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnamikaN", "html_url": "https://github.com/AnamikaN", "followers_url": "https://api.github.com/users/AnamikaN/followers", "following_url": "https://api.github.com/users/AnamikaN/following{/other_user}", "gists_url": "https://api.github.com/users/AnamikaN/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnamikaN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnamikaN/subscriptions", "organizations_url": "https://api.github.com/users/AnamikaN/orgs", "repos_url": "https://api.github.com/users/AnamikaN/repos", "events_url": "https://api.github.com/users/AnamikaN/events{/privacy}", "received_events_url": "https://api.github.com/users/AnamikaN/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-07T20:53:33Z", "updated_at": "2020-04-08T18:16:01Z", "closed_at": "2020-04-08T18:16:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "i am trying to install python-snappy in virtualenv, but it is failing with below error-\r\n\r\n Building wheels for collected packages: python-snappy, pypng, simplegeneric, networkx, scandir\r\n  Building wheel for python-snappy (setup.py): started\r\n  Building wheel for python-snappy (setup.py): finished with status 'error'\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /opt/jenkins/virtualenv/kafka_test_consumer/venv/bin/python2 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-f3ITZ2/python-snappy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-f3ITZ2/python-snappy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-ijETNP\r\n       cwd: /tmp/pip-install-f3ITZ2/python-snappy/\r\n  Complete output (22 lines):\r\n  /usr/lib64/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'cffi_modules'\r\n    warnings.warn(msg)\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-2.7\r\n  creating build/lib.linux-x86_64-2.7/snappy\r\n  copying snappy/snappy.py -> build/lib.linux-x86_64-2.7/snappy\r\n  copying snappy/snappy_cffi.py -> build/lib.linux-x86_64-2.7/snappy\r\n  copying snappy/__init__.py -> build/lib.linux-x86_64-2.7/snappy\r\n  copying snappy/hadoop_snappy.py -> build/lib.linux-x86_64-2.7/snappy\r\n  copying snappy/snappy_formats.py -> build/lib.linux-x86_64-2.7/snappy\r\n  copying snappy/__main__.py -> build/lib.linux-x86_64-2.7/snappy\r\n  copying snappy/snappy_cffi_builder.py -> build/lib.linux-x86_64-2.7/snappy\r\n  running build_ext\r\n  building 'snappy._snappy' extension\r\n  creating build/temp.linux-x86_64-2.7\r\n  creating build/temp.linux-x86_64-2.7/snappy\r\n  gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python2.7 -c snappy/snappymodule.cc -o build/temp.linux-x86_64-2.7/snappy/snappymodule.o\r\n  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\n  error: command 'gcc' failed with exit status 1\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for python-snappy\r\n  Running setup.py clean for python-snappy\r\n  Building wheel for pypng (setup.py): started\r\n  Building wheel for pypng (setup.py): finished with status 'done'\r\n  Created wheel for pypng: filename=pypng-0.0.20-py2-none-any.whl size=67161 sha256=1e385527e5e5d465524bf6061d546e95d12599403c5c2bb856d2f8e1b65b064d\r\n  Stored in directory: /tmp/kafka_test_consumer/wheels/fa/04/79/a6bdad8921c5097e2b8b1fa7d2d848f3b1b14706ecad468dbf\r\n  Building wheel for simplegeneric (setup.py): started\r\n  Building wheel for simplegeneric (setup.py): finished with status 'done'\r\n  Created wheel for simplegeneric: filename=simplegeneric-0.8.1-py2-none-any.whl size=5075 sha256=64848bd310381abc011cb0dc255ff2355a41a16c20ada3b205d792a57f0486d4\r\n  Stored in directory: /tmp/kafka_test_consumer/wheels/1b/83/23/9d866729c0090bfc62d03a5bd43080fb07a5f4fad10a09399a\r\n  Building wheel for networkx (setup.py): started\r\n  Building wheel for networkx (setup.py): finished with status 'done'\r\n  Created wheel for networkx: filename=networkx-2.2-py2.py3-none-any.whl size=1527321 sha256=1fbdf79b359a39e7b8e12812868ab00bff9afcf06e58f15040a8501f5f95276b\r\n  Stored in directory: /tmp/kafka_test_consumer/wheels/df/80/48/106e63760ff0dcd3658613d93c1ecf64301b9261172f2c1acf\r\n  Building wheel for scandir (setup.py): started\r\n  Building wheel for scandir (setup.py): finished with status 'done'\r\n  Created wheel for scandir: filename=scandir-1.10.0-cp27-cp27mu-linux_x86_64.whl size=36072 sha256=83b5355bf7d386fa97f57f20386ebf083ae4c45863c719f09cf1cb0bb81429da\r\n  Stored in directory: /tmp/kafka_test_consumer/wheels/58/2c/26/52406f7d1f19bcc47a6fbd1037a5f293492f5cf1d58c539edb\r\nSuccessfully built pypng simplegeneric networkx scandir\r\nFailed to build python-snappy\r\nInstalling collected packages: pytz, argparse, logging, certifi, chardet, idna, urllib3, requests, kafka-python, future, lz4, kafka, pypng, six, cypari, plink, FXrays, decorator, ipython-genutils, enum34, traitlets, backports.shutil-get-terminal-size, scandir, pathlib2, ptyprocess, pexpect, wcwidth, prompt-toolkit, simplegeneric, pickleshare, pygments, ipython, snappy-manifolds, networkx, spherogram, snappy, python-snappy\r\n    Running setup.py install for python-snappy: started\r\n    Running setup.py install for python-snappy: finished with status 'error'\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /opt/jenkins/virtualenv/kafka_test_consumer/venv/bin/python2 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-f3ITZ2/python-snappy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-f3ITZ2/python-snappy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-CWxoOH/install-record.txt --single-version-externally-managed --compile --install-headers /opt/jenkins/virtualenv/kafka_test_consumer/venv/include/site/python2.7/python-snappy\r\n         cwd: /tmp/pip-install-f3ITZ2/python-snappy/\r\n    Complete output (22 lines):\r\n    /usr/lib64/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'cffi_modules'\r\n      warnings.warn(msg)\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.linux-x86_64-2.7\r\n    creating build/lib.linux-x86_64-2.7/snappy\r\n    copying snappy/snappy.py -> build/lib.linux-x86_64-2.7/snappy\r\n    copying snappy/snappy_cffi.py -> build/lib.linux-x86_64-2.7/snappy\r\n    copying snappy/__init__.py -> build/lib.linux-x86_64-2.7/snappy\r\n    copying snappy/hadoop_snappy.py -> build/lib.linux-x86_64-2.7/snappy\r\n    copying snappy/snappy_formats.py -> build/lib.linux-x86_64-2.7/snappy\r\n    copying snappy/__main__.py -> build/lib.linux-x86_64-2.7/snappy\r\n    copying snappy/snappy_cffi_builder.py -> build/lib.linux-x86_64-2.7/snappy\r\n    running build_ext\r\n    building 'snappy._snappy' extension\r\n    creating build/temp.linux-x86_64-2.7\r\n    creating build/temp.linux-x86_64-2.7/snappy\r\n    gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python2.7 -c snappy/snappymodule.cc -o build/temp.linux-x86_64-2.7/snappy/snappymodule.o\r\n    gcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\n    error: command 'gcc' failed with exit status 1\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: /opt/jenkins/virtualenv/kafka_test_consumer/venv/bin/python2 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-f3ITZ2/python-snappy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-f3ITZ2/python-snappy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-CWxoOH/install-record.txt --single-version-externally-managed --compile --install-headers /opt/jenkins/virtualenv/kafka_test_consumer/venv/include/site/python2.7/python-snappy Check the logs for full command output.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2031", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2031/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2031/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2031/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2031", "id": 596102333, "node_id": "MDU6SXNzdWU1OTYxMDIzMzM=", "number": 2031, "title": "not able to install snappy", "user": {"login": "AnamikaN", "id": 25681678, "node_id": "MDQ6VXNlcjI1NjgxNjc4", "avatar_url": "https://avatars3.githubusercontent.com/u/25681678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnamikaN", "html_url": "https://github.com/AnamikaN", "followers_url": "https://api.github.com/users/AnamikaN/followers", "following_url": "https://api.github.com/users/AnamikaN/following{/other_user}", "gists_url": "https://api.github.com/users/AnamikaN/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnamikaN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnamikaN/subscriptions", "organizations_url": "https://api.github.com/users/AnamikaN/orgs", "repos_url": "https://api.github.com/users/AnamikaN/repos", "events_url": "https://api.github.com/users/AnamikaN/events{/privacy}", "received_events_url": "https://api.github.com/users/AnamikaN/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-07T19:33:53Z", "updated_at": "2020-04-08T18:15:38Z", "closed_at": "2020-04-08T18:15:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "i am trying to install snappy using ocmmands as mentioned on link  https://kafka-python.readthedocs.io/en/master/install.html#optional-snappy-install, but gettng below error. Any thoughts?\r\n\r\nconfig.status: executing libtool commands\r\n[root@slave snappy-1.1.3]# make\r\nmake all-am\r\nmake[1]: Entering directory `/home/user/snappy-1.1.3'\r\nsource='snappy.cc' object='snappy.lo' libtool=yes \\\r\nDEPDIR=.deps depmode=none /bin/sh ./depcomp \\\r\n/bin/sh ./libtool --tag=CXX --mode=compile g++ -DHAVE_CONFIG_H -I. -c -o snappy.lo snappy.cc\r\nlibtool: compile: g++ -DHAVE_CONFIG_H -I. -c snappy.cc -o .libs/snappy.o\r\n./libtool: line 1125: g++: command not found\r\nmake[1]: *** [snappy.lo] Error 1\r\nmake[1]: Leaving directory `/home/user/snappy-1.1.3'\r\nmake: *** [all] Error 2\r\n[root@slave snappy-1.1.3]# sudo make install\r\nsource='snappy.cc' object='snappy.lo' libtool=yes \\\r\nDEPDIR=.deps depmode=none /bin/sh ./depcomp \\\r\n/bin/sh ./libtool --tag=CXX --mode=compile g++ -DHAVE_CONFIG_H -I. -c -o snappy.lo snappy.cc\r\nlibtool: compile: g++ -DHAVE_CONFIG_H -I. -c snappy.cc -o .libs/snappy.o\r\n./libtool: line 1125: g++: command not found\r\nmake: *** [snappy.lo] Error 1\r\n[root@slave snappy-1.1.3]# make\r\nmake all-am\r\nmake[1]: Entering directory `/home/user/snappy-1.1.3'\r\nsource='snappy.cc' object='snappy.lo' libtool=yes \\\r\nDEPDIR=.deps depmode=none /bin/sh ./depcomp \\\r\n/bin/sh ./libtool --tag=CXX --mode=compile g++ -DHAVE_CONFIG_H -I. -c -o snappy.lo snappy.cc\r\nlibtool: compile: g++ -DHAVE_CONFIG_H -I. -c snappy.cc -o .libs/snappy.o\r\n./libtool: line 1125: g++: command not found\r\nmake[1]: *** [snappy.lo] Error 1\r\nmake[1]: Leaving directory `/home/user/snappy-1.1.3'\r\nmake: *** [all] Error 2\r\n[root@slav snappy-1.1.3]#", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2026", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2026/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2026/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2026/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2026", "id": 587322993, "node_id": "MDU6SXNzdWU1ODczMjI5OTM=", "number": 2026, "title": "Not fetching complete OffsetAndMetadata information", "user": {"login": "AnamikaN", "id": 25681678, "node_id": "MDQ6VXNlcjI1NjgxNjc4", "avatar_url": "https://avatars3.githubusercontent.com/u/25681678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnamikaN", "html_url": "https://github.com/AnamikaN", "followers_url": "https://api.github.com/users/AnamikaN/followers", "following_url": "https://api.github.com/users/AnamikaN/following{/other_user}", "gists_url": "https://api.github.com/users/AnamikaN/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnamikaN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnamikaN/subscriptions", "organizations_url": "https://api.github.com/users/AnamikaN/orgs", "repos_url": "https://api.github.com/users/AnamikaN/repos", "events_url": "https://api.github.com/users/AnamikaN/events{/privacy}", "received_events_url": "https://api.github.com/users/AnamikaN/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-24T22:41:18Z", "updated_at": "2020-03-27T22:02:23Z", "closed_at": "2020-03-27T22:02:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "when i run CLI command to describe consumer group, i get below data\r\nGROUP                   TOPIC                                      PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\r\ndata-connect-dev3-group dev.dcSacramento.sds.resourceManager       4          2834            2861            27              -               -               -\r\n\r\nBut When i run 'list_consumer_group_offsets' mthod. it pulls below data for given consumer group.\r\nTopicPartition(topic=u'dev.dcSacramento.sds.resourceManager', partition=5): OffsetAndMetadata(offset=2833, metadata=u'')\r\n\r\nBasically with CLI command, i get additional details of LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID. \r\n\r\nAny suggestions, how to fetch this info using api?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2023", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2023/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2023/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2023/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2023", "id": 584480161, "node_id": "MDU6SXNzdWU1ODQ0ODAxNjE=", "number": 2023, "title": "Kafka-python do not send message from Airflow dag", "user": {"login": "Sabutobi", "id": 12499311, "node_id": "MDQ6VXNlcjEyNDk5MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/12499311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sabutobi", "html_url": "https://github.com/Sabutobi", "followers_url": "https://api.github.com/users/Sabutobi/followers", "following_url": "https://api.github.com/users/Sabutobi/following{/other_user}", "gists_url": "https://api.github.com/users/Sabutobi/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sabutobi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sabutobi/subscriptions", "organizations_url": "https://api.github.com/users/Sabutobi/orgs", "repos_url": "https://api.github.com/users/Sabutobi/repos", "events_url": "https://api.github.com/users/Sabutobi/events{/privacy}", "received_events_url": "https://api.github.com/users/Sabutobi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-19T15:06:39Z", "updated_at": "2020-03-20T12:07:58Z", "closed_at": "2020-03-20T12:04:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, all. This is the kafka-docker.\r\n```\r\nversion: '2'\r\nservices:\r\n  zookeeper:\r\n    image: wurstmeister/zookeeper:3.4.6\r\n    ports:\r\n     - \"2181:2181\"\r\n  kafka:\r\n    image: wurstmeister/kafka\r\n    container_name: kafka_container\r\n    ports:\r\n        - \"9092:9092\"                               # expose port\r\n    environment:\r\n        KAFKA_ADVERTISED_HOST_NAME: kafka           # specify the docker host IP at which other containers can reach the broker\r\n        KAFKA_CREATE_TOPICS: 'SomeTopic:1:1,SomeTopic2:1:1'\r\n        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181     # specify where the broker can reach Zookeeper\r\n        KAFKA_LISTENERS: PLAINTEXT://:9092          # the list of addresses on which the Kafka broker will listen on for incoming connections.\r\n        KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092  # Kafka sends the value of this variable to clients during their connection. After receiving that value, the clients use it for sending/consuming records to/from the Kafka broker.y connect to it.\r\n    volumes:\r\n        - /var/run/docker.sock:/var/run/docker.sock\r\n```\r\nThat is the airflow container:\r\n```\r\nversion: '3.0'\r\nservices:\r\n    postgres:\r\n        image: postgres:9.6\r\n        environment:\r\n            - POSTGRES_USER=airflow\r\n            - POSTGRES_PASSWORD=airflow\r\n            - POSTGRES_DB=airflow\r\n        networks:\r\n            - network-mongodb-service_some-nw\r\n\r\n        logging:\r\n            options:\r\n                max-size: 10m\r\n                max-file: \"3\"\r\n\r\n    webserver:\r\n        build:\r\n            context: .\r\n        image: puckel/docker-airflow:1.10.9\r\n        restart: always\r\n        depends_on:\r\n            - postgres\r\n        environment:\r\n            - LOAD_EX=n\r\n            - EXECUTOR=Local\r\n            - whatever=${somevar}\r\n        logging:\r\n            options:\r\n                max-size: 10m\r\n                max-file: \"3\"\r\n        volumes:\r\n            - ./dags:/usr/local/airflow/dags\r\n            - ./requirements.txt:/requirements.txt\r\n        networks:\r\n            - network-mongodb-service_some-nw\r\n            - kafka_default\r\n        external_links:\r\n            - somelink:${somelink}\r\n            - kafka_container:kafka\r\n        ports:\r\n            - \"8080:8080\"\r\n        command: webserver\r\n        healthcheck:\r\n            test: [\"CMD-SHELL\", \"[ -f /usr/local/airflow/airflow-webserver.pid ]\"]\r\n            interval: 30s\r\n            timeout: 30s\r\n            retries: 3\r\n\r\nnetworks:\r\n    network-mongodb-service_some-nw:\r\n        external: true\r\n    kafka_default:\r\n        external: true\r\n```\r\nHow that works if just connect via `docker exec -it contairerId bash`?\r\n```\r\n>>> from kafka import KafkaProducer\r\n>>> kafka_host = 'kafka:9092'\r\n>>> producer = KafkaProducer(bootstrap_servers=kafka_host,acks='all',retries=0)\r\n>>> producer.send(self.where_to_send,\r\n...                              self.encoded_payload)\r\n```\r\nWorks fine and no problem met. But:\r\nIf I'll add identical code to the airflow dag:\r\n```\r\nSending (key=None value=b'{\"topic\": \"topic\", \"msg\": \"message\", \"domainName\": \"somedomain.com\"}' headers=[]) to TopicPartition(topic='sometopic', partition=0)\r\nAllocating a new 16384 byte message buffer for TopicPartition(topic='Webtraffic', partition=0)\r\nWaking up the sender since TopicPartition(topic='Webtraffic', partition=0) is either full or getting a new batch\r\n[2020-03-19 15:01:03,994] {{conn.py:378}} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('192.168.32.2', 9092)]>: connecting to kafka:9092 [('192.168.32.2', 9092) IPv4]\r\n[2020-03-19 15:01:03,995] {{conn.py:1195}} INFO - Probing node bootstrap-0 broker version\r\n[2020-03-19 15:01:03,995] {{conn.py:407}} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('192.168.32.2', 9092)]>: Connection complete.\r\n[2020-03-19 15:01:04,102] {{conn.py:1257}} INFO - Broker version identified as 1.0.0\r\n[2020-03-19 15:01:04,103] {{conn.py:1259}} INFO - Set configuration api_version=(1, 0, 0) to skip auto check_version requests on startup\r\nReceived correlation id: 3\r\n[2020-03-19 15:01:04,113] {{parser.py:139}} DEBUG - Received correlation id: 3\r\nProcessing response MetadataResponse_v1\r\n[2020-03-19 15:01:04,115] {{parser.py:166}} DEBUG - Processing response MetadataResponse_v1\r\n<BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('192.168.32.2', 9092)]> Response 3 (10.233640670776367 ms): MetadataResponse_v1(brokers=[(node_id=1001, host='kafka', port=9092, rack=None)], controller_id=1001, topics=[(error_code=0, topic='Social', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='Webtraffic', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='__consumer_offsets', is_internal=True, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=10, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=20, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=40, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=30, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=9, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=39, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=11, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=31, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=13, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=18, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=22, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=8, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=32, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=43, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=29, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=34, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=1, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=6, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=41, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=27, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=48, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=5, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=15, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=35, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=25, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=46, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=26, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=36, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=44, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=16, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=37, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=17, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=45, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=3, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=4, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=24, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=38, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=33, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=23, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=28, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=2, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=12, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=19, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=14, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=47, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=49, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=42, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=7, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=21, leader=1001, replicas=[1001], isr=[1001])])])\r\n[2020-03-19 15:01:04,121] {{conn.py:1071}} DEBUG - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('192.168.32.2', 9092)]> Response 3 (10.233640670776367 ms): MetadataResponse_v1(brokers=[(node_id=1001, host='kafka', port=9092, rack=None)], controller_id=1001, topics=[(error_code=0, topic='Social', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='Webtraffic', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='__consumer_offsets', is_internal=True, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=10, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=20, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=40, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=30, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=9, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=39, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=11, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=31, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=13, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=18, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=22, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=8, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=32, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=43, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=29, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=34, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=1, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=6, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=41, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=27, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=48, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=5, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=15, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=35, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=25, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=46, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=26, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=36, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=44, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=16, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=37, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=17, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=45, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=3, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=4, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=24, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=38, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=33, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=23, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=28, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=2, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=12, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=19, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=14, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=47, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=49, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=42, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=7, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=21, leader=1001, replicas=[1001], isr=[1001])])])\r\nUpdated cluster metadata to ClusterMetadata(brokers: 1, topics: 3, groups: 0)\r\n[2020-03-19 15:01:04,132] {{cluster.py:325}} DEBUG - Updated cluster metadata to ClusterMetadata(brokers: 1, topics: 3, groups: 0)\r\n```\r\nAnd nothing happened. Message not sent. No errors. Nothing. Maybe you'll have some fresh ideas what I've done wrong?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2022", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2022/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2022/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2022/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2022", "id": 583142456, "node_id": "MDU6SXNzdWU1ODMxNDI0NTY=", "number": 2022, "title": "kafka 1.3.5 vs kafka-python 2.0.1", "user": {"login": "fortesp", "id": 10762489, "node_id": "MDQ6VXNlcjEwNzYyNDg5", "avatar_url": "https://avatars2.githubusercontent.com/u/10762489?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fortesp", "html_url": "https://github.com/fortesp", "followers_url": "https://api.github.com/users/fortesp/followers", "following_url": "https://api.github.com/users/fortesp/following{/other_user}", "gists_url": "https://api.github.com/users/fortesp/gists{/gist_id}", "starred_url": "https://api.github.com/users/fortesp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fortesp/subscriptions", "organizations_url": "https://api.github.com/users/fortesp/orgs", "repos_url": "https://api.github.com/users/fortesp/repos", "events_url": "https://api.github.com/users/fortesp/events{/privacy}", "received_events_url": "https://api.github.com/users/fortesp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-17T16:32:45Z", "updated_at": "2020-03-19T05:14:30Z", "closed_at": "2020-03-19T05:14:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello guys,\r\nI am just a bit confused and i would like someone to help me understand why there are two packages registered in PyPi with different names. I first thought it was from different developers, but that is obviously not the case.. \r\nI believe the maintainable package is the kafka-python by looking into the release history.\r\nStill i have to ask, whats the purpose of the other one? :)\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2017", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2017/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2017/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2017/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2017", "id": 580449276, "node_id": "MDU6SXNzdWU1ODA0NDkyNzY=", "number": 2017, "title": "can not get message in previous rounds when poll message by timer", "user": {"login": "peter-wang-wsl", "id": 15857181, "node_id": "MDQ6VXNlcjE1ODU3MTgx", "avatar_url": "https://avatars2.githubusercontent.com/u/15857181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peter-wang-wsl", "html_url": "https://github.com/peter-wang-wsl", "followers_url": "https://api.github.com/users/peter-wang-wsl/followers", "following_url": "https://api.github.com/users/peter-wang-wsl/following{/other_user}", "gists_url": "https://api.github.com/users/peter-wang-wsl/gists{/gist_id}", "starred_url": "https://api.github.com/users/peter-wang-wsl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peter-wang-wsl/subscriptions", "organizations_url": "https://api.github.com/users/peter-wang-wsl/orgs", "repos_url": "https://api.github.com/users/peter-wang-wsl/repos", "events_url": "https://api.github.com/users/peter-wang-wsl/events{/privacy}", "received_events_url": "https://api.github.com/users/peter-wang-wsl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-13T08:34:58Z", "updated_at": "2020-03-15T12:23:24Z", "closed_at": "2020-03-15T05:13:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I create KafkaConsumer with group id, and assign topic. When use below code to get message , I can not get message in preivous 5 rounds\r\n```\r\nwhile True:\r\n    consumer.poll()\r\n    time.sleep(1)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2016", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2016/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2016/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2016/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2016", "id": 580410849, "node_id": "MDU6SXNzdWU1ODA0MTA4NDk=", "number": 2016, "title": "Consumer group creation ", "user": {"login": "karthikziffer", "id": 24503303, "node_id": "MDQ6VXNlcjI0NTAzMzAz", "avatar_url": "https://avatars3.githubusercontent.com/u/24503303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karthikziffer", "html_url": "https://github.com/karthikziffer", "followers_url": "https://api.github.com/users/karthikziffer/followers", "following_url": "https://api.github.com/users/karthikziffer/following{/other_user}", "gists_url": "https://api.github.com/users/karthikziffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/karthikziffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karthikziffer/subscriptions", "organizations_url": "https://api.github.com/users/karthikziffer/orgs", "repos_url": "https://api.github.com/users/karthikziffer/repos", "events_url": "https://api.github.com/users/karthikziffer/events{/privacy}", "received_events_url": "https://api.github.com/users/karthikziffer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-13T07:02:28Z", "updated_at": "2020-03-13T19:51:54Z", "closed_at": "2020-03-13T19:51:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI want to create a consumer group in python-kafka.\r\n\r\nI am using two consumers. One consumer(consumer1) fetches event from specific offset using seek and the second consumer(consumer2) uses subscribe and poll to stream events from consumer1. By mentioning both the consumer to same consumer group, consumer2 can fetch the last committed offset and start polling.  \r\n\r\nCurrently, I tried passing a string to group_id parameter in KafkaConsumer. However it throws an exception as \"Unassigned Partitions\". Can anyone share some resources of assigning two consumer to same group? \r\n\r\n[![Screenshot-2020-03-13-at-12-34-56-PM.png](https://i.postimg.cc/Kc5LrPc0/Screenshot-2020-03-13-at-12-34-56-PM.png)](https://postimg.cc/pyppPnFj)\r\n\r\n\r\n\r\n[![Screenshot-2020-03-13-at-12-06-44-PM.png](https://i.postimg.cc/kGSpzTjR/Screenshot-2020-03-13-at-12-06-44-PM.png)](https://postimg.cc/XBVLyk6n)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2014", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2014/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2014/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2014/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2014", "id": 577355137, "node_id": "MDU6SXNzdWU1NzczNTUxMzc=", "number": 2014, "title": "[Contribution]Example : Integration of Kafka with Gunicorn & Gevent", "user": {"login": "mohammedvaghjipurwala", "id": 20841625, "node_id": "MDQ6VXNlcjIwODQxNjI1", "avatar_url": "https://avatars0.githubusercontent.com/u/20841625?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mohammedvaghjipurwala", "html_url": "https://github.com/mohammedvaghjipurwala", "followers_url": "https://api.github.com/users/mohammedvaghjipurwala/followers", "following_url": "https://api.github.com/users/mohammedvaghjipurwala/following{/other_user}", "gists_url": "https://api.github.com/users/mohammedvaghjipurwala/gists{/gist_id}", "starred_url": "https://api.github.com/users/mohammedvaghjipurwala/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mohammedvaghjipurwala/subscriptions", "organizations_url": "https://api.github.com/users/mohammedvaghjipurwala/orgs", "repos_url": "https://api.github.com/users/mohammedvaghjipurwala/repos", "events_url": "https://api.github.com/users/mohammedvaghjipurwala/events{/privacy}", "received_events_url": "https://api.github.com/users/mohammedvaghjipurwala/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-07T16:06:47Z", "updated_at": "2020-03-23T09:33:18Z", "closed_at": "2020-03-09T19:58:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Example to demonstrate the integration of Gunicorn with Kafka streaming and its worker as gevent.\r\n\r\nI have not found any relevant documentation for these technologies.\r\n\r\nI had integration issues like the consumer getting hanged, Kafka connectivity issues, etc. I finally resolved this issue and I would like to contribute this solution with an example to this repository.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2013", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2013/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2013/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2013/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2013", "id": 577160959, "node_id": "MDU6SXNzdWU1NzcxNjA5NTk=", "number": 2013, "title": "CertificateError: hostname 'xx.xxx.xxx.xxx' doesn't match 'messagehub.dco.mycompany'", "user": {"login": "AnamikaN", "id": 25681678, "node_id": "MDQ6VXNlcjI1NjgxNjc4", "avatar_url": "https://avatars3.githubusercontent.com/u/25681678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnamikaN", "html_url": "https://github.com/AnamikaN", "followers_url": "https://api.github.com/users/AnamikaN/followers", "following_url": "https://api.github.com/users/AnamikaN/following{/other_user}", "gists_url": "https://api.github.com/users/AnamikaN/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnamikaN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnamikaN/subscriptions", "organizations_url": "https://api.github.com/users/AnamikaN/orgs", "repos_url": "https://api.github.com/users/AnamikaN/repos", "events_url": "https://api.github.com/users/AnamikaN/events{/privacy}", "received_events_url": "https://api.github.com/users/AnamikaN/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-06T20:32:34Z", "updated_at": "2020-03-09T19:59:12Z", "closed_at": "2020-03-09T19:59:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to test kafka-consumer and i am getting error. It looks like, i need to configure ssl for it. Below is my connection block.\r\n\r\nclient_id=consumer_api\r\nsecurity_protocol=SASL_SSL\r\nsasl_mechanism=PLAIN\r\n\r\nconsumer = KafkaConsumer(bootstrap_servers=bootstrap_servers,\r\n                                     client_id=client_id,\r\n                                     security_protocol=security_protocol,\r\n                                     sasl_mechanism=sasl_mechanism,\r\n                                     sasl_plain_username=self.username,\r\n                                     sasl_plain_password=self.password,\r\n                                     auto_offset_reset=self.offset,\r\n                                     group_id=configuration['consumer_group_id'],\r\n                                     ssl_cafile=configuration['ca_file'])\r\n\r\nconfiguration = {\r\n    'config_file': '/config.ini',\r\n    'consumer_group_id': 'jenkins_svckafkaops',\r\n    'ca_file': '/kafka.ca-cert'\r\n}\r\n\r\nhow to fix this issue?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2012", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2012/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2012/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2012/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2012", "id": 576685648, "node_id": "MDU6SXNzdWU1NzY2ODU2NDg=", "number": 2012, "title": "initilize topic by KafkaConsumer constructor, but assignment() return empty set", "user": {"login": "peter-wang-wsl", "id": 15857181, "node_id": "MDQ6VXNlcjE1ODU3MTgx", "avatar_url": "https://avatars2.githubusercontent.com/u/15857181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peter-wang-wsl", "html_url": "https://github.com/peter-wang-wsl", "followers_url": "https://api.github.com/users/peter-wang-wsl/followers", "following_url": "https://api.github.com/users/peter-wang-wsl/following{/other_user}", "gists_url": "https://api.github.com/users/peter-wang-wsl/gists{/gist_id}", "starred_url": "https://api.github.com/users/peter-wang-wsl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peter-wang-wsl/subscriptions", "organizations_url": "https://api.github.com/users/peter-wang-wsl/orgs", "repos_url": "https://api.github.com/users/peter-wang-wsl/repos", "events_url": "https://api.github.com/users/peter-wang-wsl/events{/privacy}", "received_events_url": "https://api.github.com/users/peter-wang-wsl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-06T04:10:54Z", "updated_at": "2020-03-06T05:46:22Z", "closed_at": "2020-03-06T05:46:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I initilize topic by KafkaConsumer constructor, but assignment() return empty set. If I do not set topic in\r\nKafkaConsumer constructor, and use assign() then assignment() return is not empty. Why constructor can not do assign?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2010", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2010/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2010/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2010/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2010", "id": 574546868, "node_id": "MDU6SXNzdWU1NzQ1NDY4Njg=", "number": 2010, "title": "about kafka-stream", "user": {"login": "T-yufei", "id": 59591504, "node_id": "MDQ6VXNlcjU5NTkxNTA0", "avatar_url": "https://avatars3.githubusercontent.com/u/59591504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/T-yufei", "html_url": "https://github.com/T-yufei", "followers_url": "https://api.github.com/users/T-yufei/followers", "following_url": "https://api.github.com/users/T-yufei/following{/other_user}", "gists_url": "https://api.github.com/users/T-yufei/gists{/gist_id}", "starred_url": "https://api.github.com/users/T-yufei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/T-yufei/subscriptions", "organizations_url": "https://api.github.com/users/T-yufei/orgs", "repos_url": "https://api.github.com/users/T-yufei/repos", "events_url": "https://api.github.com/users/T-yufei/events{/privacy}", "received_events_url": "https://api.github.com/users/T-yufei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-03T09:57:32Z", "updated_at": "2020-03-06T08:11:45Z", "closed_at": "2020-03-06T08:11:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Whether to support  kafka-stream\uff1f", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2009", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2009/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2009/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2009/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2009", "id": 574494568, "node_id": "MDU6SXNzdWU1NzQ0OTQ1Njg=", "number": 2009, "title": "consumer.pause error when set group id", "user": {"login": "peter-wang-wsl", "id": 15857181, "node_id": "MDQ6VXNlcjE1ODU3MTgx", "avatar_url": "https://avatars2.githubusercontent.com/u/15857181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peter-wang-wsl", "html_url": "https://github.com/peter-wang-wsl", "followers_url": "https://api.github.com/users/peter-wang-wsl/followers", "following_url": "https://api.github.com/users/peter-wang-wsl/following{/other_user}", "gists_url": "https://api.github.com/users/peter-wang-wsl/gists{/gist_id}", "starred_url": "https://api.github.com/users/peter-wang-wsl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peter-wang-wsl/subscriptions", "organizations_url": "https://api.github.com/users/peter-wang-wsl/orgs", "repos_url": "https://api.github.com/users/peter-wang-wsl/repos", "events_url": "https://api.github.com/users/peter-wang-wsl/events{/privacy}", "received_events_url": "https://api.github.com/users/peter-wang-wsl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-03T08:21:44Z", "updated_at": "2020-03-05T02:15:49Z", "closed_at": "2020-03-05T02:15:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "If initilize consumer with group id, then get error when call pause method. If not set group id then the error(KeyError: TopicPartition(topic='xx', partition=0))disappear", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2008", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2008/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2008/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2008/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2008", "id": 573633006, "node_id": "MDU6SXNzdWU1NzM2MzMwMDY=", "number": 2008, "title": "KafkaTimeoutError: Failed to update metadata (again, server name-related issue)", "user": {"login": "NetBUG", "id": 1539823, "node_id": "MDQ6VXNlcjE1Mzk4MjM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1539823?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NetBUG", "html_url": "https://github.com/NetBUG", "followers_url": "https://api.github.com/users/NetBUG/followers", "following_url": "https://api.github.com/users/NetBUG/following{/other_user}", "gists_url": "https://api.github.com/users/NetBUG/gists{/gist_id}", "starred_url": "https://api.github.com/users/NetBUG/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NetBUG/subscriptions", "organizations_url": "https://api.github.com/users/NetBUG/orgs", "repos_url": "https://api.github.com/users/NetBUG/repos", "events_url": "https://api.github.com/users/NetBUG/events{/privacy}", "received_events_url": "https://api.github.com/users/NetBUG/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-03-01T22:56:39Z", "updated_at": "2020-03-02T20:36:04Z", "closed_at": "2020-03-02T20:36:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "The situation is similar to https://github.com/dpkp/kafka-python/issues/607.\r\nI tried to realize what was the problem and turned logging on.\r\nSo I have bootstrap servers set to a specific value (`10.8.0.3:9092`), but here is what I see in logs with `kafka-python==1.4.6` and `2.0.1`:\r\n```\r\nDEBUG:kafka.producer.sender:Starting Kafka producer I/O thread.\r\nDEBUG:kafka.client:Sending metadata request MetadataRequest_v1(topics=NULL) to node bootstrap-0\r\nDEBUG:kafka.producer.kafka:Kafka producer started\r\nDEBUG:kafka.protocol.parser:Sending request MetadataRequest_v1(topics=NULL)\r\nDEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.8.0.3:9092 <connected> [IPv4 ('10.8.0.3', 9092)]> Request 3: MetadataRequest_v1(topics=NULL)\r\nDEBUG:kafka.producer.kafka:Requesting metadata update for topic train-requests\r\nDEBUG:kafka.protocol.parser:Received correlation id: 3\r\nDEBUG:kafka.protocol.parser:Processing response MetadataResponse_v1\r\nDEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=10.8.0.3:9092 <connected> [IPv4 ('10.8.0.3', 9092)]> Response 3 (112.8690242767334 ms): MetadataResponse_v1(brokers=[(node_id=1001, host='kafka', port=9092, rack=None)], controller_id=1001, topics=[(error_code=0, topic='object-access', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='logon', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='logon-failures', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='process-creation', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='anomalies-primary', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='__consumer_offsets', is_internal=True, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=10, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=20, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=40, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=30, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=9, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=11, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=31, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=39, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=13, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=18, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=22, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=8, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=32, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=43, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=29, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=34, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=1, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=6, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=41, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=27, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=48, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=5, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=15, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=35, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=25, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=46, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=26, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=36, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=44, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=16, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=37, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=17, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=45, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=3, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=24, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=38, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=33, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=23, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=28, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=2, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=12, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=19, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=14, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=4, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=47, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=49, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=42, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=7, leader=1001, replicas=[1001], isr=[1001]), (error_code=0, partition=21, leader=1001, replicas=[1001], isr=[1001])]), (error_code=0, topic='exchange', is_internal=False, partitions=[(error_code=0, partition=0, leader=1001, replicas=[1001], isr=[1001])])])\r\nDEBUG:kafka.cluster:Updated cluster metadata to ClusterMetadata(brokers: 1, topics: 7, groups: 0)\r\nDEBUG:kafka.producer.kafka:_wait_on_metadata woke after 0.11521100997924805 secs.\r\nDEBUG:kafka.producer.kafka:Requesting metadata update for topic train-requests\r\nDEBUG:kafka.client:Initializing connection to node 1001 for metadata request\r\nDEBUG:kafka.client:Initiating connection to node 1001 at kafka:9092\r\nDEBUG:kafka.metrics.metrics:Added sensor with name node-1001.bytes-sent\r\nDEBUG:kafka.metrics.metrics:Added sensor with name node-1001.bytes-received\r\nDEBUG:kafka.metrics.metrics:Added sensor with name node-1001.latency\r\nWARNING:kafka.conn:DNS lookup failed for kafka:9092, exception was [Errno 8] nodename nor servname provided, or not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?\r\nERROR:kafka.conn:DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)\r\nDEBUG:kafka.client:Give up sending metadata request since no node is available\r\nWARNING:kafka.conn:DNS lookup failed for kafka:9092, exception was [Errno 8] nodename nor servname provided, or not known. Is your advertised.listeners (called advertised.host.name before Kafka 9) correct and resolvable?\r\nERROR:kafka.conn:DNS lookup failed for kafka:9092 (AddressFamily.AF_UNSPEC)\r\nDEBUG:kafka.client:Give up sending metadata request since no node is available\r\n```\r\n\r\nThe last three lines have been repeated until the timeout occurred.\r\n\r\nThe code looks like\r\n```\r\nkafka_host = '10.8.0.3'\r\nkafka_port = '9092'\r\nkafka_server = '%s:%s' % (kafka_host, kafka_port)\r\nlogging.basicConfig(level=logging.DEBUG)\r\nproducer = KafkaProducer(bootstrap_servers=kafka_server,\r\n                        value_serializer=lambda v: json.dumps(v).encode('utf-8'), max_block_ms=5000)\r\nproducer.send('some-topic', { 'task_id': str(uuid.uuid1()) })\r\n```\r\n\r\nSeems metadata discovery tries to use hostname `kafka` instead of my server", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2006", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2006/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2006/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2006/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2006", "id": 571697745, "node_id": "MDU6SXNzdWU1NzE2OTc3NDU=", "number": 2006, "title": "NoBrokersAvailable", "user": {"login": "AnamikaN", "id": 25681678, "node_id": "MDQ6VXNlcjI1NjgxNjc4", "avatar_url": "https://avatars3.githubusercontent.com/u/25681678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AnamikaN", "html_url": "https://github.com/AnamikaN", "followers_url": "https://api.github.com/users/AnamikaN/followers", "following_url": "https://api.github.com/users/AnamikaN/following{/other_user}", "gists_url": "https://api.github.com/users/AnamikaN/gists{/gist_id}", "starred_url": "https://api.github.com/users/AnamikaN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AnamikaN/subscriptions", "organizations_url": "https://api.github.com/users/AnamikaN/orgs", "repos_url": "https://api.github.com/users/AnamikaN/repos", "events_url": "https://api.github.com/users/AnamikaN/events{/privacy}", "received_events_url": "https://api.github.com/users/AnamikaN/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-26T22:23:01Z", "updated_at": "2020-02-27T01:13:19Z", "closed_at": "2020-02-27T01:13:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "I need to make connection to brokers on port 9093. i have defined following properties:\r\n\r\nbootstrap_servers=10.xxx.xxx.xxx:9093\r\nclient_id= api\r\nsecurity_protocol=SASL_PLAINTEXT\r\nsasl_mechanism=PLAIN\r\nsasl_plain_username=xxxxxx\r\nsasl_plain_password=xxxxxxxx\r\n\r\nBelow is my code-\r\nadmin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers,\r\n                                                 client_id=client_id,\r\n                                                 security_protocol=security_protocol,\r\n                                                 sasl_mechanism=sasl_mechanism,\r\n                                                 sasl_plain_username=username,\r\n                                                 sasl_plain_password=password)\r\n\r\nit is throwing error - NoBrokersAvailable\r\n\r\nAre there any other settings needs to be done", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2002", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2002/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2002/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2002/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2002", "id": 569341665, "node_id": "MDU6SXNzdWU1NjkzNDE2NjU=", "number": 2002, "title": "UnrecognizedBrokerVersion on kafka-python 2.0.1", "user": {"login": "gcd0318", "id": 658181, "node_id": "MDQ6VXNlcjY1ODE4MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/658181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gcd0318", "html_url": "https://github.com/gcd0318", "followers_url": "https://api.github.com/users/gcd0318/followers", "following_url": "https://api.github.com/users/gcd0318/following{/other_user}", "gists_url": "https://api.github.com/users/gcd0318/gists{/gist_id}", "starred_url": "https://api.github.com/users/gcd0318/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gcd0318/subscriptions", "organizations_url": "https://api.github.com/users/gcd0318/orgs", "repos_url": "https://api.github.com/users/gcd0318/repos", "events_url": "https://api.github.com/users/gcd0318/events{/privacy}", "received_events_url": "https://api.github.com/users/gcd0318/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-22T14:26:03Z", "updated_at": "2020-02-24T07:37:16Z", "closed_at": "2020-02-24T04:30:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/2001", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2001/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2001/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/2001/events", "html_url": "https://github.com/dpkp/kafka-python/issues/2001", "id": 567589030, "node_id": "MDU6SXNzdWU1Njc1ODkwMzA=", "number": 2001, "title": "KafkaAdminClient.*_topics() is broken", "user": {"login": "StefPtrs", "id": 10376763, "node_id": "MDQ6VXNlcjEwMzc2NzYz", "avatar_url": "https://avatars1.githubusercontent.com/u/10376763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StefPtrs", "html_url": "https://github.com/StefPtrs", "followers_url": "https://api.github.com/users/StefPtrs/followers", "following_url": "https://api.github.com/users/StefPtrs/following{/other_user}", "gists_url": "https://api.github.com/users/StefPtrs/gists{/gist_id}", "starred_url": "https://api.github.com/users/StefPtrs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StefPtrs/subscriptions", "organizations_url": "https://api.github.com/users/StefPtrs/orgs", "repos_url": "https://api.github.com/users/StefPtrs/repos", "events_url": "https://api.github.com/users/StefPtrs/events{/privacy}", "received_events_url": "https://api.github.com/users/StefPtrs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-19T13:59:05Z", "updated_at": "2020-02-19T17:17:40Z", "closed_at": "2020-02-19T17:16:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "This fix (on master) solves the issue:\r\nhttps://github.com/dpkp/kafka-python/commit/7195f0369c7dbe25aea2c3fed78d2b4f772d775b\r\n\r\nIs there a new release planned that includes this fix?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1999", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1999/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1999/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1999/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1999", "id": 565912552, "node_id": "MDU6SXNzdWU1NjU5MTI1NTI=", "number": 1999, "title": "Murmur2Partitioner", "user": {"login": "DrNickBailey", "id": 12257812, "node_id": "MDQ6VXNlcjEyMjU3ODEy", "avatar_url": "https://avatars3.githubusercontent.com/u/12257812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DrNickBailey", "html_url": "https://github.com/DrNickBailey", "followers_url": "https://api.github.com/users/DrNickBailey/followers", "following_url": "https://api.github.com/users/DrNickBailey/following{/other_user}", "gists_url": "https://api.github.com/users/DrNickBailey/gists{/gist_id}", "starred_url": "https://api.github.com/users/DrNickBailey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DrNickBailey/subscriptions", "organizations_url": "https://api.github.com/users/DrNickBailey/orgs", "repos_url": "https://api.github.com/users/DrNickBailey/repos", "events_url": "https://api.github.com/users/DrNickBailey/events{/privacy}", "received_events_url": "https://api.github.com/users/DrNickBailey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-16T14:18:04Z", "updated_at": "2020-02-16T17:04:47Z", "closed_at": "2020-02-16T17:04:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Since upgrading to 2.0.0 I'm now getting an import error:\r\n\r\n> --->  from kafka.partitioner import Murmur2Partitioner\r\n> ImportError: cannot import name 'Murmur2Partitioner'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1998", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1998/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1998/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1998/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1998", "id": 563900514, "node_id": "MDU6SXNzdWU1NjM5MDA1MTQ=", "number": 1998, "title": "High CPU in kafka consumer while processing records", "user": {"login": "pnkjkragrl", "id": 46754698, "node_id": "MDQ6VXNlcjQ2NzU0Njk4", "avatar_url": "https://avatars3.githubusercontent.com/u/46754698?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pnkjkragrl", "html_url": "https://github.com/pnkjkragrl", "followers_url": "https://api.github.com/users/pnkjkragrl/followers", "following_url": "https://api.github.com/users/pnkjkragrl/following{/other_user}", "gists_url": "https://api.github.com/users/pnkjkragrl/gists{/gist_id}", "starred_url": "https://api.github.com/users/pnkjkragrl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pnkjkragrl/subscriptions", "organizations_url": "https://api.github.com/users/pnkjkragrl/orgs", "repos_url": "https://api.github.com/users/pnkjkragrl/repos", "events_url": "https://api.github.com/users/pnkjkragrl/events{/privacy}", "received_events_url": "https://api.github.com/users/pnkjkragrl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-12T10:28:29Z", "updated_at": "2020-02-12T20:16:27Z", "closed_at": "2020-02-12T20:16:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there,\r\nWe are experiencing High CPU issues in kafka consumer but unable to figure out through the logs and we don't know what is hitting this issue.\r\n\r\nkafka-python version 1.3.1.\r\nWe are polling records from consumer.poll and after sometime, for around 1 min kafka does nothing means no logs printed for 1 min and after sometime we are seeing this error **RequestTimedOutError: Request timed out after 40000 ms** and CPU reaches 100% and our program gets stuck there.\r\n\r\nSample logs are here:-\r\n`12/06/2019 11:50:36 AM [kafka.consumer.fetcher] [DEBUG]: Adding fetch request for partition TopicPartition(topic='-uve-0', partition=0) at offset 2690390\r\n12/06/2019 11:50:36 AM [kafka.consumer.fetcher] [DEBUG]: Sending FetchRequest to node 1\r\n12/06/2019 11:50:36 AM [kafka.conn] [DEBUG]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> Request 24: FetchRequest_v2(replica_id=-1, max_wait_time=500, min_bytes=1, topics=[(topic='-uve-0', partitions=[(partition=0, offset=2690390, max_bytes=1048576)])])\r\n12/06/2019 11:50:36 AM [kafka.consumer.fetcher] [DEBUG]: Adding fetch request for partition TopicPartition(topic='-uve-1', partition=0) at offset 6019422\r\n12/06/2019 11:50:36 AM [kafka.consumer.fetcher] [DEBUG]: Sending FetchRequest to node 1\r\n**12/06/2019 11:50:36 AM** [kafka.conn] [DEBUG]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> Request 31: FetchRequest_v2(replica_id=-1, max_wait_time=500, min_bytes=1, topics=[(topic='-uve-1', partitions=[(partition=0, offset=6019422, max_bytes=1048576)])])\r\n\r\n<<<<<<<NO ACTIVITY HERE for ~1 min >>>>>>>>>>>>>\r\n\r\n**12/06/2019 11:51:24 AM** [kafka.consumer.fetcher] [DEBUG]: Adding fetch request for partition TopicPartition(topic='-uve-21', partition=0) at offset 102808006\r\n12/06/2019 11:51:24 AM [kafka.consumer.fetcher] [DEBUG]: Sending FetchRequest to node 1\r\n12/06/2019 11:51:24 AM [kafka.conn] [DEBUG]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> Request 60: FetchRequest_v2(replica_id=-1, max_wait_time=500, min_bytes=1, topics=[(topic='-uve-21', partitions=[(partition=0, offset=102808006, max_bytes=1048576)])])\r\n12/06/2019 11:51:24 AM [kafka.consumer.fetcher] [DEBUG]: Adding fetch request for partition TopicPartition(topic='-uve-23', partition=0) at offset 106763480\r\n12/06/2019 11:51:24 AM [kafka.consumer.fetcher] [DEBUG]: Sending FetchRequest to node 1\r\n12/06/2019 11:51:24 AM [kafka.conn] [DEBUG]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> Request 61: FetchRequest_v2(replica_id=-1, max_wait_time=500, min_bytes=1, topics=[(topic='-uve-23', partitions=[(partition=0, offset=106763480, max_bytes=1048576)])])`\r\n\r\nIf you see in above log for ~1 min it does nothing. And after this we are getting this log:-\r\n\r\n`12/06/2019 11:51:28 AM [kafka.client] [WARNING]: Node 1 connection failed -- refreshing metadata\r\n\r\n12/06/2019 11:51:28 AM [kafka.consumer.fetcher] [ERROR]: Fetch to node 1 failed: [Error 7] RequestTimedOutError: Request timed out after 40000 ms\r\n\r\n12/06/2019 11:51:28 AM [kafka.conn] [WARNING]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> timed out after 40000 ms. Closing connection.\r\n\r\n12/06/2019 11:51:28 AM [kafka.client] [WARNING]: Node 1 connection failed -- refreshing metadata\r\n\r\n12/06/2019 11:51:28 AM [kafka.consumer.fetcher] [ERROR]: Fetch to node 1 failed: [Error 7] RequestTimedOutError: Request timed out after 40000 ms\r\n\r\n12/06/2019 11:51:28 AM [kafka.conn] [WARNING]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> timed out after 40000 ms. Closing connection.\r\n\r\n12/06/2019 11:51:28 AM [kafka.client] [WARNING]: Node 1 connection failed -- refreshing metadata\r\n\r\n**12/06/2019 11:51:28 AM** [kafka.consumer.fetcher] **[ERROR]: Fetch to node 1 failed: [Error 7] \r\nRequestTimedOutError: Request timed out after 40000 ms**\r\n\r\n12/06/2019 11:51:28 AM [kafka.conn] [WARNING]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> timed out after 40000 ms. Closing connection.\r\n\r\n12/06/2019 11:51:28 AM [kafka.client] [WARNING]: Node 1 connection failed -- refreshing metadata\r\n\r\n12/06/2019 11:51:28 AM [kafka.consumer.fetcher] **[ERROR]: Fetch to node 1 failed: [Error 7] RequestTimedOutError: Request timed out after 40000 ms**\r\n\r\n12/06/2019 11:51:28 AM [kafka.conn] [WARNING]: <BrokerConnection host=10.152.221.6/10.152.221.6 port=9092> timed out after 40000 ms. Closing connection.\r\n\r\n12/06/2019 11:51:28 AM [kafka.client] [WARNING]: Node 1 connection failed -- refreshing metadata\r\n`\r\n\r\nlogs are filled with these errors and CPU remains at 100% and my program gets stuck there.\r\n\r\nFYI, I am attaching full log file in case you want to see what is happening there.\r\n[alarm_debug.log](https://github.com/dpkp/kafka-python/files/4191170/alarm_debug.log)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1994", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1994/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1994/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1994/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1994", "id": 561281623, "node_id": "MDU6SXNzdWU1NjEyODE2MjM=", "number": 1994, "title": "describe topics needs to speak to the controller", "user": {"login": "jeffwidman", "id": 483314, "node_id": "MDQ6VXNlcjQ4MzMxNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/483314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffwidman", "html_url": "https://github.com/jeffwidman", "followers_url": "https://api.github.com/users/jeffwidman/followers", "following_url": "https://api.github.com/users/jeffwidman/following{/other_user}", "gists_url": "https://api.github.com/users/jeffwidman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffwidman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffwidman/subscriptions", "organizations_url": "https://api.github.com/users/jeffwidman/orgs", "repos_url": "https://api.github.com/users/jeffwidman/repos", "events_url": "https://api.github.com/users/jeffwidman/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffwidman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-06T21:31:11Z", "updated_at": "2020-02-06T22:20:41Z", "closed_at": "2020-02-06T22:20:41Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "https://github.com/dpkp/kafka-python/pull/1993#discussion_r376085225\r\n\r\n@TylerLubeck is putting together PR but opening this ticket so we don't lose track of this.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1992", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1992/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1992/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1992/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1992", "id": 560387776, "node_id": "MDU6SXNzdWU1NjAzODc3NzY=", "number": 1992, "title": "Help: Howto read the config options for a topic", "user": {"login": "acs", "id": 209533, "node_id": "MDQ6VXNlcjIwOTUzMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/209533?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acs", "html_url": "https://github.com/acs", "followers_url": "https://api.github.com/users/acs/followers", "following_url": "https://api.github.com/users/acs/following{/other_user}", "gists_url": "https://api.github.com/users/acs/gists{/gist_id}", "starred_url": "https://api.github.com/users/acs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acs/subscriptions", "organizations_url": "https://api.github.com/users/acs/orgs", "repos_url": "https://api.github.com/users/acs/repos", "events_url": "https://api.github.com/users/acs/events{/privacy}", "received_events_url": "https://api.github.com/users/acs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-05T13:55:21Z", "updated_at": "2020-02-12T10:24:37Z", "closed_at": "2020-02-05T19:44:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have created a new topic with:\r\n\r\n```\r\n    topic_list = [NewTopic(topic, num_partitions=3, replication_factor=1,\r\n                           topic_configs={'retention.ms': '168'})]\r\n    admin_client.create_topics(new_topics=topic_list, validate_only=False)\r\n```\r\nIs it possible to get back the **retention.ms** config using the kafka-python API?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1990", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1990/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1990/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1990/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1990", "id": 554434725, "node_id": "MDU6SXNzdWU1NTQ0MzQ3MjU=", "number": 1990, "title": "Documentation bug: KafkaConsumer doc does not list all valid values for security_protocol", "user": {"login": "pranavmarla", "id": 6486632, "node_id": "MDQ6VXNlcjY0ODY2MzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6486632?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pranavmarla", "html_url": "https://github.com/pranavmarla", "followers_url": "https://api.github.com/users/pranavmarla/followers", "following_url": "https://api.github.com/users/pranavmarla/following{/other_user}", "gists_url": "https://api.github.com/users/pranavmarla/gists{/gist_id}", "starred_url": "https://api.github.com/users/pranavmarla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pranavmarla/subscriptions", "organizations_url": "https://api.github.com/users/pranavmarla/orgs", "repos_url": "https://api.github.com/users/pranavmarla/repos", "events_url": "https://api.github.com/users/pranavmarla/events{/privacy}", "received_events_url": "https://api.github.com/users/pranavmarla/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-23T22:06:44Z", "updated_at": "2020-06-12T15:27:18Z", "closed_at": "2020-03-02T16:58:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "The [KafkaConsumer documentation](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html) says the following about the _security_protocol_ setting:\r\n`security_protocol (str) \u2013 Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL. Default: PLAINTEXT.`\r\nThis indicates that there are only 2 valid values for _security_protocol_: **PLAINTEXT** and **SSL**.\r\n\r\nHowever, further down in the same document, when talking about the _sasl_mechanism_ setting, it says the following:\r\n`sasl_mechanism (str) \u2013 Authentication mechanism when security_protocol is configured for SASL_PLAINTEXT or SASL_SSL. Valid values are: PLAIN, GSSAPI, OAUTHBEARER.`\r\nThis clearly implies that there are actually 2 **more** valid values for _security_protocol_, that are not mentioned above: **SASL_PLAINTEXT** and **SASL_SSL**.\r\n\r\nThis is confirmed by looking at the corresponding _security_protocol_ section of the [KafkaProducer documentation](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html), which correctly lists all 4 valid values:\r\n`security_protocol (str) \u2013 Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL. Default: PLAINTEXT.`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1982", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1982/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1982/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1982/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1982", "id": 545820700, "node_id": "MDU6SXNzdWU1NDU4MjA3MDA=", "number": 1982, "title": "don't send and don't receive messages", "user": {"login": "jamesRUS52", "id": 39707635, "node_id": "MDQ6VXNlcjM5NzA3NjM1", "avatar_url": "https://avatars1.githubusercontent.com/u/39707635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesRUS52", "html_url": "https://github.com/jamesRUS52", "followers_url": "https://api.github.com/users/jamesRUS52/followers", "following_url": "https://api.github.com/users/jamesRUS52/following{/other_user}", "gists_url": "https://api.github.com/users/jamesRUS52/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesRUS52/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesRUS52/subscriptions", "organizations_url": "https://api.github.com/users/jamesRUS52/orgs", "repos_url": "https://api.github.com/users/jamesRUS52/repos", "events_url": "https://api.github.com/users/jamesRUS52/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesRUS52/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-06T16:36:34Z", "updated_at": "2020-01-07T12:51:18Z", "closed_at": "2020-01-07T12:51:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello\r\nI have installed kafka-python 1.4.7 for python 2.7 and kafka 2.4\r\nI am trying to send and recieve message, but it does not work without any errors\r\n\r\nproducer\r\n\r\n```python\r\nfrom kafka import KafkaProducer\r\n\r\nproducer = KafkaProducer(bootstrap_servers=['172.31.199.52:9092'])\r\nprint (producer.bootstrap_connected())\r\nfuture = producer.send('test', key=b'foo', value=b'bar')\r\nproducer.flush()\r\n```\r\nIt output\r\n```\r\nTrue\r\n\r\nProcess finished with exit code 0\r\n``` \r\nand some seconds later exit with status code 0\r\n\r\nI am checking topic with kafka-console-consumer and no new message in it\r\n\r\nconsumer\r\n```python\r\nfrom kafka import KafkaConsumer, TopicPartition\r\n\r\nconsumer = KafkaConsumer('test',bootstrap_servers=['172.31.199.52:9092'],group_id=\"my-python\",auto_offset_reset='earliest')\r\nprint (consumer.bootstrap_connected())\r\nfor message in consumer:\r\n    # message value and key are raw bytes -- decode if necessary!\r\n    # e.g., for unicode: `message.value.decode('utf-8')`\r\n    print (\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition,\r\n                                          message.offset, message.key,\r\n                                          message.value))\r\n```\r\nit output only\r\n```\r\nTrue\r\n``` \r\nand process still running\r\n\r\nwhat wrong? how can I enable debug for it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1981", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1981/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1981/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1981/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1981", "id": 544296250, "node_id": "MDU6SXNzdWU1NDQyOTYyNTA=", "number": 1981, "title": "Topic assigment invalid after subscription", "user": {"login": "martin-intex", "id": 13764622, "node_id": "MDQ6VXNlcjEzNzY0NjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/13764622?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martin-intex", "html_url": "https://github.com/martin-intex", "followers_url": "https://api.github.com/users/martin-intex/followers", "following_url": "https://api.github.com/users/martin-intex/following{/other_user}", "gists_url": "https://api.github.com/users/martin-intex/gists{/gist_id}", "starred_url": "https://api.github.com/users/martin-intex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martin-intex/subscriptions", "organizations_url": "https://api.github.com/users/martin-intex/orgs", "repos_url": "https://api.github.com/users/martin-intex/repos", "events_url": "https://api.github.com/users/martin-intex/events{/privacy}", "received_events_url": "https://api.github.com/users/martin-intex/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-01T01:05:08Z", "updated_at": "2020-02-06T20:49:03Z", "closed_at": "2020-02-06T20:49:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using the KafkaConsumer with the subscription method a call directly after the subscription (i.e. before any message was received) to the assignment() method returns an empty set.\r\nOnly after a message was received the currently assigned partition can be examined. This knowledge however would be required to reset the offset of that consumer group. (Or is there is another way to reset the consumer group offset?)\r\n\r\nFor example:\r\n```\r\nconsumer = KafkaConsumer(bootstrap_servers='localhost:9092',\r\n                     auto_offset_reset='earliest', \r\n                     consumer_timeout_ms=1000) \r\nconsumer.subscribe(['my-topic'] \r\nprint(consumer..assignment()}) \r\n```\r\nWould result in `set()`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1973", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1973/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1973/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1973/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1973", "id": 542080236, "node_id": "MDU6SXNzdWU1NDIwODAyMzY=", "number": 1973, "title": "The \"_wait_on_metadata\"  method of the kafka.KafkaProducer is time-consuming method", "user": {"login": "renbingdong", "id": 21073320, "node_id": "MDQ6VXNlcjIxMDczMzIw", "avatar_url": "https://avatars3.githubusercontent.com/u/21073320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/renbingdong", "html_url": "https://github.com/renbingdong", "followers_url": "https://api.github.com/users/renbingdong/followers", "following_url": "https://api.github.com/users/renbingdong/following{/other_user}", "gists_url": "https://api.github.com/users/renbingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/renbingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/renbingdong/subscriptions", "organizations_url": "https://api.github.com/users/renbingdong/orgs", "repos_url": "https://api.github.com/users/renbingdong/repos", "events_url": "https://api.github.com/users/renbingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/renbingdong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-12-24T10:25:50Z", "updated_at": "2019-12-24T11:47:44Z", "closed_at": "2019-12-24T11:47:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, \r\nEnvironment:\r\n    centos: 7.6.1810\r\n    kafka: api_version 0.10.2.0\r\n    python: 3.6\r\n    kafka-python: 1.4.6\r\n\r\nAfter evaluation, the method takes about 50ms.  (method: _wait_on_metadata,  class: kafka.KafkaProducer)\r\nAfter analysis, it was found that the method \"_poll\" of the kafka.KafkaClient is time-consuming.\r\n\r\nclass KafkaClient:\r\n    def _poll(self, timeout):\r\n        ....\r\n        ready = self._selector.select(timeout)\r\n        ....\r\n\r\n\"self._selector.select(timeout)\" is time-consuming.  the select is EpollSelector.\r\n\r\nHow do I optimize to make sure the service responds in milliseconds?\r\n\r\n\r\n    ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1971", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1971/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1971/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1971/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1971", "id": 541631984, "node_id": "MDU6SXNzdWU1NDE2MzE5ODQ=", "number": 1971, "title": "kafka.errors.IncompatibleBrokerVersion: IncompatibleBrokerVersion: Kafka broker does not support the 'DescribeConfigsRequest_v0' Kafka protocol.", "user": {"login": "ccmanito", "id": 22959785, "node_id": "MDQ6VXNlcjIyOTU5Nzg1", "avatar_url": "https://avatars3.githubusercontent.com/u/22959785?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ccmanito", "html_url": "https://github.com/ccmanito", "followers_url": "https://api.github.com/users/ccmanito/followers", "following_url": "https://api.github.com/users/ccmanito/following{/other_user}", "gists_url": "https://api.github.com/users/ccmanito/gists{/gist_id}", "starred_url": "https://api.github.com/users/ccmanito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ccmanito/subscriptions", "organizations_url": "https://api.github.com/users/ccmanito/orgs", "repos_url": "https://api.github.com/users/ccmanito/repos", "events_url": "https://api.github.com/users/ccmanito/events{/privacy}", "received_events_url": "https://api.github.com/users/ccmanito/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-23T07:59:58Z", "updated_at": "2019-12-29T05:52:50Z", "closed_at": "2019-12-29T05:52:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "kafka.errors.IncompatibleBrokerVersion: IncompatibleBrokerVersion: Kafka broker does not support the 'DescribeConfigsRequest_v0' Kafka protocol.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1970", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1970/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1970/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1970/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1970", "id": 540884386, "node_id": "MDU6SXNzdWU1NDA4ODQzODY=", "number": 1970, "title": "Does this lib support prometheus exporter?", "user": {"login": "quanghn96", "id": 38189076, "node_id": "MDQ6VXNlcjM4MTg5MDc2", "avatar_url": "https://avatars0.githubusercontent.com/u/38189076?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quanghn96", "html_url": "https://github.com/quanghn96", "followers_url": "https://api.github.com/users/quanghn96/followers", "following_url": "https://api.github.com/users/quanghn96/following{/other_user}", "gists_url": "https://api.github.com/users/quanghn96/gists{/gist_id}", "starred_url": "https://api.github.com/users/quanghn96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quanghn96/subscriptions", "organizations_url": "https://api.github.com/users/quanghn96/orgs", "repos_url": "https://api.github.com/users/quanghn96/repos", "events_url": "https://api.github.com/users/quanghn96/events{/privacy}", "received_events_url": "https://api.github.com/users/quanghn96/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-20T09:23:03Z", "updated_at": "2019-12-29T05:37:42Z", "closed_at": "2019-12-29T05:37:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1969", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1969/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1969/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1969/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1969", "id": 537838826, "node_id": "MDU6SXNzdWU1Mzc4Mzg4MjY=", "number": 1969, "title": "Authenticating GSSAPI", "user": {"login": "bladerunner512", "id": 17748886, "node_id": "MDQ6VXNlcjE3NzQ4ODg2", "avatar_url": "https://avatars2.githubusercontent.com/u/17748886?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bladerunner512", "html_url": "https://github.com/bladerunner512", "followers_url": "https://api.github.com/users/bladerunner512/followers", "following_url": "https://api.github.com/users/bladerunner512/following{/other_user}", "gists_url": "https://api.github.com/users/bladerunner512/gists{/gist_id}", "starred_url": "https://api.github.com/users/bladerunner512/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bladerunner512/subscriptions", "organizations_url": "https://api.github.com/users/bladerunner512/orgs", "repos_url": "https://api.github.com/users/bladerunner512/repos", "events_url": "https://api.github.com/users/bladerunner512/events{/privacy}", "received_events_url": "https://api.github.com/users/bladerunner512/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-12-14T00:44:43Z", "updated_at": "2019-12-18T22:14:31Z", "closed_at": "2019-12-18T22:14:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "kafka-python v1.4.7\r\nkafka 1.1.1\r\n\r\nUnable to authenicate Kerberized Kakfa :\r\nDEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=kafkabroker001.com:9094 <disconnected> [unspecified None]>: creating new socket\r\nDEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=kafkabroker001.com:9094 <disconnected> [IPv4 ('10.10.10.39', 9094)]>: setting socket option (6, 1, 1)\r\nINFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=kafkabroker001.com:9094 <connecting> [IPv4 ('10.10.10.39', 9094)]>: connecting to kafkabroker001.com:9094 [('10.10.10.39', 9094) IPv4]\r\nDEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=kafkabroker001.com:9094 <connecting> [IPv4 ('10.10.10.39', 9094)]>: established TCP connection\r\nDEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=kafkabroker001.com:9094 <connecting> [IPv4 ('10.10.10.39', 9094)]>: initiating SASL authentication\r\nDEBUG:kafka.protocol.parser:Sending request SaslHandShakeRequest_v0(mechanism='GSSAPI')\r\nDEBUG:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=kafkabroker001.com:9094 <authenticating> [IPv4 ('10.10.10.39', 9094)]> Request 1: SaslHandShakeRequest_v0(mechanism='GSSAPI')\r\nERROR:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=kafkabroker001.com:9094 <authenticating> [IPv4 ('10.10.10.39', 9094)]>: socket disconnected\r\n\r\nos.environ['KAFKA_OPTS'] = '-Djava.security.auth.login.config=\"D:\\\\pyProjects\\\\krb\\\\dev\\\\jaas.conf\"'\r\n\r\n  producer = KafkaProducer(bootstrap_servers=KAFKA_BROKERS, api_version='0.10',\r\n                           security_protocol='SASL_PLAINTEXT', sasl_mechanism='GSSAPI',\r\n                           sasl_kerberos_service_name='kafka', connections_max_idle_ms=1200000,\r\n                           request_timeout_ms=1000000, api_version_auto_timeout_ms=1000000)\r\n\r\njaas.conf:\r\nKafkaClient {\r\n       com.sun.security.auth.module.Krb5LoginModule required\r\n       useKeyTab=true\r\n       keyTab=\"D:\\\\pyProjects\\\\krb\\\\dev\\\\krb.keytab\"\r\n       storeKey=true\r\n       useTicketCache=false\r\n       serviceName=\"kafka\"\r\n       principal=\"***D_KRB@***SITE.COM\";\r\n};\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1968", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1968/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1968/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1968/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1968", "id": 537533678, "node_id": "MDU6SXNzdWU1Mzc1MzM2Nzg=", "number": 1968, "title": "How to set at least once/exactly once at the clinet?", "user": {"login": "jossiekat", "id": 12046027, "node_id": "MDQ6VXNlcjEyMDQ2MDI3", "avatar_url": "https://avatars2.githubusercontent.com/u/12046027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jossiekat", "html_url": "https://github.com/jossiekat", "followers_url": "https://api.github.com/users/jossiekat/followers", "following_url": "https://api.github.com/users/jossiekat/following{/other_user}", "gists_url": "https://api.github.com/users/jossiekat/gists{/gist_id}", "starred_url": "https://api.github.com/users/jossiekat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jossiekat/subscriptions", "organizations_url": "https://api.github.com/users/jossiekat/orgs", "repos_url": "https://api.github.com/users/jossiekat/repos", "events_url": "https://api.github.com/users/jossiekat/events{/privacy}", "received_events_url": "https://api.github.com/users/jossiekat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-13T12:50:54Z", "updated_at": "2019-12-29T05:35:32Z", "closed_at": "2019-12-29T05:35:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want the client to consume the message only once.\r\nwhich parameter in the client does it?\r\nthanks\r\njoseph", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1967", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1967/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1967/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1967/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1967", "id": 536736126, "node_id": "MDU6SXNzdWU1MzY3MzYxMjY=", "number": 1967, "title": "hp-ux 11", "user": {"login": "lovrainy", "id": 28776680, "node_id": "MDQ6VXNlcjI4Nzc2Njgw", "avatar_url": "https://avatars2.githubusercontent.com/u/28776680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lovrainy", "html_url": "https://github.com/lovrainy", "followers_url": "https://api.github.com/users/lovrainy/followers", "following_url": "https://api.github.com/users/lovrainy/following{/other_user}", "gists_url": "https://api.github.com/users/lovrainy/gists{/gist_id}", "starred_url": "https://api.github.com/users/lovrainy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lovrainy/subscriptions", "organizations_url": "https://api.github.com/users/lovrainy/orgs", "repos_url": "https://api.github.com/users/lovrainy/repos", "events_url": "https://api.github.com/users/lovrainy/events{/privacy}", "received_events_url": "https://api.github.com/users/lovrainy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-12T03:30:06Z", "updated_at": "2019-12-29T05:31:46Z", "closed_at": "2019-12-29T05:30:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "how can i use it on hp-ux 11", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1965", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1965/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1965/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1965/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1965", "id": 534383132, "node_id": "MDU6SXNzdWU1MzQzODMxMzI=", "number": 1965, "title": "Fix simple typo: managementment -> management", "user": {"login": "timgates42", "id": 47873678, "node_id": "MDQ6VXNlcjQ3ODczNjc4", "avatar_url": "https://avatars1.githubusercontent.com/u/47873678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timgates42", "html_url": "https://github.com/timgates42", "followers_url": "https://api.github.com/users/timgates42/followers", "following_url": "https://api.github.com/users/timgates42/following{/other_user}", "gists_url": "https://api.github.com/users/timgates42/gists{/gist_id}", "starred_url": "https://api.github.com/users/timgates42/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timgates42/subscriptions", "organizations_url": "https://api.github.com/users/timgates42/orgs", "repos_url": "https://api.github.com/users/timgates42/repos", "events_url": "https://api.github.com/users/timgates42/events{/privacy}", "received_events_url": "https://api.github.com/users/timgates42/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-12-07T10:15:26Z", "updated_at": "2019-12-09T01:50:35Z", "closed_at": "2019-12-09T01:50:35Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "There is a small typo in kafka/coordinator/base.py.\nShould read management rather than managementment.\n\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1964", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1964/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1964/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1964/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1964", "id": 533355555, "node_id": "MDU6SXNzdWU1MzMzNTU1NTU=", "number": 1964, "title": "async KafkaConsumer", "user": {"login": "scientistnik", "id": 3358263, "node_id": "MDQ6VXNlcjMzNTgyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3358263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scientistnik", "html_url": "https://github.com/scientistnik", "followers_url": "https://api.github.com/users/scientistnik/followers", "following_url": "https://api.github.com/users/scientistnik/following{/other_user}", "gists_url": "https://api.github.com/users/scientistnik/gists{/gist_id}", "starred_url": "https://api.github.com/users/scientistnik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scientistnik/subscriptions", "organizations_url": "https://api.github.com/users/scientistnik/orgs", "repos_url": "https://api.github.com/users/scientistnik/repos", "events_url": "https://api.github.com/users/scientistnik/events{/privacy}", "received_events_url": "https://api.github.com/users/scientistnik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-12-05T13:35:23Z", "updated_at": "2020-04-30T15:30:52Z", "closed_at": "2019-12-29T05:30:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\nfrom kafka import KafkaConsumer\r\n\r\nconsumer = KafkaConsumer('my_favorite_topic')\r\nfor msg in consumer:\r\n        print (msg)\r\n```\r\nis there any way to receive messages asynchronously?\r\n\r\nI didn\u2019t think of anything better then:\r\n```python\r\nwhile True:\r\n   await asyncio.sleep(0.01)\r\n   messages = consumer.poll()\r\n   #  parse messages\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1963", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1963/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1963/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1963/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1963", "id": 531136179, "node_id": "MDU6SXNzdWU1MzExMzYxNzk=", "number": 1963, "title": "request_timeout_ms should be reset when check_version fail", "user": {"login": "zhgjun", "id": 19749304, "node_id": "MDQ6VXNlcjE5NzQ5MzA0", "avatar_url": "https://avatars1.githubusercontent.com/u/19749304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhgjun", "html_url": "https://github.com/zhgjun", "followers_url": "https://api.github.com/users/zhgjun/followers", "following_url": "https://api.github.com/users/zhgjun/following{/other_user}", "gists_url": "https://api.github.com/users/zhgjun/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhgjun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhgjun/subscriptions", "organizations_url": "https://api.github.com/users/zhgjun/orgs", "repos_url": "https://api.github.com/users/zhgjun/repos", "events_url": "https://api.github.com/users/zhgjun/events{/privacy}", "received_events_url": "https://api.github.com/users/zhgjun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-02T13:42:04Z", "updated_at": "2019-12-29T23:59:58Z", "closed_at": "2019-12-29T23:59:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have 3 kafka brokers, when i restart my client,it will chose one broker A to Probing broker version by check_version,but first try it failed for some reason. and it will try another broker B,and this time it get version succ. But, it cannot send requests to broker A any more. the log is :\r\n\r\n`\r\n2019-11-21T17:10:57.762+08:00 localhost test WARNING [pid:3011] [MainThread][tid:47912824863152] [conn.py:818 recv] <BrokerConnection node_id=2 host=192.168.0.2:6666 <connected> [IPv4 ('192.168.0.2', 6666)]> timed out after 1994.59195137 ms. Closing connection.\r\n2019-11-21T17:10:57.762+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:724 close] <BrokerConnection node_id=2 host=192.168.0.2:6666 <connected> [IPv4 ('192.168.0.2', 6666)]>: Closing connection. [Error 7] RequestTimedOutError: Request timed out after 1994.59195137 ms\r\n2019-11-21T17:10:57.762+08:00 localhost test WARNING [pid:3011] [MainThread][tid:47912824863152] [client_async.py:336 _conn_state_change] Node 2 connection failed -- refreshing metadata\r\n2019-11-21T17:10:57.763+08:00 localhost test ERROR [pid:3011] [MainThread][tid:47912824863152] [future.py:79 _call_backs] Fetch to node 2 failed: [Error 7] RequestTimedOutError: Request timed out after 1994.59195137 ms\r\n2019-11-21T17:10:57.765+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:360 connect] <BrokerConnection node_id=2 host=192.168.0.2:6666 <connecting> [IPv4 ('192.168.0.2', 6666)]>: connecting to 192.168.0.2:6666 [('192.168.0.2', 6666) IPv4]\r\n2019-11-21T17:10:57.776+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:584 _try_authenticate_plain] <BrokerConnection node_id=2 host=192.168.0.2:6666 <authenticating> [IPv4 ('192.168.0.2', 6666)]>: Authenticated as test via PLAIN\r\n2019-11-21T17:10:57.777+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:415 connect] <BrokerConnection node_id=2 host=192.168.0.2:6666 <authenticating> [IPv4 ('192.168.0.2', 6666)]>: Connection complete.\r\n\r\n2019-11-21T17:11:03.625+08:00 localhost test WARNING [pid:3011] [MainThread][tid:47912824863152] [client_async.py:649 _poll] <BrokerConnection node_id=2 host=192.168.0.2:6666 <connected> [IPv4 ('192.168.0.2', 6666)]> timed out after 1994.59195137 ms. Closing connection.\r\n2019-11-21T17:11:03.626+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:724 close] <BrokerConnection node_id=2 host=192.168.0.2:6666 <connected> [IPv4 ('192.168.0.2', 6666)]>: Closing connection. [Error 7] RequestTimedOutError: Request timed out after 1994.59195137 ms\r\n2019-11-21T17:11:03.626+08:00 localhost test WARNING [pid:3011] [MainThread][tid:47912824863152] [client_async.py:336 _conn_state_change] Node 2 connection failed -- refreshing metadata\r\n2019-11-21T17:11:03.626+08:00 localhost test ERROR [pid:3011] [MainThread][tid:47912824863152] [future.py:79 _call_backs] Fetch to node 2 failed: [Error 7] RequestTimedOutError: Request timed out after 1994.59195137 ms\r\n2019-11-21T17:11:03.628+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:360 connect] <BrokerConnection node_id=2 host=192.168.0.2:6666 <connecting> [IPv4 ('192.168.0.2', 6666)]>: connecting to 192.168.0.2:6666 [('192.168.0.2', 6666) IPv4]\r\n2019-11-21T17:11:03.647+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:584 _try_authenticate_plain] <BrokerConnection node_id=2 host=192.168.0.2:6666 <authenticating> [IPv4 ('192.168.0.2', 6666)]>: Authenticated as test via PLAIN\r\n2019-11-21T17:11:03.647+08:00 localhost test INFO [pid:3011] [MainThread][tid:47912824863152] [conn.py:415 connect] <BrokerConnection node_id=2 host=192.168.0.2:6666 <authenticating> [IPv4 ('192.168.0.2', 6666)]>: Connection complete.\r\n\r\n\r\n`\r\nI  set request_timeout_ms=60000, not 1994.59195137 ms.  and i found when it check_version it will override_config. https://github.com/dpkp/kafka-python/blob/3861e16ea4ef8d60bc6ffc51c0183da33c629642/kafka/conn.py#L1144\r\n\r\nwhen it check_version fail, it raise exception without reset the request_timeout_ms . https://github.com/dpkp/kafka-python/blob/3861e16ea4ef8d60bc6ffc51c0183da33c629642/kafka/conn.py#L1171 . As a result the connection's request_timeout_ms  update to 1994.59195137 ms, when broker need more time to handle requests , the client will failed for the requets timeout.\r\n\r\nso we should reset the configs which changed in  check_version.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1962", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1962/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1962/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1962/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1962", "id": 530020530, "node_id": "MDU6SXNzdWU1MzAwMjA1MzA=", "number": 1962, "title": "Purpose of bootstrap_connected()", "user": {"login": "Tim1000V", "id": 51831676, "node_id": "MDQ6VXNlcjUxODMxNjc2", "avatar_url": "https://avatars1.githubusercontent.com/u/51831676?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tim1000V", "html_url": "https://github.com/Tim1000V", "followers_url": "https://api.github.com/users/Tim1000V/followers", "following_url": "https://api.github.com/users/Tim1000V/following{/other_user}", "gists_url": "https://api.github.com/users/Tim1000V/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tim1000V/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tim1000V/subscriptions", "organizations_url": "https://api.github.com/users/Tim1000V/orgs", "repos_url": "https://api.github.com/users/Tim1000V/repos", "events_url": "https://api.github.com/users/Tim1000V/events{/privacy}", "received_events_url": "https://api.github.com/users/Tim1000V/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-28T17:15:16Z", "updated_at": "2019-12-29T05:17:38Z", "closed_at": "2019-12-29T05:17:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nWhen reading the documentation I was assuming that bootstrap_connected(), would allow me to check if the client was disconnected from the kafka cluster. \r\nBut in cluster.py _boostrap_broker returned by _generate_bootstrap_brokers, can't match any real broker_id (eg: [1,2,3] vs [bootstrap-1,bootstrap-2,bootstrap-3]) .\r\n\r\nSo what's the purpose of this function?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1960", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1960/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1960/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1960/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1960", "id": 527327668, "node_id": "MDU6SXNzdWU1MjczMjc2Njg=", "number": 1960, "title": "Need example describe_acls()", "user": {"login": "rootVIII", "id": 30498791, "node_id": "MDQ6VXNlcjMwNDk4Nzkx", "avatar_url": "https://avatars2.githubusercontent.com/u/30498791?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rootVIII", "html_url": "https://github.com/rootVIII", "followers_url": "https://api.github.com/users/rootVIII/followers", "following_url": "https://api.github.com/users/rootVIII/following{/other_user}", "gists_url": "https://api.github.com/users/rootVIII/gists{/gist_id}", "starred_url": "https://api.github.com/users/rootVIII/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rootVIII/subscriptions", "organizations_url": "https://api.github.com/users/rootVIII/orgs", "repos_url": "https://api.github.com/users/rootVIII/repos", "events_url": "https://api.github.com/users/rootVIII/events{/privacy}", "received_events_url": "https://api.github.com/users/rootVIII/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-22T17:40:47Z", "updated_at": "2019-12-29T05:10:04Z", "closed_at": "2019-12-29T05:10:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Can you give a simple example of describing ACLs? I\u2019m looking at the [source code](https://github.com/dpkp/kafka-python/blob/master/kafka/admin/acl_resource.py) but I\u2019m not sure how to pass a filter object to describe_acls(). Can you give an example of how to declare an ACLFilter object?\r\n\r\nFor instance how would I create a filter object to list by principal name? Or by resource (ie topic, group, or cluster)?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1958", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1958/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1958/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1958/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1958", "id": 524828581, "node_id": "MDU6SXNzdWU1MjQ4Mjg1ODE=", "number": 1958, "title": "Producer support transaction", "user": {"login": "SCUTJcfeng", "id": 14869384, "node_id": "MDQ6VXNlcjE0ODY5Mzg0", "avatar_url": "https://avatars0.githubusercontent.com/u/14869384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SCUTJcfeng", "html_url": "https://github.com/SCUTJcfeng", "followers_url": "https://api.github.com/users/SCUTJcfeng/followers", "following_url": "https://api.github.com/users/SCUTJcfeng/following{/other_user}", "gists_url": "https://api.github.com/users/SCUTJcfeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/SCUTJcfeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SCUTJcfeng/subscriptions", "organizations_url": "https://api.github.com/users/SCUTJcfeng/orgs", "repos_url": "https://api.github.com/users/SCUTJcfeng/repos", "events_url": "https://api.github.com/users/SCUTJcfeng/events{/privacy}", "received_events_url": "https://api.github.com/users/SCUTJcfeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-19T07:35:34Z", "updated_at": "2019-12-29T05:00:37Z", "closed_at": "2019-12-29T05:00:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "I can't find out any information about transaction support for producer, so here is what I am asking.\r\n\r\nWhile actually in JAVA implementation, transaction in producer is supported.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1957", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1957/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1957/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1957/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1957", "id": 524694814, "node_id": "MDU6SXNzdWU1MjQ2OTQ4MTQ=", "number": 1957, "title": "Clarification on ssl_check_hostname option", "user": {"login": "dnj12345", "id": 11042721, "node_id": "MDQ6VXNlcjExMDQyNzIx", "avatar_url": "https://avatars3.githubusercontent.com/u/11042721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dnj12345", "html_url": "https://github.com/dnj12345", "followers_url": "https://api.github.com/users/dnj12345/followers", "following_url": "https://api.github.com/users/dnj12345/following{/other_user}", "gists_url": "https://api.github.com/users/dnj12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/dnj12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dnj12345/subscriptions", "organizations_url": "https://api.github.com/users/dnj12345/orgs", "repos_url": "https://api.github.com/users/dnj12345/repos", "events_url": "https://api.github.com/users/dnj12345/events{/privacy}", "received_events_url": "https://api.github.com/users/dnj12345/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-19T00:12:56Z", "updated_at": "2019-12-29T04:58:20Z", "closed_at": "2019-12-29T04:58:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nis the ssl_cafile option required when sercurity_protocol is 'SSL'? My producer is unable to connect when my producer config includes ssl_check_hostname=False, and my broker config has ssl.client.auth=requested. If you provide a proper CA file, the producer connects properly. I was under the assumption that `ssl_check_hostname=False` implies no server cert verification. Is this not true?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1954", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1954/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1954/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1954/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1954", "id": 523414067, "node_id": "MDU6SXNzdWU1MjM0MTQwNjc=", "number": 1954, "title": "kafka producer via global django.settings doesnt not work", "user": {"login": "roushan05", "id": 13029797, "node_id": "MDQ6VXNlcjEzMDI5Nzk3", "avatar_url": "https://avatars1.githubusercontent.com/u/13029797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roushan05", "html_url": "https://github.com/roushan05", "followers_url": "https://api.github.com/users/roushan05/followers", "following_url": "https://api.github.com/users/roushan05/following{/other_user}", "gists_url": "https://api.github.com/users/roushan05/gists{/gist_id}", "starred_url": "https://api.github.com/users/roushan05/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roushan05/subscriptions", "organizations_url": "https://api.github.com/users/roushan05/orgs", "repos_url": "https://api.github.com/users/roushan05/repos", "events_url": "https://api.github.com/users/roushan05/events{/privacy}", "received_events_url": "https://api.github.com/users/roushan05/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-15T11:22:18Z", "updated_at": "2019-12-29T04:49:29Z", "closed_at": "2019-12-29T04:48:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using kafka-python in a django project. Initialised the producer in settings.py file. Calling producer.send is not sending the messages.\r\n\r\nEarlier my producer was initialised in a function everytime and I was using producer.send().get(timeout=10) to send messages and it worked fine. Now I have changed the initialisation to settings file  and removed `.get()` while calling `send` and it is not working.\r\nOld working code below:\r\n\r\nIn `module_x.py`:\r\n```\r\nfrom kafka import KafkaProducer\r\n\r\ndef my_func():\r\n   KAFKA_CONNECTION = KafkaProducer(bootstrap_servers=CONFIGS.KAFKA_URL)\r\n   producer = KAFKA_CONNECTION\r\n   producer.send(topic, key=key, value=json.dumps(data)).get(timeout=1)\r\n```\r\nChanged code below:\r\nNow changed the initialisation to settings file to avoid initialisation everytime `my_func` is called.\r\n\r\nIn `settings.py`:\r\n```\r\nKAFKA_CONNECTION = KafkaProducer(bootstrap_servers=CONFIGS.KAFKA_URL)\r\n```\r\n\r\nIn `module_x.py`:\r\n```\r\nfrom django.conf import settings\r\n\r\ndef my_func():\r\n   producer = settings.KAFKA_CONNECTION\r\n   producer.send(topic, key=key, value=json.dumps(data))\r\n```\r\nNote that I even tried with `.get(timeout=1)` on `send` but I was getting `Kafka.TiemoutError`\r\n\r\nTried using `linger_ms=5` in while initialising producer but that also doesn't help", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1952", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1952/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1952/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1952/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1952", "id": 522583555, "node_id": "MDU6SXNzdWU1MjI1ODM1NTU=", "number": 1952, "title": "Kafka write Error in Linux", "user": {"login": "Laurel-rao", "id": 42195541, "node_id": "MDQ6VXNlcjQyMTk1NTQx", "avatar_url": "https://avatars2.githubusercontent.com/u/42195541?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Laurel-rao", "html_url": "https://github.com/Laurel-rao", "followers_url": "https://api.github.com/users/Laurel-rao/followers", "following_url": "https://api.github.com/users/Laurel-rao/following{/other_user}", "gists_url": "https://api.github.com/users/Laurel-rao/gists{/gist_id}", "starred_url": "https://api.github.com/users/Laurel-rao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Laurel-rao/subscriptions", "organizations_url": "https://api.github.com/users/Laurel-rao/orgs", "repos_url": "https://api.github.com/users/Laurel-rao/repos", "events_url": "https://api.github.com/users/Laurel-rao/events{/privacy}", "received_events_url": "https://api.github.com/users/Laurel-rao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-14T02:24:20Z", "updated_at": "2019-12-29T03:40:12Z", "closed_at": "2019-12-29T03:39:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "# send_message Error\r\n\r\n- Environment : Redhat 7\r\n- producer : SimpleProducer\r\n- client : SimpleClient\r\n- kafka-python: 1.4.4\r\n\r\n> The following is log.\r\n```\r\n\r\n2019-11-13 16:23:16 -[/usr/local/src/development/SNO_collection_plat/project_dir/app/Operater/views.py:174] - ERROR - Traceback (most recent call last):\r\n  File \"/usr/local/src/development/SNO_collection_plat/project_dir/app/Operater/views.py\", line 172, in handle_device\r\n    self.write_to_db(result, device_login_msg.get(\"login_data\"))\r\n  File \"/usr/local/src/development/SNO_collection_plat/project_dir/app/Operater/views.py\", line 183, in write_to_db\r\n    kafka.produce(json.dumps(data))\r\n  File \"/usr/local/src/development/SNO_collection_plat/project_dir/app/Persistence/kafka_producer.py\", line 24, in produce\r\n    self._producer.send_messages(self._topic, msg.encode('utf-8'))\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/producer/simple.py\", line 50, in send_messages\r\n    topic, partition, *msg\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/producer/base.py\", line 387, in send_messages\r\n    return self._send_messages(topic, partition, *msg)\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/producer/base.py\", line 430, in _send_messages\r\n    fail_on_error=self.sync_fail_on_error\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/client.py\", line 643, in send_produce_request\r\n    resps = self._send_broker_aware_request(payloads, encoder, decoder)\r\n  File \"/usr/local/lib/python3.6/site-packages/kafka/client.py\", line 276, in _send_broker_aware_request\r\n    read_socks, _, _ = select.select(sockets, [], [])\r\nValueError: filedescriptor out of range in select(\uff09\r\n```\r\n\r\n- I had seen the issue #613 #640\uff0cbut they both use the client_async. Because of the data was important, I afraid the data was lost.\r\n- I have google the select function in Linux was limited by 1024, should we upgrade the select to \r\nthe selectors.\r\n- And so when i use the Windows, there's no error comes up.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1950", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1950/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1950/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1950/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1950", "id": 520577820, "node_id": "MDU6SXNzdWU1MjA1Nzc4MjA=", "number": 1950, "title": "simple consumer: WARN: Fetch size too small ", "user": {"login": "Captcool", "id": 14876126, "node_id": "MDQ6VXNlcjE0ODc2MTI2", "avatar_url": "https://avatars0.githubusercontent.com/u/14876126?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Captcool", "html_url": "https://github.com/Captcool", "followers_url": "https://api.github.com/users/Captcool/followers", "following_url": "https://api.github.com/users/Captcool/following{/other_user}", "gists_url": "https://api.github.com/users/Captcool/gists{/gist_id}", "starred_url": "https://api.github.com/users/Captcool/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Captcool/subscriptions", "organizations_url": "https://api.github.com/users/Captcool/orgs", "repos_url": "https://api.github.com/users/Captcool/repos", "events_url": "https://api.github.com/users/Captcool/events{/privacy}", "received_events_url": "https://api.github.com/users/Captcool/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-10T09:45:04Z", "updated_at": "2019-12-29T03:32:01Z", "closed_at": "2019-12-29T03:32:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "I use simple consumer to consume messages of __consumer_offsets topic to get the lags.\r\nwhen I upgrade the kafka server to 2.2 , the log was all of the warn message:\r\n2019-11-10 17:29:57,533 WARNING simple.py _fetch-432 line Fetch size too small, increase to 8388608 (2x) and retry\r\n\r\n\r\nand follow is the consumer parameter\r\n\r\n        consumer = SimpleConsumer(kafka_handler, 'lag_monitor', '__consumer_offsets', auto_commit=False,\r\n                partitions=[ID], buffer_size=MAX_FETCH_BUFFER_SIZE_BYTES, max_buffer_size=1024 * 1024 * 2) \r\n\r\nand the MAX_FETCH_BUFFER_SIZE_BYTES is 8*4096\r\n\r\nin the old 1.0 kafka server it worked well but in kafka server 2.2 it not worked\r\n\r\nthe warn :\r\n![image](https://user-images.githubusercontent.com/14876126/68541979-8f9a4b00-03e1-11ea-8995-432512bda0f6.png)\r\n\r\nthe consumer parameter:\r\n![image](https://user-images.githubusercontent.com/14876126/68541989-accf1980-03e1-11ea-94dc-3bedfe39ee91.png)\r\n\r\n\r\nwhy the warning will appear\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1949", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1949/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1949/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1949/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1949", "id": 520273757, "node_id": "MDU6SXNzdWU1MjAyNzM3NTc=", "number": 1949, "title": "Performance of assign(), seek(), poll()", "user": {"login": "zhengl7", "id": 871629, "node_id": "MDQ6VXNlcjg3MTYyOQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/871629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhengl7", "html_url": "https://github.com/zhengl7", "followers_url": "https://api.github.com/users/zhengl7/followers", "following_url": "https://api.github.com/users/zhengl7/following{/other_user}", "gists_url": "https://api.github.com/users/zhengl7/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhengl7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhengl7/subscriptions", "organizations_url": "https://api.github.com/users/zhengl7/orgs", "repos_url": "https://api.github.com/users/zhengl7/repos", "events_url": "https://api.github.com/users/zhengl7/events{/privacy}", "received_events_url": "https://api.github.com/users/zhengl7/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-08T23:00:21Z", "updated_at": "2019-12-29T01:03:12Z", "closed_at": "2019-12-29T01:03:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Not really a bug but hope to get some explanation on some performance issues seen with random access using `seek(partition, offset)`.\r\n\r\nThe Kafka has a single topic, 10 nodes, less than 100 partitions, message rate about 1.5k/sec. Has a few regular streaming consumer groups that simply subscribe to the topic, and one special consumer group.\r\n\r\nThe special consumer group doesn't simply subscribe to the topic and poll, but instead, listens to a message queue for a Kafka message's `partition` and `offset`, and then in turn grab the message from Kafka by calling the basically the following sequence\r\n\r\n```python\r\nconsumer.assign([partition])\r\nconsumer.seek(partition, offset)\r\nconsumer.poll(1, timeout)\r\n```\r\nThe offsets in the queue are all of messages recently written into Kafka.\r\n\r\nHere is where things get interesting for this special consumer, \r\n\r\n1. If only put a subset of Kafka messges' offset in the queue, let's say 1% or less, then the poll call has pretty noticeable timeout rates, that it couldn't get message for even 1 second, while success polls are less than 5ms.\r\n\r\n2. But, if we put all the Kafka's messages offsets in the message queue, meaning every single Kafka message will be obtained once using its offset by this client, the timeout is almost completely gone! \r\n\r\n3. Seems that if the message queue has delays, meaning the consumer only gets offsets of not the most recent Kafka messages (offsets a few thousands earlier than the latest), then the outgoing traffic from Kafka increased tremendously. If the ad-hoc polls are mostly on most recent messages, then nothing too abnormal.  \r\n\r\nThis led me to guess that the client is actually getting more messages back from Kafka behind the scene even if the caller only calls `poll(1)`, and has some kind of smart caching. I tried to read the source a little bit but still couldn't connect the full circle. Hopefully someone with deeper knowledge can provide some insights.  \r\n\r\n\r\nkafka-python version: 1.4.6\r\nbroker version: 2.2.0\r\nclient flags: enable_auto_commit=False\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1947", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1947/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1947/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1947/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1947", "id": 520094936, "node_id": "MDU6SXNzdWU1MjAwOTQ5MzY=", "number": 1947, "title": "Not processing messages when queue is large.", "user": {"login": "trevordjones", "id": 7663582, "node_id": "MDQ6VXNlcjc2NjM1ODI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7663582?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trevordjones", "html_url": "https://github.com/trevordjones", "followers_url": "https://api.github.com/users/trevordjones/followers", "following_url": "https://api.github.com/users/trevordjones/following{/other_user}", "gists_url": "https://api.github.com/users/trevordjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/trevordjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trevordjones/subscriptions", "organizations_url": "https://api.github.com/users/trevordjones/orgs", "repos_url": "https://api.github.com/users/trevordjones/repos", "events_url": "https://api.github.com/users/trevordjones/events{/privacy}", "received_events_url": "https://api.github.com/users/trevordjones/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-08T16:01:50Z", "updated_at": "2019-11-19T18:28:04Z", "closed_at": "2019-11-19T18:28:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi all,\r\n\r\nI've been banging my head against the wall for a while on this one. I'd be happy to consider any suggestions on this.\r\n\r\nWe have a task that runs once a day (or manually) that connects to Kafka (hosted on Heroku). Its intent is to process all the messages since its last run. Here are the code snippets for what we're doing (with typing).\r\n\r\n```python\r\nAUTO_OFFSET_RESET: Final = 'earliest'\r\nCONSUMER_TIMEOUT_MS: Final = 500\r\nGROUP_ID: Final = 'unique_group_id'\r\n\r\ndef make_consumer(topic: str) -> KafkaConsumer:\r\n    return KafkaConsumer(\r\n        topic,\r\n        auto_offset_reset=AUTO_OFFSET_RESET,\r\n        consumer_timeout_ms=CONSUMER_TIMEOUT_MS,\r\n        group_id=f'{KAFKA_PREFIX}{GROUP_ID}',\r\n        )\r\n...\r\ndef extract(\r\n        schema: avro.schema.Schema,\r\n        consumer: KafkaConsumer,\r\n        ) -> T.Iterator[T.Dict[str, object]]:\r\n    datum_reader = avro.io.DatumReader(reader_schema=schema)\r\n    for msg in consumer:\r\n        logger.info('Processing message from Kafka')\r\n        try:\r\n            assert msg.value\r\n            message_buf = io.BytesIO(msg.value)\r\n            data_reader = avro.datafile.DataFileReader(message_buf, datum_reader)\r\n            for record in data_reader:\r\n                logger.info('Processing record in data_reader')\r\n                yield record\r\n        finally:\r\n            data_reader.close()\r\n```\r\n\r\nThis seems pretty basic and works well - as long as the size of the message queue is relatively small. I don't have any hard numbers, but from what I observed, under 20MB (10-15,000 messages) in the queue is fine, over 90MB (70,000+ messages) and we never make it inside the iterator. This also is starting from \"earliest\" and none of the messages have been processed, so it's at the very beginning.\r\n\r\nI would hope the logs would provide some kind of error or warning message. We have the level set to DEBUG but there is nothing there to indicate any problems. We connect to brokers just fine, send FetchRequests, etc. No warnings or errors messages. Let me know if you'd like a particular snippet of them.\r\n\r\nAt first I thought it might be a `kafka-python` problem and not a Heroku problem because we have Ruby libraries that can process messages just fine. So I decided to try this locally using a Kafka docker image and it works just fine with a large volume (75,000+ messages).\r\n\r\nSo now I'm back to thinking it might be a Heroku problem or a combination of the two and am filing a support ticket with them as well.\r\n\r\nIf anyone has any insights on the above behavior and what might be wrong with our implementation, please let me know.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1946", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1946/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1946/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1946/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1946", "id": 519190297, "node_id": "MDU6SXNzdWU1MTkxOTAyOTc=", "number": 1946, "title": "when run  docker-compose run --rm web upgrade  has errors", "user": {"login": "18308996929", "id": 51193865, "node_id": "MDQ6VXNlcjUxMTkzODY1", "avatar_url": "https://avatars0.githubusercontent.com/u/51193865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/18308996929", "html_url": "https://github.com/18308996929", "followers_url": "https://api.github.com/users/18308996929/followers", "following_url": "https://api.github.com/users/18308996929/following{/other_user}", "gists_url": "https://api.github.com/users/18308996929/gists{/gist_id}", "starred_url": "https://api.github.com/users/18308996929/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/18308996929/subscriptions", "organizations_url": "https://api.github.com/users/18308996929/orgs", "repos_url": "https://api.github.com/users/18308996929/repos", "events_url": "https://api.github.com/users/18308996929/events{/privacy}", "received_events_url": "https://api.github.com/users/18308996929/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-07T10:52:28Z", "updated_at": "2019-11-08T12:42:44Z", "closed_at": "2019-11-08T12:42:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "  File \"/usr/local/lib/python2.7/site-packages/sentry_kafka-1.1-py2.7.egg/sentry_kafka/models.py\", line 9, in <module>\r\n    from kafka import KafkaClient, SimpleProducer\r\nImportError: cannot import name SimpleProducer\r\n\r\nwhy has these error\uff1f\uff1f", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1945", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1945/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1945/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1945/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1945", "id": 517695146, "node_id": "MDU6SXNzdWU1MTc2OTUxNDY=", "number": 1945, "title": "`reconnect_backoff_max_ms` does not work in case Kafka cluster drops", "user": {"login": "georgehdd", "id": 4894494, "node_id": "MDQ6VXNlcjQ4OTQ0OTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4894494?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgehdd", "html_url": "https://github.com/georgehdd", "followers_url": "https://api.github.com/users/georgehdd/followers", "following_url": "https://api.github.com/users/georgehdd/following{/other_user}", "gists_url": "https://api.github.com/users/georgehdd/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgehdd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgehdd/subscriptions", "organizations_url": "https://api.github.com/users/georgehdd/orgs", "repos_url": "https://api.github.com/users/georgehdd/repos", "events_url": "https://api.github.com/users/georgehdd/events{/privacy}", "received_events_url": "https://api.github.com/users/georgehdd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-05T10:48:56Z", "updated_at": "2020-01-02T16:30:05Z", "closed_at": "2019-12-29T00:42:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "When the Kafka cluster goes down, the KafkaClient will try to reconnect forever and will not fail after the `reconnect_backoff_max_ms` have elapsed.\r\n\r\n**Reproduce**: Run the following code while Kafka is available:\r\n```\r\nfrom kafka import KafkaConsumer\r\nconsumer = KafkaConsumer(\"some_topic\", bootstrap_servers=\"kafkabroker:9092\")\r\nfor message in consumer:\r\n    print(\"Received Message\")\r\n```\r\n\r\nThen while it's running, take Kafka down.\r\n\r\n**Expected behavior**: After `reconnect_backoff_max_ms` have elapsed, the client should fail / an exception should be thrown.\r\n\r\n**Actual behavior**: `for message in consumer:` is stuck infinitely", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1944", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1944/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1944/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1944/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1944", "id": 517665074, "node_id": "MDU6SXNzdWU1MTc2NjUwNzQ=", "number": 1944, "title": "callbacks for KafkaProducer?", "user": {"login": "chenly23", "id": 40755314, "node_id": "MDQ6VXNlcjQwNzU1MzE0", "avatar_url": "https://avatars1.githubusercontent.com/u/40755314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenly23", "html_url": "https://github.com/chenly23", "followers_url": "https://api.github.com/users/chenly23/followers", "following_url": "https://api.github.com/users/chenly23/following{/other_user}", "gists_url": "https://api.github.com/users/chenly23/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenly23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenly23/subscriptions", "organizations_url": "https://api.github.com/users/chenly23/orgs", "repos_url": "https://api.github.com/users/chenly23/repos", "events_url": "https://api.github.com/users/chenly23/events{/privacy}", "received_events_url": "https://api.github.com/users/chenly23/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-05T09:54:56Z", "updated_at": "2019-11-08T13:30:35Z", "closed_at": "2019-11-08T13:30:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI know there is a way to add callbacks in `future` as the document says\r\n```\r\n# produce asynchronously with callbacks\r\nproducer.send('my-topic', b'raw_bytes').add_callback(on_send_success).add_errback(on_send_error)\r\n```\r\nbut some callbacks are common, such as logging  the sent message only, but I need to add the common callback every time I send the message?\r\nIs it possible to define the callbacks in `KafkaProducer`? \r\n```\r\nproducer = KafkaProducer(callbacks=[], errbacks=[])\r\nproducer.send('my-topic', b'raw_bytes')\r\n```\r\nOr can I configue some common callbacks in settings for Kafka?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1943", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1943/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1943/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1943/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1943", "id": 517134714, "node_id": "MDU6SXNzdWU1MTcxMzQ3MTQ=", "number": 1943, "title": "NodeNotReadyError with manual commits", "user": {"login": "hcsfred", "id": 6825594, "node_id": "MDQ6VXNlcjY4MjU1OTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6825594?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hcsfred", "html_url": "https://github.com/hcsfred", "followers_url": "https://api.github.com/users/hcsfred/followers", "following_url": "https://api.github.com/users/hcsfred/following{/other_user}", "gists_url": "https://api.github.com/users/hcsfred/gists{/gist_id}", "starred_url": "https://api.github.com/users/hcsfred/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hcsfred/subscriptions", "organizations_url": "https://api.github.com/users/hcsfred/orgs", "repos_url": "https://api.github.com/users/hcsfred/repos", "events_url": "https://api.github.com/users/hcsfred/events{/privacy}", "received_events_url": "https://api.github.com/users/hcsfred/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-04T12:58:17Z", "updated_at": "2019-12-29T23:59:09Z", "closed_at": "2019-12-29T23:59:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Simple Kafka consumer where I manually commit records (`enable_auto_commit=False`):\r\n```\r\nfor record in consumer:\r\n    ...\r\n    consumer.commit()\r\n```\r\nI often encounter `Error sending OffsetCommitRequest_v2 to node coordinator-1 [NodeNotReadyError: coordinator-1]`. It seemingly does not have any functionality consequences but it does pollute the logs. Any idea why this may be happening and/or could this be a raised exception so that I may handle it myself?\r\n\r\nUsing latest version. Related issues: https://github.com/dpkp/kafka-python/issues/1354, https://github.com/dpkp/kafka-python/issues/1572\r\n\r\nFull debug log of the event: \r\n```\r\n2019-11-04 12:07:46,605 INFO 8/MainThread: Consuming record 1571 from topic TOPIC\r\n2019-11-04 12:07:46,607 DEBUG 8/MainThread: Sending offset-commit request with {TopicPartition(topic='TOPIC', partition=0): OffsetAndMetadata(offset=1572, metadata='')} for group SERVICE to coordinator-1\r\n2019-11-04 12:07:46,607 ERROR 8/MainThread: Error sending OffsetCommitRequest_v2 to node coordinator-1 [NodeNotReadyError: coordinator-1]\r\n2019-11-04 12:07:46,608 DEBUG 8/MainThread: Initiating connection to node coordinator-1 at kafka1:9092\r\n2019-11-04 12:07:46,614 DEBUG 8/MainThread: <BrokerConnection node_id=coordinator-1 host=kafka1:9092 <disconnected> [unspecified None]>: creating new socket\r\n2019-11-04 12:07:46,615 DEBUG 8/MainThread: <BrokerConnection node_id=coordinator-1 host=kafka1:9092 <disconnected> [IPv4 ('10.0.9.173', 9092)]>: setting socket option (6, 1, 1)\r\n2019-11-04 12:07:46,615 INFO 8/MainThread: <BrokerConnection node_id=coordinator-1 host=kafka1:9092 <connecting> [IPv4 ('10.0.9.173', 9092)]>: connecting to kafka1:9092 [('10.0.9.173', 9092) IPv4]\r\n2019-11-04 12:07:46,716 DEBUG 8/MainThread: Sending offset-commit request with {TopicPartition(topic='TOPIC', partition=0): OffsetAndMetadata(offset=1572, metadata='')} for group SERVICE  to coordinator-1\r\n2019-11-04 12:07:46,716 ERROR 8/MainThread: Error sending OffsetCommitRequest_v2 to node coordinator-1 [NodeNotReadyError: coordinator-1]\r\n2019-11-04 12:07:46,716 DEBUG 8/MainThread: <BrokerConnection node_id=coordinator-1 host=kafka1:9092 <connecting> [IPv4 ('10.0.9.173', 9092)]>: established TCP connection\r\n2019-11-04 12:07:46,716 INFO 8/MainThread: <BrokerConnection node_id=coordinator-1 host=kafka1:9092 <connecting> [IPv4 ('10.0.9.173', 9092)]>: Connection complete.\r\n2019-11-04 12:07:46,716 DEBUG 8/MainThread: Node coordinator-1 connected\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1942", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1942/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1942/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1942/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1942", "id": 515952277, "node_id": "MDU6SXNzdWU1MTU5NTIyNzc=", "number": 1942, "title": "Rebalancing not occurring", "user": {"login": "karthikmurugadoss", "id": 3501272, "node_id": "MDQ6VXNlcjM1MDEyNzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3501272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karthikmurugadoss", "html_url": "https://github.com/karthikmurugadoss", "followers_url": "https://api.github.com/users/karthikmurugadoss/followers", "following_url": "https://api.github.com/users/karthikmurugadoss/following{/other_user}", "gists_url": "https://api.github.com/users/karthikmurugadoss/gists{/gist_id}", "starred_url": "https://api.github.com/users/karthikmurugadoss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karthikmurugadoss/subscriptions", "organizations_url": "https://api.github.com/users/karthikmurugadoss/orgs", "repos_url": "https://api.github.com/users/karthikmurugadoss/repos", "events_url": "https://api.github.com/users/karthikmurugadoss/events{/privacy}", "received_events_url": "https://api.github.com/users/karthikmurugadoss/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-01T07:20:37Z", "updated_at": "2019-12-29T00:10:52Z", "closed_at": "2019-12-29T00:10:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Great work on the library and documentation!\r\n\r\nI'm having a bit of difficulty understanding when rebalancing should and should not occur. In particular my use case is as follows:\r\n- One topic with two partitions\r\n- Initially one consumer (a second consumer joins at a later point of time)\r\n- The producer sends 20 messages (Round robin partitioning) to the topic\r\n- The consumers take 1 minute to process each message\r\n\r\nAs soon as I send the 20 messages (10 to each partition), the single consumer as expected begins to process them. However, at this point (while consumer 1 is busy processing), if I introduce consumer 2, I had expected a rebalance to occur and the two partitions to be split - one for each consumer. \r\n\r\nHowever, the rebalance does not occur right away. The kafka logs show the following line\r\n```\r\nPreparing to rebalance group 1 in state PreparingRebalance with old generation 44 (__consumer_offsets-49) (reason: Adding new member kafka-python-1.4.7-95992b00-ff44-42bc-86a1-6374657b5bc9\r\n```\r\nI see the log message for consumer 1 `Heartbeat failed for group 1 because it is rebalancing` and this keeps repeating until eventually `Heartbeat poll expired, leaving group`. If the consumer did not expire (by reducing processing time for each message or reducing the number of message), then the rebalance occurs only after all of the 20 messages are processed by consumer 1 (that is, consumer 2 had been idle all this time). \r\n\r\nAny insight on the above would be very helpful! Am I missing something basic? (is 1 minute a very long processing time? Is rebalancing supposed to happen only when all or majority of consumers are \"idle\"?)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1939", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1939/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1939/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1939/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1939", "id": 513789111, "node_id": "MDU6SXNzdWU1MTM3ODkxMTE=", "number": 1939, "title": "Frequent rebalance of Consumer group when using kafka-python 1.4.6 and 1.4.7", "user": {"login": "naoshadmehta", "id": 11164606, "node_id": "MDQ6VXNlcjExMTY0NjA2", "avatar_url": "https://avatars3.githubusercontent.com/u/11164606?v=4", "gravatar_id": "", "url": "https://api.github.com/users/naoshadmehta", "html_url": "https://github.com/naoshadmehta", "followers_url": "https://api.github.com/users/naoshadmehta/followers", "following_url": "https://api.github.com/users/naoshadmehta/following{/other_user}", "gists_url": "https://api.github.com/users/naoshadmehta/gists{/gist_id}", "starred_url": "https://api.github.com/users/naoshadmehta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/naoshadmehta/subscriptions", "organizations_url": "https://api.github.com/users/naoshadmehta/orgs", "repos_url": "https://api.github.com/users/naoshadmehta/repos", "events_url": "https://api.github.com/users/naoshadmehta/events{/privacy}", "received_events_url": "https://api.github.com/users/naoshadmehta/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-10-29T09:22:49Z", "updated_at": "2020-07-15T20:10:47Z", "closed_at": "2020-03-02T16:59:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "We're seeing this problem with 1.4.6 and 1.4.7. We have 3 broker nodes and 6 consumers per node. Our topic has 18 partitions and for about an hour or so, we see no issues. After an hour or so, we start seeing:\r\n\r\nserver.log:[2019-10-29 01:28:16,927] INFO [GroupCoordinator 1]: Preparing to rebalance group tasks_group in state PreparingRebalance with old generation 9 (__consumer_offsets-17) (reason: removing member kafka-python-1.4.7-52039bb5-17c5-42d7-8af3-d95f0dbc3f3f on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)\r\n\r\nA few seconds later, we see a new consumer being added:\r\nserver.log:[2019-10-29 01:29:00,612] INFO [GroupCoordinator 1]: Preparing to rebalance group tasks_group in state PreparingRebalance with old generation 10 (__consumer_offsets-17) (reason: Adding new member kafka-python-1.4.7-27f5bbac-b1e6-41a9-999b-abb56868e9e2) (kafka.coordinator.group.GroupCoordinator)\r\n\r\nAfter the system gets into this state, we keep seeing remove and add of new members to the consumer group and in a few hours, it reaches a point where the rebalance takes more than 10 minutes to complete. By this time, the lag on the partitions is so high that its impossible to recover from this situation.\r\n\r\nOur broker config:\r\nmax.session.timeout.ms: 30000\r\nheartbeat.interval.ms: 10000\r\n\r\nConsumer config:\r\nmax.session.timeout.ms: 10000\r\nheartbeat.interval.ms: 3000\r\nconsumer_timeout_ms: 2000\r\n\r\nEverything else is left as default.\r\n\r\nAny suggestions on what we could tweak in the broker or consumer configs to avoid seeing this issue? I came across this other issue - https://github.com/dpkp/kafka-python/issues/1418 which has been closed after a fix was committed to 1.4.7 but we still see the problem with 1.4.7. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1937", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1937/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1937/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1937/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1937", "id": 512320564, "node_id": "MDU6SXNzdWU1MTIzMjA1NjQ=", "number": 1937, "title": "does this lib have built-in confluent-schema-register support ", "user": {"login": "hackrole", "id": 2100811, "node_id": "MDQ6VXNlcjIxMDA4MTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/2100811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hackrole", "html_url": "https://github.com/hackrole", "followers_url": "https://api.github.com/users/hackrole/followers", "following_url": "https://api.github.com/users/hackrole/following{/other_user}", "gists_url": "https://api.github.com/users/hackrole/gists{/gist_id}", "starred_url": "https://api.github.com/users/hackrole/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hackrole/subscriptions", "organizations_url": "https://api.github.com/users/hackrole/orgs", "repos_url": "https://api.github.com/users/hackrole/repos", "events_url": "https://api.github.com/users/hackrole/events{/privacy}", "received_events_url": "https://api.github.com/users/hackrole/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-25T06:01:55Z", "updated_at": "2019-11-08T13:20:17Z", "closed_at": "2019-11-08T13:20:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "this  lib [confluent-kafka-python usage](https://github.com/confluentinc/confluent-kafka-python#usage) seems have confluent-schema-register built-in support.\r\n\r\nI check the doc and the code for this lib, not find any signs like this.\r\n\r\nso I wander does it support this or have plan to support this? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1936", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1936/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1936/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1936/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1936", "id": 512048103, "node_id": "MDU6SXNzdWU1MTIwNDgxMDM=", "number": 1936, "title": "how to use partition_assignment_strategy?", "user": {"login": "venthur", "id": 128459, "node_id": "MDQ6VXNlcjEyODQ1OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/128459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/venthur", "html_url": "https://github.com/venthur", "followers_url": "https://api.github.com/users/venthur/followers", "following_url": "https://api.github.com/users/venthur/following{/other_user}", "gists_url": "https://api.github.com/users/venthur/gists{/gist_id}", "starred_url": "https://api.github.com/users/venthur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/venthur/subscriptions", "organizations_url": "https://api.github.com/users/venthur/orgs", "repos_url": "https://api.github.com/users/venthur/repos", "events_url": "https://api.github.com/users/venthur/events{/privacy}", "received_events_url": "https://api.github.com/users/venthur/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-24T16:18:35Z", "updated_at": "2019-11-08T13:44:52Z", "closed_at": "2019-11-08T13:16:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nthe `Consumer` supports as configuration the `partition_assignment_strategy`, which defaults [according to the documentation](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer) to a list of two objects: `[RangePartitionAssignor, RoundRobinPartitionAssignor]`? I don't get how this works, will the consumer chose one of them randomly? What is the correct way to chose i.e. specifically the `RoundRobinPartitionAssignor` -- if i have a list of topics distributed over 3 partitions and I want to utilize more than 3 processes to consume as fast as possible?\r\n\r\nThanks, for the great library and your work!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1935", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1935/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1935/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1935/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1935", "id": 512000170, "node_id": "MDU6SXNzdWU1MTIwMDAxNzA=", "number": 1935, "title": "KafkaConsumer  class doesn't work with security_protocol as a SASL_PLAINTEXT ", "user": {"login": "fedotarte", "id": 24719399, "node_id": "MDQ6VXNlcjI0NzE5Mzk5", "avatar_url": "https://avatars0.githubusercontent.com/u/24719399?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fedotarte", "html_url": "https://github.com/fedotarte", "followers_url": "https://api.github.com/users/fedotarte/followers", "following_url": "https://api.github.com/users/fedotarte/following{/other_user}", "gists_url": "https://api.github.com/users/fedotarte/gists{/gist_id}", "starred_url": "https://api.github.com/users/fedotarte/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fedotarte/subscriptions", "organizations_url": "https://api.github.com/users/fedotarte/orgs", "repos_url": "https://api.github.com/users/fedotarte/repos", "events_url": "https://api.github.com/users/fedotarte/events{/privacy}", "received_events_url": "https://api.github.com/users/fedotarte/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-24T14:55:47Z", "updated_at": "2019-12-28T23:35:51Z", "closed_at": "2019-12-28T23:35:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "if you create a KafkaConsumer with SASL auth parameters like that:\r\n```\r\n            consumer = KafkaConsumer(bootstrap_servers=str_broker_host,\r\n                                security_protocol='SASL_PLAINTEXT',\r\n                                sasl_mechanism='PLAIN',\r\n                                sasl_plain_username='user',\r\n                                sasl_plain_password='password')\r\n```\r\nit won't work with error Errors.NoBrokersAvailable()\r\n\r\ni need it to get topic sizes:\r\n```\r\n# topic name and partition\r\nend_offset = consumer.end_offsets([TopicPartition(topic[0], 0)])\r\n```\r\n\r\nAlso i checked the class KafkaConsumer, and there is no value  \"SASL_PLAINTEXT\" for security_protocol.\r\n\r\n\r\nHow can i get the topic size for 0 partition without calling the KafkaConsumer?\r\n\r\n\r\nThank you in advance.\r\n\r\n \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1932", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1932/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1932/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1932/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1932", "id": 509605240, "node_id": "MDU6SXNzdWU1MDk2MDUyNDA=", "number": 1932, "title": "KafkaClient not loading metadata upon creation", "user": {"login": "zarnovican", "id": 781024, "node_id": "MDQ6VXNlcjc4MTAyNA==", "avatar_url": "https://avatars3.githubusercontent.com/u/781024?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zarnovican", "html_url": "https://github.com/zarnovican", "followers_url": "https://api.github.com/users/zarnovican/followers", "following_url": "https://api.github.com/users/zarnovican/following{/other_user}", "gists_url": "https://api.github.com/users/zarnovican/gists{/gist_id}", "starred_url": "https://api.github.com/users/zarnovican/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zarnovican/subscriptions", "organizations_url": "https://api.github.com/users/zarnovican/orgs", "repos_url": "https://api.github.com/users/zarnovican/repos", "events_url": "https://api.github.com/users/zarnovican/events{/privacy}", "received_events_url": "https://api.github.com/users/zarnovican/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-20T14:51:01Z", "updated_at": "2019-10-22T17:00:18Z", "closed_at": "2019-10-21T16:23:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Problem\r\n\r\nWhen instantiated, `KafkaClient` does not have any meta-data about the cluster. This used to work prior to 1.4.5 version.\r\n\r\n```\r\nfrom kafka.client_async import KafkaClient\r\n\r\nc = KafkaClient(bootstrap_servers='localhost:9092')\r\nprint(c.cluster.partitions_for_topic('data.messages'))\r\n```\r\n\r\n## Expected Result\r\n\r\n```\r\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\r\n```\r\n\r\n## Actual Result\r\n\r\n```\r\nNone\r\n```\r\n\r\n## More Info\r\n\r\nI have a Python script that is checking the health of Kafka cluster. It is neither Kafka Producer, nor Consumer. It just connects to broker node via localhost, reads the metadata and produce alerts/metrics etc. Class `KafkaClient` exactly fit the bill..\r\n\r\nBetween 1.4.4, 1.4.5 version (commit 812de351f75beefe73bd9bef55847ab61ccc951d) it stopped working. Reading the commit, it looks like metadata are not fetched at startup anymore. I tried calling various methods of KafkaClient to trigger metadata load, with little success.\r\n\r\nI admit, that `KafkaClient` is a bit low-level and poorly documented, but it seems \"public\", so I used it. Feel free to close this issue, if you think that this is not a bug.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1931", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1931/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1931/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1931/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1931", "id": 506877480, "node_id": "MDU6SXNzdWU1MDY4Nzc0ODA=", "number": 1931, "title": "Deadlock in Python 3.7 when KafkaProducer closed as part of logging reconfiguration", "user": {"login": "nickwilliams-eventbrite", "id": 8230828, "node_id": "MDQ6VXNlcjgyMzA4Mjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/8230828?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nickwilliams-eventbrite", "html_url": "https://github.com/nickwilliams-eventbrite", "followers_url": "https://api.github.com/users/nickwilliams-eventbrite/followers", "following_url": "https://api.github.com/users/nickwilliams-eventbrite/following{/other_user}", "gists_url": "https://api.github.com/users/nickwilliams-eventbrite/gists{/gist_id}", "starred_url": "https://api.github.com/users/nickwilliams-eventbrite/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nickwilliams-eventbrite/subscriptions", "organizations_url": "https://api.github.com/users/nickwilliams-eventbrite/orgs", "repos_url": "https://api.github.com/users/nickwilliams-eventbrite/repos", "events_url": "https://api.github.com/users/nickwilliams-eventbrite/events{/privacy}", "received_events_url": "https://api.github.com/users/nickwilliams-eventbrite/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-10-14T21:13:58Z", "updated_at": "2020-02-12T20:19:33Z", "closed_at": "2020-02-12T20:19:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've reduced this to as simple a reproduction as possible, so bear with me a bit, and if anything doesn't make sense, just ask...\r\n\r\nUsing the latest 1.4.7 version of Kafka-Python.\r\n\r\nIn earlier versions of Python, if you call `logging.config.dictConfig` and then later call it again to re-configure logging, the `close` method of the previously-configured logging handlers was not called to clean up the abandoned handlers. This was a bug in Python that was fixed in Python 3.7. Now, when `dictConfig` is called a second time, the previous handlers are all cleaned up by having their `close` method called. If a Kafka producer is used in a logging handler and its `close` method is called within the handler's `close` method, it causes a deadlock that prevents program execution from continuing.\r\n\r\nFirst, here's a simple use-case _that works_ on Python 2.7, 3.5, and 3.7:\r\n\r\n```python\r\ndef replicate():\r\n    sys.stdout.write('Creating producer...\\n')\r\n    sys.stdout.flush()\r\n    producer = KafkaProducer(\r\n        acks=1,\r\n        bootstrap_servers=['kafka:9092'],\r\n        client_id='analytics_producer',\r\n        compression_type='gzip',\r\n        linger_ms=100,\r\n        max_block_ms=1000,\r\n        request_timeout_ms=3000,\r\n    )\r\n    sys.stdout.write('Closing producer...\\n')\r\n    sys.stdout.flush()\r\n    producer.close(timeout=2)\r\n\r\n>>> replicate()\r\nCreating producer...\r\nClosing producer...\r\n>>> replicate()\r\nCreating producer...\r\nClosing producer...\r\n>>> replicate()\r\nCreating producer...\r\nClosing producer...\r\n```\r\n\r\nEach call to `replicate` completes in < 1 second. As you can see, this working use case does not involve logging. But once you involve logging as below, things break:\r\n\r\n```python\r\nimport logging\r\nimport logging.config\r\nimport sys\r\nfrom kafka.producer import KafkaProducer\r\n\r\n\r\nclass KafkaAnalyticsLoggingHandler(logging.Handler):\r\n    def __init__(self):\r\n        super(KafkaAnalyticsLoggingHandler, self).__init__()\r\n        sys.stdout.write('Creating producer...\\n')\r\n        sys.stdout.flush()\r\n        self._producer = KafkaProducer(\r\n            acks=1,\r\n            bootstrap_servers=['kafka:9092'],\r\n            client_id='analytics_producer',\r\n            compression_type='gzip',\r\n            linger_ms=100,\r\n            max_block_ms=1000,\r\n            request_timeout_ms=3000,\r\n        )\r\n\r\n    def emit(self, record):\r\n        return\r\n\r\n    def close(self):\r\n        sys.stdout.write('Closing producer...\\n')\r\n        sys.stdout.flush()\r\n        self._producer.close(timeout=2)\r\n        super(KafkaAnalyticsLoggingHandler, self).close()\r\n\r\n\r\ndef replicate():\r\n    config = {'version': 1, 'handlers': {'analytics': {'level': 'INFO', 'class': '__main__.KafkaAnalyticsLoggingHandler'}}, 'loggers': {'analytics': {'handlers': ['analytics'], 'level': 'INFO'}}, 'disable_existing_loggers': False}\r\n    logging.config.dictConfig(config)\r\n    logging.config.dictConfig(config)\r\n\r\n>>> replicate()\r\nCreating producer...\r\nClosing producer...\r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 4, in replicate\r\n  File \"/usr/local/lib/python3.7/logging/config.py\", line 799, in dictConfig\r\n    dictConfigClass(config).configure()\r\n  File \"/usr/local/lib/python3.7/logging/config.py\", line 535, in configure\r\n    _clearExistingHandlers()\r\n  File \"/usr/local/lib/python3.7/logging/config.py\", line 272, in _clearExistingHandlers\r\n    logging.shutdown(logging._handlerList[:])\r\n  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 2034, in shutdown\r\n    h.close()\r\n  File \"<stdin>\", line 20, in close\r\n  File \"/usr/local/lib/python3.7/site-packages/kafka/producer/kafka.py\", line 493, in close\r\n    self._sender.join()\r\n  File \"/usr/local/lib/python3.7/threading.py\", line 1044, in join\r\n    self._wait_for_tstate_lock()\r\n  File \"/usr/local/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n```\r\n\r\nIn this case, `Closing producer...` appears and then it freezes. No prompt, no CPU usage, just total deadlock. Only Ctrl+C can get you out of it, and then you see the stack trace.\r\n\r\nThis only happens in Python 3.7, because that's when the logging config bug was fixed. In Python 2.7 and 3.5, there are no deadlocks and you see the output below. Note that it never closes the producer, because `close` is never called on the handler:\r\n\r\n```python\r\n>>> replicate()\r\nCreating producer...\r\nCreating producer...\r\n>>> replicate()\r\nCreating producer...\r\nCreating producer...\r\n>>> replicate()\r\nCreating producer...\r\nCreating producer...\r\n```\r\n\r\nI think the first, working use case demonstrates that this isn't a timing issue. It's possible to rapidly close a producer after instantiating it without errors or deadlocks. The problem only occurs when the Producer's `close` method is called from within a logging handler's `close` method _during logging reconfiguration_. Critically, _it does not happen when logging is shut down at the end of the application_! Demonstration (you need a fresh Python shell to attempt this):\r\n\r\n```python\r\n>>> config = {'version': 1, 'handlers': {'analytics': {'level': 'INFO', 'class': '__main__.KafkaAnalyticsLoggingHandler'}}, 'loggers': {'analytics': {'handlers': ['analytics'], 'level': 'INFO'}}, 'disable_existing_loggers': False}\r\n>>> logging.config.dictConfig(config)\r\nCreating producer...\r\n>>> exit()\r\nClosing producer...\r\nnwilliams $\r\n```\r\n\r\nI'm certain this has something to do with Python logging's locks interfering with the thread locks of the Kafka sender somehow, but I don't begin to understand how. \ud83e\udd14", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1930", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1930/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1930/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1930/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1930", "id": 506603707, "node_id": "MDU6SXNzdWU1MDY2MDM3MDc=", "number": 1930, "title": "1.4.7 not backwards compatible: parameter changed", "user": {"login": "DieterDePaepe", "id": 3605663, "node_id": "MDQ6VXNlcjM2MDU2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3605663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DieterDePaepe", "html_url": "https://github.com/DieterDePaepe", "followers_url": "https://api.github.com/users/DieterDePaepe/followers", "following_url": "https://api.github.com/users/DieterDePaepe/following{/other_user}", "gists_url": "https://api.github.com/users/DieterDePaepe/gists{/gist_id}", "starred_url": "https://api.github.com/users/DieterDePaepe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DieterDePaepe/subscriptions", "organizations_url": "https://api.github.com/users/DieterDePaepe/orgs", "repos_url": "https://api.github.com/users/DieterDePaepe/repos", "events_url": "https://api.github.com/users/DieterDePaepe/events{/privacy}", "received_events_url": "https://api.github.com/users/DieterDePaepe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-14T11:49:04Z", "updated_at": "2019-10-18T10:08:25Z", "closed_at": "2019-10-14T11:55:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "In 1.4.6, one of the consumer parameters was `max_poll_record`:\r\n\r\n```python\r\nkafka.KafkaConsumer(max_poll_record=100)\r\n```\r\n\r\nIn 1.4.7, it seems this was changed to `max_poll_records`, thereby breaking code using older versions.\r\n\r\nThis can be easily fixed, but I wanted to bring this to attention.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1926", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1926/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1926/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1926/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1926", "id": 506006590, "node_id": "MDU6SXNzdWU1MDYwMDY1OTA=", "number": 1926, "title": "Document the removal of Simple* and that the KafkaClient is the new one, not the old one", "user": {"login": "jeffwidman", "id": 483314, "node_id": "MDQ6VXNlcjQ4MzMxNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/483314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffwidman", "html_url": "https://github.com/jeffwidman", "followers_url": "https://api.github.com/users/jeffwidman/followers", "following_url": "https://api.github.com/users/jeffwidman/following{/other_user}", "gists_url": "https://api.github.com/users/jeffwidman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffwidman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffwidman/subscriptions", "organizations_url": "https://api.github.com/users/jeffwidman/orgs", "repos_url": "https://api.github.com/users/jeffwidman/repos", "events_url": "https://api.github.com/users/jeffwidman/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffwidman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/dpkp/kafka-python/milestones/8", "html_url": "https://github.com/dpkp/kafka-python/milestone/8", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/milestones/8/labels", "id": 3832643, "node_id": "MDk6TWlsZXN0b25lMzgzMjY0Mw==", "number": 8, "title": "2.0", "description": "Issues that we want to put in a 2.0 release as that's a good opportunity to drop backwards compatibility things...", "creator": {"login": "jeffwidman", "id": 483314, "node_id": "MDQ6VXNlcjQ4MzMxNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/483314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffwidman", "html_url": "https://github.com/jeffwidman", "followers_url": "https://api.github.com/users/jeffwidman/followers", "following_url": "https://api.github.com/users/jeffwidman/following{/other_user}", "gists_url": "https://api.github.com/users/jeffwidman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffwidman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffwidman/subscriptions", "organizations_url": "https://api.github.com/users/jeffwidman/orgs", "repos_url": "https://api.github.com/users/jeffwidman/repos", "events_url": "https://api.github.com/users/jeffwidman/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffwidman/received_events", "type": "User", "site_admin": false}, "open_issues": 5, "closed_issues": 8, "state": "open", "created_at": "2018-11-19T19:12:32Z", "updated_at": "2020-02-12T20:23:16Z", "due_on": null, "closed_at": null}, "comments": 1, "created_at": "2019-10-11T18:53:50Z", "updated_at": "2020-02-12T20:23:16Z", "closed_at": "2020-02-12T20:23:16Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "This is a followup for https://github.com/dpkp/kafka-python/pull/1196.\r\n\r\nNeed to document that the `SimpleClient`/`SimpleConsumer`/`SimpleProducer` are all gone and that if you need them, use an older version.\r\n\r\nAlso need to document that `KafkaClient` now refers to the new async client, not the old deprecated one.\r\n\r\nHonestly, the changelog is probably the only place that really needs it... maybe at the bottom of the readme and the top of the `KafkaClient` docs could mention the change as well for folks confused on why `KafkaClient` behavior suddenly changed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1924", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1924/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1924/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1924/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1924", "id": 505018838, "node_id": "MDU6SXNzdWU1MDUwMTg4Mzg=", "number": 1924, "title": "KafkaAdminClient:  cannot pin version", "user": {"login": "simpleistao", "id": 42440018, "node_id": "MDQ6VXNlcjQyNDQwMDE4", "avatar_url": "https://avatars0.githubusercontent.com/u/42440018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simpleistao", "html_url": "https://github.com/simpleistao", "followers_url": "https://api.github.com/users/simpleistao/followers", "following_url": "https://api.github.com/users/simpleistao/following{/other_user}", "gists_url": "https://api.github.com/users/simpleistao/gists{/gist_id}", "starred_url": "https://api.github.com/users/simpleistao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simpleistao/subscriptions", "organizations_url": "https://api.github.com/users/simpleistao/orgs", "repos_url": "https://api.github.com/users/simpleistao/repos", "events_url": "https://api.github.com/users/simpleistao/events{/privacy}", "received_events_url": "https://api.github.com/users/simpleistao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-10T04:01:11Z", "updated_at": "2020-02-12T20:24:04Z", "closed_at": "2020-02-12T20:24:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I cannot set \"api_version\"  for the KafkaAdminClient.   \r\n\r\nwhen api_version is not set, it would say, \r\n\"Broker version identified as 1.0.0.  Set configuration api_version=(1,0,0) to skip auto check...\"\r\n\r\nSo I tried to set \"api_version: (1,0,0)\" in the config.  It would say, \r\n\r\nError message is:\r\nIncompatibleBrokerVersion: Kafka Broker does  not support the \"MetadataRequest_v0\" Kafka protocol.  (this is from __matching_api_version()  <==   refresh_controller_id()  in admin/client.py)\r\n\r\nI started it pdb, and see that broker_api_versions returned is none (see attached screenshot). \r\n\r\nI tried setting it to many other versions it would not work.  However, I tried the same config for a consumer, and it would connect (but looks like it does not need to call __matching_api_version()).    Could you please help?\r\n\r\nBackground:\r\n1.  The only reason I want to pin the version is because I need to make sure I am parsing DescribeConfigResponse correctly, so I would like to pin to version.   Please let me know if there is alternative.\r\n\r\n2.  Kafka cluster version  2.3.0.  \r\n3.  kafka-python version 1.4.7\r\n\r\n\r\n\r\n\r\n![IMG_20191009_235325](https://user-images.githubusercontent.com/42440018/66537901-2d071400-eaf0-11e9-8c93-5207b653f4bc.jpg)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1922", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1922/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1922/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1922/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1922", "id": 502343256, "node_id": "MDU6SXNzdWU1MDIzNDMyNTY=", "number": 1922, "title": "Is it possible to run interactive queries using kafka-python?", "user": {"login": "elmehalawi", "id": 2058016, "node_id": "MDQ6VXNlcjIwNTgwMTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/2058016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elmehalawi", "html_url": "https://github.com/elmehalawi", "followers_url": "https://api.github.com/users/elmehalawi/followers", "following_url": "https://api.github.com/users/elmehalawi/following{/other_user}", "gists_url": "https://api.github.com/users/elmehalawi/gists{/gist_id}", "starred_url": "https://api.github.com/users/elmehalawi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elmehalawi/subscriptions", "organizations_url": "https://api.github.com/users/elmehalawi/orgs", "repos_url": "https://api.github.com/users/elmehalawi/repos", "events_url": "https://api.github.com/users/elmehalawi/events{/privacy}", "received_events_url": "https://api.github.com/users/elmehalawi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-03T23:12:40Z", "updated_at": "2020-02-12T20:33:14Z", "closed_at": "2020-02-12T20:31:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Confluent has [these 'interactive queries'](https://docs.confluent.io/current/streams/developer-guide/interactive-queries.html) that allow one to query the local state store.\r\n\r\nCan I do this with kafka-python? If not, is there a recommended alternative?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1919", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1919/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1919/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1919/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1919", "id": 501627073, "node_id": "MDU6SXNzdWU1MDE2MjcwNzM=", "number": 1919, "title": "Problem to Publish topic in one machine and subscribe same topic on another Kafka server machine", "user": {"login": "rohitsingh23july1986", "id": 56088093, "node_id": "MDQ6VXNlcjU2MDg4MDkz", "avatar_url": "https://avatars2.githubusercontent.com/u/56088093?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohitsingh23july1986", "html_url": "https://github.com/rohitsingh23july1986", "followers_url": "https://api.github.com/users/rohitsingh23july1986/followers", "following_url": "https://api.github.com/users/rohitsingh23july1986/following{/other_user}", "gists_url": "https://api.github.com/users/rohitsingh23july1986/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohitsingh23july1986/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohitsingh23july1986/subscriptions", "organizations_url": "https://api.github.com/users/rohitsingh23july1986/orgs", "repos_url": "https://api.github.com/users/rohitsingh23july1986/repos", "events_url": "https://api.github.com/users/rohitsingh23july1986/events{/privacy}", "received_events_url": "https://api.github.com/users/rohitsingh23july1986/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-02T17:33:43Z", "updated_at": "2019-10-02T19:23:38Z", "closed_at": "2019-10-02T19:18:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi\r\nSubject : data sending problem from one machine to another machine using Kafka python\r\n\r\nDear ,\r\n\r\nWe are facing problem when we send one topic data from one machine to another (Kafka server) ,\r\nActually my requirement is create topic in one machine send this topic via Kafka producer in another Kafka server\r\n\r\n\r\nWhen we create topic and send this topic on same machine then we are able to subscribe this topic on same machine\r\n\r\n\r\nBut as per my requirement when we send this topic from one machine to another Kafka server then we are not able to subscribe this topic and when we try to send this topic then only topic name is showing in lenses GUI tool,but inside this topic no data is comming\r\n\r\nPlease provide me dummy code. In Kafka python  and other file setting which is required for solving this problem\r\n\r\n\r\nWe are suffering this problem since from last week\r\nWe also take help from Kafka forum\r\nBut are not successful to solve this problem \r\n\r\n\r\n\r\n\r\nPlease help to solve this problem\r\nAnd send one dummy code in python Kafka and other setting which is required to solve this problem\r\n\r\nWe are using CentOS operating system\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1917", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1917/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1917/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1917/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1917", "id": 501587577, "node_id": "MDU6SXNzdWU1MDE1ODc1Nzc=", "number": 1917, "title": "Unable to send messages when consumer and producer are on different servers", "user": {"login": "vatodorov", "id": 4651239, "node_id": "MDQ6VXNlcjQ2NTEyMzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/4651239?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vatodorov", "html_url": "https://github.com/vatodorov", "followers_url": "https://api.github.com/users/vatodorov/followers", "following_url": "https://api.github.com/users/vatodorov/following{/other_user}", "gists_url": "https://api.github.com/users/vatodorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vatodorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vatodorov/subscriptions", "organizations_url": "https://api.github.com/users/vatodorov/orgs", "repos_url": "https://api.github.com/users/vatodorov/repos", "events_url": "https://api.github.com/users/vatodorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vatodorov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-02T16:10:48Z", "updated_at": "2019-12-29T16:34:23Z", "closed_at": "2019-12-29T16:34:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI can't seem to figure out how to send messages when the producer and the consumer are on different servers. The producer uses the Python package, but I start the consumer using Kafka's shell scripts. If the consumer and producer are on the same VM, I can send messages across successfully, but not when they are on different VMs. Both VMs are in the same network and there is a route between them.\r\n\r\nAny help is greatly appreciated. Thanks!\r\n\r\nThe consumer runs on 10.13.0.107. This VM has the base Kafka installation from here the official distro https://kafka.apache.org/quickstart\r\n\r\n    # Start Zookeeper and Kafka and consume messages\r\n    > KAFKA_HOST=10.13.0.107\r\n    > PORT=9092\r\n    > KAFKA_TOPIC=\"testTopic\"\r\n    > cd /opt/kafka_2.12-2.3.0/bin\r\n    > ./zookeeper-server-start.sh ../config/zookeeper.properties\r\n    > ./kafka-server-start.sh ../config/server.properties\r\n    > ./kafka-console-consumer.sh --bootstrap-server $KAFKA_HOST:$PORT --topic $KAFKA_TOPIC --from-beginning\r\n\r\n\r\n\r\nThe producer runs in Python on 10.13.0.109\r\n\r\n    import json\r\n    import random\r\n    from kafka import KafkaProducer\r\n\r\n    kafka_topic = 'testTopic'\r\n    kafka_host = '10.13.0.107'\r\n    kafka_port = '9092'\r\n    kafka_host = '{}:{}'.format(kafka_host, kafka_port)\r\n    \r\n    producer = KafkaProducer(bootstrap_servers=[kafka_host],\r\n                         api_version=(0, 10, 1),\r\n                         ssl_check_hostname=False)\r\n    \r\n    #### Generate a random IP address\r\n    def rand(lb=1, ub=255):\r\n        return random.randrange(lb, ub)\r\n    \r\n    #### Send messages to the consumer\r\n    for i in range(20):\r\n        \r\n        #### Generate sample data\r\n        ip = '{}.{}.{}.{}'.format(rand(), rand(), rand(), rand())\r\n        ioc = {\r\n            'indicator': ip,\r\n            'adversaries': 'BadGuy',\r\n            'attributes': {\r\n                'ThreatLevel': 'High'\r\n            }\r\n        }\r\n    \r\n        print print 'Sending {}'.format(ioc)\r\n    \r\n        #### Send the data to the Kafka consumer\r\n        data = json.dumps(ioc)\r\n        producer.send(kafka_topic, data)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1913", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1913/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1913/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1913/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1913", "id": 500398656, "node_id": "MDU6SXNzdWU1MDAzOTg2NTY=", "number": 1913, "title": "unable to connect to remote kafka server", "user": {"login": "harun86", "id": 12448758, "node_id": "MDQ6VXNlcjEyNDQ4NzU4", "avatar_url": "https://avatars1.githubusercontent.com/u/12448758?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harun86", "html_url": "https://github.com/harun86", "followers_url": "https://api.github.com/users/harun86/followers", "following_url": "https://api.github.com/users/harun86/following{/other_user}", "gists_url": "https://api.github.com/users/harun86/gists{/gist_id}", "starred_url": "https://api.github.com/users/harun86/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harun86/subscriptions", "organizations_url": "https://api.github.com/users/harun86/orgs", "repos_url": "https://api.github.com/users/harun86/repos", "events_url": "https://api.github.com/users/harun86/events{/privacy}", "received_events_url": "https://api.github.com/users/harun86/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-30T16:55:50Z", "updated_at": "2019-10-01T02:57:46Z", "closed_at": "2019-10-01T02:57:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "SERVER:-\r\n=======\r\ni have installed kafka 2.1.1 on centos-7 server IP(192.168.3.120)\r\nconfig/server.properties\r\nlisteners=PLAINTEXT://192.168.3.120:9092\r\nadvertised.listeners=PLAINTEXT://192.168.3.120:9092\r\nfirewall disabled\r\n\r\nCLIENT:- (Centos-7 and IP-192.168.3.121)\r\n=========\r\nfrom kafka import KafkaProducer\r\nproducer = KafkaProducer(bootstrap_servers='192.168.3.120:9092')\r\n\r\nclient not able to send message to kafka broker.\r\nkafka.errors.NoBrokersAvailable: NoBrokersAvailable\r\n\r\nPlease Help Me.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1910", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1910/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1910/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1910/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1910", "id": 499850689, "node_id": "MDU6SXNzdWU0OTk4NTA2ODk=", "number": 1910, "title": "The Source is conflict with doc for KafkaClient and SimpleClient", "user": {"login": "kingname", "id": 5440523, "node_id": "MDQ6VXNlcjU0NDA1MjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5440523?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kingname", "html_url": "https://github.com/kingname", "followers_url": "https://api.github.com/users/kingname/followers", "following_url": "https://api.github.com/users/kingname/following{/other_user}", "gists_url": "https://api.github.com/users/kingname/gists{/gist_id}", "starred_url": "https://api.github.com/users/kingname/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kingname/subscriptions", "organizations_url": "https://api.github.com/users/kingname/orgs", "repos_url": "https://api.github.com/users/kingname/repos", "events_url": "https://api.github.com/users/kingname/events{/privacy}", "received_events_url": "https://api.github.com/users/kingname/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-09-29T03:25:11Z", "updated_at": "2019-09-30T06:06:01Z", "closed_at": "2019-09-30T06:06:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "in Source Code of `kafka/__init__.py`\uff1a\r\n\r\n```python\r\nclass KafkaClient(SimpleClient):\r\n    def __init__(self, *args, **kwargs):\r\n        warnings.warn('The legacy KafkaClient interface has been moved to'\r\n                      ' kafka.SimpleClient - this import will break in a'\r\n                      ' future release', DeprecationWarning)\r\n        super(KafkaClient, self).__init__(*args, **kwargs)\r\n```\r\n\r\nthe code tells me that I should use `kafka.SimpleClient`. \r\n\r\nHowever, in the doc: https://kafka-python.readthedocs.io/en/master/simple.html\r\n\r\nit tells me that the `SimpleClient` is `DEPRECATED`:\r\n\r\n![image](https://user-images.githubusercontent.com/5440523/65825624-c130da00-e2ab-11e9-82a3-abc54a5e320d.png)\r\n\r\nso what should I use??", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1907", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1907/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1907/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1907/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1907", "id": 498242645, "node_id": "MDU6SXNzdWU0OTgyNDI2NDU=", "number": 1907, "title": "Sender may busy loop when connection is in CONNECTING state", "user": {"login": "rikonen", "id": 8136387, "node_id": "MDQ6VXNlcjgxMzYzODc=", "avatar_url": "https://avatars2.githubusercontent.com/u/8136387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rikonen", "html_url": "https://github.com/rikonen", "followers_url": "https://api.github.com/users/rikonen/followers", "following_url": "https://api.github.com/users/rikonen/following{/other_user}", "gists_url": "https://api.github.com/users/rikonen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rikonen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rikonen/subscriptions", "organizations_url": "https://api.github.com/users/rikonen/orgs", "repos_url": "https://api.github.com/users/rikonen/repos", "events_url": "https://api.github.com/users/rikonen/events{/privacy}", "received_events_url": "https://api.github.com/users/rikonen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "dpkp", "id": 843444, "node_id": "MDQ6VXNlcjg0MzQ0NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/843444?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dpkp", "html_url": "https://github.com/dpkp", "followers_url": "https://api.github.com/users/dpkp/followers", "following_url": "https://api.github.com/users/dpkp/following{/other_user}", "gists_url": "https://api.github.com/users/dpkp/gists{/gist_id}", "starred_url": "https://api.github.com/users/dpkp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dpkp/subscriptions", "organizations_url": "https://api.github.com/users/dpkp/orgs", "repos_url": "https://api.github.com/users/dpkp/repos", "events_url": "https://api.github.com/users/dpkp/events{/privacy}", "received_events_url": "https://api.github.com/users/dpkp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "dpkp", "id": 843444, "node_id": "MDQ6VXNlcjg0MzQ0NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/843444?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dpkp", "html_url": "https://github.com/dpkp", "followers_url": "https://api.github.com/users/dpkp/followers", "following_url": "https://api.github.com/users/dpkp/following{/other_user}", "gists_url": "https://api.github.com/users/dpkp/gists{/gist_id}", "starred_url": "https://api.github.com/users/dpkp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dpkp/subscriptions", "organizations_url": "https://api.github.com/users/dpkp/orgs", "repos_url": "https://api.github.com/users/dpkp/repos", "events_url": "https://api.github.com/users/dpkp/events{/privacy}", "received_events_url": "https://api.github.com/users/dpkp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2019-09-25T12:07:01Z", "updated_at": "2019-09-29T02:30:50Z", "closed_at": "2019-09-29T02:30:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following may happen:\r\n\r\n1. kafka-python successfully fetches metadata for cluster indicating that leader for topic partition `X` is node `Y`\r\n2. Client application starts producing data, including some that goes to topic partition `X`\r\n3. kafka-python successfully connects to `Y` and starts sending batches to node `Y` as soon as they're completed\r\n4. kafka-python loses connection to `Y`. Client still continues producing data\r\n5. kafka-python tries establishing connection to `Y` but the connection doesn't proceed beyond `CONNECTING` state in a while\r\n6. New batch going to node `Y` completes.\r\n7. `Sender.run_once()` gets node Y from `RecordAccumulator.ready()` but `self._client.is_ready()` returns `False` because there's no working connection. `run_once` discards node `Y` from the list of nodes to process and calls `self._client.connection_delay()` to determine how long to block in `self._client.poll()`. Because the connection to Y is in state `CONNECTING`, `self._client.connection_delay()` returns 0.\r\n8. `run_once()` calls `self._client.poll()` with timeout zero, causing `poll()` to exit practically immediately.\r\n9. `run()` calls `run_once()` again with no delay and sequence continues from step 7: kafka-python keeps on busy looping until connection to Y is successfully established, which may take very long time if the broker has some problem that prevents new connections from being established.\r\n\r\nLog extract showing this behavior below:\r\n\r\n```\r\n2019-09-23T15:54:50.542310  hostname  kafka.conn            kafka-python-producer-2-network-thread INFO      <BrokerConnection node_id=71 host=10.0.1.100:13737 <connecting> [IPv4 ('10.0.1.100', 13737)]>: connecting to 10.0.1.100:13737 [('10.0.1.100', 13737) IPv4]\r\n2019-09-23T15:54:59.819240  hostname  kafka.producer.sender  kafka-python-producer-2-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:54:59.849336  hostname  kafka.producer.sender  kafka-python-producer-1-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:54:59.865445  hostname  kafka.producer.sender  kafka-python-producer-1-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:54:59.868538  hostname  kafka.producer.sender  kafka-python-producer-2-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:54:59.869473  hostname  kafka.producer.sender  kafka-python-producer-2-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:54:59.869154  hostname  kafka.producer.sender  kafka-python-producer-1-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n... same repeats a lot ...\r\n2019-09-23T15:55:00.964323  hostname  kafka.producer.sender  kafka-python-producer-2-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:55:03.119806  hostname  kafka.client          kafka-python-producer-1-network-thread WARNING   Node 71 connection failed -- refreshing metadata\r\n2019-09-23T15:55:03.119478  hostname  kafka.conn            kafka-python-producer-1-network-thread ERROR     Connection attempt to <BrokerConnection node_id=71 host=10.0.1.100:13737 <handshake> [IPv4 ('10.0.1.100', 13737)]> timed out\r\n...\r\n2019-09-23T15:55:03.136644  hostname  kafka.conn            kafka-python-producer-1-network-thread INFO      <BrokerConnection node_id=71 host=10.0.1.100:13737 <connecting> [IPv4 ('10.0.1.100', 13737)]>: connecting to 10.0.1.100:13737 [('10.0.1.100', 13737) IPv4]\r\n2019-09-23T15:55:03.137528  hostname  kafka.producer.record_accumulator  kafka-python-producer-1-network-thread WARNING   Expired 1 batches in accumulator\r\n2019-09-23T15:55:03.137289  hostname  kafka.producer.record_accumulator  kafka-python-producer-1-network-thread WARNING   Produced messages to topic-partition TopicPartition(topic='topic-name', partition=1) with base offset -1 and error KafkaTimeoutError: Batch for TopicPartition(topic='topic-name', partition=1) containing 40 record(s) expired: 30 seconds have passed since last append.\r\n2019-09-23T15:55:03.174292  hostname  kafka.producer.record_accumulator  kafka-python-producer-1-network-thread WARNING   Expired 1 batches in accumulator\r\n2019-09-23T15:55:03.174028  hostname  kafka.producer.record_accumulator  kafka-python-producer-1-network-thread WARNING   Produced messages to topic-partition TopicPartition(topic='topic-name', partition=1) with base offset -1 and error KafkaTimeoutError: Batch for TopicPartition(topic='topic-name', partition=1) containing 49 record(s) expired: 30 seconds have passed since last append.\r\n2019-09-23T15:55:03.192773  hostname  kafka.producer.record_accumulator  kafka-python-producer-1-network-thread WARNING   Produced messages to topic-partition TopicPartition(topic='topic-name', partition=1) with base offset -1 and error KafkaTimeoutError: Batch for TopicPartition(topic='topic-name', partition=1) containing 42 record(s) expired: 30 seconds have passed since last append.\r\n2019-09-23T15:55:03.193092  hostname  kafka.producer.record_accumulator  kafka-python-producer-1-network-thread WARNING   Expired 1 batches in accumulator\r\n... same repeats but not that much ...\r\n2019-09-23T15:55:20.548887  hostname  kafka.conn            kafka-python-producer-2-network-thread INFO      <BrokerConnection node_id=71 host=10.0.1.100:13737 <handshake> [IPv4 ('10.0.1.100', 13737)]>: Closing connection. KafkaConnectionError: timeout\r\n2019-09-23T15:55:20.548724  hostname  kafka.conn            kafka-python-producer-2-network-thread ERROR     Connection attempt to <BrokerConnection node_id=71 host=10.0.1.100:13737 <handshake> [IPv4 ('10.0.1.100', 13737)]> timed out\r\n2019-09-23T15:55:20.549080  hostname  kafka.client          kafka-python-producer-2-network-thread WARNING   Node 71 connection failed -- refreshing metadata\r\n...\r\n2019-09-23T15:55:20.615466  hostname  kafka.conn            kafka-python-producer-2-network-thread INFO      <BrokerConnection node_id=71 host=10.0.1.100:13737 <connecting> [IPv4 ('10.0.1.100', 13737)]>: connecting to 10.0.1.100:13737 [('10.0.1.100', 13737) IPv4]\r\n...\r\n2019-09-23T15:55:29.819565  hostname  kafka.producer.sender  kafka-python-producer-1-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:55:29.819876  hostname  kafka.producer.sender  kafka-python-producer-1-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:55:29.820507  hostname  kafka.producer.sender  kafka-python-producer-1-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:55:29.820832  hostname  kafka.producer.sender  kafka-python-producer-2-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n... same repeats a lot ...\r\n2019-09-23T15:55:30.405083  hostname  kafka.producer.sender  kafka-python-producer-2-network-thread DEBUG     Node 71 not ready; delaying produce of accumulated batch\r\n2019-09-23T15:55:33.136181  hostname  kafka.conn            kafka-python-producer-1-network-thread ERROR     Connection attempt to <BrokerConnection node_id=71 host=10.0.1.100:13737 <handshake> [IPv4 ('10.0.1.100', 13737)]> timed out\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1906", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1906/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1906/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1906/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1906", "id": 498190274, "node_id": "MDU6SXNzdWU0OTgxOTAyNzQ=", "number": 1906, "title": "Got SyntaxError around self.async while import KafkaProducer on python 3.7.4", "user": {"login": "dynax60", "id": 155819, "node_id": "MDQ6VXNlcjE1NTgxOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/155819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dynax60", "html_url": "https://github.com/dynax60", "followers_url": "https://api.github.com/users/dynax60/followers", "following_url": "https://api.github.com/users/dynax60/following{/other_user}", "gists_url": "https://api.github.com/users/dynax60/gists{/gist_id}", "starred_url": "https://api.github.com/users/dynax60/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dynax60/subscriptions", "organizations_url": "https://api.github.com/users/dynax60/orgs", "repos_url": "https://api.github.com/users/dynax60/repos", "events_url": "https://api.github.com/users/dynax60/events{/privacy}", "received_events_url": "https://api.github.com/users/dynax60/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-09-25T10:18:32Z", "updated_at": "2020-06-07T15:37:23Z", "closed_at": "2019-09-25T10:52:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using Python 3.7.4 in PyCharm 2019.1.4 (PY-191.8026.44). I have installed kafka-1.3.5. When I import KafkaProducer, I get an error:\r\n\r\n```\r\n(venv-py) D:\\PycharmProjects\\FCOP>python\r\nPython 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 19:29:22) [MSC v.1916 32 bit (Intel)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from kafka import KafkaProducer\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\PycharmProjects\\FCOP\\venv-py\\lib\\site-packages\\kafka\\__init__.py\", line 23, in <module>\r\n    from kafka.producer import KafkaProducer\r\n  File \"D:\\PycharmProjects\\FCOP\\venv-py\\lib\\site-packages\\kafka\\producer\\__init__.py\", line 4, in <module>\r\n    from .simple import SimpleProducer\r\n  File \"D:\\PycharmProjects\\FCOP\\venv-py\\lib\\site-packages\\kafka\\producer\\simple.py\", line 54\r\n    return '<SimpleProducer batch=%s>' % self.async\r\n                                                  ^\r\nSyntaxError: invalid syntax\r\n>>>\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1905", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1905/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1905/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1905/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1905", "id": 495856746, "node_id": "MDU6SXNzdWU0OTU4NTY3NDY=", "number": 1905, "title": "polling time wile using consumer iterator interface", "user": {"login": "mirko0x5f", "id": 22907028, "node_id": "MDQ6VXNlcjIyOTA3MDI4", "avatar_url": "https://avatars3.githubusercontent.com/u/22907028?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mirko0x5f", "html_url": "https://github.com/mirko0x5f", "followers_url": "https://api.github.com/users/mirko0x5f/followers", "following_url": "https://api.github.com/users/mirko0x5f/following{/other_user}", "gists_url": "https://api.github.com/users/mirko0x5f/gists{/gist_id}", "starred_url": "https://api.github.com/users/mirko0x5f/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mirko0x5f/subscriptions", "organizations_url": "https://api.github.com/users/mirko0x5f/orgs", "repos_url": "https://api.github.com/users/mirko0x5f/repos", "events_url": "https://api.github.com/users/mirko0x5f/events{/privacy}", "received_events_url": "https://api.github.com/users/mirko0x5f/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-19T15:00:13Z", "updated_at": "2019-09-30T10:47:23Z", "closed_at": "2019-09-28T23:46:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,  \r\nI'm pretty new to kafka-python, I'm currently using the kafka-python consumer with the iterator interface.  \r\nI'm trying to understand if there is a way to change the time between poll() calls while using the iterator interface, or if this time is influenced by any setting in the consumer initialization. Can someone help?  \r\nThank you in advance!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1903", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1903/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1903/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1903/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1903", "id": 495360673, "node_id": "MDU6SXNzdWU0OTUzNjA2NzM=", "number": 1903, "title": "1.4.7", "user": {"login": "ofek", "id": 9677399, "node_id": "MDQ6VXNlcjk2NzczOTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/9677399?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ofek", "html_url": "https://github.com/ofek", "followers_url": "https://api.github.com/users/ofek/followers", "following_url": "https://api.github.com/users/ofek/following{/other_user}", "gists_url": "https://api.github.com/users/ofek/gists{/gist_id}", "starred_url": "https://api.github.com/users/ofek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ofek/subscriptions", "organizations_url": "https://api.github.com/users/ofek/orgs", "repos_url": "https://api.github.com/users/ofek/repos", "events_url": "https://api.github.com/users/ofek/events{/privacy}", "received_events_url": "https://api.github.com/users/ofek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2019-09-18T17:32:37Z", "updated_at": "2019-10-03T20:11:25Z", "closed_at": "2019-09-30T21:15:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "@dpkp Hello there! Are there any blockers for releasing `1.4.7`? If so, can I help?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1901", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1901/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1901/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1901/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1901", "id": 492620809, "node_id": "MDU6SXNzdWU0OTI2MjA4MDk=", "number": 1901, "title": "Why can't I get the correct function return?", "user": {"login": "just-OneKey", "id": 42026869, "node_id": "MDQ6VXNlcjQyMDI2ODY5", "avatar_url": "https://avatars0.githubusercontent.com/u/42026869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/just-OneKey", "html_url": "https://github.com/just-OneKey", "followers_url": "https://api.github.com/users/just-OneKey/followers", "following_url": "https://api.github.com/users/just-OneKey/following{/other_user}", "gists_url": "https://api.github.com/users/just-OneKey/gists{/gist_id}", "starred_url": "https://api.github.com/users/just-OneKey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/just-OneKey/subscriptions", "organizations_url": "https://api.github.com/users/just-OneKey/orgs", "repos_url": "https://api.github.com/users/just-OneKey/repos", "events_url": "https://api.github.com/users/just-OneKey/events{/privacy}", "received_events_url": "https://api.github.com/users/just-OneKey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-12T06:55:15Z", "updated_at": "2020-02-12T20:26:26Z", "closed_at": "2020-02-12T20:26:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, my kafka consumer does not seem to get any message from kafka broker.\r\nThen, I got beginning, current and end offset of the queue. But I find that those offsets which return from funstions(beginning_offsets(), position(),end_offsets()) did not match the actual situation.\r\n\r\nHere is the status of my queue:\r\n`TOPIC       PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG         CONSUMER-ID HOST        CLIENT-ID`\r\n`taskQueue       0          62211           62276           65              -               -               -`\r\nBut at the same time, offset I got is:\r\n`begin offset:{TopicPartition(topic='taskQueue', partition=0): 62276}`\r\n`current offset:62277`\r\n`end offset:{TopicPartition(topic='taskQueue', partition=0): 62276}`\r\n\r\nWhat's the problem?\r\nHow can I get the correct return and poll the true message?\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1900", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1900/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1900/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1900/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1900", "id": 492091823, "node_id": "MDU6SXNzdWU0OTIwOTE4MjM=", "number": 1900, "title": "How can I continue to get message from kafka after session timeout?", "user": {"login": "just-OneKey", "id": 42026869, "node_id": "MDQ6VXNlcjQyMDI2ODY5", "avatar_url": "https://avatars0.githubusercontent.com/u/42026869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/just-OneKey", "html_url": "https://github.com/just-OneKey", "followers_url": "https://api.github.com/users/just-OneKey/followers", "following_url": "https://api.github.com/users/just-OneKey/following{/other_user}", "gists_url": "https://api.github.com/users/just-OneKey/gists{/gist_id}", "starred_url": "https://api.github.com/users/just-OneKey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/just-OneKey/subscriptions", "organizations_url": "https://api.github.com/users/just-OneKey/orgs", "repos_url": "https://api.github.com/users/just-OneKey/repos", "events_url": "https://api.github.com/users/just-OneKey/events{/privacy}", "received_events_url": "https://api.github.com/users/just-OneKey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-11T08:30:07Z", "updated_at": "2019-09-26T17:04:20Z", "closed_at": "2019-09-26T17:04:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I built one kafka queue, which has only one topic and one partition.\r\nMy producers will push message in it irregularly. And usually it pushed lots of messages at one time.\r\nThen, consumers start to consume those messages. But the speed of consuming is not certain. In some case, therefore, time of consuming message will be larger than 'consumer.session_timeout_ms'.\r\nWhen this happens, consumer cannot poll any message from broker. How can I get original offset to continue to consume the rest messages?\r\n\r\nHere is some code\r\nproducer:\r\n![image](https://user-images.githubusercontent.com/42026869/64681307-0c787980-d4b2-11e9-801e-bb580eede70a.png)\r\nconsumer:\r\nmanually assign:\r\n![image](https://user-images.githubusercontent.com/42026869/64681575-8b6db200-d4b2-11e9-951d-bbca0c172e08.png)\r\n\r\nmodified process\r\n![image](https://user-images.githubusercontent.com/42026869/64681602-9aecfb00-d4b2-11e9-8098-534627b406d8.png)\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1898", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1898/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1898/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1898/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1898", "id": 491422135, "node_id": "MDU6SXNzdWU0OTE0MjIxMzU=", "number": 1898, "title": "Export errors outside the package", "user": {"login": "jacky15", "id": 8501367, "node_id": "MDQ6VXNlcjg1MDEzNjc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8501367?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacky15", "html_url": "https://github.com/jacky15", "followers_url": "https://api.github.com/users/jacky15/followers", "following_url": "https://api.github.com/users/jacky15/following{/other_user}", "gists_url": "https://api.github.com/users/jacky15/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacky15/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacky15/subscriptions", "organizations_url": "https://api.github.com/users/jacky15/orgs", "repos_url": "https://api.github.com/users/jacky15/repos", "events_url": "https://api.github.com/users/jacky15/events{/privacy}", "received_events_url": "https://api.github.com/users/jacky15/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-10T02:54:25Z", "updated_at": "2019-09-15T09:25:44Z", "closed_at": "2019-09-15T09:25:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "We have  a well defined errors, but limited to the settings inside the file` __init__.py`\uff0cthese erros are not export outside the package.  \r\nDeveloper cannot use any errors in the package, the only thing we can do is except Exception, which is not Pythonic.    \r\nSo , can we  export the errors outside the package?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1897", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1897/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1897/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1897/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1897", "id": 489491798, "node_id": "MDU6SXNzdWU0ODk0OTE3OTg=", "number": 1897, "title": "how to verify compress type of topic message?", "user": {"login": "myswhozxj", "id": 22443565, "node_id": "MDQ6VXNlcjIyNDQzNTY1", "avatar_url": "https://avatars1.githubusercontent.com/u/22443565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myswhozxj", "html_url": "https://github.com/myswhozxj", "followers_url": "https://api.github.com/users/myswhozxj/followers", "following_url": "https://api.github.com/users/myswhozxj/following{/other_user}", "gists_url": "https://api.github.com/users/myswhozxj/gists{/gist_id}", "starred_url": "https://api.github.com/users/myswhozxj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myswhozxj/subscriptions", "organizations_url": "https://api.github.com/users/myswhozxj/orgs", "repos_url": "https://api.github.com/users/myswhozxj/repos", "events_url": "https://api.github.com/users/myswhozxj/events{/privacy}", "received_events_url": "https://api.github.com/users/myswhozxj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-05T02:20:59Z", "updated_at": "2020-02-12T20:26:35Z", "closed_at": "2020-02-12T20:26:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "i want to check the compress type of topic message, it support?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1896", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1896/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1896/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1896/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1896", "id": 488483703, "node_id": "MDU6SXNzdWU0ODg0ODM3MDM=", "number": 1896, "title": "Iterator results in heartbeat failure if topic does not grow", "user": {"login": "DieterDePaepe", "id": 3605663, "node_id": "MDQ6VXNlcjM2MDU2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3605663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DieterDePaepe", "html_url": "https://github.com/DieterDePaepe", "followers_url": "https://api.github.com/users/DieterDePaepe/followers", "following_url": "https://api.github.com/users/DieterDePaepe/following{/other_user}", "gists_url": "https://api.github.com/users/DieterDePaepe/gists{/gist_id}", "starred_url": "https://api.github.com/users/DieterDePaepe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DieterDePaepe/subscriptions", "organizations_url": "https://api.github.com/users/DieterDePaepe/orgs", "repos_url": "https://api.github.com/users/DieterDePaepe/repos", "events_url": "https://api.github.com/users/DieterDePaepe/events{/privacy}", "received_events_url": "https://api.github.com/users/DieterDePaepe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-09-03T09:09:48Z", "updated_at": "2019-10-01T02:50:35Z", "closed_at": "2019-09-29T18:22:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I noticed that when iterating over a topic that is not actively growing, heartbeat failures will occur.\r\nThis implies that the iterator interface fails to properly send heartbeats while waiting.\r\n\r\nMinimal example to reproduce:\r\n- Create a kafka topic `debug` containing 10 messages\r\n- Execute the code below\r\n- Heartbeat failure occurs after 5 minutes.\r\n\r\nUsing version `1.4.6`.\r\n\r\n<details><summary>Code</summary>\r\n<p>\r\n\r\n```\r\nimport logging\r\nimport datetime\r\nimport kafka\r\n\r\nhandler = logging.StreamHandler()\r\nhandler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\r\nlogging.getLogger().addHandler(handler)\r\nlogging.getLogger().setLevel(logging.INFO)\r\n\r\nclass BalanceListener(kafka.ConsumerRebalanceListener):\r\n    def __init__(self) -> None:\r\n        self._logger = logging.getLogger(\"BalanceListener\")\r\n\r\n    def on_partitions_assigned(self, assigned):\r\n        self._logger.info(\"Assigned: %s\", assigned)\r\n\r\n    def on_partitions_revoked(self, revoked):\r\n        self._logger.info(\"Revoked: %s\", revoked)\r\n\r\nkafka_host = \"localhost:9092\"\r\nkafka_topic_events = \"debug\"\r\nkafka_consumergroup = \"group1\"\r\n\r\nkafka_event_consumer = kafka.KafkaConsumer(group_id=kafka_consumergroup,\r\n                                                         bootstrap_servers=[kafka_host],\r\n                                                         enable_auto_commit=False,\r\n                                                         auto_offset_reset=\"earliest\")\r\n\r\nkafka_event_consumer.subscribe([kafka_topic_events], listener=BalanceListener())\r\n\r\nfor msg in kafka_event_consumer:\r\n    print(datetime.datetime.now(), msg.value)\r\n\r\nprint(\"exit\")\r\n```\r\n</p>\r\n</details>\r\n\r\n<details><summary>Output</summary>\r\n<p>\r\n\r\n```\r\n2019-09-03 14:31:27,723 - kafka.conn - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\r\n2019-09-03 14:31:27,724 - kafka.conn - INFO - Probing node bootstrap-0 broker version\r\n2019-09-03 14:31:27,725 - kafka.conn - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\r\n2019-09-03 14:31:27,827 - kafka.conn - INFO - Broker version identifed as 1.0.0\r\n2019-09-03 14:31:27,827 - kafka.conn - INFO - Set configuration api_version=(1, 0, 0) to skip auto check_version requests on startup\r\n2019-09-03 14:31:27,828 - kafka.consumer.subscription_state - INFO - Updating subscribed topics to: ['debug']\r\n2019-09-03 14:31:27,829 - kafka.cluster - INFO - Group coordinator for group1 is BrokerMetadata(nodeId=0, host='tw06c212.UGent.be', port=9092, rack=None)\r\n2019-09-03 14:31:27,829 - kafka.coordinator - INFO - Discovered coordinator 0 for group group1\r\n2019-09-03 14:31:27,829 - kafka.coordinator - INFO - Starting new heartbeat thread\r\n2019-09-03 14:31:27,830 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions set() for group group1\r\n2019-09-03 14:31:27,830 - BalanceListener - INFO - Revoked: set()\r\n2019-09-03 14:31:27,831 - kafka.conn - INFO - <BrokerConnection node_id=0 host=tw06c212.UGent.be:9092 <connecting> [IPv6 ('fe80::351a:ce7:c997:67df%14', 9092, 0, 14)]>: connecting to tw06c212.UGent.be:9092 [('fe80::351a:ce7:c997:67df%14', 9092, 0, 14) IPv6]\r\n2019-09-03 14:31:27,932 - kafka.conn - INFO - <BrokerConnection node_id=0 host=tw06c212.UGent.be:9092 <connecting> [IPv6 ('fe80::351a:ce7:c997:67df%14', 9092, 0, 14)]>: Connection complete.\r\n2019-09-03 14:31:27,933 - kafka.conn - INFO - <BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \r\n2019-09-03 14:31:28,036 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:31:29,757 - kafka.coordinator - INFO - Successfully joined group group1 with generation 37\r\n2019-09-03 14:31:29,757 - kafka.consumer.subscription_state - INFO - Updated partition assignment: []\r\n2019-09-03 14:31:29,757 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions set() for group group1\r\n2019-09-03 14:31:29,757 - BalanceListener - INFO - Assigned: set()\r\n2019-09-03 14:31:45,008 - kafka.coordinator - WARNING - Heartbeat failed for group group1 because it is rebalancing\r\n2019-09-03 14:31:45,009 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions set() for group group1\r\n2019-09-03 14:31:45,009 - BalanceListener - INFO - Revoked: set()\r\n2019-09-03 14:31:45,009 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:31:45,013 - kafka.coordinator - INFO - Elected group leader -- performing partition assignments using range\r\n2019-09-03 14:31:45,017 - kafka.coordinator - INFO - Successfully joined group group1 with generation 38\r\n2019-09-03 14:31:45,017 - kafka.consumer.subscription_state - INFO - Updated partition assignment: [TopicPartition(topic='debug', partition=0)]\r\n2019-09-03 14:31:45,018 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:31:45,018 - BalanceListener - INFO - Assigned: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:31:45.178068 ConsumerRecord(topic='debug', partition=0, offset=0, timestamp=1567500233991, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:31:45.179068 ConsumerRecord(topic='debug', partition=0, offset=1, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:31:45.179068 ConsumerRecord(topic='debug', partition=0, offset=2, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=220, serialized_header_size=-1)\r\n2019-09-03 14:31:45.179068 ConsumerRecord(topic='debug', partition=0, offset=3, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:31:45.180069 ConsumerRecord(topic='debug', partition=0, offset=4, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:31:45.180069 ConsumerRecord(topic='debug', partition=0, offset=5, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:31:45.180069 ConsumerRecord(topic='debug', partition=0, offset=6, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:31:45.180069 ConsumerRecord(topic='debug', partition=0, offset=7, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:31:45.181069 ConsumerRecord(topic='debug', partition=0, offset=8, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:31:45.183070 ConsumerRecord(topic='debug', partition=0, offset=9, timestamp=1567500234010, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:36:45,071 - kafka.coordinator - WARNING - Heartbeat session expired, marking coordinator dead\r\n2019-09-03 14:36:45,072 - kafka.coordinator - WARNING - Marking the coordinator dead (node 0) for group group1: Heartbeat session expired.\r\n2019-09-03 14:36:45,076 - kafka.cluster - INFO - Group coordinator for group1 is BrokerMetadata(nodeId=0, host='tw06c212.UGent.be', port=9092, rack=None)\r\n2019-09-03 14:36:45,077 - kafka.coordinator - INFO - Discovered coordinator 0 for group group1\r\n2019-09-03 14:36:49,688 - kafka.coordinator - WARNING - Heartbeat: local member_id was not recognized; this consumer needs to re-join\r\n2019-09-03 14:36:49,688 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:36:49,688 - BalanceListener - INFO - Revoked: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:36:49,688 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:36:50,367 - kafka.coordinator - INFO - Successfully joined group group1 with generation 41\r\n2019-09-03 14:36:50,367 - kafka.consumer.subscription_state - INFO - Updated partition assignment: [TopicPartition(topic='debug', partition=0)]\r\n2019-09-03 14:36:50,368 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:36:50,368 - BalanceListener - INFO - Assigned: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:36:50.373599 ConsumerRecord(topic='debug', partition=0, offset=0, timestamp=1567500233991, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:36:50.373599 ConsumerRecord(topic='debug', partition=0, offset=1, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:36:50.373599 ConsumerRecord(topic='debug', partition=0, offset=2, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=220, serialized_header_size=-1)\r\n2019-09-03 14:36:50.373599 ConsumerRecord(topic='debug', partition=0, offset=3, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:36:50.373599 ConsumerRecord(topic='debug', partition=0, offset=4, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:36:50.374600 ConsumerRecord(topic='debug', partition=0, offset=5, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:36:50.374600 ConsumerRecord(topic='debug', partition=0, offset=6, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:36:50.374600 ConsumerRecord(topic='debug', partition=0, offset=7, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:36:50.374600 ConsumerRecord(topic='debug', partition=0, offset=8, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:36:50.378601 ConsumerRecord(topic='debug', partition=0, offset=9, timestamp=1567500234010, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:46:45,363 - kafka.coordinator - WARNING - Heartbeat session expired, marking coordinator dead\r\n2019-09-03 14:46:45,365 - kafka.coordinator - WARNING - Marking the coordinator dead (node 0) for group group1: Heartbeat session expired.\r\n2019-09-03 14:46:45,868 - kafka.cluster - INFO - Group coordinator for group1 is BrokerMetadata(nodeId=0, host='tw06c212.UGent.be', port=9092, rack=None)\r\n2019-09-03 14:46:45,869 - kafka.coordinator - INFO - Discovered coordinator 0 for group group1\r\n2019-09-03 14:46:49,495 - kafka.coordinator - WARNING - Heartbeat: local member_id was not recognized; this consumer needs to re-join\r\n2019-09-03 14:46:49,495 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:46:49,495 - BalanceListener - INFO - Revoked: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:46:49,496 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:46:49,501 - kafka.coordinator - INFO - Elected group leader -- performing partition assignments using range\r\n2019-09-03 14:46:49,504 - kafka.coordinator - INFO - Successfully joined group group1 with generation 44\r\n2019-09-03 14:46:49,504 - kafka.consumer.subscription_state - INFO - Updated partition assignment: [TopicPartition(topic='debug', partition=0)]\r\n2019-09-03 14:46:49,504 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:46:49,505 - BalanceListener - INFO - Assigned: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:46:49.665333 ConsumerRecord(topic='debug', partition=0, offset=0, timestamp=1567500233991, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:46:49.665333 ConsumerRecord(topic='debug', partition=0, offset=1, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:46:49.666333 ConsumerRecord(topic='debug', partition=0, offset=2, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=220, serialized_header_size=-1)\r\n2019-09-03 14:46:49.666333 ConsumerRecord(topic='debug', partition=0, offset=3, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:46:49.666333 ConsumerRecord(topic='debug', partition=0, offset=4, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:46:49.666333 ConsumerRecord(topic='debug', partition=0, offset=5, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:46:49.667334 ConsumerRecord(topic='debug', partition=0, offset=6, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:46:49.667334 ConsumerRecord(topic='debug', partition=0, offset=7, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:46:49.668334 ConsumerRecord(topic='debug', partition=0, offset=8, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:46:49.668334 ConsumerRecord(topic='debug', partition=0, offset=9, timestamp=1567500234010, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:47:17,444 - kafka.coordinator - WARNING - Heartbeat failed for group group1 because it is rebalancing\r\n2019-09-03 14:47:17,444 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:47:17,445 - BalanceListener - INFO - Revoked: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:47:17,445 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:47:17,449 - kafka.coordinator - INFO - Elected group leader -- performing partition assignments using range\r\n2019-09-03 14:47:17,454 - kafka.coordinator - INFO - Successfully joined group group1 with generation 45\r\n2019-09-03 14:47:17,454 - kafka.consumer.subscription_state - INFO - Updated partition assignment: []\r\n2019-09-03 14:47:17,454 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions set() for group group1\r\n2019-09-03 14:47:17,454 - BalanceListener - INFO - Assigned: set()\r\n2019-09-03 14:47:38,874 - kafka.coordinator - WARNING - Heartbeat failed for group group1 because it is rebalancing\r\n2019-09-03 14:47:38,874 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions set() for group group1\r\n2019-09-03 14:47:38,874 - BalanceListener - INFO - Revoked: set()\r\n2019-09-03 14:47:38,874 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:47:38,876 - kafka.coordinator - INFO - Elected group leader -- performing partition assignments using range\r\n2019-09-03 14:47:38,877 - kafka.coordinator - INFO - Successfully joined group group1 with generation 46\r\n2019-09-03 14:47:38,878 - kafka.consumer.subscription_state - INFO - Updated partition assignment: [TopicPartition(topic='debug', partition=0)]\r\n2019-09-03 14:47:38,878 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:47:38,878 - BalanceListener - INFO - Assigned: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:47:39.036140 ConsumerRecord(topic='debug', partition=0, offset=0, timestamp=1567500233991, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:47:39.037140 ConsumerRecord(topic='debug', partition=0, offset=1, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:47:39.037140 ConsumerRecord(topic='debug', partition=0, offset=2, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=220, serialized_header_size=-1)\r\n2019-09-03 14:47:39.037140 ConsumerRecord(topic='debug', partition=0, offset=3, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:47:39.037140 ConsumerRecord(topic='debug', partition=0, offset=4, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:47:39.038140 ConsumerRecord(topic='debug', partition=0, offset=5, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:47:39.038140 ConsumerRecord(topic='debug', partition=0, offset=6, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:47:39.038140 ConsumerRecord(topic='debug', partition=0, offset=7, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:47:39.038140 ConsumerRecord(topic='debug', partition=0, offset=8, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:47:39.040141 ConsumerRecord(topic='debug', partition=0, offset=9, timestamp=1567500234010, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:49:27,702 - kafka.coordinator - WARNING - Heartbeat session expired, marking coordinator dead\r\n2019-09-03 14:49:27,703 - kafka.coordinator - WARNING - Marking the coordinator dead (node 0) for group group1: Heartbeat session expired.\r\n2019-09-03 14:49:28,717 - kafka.cluster - INFO - Group coordinator for group1 is BrokerMetadata(nodeId=0, host='tw06c212.UGent.be', port=9092, rack=None)\r\n2019-09-03 14:49:28,717 - kafka.coordinator - INFO - Discovered coordinator 0 for group group1\r\n2019-09-03 14:49:29,273 - kafka.cluster - INFO - Group coordinator for group1 is BrokerMetadata(nodeId=0, host='tw06c212.UGent.be', port=9092, rack=None)\r\n2019-09-03 14:49:29,273 - kafka.coordinator - INFO - Discovered coordinator 0 for group group1\r\n2019-09-03 14:49:32,845 - kafka.coordinator - WARNING - Heartbeat: local member_id was not recognized; this consumer needs to re-join\r\n2019-09-03 14:49:32,845 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:49:32,845 - BalanceListener - INFO - Revoked: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:49:32,846 - kafka.coordinator - WARNING - Heartbeat: local member_id was not recognized; this consumer needs to re-join\r\n2019-09-03 14:49:32,846 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:49:32,848 - kafka.coordinator - INFO - Elected group leader -- performing partition assignments using range\r\n2019-09-03 14:49:32,849 - kafka.coordinator - INFO - Successfully joined group group1 with generation 48\r\n2019-09-03 14:49:32,850 - kafka.consumer.subscription_state - INFO - Updated partition assignment: [TopicPartition(topic='debug', partition=0)]\r\n2019-09-03 14:49:32,850 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:49:32,850 - BalanceListener - INFO - Assigned: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:49:33.007321 ConsumerRecord(topic='debug', partition=0, offset=0, timestamp=1567500233991, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:49:33.008321 ConsumerRecord(topic='debug', partition=0, offset=1, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:49:33.008321 ConsumerRecord(topic='debug', partition=0, offset=2, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=220, serialized_header_size=-1)\r\n2019-09-03 14:49:33.009321 ConsumerRecord(topic='debug', partition=0, offset=3, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:49:33.009321 ConsumerRecord(topic='debug', partition=0, offset=4, timestamp=1567500234008, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:49:33.009321 ConsumerRecord(topic='debug', partition=0, offset=5, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:49:33.010322 ConsumerRecord(topic='debug', partition=0, offset=6, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:49:33.010322 ConsumerRecord(topic='debug', partition=0, offset=7, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:49:33.010322 ConsumerRecord(topic='debug', partition=0, offset=8, timestamp=1567500234009, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=221, serialized_header_size=-1)\r\n2019-09-03 14:49:33.011322 ConsumerRecord(topic='debug', partition=0, offset=9, timestamp=1567500234010, timestamp_type=0, key=None, value=b'...', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=222, serialized_header_size=-1)\r\n2019-09-03 14:49:46,264 - kafka.coordinator - WARNING - Heartbeat failed for group group1 because it is rebalancing\r\n2019-09-03 14:49:46,264 - kafka.coordinator.consumer - INFO - Revoking previously assigned partitions {TopicPartition(topic='debug', partition=0)} for group group1\r\n2019-09-03 14:49:46,264 - BalanceListener - INFO - Revoked: {TopicPartition(topic='debug', partition=0)}\r\n2019-09-03 14:49:46,265 - kafka.coordinator - INFO - (Re-)joining group group1\r\n2019-09-03 14:49:46,268 - kafka.coordinator - INFO - Elected group leader -- performing partition assignments using range\r\n2019-09-03 14:49:46,272 - kafka.coordinator - INFO - Successfully joined group group1 with generation 49\r\n2019-09-03 14:49:46,272 - kafka.consumer.subscription_state - INFO - Updated partition assignment: []\r\n2019-09-03 14:49:46,273 - kafka.coordinator.consumer - INFO - Setting newly assigned partitions set() for group group1\r\n2019-09-03 14:49:46,273 - BalanceListener - INFO - Assigned: set()\r\n\r\n```\r\n</p>\r\n</details>", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1894", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1894/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1894/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1894/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1894", "id": 488194323, "node_id": "MDU6SXNzdWU0ODgxOTQzMjM=", "number": 1894, "title": "Kafka Consumer Doesn't reconnect when broker gets killed in the middle", "user": {"login": "progovoy", "id": 5350434, "node_id": "MDQ6VXNlcjUzNTA0MzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5350434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/progovoy", "html_url": "https://github.com/progovoy", "followers_url": "https://api.github.com/users/progovoy/followers", "following_url": "https://api.github.com/users/progovoy/following{/other_user}", "gists_url": "https://api.github.com/users/progovoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/progovoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/progovoy/subscriptions", "organizations_url": "https://api.github.com/users/progovoy/orgs", "repos_url": "https://api.github.com/users/progovoy/repos", "events_url": "https://api.github.com/users/progovoy/events{/privacy}", "received_events_url": "https://api.github.com/users/progovoy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-02T13:36:16Z", "updated_at": "2019-09-03T07:55:32Z", "closed_at": "2019-09-03T07:55:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Kafka Consumer Doesn't reconnect when broker gets killed in the middle. This happens in 1.4.6 version.\r\n\r\nSimilar issues:\r\nhttps://github.com/dpkp/kafka-python/issues/1306\r\nhttps://github.com/dpkp/kafka-python/issues/1354\r\n\r\nWhat are your plans on that one?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1893", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1893/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1893/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1893/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1893", "id": 487734367, "node_id": "MDU6SXNzdWU0ODc3MzQzNjc=", "number": 1893, "title": "KafkaConsumer dropping messages at random", "user": {"login": "Matshakan", "id": 43255036, "node_id": "MDQ6VXNlcjQzMjU1MDM2", "avatar_url": "https://avatars2.githubusercontent.com/u/43255036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Matshakan", "html_url": "https://github.com/Matshakan", "followers_url": "https://api.github.com/users/Matshakan/followers", "following_url": "https://api.github.com/users/Matshakan/following{/other_user}", "gists_url": "https://api.github.com/users/Matshakan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Matshakan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Matshakan/subscriptions", "organizations_url": "https://api.github.com/users/Matshakan/orgs", "repos_url": "https://api.github.com/users/Matshakan/repos", "events_url": "https://api.github.com/users/Matshakan/events{/privacy}", "received_events_url": "https://api.github.com/users/Matshakan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-31T05:52:42Z", "updated_at": "2019-09-02T23:20:17Z", "closed_at": "2019-09-02T23:20:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "This code is more or less taken from the README and has issues:\r\n```\r\nconsumer = KafkaConsumer('testx',bootstrap_servers='192.168.99.100:9092',group_id='my-group',auto_offset_reset = 'latest')\r\nfor message in consumer:\r\n    print (message)\r\n\r\n```\r\nI tested it with a **KafkaProducer** generating one message at a time concurrently. Works fine for a random number of messages and then freezes. When i restart the **KafkaConsume**r program with the above code the previously sent messages from the producer shows up. Then the **KafkaProducer** works for a while and gets all grumpy again. I found this post describing somewhat the same problem I have. [A tale of two Kafka Clients](https://blog.datasyndrome.com/a-tale-of-two-kafka-clients-c613efab49df)\r\n\r\nHopefully it\u00b4s me being stupid. \r\n\r\n\r\n\r\n ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1892", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1892/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1892/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1892/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1892", "id": 486870814, "node_id": "MDU6SXNzdWU0ODY4NzA4MTQ=", "number": 1892, "title": "kafka connection storm", "user": {"login": "Pushking4real", "id": 38830637, "node_id": "MDQ6VXNlcjM4ODMwNjM3", "avatar_url": "https://avatars0.githubusercontent.com/u/38830637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pushking4real", "html_url": "https://github.com/Pushking4real", "followers_url": "https://api.github.com/users/Pushking4real/followers", "following_url": "https://api.github.com/users/Pushking4real/following{/other_user}", "gists_url": "https://api.github.com/users/Pushking4real/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pushking4real/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pushking4real/subscriptions", "organizations_url": "https://api.github.com/users/Pushking4real/orgs", "repos_url": "https://api.github.com/users/Pushking4real/repos", "events_url": "https://api.github.com/users/Pushking4real/events{/privacy}", "received_events_url": "https://api.github.com/users/Pushking4real/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-29T10:19:53Z", "updated_at": "2019-09-02T23:22:55Z", "closed_at": "2019-09-02T23:22:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "when our kafka cluster disk is full,it rejected the request from kafka-python client.And later it create an connection storm which break down our firewall  with version1.3.4.  Why the client create so many syn connection?Can i upgrade it to version1.4.0 to solve the problem?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1889", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1889/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1889/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1889/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1889", "id": 484162367, "node_id": "MDU6SXNzdWU0ODQxNjIzNjc=", "number": 1889, "title": "KafkaAdminClient.list_consumer_groups() is slow", "user": {"login": "nbommu1", "id": 7593335, "node_id": "MDQ6VXNlcjc1OTMzMzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/7593335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nbommu1", "html_url": "https://github.com/nbommu1", "followers_url": "https://api.github.com/users/nbommu1/followers", "following_url": "https://api.github.com/users/nbommu1/following{/other_user}", "gists_url": "https://api.github.com/users/nbommu1/gists{/gist_id}", "starred_url": "https://api.github.com/users/nbommu1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nbommu1/subscriptions", "organizations_url": "https://api.github.com/users/nbommu1/orgs", "repos_url": "https://api.github.com/users/nbommu1/repos", "events_url": "https://api.github.com/users/nbommu1/events{/privacy}", "received_events_url": "https://api.github.com/users/nbommu1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-08-22T19:08:03Z", "updated_at": "2020-02-12T20:27:43Z", "closed_at": "2020-02-12T20:27:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "similar issue has been fixed in single node broker #1798, issue still exist in cluster.\r\n\r\nSingle node broker:\r\n\r\n```\r\nfrom kafka.admin.client import KafkaAdminClient\r\nimport time\r\nadmin_client = KafkaAdminClient(bootstrap_servers='127.0.0.1')\r\nstart = time.time()\r\nprint(len(admin_client.list_consumer_groups()))\r\nend = time.time()\r\nprint(end-start)\r\n```\r\nSingle node \r\n\r\n12\r\n0.000765085220337\r\n\r\nCluster\r\n12\r\n60.0666730404\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1888", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1888/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1888/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1888/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1888", "id": 484118291, "node_id": "MDU6SXNzdWU0ODQxMTgyOTE=", "number": 1888, "title": "KafkaConsumer runs 8x slower since v1.4.5", "user": {"login": "jutley", "id": 5660346, "node_id": "MDQ6VXNlcjU2NjAzNDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/5660346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jutley", "html_url": "https://github.com/jutley", "followers_url": "https://api.github.com/users/jutley/followers", "following_url": "https://api.github.com/users/jutley/following{/other_user}", "gists_url": "https://api.github.com/users/jutley/gists{/gist_id}", "starred_url": "https://api.github.com/users/jutley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jutley/subscriptions", "organizations_url": "https://api.github.com/users/jutley/orgs", "repos_url": "https://api.github.com/users/jutley/repos", "events_url": "https://api.github.com/users/jutley/events{/privacy}", "received_events_url": "https://api.github.com/users/jutley/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-08-22T17:22:31Z", "updated_at": "2019-10-02T18:59:55Z", "closed_at": "2019-09-29T02:51:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "My org uses the [prometheus-kafka-consumer-group-exporter](https://github.com/braedon/prometheus-kafka-consumer-group-exporter) project, which depends on this library. All this project does is read through the `__consumer_offsets` topic and generate metrics about the consumer groups. The latest version fails to read through the topic quickly enough to keep up. \r\n\r\nWith some digging, I found that this performance change started at this commit: 8c0792581d8a38822c01b40f5d3926c659b0c439. I verified this with an experiment where I ran the following script against this commit, the commit preceding it (7a99013668b798aaa0acffcf382a7e48e7bd41c1), and master:\r\n```\r\nfrom kafka import KafkaConsumer\r\nimport schedule\r\nimport datetime\r\n\r\nconsumer_config = {\r\n    'bootstrap_servers': <redacted>,\r\n    'auto_offset_reset': 'earliest',\r\n    'group_id': None\r\n}\r\n\r\nconsumer = KafkaConsumer(\r\n    '__consumer_offsets',\r\n    **consumer_config\r\n)\r\n\r\niterations=0\r\n\r\ndef print_status():\r\n    print(datetime.datetime.now(), iterations)\r\n\r\nschedule.every(5).seconds.do(print_status)\r\n\r\nwhile True:\r\n    for message in consumer:\r\n        iterations = iterations + 1\r\n        schedule.run_pending()\r\n```\r\n\r\nI ran three 2 minute trials against each commit to test the throughput of the consumer. Here are the results:\r\n![image](https://user-images.githubusercontent.com/5660346/63535281-f33d6800-c4c5-11e9-8237-e70f954f69b1.png)\r\n\r\nAs you can see, this commit cause the consumer to run 8 times slower (2,426,802 messages vs. 300,187 messages). This has not improved since then.\r\n\r\nI do not understand the details around this commit, but it has rendered this project unusuable on the latest version of kafka-python.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1884", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1884/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1884/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1884/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1884", "id": 481506278, "node_id": "MDU6SXNzdWU0ODE1MDYyNzg=", "number": 1884, "title": "Help: can\u2018t get data from kafka 2.0.0", "user": {"login": "qishengle", "id": 20926239, "node_id": "MDQ6VXNlcjIwOTI2MjM5", "avatar_url": "https://avatars2.githubusercontent.com/u/20926239?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qishengle", "html_url": "https://github.com/qishengle", "followers_url": "https://api.github.com/users/qishengle/followers", "following_url": "https://api.github.com/users/qishengle/following{/other_user}", "gists_url": "https://api.github.com/users/qishengle/gists{/gist_id}", "starred_url": "https://api.github.com/users/qishengle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qishengle/subscriptions", "organizations_url": "https://api.github.com/users/qishengle/orgs", "repos_url": "https://api.github.com/users/qishengle/repos", "events_url": "https://api.github.com/users/qishengle/events{/privacy}", "received_events_url": "https://api.github.com/users/qishengle/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-08-16T08:41:01Z", "updated_at": "2019-10-10T07:27:12Z", "closed_at": "2019-10-10T07:24:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "the consumer can connet with kafka 2.0\uff0c but cannot get any data from it", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1882", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1882/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1882/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1882/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1882", "id": 480121524, "node_id": "MDU6SXNzdWU0ODAxMjE1MjQ=", "number": 1882, "title": "Support for sasl mechanism: SCRAM-SHA-256", "user": {"login": "mlepicka", "id": 6850881, "node_id": "MDQ6VXNlcjY4NTA4ODE=", "avatar_url": "https://avatars1.githubusercontent.com/u/6850881?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mlepicka", "html_url": "https://github.com/mlepicka", "followers_url": "https://api.github.com/users/mlepicka/followers", "following_url": "https://api.github.com/users/mlepicka/following{/other_user}", "gists_url": "https://api.github.com/users/mlepicka/gists{/gist_id}", "starred_url": "https://api.github.com/users/mlepicka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mlepicka/subscriptions", "organizations_url": "https://api.github.com/users/mlepicka/orgs", "repos_url": "https://api.github.com/users/mlepicka/repos", "events_url": "https://api.github.com/users/mlepicka/events{/privacy}", "received_events_url": "https://api.github.com/users/mlepicka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-13T12:11:15Z", "updated_at": "2019-12-29T23:12:31Z", "closed_at": "2019-12-29T23:12:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there any chance for having SCRAM-SHA-256 mechanism supported in kafka-python?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1881", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1881/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1881/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1881/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1881", "id": 479932999, "node_id": "MDU6SXNzdWU0Nzk5MzI5OTk=", "number": 1881, "title": "Is it possible to merge two topics? ", "user": {"login": "sungreong", "id": 22166391, "node_id": "MDQ6VXNlcjIyMTY2Mzkx", "avatar_url": "https://avatars0.githubusercontent.com/u/22166391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sungreong", "html_url": "https://github.com/sungreong", "followers_url": "https://api.github.com/users/sungreong/followers", "following_url": "https://api.github.com/users/sungreong/following{/other_user}", "gists_url": "https://api.github.com/users/sungreong/gists{/gist_id}", "starred_url": "https://api.github.com/users/sungreong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sungreong/subscriptions", "organizations_url": "https://api.github.com/users/sungreong/orgs", "repos_url": "https://api.github.com/users/sungreong/repos", "events_url": "https://api.github.com/users/sungreong/events{/privacy}", "received_events_url": "https://api.github.com/users/sungreong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-13T02:44:25Z", "updated_at": "2019-10-07T18:27:48Z", "closed_at": "2019-10-07T18:27:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I have a question. \r\nFor example, i have three topics. One topic role is consumer.\r\nAnd the others role is producer.\r\nConsumer subscribe a message and the others are going to process it individually. \r\nAnd send to producer.\r\nSo i want to merge the others topic.\r\nHow to merge it?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1880", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1880/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1880/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1880/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1880", "id": 479582188, "node_id": "MDU6SXNzdWU0Nzk1ODIxODg=", "number": 1880, "title": "Help: Consumer has not stopped after rebalance", "user": {"login": "Lywane", "id": 31400021, "node_id": "MDQ6VXNlcjMxNDAwMDIx", "avatar_url": "https://avatars1.githubusercontent.com/u/31400021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lywane", "html_url": "https://github.com/Lywane", "followers_url": "https://api.github.com/users/Lywane/followers", "following_url": "https://api.github.com/users/Lywane/following{/other_user}", "gists_url": "https://api.github.com/users/Lywane/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lywane/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lywane/subscriptions", "organizations_url": "https://api.github.com/users/Lywane/orgs", "repos_url": "https://api.github.com/users/Lywane/repos", "events_url": "https://api.github.com/users/Lywane/events{/privacy}", "received_events_url": "https://api.github.com/users/Lywane/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-12T10:53:37Z", "updated_at": "2019-10-11T18:36:59Z", "closed_at": "2019-10-11T18:36:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "My topic owns 1 partition.I have one group which owns two consumers. When I start the first consumer, it can work normally, then I start the second consumer. I want watching rebalance happen and one of two consumers block and another consume. But now both them consume.\r\n\r\ngroup: \"my-group\"\r\ntopicName: \"my-test\"\r\npartitions: 1\r\nkafka: 2.0.1\r\nkafka-python: 1.4.6\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1878", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1878/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1878/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1878/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1878", "id": 478296297, "node_id": "MDU6SXNzdWU0NzgyOTYyOTc=", "number": 1878, "title": "Multiple Brokers, Single Machine", "user": {"login": "kenrubiooo", "id": 42333040, "node_id": "MDQ6VXNlcjQyMzMzMDQw", "avatar_url": "https://avatars1.githubusercontent.com/u/42333040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kenrubiooo", "html_url": "https://github.com/kenrubiooo", "followers_url": "https://api.github.com/users/kenrubiooo/followers", "following_url": "https://api.github.com/users/kenrubiooo/following{/other_user}", "gists_url": "https://api.github.com/users/kenrubiooo/gists{/gist_id}", "starred_url": "https://api.github.com/users/kenrubiooo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kenrubiooo/subscriptions", "organizations_url": "https://api.github.com/users/kenrubiooo/orgs", "repos_url": "https://api.github.com/users/kenrubiooo/repos", "events_url": "https://api.github.com/users/kenrubiooo/events{/privacy}", "received_events_url": "https://api.github.com/users/kenrubiooo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-08-08T07:22:11Z", "updated_at": "2019-09-26T17:10:40Z", "closed_at": "2019-09-26T17:10:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI am trying to understand how multiple brokers work in a single machine. Imagine that I have 2 brokers. I have the first script which produces an image, the second script to process the image, and a third script to show the image. I have an understanding that what broker 1 (B1), broker 2 (B2) will also have. So, if I send the image to B1 and wants to get the image from B2 for processing, then get the processed image for showing through B1 again. Where do I configure the broker connection such that I know that the two brokers are sharing information?\r\n\r\nThank you!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1877", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1877/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1877/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1877/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1877", "id": 477791762, "node_id": "MDU6SXNzdWU0Nzc3OTE3NjI=", "number": 1877, "title": "Writing data using KafkaProducer is too slow? It's only 5000  record/s", "user": {"login": "nohunt", "id": 19381274, "node_id": "MDQ6VXNlcjE5MzgxMjc0", "avatar_url": "https://avatars2.githubusercontent.com/u/19381274?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nohunt", "html_url": "https://github.com/nohunt", "followers_url": "https://api.github.com/users/nohunt/followers", "following_url": "https://api.github.com/users/nohunt/following{/other_user}", "gists_url": "https://api.github.com/users/nohunt/gists{/gist_id}", "starred_url": "https://api.github.com/users/nohunt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nohunt/subscriptions", "organizations_url": "https://api.github.com/users/nohunt/orgs", "repos_url": "https://api.github.com/users/nohunt/repos", "events_url": "https://api.github.com/users/nohunt/events{/privacy}", "received_events_url": "https://api.github.com/users/nohunt/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-08-07T08:48:58Z", "updated_at": "2019-10-11T18:37:39Z", "closed_at": "2019-10-11T18:37:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Environmental configuration\uff1a\r\nkafka-python version:1.4.6\r\nMemory\uff1a256G\r\nkafka version:2.11\r\n\r\n\r\nProblem Description\uff1a\r\nI use kafka's own tools\uff08kafka-console-consumer.sh\uff09to import more than 5 million data from files. It takes less than a minute.\r\n\r\nIt took me a long time to use KafkaProducer.Here are some examples\r\n```\r\nproducer = KafkaProducer(bootstrap_servers = ['%s:%d'%(ip,port)],buffer_memory=67108864,batch_size=2097152,max_request_size=5242880,send_buffer_bytes=131072,compression_type='gzip')\r\nwith open(path) as rf:\r\n    for line in rf.readlines():\r\n        producer.send(\"test\", line.encode('utf-8'))\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1875", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1875/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1875/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1875/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1875", "id": 475378409, "node_id": "MDU6SXNzdWU0NzUzNzg0MDk=", "number": 1875, "title": "Document namedtuples in kafka.struct module", "user": {"login": "ferozed", "id": 3730318, "node_id": "MDQ6VXNlcjM3MzAzMTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/3730318?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ferozed", "html_url": "https://github.com/ferozed", "followers_url": "https://api.github.com/users/ferozed/followers", "following_url": "https://api.github.com/users/ferozed/following{/other_user}", "gists_url": "https://api.github.com/users/ferozed/gists{/gist_id}", "starred_url": "https://api.github.com/users/ferozed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ferozed/subscriptions", "organizations_url": "https://api.github.com/users/ferozed/orgs", "repos_url": "https://api.github.com/users/ferozed/repos", "events_url": "https://api.github.com/users/ferozed/events{/privacy}", "received_events_url": "https://api.github.com/users/ferozed/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-31T22:05:59Z", "updated_at": "2019-07-31T22:06:35Z", "closed_at": "2019-07-31T22:06:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "The namedtuples in kafka.struct package need to be documented, as they are the parameters or return values of some Apis.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1873", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1873/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1873/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1873/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1873", "id": 475293774, "node_id": "MDU6SXNzdWU0NzUyOTM3NzQ=", "number": 1873, "title": "KafkaClient partitions_for_topic returns None even when initialized with topic", "user": {"login": "ferozed", "id": 3730318, "node_id": "MDQ6VXNlcjM3MzAzMTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/3730318?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ferozed", "html_url": "https://github.com/ferozed", "followers_url": "https://api.github.com/users/ferozed/followers", "following_url": "https://api.github.com/users/ferozed/following{/other_user}", "gists_url": "https://api.github.com/users/ferozed/gists{/gist_id}", "starred_url": "https://api.github.com/users/ferozed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ferozed/subscriptions", "organizations_url": "https://api.github.com/users/ferozed/orgs", "repos_url": "https://api.github.com/users/ferozed/repos", "events_url": "https://api.github.com/users/ferozed/events{/privacy}", "received_events_url": "https://api.github.com/users/ferozed/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-07-31T18:29:15Z", "updated_at": "2019-09-26T17:13:04Z", "closed_at": "2019-09-26T17:13:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is the code snippet that I have...\r\n\r\n```\r\n        self.kafka_client = KafkaConsumer(self.topic, **kafka_config)\r\n        stream_shards = self.kafka_client.partitions_for_topic(self.topic)\r\n```\r\n\r\nI am expecting `stream_shards` to contain the partitions for the topic. But it is `None`.\r\n\r\nhowever, if I call `self.kafka_client.topics()` and then call `partitions_for_topic` it returns correct partitions.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1872", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1872/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1872/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1872/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1872", "id": 474999301, "node_id": "MDU6SXNzdWU0NzQ5OTkzMDE=", "number": 1872, "title": "Kafka shutdown __del__ log issue", "user": {"login": "danncoba", "id": 2931774, "node_id": "MDQ6VXNlcjI5MzE3NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/2931774?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danncoba", "html_url": "https://github.com/danncoba", "followers_url": "https://api.github.com/users/danncoba/followers", "following_url": "https://api.github.com/users/danncoba/following{/other_user}", "gists_url": "https://api.github.com/users/danncoba/gists{/gist_id}", "starred_url": "https://api.github.com/users/danncoba/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danncoba/subscriptions", "organizations_url": "https://api.github.com/users/danncoba/orgs", "repos_url": "https://api.github.com/users/danncoba/repos", "events_url": "https://api.github.com/users/danncoba/events{/privacy}", "received_events_url": "https://api.github.com/users/danncoba/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-31T08:13:10Z", "updated_at": "2019-12-29T05:38:34Z", "closed_at": "2019-12-29T05:38:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've read couple of issues regarding this bug in these discussions, and i've seen that there is resolved and closed issue in march/april 2018 regarding this, but it's still happening to me using pip.\r\n\r\nI'm using pip kafka-python package version 1.4.6 and there is still __del__ log issue when shutting down application aldo i've found here that complete __del__ method was erased\r\n\r\nCould you please help me with this", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1868", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1868/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1868/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1868/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1868", "id": 473061108, "node_id": "MDU6SXNzdWU0NzMwNjExMDg=", "number": 1868, "title": "KafkaClient returns no topics on 1.4.5+", "user": {"login": "andystanley", "id": 24689747, "node_id": "MDQ6VXNlcjI0Njg5NzQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/24689747?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andystanley", "html_url": "https://github.com/andystanley", "followers_url": "https://api.github.com/users/andystanley/followers", "following_url": "https://api.github.com/users/andystanley/following{/other_user}", "gists_url": "https://api.github.com/users/andystanley/gists{/gist_id}", "starred_url": "https://api.github.com/users/andystanley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andystanley/subscriptions", "organizations_url": "https://api.github.com/users/andystanley/orgs", "repos_url": "https://api.github.com/users/andystanley/repos", "events_url": "https://api.github.com/users/andystanley/events{/privacy}", "received_events_url": "https://api.github.com/users/andystanley/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-25T20:42:05Z", "updated_at": "2019-09-26T17:13:13Z", "closed_at": "2019-09-26T17:13:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm upgrading from `1.4.4` to `1.4.6` and `KafkaClient` on `1.4.5` and `1.4.6` is returning an empty set of topics.\r\n```\r\nfrom kafka.client_async import KafkaClient\r\n\r\nclient_config = {\r\n    'bootstrap_servers': 'servers',\r\n    'api_version': (2, 0)\r\n}\r\nkafka_client = KafkaClient(**client_config)\r\n\r\nassert len(kafka_client.cluster.topics()) > 0\r\n# 1.4.4 True\r\n# 1.4.5 False\r\n# 1.4.6 False\r\n```\r\n\r\nAny idea what could be causing the discrepancy? Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1867", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1867/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1867/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1867/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1867", "id": 472142392, "node_id": "MDU6SXNzdWU0NzIxNDIzOTI=", "number": 1867, "title": "Gevent support for kafka-python", "user": {"login": "carsonip", "id": 9133397, "node_id": "MDQ6VXNlcjkxMzMzOTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9133397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carsonip", "html_url": "https://github.com/carsonip", "followers_url": "https://api.github.com/users/carsonip/followers", "following_url": "https://api.github.com/users/carsonip/following{/other_user}", "gists_url": "https://api.github.com/users/carsonip/gists{/gist_id}", "starred_url": "https://api.github.com/users/carsonip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carsonip/subscriptions", "organizations_url": "https://api.github.com/users/carsonip/orgs", "repos_url": "https://api.github.com/users/carsonip/repos", "events_url": "https://api.github.com/users/carsonip/events{/privacy}", "received_events_url": "https://api.github.com/users/carsonip/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-07-24T08:16:11Z", "updated_at": "2019-12-29T16:35:40Z", "closed_at": "2019-12-29T16:35:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\nI have been working on gevent support much like what @jeffwidman did a while ago. Obviously, for a proper gevent python program, avoiding to patch thread is just neither realistic nor reasonable because other parts of a gevent program may depend on thread being patched.\r\nPrevious important discussions about kafka-python and gevent include #1687 #1515 \r\n\r\nI spent some time digging into the issues of kafka-python with gevent. The issues are mostly about kafka consumers since it involves a heartbeat thread. My investigation is based on kafka-python version: 1.4.6 (the most recent release)\r\n\r\n**1. Starvation of heartbeat thread**\r\n\r\nMy observation does not agree with @jeffwidman 's observation in #1515 (given that #1515 was created a year ago, it is possible that things might have changed). It is not related to other greenlets eating up CPU and starving the heartbeat. It is about the conflict of how gevent cooperative multitasking works and the usage of locks in kafka-python. The hang / starvation is due to heartbeat thread failing to acquire client lock.\r\n\r\nLet's assume that heartbeat thread is starved (I'll explain the cause later). Why does it hang the consumer (main thread)? In consumer, https://github.com/dpkp/kafka-python/blob/1.4.6/kafka/consumer/group.py#L1088 shortcuts the fetch when `time.time() > timeout_at`, with `timeout_at = self._coordinator.time_to_next_poll() + time.time()`, which is ` 0 + time.time()` when heartbeat is starved. Since timeout_at is evaluated first, the next call to `time.time()` is almost guaranteed to be larger than the previous call of `0 + time.time()`, that's why fetch is never accessed. (There are some rare cases where the 2nd time.time() call is not greater than the first time.time(). In those cases consumer can access the fetcher and fetch one message, then get \"internal iterator timeout - breaking for poll\", and get back to the time.time lottery again. The heartbeat thread remains starved.)\r\n\r\nNow let's look at why heartbeat thread is starved. Heartbeat thread requires client (KafkaClient) lock: https://github.com/dpkp/kafka-python/blob/1.4.6/kafka/coordinator/base.py#L964 and https://github.com/dpkp/kafka-python/blob/1.4.6/kafka/coordinator/base.py#L997\r\nIn consumer's _message_generator, there is almost no way to yield to heartbeat thread's coroutine. Gevent automatically yields to other greenlets when monkey patch is applied and the patched network functions are called. But the network calls (e.g. `_poll`) inside KafkaClient are protected by client's `_lock`. In areas not protected by the locks, there are no functions that will yield cooperatively. This is how the heartbeat thread is starved.\r\n\r\nThis can be solved by adding `time.sleep(0)` before acquiring the lock in KafkaClient. This gives a chance for heartbeat thread to run and acquire the lock. https://github.com/dpkp/kafka-python/blob/1.4.6/kafka/client_async.py#L570\r\n\r\n**2. (Past issue, fixed by commit 9f0b518286ecfc6db8b7abbd2431810c16f1cc80, no releases yet) Heartbeat thread consuming fetch messages, making the fetch thread hang.**\r\n\r\nDue to a race condition, sometimes heartbeat thread consumes a fetch response. I am not sure whether this happens without gevent.\r\n\r\nIn these unfortunate cases, the fetch thread waiting for poll for the fetch response (that has been consumed by heartbeat thread) for a long time (a few minutes), as determined by https://github.com/dpkp/kafka-python/blob/1.4.6/kafka/client_async.py#L586 when timeout_ms is inf (`_consumer_timeout`). `timeout_ms` is inf when `in_flight_fetches()` returns True before acquiring the lock and False after acquiring the lock, because heartbeat thread consumes the fetch response before main thread acquires the lock.\r\n\r\nWith commit 9f0b51, before really doing a network call, the number of in_flight_request_count of KafkaClient is checked such that client will not poll with a timeout of a few minutes. The root cause of this issue is that `self._fetcher.in_flight_fetches` in `_message_generator` in consumer is not protected by a lock. Also this problematic unprotected checking is proved redundant after commit 9f0b51.\r\n\r\n**Conclusion**\r\n\r\nIn conclusion, I would like official support for gevent in kafka-python. I have yet to find any problems apart from the ones mentioned. It is actually not that hard to make it work. The key remaining part is to make sure it works reliably, meaning that more testing and code review should be done. My current understanding of the codebase is limited to this investigation. I am willing to work on it if this suggestion is accepted by the maintainers.\r\n\r\nThe motivation is that my production services which use gevent are now moving away from pykafka for stability reasons. Given that kafka-python is a mature and popular python kafka library, it would be great if kafka-python is compatible with gevent.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/dpkp/kafka-python/issues/1866", "repository_url": "https://api.github.com/repos/dpkp/kafka-python", "labels_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1866/labels{/name}", "comments_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1866/comments", "events_url": "https://api.github.com/repos/dpkp/kafka-python/issues/1866/events", "html_url": "https://github.com/dpkp/kafka-python/issues/1866", "id": 471039503, "node_id": "MDU6SXNzdWU0NzEwMzk1MDM=", "number": 1866, "title": "kafkaConsumer hangs when use gevent worker with gunicorn", "user": {"login": "mohammedvaghjipurwala", "id": 20841625, "node_id": "MDQ6VXNlcjIwODQxNjI1", "avatar_url": "https://avatars0.githubusercontent.com/u/20841625?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mohammedvaghjipurwala", "html_url": "https://github.com/mohammedvaghjipurwala", "followers_url": "https://api.github.com/users/mohammedvaghjipurwala/followers", "following_url": "https://api.github.com/users/mohammedvaghjipurwala/following{/other_user}", "gists_url": "https://api.github.com/users/mohammedvaghjipurwala/gists{/gist_id}", "starred_url": "https://api.github.com/users/mohammedvaghjipurwala/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mohammedvaghjipurwala/subscriptions", "organizations_url": "https://api.github.com/users/mohammedvaghjipurwala/orgs", "repos_url": "https://api.github.com/users/mohammedvaghjipurwala/repos", "events_url": "https://api.github.com/users/mohammedvaghjipurwala/events{/privacy}", "received_events_url": "https://api.github.com/users/mohammedvaghjipurwala/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-07-22T11:23:07Z", "updated_at": "2019-12-29T16:36:52Z", "closed_at": "2019-12-29T16:36:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "> Use Case : I want consumer to run Continuously even if no data is being sent to the topic for few days.\r\n\r\n\r\nI am using kafka-python 1.4.6, Tech stack used is Gunicorn with gevent worker with Flask and kafka-python.\r\nmy consumer becomes dead/idle and cannot receive messages.\r\n\r\n```\r\nBelow is the consumer\r\n\r\nconsumer = KafkaConsumer(\r\n            topic,\r\n            group_id='grp_id',\r\n            bootstrap_servers=['broker_list'],\r\n            enable_auto_commit=False,\r\n            auto_offset_reset=earliest,\r\n            consumer_timeout_ms=float('inf'),\r\n            max_poll_interval_ms=300000,\r\n            max_poll_records=3,\r\n            session_timeout_ms=9000\r\n            )\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}]}