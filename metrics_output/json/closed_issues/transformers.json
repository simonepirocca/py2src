{"total_count": 3566, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/huggingface/transformers/issues/6647", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6647/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6647/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6647/events", "html_url": "https://github.com/huggingface/transformers/issues/6647", "id": 683671250, "node_id": "MDU6SXNzdWU2ODM2NzEyNTA=", "number": 6647, "title": "mbart broken in summarization pipeline", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-21T16:18:50Z", "updated_at": "2020-08-21T16:58:33Z", "closed_at": "2020-08-21T16:58:33Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```\r\nsummarizer = pipeline(\"summarization\", model=\"facebook/mbart-large-cc25\", tokenizer=\"facebook/mbart-large-cc25\")\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6646", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6646/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6646/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6646/events", "html_url": "https://github.com/huggingface/transformers/issues/6646", "id": 683662338, "node_id": "MDU6SXNzdWU2ODM2NjIzMzg=", "number": 6646, "title": "Error when loading my trained model", "user": {"login": "yy147", "id": 5929774, "node_id": "MDQ6VXNlcjU5Mjk3NzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/5929774?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yy147", "html_url": "https://github.com/yy147", "followers_url": "https://api.github.com/users/yy147/followers", "following_url": "https://api.github.com/users/yy147/following{/other_user}", "gists_url": "https://api.github.com/users/yy147/gists{/gist_id}", "starred_url": "https://api.github.com/users/yy147/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yy147/subscriptions", "organizations_url": "https://api.github.com/users/yy147/orgs", "repos_url": "https://api.github.com/users/yy147/repos", "events_url": "https://api.github.com/users/yy147/events{/privacy}", "received_events_url": "https://api.github.com/users/yy147/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-21T16:03:02Z", "updated_at": "2020-08-21T17:45:10Z", "closed_at": "2020-08-21T17:45:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, \r\n\r\nI tried to train the question-answering model using `bert-base-uncased` on SQUADv1.1. The training process seems to be successfully completed. However, when I load the trained model, it said that `File \"h5py/h5f.pyx\", line 88, in h5py.h5f.open\r\nOSError: Unable to open file (file signature not found)`\r\n\r\nHere is my configuration for the training process: \r\n```\r\nmodel_name_or_path: bert-base-uncased\r\ndo_train: True\r\ndo_eval: True\r\noverwrite_output_dir: True\r\nnum_train_epochs: 10\r\nper_device_train_batch_size: 12\r\nper_device_eval_batch_size: 12\r\nwarmup_steps: 100\r\nweight_decay: 0.01\r\nlearning_rate: 3e-5\r\nevaluate_during_training: True\r\nsave_steps: 5000\r\n```\r\n\r\nAnd here is what I stored in my model directory: \r\n```\r\ncheckpoint-10000  checkpoint-35000  checkpoint-55000  pytorch_model.bin\r\ncheckpoint-15000  checkpoint-40000  checkpoint-60000  special_tokens_map.json\r\ncheckpoint-20000  checkpoint-45000  checkpoint-65000  tokenizer_config.json\r\ncheckpoint-25000  checkpoint-5000   checkpoint-70000  training_args.bin\r\ncheckpoint-30000  checkpoint-50000  config.json vocab.txt\r\n```\r\n\r\nI tried to load my model by \r\n```\r\nself._model = BertForQuestionAnswering.from_pretrained(`./model/trained_squad/`, from_tf=True)\r\n```\r\n\r\nIt would be appreciated if anyone can give me a clue about what happens here. Is there anything wrong with my training process? \r\n\r\nBest,\r\nYanchao\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6620", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6620/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6620/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6620/events", "html_url": "https://github.com/huggingface/transformers/issues/6620", "id": 682873627, "node_id": "MDU6SXNzdWU2ODI4NzM2Mjc=", "number": 6620, "title": "Pegasus: OSError: Unable to load weights from pytorch checkpoint file.", "user": {"login": "yxyzzz", "id": 5890954, "node_id": "MDQ6VXNlcjU4OTA5NTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5890954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yxyzzz", "html_url": "https://github.com/yxyzzz", "followers_url": "https://api.github.com/users/yxyzzz/followers", "following_url": "https://api.github.com/users/yxyzzz/following{/other_user}", "gists_url": "https://api.github.com/users/yxyzzz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yxyzzz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yxyzzz/subscriptions", "organizations_url": "https://api.github.com/users/yxyzzz/orgs", "repos_url": "https://api.github.com/users/yxyzzz/repos", "events_url": "https://api.github.com/users/yxyzzz/events{/privacy}", "received_events_url": "https://api.github.com/users/yxyzzz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-20T16:23:36Z", "updated_at": "2020-08-20T19:29:40Z", "closed_at": "2020-08-20T19:29:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: macOS-10.14.6-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyTorch version (GPU?): 1.6.0 (False)\r\n- Tensorflow version (GPU?): 2.2.0 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help\r\n@sshleifer \r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): google/pegasus-cnn_dailymail\r\n\r\nThe problem arises when using:\r\n```\r\nimport torch\r\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\r\n\r\n\r\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\nmodel_name = 'google/pegasus-cnn_dailymail'\r\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\r\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\r\n```\r\n\r\nTraceback:\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n~/projects/transformers/src/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    854             try:\r\n--> 855                 state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\r\n    856             except Exception:\r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\r\n    584                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\r\n--> 585         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n    586 \r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)\r\n    771         assert key in deserialized_objects\r\n--> 772         deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)\r\n    773         if offset is not None:\r\n\r\nRuntimeError: unexpected EOF, expected 10498989 more bytes. The file might be corrupted.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-1-1ae6eb884edd> in <module>\r\n      7 model_name = 'google/pegasus-cnn_dailymail'\r\n      8 tokenizer = PegasusTokenizer.from_pretrained(model_name)\r\n----> 9 model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\r\n\r\n~/projects/transformers/src/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    855                 state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\r\n    856             except Exception:\r\n--> 857                 raise OSError(\r\n    858                     \"Unable to load weights from pytorch checkpoint file. \"\r\n    859                     \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\r\n\r\nOSError: Unable to load weights from pytorch checkpoint file. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6619", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6619/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6619/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6619/events", "html_url": "https://github.com/huggingface/transformers/issues/6619", "id": 682855128, "node_id": "MDU6SXNzdWU2ODI4NTUxMjg=", "number": 6619, "title": "[DistilBert] Flaky tests", "user": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-20T16:07:40Z", "updated_at": "2020-08-20T17:23:48Z", "closed_at": "2020-08-20T17:23:48Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-5.3.0-61-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.5\r\n- PyTorch version (GPU?): 1.6.0+cpu (False)\r\n- Tensorflow version (GPU?): 2.1.1 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n\r\n### Reproduce Error:\r\n\r\n```python \r\n#!/usr/bin/env python3\r\nimport torch\r\nfrom transformers import DistilBertModel, DistilBertConfig\r\n\r\ninput_ids = torch.tensor([[55, 40, 88, 37, 12,  6, 20],\r\n        [33, 87, 56,  6, 34, 92,  2],\r\n        [ 4, 25, 95, 19,  9, 14, 80],\r\n        [96, 45, 71, 10, 78, 33, 68],\r\n        [72, 40, 59, 90,  5, 78, 44],\r\n        [36, 15, 11, 18, 74, 40, 30],\r\n        [84, 25,  5, 61, 18, 77, 35],\r\n        [70, 87,  9, 42, 24, 65, 11],\r\n        [28,  0, 28, 45, 92, 83, 96],\r\n        [75, 41, 69, 61, 83, 31, 81],\r\n        [94, 93, 79, 48, 24, 17,  9],\r\n        [97,  5, 38, 94, 75,  8, 59],\r\n        [31, 71, 87, 39, 97, 10, 22]])\r\n\r\nattention_mask = torch.tensor([[1, 1, 1, 1, 0, 1, 0],\r\n        [0, 0, 0, 0, 0, 0, 0],\r\n        [1, 1, 1, 1, 1, 1, 1],\r\n        [0, 1, 1, 1, 0, 0, 1],\r\n        [1, 1, 1, 0, 1, 1, 0],\r\n        [1, 1, 1, 1, 1, 1, 1],\r\n        [0, 0, 1, 0, 0, 1, 1],\r\n        [1, 0, 1, 0, 0, 0, 1],\r\n        [0, 1, 0, 1, 1, 1, 0],\r\n        [0, 1, 1, 0, 0, 0, 0],\r\n        [0, 1, 0, 1, 1, 0, 1],\r\n        [0, 1, 1, 1, 1, 1, 1],\r\n        [0, 1, 0, 1, 0, 0, 0]])\r\n\r\n\r\ndistil_bert_config = {\r\n  \"activation\": \"gelu\",\r\n  \"attention_dropout\": 0.1,\r\n  \"dim\": 32,\r\n  \"dropout\": 0.1,\r\n  \"hidden_act\": \"gelu\",\r\n  \"hidden_dim\": 37,\r\n  \"initializer_range\": 0.02,\r\n  \"max_position_embeddings\": 512,\r\n  \"model_type\": \"distilbert\",\r\n  \"n_heads\": 4,\r\n  \"n_layers\": 5,\r\n  \"pad_token_id\": 0,\r\n  \"qa_dropout\": 0.1,\r\n  \"return_dict\": True,\r\n  \"seq_classif_dropout\": 0.2,\r\n  \"sinusoidal_pos_embds\": False,\r\n  \"vocab_size\": 99\r\n}\r\n\r\nconfig = DistilBertConfig(**distil_bert_config)\r\ntorch.manual_seed(0)\r\nmodel = DistilBertModel(config).eval()\r\n\r\nlast_hidden_state = model(input_ids, attention_mask=attention_mask)[0]\r\n\r\nif torch.isnan(last_hidden_state).any().item():\r\n    print(\"Error with DistilBert\")\r\n```\r\n\r\nThis code example allows yields nan values.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6609", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6609/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6609/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6609/events", "html_url": "https://github.com/huggingface/transformers/issues/6609", "id": 682247720, "node_id": "MDU6SXNzdWU2ODIyNDc3MjA=", "number": 6609, "title": "PegasusForConditionalGeneration - Error in loading state dictionary", "user": {"login": "suchig", "id": 37094536, "node_id": "MDQ6VXNlcjM3MDk0NTM2", "avatar_url": "https://avatars3.githubusercontent.com/u/37094536?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suchig", "html_url": "https://github.com/suchig", "followers_url": "https://api.github.com/users/suchig/followers", "following_url": "https://api.github.com/users/suchig/following{/other_user}", "gists_url": "https://api.github.com/users/suchig/gists{/gist_id}", "starred_url": "https://api.github.com/users/suchig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suchig/subscriptions", "organizations_url": "https://api.github.com/users/suchig/orgs", "repos_url": "https://api.github.com/users/suchig/repos", "events_url": "https://api.github.com/users/suchig/events{/privacy}", "received_events_url": "https://api.github.com/users/suchig/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-08-19T23:31:38Z", "updated_at": "2020-08-20T21:54:02Z", "closed_at": "2020-08-20T19:34:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-5.3.0-1034-azure-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.7\r\n- PyTorch version (GPU?): 1.6.0 (True)\r\n- Tensorflow version (GPU?): not installed\r\n- Using GPU in script?: Tried both\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help\r\n@sshleifer\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): google/pegasus-arxiv\r\n\r\nThe problem arises when using:\r\nthe official example scripts: \r\n\r\nThe tasks I am working on is:\r\ngenerating summary using pegasus-arxiv\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\nrun the below script\r\n```ruby\r\nmname = \"google/pegasus-arxiv\"\r\nmodel = PegasusForConditionalGeneration.from_pretrained(mname)\r\n```\r\n\r\nThis is throwing error\r\n File \"abstractive_summarizer.py\", line 21, in <module>\r\n    model = PegasusForConditionalGeneration.from_pretrained(mname, force_download=True)\r\n  File \"/anaconda/envs/py37_default/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 894, in from_pretrained\r\n    model.__class__.__name__, \"\\n\\t\".join(error_msgs)\r\nRuntimeError: Error(s) in loading state_dict for PegasusForConditionalGeneration:\r\n\tsize mismatch for model.encoder.embed_positions.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n\tsize mismatch for model.decoder.embed_positions.weight: copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\r\n\r\n## Expected behavior\r\nI tried running a sample in console this morning and it worked fine and I was able to generate summary using pegasus-arxiv. Once I transferred this to Jupyter notebook for some trial purpose, it downloaded pegasus-arxiv and after the same has been giving this error.\r\n\r\n(If not able to simulate, please try with force_download=True)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6599", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6599/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6599/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6599/events", "html_url": "https://github.com/huggingface/transformers/issues/6599", "id": 681945105, "node_id": "MDU6SXNzdWU2ODE5NDUxMDU=", "number": 6599, "title": "Pegasus: IndexError: index out of range in self", "user": {"login": "yxyzzz", "id": 5890954, "node_id": "MDQ6VXNlcjU4OTA5NTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5890954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yxyzzz", "html_url": "https://github.com/yxyzzz", "followers_url": "https://api.github.com/users/yxyzzz/followers", "following_url": "https://api.github.com/users/yxyzzz/following{/other_user}", "gists_url": "https://api.github.com/users/yxyzzz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yxyzzz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yxyzzz/subscriptions", "organizations_url": "https://api.github.com/users/yxyzzz/orgs", "repos_url": "https://api.github.com/users/yxyzzz/repos", "events_url": "https://api.github.com/users/yxyzzz/events{/privacy}", "received_events_url": "https://api.github.com/users/yxyzzz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2020-08-19T15:40:55Z", "updated_at": "2020-08-20T19:33:55Z", "closed_at": "2020-08-20T19:33:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "@sshleifer\r\n\r\n## Environment info\r\n- `transformers` version: 3.0.2\r\n- Platform: macOS-10.14.6-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyTorch version (GPU?): 1.6.0 (False)\r\n- Tensorflow version (GPU?): 2.2.0 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n## Information\r\nModel I am using: google/pegasus-cnn_dailymail\r\n\r\nThe problem arises when using:\r\n```\r\nimport torch\r\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\r\n\r\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\nmodel = AutoModelWithLMHead.from_pretrained(\"google/pegasus-cnn_dailymail\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")\r\n\r\nsrc_text = [\r\n    '(CNN)James Best, best known for his portrayal of bumbling sheriff Rosco P. Coltrane on TV\\'s \"The Dukes of Hazzard,\" died Monday after a brief illness. He was 88. Best died in hospice in Hickory, North Carolina, of complications from pneumonia, said Steve Latshaw, a longtime friend and Hollywood colleague. Although he\\'d been a busy actor for decades in theater and in Hollywood, Best didn\\'t become famous until 1979, when \"The Dukes of Hazzard\\'s\" cornpone charms began beaming into millions of American homes almost every Friday night. For seven seasons, Best\\'s Rosco P. Coltrane chased the moonshine-running Duke boys back and forth across the back roads of fictitious Hazzard County, Georgia, although his \"hot pursuit\" usually ended with him crashing his patrol car. Although Rosco was slow-witted and corrupt, Best gave him a childlike enthusiasm that got laughs and made him endearing. His character became known for his distinctive \"kew-kew-kew\" chuckle and for goofy catchphrases such as \"cuff \\'em and stuff \\'em!\" upon making an arrest. Among the most popular shows on TV in the early \\'80s, \"The Dukes of Hazzard\" ran until 1985 and spawned TV movies, an animated series and video games. Several of Best\\'s \"Hazzard\" co-stars paid tribute to the late actor on social media. \"I laughed and learned more from Jimmie in one hour than from anyone else in a whole year,\" co-star John Schneider, who played Bo Duke, said on Twitter. \"Give Uncle Jesse my love when you see him dear friend.\" \"Jimmy Best was the most constantly creative person I have ever known,\" said Ben Jones, who played mechanic Cooter on the show, in a Facebook post. \"Every minute of his long life was spent acting, writing, producing, painting, teaching, fishing, or involved in another of his life\\'s many passions.\" Born Jewel Guy on July 26, 1926, in Powderly, Kentucky, Best was orphaned at 3 and adopted by Armen and Essa Best, who renamed him James and raised him in rural Indiana. Best served in the Army during World War II before launching his acting career. In the 1950s and 1960s, he accumulated scores of credits, playing a range of colorful supporting characters in such TV shows as \"The Twilight Zone,\" \"Bonanza,\" \"The Andy Griffith Show\" and \"Gunsmoke.\" He later appeared in a handful of Burt Reynolds\\' movies, including \"Hooper\" and \"The End.\" But Best will always be best known for his \"Hazzard\" role, which lives on in reruns. \"Jimmie was my teacher, mentor, close friend and collaborator for 26 years,\" Latshaw said. \"I directed two of his feature films, including the recent \\'Return of the Killer Shrews,\\' a sequel he co-wrote and was quite proud of as he had made the first one more than 50 years earlier.\" People we\\'ve lost in 2015 . CNN\\'s Stella Chan contributed to this story.'\r\n]\r\nbatch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest').to(torch_device)\r\ntranslated = model.generate(**batch)\r\ntgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\r\n```\r\n\r\nThe tasks I am working on is:\r\n* abstractive text summarization\r\n\r\n## To reproduce\r\n1. Run the script above. The traceback:\r\n```\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-1-ac154ffeb574> in <module>\r\n     13 ]\r\n     14 batch = tokenizer.prepare_seq2seq_batch(src_text, truncation=True, padding='longest').to(torch_device)\r\n---> 15 translated = model.generate(**batch)\r\n     16 tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\r\n     13         def decorate_context(*args, **kwargs):\r\n     14             with self:\r\n---> 15                 return func(*args, **kwargs)\r\n     16         return decorate_context\r\n     17 \r\n\r\n~/projects/transformers/src/transformers/generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs)\r\n    394             encoder = self.get_encoder()\r\n    395 \r\n--> 396             encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)\r\n    397 \r\n    398         # Expand input ids if num_beams > 1 or num_return_sequences > 1\r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\r\n    720             result = self._slow_forward(*input, **kwargs)\r\n    721         else:\r\n--> 722             result = self.forward(*input, **kwargs)\r\n    723         for hook in itertools.chain(\r\n    724                 _global_forward_hooks.values(),\r\n\r\n~/projects/transformers/src/transformers/modeling_bart.py in forward(self, input_ids, attention_mask, output_attentions, output_hidden_states, return_dict)\r\n    328 \r\n    329         inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\r\n--> 330         embed_pos = self.embed_positions(input_ids)\r\n    331         x = inputs_embeds + embed_pos\r\n    332         x = self.layernorm_embedding(x)\r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\r\n    720             result = self._slow_forward(*input, **kwargs)\r\n    721         else:\r\n--> 722             result = self.forward(*input, **kwargs)\r\n    723         for hook in itertools.chain(\r\n    724                 _global_forward_hooks.values(),\r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)\r\n     13         def decorate_context(*args, **kwargs):\r\n     14             with self:\r\n---> 15                 return func(*args, **kwargs)\r\n     16         return decorate_context\r\n     17 \r\n\r\n~/projects/transformers/src/transformers/modeling_bart.py in forward(self, input_ids, use_cache)\r\n   1337             # starts at 0, ends at 1-seq_len\r\n   1338             positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\r\n-> 1339         return super().forward(positions)\r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/nn/modules/sparse.py in forward(self, input)\r\n    122 \r\n    123     def forward(self, input: Tensor) -> Tensor:\r\n--> 124         return F.embedding(\r\n    125             input, self.weight, self.padding_idx, self.max_norm,\r\n    126             self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n\r\n~/anaconda3/envs/abstractive_summarizer/lib/python3.8/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\r\n   1812         # remove once script supports set_grad_enabled\r\n   1813         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\n-> 1814     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n   1815 \r\n   1816 \r\n\r\nIndexError: index out of range in self\r\n```\r\n\r\n## Expected behavior\r\nIndexError: index out of range in self\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6597", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6597/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6597/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6597/events", "html_url": "https://github.com/huggingface/transformers/issues/6597", "id": 681889401, "node_id": "MDU6SXNzdWU2ODE4ODk0MDE=", "number": 6597, "title": " tf2 transformers cache dir", "user": {"login": "VasudevGupta7", "id": 53136577, "node_id": "MDQ6VXNlcjUzMTM2NTc3", "avatar_url": "https://avatars2.githubusercontent.com/u/53136577?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VasudevGupta7", "html_url": "https://github.com/VasudevGupta7", "followers_url": "https://api.github.com/users/VasudevGupta7/followers", "following_url": "https://api.github.com/users/VasudevGupta7/following{/other_user}", "gists_url": "https://api.github.com/users/VasudevGupta7/gists{/gist_id}", "starred_url": "https://api.github.com/users/VasudevGupta7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VasudevGupta7/subscriptions", "organizations_url": "https://api.github.com/users/VasudevGupta7/orgs", "repos_url": "https://api.github.com/users/VasudevGupta7/repos", "events_url": "https://api.github.com/users/VasudevGupta7/events{/privacy}", "received_events_url": "https://api.github.com/users/VasudevGupta7/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-19T14:32:12Z", "updated_at": "2020-08-21T14:36:38Z", "closed_at": "2020-08-21T14:36:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "what is the directory of tf2 transformers cache weights?\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6588", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6588/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6588/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6588/events", "html_url": "https://github.com/huggingface/transformers/issues/6588", "id": 681508638, "node_id": "MDU6SXNzdWU2ODE1MDg2Mzg=", "number": 6588, "title": "all_hidden_states indentation bug in modeling_bert.py", "user": {"login": "YuanEric88", "id": 32417149, "node_id": "MDQ6VXNlcjMyNDE3MTQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/32417149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YuanEric88", "html_url": "https://github.com/YuanEric88", "followers_url": "https://api.github.com/users/YuanEric88/followers", "following_url": "https://api.github.com/users/YuanEric88/following{/other_user}", "gists_url": "https://api.github.com/users/YuanEric88/gists{/gist_id}", "starred_url": "https://api.github.com/users/YuanEric88/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YuanEric88/subscriptions", "organizations_url": "https://api.github.com/users/YuanEric88/orgs", "repos_url": "https://api.github.com/users/YuanEric88/repos", "events_url": "https://api.github.com/users/YuanEric88/events{/privacy}", "received_events_url": "https://api.github.com/users/YuanEric88/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-19T02:59:54Z", "updated_at": "2020-08-19T03:07:35Z", "closed_at": "2020-08-19T03:07:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version:\r\n- Platform:\r\n- Python version:\r\n- PyTorch version (GPU?):\r\n- Tensorflow version (GPU?):\r\n- Using GPU in script?:\r\n- Using distributed or parallel set-up in script?:\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n examples/bert-loses-patience: @JetRunner\r\n tensorflow: @jplu\r\n examples/token-classification: @stefan-it\r\n documentation: @sgugger\r\n -->\r\n@lavanyashukla \r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): Bert\r\n\r\nThe problem arises when using:\r\n* [x] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [ ] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\nhttps://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L490\r\n## Expected behavior\r\nhttps://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L490 There is a small bug here, I suppose Line491 and Line491 should be inside loop chunk. \r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6585", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6585/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6585/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6585/events", "html_url": "https://github.com/huggingface/transformers/issues/6585", "id": 681399065, "node_id": "MDU6SXNzdWU2ODEzOTkwNjU=", "number": 6585, "title": "Failing bart-base slow test", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-08-18T23:22:42Z", "updated_at": "2020-08-19T01:28:10Z", "closed_at": "2020-08-19T01:28:10Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```\r\n=================================== FAILURES ===================================\r\n____________ BartModelIntegrationTests.test_bart_base_mask_filling _____________\r\n[gw0] linux -- Python 3.7.6 /home/hf/actions-runner_transformers/_work/transformers/transformers/.env/bin/python\r\n\r\nself = <tests.test_modeling_bart.BartModelIntegrationTests testMethod=test_bart_base_mask_filling>\r\n\r\n    @slow\r\n    def test_bart_base_mask_filling(self):\r\n        pbase = pipeline(task=\"fill-mask\", model=\"facebook/bart-base\")\r\n        src_text = [\" I went to the <mask>.\"]\r\n        results = [x[\"token_str\"] for x in pbase(src_text)]\r\n        expected_results = [\"\u0120bathroom\", \"\u0120restroom\", \"\u0120hospital\", \"\u0120kitchen\", \"\u0120car\"]\r\n>       self.assertListEqual(results, expected_results)\r\nE       AssertionError: Lists differ: ['\u0120library', '\u0120hospital', '\u0120bathroom', '\u0120movies', '\u0120police'] != ['\u0120bathroom', '\u0120restroom', '\u0120hospital', '\u0120kitchen', '\u0120car']\r\nE       \r\nE       First differing element 0:\r\n```\r\n\r\nhttps://github.com/huggingface/transformers/runs/996031882?check_suite_focus=true", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6575", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6575/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6575/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6575/events", "html_url": "https://github.com/huggingface/transformers/issues/6575", "id": 681084803, "node_id": "MDU6SXNzdWU2ODEwODQ4MDM=", "number": 6575, "title": "Tokenizer further tokenizes pretokenized input", "user": {"login": "bogdankostic", "id": 48713846, "node_id": "MDQ6VXNlcjQ4NzEzODQ2", "avatar_url": "https://avatars2.githubusercontent.com/u/48713846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bogdankostic", "html_url": "https://github.com/bogdankostic", "followers_url": "https://api.github.com/users/bogdankostic/followers", "following_url": "https://api.github.com/users/bogdankostic/following{/other_user}", "gists_url": "https://api.github.com/users/bogdankostic/gists{/gist_id}", "starred_url": "https://api.github.com/users/bogdankostic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bogdankostic/subscriptions", "organizations_url": "https://api.github.com/users/bogdankostic/orgs", "repos_url": "https://api.github.com/users/bogdankostic/repos", "events_url": "https://api.github.com/users/bogdankostic/events{/privacy}", "received_events_url": "https://api.github.com/users/bogdankostic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sgugger", "id": 35901082, "node_id": "MDQ6VXNlcjM1OTAxMDgy", "avatar_url": "https://avatars3.githubusercontent.com/u/35901082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgugger", "html_url": "https://github.com/sgugger", "followers_url": "https://api.github.com/users/sgugger/followers", "following_url": "https://api.github.com/users/sgugger/following{/other_user}", "gists_url": "https://api.github.com/users/sgugger/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgugger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgugger/subscriptions", "organizations_url": "https://api.github.com/users/sgugger/orgs", "repos_url": "https://api.github.com/users/sgugger/repos", "events_url": "https://api.github.com/users/sgugger/events{/privacy}", "received_events_url": "https://api.github.com/users/sgugger/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sgugger", "id": 35901082, "node_id": "MDQ6VXNlcjM1OTAxMDgy", "avatar_url": "https://avatars3.githubusercontent.com/u/35901082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgugger", "html_url": "https://github.com/sgugger", "followers_url": "https://api.github.com/users/sgugger/followers", "following_url": "https://api.github.com/users/sgugger/following{/other_user}", "gists_url": "https://api.github.com/users/sgugger/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgugger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgugger/subscriptions", "organizations_url": "https://api.github.com/users/sgugger/orgs", "repos_url": "https://api.github.com/users/sgugger/repos", "events_url": "https://api.github.com/users/sgugger/events{/privacy}", "received_events_url": "https://api.github.com/users/sgugger/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-18T14:23:54Z", "updated_at": "2020-08-19T17:04:37Z", "closed_at": "2020-08-19T17:04:37Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: current master\r\n- Platform: MacOS\r\n- Python version: 3.7\r\n\r\n### Who can help\r\n@mfuntowicz \r\n\r\n## Information\r\nIt seems that passing pretokenized input to the Tokenizer and setting `is_pretokenized=True` doesn't prevent the Tokenizer from further tokenizing the input. This issue already came up in #6046 and the reason for this seems to be #6573 . A workaround is to set `is_pretokenized=False`. \r\nWhat hasn't been reported yet is that this issue also arises with FastTokenizers where we see the same behavior. However, there is no workaround for FastTokenizers (or at least I haven't found one...). Setting `is_pretokenized=False` will raise a ValueError.\r\n\r\n## To reproduce\r\n\r\n```python\r\nfrom transformers.tokenization_auto import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\r\nfast_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\", use_fast=True)\r\n\r\ntext = \"Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot ist\"\r\npretokenized_text = ['Schar', '##tau', 'sagte', 'dem', 'Tages', '##spiegel', ',', 'dass', 'Fischer', 'ein', 'Id', '##iot', 'ist']\r\n\r\ntokenized = tokenizer.encode(text)\r\n# returns list of len 15 -> 13 tokens + 2 special tokens\r\npretokenized_tok = tokenizer.encode(pretokenized_text, is_pretokenized=True)\r\n# returns list of len 23 -> too large\r\npretokenized_tok_2 = tokenizer.encode(pretokenized_text, is_pretokenized=False)\r\n# returns list of len 15 -> 13 tokens + 2 special tokens\r\n\r\nfast_tokenized = fast_tokenizer.encode(text)\r\n# returns list of len 15 -> 13 tokens + 2 special tokens\r\nfast_pretokenized_tok = fast_tokenizer.encode(pretokenized_text, is_pretokenized=True)\r\n# returns list of len 23 -> too large\r\n# fast_pretokenizer_tok2 = fast_tokenizer.encode(pretokenized_text, is_pretokenized=False)\r\n# would raise: 'ValueError: TextInputSequence must be str'\r\n\r\n\r\ntokenized_decoded = tokenizer.decode(tokenized)\r\n# returns '[CLS] Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot ist [SEP]'\r\npretokenized_tok_decoded = tokenizer.decode(pretokenized_tok)\r\n# returns '[CLS] Schar # # tau sagte dem Tages # # spiegel, dass Fischer ein Id # # iot ist [SEP]'\r\npretokenized_tok_2_decoded = tokenizer.decode(pretokenized_tok_2)\r\n# returns '[CLS] Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot ist [SEP]'\r\n\r\n\r\nfast_tokenized_decoded = fast_tokenizer.decode(fast_tokenized)\r\n# returns '[CLS] Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot ist [SEP]'\r\nfast_pretokenized_tok_decoded = fast_tokenizer.decode(fast_pretokenized_tok)\r\n# returns '[CLS] Schar # # tau sagte dem Tages # # spiegel, dass Fischer ein Id # # iot ist [SEP]'\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6552", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6552/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6552/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6552/events", "html_url": "https://github.com/huggingface/transformers/issues/6552", "id": 680623420, "node_id": "MDU6SXNzdWU2ODA2MjM0MjA=", "number": 6552, "title": "Add model card for facebook/mbart-large-en-ro", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-18T01:33:34Z", "updated_at": "2020-08-20T16:46:46Z", "closed_at": "2020-08-20T16:46:46Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Suggested content:\r\n\r\n___\r\ntags:\r\n- translation\r\nlanguage: en\r\n___\r\n\r\nlink to fairseq readme\r\nlink to docs.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6550", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6550/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6550/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6550/events", "html_url": "https://github.com/huggingface/transformers/issues/6550", "id": 680584024, "node_id": "MDU6SXNzdWU2ODA1ODQwMjQ=", "number": 6550, "title": "504 Gateway Time-out when trying to access Uploaded Model page ", "user": {"login": "paulowoicho", "id": 28223751, "node_id": "MDQ6VXNlcjI4MjIzNzUx", "avatar_url": "https://avatars3.githubusercontent.com/u/28223751?v=4", "gravatar_id": "", "url": "https://api.github.com/users/paulowoicho", "html_url": "https://github.com/paulowoicho", "followers_url": "https://api.github.com/users/paulowoicho/followers", "following_url": "https://api.github.com/users/paulowoicho/following{/other_user}", "gists_url": "https://api.github.com/users/paulowoicho/gists{/gist_id}", "starred_url": "https://api.github.com/users/paulowoicho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/paulowoicho/subscriptions", "organizations_url": "https://api.github.com/users/paulowoicho/orgs", "repos_url": "https://api.github.com/users/paulowoicho/repos", "events_url": "https://api.github.com/users/paulowoicho/events{/privacy}", "received_events_url": "https://api.github.com/users/paulowoicho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-17T23:27:22Z", "updated_at": "2020-08-21T11:36:50Z", "closed_at": "2020-08-21T11:36:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\nI recently uploaded a model available at `https://huggingface.co/paulowoicho/t5-podcast-summarisation`. I made a few changes to the model card and reuploaded. Now, the model's page does not load at all. Instead, it shows a 504 Gateway Time-out error.\r\n\r\n![image](https://user-images.githubusercontent.com/28223751/90789131-71616400-e2fe-11ea-928c-070da9ffb449.png)\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6540", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6540/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6540/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6540/events", "html_url": "https://github.com/huggingface/transformers/issues/6540", "id": 680347583, "node_id": "MDU6SXNzdWU2ODAzNDc1ODM=", "number": 6540, "title": "bart-base config.*attention_heads (should be 12 was 16)", "user": {"login": "ibeltagy", "id": 2287797, "node_id": "MDQ6VXNlcjIyODc3OTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2287797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ibeltagy", "html_url": "https://github.com/ibeltagy", "followers_url": "https://api.github.com/users/ibeltagy/followers", "following_url": "https://api.github.com/users/ibeltagy/following{/other_user}", "gists_url": "https://api.github.com/users/ibeltagy/gists{/gist_id}", "starred_url": "https://api.github.com/users/ibeltagy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ibeltagy/subscriptions", "organizations_url": "https://api.github.com/users/ibeltagy/orgs", "repos_url": "https://api.github.com/users/ibeltagy/repos", "events_url": "https://api.github.com/users/ibeltagy/events{/privacy}", "received_events_url": "https://api.github.com/users/ibeltagy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-17T16:05:40Z", "updated_at": "2020-08-17T16:23:50Z", "closed_at": "2020-08-17T16:23:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "[Bart-base configuration](https://s3.amazonaws.com/models.huggingface.co/bert/facebook/bart-base/config.json) has the wrong values for `decoder_attention_heads` and `encoder_attention_heads`. This messes up the self-attention computation and make model totally unusable.  \r\n\r\n### Who can help\r\n@sshleifer\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6534", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6534/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6534/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6534/events", "html_url": "https://github.com/huggingface/transformers/issues/6534", "id": 680127756, "node_id": "MDU6SXNzdWU2ODAxMjc3NTY=", "number": 6534, "title": "How to fine-tune GPT2 on Arithmetic Problem", "user": {"login": "KYRIEZX", "id": 56826566, "node_id": "MDQ6VXNlcjU2ODI2NTY2", "avatar_url": "https://avatars0.githubusercontent.com/u/56826566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KYRIEZX", "html_url": "https://github.com/KYRIEZX", "followers_url": "https://api.github.com/users/KYRIEZX/followers", "following_url": "https://api.github.com/users/KYRIEZX/following{/other_user}", "gists_url": "https://api.github.com/users/KYRIEZX/gists{/gist_id}", "starred_url": "https://api.github.com/users/KYRIEZX/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KYRIEZX/subscriptions", "organizations_url": "https://api.github.com/users/KYRIEZX/orgs", "repos_url": "https://api.github.com/users/KYRIEZX/repos", "events_url": "https://api.github.com/users/KYRIEZX/events{/privacy}", "received_events_url": "https://api.github.com/users/KYRIEZX/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-17T10:30:27Z", "updated_at": "2020-08-18T21:12:46Z", "closed_at": "2020-08-18T21:12:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "# #  I want to use the GPT2 to solve simple Arithmetic Problem, how should I make the dataset?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6517", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6517/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6517/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6517/events", "html_url": "https://github.com/huggingface/transformers/issues/6517", "id": 679744195, "node_id": "MDU6SXNzdWU2Nzk3NDQxOTU=", "number": 6517, "title": "Can't load t5-11b from pre-trained", "user": {"login": "saareliad", "id": 22762845, "node_id": "MDQ6VXNlcjIyNzYyODQ1", "avatar_url": "https://avatars1.githubusercontent.com/u/22762845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saareliad", "html_url": "https://github.com/saareliad", "followers_url": "https://api.github.com/users/saareliad/followers", "following_url": "https://api.github.com/users/saareliad/following{/other_user}", "gists_url": "https://api.github.com/users/saareliad/gists{/gist_id}", "starred_url": "https://api.github.com/users/saareliad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saareliad/subscriptions", "organizations_url": "https://api.github.com/users/saareliad/orgs", "repos_url": "https://api.github.com/users/saareliad/repos", "events_url": "https://api.github.com/users/saareliad/events{/privacy}", "received_events_url": "https://api.github.com/users/saareliad/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-16T12:14:23Z", "updated_at": "2020-08-17T18:45:53Z", "closed_at": "2020-08-17T18:45:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform:\r\n- Python version: 3.8.2\r\n- PyTorch version 1.6\r\n\r\n\r\n### Who can help\r\n\r\n T5: @patrickvonplaten\r\n\r\n## Information\r\n\r\nThe model I am using: T5\r\n\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport transformers\r\ntransformers.T5ForConditionalGeneration.from_pretrained(\"t5-11b\")\r\n```\r\n\r\n```\r\n\r\n\r\nOSError: Can't load weights for 't5-11b'. Make sure that:\r\n\r\n- 't5-11b' is a correct model identifier listed on 'https://huggingface.co/models'\r\n\r\n- or 't5-11b' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.\r\n\r\n```\r\n\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\nthe model should be loaded.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6513", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6513/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6513/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6513/events", "html_url": "https://github.com/huggingface/transformers/issues/6513", "id": 679703746, "node_id": "MDU6SXNzdWU2Nzk3MDM3NDY=", "number": 6513, "title": "Longformer pretrained weights are not really pretrained?", "user": {"login": "dvirginz", "id": 31047807, "node_id": "MDQ6VXNlcjMxMDQ3ODA3", "avatar_url": "https://avatars2.githubusercontent.com/u/31047807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dvirginz", "html_url": "https://github.com/dvirginz", "followers_url": "https://api.github.com/users/dvirginz/followers", "following_url": "https://api.github.com/users/dvirginz/following{/other_user}", "gists_url": "https://api.github.com/users/dvirginz/gists{/gist_id}", "starred_url": "https://api.github.com/users/dvirginz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dvirginz/subscriptions", "organizations_url": "https://api.github.com/users/dvirginz/orgs", "repos_url": "https://api.github.com/users/dvirginz/repos", "events_url": "https://api.github.com/users/dvirginz/events{/privacy}", "received_events_url": "https://api.github.com/users/dvirginz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-16T07:04:17Z", "updated_at": "2020-08-19T04:08:27Z", "closed_at": "2020-08-19T04:08:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version:3.0.2\r\n- Platform:Ubuntu 18.04\r\n- Python version:3.7\r\n- PyTorch version (GPU?):1.6 Yes\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n @patrickvonplaten\r\n\r\n\r\nModel I am using (Bert, XLNet ...): Longformer\r\n\r\nThe problem arises when using:\r\n* [V] my own modified scripts\r\n\r\nThe tasks I am working on is:\r\n* [V] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nAs a preprocessing step in my pipeline I train a pre-trained model on a subset of Wikipedia dataset.\r\nWhen using roBERTa the results of my fine-tuning steps are as follows (LM task):\r\n![image](https://user-images.githubusercontent.com/31047807/90328740-71ccc880-dfa7-11ea-9df5-0163ae48533b.png)\r\n\r\nUnfortunately, when simply plugging in longformer (and pointing the pretrained path to `allenai/longformer-base-4096`\r\n![image](https://user-images.githubusercontent.com/31047807/90328734-5e216200-dfa7-11ea-888b-53e759fd2e29.png)\r\nWhich seems like the weights are simply random-initialized.\r\nI both cases (roberta and longformer) I get the message stating the weights have been initialized.\r\nWhat went wrong?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6501", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6501/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6501/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6501/events", "html_url": "https://github.com/huggingface/transformers/issues/6501", "id": 679588124, "node_id": "MDU6SXNzdWU2Nzk1ODgxMjQ=", "number": 6501, "title": "Longformer slow than Bert", "user": {"login": "Maybewuss", "id": 38156589, "node_id": "MDQ6VXNlcjM4MTU2NTg5", "avatar_url": "https://avatars0.githubusercontent.com/u/38156589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Maybewuss", "html_url": "https://github.com/Maybewuss", "followers_url": "https://api.github.com/users/Maybewuss/followers", "following_url": "https://api.github.com/users/Maybewuss/following{/other_user}", "gists_url": "https://api.github.com/users/Maybewuss/gists{/gist_id}", "starred_url": "https://api.github.com/users/Maybewuss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Maybewuss/subscriptions", "organizations_url": "https://api.github.com/users/Maybewuss/orgs", "repos_url": "https://api.github.com/users/Maybewuss/repos", "events_url": "https://api.github.com/users/Maybewuss/events{/privacy}", "received_events_url": "https://api.github.com/users/Maybewuss/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-15T14:59:35Z", "updated_at": "2020-08-18T21:11:57Z", "closed_at": "2020-08-18T21:11:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "When i set max_length = 2048, i found Longformer's speed is slower than commen bert, why? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6500", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6500/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6500/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6500/events", "html_url": "https://github.com/huggingface/transformers/issues/6500", "id": 679554882, "node_id": "MDU6SXNzdWU2Nzk1NTQ4ODI=", "number": 6500, "title": "Always got RuntimeError while converting ALBERT model to TorchScript (.pt file)", "user": {"login": "xf05888", "id": 33285394, "node_id": "MDQ6VXNlcjMzMjg1Mzk0", "avatar_url": "https://avatars1.githubusercontent.com/u/33285394?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xf05888", "html_url": "https://github.com/xf05888", "followers_url": "https://api.github.com/users/xf05888/followers", "following_url": "https://api.github.com/users/xf05888/following{/other_user}", "gists_url": "https://api.github.com/users/xf05888/gists{/gist_id}", "starred_url": "https://api.github.com/users/xf05888/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xf05888/subscriptions", "organizations_url": "https://api.github.com/users/xf05888/orgs", "repos_url": "https://api.github.com/users/xf05888/repos", "events_url": "https://api.github.com/users/xf05888/events{/privacy}", "received_events_url": "https://api.github.com/users/xf05888/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-15T12:17:53Z", "updated_at": "2020-08-17T07:55:38Z", "closed_at": "2020-08-17T07:55:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to convert ALBERT to a `.pt` file from the original albert model from transformers.(I am not very familiar with TorchScript so I want the `.pt` to be clean)\r\n\r\nThe code I ran (following the tutorial from [https://huggingface.co/transformers/torchscript.html](https://huggingface.co/transformers/torchscript.html)):\r\n```\r\n\r\nfrom transformers import AlbertModel, AlbertTokenizer, AlbertConfig\r\nimport torch\r\n\r\nenc = AlbertTokenizer.from_pretrained(\"albert-xxlarge-v2\")\r\ntext = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\r\ntokenized_text = enc.tokenize(text)\r\nmasked_index = 8\r\ntokenized_text[masked_index] = '[MASK]'\r\nindexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\r\nsegments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\r\ntokens_tensor = torch.tensor([indexed_tokens])\r\nsegments_tensors = torch.tensor([segments_ids])\r\ndummy_input = [tokens_tensor, segments_tensors]\r\nconfig = AlbertConfig(vocab_size_or_config_json_file=73000, hidden_size=4096,\r\n    num_hidden_layers=12, num_attention_heads=64, intermediate_size=16384, torchscript=True)\r\nmodel = AlbertModel(config)\r\nmodel.eval()\r\nmodel = AlbertModel.from_pretrained(\"albert-xxlarge-v2\", torchscript=True)\r\ntraced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\r\ntorch.jit.save(traced_model, \"albert-xxlarge-v2.pt\")\r\n```\r\nBut the second last line threw out a error:\r\n`RuntimeError: The size of tensor a (15) must match the size of tensor b (14) at non-singleton dimension 3`\r\n\r\nFrom the tutorial:\r\n```\r\nThe trace is created relatively to the inputs\u2019 dimensions. It is therefore constrained by the dimensions of the dummy input, and will not work for any other sequence length or batch size. When trying with a different size, an error such as:\r\n\r\nThe expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2\r\n```\r\n\r\nSo I tried changing `vocab_size_or_config_json_file` to a larger value, but still got the same error.\r\nAm I doing something wrong? Thanks for any advice.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6495", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6495/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6495/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6495/events", "html_url": "https://github.com/huggingface/transformers/issues/6495", "id": 679502893, "node_id": "MDU6SXNzdWU2Nzk1MDI4OTM=", "number": 6495, "title": "Model Upload does not show up `german-nlp-group/electra-base-german-uncased`", "user": {"login": "PhilipMay", "id": 229382, "node_id": "MDQ6VXNlcjIyOTM4Mg==", "avatar_url": "https://avatars3.githubusercontent.com/u/229382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilipMay", "html_url": "https://github.com/PhilipMay", "followers_url": "https://api.github.com/users/PhilipMay/followers", "following_url": "https://api.github.com/users/PhilipMay/following{/other_user}", "gists_url": "https://api.github.com/users/PhilipMay/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilipMay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilipMay/subscriptions", "organizations_url": "https://api.github.com/users/PhilipMay/orgs", "repos_url": "https://api.github.com/users/PhilipMay/repos", "events_url": "https://api.github.com/users/PhilipMay/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilipMay/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-08-15T05:11:55Z", "updated_at": "2020-08-17T19:49:11Z", "closed_at": "2020-08-17T19:49:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\nyesterday I uploaded a new model to `german-nlp-group/electra-base-german-uncased`:\r\n\r\n```bash\r\n$ transformers-cli s3 ls --organization german-nlp-group\r\nNeither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\r\nFilename                                          LastModified             ETag                               Size      \r\n------------------------------------------------- ------------------------ ---------------------------------- --------- \r\nelectra-base-german-uncased/config.json           2020-08-14T17:13:01.000Z \"10c75064301189f269b4898d4265cd61\"       467 \r\nelectra-base-german-uncased/pytorch_model.bin     2020-08-14T17:13:37.000Z \"a621e1cb07af0a08aaa643af52f9f189\" 444881731 \r\nelectra-base-german-uncased/tokenizer_config.json 2020-08-14T17:43:33.000Z \"7f6d7cb22bc6342b9c942da874754264\"        86 \r\nelectra-base-german-uncased/vocab.txt             2020-08-14T17:43:31.000Z \"e9fa1e40c556fc02c62ebaa214a52dc4\"    275501\r\n```\r\n\r\nBut it does not show up. See here: https://huggingface.co/german-nlp-group\r\n\r\nWhat happened here? Could you fix that?\r\n\r\nThanks\r\nPhilip", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6487", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6487/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6487/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6487/events", "html_url": "https://github.com/huggingface/transformers/issues/6487", "id": 679253414, "node_id": "MDU6SXNzdWU2NzkyNTM0MTQ=", "number": 6487, "title": "about encoder and decoder input when using seq2seq model", "user": {"login": "jungwhank", "id": 53588015, "node_id": "MDQ6VXNlcjUzNTg4MDE1", "avatar_url": "https://avatars1.githubusercontent.com/u/53588015?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jungwhank", "html_url": "https://github.com/jungwhank", "followers_url": "https://api.github.com/users/jungwhank/followers", "following_url": "https://api.github.com/users/jungwhank/following{/other_user}", "gists_url": "https://api.github.com/users/jungwhank/gists{/gist_id}", "starred_url": "https://api.github.com/users/jungwhank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jungwhank/subscriptions", "organizations_url": "https://api.github.com/users/jungwhank/orgs", "repos_url": "https://api.github.com/users/jungwhank/repos", "events_url": "https://api.github.com/users/jungwhank/events{/privacy}", "received_events_url": "https://api.github.com/users/jungwhank/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-08-14T16:01:47Z", "updated_at": "2020-08-18T13:30:55Z", "closed_at": "2020-08-18T13:17:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\nHello, I'm trying to using seq2seq model (such as bart and EncoderDecoderModel(bert2bert))\r\nAnd I'm little bit confused about input_ids, decoder_input_ids, tgt in model inputs.\r\n\r\nAs I know in seq2seq model, decoder_input should have special token(\\<s> or something) before the sentence and target should have special token(\\</s> or somethin) after the sentence. for example, `decoder_input = <s> A B C D E` , `target = A B C D E</s>`\r\n\r\nso my question is \r\n1. Should I put the these special tokens in decoder_inputs_ids and tgt_ids when using seq2seq model in this library?\r\nor can i just pass the decoder_input_ids and tgt_ids without any special token ids?\r\n\r\n2. Also, should I put `add_special_tokens=True` for encoder input_ids and put \\</s> or \\<eos> token after target ids?\r\nfor example, `input = a b c d e, decoder_input = <s>A B C D E, target = A B C D E</s>`\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6486", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6486/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6486/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6486/events", "html_url": "https://github.com/huggingface/transformers/issues/6486", "id": 679154589, "node_id": "MDU6SXNzdWU2NzkxNTQ1ODk=", "number": 6486, "title": "from_pretrained() never works", "user": {"login": "sadaszewski", "id": 1378525, "node_id": "MDQ6VXNlcjEzNzg1MjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1378525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sadaszewski", "html_url": "https://github.com/sadaszewski", "followers_url": "https://api.github.com/users/sadaszewski/followers", "following_url": "https://api.github.com/users/sadaszewski/following{/other_user}", "gists_url": "https://api.github.com/users/sadaszewski/gists{/gist_id}", "starred_url": "https://api.github.com/users/sadaszewski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sadaszewski/subscriptions", "organizations_url": "https://api.github.com/users/sadaszewski/orgs", "repos_url": "https://api.github.com/users/sadaszewski/repos", "events_url": "https://api.github.com/users/sadaszewski/events{/privacy}", "received_events_url": "https://api.github.com/users/sadaszewski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-08-14T13:23:55Z", "updated_at": "2020-08-14T18:15:47Z", "closed_at": "2020-08-14T18:15:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux\r\n- Python version: 3.6\r\n- PyTorch version (GPU?): 1.5.1 (yes)\r\n- Tensorflow version (GPU?): \r\n- Using GPU in script?: not relevant\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n### Who can help\r\n@LysandreJik , @TevenLeScao , @mfuntowicz \r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n -->\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): any\r\n\r\nThe problem arises when using:\r\n* [ ] the official example scripts: (give details below)\r\n* [X] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [X] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. `import transformers as pt`\r\n2. `pt.AutoModelForSequenceClassification.from_pretrained(<any_valid_model_id>)`\r\n3. Observe the error below\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n```python\r\n>>> pt.AutoModelForSequenceClassification.from_pretrained('xlnet-base-cased')\r\nI0814 15:00:47.832349 46912496391360 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /xxx/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6\r\nI0814 15:00:47.832984 46912496391360 configuration_utils.py:300] Model config XLNetConfig {\r\n  \"architectures\": [\r\n    \"XLNetLMHeadModel\"\r\n  ],\r\n  \"attn_type\": \"bi\",\r\n  \"bi_data\": false,\r\n  \"bos_token_id\": 1,\r\n  \"clamp_len\": -1,\r\n  \"d_head\": 64,\r\n  \"d_inner\": 3072,\r\n  \"d_model\": 768,\r\n  \"dropout\": 0.1,\r\n  \"end_n_top\": 5,\r\n  \"eos_token_id\": 2,\r\n  \"ff_activation\": \"gelu\",\r\n  \"initializer_range\": 0.02,\r\n  \"layer_norm_eps\": 1e-12,\r\n  \"mem_len\": null,\r\n  \"model_type\": \"xlnet\",\r\n  \"n_head\": 12,\r\n  \"n_layer\": 12,\r\n  \"pad_token_id\": 5,\r\n  \"reuse_len\": null,\r\n  \"same_length\": false,\r\n  \"start_n_top\": 5,\r\n  \"summary_activation\": \"tanh\",\r\n  \"summary_last_dropout\": 0.1,\r\n  \"summary_type\": \"last\",\r\n  \"summary_use_proj\": true,\r\n  \"task_specific_params\": {\r\n    \"text-generation\": {\r\n      \"do_sample\": true,\r\n      \"max_length\": 250\r\n    }\r\n  },\r\n  \"untie_r\": true,\r\n  \"vocab_size\": 32000\r\n}\r\n\r\nTraceback (most recent call last):\r\n  File \"/xxx/.conda/envs/xxx/lib/python3.6/site-packages/transformers/modeling_utils.py\", line 655, in from_pretrained\r\n    raise EnvironmentError\r\nOSError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/xxx/.conda/envs/xxx/lib/python3.6/site-packages/transformers/modeling_auto.py\", line 1363, in from_pretrained\r\n    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\r\n  File \"/xxx/.conda/envs/xxx/lib/python3.6/site-packages/transformers/modeling_utils.py\", line 662, in from_pretrained\r\n    raise EnvironmentError(msg)\r\nOSError: Can't load weights for 'xlnet-base-cased'. Make sure that:\r\n\r\n- 'xlnet-base-cased' is a correct model identifier listed on 'https://huggingface.co/models'\r\n\r\n- or 'xlnet-base-cased' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\nA pretrained model should be loaded. This worked (and still works) great in `pytorch_transformers`. I switched to `transformers` because XLNet-based models stopped working in `pytorch_transformers`. But surprise surprise in `transformers` no model whatsoever works for me.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6483", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6483/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6483/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6483/events", "html_url": "https://github.com/huggingface/transformers/issues/6483", "id": 679093615, "node_id": "MDU6SXNzdWU2NzkwOTM2MTU=", "number": 6483, "title": "Regarding GPU use for LM", "user": {"login": "shubhujf", "id": 10284584, "node_id": "MDQ6VXNlcjEwMjg0NTg0", "avatar_url": "https://avatars2.githubusercontent.com/u/10284584?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shubhujf", "html_url": "https://github.com/shubhujf", "followers_url": "https://api.github.com/users/shubhujf/followers", "following_url": "https://api.github.com/users/shubhujf/following{/other_user}", "gists_url": "https://api.github.com/users/shubhujf/gists{/gist_id}", "starred_url": "https://api.github.com/users/shubhujf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shubhujf/subscriptions", "organizations_url": "https://api.github.com/users/shubhujf/orgs", "repos_url": "https://api.github.com/users/shubhujf/repos", "events_url": "https://api.github.com/users/shubhujf/events{/privacy}", "received_events_url": "https://api.github.com/users/shubhujf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-14T11:26:31Z", "updated_at": "2020-08-14T14:01:18Z", "closed_at": "2020-08-14T14:01:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Hi,\r\nI am running example given in README.md of language_modeling using following command:\r\nexport TRAIN_FILE=/path/to/dataset/wiki.train.raw\r\nexport TEST_FILE=/path/to/dataset/wiki.test.raw\r\n\r\npython run_language_modeling.py \\\r\n    --output_dir=output \\\r\n    --model_type=gpt2 \\\r\n    --model_name_or_path=gpt2 \\\r\n    --do_train \\\r\n    --train_data_file=$TRAIN_FILE \\\r\n    --do_eval \\\r\n    --eval_data_file=$TEST_FILE\r\n\r\nIt has started training but it is not using GPU (TITAN X) at all, when I see throug nvidia-smi command \r\nI am new to this So Can you please let me know if I'm missing anything here.\r\n\r\nThanks.-->\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n**A link to original question on the forum/Stack Overflow**:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6481", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6481/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6481/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6481/events", "html_url": "https://github.com/huggingface/transformers/issues/6481", "id": 679071647, "node_id": "MDU6SXNzdWU2NzkwNzE2NDc=", "number": 6481, "title": "what's the difference between TFBertOutput and TFBertSelfOutput?", "user": {"login": "xiongma", "id": 30991932, "node_id": "MDQ6VXNlcjMwOTkxOTMy", "avatar_url": "https://avatars2.githubusercontent.com/u/30991932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiongma", "html_url": "https://github.com/xiongma", "followers_url": "https://api.github.com/users/xiongma/followers", "following_url": "https://api.github.com/users/xiongma/following{/other_user}", "gists_url": "https://api.github.com/users/xiongma/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiongma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiongma/subscriptions", "organizations_url": "https://api.github.com/users/xiongma/orgs", "repos_url": "https://api.github.com/users/xiongma/repos", "events_url": "https://api.github.com/users/xiongma/events{/privacy}", "received_events_url": "https://api.github.com/users/xiongma/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-14T10:43:23Z", "updated_at": "2020-08-18T21:10:16Z", "closed_at": "2020-08-18T21:10:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n\r\n`TFBertOutput` with `TFBertSelfOutput` seem same in their codes. why did you write two same layers? is there for some reasons?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6478", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6478/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6478/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6478/events", "html_url": "https://github.com/huggingface/transformers/issues/6478", "id": 679008435, "node_id": "MDU6SXNzdWU2NzkwMDg0MzU=", "number": 6478, "title": "Upladed model is not indexed", "user": {"login": "mrm8488", "id": 3653789, "node_id": "MDQ6VXNlcjM2NTM3ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3653789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrm8488", "html_url": "https://github.com/mrm8488", "followers_url": "https://api.github.com/users/mrm8488/followers", "following_url": "https://api.github.com/users/mrm8488/following{/other_user}", "gists_url": "https://api.github.com/users/mrm8488/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrm8488/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrm8488/subscriptions", "organizations_url": "https://api.github.com/users/mrm8488/orgs", "repos_url": "https://api.github.com/users/mrm8488/repos", "events_url": "https://api.github.com/users/mrm8488/events{/privacy}", "received_events_url": "https://api.github.com/users/mrm8488/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-14T08:52:26Z", "updated_at": "2020-08-17T19:51:45Z", "closed_at": "2020-08-17T19:51:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\nHi guys, I uploaded a model several hours ago (t5-base-finetuned-boolq) and it is not indexed in the model hub search engine yet!\r\n\r\nThanks, Manu", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6477", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6477/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6477/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6477/events", "html_url": "https://github.com/huggingface/transformers/issues/6477", "id": 678909675, "node_id": "MDU6SXNzdWU2Nzg5MDk2NzU=", "number": 6477, "title": "finetune.py: error: unrecognized arguments", "user": {"login": "KylePiira", "id": 17210104, "node_id": "MDQ6VXNlcjE3MjEwMTA0", "avatar_url": "https://avatars2.githubusercontent.com/u/17210104?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KylePiira", "html_url": "https://github.com/KylePiira", "followers_url": "https://api.github.com/users/KylePiira/followers", "following_url": "https://api.github.com/users/KylePiira/following{/other_user}", "gists_url": "https://api.github.com/users/KylePiira/gists{/gist_id}", "starred_url": "https://api.github.com/users/KylePiira/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KylePiira/subscriptions", "organizations_url": "https://api.github.com/users/KylePiira/orgs", "repos_url": "https://api.github.com/users/KylePiira/repos", "events_url": "https://api.github.com/users/KylePiira/events{/privacy}", "received_events_url": "https://api.github.com/users/KylePiira/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-14T05:30:39Z", "updated_at": "2020-08-16T17:36:39Z", "closed_at": "2020-08-16T17:36:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Who can help\r\n\r\n examples/distillation: @VictorSanh\r\n examples/seq2seq: @sshleifer\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...):\r\n\r\nThe problem arises when using:\r\n* [X] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [X] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Try running seq2seq/finetune.sh with data_dir or output_dir with escaped spaces in it\r\n2. You'll get a `finetune.py: error: unrecognized arguments`\r\n\r\nThis is bad because Google Drive mounts at `/content/drive/My Drive/` in Colab and thus the example scripts won't work if saving or reading from Drive.\r\n\r\nI've created a [Colab Notebook](https://colab.research.google.com/drive/1N-8m9FC9GbAywVJZAgSBkLqe24SPRfl8?usp=sharing) with repro.\r\n\r\nThe fix I've found is to change:\r\n```\r\npython finetune.py \\\r\n    --learning_rate=3e-5 \\\r\n    --fp16 \\\r\n    --gpus 1 \\\r\n    --do_train \\\r\n    --do_predict \\\r\n    --n_val 1000 \\\r\n    --val_check_interval 0.1 \\\r\n    $@\r\n```\r\nto \r\n```\r\npython finetune.py \\\r\n    --learning_rate=3e-5 \\\r\n    --fp16 \\\r\n    --gpus 1 \\\r\n    --do_train \\\r\n    --do_predict \\\r\n    --n_val 1000 \\\r\n    --val_check_interval 0.1 \\\r\n    \"$@\"\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6474", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6474/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6474/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6474/events", "html_url": "https://github.com/huggingface/transformers/issues/6474", "id": 678854141, "node_id": "MDU6SXNzdWU2Nzg4NTQxNDE=", "number": 6474, "title": "Training Data of  xlm-roberta-large-finetuned-conll03-* models", "user": {"login": "wangxinyu0922", "id": 17926734, "node_id": "MDQ6VXNlcjE3OTI2NzM0", "avatar_url": "https://avatars3.githubusercontent.com/u/17926734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangxinyu0922", "html_url": "https://github.com/wangxinyu0922", "followers_url": "https://api.github.com/users/wangxinyu0922/followers", "following_url": "https://api.github.com/users/wangxinyu0922/following{/other_user}", "gists_url": "https://api.github.com/users/wangxinyu0922/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangxinyu0922/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangxinyu0922/subscriptions", "organizations_url": "https://api.github.com/users/wangxinyu0922/orgs", "repos_url": "https://api.github.com/users/wangxinyu0922/repos", "events_url": "https://api.github.com/users/wangxinyu0922/events{/privacy}", "received_events_url": "https://api.github.com/users/wangxinyu0922/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "stefan-it", "id": 20651387, "node_id": "MDQ6VXNlcjIwNjUxMzg3", "avatar_url": "https://avatars1.githubusercontent.com/u/20651387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-it", "html_url": "https://github.com/stefan-it", "followers_url": "https://api.github.com/users/stefan-it/followers", "following_url": "https://api.github.com/users/stefan-it/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-it/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-it/subscriptions", "organizations_url": "https://api.github.com/users/stefan-it/orgs", "repos_url": "https://api.github.com/users/stefan-it/repos", "events_url": "https://api.github.com/users/stefan-it/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-it/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "stefan-it", "id": 20651387, "node_id": "MDQ6VXNlcjIwNjUxMzg3", "avatar_url": "https://avatars1.githubusercontent.com/u/20651387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-it", "html_url": "https://github.com/stefan-it", "followers_url": "https://api.github.com/users/stefan-it/followers", "following_url": "https://api.github.com/users/stefan-it/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-it/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-it/subscriptions", "organizations_url": "https://api.github.com/users/stefan-it/orgs", "repos_url": "https://api.github.com/users/stefan-it/repos", "events_url": "https://api.github.com/users/stefan-it/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-it/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-08-14T02:36:51Z", "updated_at": "2020-08-16T02:53:45Z", "closed_at": "2020-08-16T02:53:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I'm curious about the training data of xlm-r models finetuned on conll ner datasets (e.g.  xlm-roberta-large-finetuned-conll03-german,  xlm-roberta-large-finetuned-conll03-english), are the models trained on train+dev sets?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6472", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6472/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6472/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6472/events", "html_url": "https://github.com/huggingface/transformers/issues/6472", "id": 678839647, "node_id": "MDU6SXNzdWU2Nzg4Mzk2NDc=", "number": 6472, "title": "\"BertEncoder' object has no attribute 'output_hidden_states\"", "user": {"login": "thanish", "id": 4056145, "node_id": "MDQ6VXNlcjQwNTYxNDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4056145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thanish", "html_url": "https://github.com/thanish", "followers_url": "https://api.github.com/users/thanish/followers", "following_url": "https://api.github.com/users/thanish/following{/other_user}", "gists_url": "https://api.github.com/users/thanish/gists{/gist_id}", "starred_url": "https://api.github.com/users/thanish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thanish/subscriptions", "organizations_url": "https://api.github.com/users/thanish/orgs", "repos_url": "https://api.github.com/users/thanish/repos", "events_url": "https://api.github.com/users/thanish/events{/privacy}", "received_events_url": "https://api.github.com/users/thanish/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-08-14T01:50:38Z", "updated_at": "2020-08-20T17:43:41Z", "closed_at": "2020-08-20T17:43:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi I have trained a Bert token classification model for the Italian language using the \"dbmdz/bert-base-italian-uncased\". I have trained the model in a machine running Pytorch-1.4.0 and transformer 3.0.2, when I installed it few days back as it's the latest version.\r\nI copied the saved best model to a server that runs Pytorch-1.4.0 & transformer version 2.3.0. I sent a request to the model to get the predictions, but I got the following warnings.\r\n\r\n\r\n# Inference code\r\n```\r\ntokenizer = transformers.BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-uncased\", do_lower_case=False)\r\n\r\nAssuming I have tokenized the requested text into the variable \"tokens\"\r\n\r\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\r\nsegments_ids = [0] * len(tokens)\r\ntokens_tensor = torch.tensor([indexed_tokens]).to(device)\r\nsegments_tensors = torch.tensor([segments_ids]).to(device)\r\nlogit = model(tokens_tensor, token_type_ids=None, attention_mask=segments_tensors)\r\n```\r\n\r\n# Warnings\r\n```\r\nModel name 'dbmdz/bert-base-italian-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'dbmdz/bert-base-italian-uncased' is a path or url to a directory containing tokenizer files.\r\nDidn't find file dbmdz/bert-base-italian-uncased/added_tokens.json. We won't load it.\r\nDidn't find file dbmdz/bert-base-italian-uncased/special_tokens_map.json. We won't load it.\r\nDidn't find file dbmdz/bert-base-italian-uncased/tokenizer_config.json. We won't load it.\r\nloading file https://s3.amazonaws.com/models.huggingface.co/bert/dbmdz/bert-base-italian-uncased/vocab.txt from cache at /root/.cache/torch/transformers/02b5ab8ef6a3a1d4af18c318bb4c53155a59a3893dd557b922d2467b269cd405.5cbaac66fdfadbe363aad01956dac0be9bf700f2c8c87012dc078b87e2fa4181\r\nloading file None\r\nloading file None\r\nloading file None\r\n```\r\n```\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertForTokenClassification' has changed. Saved a reverse patch to BertForTokenClassification.patch. Run `patch -p0 < BertForTokenClassification.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertModel' has changed. Saved a reverse patch to BertModel.patch. Run `patch -p0 < BertModel.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertEmbeddings' has changed. Saved a reverse patch to BertEmbeddings.patch. Run `patch -p0 < BertEmbeddings.patch` to revert your changes.  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.normalization.LayerNorm' has changed. Saved a reverse patch to LayerNorm.patch. Run `patch -p0 < LayerNorm.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertEncoder' has changed. Saved a reverse patch to BertEncoder.patch. Run `patch -p0 < BertEncoder.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. Saved a reverse patch to ModuleList.patch. Run `patch -p0 < ModuleList.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertLayer' has changed. Saved a reverse patch to BertLayer.patch. Run `patch -p0 < BertLayer.patch` to revert your changes.  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertAttention' has changed. Saved a reverse patch to BertAttention.patch. Run `patch -p0 < BertAttention.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertSelfAttention' has changed. Saved a reverse patch to BertSelfAttention.patch. Run `patch -p0 < BertSelfAttention.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. Saved a reverse patch to Linear.patch. Run `patch -p0 < Linear.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertSelfOutput' has changed. Saved a reverse patch to BertSelfOutput.patch. Run `patch -p0 < BertSelfOutput.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertIntermediate' has changed. Saved a reverse patch to BertIntermediate.patch. Run `patch -p0 < BertIntermediate.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertOutput' has changed. Saved a reverse patch to BertOutput.patch. Run `patch -p0 < BertOutput.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertPooler' has changed. Saved a reverse patch to BertPooler.patch. Run `patch -p0 < BertPooler.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. Saved a reverse patch to Tanh.patch. Run `patch -p0 < Tanh.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n```\r\nand finally it ended with the below error.\r\n\r\n```\r\n\"BertEncoder' object has no attribute 'output_hidden_states\".\r\n```\r\nCan someone help me understand Is it because of the Pytorch, transformer version mismatch between the trained model on a machine and the inference on the server? or if \"dbmdz/bert-base-italian-uncased\" is available in the 2.3.0 version or not? or is there any other way I can make this work instead of retraining the model at a lower version to match the version of the server?\r\nAssuming that changing the versions in the server is not quite possible as of now.\r\nAppreciate your help.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6468", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6468/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6468/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6468/events", "html_url": "https://github.com/huggingface/transformers/issues/6468", "id": 678647132, "node_id": "MDU6SXNzdWU2Nzg2NDcxMzI=", "number": 6468, "title": "convert_graph_to_onnx not working as expected.", "user": {"login": "Zhen-hao", "id": 10957195, "node_id": "MDQ6VXNlcjEwOTU3MTk1", "avatar_url": "https://avatars3.githubusercontent.com/u/10957195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zhen-hao", "html_url": "https://github.com/Zhen-hao", "followers_url": "https://api.github.com/users/Zhen-hao/followers", "following_url": "https://api.github.com/users/Zhen-hao/following{/other_user}", "gists_url": "https://api.github.com/users/Zhen-hao/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zhen-hao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zhen-hao/subscriptions", "organizations_url": "https://api.github.com/users/Zhen-hao/orgs", "repos_url": "https://api.github.com/users/Zhen-hao/repos", "events_url": "https://api.github.com/users/Zhen-hao/events{/privacy}", "received_events_url": "https://api.github.com/users/Zhen-hao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-13T18:25:00Z", "updated_at": "2020-08-15T17:52:52Z", "closed_at": "2020-08-15T17:52:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\nnot sure if there is a bug. \r\n\r\nwhen running \r\n```python\r\nfrom transformers.convert_graph_to_onnx import convert\r\nconvert(framework=\"tf\", model = my_fine_tuned_bert_model, output=\"onnx-fine-tuned/model.onnx\", opset=11, tokenizer=tokenizer)\r\n```\r\nI got the following log/output\r\n```\r\nONNX opset version set to: 11\r\nLoading pipeline (model: <__main__.TFBertForMultiClassification object at 0x7f2c37ba9b50>, tokenizer: <transformers.tokenization_bert.BertTokenizerFast object at 0x7f2c37ba9ad0>)\r\nCreating folder onnx-fine-tuned\r\n/!\\ Please note TensorFlow doesn't support exporting model > 2Gb /!\\\r\nUsing framework TensorFlow: 2.1.0, keras2onnx: 1.7.0\r\nFound input input_ids with shape: {0: 'batch', 1: 'sequence'}\r\nFound input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\r\nFound input attention_mask with shape: {0: 'batch', 1: 'sequence'}\r\nFound output output_0 with shape: {0: 'batch'}\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertForMultiClassification.call of <__main__.TFBertForMultiClassification object at 0x7f2c37ba9b50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Num'\r\nWARNING: AutoGraph could not transform <bound method TFBertForMultiClassification.call of <__main__.TFBertForMultiClassification object at 0x7f2c37ba9b50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.modeling_tf_bert.TFBertMainLayer object at 0x7f2c3dc34910>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Num'\r\nWARNING: AutoGraph could not transform <bound method TFBertMainLayer.call of <transformers.modeling_tf_bert.TFBertMainLayer object at 0x7f2c3dc34910>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c371ffe90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c371ffe90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c3769bb50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c3769bb50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c3769bed0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c3769bed0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37682e10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37682e10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c8ca90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c8ca90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c9b050>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c9b050>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37ca6c90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37ca6c90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37cae910>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37cae910>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37caee90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37caee90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c3783eb10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c3783eb10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37834790>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37834790>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37834d10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37834d10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c3781e990>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c3781e990>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37814610>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37814610>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37814b90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37814b90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37778850>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37778850>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c377704d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c377704d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37770a50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37770a50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c3775b6d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c3775b6d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37751350>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37751350>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c377518d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c377518d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c42550>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c42550>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c4b1d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c4b1d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c4b750>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c4b750>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c5f3d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c5f3d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c5ffd0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c5ffd0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c685d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c685d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c7c210>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c7c210>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c7ce50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c7ce50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c07410>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c07410>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c1a0d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c1a0d0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c1acd0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c1acd0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c24290>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37c24290>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c2ded0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertSelfOutput.call of <transformers.modeling_tf_bert.TFBertSelfOutput object at 0x7f2c37c2ded0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c35b50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertIntermediate.call of <transformers.modeling_tf_bert.TFBertIntermediate object at 0x7f2c37c35b50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37bc3110>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertOutput.call of <transformers.modeling_tf_bert.TFBertOutput object at 0x7f2c37bc3110>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:AutoGraph could not transform <bound method TFBertPooler.call of <transformers.modeling_tf_bert.TFBertPooler object at 0x7f2c37bc3cd0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\nWARNING: AutoGraph could not transform <bound method TFBertPooler.call of <transformers.modeling_tf_bert.TFBertPooler object at 0x7f2c37bc3cd0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Bad argument number for Name: 3, expecting 4\r\ntf executing eager_mode: True\r\ntf.keras model eager_mode: False\r\nThe ONNX operator number change on the optimization: 2579 -> 1674\r\n```\r\n\r\nshould I ignore the warning?\r\n\r\nThe shape of the exported onnx model is\r\n```\r\ngraph_name: tf_bert_for_multi_classification\r\ndomain: onnxmltools\r\ndescription: \r\ninput 0: \"attention_mask\" [\"N\", 7] Int32\r\ninput 1: \"input_ids\" [\"N\", 7] Int32\r\ninput 2: \"token_type_ids\" [\"N\", 7] Int32\r\noutput 0: \"output_1\" [\"N\", 4404, 1] Float\r\n```\r\n\r\nI don't think that's correct. where are \"N\" and 7 from?\r\nwhen I try to run the model on input \r\n```\r\n{'input_ids': array([ 101,  146, 1169, 1631, 1103, 3974,  117, 1169, 1128,  136,  102]),\r\n 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\r\n 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\r\n```\r\nI got error\r\n```\r\n>>> results = session.run(None, inputs_onnx)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/nix/store/xws61xnjc03fjiwfh7ci5cwgg1chmp3l-python3.7-onnxruntime-1.4.0/lib/python3.7/site-packages/onnxruntime/capi/session.py\", line 110, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (N11onnxruntime17PrimitiveDataTypeIlEE) , expected: (N11onnxruntime17PrimitiveDataTypeIiEE)\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6460", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6460/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6460/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6460/events", "html_url": "https://github.com/huggingface/transformers/issues/6460", "id": 678405042, "node_id": "MDU6SXNzdWU2Nzg0MDUwNDI=", "number": 6460, "title": "Hashing a tokenizer using the \ud83e\udd17 nlp lib is not deterministic", "user": {"login": "lhoestq", "id": 42851186, "node_id": "MDQ6VXNlcjQyODUxMTg2", "avatar_url": "https://avatars2.githubusercontent.com/u/42851186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhoestq", "html_url": "https://github.com/lhoestq", "followers_url": "https://api.github.com/users/lhoestq/followers", "following_url": "https://api.github.com/users/lhoestq/following{/other_user}", "gists_url": "https://api.github.com/users/lhoestq/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhoestq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhoestq/subscriptions", "organizations_url": "https://api.github.com/users/lhoestq/orgs", "repos_url": "https://api.github.com/users/lhoestq/repos", "events_url": "https://api.github.com/users/lhoestq/events{/privacy}", "received_events_url": "https://api.github.com/users/lhoestq/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "LysandreJik", "id": 30755778, "node_id": "MDQ6VXNlcjMwNzU1Nzc4", "avatar_url": "https://avatars1.githubusercontent.com/u/30755778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LysandreJik", "html_url": "https://github.com/LysandreJik", "followers_url": "https://api.github.com/users/LysandreJik/followers", "following_url": "https://api.github.com/users/LysandreJik/following{/other_user}", "gists_url": "https://api.github.com/users/LysandreJik/gists{/gist_id}", "starred_url": "https://api.github.com/users/LysandreJik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LysandreJik/subscriptions", "organizations_url": "https://api.github.com/users/LysandreJik/orgs", "repos_url": "https://api.github.com/users/LysandreJik/repos", "events_url": "https://api.github.com/users/LysandreJik/events{/privacy}", "received_events_url": "https://api.github.com/users/LysandreJik/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "LysandreJik", "id": 30755778, "node_id": "MDQ6VXNlcjMwNzU1Nzc4", "avatar_url": "https://avatars1.githubusercontent.com/u/30755778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LysandreJik", "html_url": "https://github.com/LysandreJik", "followers_url": "https://api.github.com/users/LysandreJik/followers", "following_url": "https://api.github.com/users/LysandreJik/following{/other_user}", "gists_url": "https://api.github.com/users/LysandreJik/gists{/gist_id}", "starred_url": "https://api.github.com/users/LysandreJik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LysandreJik/subscriptions", "organizations_url": "https://api.github.com/users/LysandreJik/orgs", "repos_url": "https://api.github.com/users/LysandreJik/repos", "events_url": "https://api.github.com/users/LysandreJik/events{/privacy}", "received_events_url": "https://api.github.com/users/LysandreJik/received_events", "type": "User", "site_admin": false}, {"login": "sgugger", "id": 35901082, "node_id": "MDQ6VXNlcjM1OTAxMDgy", "avatar_url": "https://avatars3.githubusercontent.com/u/35901082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgugger", "html_url": "https://github.com/sgugger", "followers_url": "https://api.github.com/users/sgugger/followers", "following_url": "https://api.github.com/users/sgugger/following{/other_user}", "gists_url": "https://api.github.com/users/sgugger/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgugger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgugger/subscriptions", "organizations_url": "https://api.github.com/users/sgugger/orgs", "repos_url": "https://api.github.com/users/sgugger/repos", "events_url": "https://api.github.com/users/sgugger/events{/privacy}", "received_events_url": "https://api.github.com/users/sgugger/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-08-13T12:39:35Z", "updated_at": "2020-08-14T08:36:58Z", "closed_at": "2020-08-14T08:36:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In the `nlp` library it is common to use a tokenizer on a dataset.\r\nThe library takes care of caching the results, so that if you run the tokenization twice, it will reuse the previous results.\r\nTo make the caching work, we compute a hash of the tokenizer.\r\n\r\nHowever the `unique_no_split_tokens` attribute of tokenizers is not deterministic, and it makes the hashing return different hashes for the same tokenizer over different sessions.\r\n\r\n`unique_no_split_tokens` can be a list like `['[CLS]', '[MASK]', '[PAD]', '[SEP]', '[UNK]']` for example. But it happens that re-loading a tokenizer in another session shuffles the tokens in the list.\r\n\r\nFor example this code doesn't always return the same output over different sessions:\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\nprint(tokenizer.unique_no_split_tokens)\r\n```\r\n\r\nReproduce on google colab: https://colab.research.google.com/drive/1nyskaLavcTCkXibZBlYX71bkG476uSzz?usp=sharing", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6459", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6459/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6459/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6459/events", "html_url": "https://github.com/huggingface/transformers/issues/6459", "id": 678340367, "node_id": "MDU6SXNzdWU2NzgzNDAzNjc=", "number": 6459, "title": "Autotokenizer not returning instance of LongformerTokenizerFast", "user": {"login": "pratikdk", "id": 20542313, "node_id": "MDQ6VXNlcjIwNTQyMzEz", "avatar_url": "https://avatars2.githubusercontent.com/u/20542313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pratikdk", "html_url": "https://github.com/pratikdk", "followers_url": "https://api.github.com/users/pratikdk/followers", "following_url": "https://api.github.com/users/pratikdk/following{/other_user}", "gists_url": "https://api.github.com/users/pratikdk/gists{/gist_id}", "starred_url": "https://api.github.com/users/pratikdk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pratikdk/subscriptions", "organizations_url": "https://api.github.com/users/pratikdk/orgs", "repos_url": "https://api.github.com/users/pratikdk/repos", "events_url": "https://api.github.com/users/pratikdk/events{/privacy}", "received_events_url": "https://api.github.com/users/pratikdk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-13T10:46:03Z", "updated_at": "2020-08-13T16:06:44Z", "closed_at": "2020-08-13T16:06:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.6.0+cu101 (False)\r\n- Tensorflow version (GPU?): 2.3.0 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: Google Colab\r\n\r\n## Information\r\n\r\nModel I am using :  Longformer \r\nPath: **'allenai/longformer-base-4096'** and **'allenai/longformer-large-4096'**\r\n\r\nThe problem arises when trying to load 'Fast' version for Longformer using Autotokenizer, the returned tokenizer instance is an object of LongformerTokenizer and not LongformerTokenizerFast. \r\n\r\n![Annotation 2020-08-13 160156](https://user-images.githubusercontent.com/20542313/90124665-68ace300-dd7e-11ea-8cc7-fdd80f070bec.png)\r\n\r\nI require the offset mappings for a sub task of extracting word embeddings.\r\n\r\n## To reproduce\r\nJust as in the screenshot i am adding the code below to instantiate the tokenizer object:\r\n```\r\nlongformer_tokenizer = AutoTokenizer.from_pretrained(\r\n    pretrained_model_name_or_path = 'allenai/longformer-base-4096', use_fast=True)\r\nprint(longformer_tokenizer.is_fast)\r\nprint(longformer_tokenizer)\r\n```\r\nAnd since its not an instance of  transformers.LongformerTokenizerFast, I cannot `return_offsets_mapping=True`\r\nAs in the below code throws `NotImplementedError`\r\n\r\n```\r\nlongformer_encoded_dict = longformer_tokenizer.encode_plus(text=sequence_3,\r\n                                                    add_special_tokens = True,\r\n                                                    max_length = 75,\r\n                                                    truncation = True,\r\n                                                    pad_to_max_length = False,\r\n                                                    return_token_type_ids = False,\r\n                                                    return_attention_mask = True,\r\n                                                    return_overflowing_tokens = False,\r\n                                                    return_special_tokens_mask = False,\r\n                                                    return_offsets_mapping=True)\r\n```\r\n**Error**\r\n`\r\nNotImplementedError: return_offsets_mapping is not available when using Python tokenizers.To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.`\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n\r\n@mfuntowicz\r\n@patrickvonplaten\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6458", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6458/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6458/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6458/events", "html_url": "https://github.com/huggingface/transformers/issues/6458", "id": 678318571, "node_id": "MDU6SXNzdWU2NzgzMTg1NzE=", "number": 6458, "title": "Unknown task zero-shot-classification", "user": {"login": "amarlearning", "id": 9383897, "node_id": "MDQ6VXNlcjkzODM4OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/9383897?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amarlearning", "html_url": "https://github.com/amarlearning", "followers_url": "https://api.github.com/users/amarlearning/followers", "following_url": "https://api.github.com/users/amarlearning/following{/other_user}", "gists_url": "https://api.github.com/users/amarlearning/gists{/gist_id}", "starred_url": "https://api.github.com/users/amarlearning/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amarlearning/subscriptions", "organizations_url": "https://api.github.com/users/amarlearning/orgs", "repos_url": "https://api.github.com/users/amarlearning/repos", "events_url": "https://api.github.com/users/amarlearning/events{/privacy}", "received_events_url": "https://api.github.com/users/amarlearning/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-13T10:08:55Z", "updated_at": "2020-08-18T16:46:08Z", "closed_at": "2020-08-13T10:39:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Ubuntu 18\r\n- Python version: 3.7\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n -->\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...):\r\n\r\nThe problem arises when using:\r\n* [ ] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [ ] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. I downloaded transformer version 3.0.2\r\n2. From transformer, I imported pipeline\r\n3. And from the pipeline, I was trying to load this task `zero-shot-classification` and then I got the error.\r\n\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-12-1f0825594ce1> in <module>\r\n----> 1 classifier = pipeline(\"zero-shot-classification\")\r\n\r\n~/anaconda3/lib/python3.7/site-packages/transformers/pipelines.py in pipeline(task, model, config, tokenizer, framework, **kwargs)\r\n   1819     # Retrieve the task\r\n   1820     if task not in SUPPORTED_TASKS:\r\n-> 1821         raise KeyError(\"Unknown task {}, available tasks are {}\".format(task, list(SUPPORTED_TASKS.keys())))\r\n   1822 \r\n   1823     framework = framework or get_framework(model)\r\n\r\nKeyError: \"Unknown task zero-shot-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'fill-mask', 'summarization', 'translation_en_to_fr', 'translation_en_to_de', 'translation_en_to_ro', 'text-generation']\"\r\n```\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6454", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6454/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6454/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6454/events", "html_url": "https://github.com/huggingface/transformers/issues/6454", "id": 678222948, "node_id": "MDU6SXNzdWU2NzgyMjI5NDg=", "number": 6454, "title": "Memory Issue while following LM tutorial", "user": {"login": "raceee", "id": 43013378, "node_id": "MDQ6VXNlcjQzMDEzMzc4", "avatar_url": "https://avatars2.githubusercontent.com/u/43013378?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raceee", "html_url": "https://github.com/raceee", "followers_url": "https://api.github.com/users/raceee/followers", "following_url": "https://api.github.com/users/raceee/following{/other_user}", "gists_url": "https://api.github.com/users/raceee/gists{/gist_id}", "starred_url": "https://api.github.com/users/raceee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raceee/subscriptions", "organizations_url": "https://api.github.com/users/raceee/orgs", "repos_url": "https://api.github.com/users/raceee/repos", "events_url": "https://api.github.com/users/raceee/events{/privacy}", "received_events_url": "https://api.github.com/users/raceee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-13T07:37:21Z", "updated_at": "2020-08-13T21:31:04Z", "closed_at": "2020-08-13T21:23:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "(Didn't get answer)\r\nhttps://stackoverflow.com/questions/63387831/memory-issue-while-following-lm-tutorial\r\n\r\nSPECS:\r\ntorch==1.5.0\r\ntransformers==3.0.2\r\nOS: Windows 10\r\nCUDA: 10.1\r\nGPU: RTX 2060 6G VRAM (x2)\r\nRAM: 32GB\r\ntutorial: https://huggingface.co/blog/how-to-train\r\n\r\n\r\nHello I am trying to train my own language model and I have had some memory issues. I have tried to run some of this code in Pycharm on my computer and then trying to replicate in my Collab Pro Notebook.\r\n\r\n## First, my code\r\n```\r\nfrom transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM, LineByLineTextDataset\r\nfrom transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\r\n\r\nconfig = RobertaConfig(vocab_size=60000, max_position_embeddings=514, num_attention_heads=12, num_hidden_layers=6,\r\n                       type_vocab_size=1)\r\n\r\ntokenizer = RobertaTokenizerFast.from_pretrained(\"./MODEL DIRECTORY\", max_len=512)\r\n\r\nmodel = RobertaForMaskedLM(config=config)\r\n\r\nprint(\"making dataset\")\r\n\r\ndataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=\"./total_text.txt\", block_size=128)\r\n\r\nprint(\"making c\")\r\n\r\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r\n\r\ntraining_args = TrainingArguments(output_dir=\"./MODEL DIRECTORY\", overwrite_output_dir=True, num_train_epochs=1,\r\n                                  per_gpu_train_batch_size=64, save_steps=10000, save_total_limit=2)\r\nprint(\"Building trainer\")\r\ntrainer = Trainer(model=model, args=training_args, data_collator=data_collator, train_dataset=dataset,\r\n                  prediction_loss_only=True)\r\ntrainer.train()\r\n\r\ntrainer.save_model(\"./MODEL DIRECTORY\")\r\n\r\n```\r\n`\"./total_text.txt\"` being a 1.7GB text file.\r\n\r\n## PyCharm Attempt\r\n\r\nThis code on pycharm builds the dataset and then would throw an error saying that my preferred gpu was running out of memory, and that Torch was already using 3.7GiB of memory.\r\n\r\nI tried:\r\n* import gc doing a gc clear to try to flush what ever was going on my gpu\r\n* Decreasing my batch size for my gpu (training only happened on a batch size of 8 resulting in 200,000+ epochs that all took 1.17 seconds)\r\n* Setting my `os.environ[\"CUDA_VISIBLE_OBJECTS\"] =\"\"` so that torch would have to use my CPU and not my GPU. Still threw same gpu memory error...\r\n\r\nSo succumbing to the fact that torch, for the time being, was forcing itself to use my gpu, I decided to go to Collab.\r\n\r\n## Collab Attempt\r\n\r\nCollab has different issues with my code. It does not have the memory to build the dataset, and crashes due to RAM shortages. I purchased a Pro account and then increased the usable RAM to 25GB, still memory shortages.\r\n\r\nCheers!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6450", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6450/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6450/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6450/events", "html_url": "https://github.com/huggingface/transformers/issues/6450", "id": 678065558, "node_id": "MDU6SXNzdWU2NzgwNjU1NTg=", "number": 6450, "title": "Error in PyTorch Trainer when used with TPU", "user": {"login": "M-Salti", "id": 9285264, "node_id": "MDQ6VXNlcjkyODUyNjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9285264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/M-Salti", "html_url": "https://github.com/M-Salti", "followers_url": "https://api.github.com/users/M-Salti/followers", "following_url": "https://api.github.com/users/M-Salti/following{/other_user}", "gists_url": "https://api.github.com/users/M-Salti/gists{/gist_id}", "starred_url": "https://api.github.com/users/M-Salti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/M-Salti/subscriptions", "organizations_url": "https://api.github.com/users/M-Salti/orgs", "repos_url": "https://api.github.com/users/M-Salti/repos", "events_url": "https://api.github.com/users/M-Salti/events{/privacy}", "received_events_url": "https://api.github.com/users/M-Salti/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-13T00:46:35Z", "updated_at": "2020-08-21T12:35:05Z", "closed_at": "2020-08-21T12:35:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.5.0a0+d6149a7 (False)\r\n- Tensorflow version (GPU?): 2.3.0 (False)\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n --> @sgugger\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): BERT\r\n\r\nThe problem arises when using:\r\n* [ ] the official example scripts: (give details below)\r\n* [x] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [x] an official GLUE/SQUaD task: SQUaD\r\n* [ ] my own task or dataset: (give details below)\r\n\r\nThe following error arises when using the `run_squad_trainer.py` script with TPU:\r\n```python\r\nEpoch:   0% 0/2 [00:00<?, ?it/s]\r\nIteration: 0it [00:00, ?it/s]Exception in device=TPU:0: 'NoneType' object cannot be interpreted as an integer\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 119, in _start_fn\r\n    fn(gindex, *args)\r\n  File \"/content/transformers/examples/question-answering/run_squad_trainer.py\", line 156, in _mp_fn\r\n    main()\r\n  File \"/content/transformers/examples/question-answering/run_squad_trainer.py\", line 145, in main\r\n    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 584, in train\r\n    self.epoch = epoch + (step + 1) / len(epoch_iterator)\r\nTypeError: 'NoneType' object cannot be interpreted as an integer\r\n```\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. install transformers from the master branch\r\n2. install pytorch-xla using the following command:\r\n```shell\r\nVERSION = \"20200325\"\r\ncurl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\r\npython pytorch-xla-env-setup.py --version $VERSION\r\n```\r\n3. run the training script (I'm using 1 tpu core merely to simplify the logs. The error is the same (for each core) when using 8 cores):\r\n```shell\r\ncd transformers/examples/\r\n\r\npython ./xla_spawn.py --num_cores 1 \\\r\nquestion-answering/run_squad_trainer.py \\\r\n    --model_name_or_path bert-base-multilingual-cased \\\r\n    --model_type bert \\\r\n    --data_dir $DATA_DIR \\\r\n    --do_train \\\r\n    --per_device_train_batch_size 64 \\\r\n    --learning_rate 3e-5 \\\r\n    --num_train_epochs 2.0 \\\r\n    --max_seq_length 384 \\\r\n    --doc_stride 128 \\\r\n    --output_dir $OUT_DIR \\\r\n    --overwrite_output_dir\r\n```\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\nThe script runs and trains the model\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6444", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6444/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6444/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6444/events", "html_url": "https://github.com/huggingface/transformers/issues/6444", "id": 677793073, "node_id": "MDU6SXNzdWU2Nzc3OTMwNzM=", "number": 6444, "title": "Can't download  'Helsinki-NLP/opus-mt-hye-eng' model", "user": {"login": "sonja-lo", "id": 58326920, "node_id": "MDQ6VXNlcjU4MzI2OTIw", "avatar_url": "https://avatars3.githubusercontent.com/u/58326920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sonja-lo", "html_url": "https://github.com/sonja-lo", "followers_url": "https://api.github.com/users/sonja-lo/followers", "following_url": "https://api.github.com/users/sonja-lo/following{/other_user}", "gists_url": "https://api.github.com/users/sonja-lo/gists{/gist_id}", "starred_url": "https://api.github.com/users/sonja-lo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sonja-lo/subscriptions", "organizations_url": "https://api.github.com/users/sonja-lo/orgs", "repos_url": "https://api.github.com/users/sonja-lo/repos", "events_url": "https://api.github.com/users/sonja-lo/events{/privacy}", "received_events_url": "https://api.github.com/users/sonja-lo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2039044877, "node_id": "MDU6TGFiZWwyMDM5MDQ0ODc3", "url": "https://api.github.com/repos/huggingface/transformers/labels/marian", "name": "marian", "color": "30cc95", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-08-12T15:40:22Z", "updated_at": "2020-08-13T09:44:09Z", "closed_at": "2020-08-13T09:44:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-5.3.0-51-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.3.1 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: NO\r\n- Using distributed or parallel set-up in script?: not sure\r\n\r\n\r\n\r\n## Information\r\n\r\nModel I am using: MarianMTModel, AutoModelWithLMHead\r\n\r\nThe problem arises when using the official example scripts (https://huggingface.co/Helsinki-NLP/opus-mt-hye-eng):\r\n\r\n```python \r\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\r\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-hye-eng\")\r\nmodel = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-hye-eng\")\r\n```\r\n\r\nGives error \r\n\r\n```\r\n/home/sonja/.local/lib/python3.6/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\r\n  FutureWarning,\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n~/.local/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    654                 if resolved_archive_file is None:\r\n--> 655                     raise EnvironmentError\r\n    656             except EnvironmentError:\r\n\r\nOSError: \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-2-04055899a280> in <module>\r\n      1 from transformers import AutoTokenizer, AutoModelWithLMHead\r\n      2 tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-hye-eng\")\r\n----> 3 model = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-hye-eng\")\r\n\r\n~/.local/lib/python3.6/site-packages/transformers/modeling_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    804         for config_class, model_class in MODEL_WITH_LM_HEAD_MAPPING.items():\r\n    805             if isinstance(config, config_class):\r\n--> 806                 return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\r\n    807         raise ValueError(\r\n    808             \"Unrecognized configuration class {} for this kind of AutoModel: {}.\\n\"\r\n\r\n~/.local/lib/python3.6/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\n    660                     f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {WEIGHTS_NAME}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}.\\n\\n\"\r\n    661                 )\r\n--> 662                 raise EnvironmentError(msg)\r\n    663 \r\n    664             if resolved_archive_file == archive_file:\r\n\r\nOSError: Can't load weights for 'Helsinki-NLP/opus-mt-hye-eng'. Make sure that:\r\n\r\n- 'Helsinki-NLP/opus-mt-hye-eng' is a correct model identifier listed on 'https://huggingface.co/models'\r\n\r\n- or 'Helsinki-NLP/opus-mt-hye-eng' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.\r\n```\r\n\r\nTried to download the model manually from link I got while debugging (https://cdn.huggingface.co/Helsinki-NLP/opus-mt-hye-eng/pytorch_model.bin) but it doesn't return anything relatable. Although for 'hye-rus' model (https://cdn.huggingface.co/Helsinki-NLP/opus-mt-hye-rus/pytorch_model.bin) I can easily download the file. Works fine for \"eng-hye\" and \"rus-hye\" too.\r\n\r\nHj\u00e4lp, @sshleifer (sorry if mistagged)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6443", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6443/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6443/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6443/events", "html_url": "https://github.com/huggingface/transformers/issues/6443", "id": 677784457, "node_id": "MDU6SXNzdWU2Nzc3ODQ0NTc=", "number": 6443, "title": "Simple train from the start for translation transformer", "user": {"login": "felipeboffnunes", "id": 51033921, "node_id": "MDQ6VXNlcjUxMDMzOTIx", "avatar_url": "https://avatars2.githubusercontent.com/u/51033921?v=4", "gravatar_id": "", "url": "https://api.github.com/users/felipeboffnunes", "html_url": "https://github.com/felipeboffnunes", "followers_url": "https://api.github.com/users/felipeboffnunes/followers", "following_url": "https://api.github.com/users/felipeboffnunes/following{/other_user}", "gists_url": "https://api.github.com/users/felipeboffnunes/gists{/gist_id}", "starred_url": "https://api.github.com/users/felipeboffnunes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/felipeboffnunes/subscriptions", "organizations_url": "https://api.github.com/users/felipeboffnunes/orgs", "repos_url": "https://api.github.com/users/felipeboffnunes/repos", "events_url": "https://api.github.com/users/felipeboffnunes/events{/privacy}", "received_events_url": "https://api.github.com/users/felipeboffnunes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-12T15:28:31Z", "updated_at": "2020-08-13T14:13:25Z", "closed_at": "2020-08-13T14:13:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, sorry to bother. I am trying to train a translation transformer, I have seen the documentation but I am still really lost.\r\n\r\nI have two datasets, the original message and the translated message.\r\n\r\nExample:\r\ndataset_x.txt\r\nThis is the message.\r\nThis is another message.\r\nAnother one.\r\n\r\ndataset_y.txt\r\nThis<&>is the message<^>.\r\nThis is another<&> message.\r\nAnother one<%>.\r\n\r\nI wanted a simple script which could tokenize these datasets and train any suited model from scratch.\r\nCould anyone help me? Thanks a bunch!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6439", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6439/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6439/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6439/events", "html_url": "https://github.com/huggingface/transformers/issues/6439", "id": 677699403, "node_id": "MDU6SXNzdWU2Nzc2OTk0MDM=", "number": 6439, "title": "TrainingArguments are ignored?!", "user": {"login": "BerndStromberg", "id": 7240417, "node_id": "MDQ6VXNlcjcyNDA0MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7240417?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BerndStromberg", "html_url": "https://github.com/BerndStromberg", "followers_url": "https://api.github.com/users/BerndStromberg/followers", "following_url": "https://api.github.com/users/BerndStromberg/following{/other_user}", "gists_url": "https://api.github.com/users/BerndStromberg/gists{/gist_id}", "starred_url": "https://api.github.com/users/BerndStromberg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BerndStromberg/subscriptions", "organizations_url": "https://api.github.com/users/BerndStromberg/orgs", "repos_url": "https://api.github.com/users/BerndStromberg/repos", "events_url": "https://api.github.com/users/BerndStromberg/events{/privacy}", "received_events_url": "https://api.github.com/users/BerndStromberg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-12T13:34:48Z", "updated_at": "2020-08-12T18:28:13Z", "closed_at": "2020-08-12T18:28:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.6.0+cu101 (True)\r\n- Tensorflow version (GPU?): 2.3.0 (True)\r\n- Using GPU in script?: yes\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n### Who can help\r\nTrainer: @sgugger\r\n\r\n## Information\r\n\r\nHey, I'm using `01_how-to-train.ipynb` to get a feeling for the object oriented way of training a bert model from scratch. Until now I've been using the scripts offered by offical bert repository. My target is to train all of my future Transformer models with your Huggingface interface (from scratch and of course fine tuning too).\r\n\r\nI used `max_steps = 500_000` but it gets completely ignored. After training is started the output says:\r\n```\r\nIteration: 11639/422095 [1:52:03<70:16:42, 1.62it/s]\r\nEpoch 0/2 [00:00<?, ?it/s]\r\n```\r\n**Two epochs and 422095 iterations seems wrong!?** Official docs say _\"max_steps = the total number of training steps to perform\"_ Am I misinterpreting something?\r\n\r\nModel I am using (Bert, XLNet ...): Bert\r\n\r\nThe problem arises when using:\r\n* [x] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [x] my own task or dataset:\r\n  * line by line dataset\r\n  * training a bert language model from scratch (generating vocab, setting a config, ...)\r\n\r\n## To reproduce\r\n\r\nUse colab \"01_how-to-train.ipynb\" (https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\r\n) and change TrainingArguments to the following:\r\n```\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./smallBERTa\",\r\n    overwrite_output_dir=True,\r\n    do_train=True,\r\n    warmup_steps=5000,\r\n    max_steps=500000,\r\n    per_gpu_train_batch_size=64,\r\n    save_steps=10_000,\r\n    save_total_limit=2,\r\n)\r\n```\r\nYes, I am passing the `training_args` to the `Trainer()` object.\r\n\r\n\r\n## Expected behavior\r\n\r\nI'm expecting to get 500.000 global training steps and just one epoch.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6438", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6438/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6438/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6438/events", "html_url": "https://github.com/huggingface/transformers/issues/6438", "id": 677674267, "node_id": "MDU6SXNzdWU2Nzc2NzQyNjc=", "number": 6438, "title": "Training GPT2 and Reformer from scratch.  ", "user": {"login": "VikasRajashekar", "id": 52132904, "node_id": "MDQ6VXNlcjUyMTMyOTA0", "avatar_url": "https://avatars0.githubusercontent.com/u/52132904?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VikasRajashekar", "html_url": "https://github.com/VikasRajashekar", "followers_url": "https://api.github.com/users/VikasRajashekar/followers", "following_url": "https://api.github.com/users/VikasRajashekar/following{/other_user}", "gists_url": "https://api.github.com/users/VikasRajashekar/gists{/gist_id}", "starred_url": "https://api.github.com/users/VikasRajashekar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VikasRajashekar/subscriptions", "organizations_url": "https://api.github.com/users/VikasRajashekar/orgs", "repos_url": "https://api.github.com/users/VikasRajashekar/repos", "events_url": "https://api.github.com/users/VikasRajashekar/events{/privacy}", "received_events_url": "https://api.github.com/users/VikasRajashekar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-12T12:59:15Z", "updated_at": "2020-08-13T14:50:09Z", "closed_at": "2020-08-13T14:50:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I am looking, for example, script/notebook to train GPT2 and Reformer model from scratch in German. \r\nSomething similar to : \r\nhttps://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\r\n\r\nI am trying to modify the same notebook but GPT2 doesn't seem to accept LinebyLineDataset or padding. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6428", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6428/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6428/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6428/events", "html_url": "https://github.com/huggingface/transformers/issues/6428", "id": 677274110, "node_id": "MDU6SXNzdWU2NzcyNzQxMTA=", "number": 6428, "title": "Error in run_tf_squad.py script", "user": {"login": "M-Salti", "id": 9285264, "node_id": "MDQ6VXNlcjkyODUyNjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9285264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/M-Salti", "html_url": "https://github.com/M-Salti", "followers_url": "https://api.github.com/users/M-Salti/followers", "following_url": "https://api.github.com/users/M-Salti/following{/other_user}", "gists_url": "https://api.github.com/users/M-Salti/gists{/gist_id}", "starred_url": "https://api.github.com/users/M-Salti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/M-Salti/subscriptions", "organizations_url": "https://api.github.com/users/M-Salti/orgs", "repos_url": "https://api.github.com/users/M-Salti/repos", "events_url": "https://api.github.com/users/M-Salti/events{/privacy}", "received_events_url": "https://api.github.com/users/M-Salti/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-11T23:57:54Z", "updated_at": "2020-08-12T12:47:32Z", "closed_at": "2020-08-12T12:47:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.6.0+cu101 (True)\r\n- Tensorflow version (GPU?): 2.3.0 (True)\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n --> @sgugger\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...):\r\n\r\nThe problem arises when using:\r\n* [x] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [x] an official GLUE/SQUaD task: SQUaD\r\n* [ ] my own task or dataset: (give details below)\r\n\r\nI'm simply trying to train a new question answering model using the TF trainer script, and I get the following error:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"run_tf_squad.py\", line 244, in <module>\r\n    main()\r\n  File \"run_tf_squad.py\", line 123, in main\r\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/hf_argparser.py\", line 40, in __init__\r\n    self._add_dataclass_arguments(dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/hf_argparser.py\", line 72, in _add_dataclass_arguments\r\n    elif hasattr(field.type, \"__origin__\") and issubclass(field.type.__origin__, List):\r\n  File \"/usr/lib/python3.6/typing.py\", line 1154, in __subclasscheck__\r\n    return super().__subclasscheck__(cls)\r\n  File \"/usr/lib/python3.6/abc.py\", line 209, in __subclasscheck__\r\n    ok = cls.__subclasshook__(subclass)\r\n  File \"/usr/lib/python3.6/typing.py\", line 890, in __extrahook__\r\n    if cls.__extra__ and issubclass(subclass, cls.__extra__):\r\nTypeError: issubclass() arg 1 must be a class\r\n```\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.install transformers from the master branch\r\n2.run the example script in question-answering:\r\n```\r\npython run_tf_squad.py \\\r\n    --model_name_or_path bert-base-uncased \\\r\n    --output_dir model \\\r\n    --max_seq_length 384 \\\r\n    --num_train_epochs 2 \\\r\n    --per_gpu_train_batch_size 8 \\\r\n    --per_gpu_eval_batch_size 16 \\\r\n    --do_train \\\r\n    --logging_dir logs \\    \r\n    --logging_steps 10 \\\r\n    --learning_rate 3e-5 \\\r\n    --doc_stride 128    \r\n```\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\nThe script should run normally and train the model\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6421", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6421/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6421/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6421/events", "html_url": "https://github.com/huggingface/transformers/issues/6421", "id": 677093083, "node_id": "MDU6SXNzdWU2NzcwOTMwODM=", "number": 6421, "title": "test_run_glue_with_pabee failing", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "JetRunner", "id": 22514219, "node_id": "MDQ6VXNlcjIyNTE0MjE5", "avatar_url": "https://avatars2.githubusercontent.com/u/22514219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JetRunner", "html_url": "https://github.com/JetRunner", "followers_url": "https://api.github.com/users/JetRunner/followers", "following_url": "https://api.github.com/users/JetRunner/following{/other_user}", "gists_url": "https://api.github.com/users/JetRunner/gists{/gist_id}", "starred_url": "https://api.github.com/users/JetRunner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JetRunner/subscriptions", "organizations_url": "https://api.github.com/users/JetRunner/orgs", "repos_url": "https://api.github.com/users/JetRunner/repos", "events_url": "https://api.github.com/users/JetRunner/events{/privacy}", "received_events_url": "https://api.github.com/users/JetRunner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "JetRunner", "id": 22514219, "node_id": "MDQ6VXNlcjIyNTE0MjE5", "avatar_url": "https://avatars2.githubusercontent.com/u/22514219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JetRunner", "html_url": "https://github.com/JetRunner", "followers_url": "https://api.github.com/users/JetRunner/followers", "following_url": "https://api.github.com/users/JetRunner/following{/other_user}", "gists_url": "https://api.github.com/users/JetRunner/gists{/gist_id}", "starred_url": "https://api.github.com/users/JetRunner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JetRunner/subscriptions", "organizations_url": "https://api.github.com/users/JetRunner/orgs", "repos_url": "https://api.github.com/users/JetRunner/repos", "events_url": "https://api.github.com/users/JetRunner/events{/privacy}", "received_events_url": "https://api.github.com/users/JetRunner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-08-11T18:20:08Z", "updated_at": "2020-08-14T17:30:42Z", "closed_at": "2020-08-14T17:30:42Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "examples/bert-loses-patience/test_run_glue_with_pabee.py::PabeeTests::test_run_glue\r\n\r\nhttps://app.circleci.com/pipelines/github/huggingface/transformers/10373/workflows/0c9f2e61-2732-4857-84f0-71b59ddf10a9/jobs/71646\r\n\r\n\r\n@JetRunner ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6419", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6419/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6419/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6419/events", "html_url": "https://github.com/huggingface/transformers/issues/6419", "id": 677077536, "node_id": "MDU6SXNzdWU2NzcwNzc1MzY=", "number": 6419, "title": "Add pegasus model cards", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-08-11T17:56:42Z", "updated_at": "2020-08-20T14:39:06Z", "closed_at": "2020-08-20T14:39:06Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6418", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6418/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6418/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6418/events", "html_url": "https://github.com/huggingface/transformers/issues/6418", "id": 677037754, "node_id": "MDU6SXNzdWU2NzcwMzc3NTQ=", "number": 6418, "title": "All learning rates are 0 warning", "user": {"login": "sajastu", "id": 10419055, "node_id": "MDQ6VXNlcjEwNDE5MDU1", "avatar_url": "https://avatars2.githubusercontent.com/u/10419055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sajastu", "html_url": "https://github.com/sajastu", "followers_url": "https://api.github.com/users/sajastu/followers", "following_url": "https://api.github.com/users/sajastu/following{/other_user}", "gists_url": "https://api.github.com/users/sajastu/gists{/gist_id}", "starred_url": "https://api.github.com/users/sajastu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sajastu/subscriptions", "organizations_url": "https://api.github.com/users/sajastu/orgs", "repos_url": "https://api.github.com/users/sajastu/repos", "events_url": "https://api.github.com/users/sajastu/events{/privacy}", "received_events_url": "https://api.github.com/users/sajastu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-11T16:55:36Z", "updated_at": "2020-08-15T16:44:18Z", "closed_at": "2020-08-15T16:44:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "- `transformers` version: 3.0.2\r\n- Platform: Linux\r\n- Python version: 3.6\r\n- PyTorch version (GPU?): 1.4 (GPU)\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help\r\n@sshleifer \r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): BART\r\n\r\nThe problem arises when using:\r\n* [x] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [x] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Running the example script in https://github.com/huggingface/transformers/tree/master/examples/seq2seq (finetune_bart_tiny.sh), I'm getting this warning in the beginning of training. However, the training process is continuing after that. \r\n\r\nWarning:\r\n```\r\nfinetune.py:245: UserWarning: All learning rates are 0\r\n  warnings.warn(\"All learning rates are 0\")\r\nEpoch 1:   0%|      \r\n/home/sajad/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\r\n  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\r\n```\r\n\r\n## Expected behavior\r\nWhile the training seemingly goes well, I'm wondering if this warning would cause problems, leading to deteriorate model's final performance? As a add-on, I've also incorporated the gradient checkpointing to some computational blocks of BART (modifying `modelling_bart.py` script a bit). But even w/o incorporating this module, I'm still getting this warning message? Any thoughts of how to solve it? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6417", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6417/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6417/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6417/events", "html_url": "https://github.com/huggingface/transformers/issues/6417", "id": 677033681, "node_id": "MDU6SXNzdWU2NzcwMzM2ODE=", "number": 6417, "title": "how to fine tune t5 model for summarization task using tensorflow2?", "user": {"login": "banunitte", "id": 6847024, "node_id": "MDQ6VXNlcjY4NDcwMjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/6847024?v=4", "gravatar_id": "", "url": "https://api.github.com/users/banunitte", "html_url": "https://github.com/banunitte", "followers_url": "https://api.github.com/users/banunitte/followers", "following_url": "https://api.github.com/users/banunitte/following{/other_user}", "gists_url": "https://api.github.com/users/banunitte/gists{/gist_id}", "starred_url": "https://api.github.com/users/banunitte/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/banunitte/subscriptions", "organizations_url": "https://api.github.com/users/banunitte/orgs", "repos_url": "https://api.github.com/users/banunitte/repos", "events_url": "https://api.github.com/users/banunitte/events{/privacy}", "received_events_url": "https://api.github.com/users/banunitte/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-11T16:49:25Z", "updated_at": "2020-08-11T16:53:02Z", "closed_at": "2020-08-11T16:53:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n**A link to original question on the forum/Stack Overflow**:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6416", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6416/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6416/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6416/events", "html_url": "https://github.com/huggingface/transformers/issues/6416", "id": 677014260, "node_id": "MDU6SXNzdWU2NzcwMTQyNjA=", "number": 6416, "title": "Docs: Separate documentation for mbart", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1834067346, "node_id": "MDU6TGFiZWwxODM0MDY3MzQ2", "url": "https://api.github.com/repos/huggingface/transformers/labels/Documentation", "name": "Documentation", "color": "77cc3b", "default": false, "description": ""}, {"id": 1108649053, "node_id": "MDU6TGFiZWwxMTA4NjQ5MDUz", "url": "https://api.github.com/repos/huggingface/transformers/labels/Help%20wanted", "name": "Help wanted", "color": "008672", "default": false, "description": "Extra attention is needed, help appreciated"}], "state": "closed", "locked": false, "assignee": {"login": "patil-suraj", "id": 27137566, "node_id": "MDQ6VXNlcjI3MTM3NTY2", "avatar_url": "https://avatars0.githubusercontent.com/u/27137566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patil-suraj", "html_url": "https://github.com/patil-suraj", "followers_url": "https://api.github.com/users/patil-suraj/followers", "following_url": "https://api.github.com/users/patil-suraj/following{/other_user}", "gists_url": "https://api.github.com/users/patil-suraj/gists{/gist_id}", "starred_url": "https://api.github.com/users/patil-suraj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patil-suraj/subscriptions", "organizations_url": "https://api.github.com/users/patil-suraj/orgs", "repos_url": "https://api.github.com/users/patil-suraj/repos", "events_url": "https://api.github.com/users/patil-suraj/events{/privacy}", "received_events_url": "https://api.github.com/users/patil-suraj/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patil-suraj", "id": 27137566, "node_id": "MDQ6VXNlcjI3MTM3NTY2", "avatar_url": "https://avatars0.githubusercontent.com/u/27137566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patil-suraj", "html_url": "https://github.com/patil-suraj", "followers_url": "https://api.github.com/users/patil-suraj/followers", "following_url": "https://api.github.com/users/patil-suraj/following{/other_user}", "gists_url": "https://api.github.com/users/patil-suraj/gists{/gist_id}", "starred_url": "https://api.github.com/users/patil-suraj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patil-suraj/subscriptions", "organizations_url": "https://api.github.com/users/patil-suraj/orgs", "repos_url": "https://api.github.com/users/patil-suraj/repos", "events_url": "https://api.github.com/users/patil-suraj/events{/privacy}", "received_events_url": "https://api.github.com/users/patil-suraj/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-08-11T16:21:01Z", "updated_at": "2020-08-21T15:57:35Z", "closed_at": "2020-08-21T15:57:35Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Currently all mbart documentation is stuffed into docs/\r\nhttps://github.com/huggingface/transformers/blob/353b8f1e7a7361c0afd9e391381bc226b4a5ca8f/docs/source/model_doc/bart.rst#L42\r\n\r\nmbart should have it's own `model_doc/mbart.rst` and entry in `pretrained_models.rst`.\r\n\r\n\r\n\r\nOptionally you can also create a new `src/transformers/modeling_mbart.py` with roughly these contents:\r\n\r\n```python\r\nfrom .modeling_bart import BartForConditionalGeneration\r\nfrom .configuration_bart import MbartConfig\r\nclass MBartForConditionalGeneration(BartForConditionalGeneration):\r\n    config_class = MbartConfig\r\n\t# this model fully inherits its implementation from bart\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6414", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6414/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6414/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6414/events", "html_url": "https://github.com/huggingface/transformers/issues/6414", "id": 676926214, "node_id": "MDU6SXNzdWU2NzY5MjYyMTQ=", "number": 6414, "title": "TypeError: forward() got an unexpected keyword argument 'labels'", "user": {"login": "vgoklani", "id": 180487, "node_id": "MDQ6VXNlcjE4MDQ4Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/180487?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vgoklani", "html_url": "https://github.com/vgoklani", "followers_url": "https://api.github.com/users/vgoklani/followers", "following_url": "https://api.github.com/users/vgoklani/following{/other_user}", "gists_url": "https://api.github.com/users/vgoklani/gists{/gist_id}", "starred_url": "https://api.github.com/users/vgoklani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vgoklani/subscriptions", "organizations_url": "https://api.github.com/users/vgoklani/orgs", "repos_url": "https://api.github.com/users/vgoklani/repos", "events_url": "https://api.github.com/users/vgoklani/events{/privacy}", "received_events_url": "https://api.github.com/users/vgoklani/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-11T14:30:00Z", "updated_at": "2020-08-11T16:15:32Z", "closed_at": "2020-08-11T16:15:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n\r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-5.3.0-53-generic-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.7\r\n- PyTorch version (GPU?): 1.6.0 (True)\r\n- Tensorflow version (GPU?): 2.3.0 (False)\r\n- Using GPU in script?: True\r\n- Using distributed or parallel set-up in script?: Distributed\r\n\r\nHey there,\r\n\r\nI've run into this issue and not sure how to fix it:\r\n\r\n    TypeError: forward() got an unexpected keyword argument 'labels'\r\n\r\nI'm running transformers v3.0.2 installed via pip\r\n\r\nPlease see my code below. There is nothing fancy going on, I'm just trying to train RobertaMLM for a few more epochs on a different dataset. \r\n\r\n```python\r\nimport os\r\nimport argparse\r\nimport datetime\r\nfrom torch.utils.tensorboard import SummaryWriter\r\nfrom transformers import RobertaModel, RobertaConfig, RobertaTokenizerFast, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\r\nfrom configs import model_directory, tensorboard_directory\r\nfrom logger import get_logger\r\n\r\nlog = get_logger(__name__)\r\n\r\n\r\nargs = argparse.Namespace(\r\n\tseed=42,\r\n\tmodel_id=\"Roberta2\",\r\n\tpretrained_model_name_or_path=\"roberta-base\",\r\n\tvocab_file=\"/data/nlp/roberta_vocabulary/roberta-base-vocab.json\",\r\n\tmerges_file=\"/data/nlp/roberta_vocabulary/roberta-base-merges.txt\",\r\n\tfilename=\"/data/nlp/trc2.txt\",\r\n\tblock_size=2**7,\r\n\tepochs=25,\r\n)\r\n\r\noutput_directory = os.path.join(model_directory, args.model_id)\r\nos.makedirs(output_directory, exist_ok=True)\r\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\r\n\r\n\r\ndef build_model():\r\n\ttokenizer = RobertaTokenizerFast.from_pretrained(pretrained_model_name_or_path=args.pretrained_model_name_or_path, lowercase=True, add_prefix_space=True, max_len=512)\r\n\r\n\tconfig = RobertaConfig.from_pretrained(args.pretrained_model_name_or_path)\r\n\tconfig.output_hidden_states = False\r\n\r\n\tmodel = RobertaModel.from_pretrained(pretrained_model_name_or_path=args.pretrained_model_name_or_path, config=config, cache_dir=output_directory)\r\n\r\n\tdataset = LineByLineTextDataset(\r\n\t\ttokenizer=tokenizer,\r\n\t\tfile_path=args.filename,\r\n\t\tblock_size=args.block_size,\r\n\t)\r\n\r\n\tdata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r\n\r\n\ttraining_args = TrainingArguments(\r\n\t\tseed=args.seed,\r\n\t\toutput_dir=output_directory,\r\n\t\toverwrite_output_dir=True,\r\n\t\tnum_train_epochs=args.epochs,\r\n\t\tper_device_train_batch_size=128,\r\n\t\tsave_steps=10_000,\r\n\t\t# save_total_limit=2,\r\n\t\tfp16=True,\r\n\t\tfp16_opt_level=\"O1\"\r\n\t)\r\n\r\n\ttrainer = Trainer(\r\n\t\tmodel=model,\r\n\t\targs=training_args,\r\n\t\tdata_collator=data_collator,\r\n\t\ttrain_dataset=dataset,\r\n\t\tprediction_loss_only=True,\r\n\t)\r\n\r\n\ttrainer.train()\r\n\ttrainer.save_model(output_directory)\r\n\r\n```\r\n\r\n\r\ntag: @sgugger ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6407", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6407/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6407/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6407/events", "html_url": "https://github.com/huggingface/transformers/issues/6407", "id": 676614358, "node_id": "MDU6SXNzdWU2NzY2MTQzNTg=", "number": 6407, "title": "Slow Decoding Speed when using BertForLMModel", "user": {"login": "JamesHujy", "id": 48405323, "node_id": "MDQ6VXNlcjQ4NDA1MzIz", "avatar_url": "https://avatars2.githubusercontent.com/u/48405323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JamesHujy", "html_url": "https://github.com/JamesHujy", "followers_url": "https://api.github.com/users/JamesHujy/followers", "following_url": "https://api.github.com/users/JamesHujy/following{/other_user}", "gists_url": "https://api.github.com/users/JamesHujy/gists{/gist_id}", "starred_url": "https://api.github.com/users/JamesHujy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JamesHujy/subscriptions", "organizations_url": "https://api.github.com/users/JamesHujy/orgs", "repos_url": "https://api.github.com/users/JamesHujy/repos", "events_url": "https://api.github.com/users/JamesHujy/events{/privacy}", "received_events_url": "https://api.github.com/users/JamesHujy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-08-11T06:16:53Z", "updated_at": "2020-08-13T09:23:10Z", "closed_at": "2020-08-13T09:23:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "I set the BertLMHeadModel as Decoder in my Seq2Seq model. It seems to work well in training. But when decoing, it decodes very slowly. I think there is no layer_past used in GPT2, XLNet in BertLMHeadModel and many attentions are computed repetitively? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6400", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6400/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6400/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6400/events", "html_url": "https://github.com/huggingface/transformers/issues/6400", "id": 676406977, "node_id": "MDU6SXNzdWU2NzY0MDY5Nzc=", "number": 6400, "title": "ZeroDivisionError with Reformer", "user": {"login": "eliasjacob", "id": 34211393, "node_id": "MDQ6VXNlcjM0MjExMzkz", "avatar_url": "https://avatars3.githubusercontent.com/u/34211393?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliasjacob", "html_url": "https://github.com/eliasjacob", "followers_url": "https://api.github.com/users/eliasjacob/followers", "following_url": "https://api.github.com/users/eliasjacob/following{/other_user}", "gists_url": "https://api.github.com/users/eliasjacob/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliasjacob/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliasjacob/subscriptions", "organizations_url": "https://api.github.com/users/eliasjacob/orgs", "repos_url": "https://api.github.com/users/eliasjacob/repos", "events_url": "https://api.github.com/users/eliasjacob/events{/privacy}", "received_events_url": "https://api.github.com/users/eliasjacob/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-10T20:47:48Z", "updated_at": "2020-08-11T10:57:34Z", "closed_at": "2020-08-11T10:57:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.0\r\n- Platform: Linux-5.4.0-42-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.6.0 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): **Reformer**\r\n\r\nThe problem arises when using:\r\n* [ ] the official example scripts: (give details below)\r\n* [X] my own modified scripts: (give details below) (well, actually, not my own, but @patrickvonplaten 's) \r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [x] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Execute @patrickvonplaten 's notebook available at https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb \r\n2. I've tried to run it on google colab and works fine. The problem appears when I try to run on my machine.\r\n3. I've tried it with two different clean virtual environments (python 3.6 and 3.7), but they've both failed.\r\n4. I haven't change the dataset, nor any model config/training args.\r\n4. After calling trainer.train() I get the following error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nZeroDivisionError                         Traceback (most recent call last)\r\n<ipython-input-13-02431faf649a> in <module>\r\n      8 \r\n      9 # train\r\n---> 10 trainer.train()\r\n\r\n/data/venv36/lib/python3.6/site-packages/transformers/trainer.py in train(self, model_path)\r\n    394             t_total = self.args.max_steps\r\n    395             num_train_epochs = (\r\n--> 396                 self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\r\n    397             )\r\n    398         else:\r\n\r\nZeroDivisionError: integer division or modulo by zero\r\n\r\n```\r\n## Expected behavior\r\n\r\nThe model should begin to train\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6395", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6395/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6395/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6395/events", "html_url": "https://github.com/huggingface/transformers/issues/6395", "id": 676273276, "node_id": "MDU6SXNzdWU2NzYyNzMyNzY=", "number": 6395, "title": "Bug in the question answering pipeline", "user": {"login": "elronbandel", "id": 23455264, "node_id": "MDQ6VXNlcjIzNDU1MjY0", "avatar_url": "https://avatars3.githubusercontent.com/u/23455264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elronbandel", "html_url": "https://github.com/elronbandel", "followers_url": "https://api.github.com/users/elronbandel/followers", "following_url": "https://api.github.com/users/elronbandel/following{/other_user}", "gists_url": "https://api.github.com/users/elronbandel/gists{/gist_id}", "starred_url": "https://api.github.com/users/elronbandel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elronbandel/subscriptions", "organizations_url": "https://api.github.com/users/elronbandel/orgs", "repos_url": "https://api.github.com/users/elronbandel/repos", "events_url": "https://api.github.com/users/elronbandel/events{/privacy}", "received_events_url": "https://api.github.com/users/elronbandel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-10T16:45:57Z", "updated_at": "2020-08-11T15:39:07Z", "closed_at": "2020-08-11T09:14:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.6.0+cu101 (False)\r\n- Tensorflow version (GPU?): 2.3.0 (False)\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n## Information\r\nThe bug appears since transformers 3.0.1 but not before.\r\n\r\nModel I am using distilbert-base-cased-distilled-squad:\r\n\r\nThe problem arises when using:\r\n* [ ] my own modified scripts: \r\n```\r\n\r\nfrom transformers import pipeline\r\nmodel = \"distilbert-base-cased-distilled-squad\"\r\nqa_pipeline = pipeline(\r\n    \"question-answering\",\r\n    model=model,\r\n    tokenizer=model,\r\n)\r\n\r\ninstance = {\r\n  \"question\": \"what is your product?\",\r\n  \"context\": \" is an amazing new platform that help businesses of students from BarIlan University that are enthusiastic about conversational AI. The difference between our Sprybot platform and other chat bots is that constructing chat bot is a long and hard process and with Sprybot you can do it quickly and eaily. You can construct chatbot using our platform just by feeding textual description of you business that contain any details important for costumers. The time it takes to create a bot using our platform is the time takes you to describe your business. In order to create Sprybot we used natural language processing and state of the art deep learning artificial intelligence. At the moment you cant buy our product because its still under construction. Sprybot can answer questions about your business but it can not talk about anything else other than the information was fed to it.\"\r\n}\r\nqa_pipeline(instance)\r\n\r\n```\r\n\r\n\r\nNotice: little changes in the context text make the bug to not show up\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. [fully reproduced on google colab](https://colab.research.google.com/drive/1YqamXA6qq8xxWXhq6VqEA9clHsEVW7sh?usp=sharing)\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-5-a5f26c48556d> in <module>()\r\n      4 }\r\n      5 \r\n----> 6 qa_pipeline(instance)\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py in __call__(self, *args, **kwargs)\r\n   1314                         ),\r\n   1315                     }\r\n-> 1316                     for s, e, score in zip(starts, ends, scores)\r\n   1317                 ]\r\n   1318 \r\n\r\n/usr/local/lib/python3.6/dist-packages/transformers/pipelines.py in <listcomp>(.0)\r\n   1314                         ),\r\n   1315                     }\r\n-> 1316                     for s, e, score in zip(starts, ends, scores)\r\n   1317                 ]\r\n   1318 \r\n\r\nKeyError: 0\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nget the qa pipline output with no errors\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6392", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6392/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6392/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6392/events", "html_url": "https://github.com/huggingface/transformers/issues/6392", "id": 676226578, "node_id": "MDU6SXNzdWU2NzYyMjY1Nzg=", "number": 6392, "title": "seq2seq examples require pytest", "user": {"login": "dmlap", "id": 56667, "node_id": "MDQ6VXNlcjU2NjY3", "avatar_url": "https://avatars1.githubusercontent.com/u/56667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmlap", "html_url": "https://github.com/dmlap", "followers_url": "https://api.github.com/users/dmlap/followers", "following_url": "https://api.github.com/users/dmlap/following{/other_user}", "gists_url": "https://api.github.com/users/dmlap/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmlap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmlap/subscriptions", "organizations_url": "https://api.github.com/users/dmlap/orgs", "repos_url": "https://api.github.com/users/dmlap/repos", "events_url": "https://api.github.com/users/dmlap/events{/privacy}", "received_events_url": "https://api.github.com/users/dmlap/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-10T15:32:16Z", "updated_at": "2020-08-11T21:58:10Z", "closed_at": "2020-08-11T21:58:10Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.2\r\n- PyTorch version (GPU?): 1.6.0 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: n/a\r\n- Using distributed or parallel set-up in script?: n/a\r\n\r\n### Who can help\r\n examples/seq2seq: @sshleifer\r\ndocumentation: @sgugger\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a new virtual environment and set it up to run the examples tests. Do _not_ install `pytest` and `pytest-xdist`.\r\n2. Run the tests with `unittest` as [described in the docs](https://github.com/huggingface/transformers/blob/master/CONTRIBUTING.md#tests)\r\n\r\n## Expected behavior\r\nThe examples tests pass. Actual behavior:\r\n\r\n```sh\r\n======================================================================\r\nERROR: seq2seq.test_bash_script (unittest.loader._FailedTest)\r\n----------------------------------------------------------------------\r\nImportError: Failed to import test module: seq2seq.test_bash_script\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/unittest/loader.py\", line 436, in _find_test_path\r\n    module = self._get_module_from_name(name)\r\n  File \"/usr/lib/python3.8/unittest/loader.py\", line 377, in _get_module_from_name\r\n    __import__(name)\r\n  File \"/home/dmlap/projects/transformers/examples/seq2seq/test_bash_script.py\", line 8, in <module>\r\n    import pytest\r\nModuleNotFoundError: No module named 'pytest'\r\n\r\n\r\n======================================================================\r\nERROR: seq2seq.test_seq2seq_examples (unittest.loader._FailedTest)\r\n----------------------------------------------------------------------\r\nImportError: Failed to import test module: seq2seq.test_seq2seq_examples\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/unittest/loader.py\", line 436, in _find_test_path\r\n    module = self._get_module_from_name(name)\r\n  File \"/usr/lib/python3.8/unittest/loader.py\", line 377, in _get_module_from_name\r\n    __import__(name)\r\n  File \"/home/dmlap/projects/transformers/examples/seq2seq/test_seq2seq_examples.py\", line 10, in <module>\r\n    import pytest\r\nModuleNotFoundError: No module named 'pytest'\r\n\r\n\r\n----------------------------------------------------------------------\r\nRan 16 tests in 179.454s\r\n\r\nFAILED (errors=2)\r\n```\r\n\r\nPerhaps the documentation should be updated to require `pytest`?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6383", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6383/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6383/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6383/events", "html_url": "https://github.com/huggingface/transformers/issues/6383", "id": 676095038, "node_id": "MDU6SXNzdWU2NzYwOTUwMzg=", "number": 6383, "title": "hi", "user": {"login": "Xinghao93", "id": 49721621, "node_id": "MDQ6VXNlcjQ5NzIxNjIx", "avatar_url": "https://avatars2.githubusercontent.com/u/49721621?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Xinghao93", "html_url": "https://github.com/Xinghao93", "followers_url": "https://api.github.com/users/Xinghao93/followers", "following_url": "https://api.github.com/users/Xinghao93/following{/other_user}", "gists_url": "https://api.github.com/users/Xinghao93/gists{/gist_id}", "starred_url": "https://api.github.com/users/Xinghao93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Xinghao93/subscriptions", "organizations_url": "https://api.github.com/users/Xinghao93/orgs", "repos_url": "https://api.github.com/users/Xinghao93/repos", "events_url": "https://api.github.com/users/Xinghao93/events{/privacy}", "received_events_url": "https://api.github.com/users/Xinghao93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-10T12:23:10Z", "updated_at": "2020-08-10T12:28:30Z", "closed_at": "2020-08-10T12:28:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version:\r\n- Platform:\r\n- Python version:\r\n- PyTorch version (GPU?):\r\n- Tensorflow version (GPU?):\r\n- Using GPU in script?:\r\n- Using distributed or parallel set-up in script?:\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n -->\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...):\r\n\r\nThe problem arises when using:\r\n* [ ] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [ ] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6375", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6375/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6375/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6375/events", "html_url": "https://github.com/huggingface/transformers/issues/6375", "id": 675901486, "node_id": "MDU6SXNzdWU2NzU5MDE0ODY=", "number": 6375, "title": "CUDA Out of Memory", "user": {"login": "mc2259", "id": 57819870, "node_id": "MDQ6VXNlcjU3ODE5ODcw", "avatar_url": "https://avatars0.githubusercontent.com/u/57819870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mc2259", "html_url": "https://github.com/mc2259", "followers_url": "https://api.github.com/users/mc2259/followers", "following_url": "https://api.github.com/users/mc2259/following{/other_user}", "gists_url": "https://api.github.com/users/mc2259/gists{/gist_id}", "starred_url": "https://api.github.com/users/mc2259/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mc2259/subscriptions", "organizations_url": "https://api.github.com/users/mc2259/orgs", "repos_url": "https://api.github.com/users/mc2259/repos", "events_url": "https://api.github.com/users/mc2259/events{/privacy}", "received_events_url": "https://api.github.com/users/mc2259/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-10T06:28:09Z", "updated_at": "2020-08-10T07:49:10Z", "closed_at": "2020-08-10T07:49:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     https://stackoverflow.com/questions/63335442/how-do-i-deal-with-cuda-out-of-memory-while-finetuning-bart\r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\nI was trying to finetune BART on google collab using the xsum dataset and the finetuning script and I got this:\r\nRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.73 GiB total capacity; 13.67 GiB already allocated; 15.88 MiB free; 13.72 GiB reserved in total by PyTorch)\r\nDoes this mean I have to use a smaller model?\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n**A link to original question on the forum/Stack Overflow**:\r\nhttps://stackoverflow.com/questions/63335442/how-do-i-deal-with-cuda-out-of-memory-while-finetuning-bart", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6362", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6362/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6362/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6362/events", "html_url": "https://github.com/huggingface/transformers/issues/6362", "id": 675635648, "node_id": "MDU6SXNzdWU2NzU2MzU2NDg=", "number": 6362, "title": "[TFTrainer] Error \"iterating over `tf.Tensor` is not allowed\"", "user": {"login": "EibrielInv", "id": 172656, "node_id": "MDQ6VXNlcjE3MjY1Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/172656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EibrielInv", "html_url": "https://github.com/EibrielInv", "followers_url": "https://api.github.com/users/EibrielInv/followers", "following_url": "https://api.github.com/users/EibrielInv/following{/other_user}", "gists_url": "https://api.github.com/users/EibrielInv/gists{/gist_id}", "starred_url": "https://api.github.com/users/EibrielInv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EibrielInv/subscriptions", "organizations_url": "https://api.github.com/users/EibrielInv/orgs", "repos_url": "https://api.github.com/users/EibrielInv/repos", "events_url": "https://api.github.com/users/EibrielInv/events{/privacy}", "received_events_url": "https://api.github.com/users/EibrielInv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-09T04:43:46Z", "updated_at": "2020-08-21T13:21:54Z", "closed_at": "2020-08-21T13:21:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2 (from pip)\r\n- Platform: Linux-4.15.0-91-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.7.6\r\n- PyTorch version (GPU?): not installed (NA)\r\n- Tensorflow version (GPU?): 2.3.0 (True) (Same error on TF2.2 and TF2.1)\r\n- Using GPU in script?: Yes - GeForce GTX 1080 Ti\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help\r\n Trainer: @sgugger  tensorflow: @jplu \r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n -->\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): GPT2\r\n\r\nThe problem arises when using:\r\n* [ ] the official example scripts: (give details below)\r\n* [X] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [X] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n1.  Install Tensorflow 2.3.0, Transformers 3.0.2\r\n\r\n1. Run the following code:\r\n\r\n```python3\r\nfrom transformers import TFGPT2LMHeadModel, TFTrainer, TFTrainingArguments\r\nimport tensorflow as tf\r\n\r\ntfds_train_dataset = tf.data.Dataset.from_tensor_slices(\r\n    tf.random.uniform([4000, 1024], minval=1, maxval=10, dtype=tf.int32))\r\n\r\nmodel = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\r\n\r\ntraining_args = TFTrainingArguments(\r\n    output_dir='./results',\r\n    num_train_epochs=3,\r\n    per_device_train_batch_size=16,\r\n    per_device_eval_batch_size=64,\r\n    warmup_steps=500,\r\n    weight_decay=0.01,\r\n    logging_dir='./logs',\r\n)\r\n\r\ntrainer = TFTrainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=tfds_train_dataset,\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\n2. Results in the following output + error:\r\n```\r\n2020-08-09 01:41:28.331697: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-09 01:41:30.461375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-09 01:41:30.466239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-08-09 01:41:30.466271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-09 01:41:30.468575: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-09 01:41:30.470629: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-09 01:41:30.471013: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-09 01:41:30.473522: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-09 01:41:30.474947: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-09 01:41:30.481193: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-09 01:41:30.482710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-09 01:41:30.483080: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-09 01:41:30.512602: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3210790000 Hz\r\n2020-08-09 01:41:30.514335: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c678f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-09 01:41:30.514408: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-09 01:41:30.648534: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c92000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-08-09 01:41:30.648597: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-08-09 01:41:30.650365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-08-09 01:41:30.650446: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-09 01:41:30.650523: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-09 01:41:30.650586: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-09 01:41:30.650646: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-09 01:41:30.650708: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-09 01:41:30.650767: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-09 01:41:30.650829: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-09 01:41:30.653179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-08-09 01:41:30.653232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-09 01:41:31.392168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-09 01:41:31.392212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-08-09 01:41:31.392225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-08-09 01:41:31.393566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7389 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-08-09 01:41:34.003855: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2020-08-09 01:41:34.145974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\nAll model checkpoint weights were used when initializing TFGPT2LMHeadModel.\r\n\r\nAll the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\r\nIf your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\r\nTraceback (most recent call last):\r\n  File \"gpt2-training_bug.py\", line 26, in <module>\r\n    trainer.train()\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py\", line 412, in train\r\n    for step, training_loss in enumerate(self._training_steps(train_ds, optimizer)):\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py\", line 459, in _training_steps\r\n    for i, loss in enumerate(self._accumulate_next_gradients(ds)):\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py\", line 492, in _accumulate_next_gradients\r\n    yield _accumulate_next()\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 697, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 3075, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 973, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:\r\n\r\n    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/transformers/trainer_tf.py:486 _accumulate_next  *\r\n        per_replica_features, per_replica_labels = next(iterator)\r\n    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:503 __iter__\r\n        self._disallow_iteration()\r\n    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:496 _disallow_iteration\r\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    /home/gabriel/venv/GPT-Hug/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:474 _disallow_when_autograph_enabled\r\n        \" indicate you are trying to use an unsupported feature.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\n```\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\n\r\nStart Training\r\n\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6347", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6347/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6347/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6347/events", "html_url": "https://github.com/huggingface/transformers/issues/6347", "id": 675536130, "node_id": "MDU6SXNzdWU2NzU1MzYxMzA=", "number": 6347, "title": "ModuleNotFoundError: No module named 'transformers' on Google Colab", "user": {"login": "Mohd-Misran", "id": 55659231, "node_id": "MDQ6VXNlcjU1NjU5MjMx", "avatar_url": "https://avatars2.githubusercontent.com/u/55659231?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mohd-Misran", "html_url": "https://github.com/Mohd-Misran", "followers_url": "https://api.github.com/users/Mohd-Misran/followers", "following_url": "https://api.github.com/users/Mohd-Misran/following{/other_user}", "gists_url": "https://api.github.com/users/Mohd-Misran/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mohd-Misran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mohd-Misran/subscriptions", "organizations_url": "https://api.github.com/users/Mohd-Misran/orgs", "repos_url": "https://api.github.com/users/Mohd-Misran/repos", "events_url": "https://api.github.com/users/Mohd-Misran/events{/privacy}", "received_events_url": "https://api.github.com/users/Mohd-Misran/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-08T14:45:03Z", "updated_at": "2020-08-11T22:18:51Z", "closed_at": "2020-08-11T22:18:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "I installed **transformers** using the command `!pip install transformers` on **Google Colab Notebook**\r\nBut then I try to `import transformers` it throws an error.\r\n\r\nThis is the output of the pip install command:\r\n\r\nRequirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages/transformers-3.0.2-py3.6.egg (3.0.2)\r\nRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\r\nRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\r\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\r\nRequirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages/sacremoses-0.0.43-py3.6.egg (from transformers) (0.0.43)\r\nRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages/sentencepiece-0.1.91-py3.6-linux-x86_64.egg (from transformers) (0.1.91)\r\nRequirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages/tokenizers-0.8.1rc1-py3.6-linux-x86_64.egg (from transformers) (0.8.1rc1)\r\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\r\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\r\nRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\r\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\r\nRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\r\nRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6345", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6345/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6345/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6345/events", "html_url": "https://github.com/huggingface/transformers/issues/6345", "id": 675509199, "node_id": "MDU6SXNzdWU2NzU1MDkxOTk=", "number": 6345, "title": "Is it necessary to provide attention_mask, or model will calculate itself?", "user": {"login": "saahiluppal", "id": 47444392, "node_id": "MDQ6VXNlcjQ3NDQ0Mzky", "avatar_url": "https://avatars1.githubusercontent.com/u/47444392?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saahiluppal", "html_url": "https://github.com/saahiluppal", "followers_url": "https://api.github.com/users/saahiluppal/followers", "following_url": "https://api.github.com/users/saahiluppal/following{/other_user}", "gists_url": "https://api.github.com/users/saahiluppal/gists{/gist_id}", "starred_url": "https://api.github.com/users/saahiluppal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saahiluppal/subscriptions", "organizations_url": "https://api.github.com/users/saahiluppal/orgs", "repos_url": "https://api.github.com/users/saahiluppal/repos", "events_url": "https://api.github.com/users/saahiluppal/events{/privacy}", "received_events_url": "https://api.github.com/users/saahiluppal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-08T11:15:25Z", "updated_at": "2020-08-08T18:26:25Z", "closed_at": "2020-08-08T18:26:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it necessary to provide attention_mask, or model will calculate itself?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6336", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6336/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6336/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6336/events", "html_url": "https://github.com/huggingface/transformers/issues/6336", "id": 675380598, "node_id": "MDU6SXNzdWU2NzUzODA1OTg=", "number": 6336, "title": "broken ONNX slow test", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1834088753, "node_id": "MDU6TGFiZWwxODM0MDg4NzUz", "url": "https://api.github.com/repos/huggingface/transformers/labels/Tests", "name": "Tests", "color": "a6fcca", "default": false, "description": "Related to tests"}], "state": "closed", "locked": false, "assignee": {"login": "mfuntowicz", "id": 2241520, "node_id": "MDQ6VXNlcjIyNDE1MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2241520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfuntowicz", "html_url": "https://github.com/mfuntowicz", "followers_url": "https://api.github.com/users/mfuntowicz/followers", "following_url": "https://api.github.com/users/mfuntowicz/following{/other_user}", "gists_url": "https://api.github.com/users/mfuntowicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfuntowicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfuntowicz/subscriptions", "organizations_url": "https://api.github.com/users/mfuntowicz/orgs", "repos_url": "https://api.github.com/users/mfuntowicz/repos", "events_url": "https://api.github.com/users/mfuntowicz/events{/privacy}", "received_events_url": "https://api.github.com/users/mfuntowicz/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mfuntowicz", "id": 2241520, "node_id": "MDQ6VXNlcjIyNDE1MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2241520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfuntowicz", "html_url": "https://github.com/mfuntowicz", "followers_url": "https://api.github.com/users/mfuntowicz/followers", "following_url": "https://api.github.com/users/mfuntowicz/following{/other_user}", "gists_url": "https://api.github.com/users/mfuntowicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfuntowicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfuntowicz/subscriptions", "organizations_url": "https://api.github.com/users/mfuntowicz/orgs", "repos_url": "https://api.github.com/users/mfuntowicz/repos", "events_url": "https://api.github.com/users/mfuntowicz/events{/privacy}", "received_events_url": "https://api.github.com/users/mfuntowicz/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-08T00:42:31Z", "updated_at": "2020-08-17T13:04:36Z", "closed_at": "2020-08-17T13:04:36Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```\r\n    def test_quantize_pytorch(self):\r\n        for model in OnnxExportTestCase.MODEL_TO_TEST:\r\n            path = self._test_export(model, \"pt\", 12)\r\n>           quantized_path = quantize(Path(path))\r\n```\r\ntests/test_onnx.py:75:  `path` is None\r\n\r\nhttps://github.com/huggingface/transformers/runs/960368281?check_suite_focus=true", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6333", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6333/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6333/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6333/events", "html_url": "https://github.com/huggingface/transformers/issues/6333", "id": 675327498, "node_id": "MDU6SXNzdWU2NzUzMjc0OTg=", "number": 6333, "title": "add tests/test_tokenization_reformer.py", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1108649053, "node_id": "MDU6TGFiZWwxMTA4NjQ5MDUz", "url": "https://api.github.com/repos/huggingface/transformers/labels/Help%20wanted", "name": "Help wanted", "color": "008672", "default": false, "description": "Extra attention is needed, help appreciated"}, {"id": 1834088753, "node_id": "MDU6TGFiZWwxODM0MDg4NzUz", "url": "https://api.github.com/repos/huggingface/transformers/labels/Tests", "name": "Tests", "color": "a6fcca", "default": false, "description": "Related to tests"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-08-07T22:51:14Z", "updated_at": "2020-08-21T15:54:33Z", "closed_at": "2020-08-21T15:54:33Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "I don't think there is any common test coverage for ReformerTokenizer. besides through integration tests.\r\nGood source for copy/modification is `XLMRobertaTokenizationTest`\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6331", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6331/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6331/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6331/events", "html_url": "https://github.com/huggingface/transformers/issues/6331", "id": 675263507, "node_id": "MDU6SXNzdWU2NzUyNjM1MDc=", "number": 6331, "title": "Delete this line in label_smoothed_nll_loss", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-08-07T20:41:14Z", "updated_at": "2020-08-08T08:21:13Z", "closed_at": "2020-08-08T08:21:13Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```python\r\nbs = pad_mask.long().sum() \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6323", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6323/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6323/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6323/events", "html_url": "https://github.com/huggingface/transformers/issues/6323", "id": 674928799, "node_id": "MDU6SXNzdWU2NzQ5Mjg3OTk=", "number": 6323, "title": "Hi , I am having trouble locating the transformers/examples/summarization/bart/ file. I was wondering if it has been renamed or changed?", "user": {"login": "mc2259", "id": 57819870, "node_id": "MDQ6VXNlcjU3ODE5ODcw", "avatar_url": "https://avatars0.githubusercontent.com/u/57819870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mc2259", "html_url": "https://github.com/mc2259", "followers_url": "https://api.github.com/users/mc2259/followers", "following_url": "https://api.github.com/users/mc2259/following{/other_user}", "gists_url": "https://api.github.com/users/mc2259/gists{/gist_id}", "starred_url": "https://api.github.com/users/mc2259/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mc2259/subscriptions", "organizations_url": "https://api.github.com/users/mc2259/orgs", "repos_url": "https://api.github.com/users/mc2259/repos", "events_url": "https://api.github.com/users/mc2259/events{/privacy}", "received_events_url": "https://api.github.com/users/mc2259/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-07T10:40:53Z", "updated_at": "2020-08-07T14:23:35Z", "closed_at": "2020-08-07T14:23:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n**A link to original question on the forum/Stack Overflow**:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6313", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6313/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6313/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6313/events", "html_url": "https://github.com/huggingface/transformers/issues/6313", "id": 674674583, "node_id": "MDU6SXNzdWU2NzQ2NzQ1ODM=", "number": 6313, "title": "Error trying to import SquadDataset", "user": {"login": "brian8128", "id": 10691563, "node_id": "MDQ6VXNlcjEwNjkxNTYz", "avatar_url": "https://avatars3.githubusercontent.com/u/10691563?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brian8128", "html_url": "https://github.com/brian8128", "followers_url": "https://api.github.com/users/brian8128/followers", "following_url": "https://api.github.com/users/brian8128/following{/other_user}", "gists_url": "https://api.github.com/users/brian8128/gists{/gist_id}", "starred_url": "https://api.github.com/users/brian8128/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brian8128/subscriptions", "organizations_url": "https://api.github.com/users/brian8128/orgs", "repos_url": "https://api.github.com/users/brian8128/repos", "events_url": "https://api.github.com/users/brian8128/events{/privacy}", "received_events_url": "https://api.github.com/users/brian8128/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-07T00:20:07Z", "updated_at": "2020-08-07T20:16:10Z", "closed_at": "2020-08-07T20:16:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n\r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.15.0-108-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.2\r\n- PyTorch version (GPU?): 1.4.0 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in>\r\n\r\n### Who can help\r\n@sgugger @julien-c \r\n \r\n## Information\r\n\r\nI am trying to follow the run_squad_trainer example. However I am unable to import the SquadDataset from transformers. I tried updating to 3.0.2 but got the same error. \r\nhttps://github.com/huggingface/transformers/blob/master/examples/question-answering/run_squad_trainer.py\r\n\r\n```\r\nfrom transformers import SquadDataset\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-6-13f8e9ce9352> in <module>\r\n----> 1 from transformers import SquadDataset\r\n\r\nImportError: cannot import name 'SquadDataset' from 'transformers' (/home/brian/miniconda3/envs/ML38/lib/python3.8/site-packages/transformers/__init__.py)\r\n```\r\n\r\n## Expected behavior\r\n\r\nImport runs without error. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6308", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6308/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6308/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6308/events", "html_url": "https://github.com/huggingface/transformers/issues/6308", "id": 674631919, "node_id": "MDU6SXNzdWU2NzQ2MzE5MTk=", "number": 6308, "title": "Debug flag to `run_language_modeling` triggers error", "user": {"login": "dmlap", "id": 56667, "node_id": "MDQ6VXNlcjU2NjY3", "avatar_url": "https://avatars1.githubusercontent.com/u/56667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmlap", "html_url": "https://github.com/dmlap", "followers_url": "https://api.github.com/users/dmlap/followers", "following_url": "https://api.github.com/users/dmlap/following{/other_user}", "gists_url": "https://api.github.com/users/dmlap/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmlap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmlap/subscriptions", "organizations_url": "https://api.github.com/users/dmlap/orgs", "repos_url": "https://api.github.com/users/dmlap/repos", "events_url": "https://api.github.com/users/dmlap/events{/privacy}", "received_events_url": "https://api.github.com/users/dmlap/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-06T22:15:45Z", "updated_at": "2020-08-11T09:47:02Z", "closed_at": "2020-08-11T09:47:01Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Environment info\r\n\r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.2\r\n- PyTorch version (GPU?): 1.6.0+cu101 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: yes, run_language_modeling.py\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n### Who can help\r\n \r\nI'd guess @sgugger or @julien-c\r\n\r\n## Information\r\nI'm using [run_language_modeling.py](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) and turned on debug output to double check things were working as I expected. Unfortunately, [trainer.py](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L628) keys off that debug option to invoke `xm.master_print(...)` and `xm`/`torch_xla.core.xla_model` isn't loaded because I'm not working on a TPU-based system.\r\n\r\nThe problem arises when using:\r\n* [X] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [X] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nAll steps should be run on a system with a GPU but no TPU. Steps to reproduce the behavior:\r\n\r\n1. Run `run_language_modeling.py` with the debug flag:\r\n```sh\r\npython run_language_modeling.py \\\r\n  --output_dir ./output \\\r\n  --model_type gpt2 \\\r\n  --model_name_or_path gpt2 \\\r\n  --do_train \\\r\n  --train_data_file ./train.txt \\\r\n  --learning_rate 1e-4 \\\r\n  --num_train_epochs 1 \\\r\n  --save_total_limit 2 \\\r\n  --save_steps 200 \\\r\n  --do_eval \\\r\n  --eval_data_file ./eval.txt \\\r\n  --debug\r\n```\r\n2. Allow the script to run.\r\n\r\nThe command will error towards the end with this traceback:\r\n```sh\r\nEpoch:   0%|                                                                                                                                                   | 0/1 [54:06<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"run_language_modeling.py\", line 281, in <module>\r\n    main()\r\n  File \"run_language_modeling.py\", line 245, in main\r\n    trainer.train(model_path=model_path)\r\n  File \"/home/user/project/env/lib/python3.8/site-packages/transformers/trainer.py\", line 570, in train\r\n    xm.master_print(met.metrics_report())\r\nNameError: name 'xm' is not defined\r\n```\r\n## Expected behavior\r\n\r\nThe script exits without error.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6302", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6302/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6302/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6302/events", "html_url": "https://github.com/huggingface/transformers/issues/6302", "id": 674542965, "node_id": "MDU6SXNzdWU2NzQ1NDI5NjU=", "number": 6302, "title": "Default value of `n_tpu_cores` in lightning_base.py", "user": {"login": "xujiaze13", "id": 37360975, "node_id": "MDQ6VXNlcjM3MzYwOTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/37360975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xujiaze13", "html_url": "https://github.com/xujiaze13", "followers_url": "https://api.github.com/users/xujiaze13/followers", "following_url": "https://api.github.com/users/xujiaze13/following{/other_user}", "gists_url": "https://api.github.com/users/xujiaze13/gists{/gist_id}", "starred_url": "https://api.github.com/users/xujiaze13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xujiaze13/subscriptions", "organizations_url": "https://api.github.com/users/xujiaze13/orgs", "repos_url": "https://api.github.com/users/xujiaze13/repos", "events_url": "https://api.github.com/users/xujiaze13/events{/privacy}", "received_events_url": "https://api.github.com/users/xujiaze13/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-06T19:15:36Z", "updated_at": "2020-08-06T19:26:00Z", "closed_at": "2020-08-06T19:26:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/huggingface/transformers/blob/2804fff8393dbda5098b8c9f5e36235e89c50023/examples/lightning_base.py#L294\r\n\r\nThe default setting ``0`` raise Error\r\n``pytorch_lightning.utilities.exceptions.MisconfigurationException: `tpu_cores` can only be 1, 8 or [<1-8>]``\r\nIt should be replaced by ``None``.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6301", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6301/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6301/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6301/events", "html_url": "https://github.com/huggingface/transformers/issues/6301", "id": 674487559, "node_id": "MDU6SXNzdWU2NzQ0ODc1NTk=", "number": 6301, "title": "Redundant code", "user": {"login": "xujiaze13", "id": 37360975, "node_id": "MDQ6VXNlcjM3MzYwOTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/37360975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xujiaze13", "html_url": "https://github.com/xujiaze13", "followers_url": "https://api.github.com/users/xujiaze13/followers", "following_url": "https://api.github.com/users/xujiaze13/following{/other_user}", "gists_url": "https://api.github.com/users/xujiaze13/gists{/gist_id}", "starred_url": "https://api.github.com/users/xujiaze13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xujiaze13/subscriptions", "organizations_url": "https://api.github.com/users/xujiaze13/orgs", "repos_url": "https://api.github.com/users/xujiaze13/repos", "events_url": "https://api.github.com/users/xujiaze13/events{/privacy}", "received_events_url": "https://api.github.com/users/xujiaze13/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1108649053, "node_id": "MDU6TGFiZWwxMTA4NjQ5MDUz", "url": "https://api.github.com/repos/huggingface/transformers/labels/Help%20wanted", "name": "Help wanted", "color": "008672", "default": false, "description": "Extra attention is needed, help appreciated"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-06T17:37:00Z", "updated_at": "2020-08-07T01:18:57Z", "closed_at": "2020-08-07T01:18:57Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/huggingface/transformers/blob/2f2aa0c89cab9a77560e6845578f917a61081c67/examples/text-classification/run_pl_glue.py#L57\r\n\r\nThis line is useless", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6297", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6297/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6297/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6297/events", "html_url": "https://github.com/huggingface/transformers/issues/6297", "id": 674415420, "node_id": "MDU6SXNzdWU2NzQ0MTU0MjA=", "number": 6297, "title": "Question about BERT model size (transformer block number) ", "user": {"login": "ZLKong", "id": 28882362, "node_id": "MDQ6VXNlcjI4ODgyMzYy", "avatar_url": "https://avatars3.githubusercontent.com/u/28882362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZLKong", "html_url": "https://github.com/ZLKong", "followers_url": "https://api.github.com/users/ZLKong/followers", "following_url": "https://api.github.com/users/ZLKong/following{/other_user}", "gists_url": "https://api.github.com/users/ZLKong/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZLKong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZLKong/subscriptions", "organizations_url": "https://api.github.com/users/ZLKong/orgs", "repos_url": "https://api.github.com/users/ZLKong/repos", "events_url": "https://api.github.com/users/ZLKong/events{/privacy}", "received_events_url": "https://api.github.com/users/ZLKong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-06T15:47:34Z", "updated_at": "2020-08-06T16:32:27Z", "closed_at": "2020-08-06T16:32:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\nHi,\r\n\r\nThank you for your interesting work! I have just started to learn BERT and distillation recently. I have some general questions regarding this topic.\r\n\r\n1. I want to compare the performance of BERT with different model size (transformer block number). Is it necessary to do distillation? If I just train a BERT with 6 Layers without distillation, does the performance look bad?\r\n\r\n2. Do you have to do pretraining every time you change the layer number of BERT? Is it possible to just remove some layers in an existing pre-trained model and finetune on tasks?\r\n\r\n3. Why BERT has 12 blocks? Not 11 or 13 etc. ? I couldn't find any explanation.\r\n\r\nThanks,\r\nZLK", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6291", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6291/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6291/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6291/events", "html_url": "https://github.com/huggingface/transformers/issues/6291", "id": 674347603, "node_id": "MDU6SXNzdWU2NzQzNDc2MDM=", "number": 6291, "title": "Why is the lm_head layer in GPT2LMHeadModel not a parameter?", "user": {"login": "bangbangjim", "id": 32017400, "node_id": "MDQ6VXNlcjMyMDE3NDAw", "avatar_url": "https://avatars0.githubusercontent.com/u/32017400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bangbangjim", "html_url": "https://github.com/bangbangjim", "followers_url": "https://api.github.com/users/bangbangjim/followers", "following_url": "https://api.github.com/users/bangbangjim/following{/other_user}", "gists_url": "https://api.github.com/users/bangbangjim/gists{/gist_id}", "starred_url": "https://api.github.com/users/bangbangjim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bangbangjim/subscriptions", "organizations_url": "https://api.github.com/users/bangbangjim/orgs", "repos_url": "https://api.github.com/users/bangbangjim/repos", "events_url": "https://api.github.com/users/bangbangjim/events{/privacy}", "received_events_url": "https://api.github.com/users/bangbangjim/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-06T14:14:12Z", "updated_at": "2020-08-06T16:40:03Z", "closed_at": "2020-08-06T16:40:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I loaded the model by \r\n```\r\nfrom transformers import GPT2LMHeadModel\r\ngpt2 = GPT2LMHeadModel.from_pretrained('distilgpt2')\r\n```\r\ndoing `[n for n,p in gpt2.named_parameters()]` gives me:\r\n```\r\n['gpt2.transformer.wte.weight', 'gpt2.transformer.wpe.weight', 'gpt2.transformer.h.0.ln_1.weight', 'gpt2.transformer.h.0.ln_1.bias', 'gpt2.transformer.h.0.attn.c_attn.weight', 'gpt2.transformer.h.0.attn.c_attn.bias', 'gpt2.transformer.h.0.attn.c_proj.weight', 'gpt2.transformer.h.0.attn.c_proj.bias', 'gpt2.transformer.h.0.ln_2.weight', 'gpt2.transformer.h.0.ln_2.bias', 'gpt2.transformer.h.0.mlp.c_fc.weight', 'gpt2.transformer.h.0.mlp.c_fc.bias', 'gpt2.transformer.h.0.mlp.c_proj.weight', 'gpt2.transformer.h.0.mlp.c_proj.bias', 'gpt2.transformer.h.1.ln_1.weight', 'gpt2.transformer.h.1.ln_1.bias', 'gpt2.transformer.h.1.attn.c_attn.weight', 'gpt2.transformer.h.1.attn.c_attn.bias', 'gpt2.transformer.h.1.attn.c_proj.weight', 'gpt2.transformer.h.1.attn.c_proj.bias', 'gpt2.transformer.h.1.ln_2.weight', 'gpt2.transformer.h.1.ln_2.bias', 'gpt2.transformer.h.1.mlp.c_fc.weight', 'gpt2.transformer.h.1.mlp.c_fc.bias', 'gpt2.transformer.h.1.mlp.c_proj.weight', 'gpt2.transformer.h.1.mlp.c_proj.bias', 'gpt2.transformer.h.2.ln_1.weight', 'gpt2.transformer.h.2.ln_1.bias', 'gpt2.transformer.h.2.attn.c_attn.weight', 'gpt2.transformer.h.2.attn.c_attn.bias', 'gpt2.transformer.h.2.attn.c_proj.weight', 'gpt2.transformer.h.2.attn.c_proj.bias', 'gpt2.transformer.h.2.ln_2.weight', 'gpt2.transformer.h.2.ln_2.bias', 'gpt2.transformer.h.2.mlp.c_fc.weight', 'gpt2.transformer.h.2.mlp.c_fc.bias', 'gpt2.transformer.h.2.mlp.c_proj.weight', 'gpt2.transformer.h.2.mlp.c_proj.bias', 'gpt2.transformer.h.3.ln_1.weight', 'gpt2.transformer.h.3.ln_1.bias', 'gpt2.transformer.h.3.attn.c_attn.weight', 'gpt2.transformer.h.3.attn.c_attn.bias', 'gpt2.transformer.h.3.attn.c_proj.weight', 'gpt2.transformer.h.3.attn.c_proj.bias', 'gpt2.transformer.h.3.ln_2.weight', 'gpt2.transformer.h.3.ln_2.bias', 'gpt2.transformer.h.3.mlp.c_fc.weight', 'gpt2.transformer.h.3.mlp.c_fc.bias', 'gpt2.transformer.h.3.mlp.c_proj.weight', 'gpt2.transformer.h.3.mlp.c_proj.bias', 'gpt2.transformer.h.4.ln_1.weight', 'gpt2.transformer.h.4.ln_1.bias', 'gpt2.transformer.h.4.attn.c_attn.weight', 'gpt2.transformer.h.4.attn.c_attn.bias', 'gpt2.transformer.h.4.attn.c_proj.weight', 'gpt2.transformer.h.4.attn.c_proj.bias', 'gpt2.transformer.h.4.ln_2.weight', 'gpt2.transformer.h.4.ln_2.bias', 'gpt2.transformer.h.4.mlp.c_fc.weight', 'gpt2.transformer.h.4.mlp.c_fc.bias', 'gpt2.transformer.h.4.mlp.c_proj.weight', 'gpt2.transformer.h.4.mlp.c_proj.bias', 'gpt2.transformer.h.5.ln_1.weight', 'gpt2.transformer.h.5.ln_1.bias', 'gpt2.transformer.h.5.attn.c_attn.weight', 'gpt2.transformer.h.5.attn.c_attn.bias', 'gpt2.transformer.h.5.attn.c_proj.weight', 'gpt2.transformer.h.5.attn.c_proj.bias', 'gpt2.transformer.h.5.ln_2.weight', 'gpt2.transformer.h.5.ln_2.bias', 'gpt2.transformer.h.5.mlp.c_fc.weight', 'gpt2.transformer.h.5.mlp.c_fc.bias', 'gpt2.transformer.h.5.mlp.c_proj.weight', 'gpt2.transformer.h.5.mlp.c_proj.bias', 'gpt2.transformer.ln_f.weight', 'gpt2.transformer.ln_f.bias']\r\n```\r\nwhile running `print (gpt2)` gives me:\r\n```\r\nGPT2LMHeadModel(\r\n  (transformer): GPT2Model(\r\n    (wte): Embedding(50257, 768)\r\n    (wpe): Embedding(1024, 768)\r\n    (drop): Dropout(p=0.1, inplace=False)\r\n    (h): ModuleList(\r\n      (0): Block(\r\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (attn): Attention(\r\n          (c_attn): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (attn_dropout): Dropout(p=0.1, inplace=False)\r\n          (resid_dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (mlp): MLP(\r\n          (c_fc): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n      )\r\n      (1): Block(\r\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (attn): Attention(\r\n          (c_attn): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (attn_dropout): Dropout(p=0.1, inplace=False)\r\n          (resid_dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (mlp): MLP(\r\n          (c_fc): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n      )\r\n      (2): Block(\r\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (attn): Attention(\r\n          (c_attn): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (attn_dropout): Dropout(p=0.1, inplace=False)\r\n          (resid_dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (mlp): MLP(\r\n          (c_fc): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n      )\r\n      (3): Block(\r\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (attn): Attention(\r\n          (c_attn): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (attn_dropout): Dropout(p=0.1, inplace=False)\r\n          (resid_dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (mlp): MLP(\r\n          (c_fc): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n      )\r\n      (4): Block(\r\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (attn): Attention(\r\n          (c_attn): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (attn_dropout): Dropout(p=0.1, inplace=False)\r\n          (resid_dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (mlp): MLP(\r\n          (c_fc): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n      )\r\n      (5): Block(\r\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (attn): Attention(\r\n          (c_attn): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (attn_dropout): Dropout(p=0.1, inplace=False)\r\n          (resid_dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n        (mlp): MLP(\r\n          (c_fc): Conv1D()\r\n          (c_proj): Conv1D()\r\n          (dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n      )\r\n    )\r\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\r\n)\r\n```\r\n\r\nMy question is why is the lm_head layer not included as the model's parameters? It bothers me at the moment because I am trying to only finetune the LM layer and realise I cant because doing something like `torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=lr, eps=1e-08)` will results in an error as the parameter list is empty\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6279", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6279/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6279/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6279/events", "html_url": "https://github.com/huggingface/transformers/issues/6279", "id": 673982480, "node_id": "MDU6SXNzdWU2NzM5ODI0ODA=", "number": 6279, "title": "TFRobertaMarkedLM model output issue", "user": {"login": "yxu02", "id": 31361743, "node_id": "MDQ6VXNlcjMxMzYxNzQz", "avatar_url": "https://avatars3.githubusercontent.com/u/31361743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yxu02", "html_url": "https://github.com/yxu02", "followers_url": "https://api.github.com/users/yxu02/followers", "following_url": "https://api.github.com/users/yxu02/following{/other_user}", "gists_url": "https://api.github.com/users/yxu02/gists{/gist_id}", "starred_url": "https://api.github.com/users/yxu02/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yxu02/subscriptions", "organizations_url": "https://api.github.com/users/yxu02/orgs", "repos_url": "https://api.github.com/users/yxu02/repos", "events_url": "https://api.github.com/users/yxu02/events{/privacy}", "received_events_url": "https://api.github.com/users/yxu02/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-06T03:17:56Z", "updated_at": "2020-08-06T16:56:22Z", "closed_at": "2020-08-06T16:56:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\nI fine tuned a TFRobertaMarkedLM thanks to this notebook:\r\nhttps://www.kaggle.com/riblidezso/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm\r\n\r\nMy model config is like this:\r\n{\r\n  \"architectures\": [\r\n    \"RobertaForMaskedLM\"\r\n  ],\r\n  \"attention_probs_dropout_prob\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"eos_token_id\": 2,\r\n  \"gradient_checkpointing\": false,\r\n  \"hidden_act\": \"gelu\",\r\n  \"hidden_dropout_prob\": 0.1,\r\n  \"hidden_size\": 768,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 3072,\r\n  \"layer_norm_eps\": 1e-05,\r\n  \"max_position_embeddings\": 514,\r\n  \"model_type\": \"roberta\",\r\n  \"num_attention_heads\": 12,\r\n  \"num_hidden_layers\": 12,\r\n  \"pad_token_id\": 1,\r\n  \"type_vocab_size\": 1,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\nI found my model output shape is **[n, sent_max_len, vocb_size]**, whereas I'd like to get **[n, sent_max_len, emb_size]**. \r\n\r\nIs it because I should not use model.predict(test_sent)[0]? Thanks!\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n**A link to original question on the forum/Stack Overflow**:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6278", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6278/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6278/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6278/events", "html_url": "https://github.com/huggingface/transformers/issues/6278", "id": 673956215, "node_id": "MDU6SXNzdWU2NzM5NTYyMTU=", "number": 6278, "title": "Returning the attention heads using Longformer", "user": {"login": "codeninja", "id": 14914, "node_id": "MDQ6VXNlcjE0OTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/14914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/codeninja", "html_url": "https://github.com/codeninja", "followers_url": "https://api.github.com/users/codeninja/followers", "following_url": "https://api.github.com/users/codeninja/following{/other_user}", "gists_url": "https://api.github.com/users/codeninja/gists{/gist_id}", "starred_url": "https://api.github.com/users/codeninja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/codeninja/subscriptions", "organizations_url": "https://api.github.com/users/codeninja/orgs", "repos_url": "https://api.github.com/users/codeninja/repos", "events_url": "https://api.github.com/users/codeninja/events{/privacy}", "received_events_url": "https://api.github.com/users/codeninja/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-06T01:55:30Z", "updated_at": "2020-08-06T01:55:54Z", "closed_at": "2020-08-06T01:55:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\nIs it possible to get the longformer classification model to return the attentions learned? I don't see a way to do this with in the documentation or code.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6277", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6277/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6277/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6277/events", "html_url": "https://github.com/huggingface/transformers/issues/6277", "id": 673872255, "node_id": "MDU6SXNzdWU2NzM4NzIyNTU=", "number": 6277, "title": "Reformer now requires PyTorch 1.6.0", "user": {"login": "sgugger", "id": 35901082, "node_id": "MDQ6VXNlcjM1OTAxMDgy", "avatar_url": "https://avatars3.githubusercontent.com/u/35901082?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgugger", "html_url": "https://github.com/sgugger", "followers_url": "https://api.github.com/users/sgugger/followers", "following_url": "https://api.github.com/users/sgugger/following{/other_user}", "gists_url": "https://api.github.com/users/sgugger/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgugger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgugger/subscriptions", "organizations_url": "https://api.github.com/users/sgugger/orgs", "repos_url": "https://api.github.com/users/sgugger/repos", "events_url": "https://api.github.com/users/sgugger/events{/privacy}", "received_events_url": "https://api.github.com/users/sgugger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-05T21:56:09Z", "updated_at": "2020-08-06T19:14:46Z", "closed_at": "2020-08-06T19:14:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "@patrickvonplaten, one of your recent PR (#6244) on Reformer introduces a dep on PyTorch 1.6.0 minimum by using `torch.cuda.default_generators`. We should see if we can find a way to work around this.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6275", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6275/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6275/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6275/events", "html_url": "https://github.com/huggingface/transformers/issues/6275", "id": 673715204, "node_id": "MDU6SXNzdWU2NzM3MTUyMDQ=", "number": 6275, "title": "How to access the parameters of the uppermost layer of the HuggingFace Transformers via \".modules()\"?", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-05T17:19:02Z", "updated_at": "2020-08-08T18:22:38Z", "closed_at": "2020-08-07T18:35:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI would like to apply the function `module.PyroSample()` to the parameters that pertains to the 24th layer (the uppermost layer) of the `RobertaForMultipleChoice` pre-trained model (`roberta-large`). How should I fix the loop below so that I only fix the parameters that are from the 24th layer? Currently, the loop applies `module.PyroSample()` to every parameter except the `_dummy_param`.\r\nThank you,\r\n\r\n```python\r\nfor m in model_RobertaForMultipleChoice.modules():\r\n     for name, value in list(m.named_parameters(recurse=False)):\r\n         if name != \"_dummy_param\":\r\n                    setattr(m, name, module.PyroSample(prior=dist.Normal(0, 1)\r\n                                              .expand(value.shape)\r\n                                              .to_event(value.dim()))) ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6267", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6267/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6267/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6267/events", "html_url": "https://github.com/huggingface/transformers/issues/6267", "id": 673525932, "node_id": "MDU6SXNzdWU2NzM1MjU5MzI=", "number": 6267, "title": "Unable to make inference from hosted api for a pretrained model that I uploaded. ", "user": {"login": "JME-P", "id": 55997171, "node_id": "MDQ6VXNlcjU1OTk3MTcx", "avatar_url": "https://avatars2.githubusercontent.com/u/55997171?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JME-P", "html_url": "https://github.com/JME-P", "followers_url": "https://api.github.com/users/JME-P/followers", "following_url": "https://api.github.com/users/JME-P/following{/other_user}", "gists_url": "https://api.github.com/users/JME-P/gists{/gist_id}", "starred_url": "https://api.github.com/users/JME-P/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JME-P/subscriptions", "organizations_url": "https://api.github.com/users/JME-P/orgs", "repos_url": "https://api.github.com/users/JME-P/repos", "events_url": "https://api.github.com/users/JME-P/events{/privacy}", "received_events_url": "https://api.github.com/users/JME-P/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "julien-c", "id": 326577, "node_id": "MDQ6VXNlcjMyNjU3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/326577?v=4", "gravatar_id": "", "url": "https://api.github.com/users/julien-c", "html_url": "https://github.com/julien-c", "followers_url": "https://api.github.com/users/julien-c/followers", "following_url": "https://api.github.com/users/julien-c/following{/other_user}", "gists_url": "https://api.github.com/users/julien-c/gists{/gist_id}", "starred_url": "https://api.github.com/users/julien-c/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/julien-c/subscriptions", "organizations_url": "https://api.github.com/users/julien-c/orgs", "repos_url": "https://api.github.com/users/julien-c/repos", "events_url": "https://api.github.com/users/julien-c/events{/privacy}", "received_events_url": "https://api.github.com/users/julien-c/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "julien-c", "id": 326577, "node_id": "MDQ6VXNlcjMyNjU3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/326577?v=4", "gravatar_id": "", "url": "https://api.github.com/users/julien-c", "html_url": "https://github.com/julien-c", "followers_url": "https://api.github.com/users/julien-c/followers", "following_url": "https://api.github.com/users/julien-c/following{/other_user}", "gists_url": "https://api.github.com/users/julien-c/gists{/gist_id}", "starred_url": "https://api.github.com/users/julien-c/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/julien-c/subscriptions", "organizations_url": "https://api.github.com/users/julien-c/orgs", "repos_url": "https://api.github.com/users/julien-c/repos", "events_url": "https://api.github.com/users/julien-c/events{/privacy}", "received_events_url": "https://api.github.com/users/julien-c/received_events", "type": "User", "site_admin": false}, {"login": "LysandreJik", "id": 30755778, "node_id": "MDQ6VXNlcjMwNzU1Nzc4", "avatar_url": "https://avatars1.githubusercontent.com/u/30755778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LysandreJik", "html_url": "https://github.com/LysandreJik", "followers_url": "https://api.github.com/users/LysandreJik/followers", "following_url": "https://api.github.com/users/LysandreJik/following{/other_user}", "gists_url": "https://api.github.com/users/LysandreJik/gists{/gist_id}", "starred_url": "https://api.github.com/users/LysandreJik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LysandreJik/subscriptions", "organizations_url": "https://api.github.com/users/LysandreJik/orgs", "repos_url": "https://api.github.com/users/LysandreJik/repos", "events_url": "https://api.github.com/users/LysandreJik/events{/privacy}", "received_events_url": "https://api.github.com/users/LysandreJik/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-05T13:07:47Z", "updated_at": "2020-08-07T08:26:00Z", "closed_at": "2020-08-05T15:10:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I have successfully managed to upload a model (https://huggingface.co/shrugging-grace/tweetclassifier) via the transformers cli. I am also able to generate inferences from a local Jupyter notebook to that model.\r\n\r\nHowever, when I go on the https://huggingface.co/shrugging-grace/tweetclassifier, it comes up with the following error when I try to make an inference:\r\n\r\n_\"Can't load config for 'shrugging-grace/tweetclassifier'. Make sure that: - 'shrugging-grace/tweetclassifier' is a correct model identifier listed on 'https://huggingface.co/models' - or 'shrugging-grace/tweetclassifier' is the correct path to a directory containing a config.json file\"_\r\n\r\nHowever:\r\n- shrugging-grace/tweetclassifier seems to be the correct model identifier, and\r\n- the JSON file seems to be present (https://s3.amazonaws.com/models.huggingface.co/bert/shrugging-grace/tweetclassifier/config.json) and working correctly when I make the inference from my local machine. \r\n\r\nPlease could someone assist me in understanding how to get the hosted api (on https://huggingface.co/shrugging-grace/tweetclassifier) to work correctly ? Potentially one for: @LysandreJik  \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6263", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6263/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6263/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6263/events", "html_url": "https://github.com/huggingface/transformers/issues/6263", "id": 673317977, "node_id": "MDU6SXNzdWU2NzMzMTc5Nzc=", "number": 6263, "title": "RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`", "user": {"login": "mt324010", "id": 35977320, "node_id": "MDQ6VXNlcjM1OTc3MzIw", "avatar_url": "https://avatars2.githubusercontent.com/u/35977320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mt324010", "html_url": "https://github.com/mt324010", "followers_url": "https://api.github.com/users/mt324010/followers", "following_url": "https://api.github.com/users/mt324010/following{/other_user}", "gists_url": "https://api.github.com/users/mt324010/gists{/gist_id}", "starred_url": "https://api.github.com/users/mt324010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mt324010/subscriptions", "organizations_url": "https://api.github.com/users/mt324010/orgs", "repos_url": "https://api.github.com/users/mt324010/repos", "events_url": "https://api.github.com/users/mt324010/events{/privacy}", "received_events_url": "https://api.github.com/users/mt324010/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-05T07:24:25Z", "updated_at": "2020-08-14T15:29:42Z", "closed_at": "2020-08-05T07:51:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I tried to add some other embeddings in your BertEmbedding source code and then load the pretrained weights 'bert-base-chinese'. \r\nWhen I run the forward method, I got the issue\r\n'RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`'\r\nCan someone help please? Thanks a lot", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6262", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6262/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6262/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6262/events", "html_url": "https://github.com/huggingface/transformers/issues/6262", "id": 673295998, "node_id": "MDU6SXNzdWU2NzMyOTU5OTg=", "number": 6262, "title": "Incorrect tokenization for MarianNMT models in example script.", "user": {"login": "yvespeirsman", "id": 3431621, "node_id": "MDQ6VXNlcjM0MzE2MjE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3431621?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yvespeirsman", "html_url": "https://github.com/yvespeirsman", "followers_url": "https://api.github.com/users/yvespeirsman/followers", "following_url": "https://api.github.com/users/yvespeirsman/following{/other_user}", "gists_url": "https://api.github.com/users/yvespeirsman/gists{/gist_id}", "starred_url": "https://api.github.com/users/yvespeirsman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yvespeirsman/subscriptions", "organizations_url": "https://api.github.com/users/yvespeirsman/orgs", "repos_url": "https://api.github.com/users/yvespeirsman/repos", "events_url": "https://api.github.com/users/yvespeirsman/events{/privacy}", "received_events_url": "https://api.github.com/users/yvespeirsman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-08-05T06:44:11Z", "updated_at": "2020-08-06T18:58:39Z", "closed_at": "2020-08-06T18:58:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.4.0-1110-aws-x86_64-with-Ubuntu-16.04-xenial\r\n- Python version: 3.7.8\r\n- PyTorch version (GPU?): 1.5.1+cu101 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: yes\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n### Who can help\r\n Marian: @sshleifer \r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...): Marian (Helsinki-NLP/opus-mt-nl-fr)\r\n\r\nThe problem arises when using:\r\n* [x] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [x] my own task or dataset: (give details below)\r\n\r\nI'm trying to finetune a MarianMT model (Helsinki-NLP/opus-mt-nl-fr), using the example finetuning script `examples/seq2seq/finetune.py`. Marian models have different sentencepiece models for the encoder and decoder, but it appears the script does not take this into account, as the source line and the target line are tokenized in exactly the same way (in `utils.py`, lines 115-116):\r\n\r\n```\r\n        source_inputs = encode_line(self.tokenizer, source_line, self.max_source_length)\r\n        target_inputs = encode_line(self.tokenizer, tgt_line, self.max_target_length)\r\n```\r\n\r\nI suggest checking whether the tokenizer is a Marian tokenizer and if so, encoding with the tokenizer's `prepare_translation_batch` method. Doing this improves BLEU score on my validation data by >3 points.\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Simply run the `examples/seq2seq/finetune.sh` with a MarianModel.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6240", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6240/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6240/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6240/events", "html_url": "https://github.com/huggingface/transformers/issues/6240", "id": 672850875, "node_id": "MDU6SXNzdWU2NzI4NTA4NzU=", "number": 6240, "title": "Documentation bug in GPT2Config", "user": {"login": "miggymigz", "id": 6831138, "node_id": "MDQ6VXNlcjY4MzExMzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6831138?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miggymigz", "html_url": "https://github.com/miggymigz", "followers_url": "https://api.github.com/users/miggymigz/followers", "following_url": "https://api.github.com/users/miggymigz/following{/other_user}", "gists_url": "https://api.github.com/users/miggymigz/gists{/gist_id}", "starred_url": "https://api.github.com/users/miggymigz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miggymigz/subscriptions", "organizations_url": "https://api.github.com/users/miggymigz/orgs", "repos_url": "https://api.github.com/users/miggymigz/repos", "events_url": "https://api.github.com/users/miggymigz/events{/privacy}", "received_events_url": "https://api.github.com/users/miggymigz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-04T14:47:17Z", "updated_at": "2020-08-08T18:37:29Z", "closed_at": "2020-08-08T18:37:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "In [this page](https://huggingface.co/transformers/model_doc/gpt2.html), GPT2Config's initializer_range default value is 16. But the source code [says](https://github.com/huggingface/transformers/blob/master/src/transformers/configuration_gpt2.py#L130) it is 0.02.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6238", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6238/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6238/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6238/events", "html_url": "https://github.com/huggingface/transformers/issues/6238", "id": 672771917, "node_id": "MDU6SXNzdWU2NzI3NzE5MTc=", "number": 6238, "title": "Discrepancy in the pad_token_id between the tokenizer and the model code of the T5", "user": {"login": "eyal-str", "id": 3134190, "node_id": "MDQ6VXNlcjMxMzQxOTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/3134190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eyal-str", "html_url": "https://github.com/eyal-str", "followers_url": "https://api.github.com/users/eyal-str/followers", "following_url": "https://api.github.com/users/eyal-str/following{/other_user}", "gists_url": "https://api.github.com/users/eyal-str/gists{/gist_id}", "starred_url": "https://api.github.com/users/eyal-str/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eyal-str/subscriptions", "organizations_url": "https://api.github.com/users/eyal-str/orgs", "repos_url": "https://api.github.com/users/eyal-str/repos", "events_url": "https://api.github.com/users/eyal-str/events{/privacy}", "received_events_url": "https://api.github.com/users/eyal-str/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-04T12:56:46Z", "updated_at": "2020-08-04T18:43:19Z", "closed_at": "2020-08-04T18:43:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "- `transformers` version: 2.11.0\r\n- Platform: Darwin-19.3.0-x86_64-i386-64bit\r\n- Python version: 3.7.6\r\n\r\n tokenizers: @mfuntowicz\r\n T5: @patrickvonplaten\r\n\r\nIt seems that there's a discrepancy between the tokenizer and the model code of the T5 regarding the pad_token_id.\r\nlooking at the following output the pad_token_id is 0:\r\n```\r\nfrom transformers import T5Tokenizer\r\ntokenizer = T5Tokenizer.from_pretrained('t5-large')\r\ntokenizer.pad_token_id\r\nOut[4]: 0\r\n```\r\n\r\nWhen calculating the loss. I would expect the calculation to ignore the paddings, but when looking at the code, it ignores token_id = -100.\r\nmodeling_t5.py:\r\n```\r\n        if lm_labels is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-100)\r\n            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\r\n            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\r\n            decoder_outputs = (loss,) + decoder_outputs\r\n```\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6226", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6226/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6226/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6226/events", "html_url": "https://github.com/huggingface/transformers/issues/6226", "id": 672330181, "node_id": "MDU6SXNzdWU2NzIzMzAxODE=", "number": 6226, "title": "Can't load config for [community model]", "user": {"login": "abedkhooli", "id": 11407254, "node_id": "MDQ6VXNlcjExNDA3MjU0", "avatar_url": "https://avatars2.githubusercontent.com/u/11407254?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abedkhooli", "html_url": "https://github.com/abedkhooli", "followers_url": "https://api.github.com/users/abedkhooli/followers", "following_url": "https://api.github.com/users/abedkhooli/following{/other_user}", "gists_url": "https://api.github.com/users/abedkhooli/gists{/gist_id}", "starred_url": "https://api.github.com/users/abedkhooli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abedkhooli/subscriptions", "organizations_url": "https://api.github.com/users/abedkhooli/orgs", "repos_url": "https://api.github.com/users/abedkhooli/repos", "events_url": "https://api.github.com/users/abedkhooli/events{/privacy}", "received_events_url": "https://api.github.com/users/abedkhooli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-03T20:40:43Z", "updated_at": "2020-08-19T20:17:06Z", "closed_at": "2020-08-05T15:27:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Although I can use a fine-tuned GPT2 model from code, the model page complains about the config file (which is already uploaded). \r\nat https://huggingface.co/akhooli/gpt2-small-arabic-poetry (for a prompt), I get: \r\n```\r\nCan't load config for 'akhooli/gpt2-small-arabic-poetry'. Make sure that: - 'akhooli/gpt2-small-arabic-poetry' is a correct model identifier listed on 'https://huggingface.co/models' - or 'akhooli/gpt2-small-arabic-poetry' is the correct path to a directory containing a config.json file \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6211", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6211/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6211/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6211/events", "html_url": "https://github.com/huggingface/transformers/issues/6211", "id": 672036196, "node_id": "MDU6SXNzdWU2NzIwMzYxOTY=", "number": 6211, "title": "Error when fine tuning GPT2 on GPU ", "user": {"login": "antocapp", "id": 26765504, "node_id": "MDQ6VXNlcjI2NzY1NTA0", "avatar_url": "https://avatars0.githubusercontent.com/u/26765504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/antocapp", "html_url": "https://github.com/antocapp", "followers_url": "https://api.github.com/users/antocapp/followers", "following_url": "https://api.github.com/users/antocapp/following{/other_user}", "gists_url": "https://api.github.com/users/antocapp/gists{/gist_id}", "starred_url": "https://api.github.com/users/antocapp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/antocapp/subscriptions", "organizations_url": "https://api.github.com/users/antocapp/orgs", "repos_url": "https://api.github.com/users/antocapp/repos", "events_url": "https://api.github.com/users/antocapp/events{/privacy}", "received_events_url": "https://api.github.com/users/antocapp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-03T12:19:09Z", "updated_at": "2020-08-03T15:09:24Z", "closed_at": "2020-08-03T14:19:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "An error occur when training with `run_language_modeling.py` file (Ubuntu 18, pytorch with cuda compiled), while the error does not occur on Macbook without the GPU\r\n\r\n```\r\n  File \"train.py\", line 283, in <module>\r\n    main()\r\n  File \"train.py\", line 247, in main\r\n    trainer.train(model_path=model_path)\r\n  File \"/home/antonio/anaconda3/lib/python3.7/site-packages/transformers/trainer.py\", line 518, in train\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"/home/antonio/anaconda3/lib/python3.7/site-packages/tqdm/_tqdm.py\", line 1017, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/antonio/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 346, in __next__\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/home/antonio/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\r\n    return self.collate_fn(data)\r\n  File \"/home/antonio/anaconda3/lib/python3.7/site-packages/transformers/data/data_collator.py\", line 90, in __call__\r\n    labels[labels == self.tokenizer.pad_token_id] = -100\r\nTypeError: eq() received an invalid combination of arguments - got (NoneType), but expected one of:\r\n * (Tensor other)\r\n      didn't match because some of the arguments have invalid types: (NoneType)\r\n * (Number other)\r\n      didn't match because some of the arguments have invalid types: (NoneType)\r\n```\r\n\r\nCommand and arguments used to run:\r\n```bash\r\npython train.py \\\r\n    --output_dir ckpts/prova \\\r\n    --model_type=gpt2 \\\r\n    --model_name_or_path=gpt2 \\\r\n    --do_train \\\r\n    --train_data_file data.train \\\r\n    --num_train_epochs 1 \\\r\n    --per_device_train_batch_size 1 \\\r\n    --gradient_accumulation_steps 1 \\\r\n    --save_steps 200\r\n```\r\n\r\nAny idea on how to solve this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6206", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6206/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6206/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6206/events", "html_url": "https://github.com/huggingface/transformers/issues/6206", "id": 671947240, "node_id": "MDU6SXNzdWU2NzE5NDcyNDA=", "number": 6206, "title": "Error while saving electra model in tensorflow \"savedModel\" format", "user": {"login": "nirajkale", "id": 40765055, "node_id": "MDQ6VXNlcjQwNzY1MDU1", "avatar_url": "https://avatars3.githubusercontent.com/u/40765055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nirajkale", "html_url": "https://github.com/nirajkale", "followers_url": "https://api.github.com/users/nirajkale/followers", "following_url": "https://api.github.com/users/nirajkale/following{/other_user}", "gists_url": "https://api.github.com/users/nirajkale/gists{/gist_id}", "starred_url": "https://api.github.com/users/nirajkale/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nirajkale/subscriptions", "organizations_url": "https://api.github.com/users/nirajkale/orgs", "repos_url": "https://api.github.com/users/nirajkale/repos", "events_url": "https://api.github.com/users/nirajkale/events{/privacy}", "received_events_url": "https://api.github.com/users/nirajkale/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "LysandreJik", "id": 30755778, "node_id": "MDQ6VXNlcjMwNzU1Nzc4", "avatar_url": "https://avatars1.githubusercontent.com/u/30755778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LysandreJik", "html_url": "https://github.com/LysandreJik", "followers_url": "https://api.github.com/users/LysandreJik/followers", "following_url": "https://api.github.com/users/LysandreJik/following{/other_user}", "gists_url": "https://api.github.com/users/LysandreJik/gists{/gist_id}", "starred_url": "https://api.github.com/users/LysandreJik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LysandreJik/subscriptions", "organizations_url": "https://api.github.com/users/LysandreJik/orgs", "repos_url": "https://api.github.com/users/LysandreJik/repos", "events_url": "https://api.github.com/users/LysandreJik/events{/privacy}", "received_events_url": "https://api.github.com/users/LysandreJik/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "LysandreJik", "id": 30755778, "node_id": "MDQ6VXNlcjMwNzU1Nzc4", "avatar_url": "https://avatars1.githubusercontent.com/u/30755778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LysandreJik", "html_url": "https://github.com/LysandreJik", "followers_url": "https://api.github.com/users/LysandreJik/followers", "following_url": "https://api.github.com/users/LysandreJik/following{/other_user}", "gists_url": "https://api.github.com/users/LysandreJik/gists{/gist_id}", "starred_url": "https://api.github.com/users/LysandreJik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LysandreJik/subscriptions", "organizations_url": "https://api.github.com/users/LysandreJik/orgs", "repos_url": "https://api.github.com/users/LysandreJik/repos", "events_url": "https://api.github.com/users/LysandreJik/events{/privacy}", "received_events_url": "https://api.github.com/users/LysandreJik/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-08-03T09:45:10Z", "updated_at": "2020-08-03T19:02:23Z", "closed_at": "2020-08-03T19:02:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-5.3.0-1034-azure-x86_64-with-debian-buster-sid\r\n- Python version: 3.6.10\r\n- PyTorch version (GPU?): 1.5.1 (True)\r\n- Tensorflow version (GPU?): 2.0.0 (True)\r\n- Using GPU in script?: Yes (Tesla P40)\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n tensorflow: @jplu \r\n\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\n -->\r\n\r\n## Information\r\n\r\nModel I am using \"Electra\" with TFElectraForQuestionAnswering:\r\n\r\nThe tasks I am working on is:\r\nI'm trying to use [Pre-trained electra-large](https://huggingface.co/ahotrod/electra_large_discriminator_squad2_512) on squad 2.0 dataset.\r\nThe model seems to be working fine. The problem arises when I treat it as a Keras model & \r\ntry to save it in tensorflow's binary/SavedModel format.\r\nThe reason I'm using SavedModel format is because I want to host the model using tensorflow model server.\r\n\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.Load the model with TFElectraForQuestionAnswering\r\n2.Add input layers \r\n3.save the model\r\n```\r\n\r\nfrom transformers import BertTokenizer, TFElectraForQuestionAnswering\r\nimport tensorflow as tf\r\n\r\nmodel_name_or_path= 'ahotrod/electra_large_discriminator_squad2_512'\r\nmax_len = None\r\n\r\ntokenizer = BertTokenizer.from_pretrained(model_name_or_path, cache_dir='transformers')\r\nmodel = TFElectraForQuestionAnswering.from_pretrained(model_name_or_path, cache_dir='transformers')\r\n\r\ninput_ids = tf.keras.layers.Input(shape=(max_len,), name='input_ids', dtype='int32')\r\nattention_mask = tf.keras.layers.Input(shape=(max_len,), name='attention_mask', dtype='int32')\r\ntoken_type_ids = tf.keras.layers.Input(shape=(max_len,), name='token_type_ids', dtype='int32')\r\nkeras_input = [input_ids, attention_mask, token_type_ids]\r\n\r\nqa_output = model(keras_input)\r\nkeras_model = tf.keras.Model(inputs= keras_input, outputs = qa_output)\r\nprint(keras_model.summary())\r\n\r\nkeras_model.save(r'exported/electra_large/0011')\r\n```\r\n\r\nWhen I run above snippet it gives error:\r\n\r\n`Exception has occurred: TypeError\r\nExpected Operation, Variable, or Tensor, got None\r\n  File \"/datadrive/users/niraj/qa/export_electra.py\", line 31, in <module>\r\n    keras_model.save(r'exported/electra_large/0011')`\r\n\r\n## Expected behavior\r\nNot able to figure out the reason for this but above snipper works if i use bert-large or any other model instead of electra. Probably something wrong with modeling scripts for electra ?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6193", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6193/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6193/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6193/events", "html_url": "https://github.com/huggingface/transformers/issues/6193", "id": 671280841, "node_id": "MDU6SXNzdWU2NzEyODA4NDE=", "number": 6193, "title": "Some weights not initialized in pre-trained RobertaForMaskedLM", "user": {"login": "HarshTrivedi", "id": 3285313, "node_id": "MDQ6VXNlcjMyODUzMTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3285313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HarshTrivedi", "html_url": "https://github.com/HarshTrivedi", "followers_url": "https://api.github.com/users/HarshTrivedi/followers", "following_url": "https://api.github.com/users/HarshTrivedi/following{/other_user}", "gists_url": "https://api.github.com/users/HarshTrivedi/gists{/gist_id}", "starred_url": "https://api.github.com/users/HarshTrivedi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HarshTrivedi/subscriptions", "organizations_url": "https://api.github.com/users/HarshTrivedi/orgs", "repos_url": "https://api.github.com/users/HarshTrivedi/repos", "events_url": "https://api.github.com/users/HarshTrivedi/events{/privacy}", "received_events_url": "https://api.github.com/users/HarshTrivedi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-01T23:07:00Z", "updated_at": "2020-08-03T14:13:05Z", "closed_at": "2020-08-03T14:11:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "The bug is similar to #2202.\r\n\r\nI am trying to evaluate MLM perplexity (without training/finetuning) using Roberta with `run_language_modeling.py` (from the [official example](https://github.com/huggingface/transformers/tree/master/examples/language-modeling)). However, some weights seems to be reinitialized instead of getting loading from the pretrained Roberta checkpoint.\r\n\r\n## To Reproduce (~~with master branch~~):\r\n\r\n```\r\nimport logging\r\nlogging.basicConfig(level=logging.INFO)\r\nfrom transformers import RobertaForMaskedLM\r\n_ = RobertaForMaskedLM.from_pretrained('roberta-base')\r\n```\r\n\r\nIt gives the following warning message:\r\n```\r\nWARNING:transformers.modeling_utils:Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'lm_head.decoder.bias']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n```\r\n\r\nThe perplexities I get on direct evaluation on Wikitext-2/103 datasets are also much higher than the official Roberta implementation from fairseq. I suspect this could be the reason.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6192", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6192/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6192/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6192/events", "html_url": "https://github.com/huggingface/transformers/issues/6192", "id": 671112086, "node_id": "MDU6SXNzdWU2NzExMTIwODY=", "number": 6192, "title": "GPT2 crashing at loss.backward()", "user": {"login": "vibhavagarwal5", "id": 23319631, "node_id": "MDQ6VXNlcjIzMzE5NjMx", "avatar_url": "https://avatars1.githubusercontent.com/u/23319631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vibhavagarwal5", "html_url": "https://github.com/vibhavagarwal5", "followers_url": "https://api.github.com/users/vibhavagarwal5/followers", "following_url": "https://api.github.com/users/vibhavagarwal5/following{/other_user}", "gists_url": "https://api.github.com/users/vibhavagarwal5/gists{/gist_id}", "starred_url": "https://api.github.com/users/vibhavagarwal5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vibhavagarwal5/subscriptions", "organizations_url": "https://api.github.com/users/vibhavagarwal5/orgs", "repos_url": "https://api.github.com/users/vibhavagarwal5/repos", "events_url": "https://api.github.com/users/vibhavagarwal5/events{/privacy}", "received_events_url": "https://api.github.com/users/vibhavagarwal5/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 20, "created_at": "2020-08-01T19:01:41Z", "updated_at": "2020-08-03T09:51:53Z", "closed_at": "2020-08-03T08:56:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Ubuntu\r\n- Python version: 3.6\r\n- PyTorch version (GPU?): 1.5.0\r\n- Using GPU in script?: Yes\r\n- Using distributed or parallel set-up in script?: Yes\r\n\r\n@LysandreJik \r\n\r\n## Information\r\n\r\nTrying to finetune GPT2 model but the GPU is crashing after `loss.backward()`. I thought it might be just my code but I ran some different code involving finetuning GPT2 and that as well crashed in the same manner. \r\n\r\nGetting this warning as well. \r\n```\r\nWARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n``` \r\n\r\nA week or 2 back, everything was working fine but now the same code is crashing on `loss.backward()`. \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6191", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6191/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6191/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6191/events", "html_url": "https://github.com/huggingface/transformers/issues/6191", "id": 671086430, "node_id": "MDU6SXNzdWU2NzEwODY0MzA=", "number": 6191, "title": "How to integrate the Pyro module with HuggingFace Transformers?", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-08-01T18:24:51Z", "updated_at": "2020-08-08T18:10:45Z", "closed_at": "2020-08-07T20:30:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI am trying to convert the HuggingFace Transformer into a Bayesian neural network by using the `Pyro` module.\r\nI provided my code below. Everything works well except I am stuck at the line `svi_loss = svi.step(input_ids = input_ids, attention_mask = attention_mask, labels = label)`. At that line an error is generated, because after converting the HuggingFace Transformer into a Pyro model, the new model does not have any set parameter (since it is a Bayesian model...so the weights for a Pyro model are not fixed, meaning the weights are sampled from a statistical distribution). Is there any way that I can get around this issue? I have also posted the similar question on Pyro forum. Thank you,\r\n\r\nCODE: \r\n\r\n```python \r\nimport torch\r\nfrom torch import distributions\r\nfrom transformers import RobertaTokenizer, RobertaForMultipleChoice, AdamW, get_constant_schedule\r\nimport pyro\r\nimport pyro.infer\r\nimport pyro.optim\r\nimport pyro.distributions as dist\r\nimport pyro.nn.module as module\r\nimport pyro.infer.autoguide.guides as guides\r\nfrom torch import nn\r\nfrom pyro.optim import Adam\r\nfrom pyro.infer import SVI\r\nfrom pyro.infer import Trace_ELBO\r\nfrom pyro.infer import Predictive\r\n\r\n# get the pre-trained HuggingFace RobertaForMultipleChoice and resize the token embeddings \r\n# after adding the special token\r\nmodel_RobertaForMultipleChoice = RobertaForMultipleChoice.from_pretrained('Roberta-base')\r\n        \r\n# convert the HuggingFace model into a pyro model\r\nmodule.to_pyro_module_(model_RobertaForMultipleChoice)\r\n\r\nfor m in model_RobertaForMultipleChoice.modules():\r\n      for name, value in list(m.named_parameters(recurse=False)):\r\n                setattr(m, name, module.PyroSample(prior=dist.Normal(0, 1)\r\n                                             .expand(value.shape)\r\n                                             .to_event(value.dim())))\r\n        \r\n        \r\n# define parameters for training      \r\nguide_delta = guides.AutoDelta(model_RobertaForMultipleChoice)\r\noptimizer_2 = Adam({\"lr\": 0.000000055}) \r\nscheduler_2 = pyro.optim.StepLR({'optimizer': optimizer_2, 'optim_args': {'lr': 0.000000055}})\r\nsvi_delta = SVI(model_RobertaForMultipleChoice, guide_delta, optimizer_2, loss=Trace_ELBO())\r\n\r\n# training loop\r\nfor m in range(num_iter):\r\n\r\n        # calculate the loss and take a gradient step for svi\r\n        # ERRORS OCCUR HERE \r\n        svi_loss = svi.step(input_ids = input_ids, \r\n                          attention_mask = attention_mask, \r\n                          labels = label)\r\n\r\n        # update the with the calculated loss \r\n        total_svi_loss = total_svi_loss + svi_loss\r\n       \r\n        if m % log_interval == 0 and m > 0:\r\n            cur_svi_loss = total_svi_loss / log_interval\r\n            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.9f} | \r\n                   loss {:5.4f} | ppl {:8.4f}'.format(\r\n                    epoch, m, int(num_lines_train/4), scheduler.get_lr()[0], \r\n                    cur_svi_loss, math.exp(cur_svi_loss)))\r\n                   \r\n            total_svi_loss = 0 \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6186", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6186/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6186/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6186/events", "html_url": "https://github.com/huggingface/transformers/issues/6186", "id": 670739315, "node_id": "MDU6SXNzdWU2NzA3MzkzMTU=", "number": 6186, "title": "Remove inconsistency between BertTokenizer and BertTokenizerFast ", "user": {"login": "PhilipMay", "id": 229382, "node_id": "MDQ6VXNlcjIyOTM4Mg==", "avatar_url": "https://avatars3.githubusercontent.com/u/229382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilipMay", "html_url": "https://github.com/PhilipMay", "followers_url": "https://api.github.com/users/PhilipMay/followers", "following_url": "https://api.github.com/users/PhilipMay/following{/other_user}", "gists_url": "https://api.github.com/users/PhilipMay/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilipMay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilipMay/subscriptions", "organizations_url": "https://api.github.com/users/PhilipMay/orgs", "repos_url": "https://api.github.com/users/PhilipMay/repos", "events_url": "https://api.github.com/users/PhilipMay/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilipMay/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-01T10:24:56Z", "updated_at": "2020-08-06T12:31:23Z", "closed_at": "2020-08-06T12:31:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "# \ud83d\ude80 Feature request\r\n`BertTokenizerFast` has the option to specify `strip_accents=False`. The `BertTokenizer` does not have this option. This inconsistency should be removed by adding the `strip_accents` parameter to `BertTokenizer`.\r\n\r\n## Motivation\r\nWithout adding this, the `BertTokenizer` can not be used for language models which are lowercase but have accents.\r\n\r\nIn case of a language model with lowercase and with accents you are forced to load the tokenizer by this:\r\n\r\n```python\r\ntokenizer = AutoTokenizer.from_pretrained(\"<model_name_or_path>\", use_fast=True, strip_accents=False)\r\n```\r\n\r\nThis will NOT work: `tokenizer = AutoTokenizer.from_pretrained(\"<model_name_or_path>\")`\r\n\r\nAnd even this would not work: `tokenizer = AutoTokenizer.from_pretrained(\"<model_name_or_path>\", strip_accents=False)`\r\n\r\n## Your contribution\r\nWith some hints I am willing to contribute.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6182", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6182/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6182/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6182/events", "html_url": "https://github.com/huggingface/transformers/issues/6182", "id": 670244665, "node_id": "MDU6SXNzdWU2NzAyNDQ2NjU=", "number": 6182, "title": "Failing XLMModelTest", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-31T21:19:34Z", "updated_at": "2020-08-07T06:58:16Z", "closed_at": "2020-08-07T06:58:16Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "https://github.com/huggingface/transformers/runs/929962952?check_suite_focus=true\r\n\r\nFAILED tests/test_modeling_xlm.py::XLMModelTest::test_inputs_embeds - RuntimeError\r\n\r\n\r\nFailure introduced somewhere in here:\r\n```\r\n* d951c14a Sylvain Gugger: Model output test (#6155) -   (8 hours ago)\r\n* 86caab1e Sylvain Gugger: Harmonize both Trainers API (#6157) -   (8 hours ago)\r\n* 603cd81a Mehrdad Farahani: readme m3hrdadfi/albert-fa-base-v2 (#6153) -   (11 hours ago)\r\n* 838dc06f Suraj Patil: parse arguments from dict (#4869) -   (13 hours ago)\r\n* cf3cf304 Paul O'Leary McCann: Replace mecab-python3 with fugashi for Japanese tokenization (#6086) -   (13 hours ago)\r\n* f250beb8 Stas Bekman: enable easy checkout switch (#5645) -   (13 hours ago)\r\n* 7d50af4b kolk: Create README.md (#6169) -   (13 hours ago)\r\n* 0034a1d2 Prajjwal Bhargava: Add Pytorch Native AMP support in Trainer (#6151) -   (13 hours ago)\r\n* 7231f7b5 Funtowicz Morgan: Enable ONNX/ONNXRuntime optimizations through converter script (#6131) -   (14 hours ago)\r\n* c0b93a1c Stas Bekman: correct the correction (#6163) -   (23 hours ago)\r\n* a2f6d521 Stas Bekman: typos (#6162) -   (24 hours ago)\r\n* f3065abd Sylvain Gugger: Doc tokenizer (#6110) -   (26 hours ago)\r\n* e642c789 guillaume-be: Addition of a DialoguePipeline (#5516) -   (27 hours ago)\r\n* ec026747 Lysandre Debut: Fix FlauBERT GPU test (#6142) -   (30 hours ago)\r\n* 91cb9546 Sylvain Gugger: Switch from return_tuple to return_dict (#6138) -   (32 hours ago)\r\n* 562b6369 Sylvain Gugger: Tf trainer cleanup (#6143) -   (32 hours ago)\r\n* c127d055 Oren Amsalem: add another e.g. to avoid confusion (#6055) -   (32 hours ago)\r\n* d24ea708 Oren Amsalem: Actually the extra_id are from 0-99 and not from 1-100 (#5967) -   (35 hours ago)\r\n* 3212b885 Stas Bekman: [s2s] add support for overriding config params (#6149) -   (2 days ago)\r\n* 54f9fbef Julien Plu: Rework TF trainer (#6038) -   (2 days ago)\r\n* 3f94170a Lysandre Debut: [WIP] Test TF Flaubert + Add {XLM, Flaubert}{TokenClassification, MultipleC\u2026 (#5614) -   (2 days ago)\r\n* 8a8ae276 Sylvain Gugger: Use google style to document properties (#6130) -   (2 days ago)\r\n* fc64559c Julien Plu: Fix TF CTRL model naming (#6134) -   (2 days ago)\r\n* 641b873c Lysandre Debut: XLNet PLM Readme (#6121) -   (2 days ago)\r\n* 8d157c93 Timo Moeller: add deepset/xlm-roberta-large-squad2 model card (#6128) -   (2 days ago)\r\n* 6c002853 Funtowicz Morgan: Added capability to quantize a model while exporting through ONNX. (#6089) -   (2 days ago)\r\n* 25de74cc Sylvain Gugger: Use FutureWarning to deprecate (#6111) -   (3 days ago)\r\n* 640550fc Funtowicz Morgan: ONNX documentation (#5992) -   (3 days ago)\r\n```\r\n\r\n\r\nAny idea on this @sgugger or @LysandreJik  ? Otherwise I'll dig in.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6179", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6179/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6179/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6179/events", "html_url": "https://github.com/huggingface/transformers/issues/6179", "id": 670233002, "node_id": "MDU6SXNzdWU2NzAyMzMwMDI=", "number": 6179, "title": "HANS Dataset: Incorrect `label_list` and `label`.", "user": {"login": "HanGuo97", "id": 18187806, "node_id": "MDQ6VXNlcjE4MTg3ODA2", "avatar_url": "https://avatars2.githubusercontent.com/u/18187806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HanGuo97", "html_url": "https://github.com/HanGuo97", "followers_url": "https://api.github.com/users/HanGuo97/followers", "following_url": "https://api.github.com/users/HanGuo97/following{/other_user}", "gists_url": "https://api.github.com/users/HanGuo97/gists{/gist_id}", "starred_url": "https://api.github.com/users/HanGuo97/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HanGuo97/subscriptions", "organizations_url": "https://api.github.com/users/HanGuo97/orgs", "repos_url": "https://api.github.com/users/HanGuo97/repos", "events_url": "https://api.github.com/users/HanGuo97/events{/privacy}", "received_events_url": "https://api.github.com/users/HanGuo97/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-31T21:05:02Z", "updated_at": "2020-08-12T02:50:39Z", "closed_at": "2020-08-03T16:36:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version:\r\n- Platform: n/a\r\n- Python version: n/a\r\n- PyTorch version (GPU?): n/a\r\n- Tensorflow version (GPU?): n/a\r\n- Using GPU in script?: n/a\r\n- Using distributed or parallel set-up in script?: n/a\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n -->\r\n@VictorSanh\r\n\r\n## Information\r\n\r\nModel I am using (Bert, XLNet ...):\r\n\r\nThe problem arises when using:\r\n* [ ] the official example scripts: (give details below)\r\n* [ ] my own modified scripts: (give details below)\r\n\r\nThe tasks I am working on is:\r\n* [ ] an official GLUE/SQUaD task: (give the name)\r\n* [ ] my own task or dataset: (give details below)\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n### Incorrect `label_list`\r\nSee line: https://github.com/huggingface/transformers/blob/master/examples/adversarial/utils_hans.py#L259\r\n```python\r\n    def get_labels(self):\r\n        \"\"\"See base class.\"\"\"\r\n        return [\"contradiction\", \"entailment\", \"neutral\"]\r\n```\r\nHANS dataset has only two labels, non-entailment, and entailment, but here three are given. Similarly, when mapping from text label to label-id, the label \"non-entailment\" (which exists in the task but not in the afore-defined labels), the below line is used. I'm curious if this is intentional? If so, would be great to add a warning/comment as those might cause subtle errors in the future.\r\n\r\nhttps://github.com/huggingface/transformers/blob/master/examples/adversarial/utils_hans.py#L311\r\n```python\r\nlabel = label_map[example.label] if example.label in label_map else 0\r\n```\r\n\r\n### Incorrect `label` index\r\nThe below line uses the last column as the label. However, the HANS dataset uses the first column for `label`\r\nhttps://github.com/huggingface/transformers/blob/master/examples/adversarial/utils_hans.py#L271\r\n```python\r\n            label = line[-1]\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6177", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6177/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6177/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6177/events", "html_url": "https://github.com/huggingface/transformers/issues/6177", "id": 670219925, "node_id": "MDU6SXNzdWU2NzAyMTk5MjU=", "number": 6177, "title": "RoBERTa for QuestionAnswering ", "user": {"login": "mchari", "id": 30506151, "node_id": "MDQ6VXNlcjMwNTA2MTUx", "avatar_url": "https://avatars0.githubusercontent.com/u/30506151?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mchari", "html_url": "https://github.com/mchari", "followers_url": "https://api.github.com/users/mchari/followers", "following_url": "https://api.github.com/users/mchari/following{/other_user}", "gists_url": "https://api.github.com/users/mchari/gists{/gist_id}", "starred_url": "https://api.github.com/users/mchari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mchari/subscriptions", "organizations_url": "https://api.github.com/users/mchari/orgs", "repos_url": "https://api.github.com/users/mchari/repos", "events_url": "https://api.github.com/users/mchari/events{/privacy}", "received_events_url": "https://api.github.com/users/mchari/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-07-31T20:49:29Z", "updated_at": "2020-08-19T20:05:15Z", "closed_at": "2020-08-19T20:05:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to replicate the example in this link \r\nhttps://github.com/huggingface/transformers/pull/1502/files, but I get the following error : \r\n``\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-823cc70a5d4f> in <module>\r\n----> 1 token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\r\n      2 start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\r\n      3 all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\r\n      4 print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\r\n\r\n<ipython-input-23-823cc70a5d4f> in <listcomp>(.0)\r\n----> 1 token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\r\n      2 start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\r\n      3 all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\r\n      4 print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\r\n\r\nValueError: 102 is not in list\r\n``\r\nI see the same error discussed in https://github.com/huggingface/transformers/issues/2261\r\n\r\nAny ideas how I could resolve this issue ?\r\nThanks in advance.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6174", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6174/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6174/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6174/events", "html_url": "https://github.com/huggingface/transformers/issues/6174", "id": 669976857, "node_id": "MDU6SXNzdWU2Njk5NzY4NTc=", "number": 6174, "title": "t", "user": {"login": "mad912584-commit", "id": 60356326, "node_id": "MDQ6VXNlcjYwMzU2MzI2", "avatar_url": "https://avatars1.githubusercontent.com/u/60356326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mad912584-commit", "html_url": "https://github.com/mad912584-commit", "followers_url": "https://api.github.com/users/mad912584-commit/followers", "following_url": "https://api.github.com/users/mad912584-commit/following{/other_user}", "gists_url": "https://api.github.com/users/mad912584-commit/gists{/gist_id}", "starred_url": "https://api.github.com/users/mad912584-commit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mad912584-commit/subscriptions", "organizations_url": "https://api.github.com/users/mad912584-commit/orgs", "repos_url": "https://api.github.com/users/mad912584-commit/repos", "events_url": "https://api.github.com/users/mad912584-commit/events{/privacy}", "received_events_url": "https://api.github.com/users/mad912584-commit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-31T16:14:28Z", "updated_at": "2020-07-31T16:16:44Z", "closed_at": "2020-07-31T16:16:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n**A link to original question on the forum/Stack Overflow**:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6173", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6173/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6173/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6173/events", "html_url": "https://github.com/huggingface/transformers/issues/6173", "id": 669742696, "node_id": "MDU6SXNzdWU2Njk3NDI2OTY=", "number": 6173, "title": "My finetuned gpt2 model is taking wayy too long to generate samples, like 5-8 minutes", "user": {"login": "Krish-Nerkar", "id": 49949733, "node_id": "MDQ6VXNlcjQ5OTQ5NzMz", "avatar_url": "https://avatars3.githubusercontent.com/u/49949733?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Krish-Nerkar", "html_url": "https://github.com/Krish-Nerkar", "followers_url": "https://api.github.com/users/Krish-Nerkar/followers", "following_url": "https://api.github.com/users/Krish-Nerkar/following{/other_user}", "gists_url": "https://api.github.com/users/Krish-Nerkar/gists{/gist_id}", "starred_url": "https://api.github.com/users/Krish-Nerkar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Krish-Nerkar/subscriptions", "organizations_url": "https://api.github.com/users/Krish-Nerkar/orgs", "repos_url": "https://api.github.com/users/Krish-Nerkar/repos", "events_url": "https://api.github.com/users/Krish-Nerkar/events{/privacy}", "received_events_url": "https://api.github.com/users/Krish-Nerkar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-07-31T11:59:52Z", "updated_at": "2020-08-11T16:55:05Z", "closed_at": "2020-08-11T16:55:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "I fine tuned the gpt2 model using transformers, i trained it on a lyrics dataset, and after successful training, when i do model.generate(args), it takes like a hell lot of time to genrate results\r\nWhat Should i do?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6170", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6170/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6170/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6170/events", "html_url": "https://github.com/huggingface/transformers/issues/6170", "id": 669517063, "node_id": "MDU6SXNzdWU2Njk1MTcwNjM=", "number": 6170, "title": "[Benchmark]", "user": {"login": "julio3361", "id": 67687496, "node_id": "MDQ6VXNlcjY3Njg3NDk2", "avatar_url": "https://avatars2.githubusercontent.com/u/67687496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/julio3361", "html_url": "https://github.com/julio3361", "followers_url": "https://api.github.com/users/julio3361/followers", "following_url": "https://api.github.com/users/julio3361/following{/other_user}", "gists_url": "https://api.github.com/users/julio3361/gists{/gist_id}", "starred_url": "https://api.github.com/users/julio3361/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/julio3361/subscriptions", "organizations_url": "https://api.github.com/users/julio3361/orgs", "repos_url": "https://api.github.com/users/julio3361/repos", "events_url": "https://api.github.com/users/julio3361/events{/privacy}", "received_events_url": "https://api.github.com/users/julio3361/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-31T07:38:28Z", "updated_at": "2020-07-31T09:12:15Z", "closed_at": "2020-07-31T09:12:04Z", "author_association": "NONE", "active_lock_reason": "spam", "body": "# \ud83d\udda5 Benchmarking `transformers`\r\n\r\n## Benchmark\r\n\r\nWhich part of `transformers` did you benchmark?\r\n\r\n## Set-up\r\n\r\nWhat did you run your benchmarks on? Please include details, such as: CPU, GPU? If using multiple GPUs, which parallelization did you use?\r\n\r\n## Results\r\n\r\nPut your results here!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6156", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6156/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6156/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6156/events", "html_url": "https://github.com/huggingface/transformers/issues/6156", "id": 668946837, "node_id": "MDU6SXNzdWU2Njg5NDY4Mzc=", "number": 6156, "title": "should mBART-large-en-ro have decoder_start_token_id by default?", "user": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1108649053, "node_id": "MDU6TGFiZWwxMTA4NjQ5MDUz", "url": "https://api.github.com/repos/huggingface/transformers/labels/Help%20wanted", "name": "Help wanted", "color": "008672", "default": false, "description": "Extra attention is needed, help appreciated"}, {"id": 2009457320, "node_id": "MDU6TGFiZWwyMDA5NDU3MzIw", "url": "https://api.github.com/repos/huggingface/transformers/labels/translation", "name": "translation", "color": "b2d2f4", "default": false, "description": "machine translation utilities and models"}], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2020-07-30T16:35:52Z", "updated_at": "2020-08-22T02:03:49Z", "closed_at": "2020-08-22T02:03:48Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Hypothesis: since the argument `prepend_bos` is set to \"False\" in fairseq/examples/README.md, mbart-large-en-ro does not need `decoder_start_token_id`.\r\n\r\nTODO:\r\n- create branch that deletes `decoder_start_token_id`. Setting it to None in the config might not be enough.\r\n- verify that decoder_start_token_id is in fact not being used by setting a breakpoint in `generate`.\r\n- run_eval.py on wmt-en-ro/test and see if BLEU is >= 26.46, the score with decoder_start_token_id=250020.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6154", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6154/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6154/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6154/events", "html_url": "https://github.com/huggingface/transformers/issues/6154", "id": 668822836, "node_id": "MDU6SXNzdWU2Njg4MjI4MzY=", "number": 6154, "title": "Hidden State Embedding-Transformers", "user": {"login": "DaniMlk", "id": 28568281, "node_id": "MDQ6VXNlcjI4NTY4Mjgx", "avatar_url": "https://avatars1.githubusercontent.com/u/28568281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DaniMlk", "html_url": "https://github.com/DaniMlk", "followers_url": "https://api.github.com/users/DaniMlk/followers", "following_url": "https://api.github.com/users/DaniMlk/following{/other_user}", "gists_url": "https://api.github.com/users/DaniMlk/gists{/gist_id}", "starred_url": "https://api.github.com/users/DaniMlk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DaniMlk/subscriptions", "organizations_url": "https://api.github.com/users/DaniMlk/orgs", "repos_url": "https://api.github.com/users/DaniMlk/repos", "events_url": "https://api.github.com/users/DaniMlk/events{/privacy}", "received_events_url": "https://api.github.com/users/DaniMlk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-30T14:52:16Z", "updated_at": "2020-07-31T15:41:33Z", "closed_at": "2020-07-31T15:41:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi everybody, I want to use Bert model to get the embedding for a sentence after I fine-tuned it with raw texts. I was wondering if is that possible or not and anybody can help me with that? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6145", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6145/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6145/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6145/events", "html_url": "https://github.com/huggingface/transformers/issues/6145", "id": 668266622, "node_id": "MDU6SXNzdWU2NjgyNjY2MjI=", "number": 6145, "title": "TOKENIZER: truncation not working for batch", "user": {"login": "PyAntony", "id": 24689636, "node_id": "MDQ6VXNlcjI0Njg5NjM2", "avatar_url": "https://avatars1.githubusercontent.com/u/24689636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PyAntony", "html_url": "https://github.com/PyAntony", "followers_url": "https://api.github.com/users/PyAntony/followers", "following_url": "https://api.github.com/users/PyAntony/following{/other_user}", "gists_url": "https://api.github.com/users/PyAntony/gists{/gist_id}", "starred_url": "https://api.github.com/users/PyAntony/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PyAntony/subscriptions", "organizations_url": "https://api.github.com/users/PyAntony/orgs", "repos_url": "https://api.github.com/users/PyAntony/repos", "events_url": "https://api.github.com/users/PyAntony/events{/privacy}", "received_events_url": "https://api.github.com/users/PyAntony/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-30T00:59:28Z", "updated_at": "2020-08-04T19:02:52Z", "closed_at": "2020-08-04T19:02:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n\r\n- `transformers` version: 2.11.0\r\n- Platform: Linux-4.15.0-106-generic-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.4\r\n- PyTorch version (GPU?): 1.5.1 (False)\r\n- Tensorflow version (GPU?): 2.2.0 (False)\r\n- Using GPU in script?: NO\r\n- Using distributed or parallel set-up in script?: NO\r\n\r\n### Who can help\r\n \r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n examples/distillation: @VictorSanh\r\n\r\n## Information\r\n\r\nModel I am using: DistilBertForSequenceClassification.\r\n\r\nThe tokenizer does not truncate when I pass a list of strings. It only works when I pass a single string.\r\n\r\n## To reproduce\r\n\r\nCopy/paste (or just read) code below.  \r\n\r\n```python\r\nfrom transformers import DistilBertTokenizer\r\n\r\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\r\n\r\nsentence = \"Submit a bug report to help us improve transformers.\"\r\noutput = tokenizer(sentence, padding=True, truncation=True, max_length=4)\r\n\r\nprint(output['input_ids'])\r\n# 4 tokens as expected from *max_length=4*\r\n# out: [101, 12040, 1037, 102]\r\n\r\n# now let's test with multiple sentences\r\nsentences = [\r\n  \"Submit a bug report to help us improve transformers.\", \r\n  \"Benchmark a part of this library and share your results\"\r\n] \r\n\r\noutput = tokenizer(sentences, padding=True, truncation=True, max_length=4)\r\nprint(output['input_ids'])\r\n# output is returning all tokens, it is not truncating to max_length!\r\n# out: [[101, 12040, 1037, 11829, 3189, 2000, 2393, 2149, 5335, 19081, 1012, 102, 0], \r\n#       [101, 6847, 10665, 1037, 2112, 1997, 2023, 3075, 1998, 3745, 2115, 3463, 102]]\r\n```\r\n\r\n## Expected \r\n```python\r\n# output truncated to max_length (4 as in the example)\r\n# out: [[101, 12040, 1037, 102], \r\n#       [101, 6847, 10665, 102]]\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6141", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6141/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6141/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6141/events", "html_url": "https://github.com/huggingface/transformers/issues/6141", "id": 668058851, "node_id": "MDU6SXNzdWU2NjgwNTg4NTE=", "number": 6141, "title": "Bug in language_modeling.py calling tokenizer.num_special_tokens_to_add", "user": {"login": "frarito", "id": 930259, "node_id": "MDQ6VXNlcjkzMDI1OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/930259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frarito", "html_url": "https://github.com/frarito", "followers_url": "https://api.github.com/users/frarito/followers", "following_url": "https://api.github.com/users/frarito/following{/other_user}", "gists_url": "https://api.github.com/users/frarito/gists{/gist_id}", "starred_url": "https://api.github.com/users/frarito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frarito/subscriptions", "organizations_url": "https://api.github.com/users/frarito/orgs", "repos_url": "https://api.github.com/users/frarito/repos", "events_url": "https://api.github.com/users/frarito/events{/privacy}", "received_events_url": "https://api.github.com/users/frarito/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-29T18:41:26Z", "updated_at": "2020-07-29T20:30:38Z", "closed_at": "2020-07-29T20:29:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Linux-4.4.0-1081-aws-x86_64-with-Ubuntu-18.04-bionic\r\n- Python version: 3.6.9\r\n- PyTorch version (GPU?): 1.6.0+cu101 (True)\r\n- Tensorflow version (GPU?): 2.1.0 (True)\r\n- Using GPU in script?: yes\r\n- Using distributed or parallel set-up in script?: no\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n tokenizers: @mfuntowicz\r\n -->\r\n\r\n## Information\r\n\r\nModel I am using GPT-2\r\n\r\nThe class TextDataset call tokenizer.num_special_tokens_to_add(pair=False) but the correct argument is called is_pair. I assume the bugfix corresponds to transformers repo.\r\n\r\n## To reproduce\r\n\r\nhttps://github.com/huggingface/transformers/blob/e49393c3617e877f0370f7bad7c7e823808c5bfb/src/transformers/data/datasets/language_modeling.py#L27\r\n\r\nhttps://github.com/huggingface/tokenizers/blob/master/bindings/python/tokenizers/implementations/base_tokenizer.py#L20\r\n\r\n\r\nI'm using transformers 3.0.2 and tokenizers 0.8.1rc1 (try to update but says its incompatible)\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6139", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6139/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6139/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6139/events", "html_url": "https://github.com/huggingface/transformers/issues/6139", "id": 668000980, "node_id": "MDU6SXNzdWU2NjgwMDA5ODA=", "number": 6139, "title": "Applying hugging face transformer in sequence labeling problem", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-29T17:07:25Z", "updated_at": "2020-07-31T00:42:14Z", "closed_at": "2020-07-31T00:42:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello.. Thanks for your great framework. Btw, what I'd like to know is that can I apply these hugging face transformer models in **sequence labeling** problems like **part of speech tagging** and **word segmentation**(because I see only **ner** model in example folder). If I can, **how** can I do that?? Can I get some helps like **example scripts** about how to apply these transformers for sequence labeling problems??", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6127", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6127/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6127/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6127/events", "html_url": "https://github.com/huggingface/transformers/issues/6127", "id": 667780751, "node_id": "MDU6SXNzdWU2Njc3ODA3NTE=", "number": 6127, "title": "Initializing XLMRobertaTokenizer using pretrained tokenizer expects serialized vocab", "user": {"login": "aoxolotl", "id": 53764708, "node_id": "MDQ6VXNlcjUzNzY0NzA4", "avatar_url": "https://avatars3.githubusercontent.com/u/53764708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aoxolotl", "html_url": "https://github.com/aoxolotl", "followers_url": "https://api.github.com/users/aoxolotl/followers", "following_url": "https://api.github.com/users/aoxolotl/following{/other_user}", "gists_url": "https://api.github.com/users/aoxolotl/gists{/gist_id}", "starred_url": "https://api.github.com/users/aoxolotl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aoxolotl/subscriptions", "organizations_url": "https://api.github.com/users/aoxolotl/orgs", "repos_url": "https://api.github.com/users/aoxolotl/repos", "events_url": "https://api.github.com/users/aoxolotl/events{/privacy}", "received_events_url": "https://api.github.com/users/aoxolotl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-29T11:45:28Z", "updated_at": "2020-07-31T06:36:27Z", "closed_at": "2020-07-31T06:36:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI am training an XLMRoberta model from scratch on Hindi. I am using a sentencepiece tokenizer trained exclusively on monolingual data following the steps mentioned in the [tokenizers repository](https://github.com/huggingface/tokenizers/tree/704cf3fdd2f607ead58a561b892b510b49c301db/bindings/python#using-the-provided-tokenizers). This results in the creation of `vocab.json` and `merges.txt`.\r\nHowever when I try to initialize the tokenizer using `XLMRobertaTokenizer.from_pretrained` I get an error saying\r\n```assumed 'models/sentencepiece' was a path, a model identifier, or url to a directory containing vocabulary files named ['sentencepiece.bpe.model'] but couldn't find such vocabulary files at this path or url. ``` \r\nI am assuming this is a serialized file based on [huggingface.co model](https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model) but don't know how to serialize my vocab.json file. I have already tried using `pickle` and `numpy`\r\nVersions used:\r\ntransformers: 2.9.1\r\ntokenizers: 0.7.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6126", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6126/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6126/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6126/events", "html_url": "https://github.com/huggingface/transformers/issues/6126", "id": 667746197, "node_id": "MDU6SXNzdWU2Njc3NDYxOTc=", "number": 6126, "title": "Add decoding inputs to generate", "user": {"login": "guyeyal", "id": 3502557, "node_id": "MDQ6VXNlcjM1MDI1NTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/3502557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guyeyal", "html_url": "https://github.com/guyeyal", "followers_url": "https://api.github.com/users/guyeyal/followers", "following_url": "https://api.github.com/users/guyeyal/following{/other_user}", "gists_url": "https://api.github.com/users/guyeyal/gists{/gist_id}", "starred_url": "https://api.github.com/users/guyeyal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guyeyal/subscriptions", "organizations_url": "https://api.github.com/users/guyeyal/orgs", "repos_url": "https://api.github.com/users/guyeyal/repos", "events_url": "https://api.github.com/users/guyeyal/events{/privacy}", "received_events_url": "https://api.github.com/users/guyeyal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-29T10:50:18Z", "updated_at": "2020-07-30T20:23:31Z", "closed_at": "2020-07-30T20:23:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \ud83d\ude80 Feature request\r\n\r\nAdd decoding inputs to generate\r\n\r\n## Motivation\r\n\r\nWhen generating with encoder-decoder, one may want to insert context for the decoder. \r\nI'm currently working on summarization given that I know some parts of the gt. But other ideas can come to mind.\r\n\r\n## Your contribution\r\n\r\n```\r\n    @torch.no_grad()\r\n    def generate(\r\n        self,\r\n        input_ids: Optional[torch.LongTensor] = None,\r\n        max_length: Optional[int] = None,\r\n        min_length: Optional[int] = None,\r\n        do_sample: Optional[bool] = None,\r\n        early_stopping: Optional[bool] = None,\r\n        num_beams: Optional[int] = None,\r\n        temperature: Optional[float] = None,\r\n        top_k: Optional[int] = None,\r\n        top_p: Optional[float] = None,\r\n        repetition_penalty: Optional[float] = None,\r\n        bad_words_ids: Optional[Iterable[int]] = None,\r\n        bos_token_id: Optional[int] = None,\r\n        pad_token_id: Optional[int] = None,\r\n        eos_token_id: Optional[int] = None,\r\n        length_penalty: Optional[float] = None,\r\n        no_repeat_ngram_size: Optional[int] = None,\r\n        num_return_sequences: Optional[int] = None,\r\n        attention_mask: Optional[torch.LongTensor] = None,\r\n        decoder_start_token_id: Optional[int] = None,\r\n        decoder_input_ids: Optional[torch.LongTensor] = None,\r\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\r\n        use_cache: Optional[bool] = None,\r\n        **model_specific_kwargs\r\n    ) -> torch.LongTensor:\r\n```\r\n        r\"\"\" Generates sequences for models with a LM head. The method currently supports greedy decoding, beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.\r\n\r\n        Adapted in part from Facebook's XLM beam search code_.\r\n\r\n        .. _Facebook's XLM beam search code:\r\n           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\r\n\r\n\r\n        Parameters:\r\n\r\n            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\r\n                The sequence used as a prompt for the generation. If `None` the method initializes\r\n                it as an empty `torch.LongTensor` of shape `(1,)`.\r\n\r\n            max_length: (`optional`) int\r\n                The max length of the sequence to be generated.  Between `min_length` and infinity. Default to 20.\r\n\r\n            min_length: (`optional`) int\r\n                The min length of the sequence to be generated.  Between 0 and infinity. Default to 0.\r\n\r\n            do_sample: (`optional`) bool\r\n                If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\r\n\r\n            early_stopping: (`optional`) bool\r\n                if set to `True` beam search is stopped when at least `num_beams` sentences finished per batch. Defaults to `False` as defined in `configuration_utils.PretrainedConfig`.\r\n\r\n            num_beams: (`optional`) int\r\n                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\r\n\r\n            temperature: (`optional`) float\r\n                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\r\n\r\n            top_k: (`optional`) int\r\n                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\r\n\r\n            top_p: (`optional`) float\r\n                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\r\n\r\n            repetition_penalty: (`optional`) float\r\n                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\r\n\r\n            pad_token_id: (`optional`) int\r\n                Padding token. Default to specicic model pad_token_id or None if it does not exist.\r\n\r\n            bos_token_id: (`optional`) int\r\n                BOS token. Defaults to `bos_token_id` as defined in the models config.\r\n\r\n            eos_token_id: (`optional`) int\r\n                EOS token. Defaults to `eos_token_id` as defined in the models config.\r\n\r\n            length_penalty: (`optional`) float\r\n                Exponential penalty to the length. Default to 1.\r\n\r\n            no_repeat_ngram_size: (`optional`) int\r\n                If set to int > 0, all ngrams of size `no_repeat_ngram_size` can only occur once.\r\n            bad_words_ids: (`optional`) list of lists of int\r\n                `bad_words_ids` contains tokens that are not allowed to be generated. In order to get the tokens of the words that should not appear in the generated text, use `tokenizer.encode(bad_word, add_prefix_space=True)`.\r\n\r\n            num_return_sequences: (`optional`) int\r\n                The number of independently computed returned sequences for each element in the batch. Default to 1.\r\n\r\n            attention_mask (`optional`) obj: `torch.LongTensor` of same shape as `input_ids`\r\n                Mask to avoid performing attention on padding token indices.\r\n                Mask values selected in ``[0, 1]``:\r\n                ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\r\n                Defaults to `None`.\r\n\r\n                `What are attention masks? <../glossary.html#attention-mask>`__\r\n\r\n            decoder_start_token_id=None: (`optional`) int\r\n                If an encoder-decoder model starts decoding with a different token than BOS.\r\n                Defaults to `None` and is changed to `BOS` later.\r\n\r\n            use_cache: (`optional`) bool\r\n                If `use_cache` is True, past key values are used to speed up decoding if applicable to model. Defaults to `True`.\r\n\r\n            model_specific_kwargs: (`optional`) dict\r\n                Additional model specific kwargs will be forwarded to the `forward` function of the model.\r\n\r\n        Return:\r\n\r\n            output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\r\n                sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\r\n\r\n        Examples::\r\n\r\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\r\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\r\n            outputs = model.generate(max_length=40)  # do greedy decoding\r\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\r\n\r\n            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\r\n            model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\r\n            input_context = 'The dog'\r\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\r\n            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\r\n            for i in range(3): #  3 output sequences were generated\r\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\r\n\r\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\r\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\r\n            input_context = 'The dog'\r\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\r\n            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3)  # 3 generate sequences using by sampling\r\n            for i in range(3): #  3 output sequences were generated\r\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\r\n\r\n            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\r\n            model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\r\n            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\r\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\r\n            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\r\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\r\n\r\n            tokenizer = AutoTokenizer.from_pretrained('gpt2')   # Initialize tokenizer\r\n            model = AutoModelWithLMHead.from_pretrained('gpt2')    # Download model and configuration from S3 and cache.\r\n            input_context = 'My cute dog'  # \"Legal\" is one of the control codes for ctrl\r\n            bad_words_ids = [tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ['idiot', 'stupid', 'shut up']]\r\n            input_ids = tokenizer.encode(input_context, return_tensors='pt')  # encode input context\r\n            outputs = model.generate(input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids)  # generate sequences without allowing bad_words to be generated\r\n        \"\"\"\r\n\r\n        # We cannot generate if the model does not have a LM head\r\n        if self.get_output_embeddings() is None:\r\n            raise AttributeError(\r\n                \"You tried to generate sequences with a model that does not have a LM Head.\"\r\n                \"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`, `XLMWithLMHeadModel`, `BartForConditionalGeneration` )\"\r\n            )\r\n\r\n        max_length = max_length if max_length is not None else self.config.max_length\r\n        min_length = min_length if min_length is not None else self.config.min_length\r\n        do_sample = do_sample if do_sample is not None else self.config.do_sample\r\n        early_stopping = early_stopping if early_stopping is not None else self.config.early_stopping\r\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\r\n        num_beams = num_beams if num_beams is not None else self.config.num_beams\r\n        temperature = temperature if temperature is not None else self.config.temperature\r\n        top_k = top_k if top_k is not None else self.config.top_k\r\n        top_p = top_p if top_p is not None else self.config.top_p\r\n        repetition_penalty = repetition_penalty if repetition_penalty is not None else self.config.repetition_penalty\r\n        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id\r\n        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id\r\n        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id\r\n        length_penalty = length_penalty if length_penalty is not None else self.config.length_penalty\r\n        no_repeat_ngram_size = (\r\n            no_repeat_ngram_size if no_repeat_ngram_size is not None else self.config.no_repeat_ngram_size\r\n        )\r\n        bad_words_ids = bad_words_ids if bad_words_ids is not None else self.config.bad_words_ids\r\n        num_return_sequences = (\r\n            num_return_sequences if num_return_sequences is not None else self.config.num_return_sequences\r\n        )\r\n        decoder_start_token_id = (\r\n            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id\r\n        )\r\n\r\n        if input_ids is not None:\r\n            batch_size = input_ids.shape[0]  # overriden by the input batch_size\r\n        else:\r\n            batch_size = 1\r\n\r\n        assert isinstance(max_length, int) and max_length > 0, \"`max_length` should be a strictly positive integer.\"\r\n        assert isinstance(min_length, int) and min_length >= 0, \"`min_length` should be a positive integer.\"\r\n        assert isinstance(do_sample, bool), \"`do_sample` should be a boolean.\"\r\n        assert isinstance(early_stopping, bool), \"`early_stopping` should be a boolean.\"\r\n        assert isinstance(use_cache, bool), \"`use_cache` should be a boolean.\"\r\n        assert isinstance(num_beams, int) and num_beams > 0, \"`num_beams` should be a strictly positive integer.\"\r\n        assert temperature > 0, \"`temperature` should be strictly positive.\"\r\n        assert isinstance(top_k, int) and top_k >= 0, \"`top_k` should be a positive integer.\"\r\n        assert 0 <= top_p <= 1, \"`top_p` should be between 0 and 1.\"\r\n        assert repetition_penalty >= 1.0, \"`repetition_penalty` should be >= 1.\"\r\n        assert input_ids is not None or (\r\n            isinstance(bos_token_id, int) and bos_token_id >= 0\r\n        ), \"If input_ids is not defined, `bos_token_id` should be a positive integer.\"\r\n        assert pad_token_id is None or (\r\n            isinstance(pad_token_id, int) and (pad_token_id >= 0)\r\n        ), \"`pad_token_id` should be a positive integer.\"\r\n        assert (eos_token_id is None) or (\r\n            isinstance(eos_token_id, int) and (eos_token_id >= 0)\r\n        ), \"`eos_token_id` should be a positive integer.\"\r\n        assert length_penalty > 0, \"`length_penalty` should be strictly positive.\"\r\n        assert (\r\n            isinstance(no_repeat_ngram_size, int) and no_repeat_ngram_size >= 0\r\n        ), \"`no_repeat_ngram_size` should be a positive integer.\"\r\n        assert (\r\n            isinstance(num_return_sequences, int) and num_return_sequences > 0\r\n        ), \"`num_return_sequences` should be a strictly positive integer.\"\r\n        assert (\r\n            bad_words_ids is None or isinstance(bad_words_ids, list) and isinstance(bad_words_ids[0], list)\r\n        ), \"`bad_words_ids` is either `None` or a list of lists of tokens that should not be generated\"\r\n\r\n        if input_ids is None:\r\n            assert isinstance(bos_token_id, int) and bos_token_id >= 0, (\r\n                \"you should either supply a context to complete as `input_ids` input \"\r\n                \"or a `bos_token_id` (integer >= 0) as a first token to start the generation.\"\r\n            )\r\n            input_ids = torch.full(\r\n                (batch_size, 1), bos_token_id, dtype=torch.long, device=next(self.parameters()).device,\r\n            )\r\n        else:\r\n            assert input_ids.dim() == 2, \"Input prompt should be of shape (batch_size, sequence length).\"\r\n\r\n        # not allow to duplicate outputs when greedy decoding\r\n        if do_sample is False:\r\n            if num_beams == 1:\r\n                # no_beam_search greedy generation conditions\r\n                assert (\r\n                    num_return_sequences == 1\r\n                ), \"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"\r\n\r\n            else:\r\n                # beam_search greedy generation conditions\r\n                assert (\r\n                    num_beams >= num_return_sequences\r\n                ), \"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"\r\n\r\n        # create attention mask if necessary\r\n        # TODO (PVP): this should later be handled by the forward fn() in each model in the future see PR 3140\r\n        if (attention_mask is None) and (pad_token_id is not None) and (pad_token_id in input_ids):\r\n            attention_mask = input_ids.ne(pad_token_id).long()\r\n        elif attention_mask is None:\r\n            attention_mask = input_ids.new_ones(input_ids.shape)\r\n\r\n        # set pad_token_id to eos_token_id if not set. Important that this is done after\r\n        # attention_mask is created\r\n        if pad_token_id is None and eos_token_id is not None:\r\n            logger.warning(\r\n                \"Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence\".format(eos_token_id)\r\n            )\r\n            pad_token_id = eos_token_id\r\n\r\n        # current position and vocab size\r\n        if hasattr(self.config, \"vocab_size\"):\r\n            vocab_size = self.config.vocab_size\r\n        elif (\r\n            self.config.is_encoder_decoder\r\n            and hasattr(self.config, \"decoder\")\r\n            and hasattr(self.config.decoder, \"vocab_size\")\r\n        ):\r\n            vocab_size = self.config.decoder.vocab_size\r\n\r\n        # set effective batch size and effective batch multiplier according to do_sample\r\n        if do_sample:\r\n            effective_batch_size = batch_size * num_return_sequences\r\n            effective_batch_mult = num_return_sequences\r\n        else:\r\n            effective_batch_size = batch_size\r\n            effective_batch_mult = 1\r\n\r\n        if self.config.is_encoder_decoder:\r\n            if decoder_start_token_id is None:\r\n                decoder_start_token_id = bos_token_id\r\n\r\n            assert (\r\n                decoder_start_token_id is not None\r\n            ), \"decoder_start_token_id or bos_token_id has to be defined for encoder-decoder generation\"\r\n            assert hasattr(self, \"get_encoder\"), \"{} should have a 'get_encoder' function defined\".format(self)\r\n            assert callable(self.get_encoder), \"{} should be a method\".format(self.get_encoder)\r\n\r\n            # get encoder and store encoder outputs\r\n            encoder = self.get_encoder()\r\n\r\n            encoder_outputs: tuple = encoder(input_ids, attention_mask=attention_mask)\r\n\r\n        # Expand input ids if num_beams > 1 or num_return_sequences > 1\r\n\r\n\r\n\r\n        if self.config.is_encoder_decoder:\r\n            if decoder_input_ids is not None:\r\n                input_ids = decoder_input_ids\r\n            else:\r\n                # create empty decoder_input_ids\r\n                input_ids =  torch.full(\r\n                (effective_batch_size * num_beams, 1),\r\n                decoder_start_token_id,\r\n                dtype=torch.long,\r\n                device=next(self.parameters()).device,\r\n            )\r\n            cur_len = 1\r\n\r\n            assert (\r\n                batch_size == encoder_outputs[0].shape[0]\r\n            ), f\"expected encoder_outputs[0] to have 1st dimension bs={batch_size}, got {encoder_outputs[0].shape[0]} \"\r\n\r\n            # expand batch_idx to assign correct encoder output for expanded input_ids (due to num_beams > 1 and num_return_sequences > 1)\r\n            expanded_batch_idxs = (\r\n                torch.arange(batch_size)\r\n                .view(-1, 1)\r\n                .repeat(1, num_beams * effective_batch_mult)\r\n                .view(-1)\r\n                .to(input_ids.device)\r\n            )\r\n            # expand encoder_outputs\r\n            encoder_outputs = (encoder_outputs[0].index_select(0, expanded_batch_idxs), *encoder_outputs[1:])\r\n\r\n        else:\r\n            encoder_outputs = None\r\n            cur_len = input_ids.shape[-1]\r\n\r\n        assert (\r\n            cur_len < max_length\r\n        ), f\"The context has {cur_len} number of tokens, but `max_length` is only {max_length}. Please make sure that `max_length` is bigger than the number of tokens, by setting either `generate(max_length=...,...)` or `config.max_length = ...`\"\r\n\r\n        if num_return_sequences > 1 or num_beams > 1:\r\n            input_ids_len = input_ids.shape[-1]\r\n            input_ids = input_ids.unsqueeze(1).expand(batch_size, effective_batch_mult * num_beams, input_ids_len)\r\n            attention_mask = attention_mask.unsqueeze(1).expand(\r\n                batch_size, effective_batch_mult * num_beams, attention_mask.shape[-1]\r\n            )\r\n\r\n            input_ids = input_ids.contiguous().view(\r\n                effective_batch_size * num_beams, input_ids_len\r\n            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\r\n            attention_mask = attention_mask.contiguous().view(\r\n                effective_batch_size * num_beams, attention_mask.shape[-1]\r\n            )  # shape: (batch_size * num_return_sequences * num_beams, cur_len)\r\n\r\n\r\n        if num_beams > 1:\r\n            output = self._generate_beam_search(\r\n                input_ids,\r\n                cur_len=cur_len,\r\n                max_length=max_length,\r\n                min_length=min_length,\r\n                do_sample=do_sample,\r\n                early_stopping=early_stopping,\r\n                temperature=temperature,\r\n                top_k=top_k,\r\n                top_p=top_p,\r\n                repetition_penalty=repetition_penalty,\r\n                no_repeat_ngram_size=no_repeat_ngram_size,\r\n                bad_words_ids=bad_words_ids,\r\n                pad_token_id=pad_token_id,\r\n                eos_token_id=eos_token_id,\r\n                batch_size=effective_batch_size,\r\n                num_return_sequences=num_return_sequences,\r\n                length_penalty=length_penalty,\r\n                num_beams=num_beams,\r\n                vocab_size=vocab_size,\r\n                encoder_outputs=encoder_outputs,\r\n                attention_mask=attention_mask,\r\n                use_cache=use_cache,\r\n                decoder_attention_mask=decoder_attention_mask,\r\n                model_specific_kwargs=model_specific_kwargs,\r\n            )\r\n        else:\r\n            output = self._generate_no_beam_search(\r\n                input_ids,\r\n                cur_len=cur_len,\r\n                max_length=max_length,\r\n                min_length=min_length,\r\n                do_sample=do_sample,\r\n                temperature=temperature,\r\n                top_k=top_k,\r\n                top_p=top_p,\r\n                repetition_penalty=repetition_penalty,\r\n                no_repeat_ngram_size=no_repeat_ngram_size,\r\n                bad_words_ids=bad_words_ids,\r\n                pad_token_id=pad_token_id,\r\n                eos_token_id=eos_token_id,\r\n                batch_size=effective_batch_size,\r\n                encoder_outputs=encoder_outputs,\r\n                attention_mask=attention_mask,\r\n                use_cache=use_cache,\r\n                decoder_attention_mask=decoder_attention_mask,\r\n                model_specific_kwargs=model_specific_kwargs,\r\n            )\r\n\r\n        return output\r\n\r\n    def _generate_no_beam_search(\r\n        self,\r\n        input_ids,\r\n        cur_len,\r\n        max_length,\r\n        min_length,\r\n        do_sample,\r\n        temperature,\r\n        top_k,\r\n        top_p,\r\n        repetition_penalty,\r\n        no_repeat_ngram_size,\r\n        bad_words_ids,\r\n        pad_token_id,\r\n        eos_token_id,\r\n        batch_size,\r\n        encoder_outputs,\r\n        attention_mask,\r\n        use_cache,\r\n        decoder_attention_mask,\r\n        model_specific_kwargs,\r\n    ):\r\n        \"\"\" Generate sequences for each example without beam search (num_beams == 1).\r\n            All returned sequence are generated independantly.\r\n        \"\"\"\r\n        # length of generated sentences / unfinished sentences\r\n        unfinished_sents = input_ids.new(batch_size).fill_(1)\r\n        sent_lengths = input_ids.new(batch_size).fill_(max_length)\r\n\r\n        past = (encoder_outputs, None) if encoder_outputs is not None else None\r\n        while cur_len < max_length:\r\n            model_inputs = self.prepare_inputs_for_generation(\r\n                input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache, **model_specific_kwargs\r\n            )\r\n\r\n            model_inputs['decoder_attention_mask'] = decoder_attention_mask\r\n            outputs = self(**model_inputs)\r\n\r\n            next_token_logits = outputs[0][:, -1, :]\r\n\r\n            scores = self.postprocess_next_token_scores(\r\n                scores=next_token_logits,\r\n                input_ids=input_ids,\r\n                no_repeat_ngram_size=no_repeat_ngram_size,\r\n                bad_words_ids=bad_words_ids,\r\n                cur_len=cur_len,\r\n                min_length=min_length,\r\n                max_length=max_length,\r\n                eos_token_id=eos_token_id,\r\n                repetition_penalty=repetition_penalty,\r\n                batch_size=batch_size,\r\n                num_beams=1,\r\n            )\r\n\r\n            # if model has past, then set the past variable to speed up decoding\r\n            if self._use_cache(outputs, use_cache):\r\n                past = outputs[1]\r\n\r\n            if do_sample:\r\n                # Temperature (higher temperature => more likely to sample low probability tokens)\r\n                if temperature != 1.0:\r\n                    scores = scores / temperature\r\n                # Top-p/top-k filtering\r\n                next_token_logscores = top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)\r\n                # Sample\r\n                probs = F.softmax(next_token_logscores, dim=-1)\r\n                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\r\n            else:\r\n                # Greedy decoding\r\n                next_token = torch.argmax(next_token_logits, dim=-1)\r\n\r\n            # update generations and finished sentences\r\n            if eos_token_id is not None:\r\n                # pad finished sentences if eos_token_id exist\r\n                tokens_to_add = next_token * unfinished_sents + (pad_token_id) * (1 - unfinished_sents)\r\n            else:\r\n                tokens_to_add = next_token\r\n\r\n            # add token and increase length by one\r\n            input_ids = torch.cat([input_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\r\n            cur_len = cur_len + 1\r\n\r\n            if eos_token_id is not None:\r\n                eos_in_sents = tokens_to_add == eos_token_id\r\n                # if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length\r\n                is_sents_unfinished_and_token_to_add_is_eos = unfinished_sents.mul(eos_in_sents.long()).bool()\r\n                sent_lengths.masked_fill_(is_sents_unfinished_and_token_to_add_is_eos, cur_len)\r\n                # unfinished_sents is set to zero if eos in sentence\r\n                unfinished_sents.mul_((~eos_in_sents).long())\r\n\r\n            # stop when there is a </s> in each sentence, or if we exceed the maximul length\r\n            if unfinished_sents.max() == 0:\r\n                break\r\n\r\n            # extend attention_mask for new generated input if only decoder\r\n            if self.config.is_encoder_decoder is False:\r\n                attention_mask = torch.cat(\r\n                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\r\n                )\r\n\r\n        return input_ids\r\n\r\n    def _generate_beam_search(\r\n        self,\r\n        input_ids,\r\n        cur_len,\r\n        max_length,\r\n        min_length,\r\n        do_sample,\r\n        early_stopping,\r\n        temperature,\r\n        top_k,\r\n        top_p,\r\n        repetition_penalty,\r\n        no_repeat_ngram_size,\r\n        bad_words_ids,\r\n        pad_token_id,\r\n        eos_token_id,\r\n        batch_size,\r\n        num_return_sequences,\r\n        length_penalty,\r\n        num_beams,\r\n        vocab_size,\r\n        encoder_outputs,\r\n        attention_mask,\r\n        use_cache,\r\n        decoder_attention_mask,\r\n        model_specific_kwargs,\r\n    ):\r\n        \"\"\" Generate sequences for each example with beam search.\r\n        \"\"\"\r\n\r\n        # generated hypotheses\r\n        generated_hyps = [\r\n            BeamHypotheses(num_beams, max_length, length_penalty, early_stopping=early_stopping)\r\n            for _ in range(batch_size)\r\n        ]\r\n\r\n        # scores for each sentence in the beam\r\n        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\r\n\r\n        # for greedy decoding it is made sure that only tokens of the first beam are considered to avoid sampling the exact same tokens three times\r\n        if do_sample is False:\r\n            beam_scores[:, 1:] = -1e9\r\n        beam_scores = beam_scores.view(-1)  # shape (batch_size * num_beams,)\r\n\r\n        # cache compute states\r\n        past = (encoder_outputs, None) if encoder_outputs is not None else None\r\n\r\n        # done sentences\r\n        done = [False for _ in range(batch_size)]\r\n\r\n        while cur_len < max_length:\r\n            model_inputs = self.prepare_inputs_for_generation(\r\n                input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache, **model_specific_kwargs\r\n            )\r\n            model_inputs['decoder_attention_mask'] = decoder_attention_mask\r\n            outputs = self(**model_inputs)  # (batch_size * num_beams, cur_len, vocab_size)\r\n            next_token_logits = outputs[0][:, -1, :]  # (batch_size * num_beams, vocab_size)\r\n\r\n            # if model has past, then set the past variable to speed up decoding\r\n            if self._use_cache(outputs, use_cache):\r\n                past = outputs[1]\r\n            if self.config.is_encoder_decoder and do_sample is False:\r\n                # TODO (PVP) still a bit hacky here - there might be a better solution\r\n                next_token_logits = self.adjust_logits_during_generation(\r\n                    next_token_logits, cur_len=cur_len, max_length=max_length\r\n                )\r\n\r\n            scores = F.log_softmax(next_token_logits, dim=-1)  # (batch_size * num_beams, vocab_size)\r\n\r\n            scores = self.postprocess_next_token_scores(\r\n                scores=scores,\r\n                input_ids=input_ids,\r\n                no_repeat_ngram_size=no_repeat_ngram_size,\r\n                bad_words_ids=bad_words_ids,\r\n                cur_len=cur_len,\r\n                min_length=min_length,\r\n                max_length=max_length,\r\n                eos_token_id=eos_token_id,\r\n                repetition_penalty=repetition_penalty,\r\n                batch_size=batch_size,\r\n                num_beams=num_beams,\r\n            )\r\n\r\n            assert scores.shape == (batch_size * num_beams, vocab_size), \"Shapes of scores: {} != {}\".format(\r\n                scores.shape, (batch_size * num_beams, vocab_size)\r\n            )\r\n\r\n            if do_sample:\r\n                _scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\r\n                # Temperature\r\n                if temperature != 1.0:\r\n                    _scores = _scores / temperature\r\n                # Top-p/top-k filtering\r\n                _scores = top_k_top_p_filtering(\r\n                    _scores, top_k=top_k, top_p=top_p, min_tokens_to_keep=2\r\n                )  # (batch_size * num_beams, vocab_size)\r\n                # re-organize to group the beam together to sample from all beam_idxs\r\n                _scores = _scores.contiguous().view(\r\n                    batch_size, num_beams * vocab_size\r\n                )  # (batch_size, num_beams * vocab_size)\r\n\r\n                # Sample 2 next tokens for each beam (so we have some spare tokens and match output of greedy beam search)\r\n                probs = F.softmax(_scores, dim=-1)\r\n                next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)  # (batch_size, num_beams * 2)\r\n                # Compute next scores\r\n                next_scores = torch.gather(_scores, -1, next_tokens)  # (batch_size, num_beams * 2)\r\n                # sort the sampled vector to make sure that the first num_beams samples are the best\r\n                next_scores, next_scores_indices = torch.sort(next_scores, descending=True, dim=1)\r\n                next_tokens = torch.gather(next_tokens, -1, next_scores_indices)  # (batch_size, num_beams * 2)\r\n\r\n            else:\r\n                next_scores = scores + beam_scores[:, None].expand_as(scores)  # (batch_size * num_beams, vocab_size)\r\n\r\n                # re-organize to group the beam together (we are keeping top hypothesis accross beams)\r\n                next_scores = next_scores.view(\r\n                    batch_size, num_beams * vocab_size\r\n                )  # (batch_size, num_beams * vocab_size)\r\n\r\n                next_scores, next_tokens = torch.topk(next_scores, 2 * num_beams, dim=1, largest=True, sorted=True)\r\n\r\n            assert next_scores.size() == next_tokens.size() == (batch_size, 2 * num_beams)\r\n\r\n            # next batch beam content\r\n            next_batch_beam = []\r\n\r\n            # for each sentence\r\n            for batch_idx in range(batch_size):\r\n\r\n                # if we are done with this sentence, add a pad token\r\n                if done[batch_idx]:\r\n                    assert (\r\n                        len(generated_hyps[batch_idx]) >= num_beams\r\n                    ), \"Batch can only be done if at least {} beams have been generated\".format(num_beams)\r\n                    assert (\r\n                        eos_token_id is not None and pad_token_id is not None\r\n                    ), \"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"\r\n                    next_batch_beam.extend([(0, pad_token_id, 0)] * num_beams)  # pad the batch\r\n                    continue\r\n\r\n                # next sentence beam content, this will get added to next_batch_beam\r\n                next_sent_beam = []\r\n\r\n                # next tokens for this sentence\r\n                for beam_token_rank, (beam_token_id, beam_token_score) in enumerate(\r\n                    zip(next_tokens[batch_idx], next_scores[batch_idx])\r\n                ):\r\n                    # get beam and token IDs\r\n                    beam_id = beam_token_id // vocab_size\r\n                    token_id = beam_token_id % vocab_size\r\n\r\n                    effective_beam_id = batch_idx * num_beams + beam_id\r\n                    # add to generated hypotheses if end of sentence\r\n                    if (eos_token_id is not None) and (token_id.item() == eos_token_id):\r\n                        # if beam_token does not belong to top num_beams tokens, it should not be added\r\n                        is_beam_token_worse_than_top_num_beams = beam_token_rank >= num_beams\r\n                        if is_beam_token_worse_than_top_num_beams:\r\n                            continue\r\n                        generated_hyps[batch_idx].add(\r\n                            input_ids[effective_beam_id].clone(), beam_token_score.item(),\r\n                        )\r\n                    else:\r\n                        # add next predicted token since it is not eos_token\r\n                        next_sent_beam.append((beam_token_score, token_id, effective_beam_id))\r\n\r\n                    # once the beam for next step is full, don't add more tokens to it.\r\n                    if len(next_sent_beam) == num_beams:\r\n                        break\r\n\r\n                # Check if we are done so that we can save a pad step if all(done)\r\n                done[batch_idx] = done[batch_idx] or generated_hyps[batch_idx].is_done(\r\n                    next_scores[batch_idx].max().item(), cur_len\r\n                )\r\n\r\n                # update next beam content\r\n                assert len(next_sent_beam) == num_beams, \"Beam should always be full\"\r\n                next_batch_beam.extend(next_sent_beam)\r\n                assert len(next_batch_beam) == num_beams * (batch_idx + 1), \"We should have added num_beams each step\"\r\n\r\n            # stop when we are done with each sentence\r\n            if all(done):\r\n                break\r\n\r\n            # sanity check / prepare next batch\r\n            assert len(next_batch_beam) == batch_size * num_beams\r\n            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\r\n            beam_tokens = input_ids.new([x[1] for x in next_batch_beam])\r\n            beam_idx = input_ids.new([x[2] for x in next_batch_beam])\r\n\r\n            # re-order batch and update current length\r\n            input_ids = input_ids[beam_idx, :]\r\n            input_ids = torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1)\r\n            cur_len = cur_len + 1\r\n\r\n            # re-order internal states\r\n            if past is not None:\r\n                past = self._reorder_cache(past, beam_idx)\r\n\r\n            # extend attention_mask for new generated input if only decoder\r\n            if self.config.is_encoder_decoder is False:\r\n                attention_mask = torch.cat(\r\n                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\r\n                )\r\n\r\n        # finalize all open beam hypotheses and add to generated hypotheses\r\n        for batch_idx in range(batch_size):\r\n            if done[batch_idx]:\r\n                continue\r\n\r\n            # test that beam scores match previously calculated scores if not eos and batch_idx not done\r\n            if eos_token_id is not None and all(\r\n                (token_id % vocab_size).item() != eos_token_id for token_id in next_tokens[batch_idx]\r\n            ):\r\n                assert torch.all(\r\n                    next_scores[batch_idx, :num_beams] == beam_scores.view(batch_size, num_beams)[batch_idx]\r\n                ), \"If batch_idx is not done, final next scores: {} have to equal to accumulated beam_scores: {}\".format(\r\n                    next_scores[:, :num_beams][batch_idx], beam_scores.view(batch_size, num_beams)[batch_idx],\r\n                )\r\n\r\n            # need to add best num_beams hypotheses to generated hyps\r\n            for beam_id in range(num_beams):\r\n                effective_beam_id = batch_idx * num_beams + beam_id\r\n                final_score = beam_scores[effective_beam_id].item()\r\n                final_tokens = input_ids[effective_beam_id]\r\n                generated_hyps[batch_idx].add(final_tokens, final_score)\r\n\r\n        # depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch\r\n        output_batch_size = batch_size if do_sample else batch_size * num_return_sequences\r\n        output_num_return_sequences_per_batch = 1 if do_sample else num_return_sequences\r\n\r\n        # select the best hypotheses\r\n        sent_lengths = input_ids.new(output_batch_size)\r\n        best = []\r\n\r\n        # retrieve best hypotheses\r\n        for i, hypotheses in enumerate(generated_hyps):\r\n            sorted_hyps = sorted(hypotheses.beams, key=lambda x: x[0])\r\n            for j in range(output_num_return_sequences_per_batch):\r\n                effective_batch_idx = output_num_return_sequences_per_batch * i + j\r\n                best_hyp = sorted_hyps.pop()[1]\r\n                sent_lengths[effective_batch_idx] = len(best_hyp)\r\n                best.append(best_hyp)\r\n\r\n        # shorter batches are padded\r\n        if sent_lengths.min().item() != sent_lengths.max().item():\r\n            assert pad_token_id is not None, \"`Pad_token_id` has to be defined\"\r\n            sent_max_len = min(sent_lengths.max().item() + 1, max_length)\r\n            decoded = input_ids.new(output_batch_size, sent_max_len).fill_(pad_token_id)\r\n\r\n            # fill with hypothesis and eos_token_id if necessary\r\n            for i, hypo in enumerate(best):\r\n                decoded[i, : sent_lengths[i]] = hypo\r\n                if sent_lengths[i] < max_length:\r\n                    decoded[i, sent_lengths[i]] = eos_token_id\r\n        else:\r\n            # none of the hypotheses have an eos_token\r\n            assert (len(hypo) == max_length for hypo in best)\r\n            decoded = torch.stack(best).type(torch.long).to(next(self.parameters()).device)\r\n\r\n        return decoded\r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6125", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6125/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6125/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6125/events", "html_url": "https://github.com/huggingface/transformers/issues/6125", "id": 667744938, "node_id": "MDU6SXNzdWU2Njc3NDQ5Mzg=", "number": 6125, "title": "problem about geting hidden_states using TFBertModel", "user": {"login": "jianrui1995", "id": 20520524, "node_id": "MDQ6VXNlcjIwNTIwNTI0", "avatar_url": "https://avatars1.githubusercontent.com/u/20520524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jianrui1995", "html_url": "https://github.com/jianrui1995", "followers_url": "https://api.github.com/users/jianrui1995/followers", "following_url": "https://api.github.com/users/jianrui1995/following{/other_user}", "gists_url": "https://api.github.com/users/jianrui1995/gists{/gist_id}", "starred_url": "https://api.github.com/users/jianrui1995/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jianrui1995/subscriptions", "organizations_url": "https://api.github.com/users/jianrui1995/orgs", "repos_url": "https://api.github.com/users/jianrui1995/repos", "events_url": "https://api.github.com/users/jianrui1995/events{/privacy}", "received_events_url": "https://api.github.com/users/jianrui1995/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-29T10:48:15Z", "updated_at": "2020-08-05T11:25:31Z", "closed_at": "2020-08-05T11:25:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n- Platform: Pycharm\r\n- Python version: 3.6\r\n- PyTorch version (GPU?): None\r\n- Tensorflow version (GPU?): 2.2.0 GPU\r\n- Using GPU in script?: No\r\n- Using distributed or parallel set-up in script?: No\r\n\r\n### Who can help\r\n<!-- Your issue will be replied to more quickly if you can figure out the right person to tag with @\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\n albert, bert, GPT2, XLM: @LysandreJik \r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n Speed and Memory Benchmarks: @patrickvonplaten\r\n Model Cards: @julien-c\r\n Translation: @sshleifer\r\n Summarization: @sshleifer\r\n TextGeneration: @TevenLeScao \r\n examples/distillation: @VictorSanh\r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n Text Generation: @TevenLeScao\r\n blenderbot: @mariamabarham\r\n Bart: @sshleifer\r\n Marian: @sshleifer\r\n T5: @patrickvonplaten\r\n Longformer/Reformer: @patrickvonplaten\r\n TransfoXL/XLNet: @TevenLeScao \r\n examples/seq2seq: @sshleifer\r\n tensorflow: @jplu \r\ndocumentation: @sgugger\r\n -->\r\n @LysandreJik @jplu \r\n## Information\r\n\r\nModel I am using Bert: I want to the outpts of **TFBertModel** class inclue **hidden_states**.  unfortunately\uff0c I use two method provided by transformers' document north can achieve this goal.\r\n\r\nThe first method:  **output_hidden_states=True** is passed to call(). code as follow:\r\n```\r\nclass Model(tf.keras.Model):\r\n    def __init__(self,init):\r\n        super(Model,self).__init__()\r\n        conf = transformers.BertConfig.from_json_file(\"model/chinese_L-12_H-768_A-12/config.json\")\r\n        self.bertmodel = transformers.TFBertModel.from_pretrained(\"bert-base-chinese\")\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None, mask=None):\r\n        out_bert = self.bertmodel(inputs,output_hidden_states=True)\r\n        return out_bert\r\n\r\nif __name__ == \"__main__\":\r\n    tokenizer = transformers.BertTokenizer(\"model/chinese_L-12_H-768_A-12/vocab.txt\")\r\n    text_2 = tokenizer.batch_encode_plus([\"\u4f60\u4e70\u554a\uff0c\u4e70\u4e86\u5c31\u662f\u6210\u90fd\u4eba\", \"\u4f60\u6765\u554a\uff0c\u6765\u4e86\u5c31\u662f\u6df1\u5733\u4eba\"], max_length=20, pad_to_max_length=True)\r\n    print(text_2)\r\n    model = Model()\r\n    out = model([tf.convert_to_tensor(text_2[\"input_ids\"]),tf.convert_to_tensor(text_2['attention_mask'])])\r\n    print(\"out\",out)\r\n```\r\n the print in consol only two tensor :**last_hidden_state** and **pooler_output**\r\n\r\nthe second method\uff1a set **config.output_hidden_states=True**. code as follow:\r\nconfig.json:\r\n```\r\n{\r\n  \"attention_probs_dropout_prob\": 0.1, \r\n  \"directionality\": \"bidi\", \r\n  \"hidden_act\": \"gelu\", \r\n  \"hidden_dropout_prob\": 0.1, \r\n  \"hidden_size\": 768, \r\n  \"initializer_range\": 0.02, \r\n  \"intermediate_size\": 3072, \r\n  \"max_position_embeddings\": 512, \r\n  \"num_attention_heads\": 12, \r\n  \"num_hidden_layers\": 12, \r\n  \"pooler_fc_size\": 768, \r\n  \"pooler_num_attention_heads\": 12, \r\n  \"pooler_num_fc_layers\": 3, \r\n  \"pooler_size_per_head\": 128, \r\n  \"pooler_type\": \"first_token_transform\", \r\n  \"type_vocab_size\": 2, \r\n  \"vocab_size\": 21128,\r\n  \"output_hidden_states\": true\r\n}\r\n```\r\nI set **\"output_hidden_states\": true** at last line.\r\ncode:\r\n```\r\nclass Model(tf.keras.Model):\r\n    def __init__(self,init):\r\n        super(Model,self).__init__()\r\n        conf = transformers.BertConfig.from_json_file(\"model/chinese_L-12_H-768_A-12/config.json\")\r\n        self.bertmodel = transformers.TFBertModel.from_pretrained(\"bert-base-chinese\",config=conf)\r\n\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=None, mask=None):\r\n        out_bert = self.bertmodel(inputs)\r\n        return out_bert\r\n\r\nif __name__ == \"__main__\":\r\n    tokenizer = transformers.BertTokenizer(\"model/chinese_L-12_H-768_A-12/vocab.txt\")\r\n    text_2 = tokenizer.batch_encode_plus([\"\u4f60\u4e70\u554a\uff0c\u4e70\u4e86\u5c31\u662f\u6210\u90fd\u4eba\", \"\u4f60\u6765\u554a\uff0c\u6765\u4e86\u5c31\u662f\u6df1\u5733\u4eba\"], max_length=20, pad_to_max_length=True)\r\n    print(text_2)\r\n    model = Model()\r\n    out = model([tf.convert_to_tensor(text_2[\"input_ids\"]),tf.convert_to_tensor(text_2['attention_mask'])])\r\n    print(\"out\",out)\r\n```\r\nthe print in consol is same as method one.\r\n\r\nbut if I cancel **@tf.function** in front of call(). the print is what I expect.\r\n\r\n\r\n## To reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.\r\n2.\r\n3.\r\n\r\n<!-- If you have code snippets, error messages, stack traces please provide them here as well.\r\n     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting\r\n     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you would expect to happen. -->\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6116", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6116/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6116/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6116/events", "html_url": "https://github.com/huggingface/transformers/issues/6116", "id": 667521606, "node_id": "MDU6SXNzdWU2Njc1MjE2MDY=", "number": 6116, "title": "No button for creating new post at the forum.", "user": {"login": "guotong1988", "id": 4702353, "node_id": "MDQ6VXNlcjQ3MDIzNTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4702353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guotong1988", "html_url": "https://github.com/guotong1988", "followers_url": "https://api.github.com/users/guotong1988/followers", "following_url": "https://api.github.com/users/guotong1988/following{/other_user}", "gists_url": "https://api.github.com/users/guotong1988/gists{/gist_id}", "starred_url": "https://api.github.com/users/guotong1988/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guotong1988/subscriptions", "organizations_url": "https://api.github.com/users/guotong1988/orgs", "repos_url": "https://api.github.com/users/guotong1988/repos", "events_url": "https://api.github.com/users/guotong1988/events{/privacy}", "received_events_url": "https://api.github.com/users/guotong1988/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-29T03:35:07Z", "updated_at": "2020-07-29T08:16:40Z", "closed_at": "2020-07-29T08:11:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "![image](https://user-images.githubusercontent.com/4702353/88753699-76b01080-d18f-11ea-80c3-905a006ad31a.png)\r\ndiscuss.huggingface.co", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6115", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6115/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6115/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6115/events", "html_url": "https://github.com/huggingface/transformers/issues/6115", "id": 667512729, "node_id": "MDU6SXNzdWU2Njc1MTI3Mjk=", "number": 6115, "title": "Usage of Pytorch Native AMP in place of apex (Pytorch 1.6) in Trainer", "user": {"login": "prajjwal1", "id": 24690051, "node_id": "MDQ6VXNlcjI0NjkwMDUx", "avatar_url": "https://avatars2.githubusercontent.com/u/24690051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prajjwal1", "html_url": "https://github.com/prajjwal1", "followers_url": "https://api.github.com/users/prajjwal1/followers", "following_url": "https://api.github.com/users/prajjwal1/following{/other_user}", "gists_url": "https://api.github.com/users/prajjwal1/gists{/gist_id}", "starred_url": "https://api.github.com/users/prajjwal1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prajjwal1/subscriptions", "organizations_url": "https://api.github.com/users/prajjwal1/orgs", "repos_url": "https://api.github.com/users/prajjwal1/repos", "events_url": "https://api.github.com/users/prajjwal1/events{/privacy}", "received_events_url": "https://api.github.com/users/prajjwal1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-29T03:08:06Z", "updated_at": "2020-07-31T08:23:30Z", "closed_at": "2020-07-31T08:23:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "# \ud83d\ude80 Feature request\r\nIt would be nice to remove the Apex dependency for `fp16` training and use the native [Pytorch's AMP methods](https://github.com/pytorch/pytorch/releases) in the `Trainer` method. Pytorch recommends Apex users to switch to it's native implementation, even [Apex does it so](https://github.com/NVIDIA/apex/issues/818). Moreover, it will eliminate the need for users to build apex by themselves. \r\n\r\n## Your contribution\r\nI am happy to submit a PR if you think it would be a good addition. Please let me know.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6112", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6112/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6112/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6112/events", "html_url": "https://github.com/huggingface/transformers/issues/6112", "id": 667469044, "node_id": "MDU6SXNzdWU2Njc0NjkwNDQ=", "number": 6112, "title": "Is there any way that I can use the HuggingFace Transformers as Pyro models?", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-29T00:48:12Z", "updated_at": "2020-07-29T16:12:00Z", "closed_at": "2020-07-29T16:12:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\n`Pyro` is a Python module that allows users to convert a given (frequentist) neural network into a Bayesian neural network.\r\nI can convert a HuggingFace Transformer into a Pyro model like below:\r\n```python\r\n\r\nimport torch\r\nfrom torch import distributions\r\nfrom transformers import RobertaTokenizer, RobertaForMultipleChoice\r\nimport pyro\r\nimport pyro.infer\r\nimport pyro.optim\r\nimport pyro.distributions as dist\r\nimport pyro.nn.module as module\r\nfrom torch import nn\r\nfrom pyro.infer import SVI\r\n\r\n# get the pre-trained HuggingFace RobertaForMultipleChoice and resize the token embeddings after adding the special token\r\nmodel_RobertaForMultipleChoice = RobertaForMultipleChoice.from_pretrained('roberta-large', output_hidden_states = True)\r\n        \r\nmodule.to_pyro_module_(model_RobertaForMultipleChoice)\r\n        \r\n# Now we can attempt to be fully Bayesian:\r\nfor m in model_RobertaForMultipleChoice.modules():\r\n      for name, value in list(m.named_parameters(recurse=False)):\r\n                setattr(m, name, module.PyroSample(prior=dist.Normal(0, 1)\r\n                                              .expand(value.shape)\r\n                                              .to_event(value.dim())))\r\n\r\n# define parameters for training      \r\nguide_delta = guides.AutoDelta(model_RobertaForMultipleChoice)\r\n```\r\n\r\nBut when I try to compute the mc_loss from this Bayesian Transformer, Python generates an error:\r\n\r\n```python\r\nmc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels = mc_labels)[0]\r\n\r\nTraceback (most recent call last):\r\n  File \"STAT946_final_project_code_v4.py\", line 625, in <module>\r\n    success_rate_list_diag_normal = main_function_diag_normal('/home/ec2-user/test.txt', 'test_ans_num.txt', num_iter, log_interval)\r\n  File \"STAT946_final_project_code_v4.py\", line 415, in main_function_diag_normal\r\n    best_model_RobertaForMultipleChoice_diag_normal = train_loop(model_RobertaForMultipleChoice, tokenizer, optimizer_1, scheduler_1, log_interval, svi_diag_normal, guide_diag_normal, best_model_RobertaForMultipleChoice_diag_normal)\r\n  File \"STAT946_final_project_code_v4.py\", line 342, in train_loop\r\n    optimizer, scheduler, log_interval, svi, guide, epoch)\r\n  File \"STAT946_final_project_code_v4.py\", line 237, in train_mc_head\r\n    mc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels = mc_labels)[0]\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py\", line 413, in __call__\r\n    return super().__call__(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_roberta.py\", line 441, in forward\r\n    output_hidden_states=output_hidden_states,\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py\", line 413, in __call__\r\n    return super().__call__(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\", line 732, in forward\r\n    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 228, in get_extended_attention_mask\r\n    extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 159, in dtype\r\n    first_tuple = next(gen)\r\nStopIteration\r\n```\r\n\r\nIs there any way that I can compute the mc_loss in the regular way after converting HuggingFace Transformer into a Bayesian Transformer?\r\n\r\nThank you.,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6109", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6109/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6109/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6109/events", "html_url": "https://github.com/huggingface/transformers/issues/6109", "id": 667353926, "node_id": "MDU6SXNzdWU2NjczNTM5MjY=", "number": 6109, "title": "StopIteration error in RobertaForMultipleChoice", "user": {"login": "h56cho", "id": 52889259, "node_id": "MDQ6VXNlcjUyODg5MjU5", "avatar_url": "https://avatars1.githubusercontent.com/u/52889259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h56cho", "html_url": "https://github.com/h56cho", "followers_url": "https://api.github.com/users/h56cho/followers", "following_url": "https://api.github.com/users/h56cho/following{/other_user}", "gists_url": "https://api.github.com/users/h56cho/gists{/gist_id}", "starred_url": "https://api.github.com/users/h56cho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h56cho/subscriptions", "organizations_url": "https://api.github.com/users/h56cho/orgs", "repos_url": "https://api.github.com/users/h56cho/repos", "events_url": "https://api.github.com/users/h56cho/events{/privacy}", "received_events_url": "https://api.github.com/users/h56cho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-28T20:29:57Z", "updated_at": "2020-07-29T00:48:28Z", "closed_at": "2020-07-29T00:48:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI am trying to execute the line below for my `RobertaForMultipleChoice` model:\r\n```python\r\n# retrieve the resulting mc_loss\r\nmc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels =  mc_labels)[0]\r\n```\r\nbut this generates the following error:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"STAT946_final_project_code_v4.py\", line 623, in <module>\r\n    success_rate_list_diag_normal = main_function_diag_normal('/home/ec2-user/test.txt', 'test_ans_num.txt', num_iter, log_interval)\r\n  File \"STAT946_final_project_code_v4.py\", line 414, in main_function_diag_normal\r\n    best_model_RobertaForMultipleChoice_diag_normal = train_loop(model_RobertaForMultipleChoice, tokenizer, optimizer_1, scheduler_1, log_interval, svi_diag_normal, guide_diag_normal, best_model_RobertaForMultipleChoice_diag_normal)\r\n  File \"STAT946_final_project_code_v4.py\", line 341, in train_loop\r\n    optimizer, scheduler, log_interval, svi, guide, epoch)\r\n  File \"STAT946_final_project_code_v4.py\", line 236, in train_mc_head\r\n    mc_loss = model(input_ids = input_ids, attention_mask = attention_mask, labels =  mc_labels)[0]\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py\", line 413, in __call__\r\n    return super().__call__(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_roberta.py\", line 441, in forward\r\n    output_hidden_states=output_hidden_states,\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/pyro/nn/module.py\", line 413, in __call__\r\n    return super().__call__(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_bert.py\", line 732, in forward\r\n    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 228, in get_extended_attention_mask\r\n    extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\r\n  File \"/home/ec2-user/anaconda3/lib/python3.7/site-packages/transformers/modeling_utils.py\", line 159, in dtype\r\n    first_tuple = next(gen)\r\nStopIteration\r\n```\r\n\r\nHow can I get around this type of error? Thank you,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6108", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6108/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6108/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6108/events", "html_url": "https://github.com/huggingface/transformers/issues/6108", "id": 667338760, "node_id": "MDU6SXNzdWU2NjczMzg3NjA=", "number": 6108, "title": "allenai/longformer-large-4096 unavailable", "user": {"login": "CMobley7", "id": 10121829, "node_id": "MDQ6VXNlcjEwMTIxODI5", "avatar_url": "https://avatars0.githubusercontent.com/u/10121829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CMobley7", "html_url": "https://github.com/CMobley7", "followers_url": "https://api.github.com/users/CMobley7/followers", "following_url": "https://api.github.com/users/CMobley7/following{/other_user}", "gists_url": "https://api.github.com/users/CMobley7/gists{/gist_id}", "starred_url": "https://api.github.com/users/CMobley7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CMobley7/subscriptions", "organizations_url": "https://api.github.com/users/CMobley7/orgs", "repos_url": "https://api.github.com/users/CMobley7/repos", "events_url": "https://api.github.com/users/CMobley7/events{/privacy}", "received_events_url": "https://api.github.com/users/CMobley7/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-28T20:05:14Z", "updated_at": "2020-07-28T22:14:14Z", "closed_at": "2020-07-28T22:14:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "For some reason, I'm unable to download allenai/longformer-large-4096. Everything was working an hour ago, but all of a sudden I get the error included below. It's still listed on https://huggingface.co/models?search=allenai%2Flongformer-large-4096. I'm not sure what's up. Any ideas?\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\", line 242, in get_config_dict\r\n    raise EnvironmentError\r\nOSError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/ptcc/run_glue.py\", line 246, in <module>\r\n    main()\r\n  File \"/ptcc/run_glue.py\", line 123, in main\r\n    cache_dir=model_args.cache_dir,\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_auto.py\", line 203, in from_pretrained\r\n    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\", line 251, in get_config_dict\r\n    raise EnvironmentError(msg)\r\nOSError: Can't load config for 'allenai/longformer-large-4096'. Make sure that:\r\n\r\n- 'allenai/longformer-large-4096' is a correct model identifier listed on 'https://huggingface.co/models'\r\n\r\n- or 'allenai/longformer-large-4096' is the correct path to a directory containing a config.json file\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\", line 242, in get_config_dict\r\n    raise EnvironmentError\r\nOSError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/ptcc/run_glue.py\", line 246, in <module>\r\n    main()\r\n  File \"/ptcc/run_glue.py\", line 123, in main\r\n    cache_dir=model_args.cache_dir,\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_auto.py\", line 203, in from_pretrained\r\n    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\", line 251, in get_config_dict\r\n    raise EnvironmentError(msg)\r\nOSError: Can't load config for 'allenai/longformer-large-4096'. Make sure that:\r\n\r\n- 'allenai/longformer-large-4096' is a correct model identifier listed on 'https://huggingface.co/models'\r\n\r\n- or 'allenai/longformer-large-4096' is the correct path to a directory containing a config.json file\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6107", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6107/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6107/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6107/events", "html_url": "https://github.com/huggingface/transformers/issues/6107", "id": 667326244, "node_id": "MDU6SXNzdWU2NjczMjYyNDQ=", "number": 6107, "title": "Where do the Masked Language Model perform mask on the input data", "user": {"login": "SusanSun8", "id": 61705975, "node_id": "MDQ6VXNlcjYxNzA1OTc1", "avatar_url": "https://avatars2.githubusercontent.com/u/61705975?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SusanSun8", "html_url": "https://github.com/SusanSun8", "followers_url": "https://api.github.com/users/SusanSun8/followers", "following_url": "https://api.github.com/users/SusanSun8/following{/other_user}", "gists_url": "https://api.github.com/users/SusanSun8/gists{/gist_id}", "starred_url": "https://api.github.com/users/SusanSun8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SusanSun8/subscriptions", "organizations_url": "https://api.github.com/users/SusanSun8/orgs", "repos_url": "https://api.github.com/users/SusanSun8/repos", "events_url": "https://api.github.com/users/SusanSun8/events{/privacy}", "received_events_url": "https://api.github.com/users/SusanSun8/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "patrickvonplaten", "id": 23423619, "node_id": "MDQ6VXNlcjIzNDIzNjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/23423619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patrickvonplaten", "html_url": "https://github.com/patrickvonplaten", "followers_url": "https://api.github.com/users/patrickvonplaten/followers", "following_url": "https://api.github.com/users/patrickvonplaten/following{/other_user}", "gists_url": "https://api.github.com/users/patrickvonplaten/gists{/gist_id}", "starred_url": "https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patrickvonplaten/subscriptions", "organizations_url": "https://api.github.com/users/patrickvonplaten/orgs", "repos_url": "https://api.github.com/users/patrickvonplaten/repos", "events_url": "https://api.github.com/users/patrickvonplaten/events{/privacy}", "received_events_url": "https://api.github.com/users/patrickvonplaten/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-28T19:46:47Z", "updated_at": "2020-08-11T16:58:08Z", "closed_at": "2020-08-11T16:58:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \u2753 Questions & Help\r\n\r\n<!-- The GitHub issue tracker is primarly intended for bugs, feature requests,\r\n     new models and benchmarks, and migration questions. For all other questions,\r\n     we direct you to the Hugging Face forum: https://discuss.huggingface.co/ .\r\n     You can also try Stack Overflow (SO) where a whole community of PyTorch and\r\n     Tensorflow enthusiast can help you out. In this case, make sure to tag your\r\n     question with the right deep learning framework as well as the\r\n     huggingface-transformers tag: \r\n     https://stackoverflow.com/questions/tagged/huggingface-transformers \r\n     -->\r\n\r\n## Details\r\n<!-- Description of your issue -->\r\n\r\nI am trying to pre-train a bert from scratch with my own word set using only the Masked Language Model. \r\nI have a trouble finding where exactly the code masks 15% of the words and replaces it with 80% mask, 10% random, and 10% original. \r\n\r\nI noticed that the input \"labels\" kind of refers to the places where words are masked. Does it mean that when I preprocess the data, I need to masked it myself and then indicates the position of masks in the \"labels\" input? If so, is \"labels\" the only input that would be affect? Is there any other input variables, such as the \"masked_lm_positions\" and \"masked_lm_ids\" in google-bert that I need to take care of?\r\n\r\n\r\n<!-- You should first ask your question on the forum or SO, and only if\r\n     you didn't get an answer ask it here on GitHub. -->\r\n**A link to original question on the forum/Stack Overflow**:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/transformers/issues/6096", "repository_url": "https://api.github.com/repos/huggingface/transformers", "labels_url": "https://api.github.com/repos/huggingface/transformers/issues/6096/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/transformers/issues/6096/comments", "events_url": "https://api.github.com/repos/huggingface/transformers/issues/6096/events", "html_url": "https://github.com/huggingface/transformers/issues/6096", "id": 667117856, "node_id": "MDU6SXNzdWU2NjcxMTc4NTY=", "number": 6096, "title": "mBART: incorrect <mask> token id", "user": {"login": "OlegPlatonov", "id": 32016523, "node_id": "MDQ6VXNlcjMyMDE2NTIz", "avatar_url": "https://avatars1.githubusercontent.com/u/32016523?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OlegPlatonov", "html_url": "https://github.com/OlegPlatonov", "followers_url": "https://api.github.com/users/OlegPlatonov/followers", "following_url": "https://api.github.com/users/OlegPlatonov/following{/other_user}", "gists_url": "https://api.github.com/users/OlegPlatonov/gists{/gist_id}", "starred_url": "https://api.github.com/users/OlegPlatonov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OlegPlatonov/subscriptions", "organizations_url": "https://api.github.com/users/OlegPlatonov/orgs", "repos_url": "https://api.github.com/users/OlegPlatonov/repos", "events_url": "https://api.github.com/users/OlegPlatonov/events{/privacy}", "received_events_url": "https://api.github.com/users/OlegPlatonov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sshleifer", "id": 6045025, "node_id": "MDQ6VXNlcjYwNDUwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/6045025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sshleifer", "html_url": "https://github.com/sshleifer", "followers_url": "https://api.github.com/users/sshleifer/followers", "following_url": "https://api.github.com/users/sshleifer/following{/other_user}", "gists_url": "https://api.github.com/users/sshleifer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sshleifer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sshleifer/subscriptions", "organizations_url": "https://api.github.com/users/sshleifer/orgs", "repos_url": "https://api.github.com/users/sshleifer/repos", "events_url": "https://api.github.com/users/sshleifer/events{/privacy}", "received_events_url": "https://api.github.com/users/sshleifer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-28T14:25:21Z", "updated_at": "2020-07-28T22:27:59Z", "closed_at": "2020-07-28T22:27:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "# \ud83d\udc1b Bug\r\n\r\n## Information\r\n\r\nModel I am using: mBART\r\n\r\n## To reproduce\r\n\r\n```\r\nfrom transformers import MBartTokenizer\r\ntokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-cc25')\r\nprint(tokenizer.convert_tokens_to_ids(['<mask>', 'ar_AR']))\r\n```\r\nThe output for the above code is `[250001, 250001]`  - two different special tokens are mapped to the same id.\r\n\r\n## Expected behavior\r\n\r\nAs far as I can tell, `<mask>` token should be mapped to id 250026.\r\n\r\nI've checked [fairseq implementation](https://github.com/pytorch/fairseq/blob/master/fairseq/tasks/multilingual_denoising.py) and it seems that `<mask>` token is added after all the language codes, so it should be the last token in the vocab.\r\n\r\nCurrently, when I try to use mBART to denoise text with `<mask>` tokens, it mostly just ignores them, but if I replace mask ids with 250026, the model actually generates new text in place of `<mask>` tokens:\r\n\r\n```\r\nfrom transformers import MBartTokenizer, BartForConditionalGeneration\r\n\r\ntokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-cc25')\r\nmodel = BartForConditionalGeneration.from_pretrained('facebook/mbart-large-cc25')\r\n\r\ntext = 'I highly recommend <mask> - it is one of the best <mask> ever read!'\r\ninputs = tokenizer.prepare_translation_batch([text], src_lang='en_XX')\r\n\r\noutputs = model.generate(inputs['input_ids'], decoder_start_token_id=tokenizer.lang_code_to_id['en_XX'], \r\n                         num_beams=5)\r\nprint(tokenizer.batch_decode(outputs)[0])\r\n```\r\n\r\nThe output is:\r\n```\r\nen_XX<s> highly recommend - it is one of the best ever read!\r\n```\r\nReplacing mask ids:\r\n\r\n```\r\nwhere = (inputs['input_ids'] == 250001)\r\ninputs['input_ids'][where] = 250026\r\n\r\noutputs = model.generate(inputs['input_ids'], decoder_start_token_id=tokenizer.lang_code_to_id['en_XX'], \r\n                         num_beams=5)\r\nprint(tokenizer.batch_decode(outputs)[0])\r\n```\r\n\r\nThe output is:\r\n```\r\nen_XX<s> highly recommend this book - it is one of the best books I have ever read!\r\n```\r\n\r\n(In both cases, the model also skips the first input token when generating output, as discussed in #5755.)\r\n\r\nI've also noticed that fairseq is using [language code tokens](https://github.com/pytorch/fairseq/blob/108bb2560b1ec01524ba723bc7c69186875afa0a/fairseq/tasks/multilingual_denoising.py#L62) of the form `[en_XX]` rather than just `en_XX`, which can lead to different tokenization if words like  `en_XX` appear in the text, but that's a rather contrived case.\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.\r\n     Don't forget to fill out the missing fields in that output! -->\r\n     \r\n- `transformers` version: 3.0.2\r\n\r\n\r\n@sshleifer\r\n", "performed_via_github_app": null, "score": 1.0}]}