{"total_count": 24, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/83", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/83/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/83/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/83/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/83", "id": 481054100, "node_id": "MDU6SXNzdWU0ODEwNTQxMDA=", "number": 83, "title": "Spider crawlera_enabled argument priority", "user": {"login": "edorofeev", "id": 32255827, "node_id": "MDQ6VXNlcjMyMjU1ODI3", "avatar_url": "https://avatars3.githubusercontent.com/u/32255827?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edorofeev", "html_url": "https://github.com/edorofeev", "followers_url": "https://api.github.com/users/edorofeev/followers", "following_url": "https://api.github.com/users/edorofeev/following{/other_user}", "gists_url": "https://api.github.com/users/edorofeev/gists{/gist_id}", "starred_url": "https://api.github.com/users/edorofeev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edorofeev/subscriptions", "organizations_url": "https://api.github.com/users/edorofeev/orgs", "repos_url": "https://api.github.com/users/edorofeev/repos", "events_url": "https://api.github.com/users/edorofeev/events{/privacy}", "received_events_url": "https://api.github.com/users/edorofeev/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-15T08:44:14Z", "updated_at": "2019-11-27T12:27:05Z", "closed_at": "2019-11-27T12:27:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I just want to understand, is it expectable behavior or not :)\r\n\r\nhttps://github.com/scrapy-plugins/scrapy-crawlera/blob/c416d65cff762f6cbf372a99e5e6d81cddf01a4d/scrapy_crawlera/middleware.py#L141\r\n\r\n```\r\nreturn(\r\n    getattr(spider, 'crawlera_enabled', False) or\r\n    getattr(spider, 'use_hubproxy', False) or\r\n    self.crawler.settings.getbool(\"CRAWLERA_ENABLED\") or\r\n    self.crawler.settings.getbool(\"HUBPROXY_ENABLED\")\r\n    )\r\n```\r\n\r\nThis construction will return True if anyone will be True. But in this case, if we have `CRAWLERA_ENABLED = True` in settings.py, but `crawlera_enabled = False` as spider argument - crawlera will be enabled.\r\nIn my opinion, it's a little bit confused. I think spider argument should have more priority than `crawler.settings`.\r\nLike here: https://github.com/scrapy-plugins/scrapy-crawlera/blob/master/scrapy_crawlera/middleware.py#L120", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/77", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/77/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/77/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/77/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/77", "id": 457527462, "node_id": "MDU6SXNzdWU0NTc1Mjc0NjI=", "number": 77, "title": "Enhance exception description when CRAWLERA_URL is missing the scheme", "user": {"login": "raphapassini", "id": 133731, "node_id": "MDQ6VXNlcjEzMzczMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/133731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raphapassini", "html_url": "https://github.com/raphapassini", "followers_url": "https://api.github.com/users/raphapassini/followers", "following_url": "https://api.github.com/users/raphapassini/following{/other_user}", "gists_url": "https://api.github.com/users/raphapassini/gists{/gist_id}", "starred_url": "https://api.github.com/users/raphapassini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raphapassini/subscriptions", "organizations_url": "https://api.github.com/users/raphapassini/orgs", "repos_url": "https://api.github.com/users/raphapassini/repos", "events_url": "https://api.github.com/users/raphapassini/events{/privacy}", "received_events_url": "https://api.github.com/users/raphapassini/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-18T14:48:21Z", "updated_at": "2019-08-02T18:37:45Z", "closed_at": "2019-08-02T18:37:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "If you inadvertently set you `CRAWLERA_URL` setting without the URL scheme like:\r\n\r\n ```CRAWLERA_URL = \"proxy.crawlera.com:8010\" ```\r\n\r\nYou'll receive a non-descriptive twisted exception when trying to crawl `http://`\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/bin/scrapy\", line 10, in <module>\r\n    sys.exit(execute())\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/scrapy/cmdline.py\", line 150, in execute\r\n    _run_print_help(parser, _run_command, cmd, args, opts)\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/scrapy/cmdline.py\", line 90, in _run_print_help\r\n    func(*a, **kw)\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/scrapy/cmdline.py\", line 157, in _run_command\r\n    cmd.run(args, opts)\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/scrapy/commands/shell.py\", line 74, in run\r\n    shell.start(url=url, redirect=not opts.no_redirect)\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/scrapy/shell.py\", line 47, in start\r\n    self.fetch(url, spider, redirect=redirect)\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/scrapy/shell.py\", line 120, in fetch\r\n    reactor, self._schedule, request, spider\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/twisted/internet/threads.py\", line 122, in blockingCallFromThread\r\n    result.raiseException()\r\n  File \"/home/raphael/.virtualenvs/myproject-jQmK5Pxo/lib/python3.6/site-packages/twisted/python/failure.py\", line 488, in raiseException\r\n    raise self.value.with_traceback(self.tb)\r\ntwisted.web.error.SchemeNotSupported: Unsupported scheme: b''\r\n```\r\n\r\nI think a good approach would be to identify the lack of the scheme on `CRAWLERA_URL` and throw a descriptive expection. This can be done at  `spider_open` signal we listen to on `CrawleraMiddleware`.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/76", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/76/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/76/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/76/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/76", "id": 456034937, "node_id": "MDU6SXNzdWU0NTYwMzQ5Mzc=", "number": 76, "title": "Version v1.6.0 is much unfriendly towards non-HTTP requests", "user": {"login": "starrify", "id": 388828, "node_id": "MDQ6VXNlcjM4ODgyOA==", "avatar_url": "https://avatars0.githubusercontent.com/u/388828?v=4", "gravatar_id": "", "url": "https://api.github.com/users/starrify", "html_url": "https://github.com/starrify", "followers_url": "https://api.github.com/users/starrify/followers", "following_url": "https://api.github.com/users/starrify/following{/other_user}", "gists_url": "https://api.github.com/users/starrify/gists{/gist_id}", "starred_url": "https://api.github.com/users/starrify/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/starrify/subscriptions", "organizations_url": "https://api.github.com/users/starrify/orgs", "repos_url": "https://api.github.com/users/starrify/repos", "events_url": "https://api.github.com/users/starrify/events{/privacy}", "received_events_url": "https://api.github.com/users/starrify/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-14T02:42:39Z", "updated_at": "2019-06-25T16:04:57Z", "closed_at": "2019-06-25T16:04:57Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "## 1. Summary of the Issue\r\n\r\nThe middleware may attempt to retry non-HTTP requests since version v1.6.0, or precisely, commit 11879301.\r\nAnd such retrying mechanism is not properly implemented:\r\n- If the original request comes with `dont_filter=False`, it'll be dropped when being enqueued the 2nd time in the scheduler for being duplicate.\r\n- If the original request comes with `dont_filter=True`, `CrawleraMiddleware` would attempt to retry it indefinitely.\r\n\r\n## 2. To Reproduce the Issue\r\n\r\nSample spider for reproducing the issue:\r\n\r\n```python\r\n# coding: utf8\r\n\r\nimport scrapy\r\n\r\n\r\nclass TestSpider(scrapy.Spider):\r\n    name = 'test'\r\n    custom_settings = {\r\n        'DOWNLOADER_MIDDLEWARES': {\r\n            'scrapy_crawlera.CrawleraMiddleware': 610,\r\n        },\r\n        'CRAWLERA_ENABLED': True,\r\n        'CRAWLERA_APIKEY': 'foobar',  # no worries. this'll work\r\n    }\r\n\r\n    def start_requests(self):\r\n        yield scrapy.Request('file:///tmp/foobar.txt')\r\n\r\n    def parse(self, response):\r\n        self.logger.info(response.text)\r\n```\r\nSample logs with `scrapy-crawlera==1.6.0`:\r\n```\r\n$ echo -n baz > /tmp/foobar.txt && scrapy runspider test.py\r\n2019-06-14 04:05:38 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\r\n2019-06-14 04:05:38 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.0, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.3 (default, Mar 26 2019, 21:43:19) - [GCC 8.2.1 20181127], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Linux-5.1.4-arch1-1-ARCH-x86_64-with-arch\r\n2019-06-14 04:05:38 [scrapy.crawler] INFO: Overridden settings: {'EDITOR': 'vim', 'SPIDER_LOADER_WARN_ONLY': True}\r\n2019-06-14 04:05:38 [scrapy.extensions.telnet] INFO: Telnet Password: 5dd83baa8682f121\r\n2019-06-14 04:05:38 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2019-06-14 04:05:38 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy_crawlera.CrawleraMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2019-06-14 04:05:38 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2019-06-14 04:05:38 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2019-06-14 04:05:38 [scrapy.core.engine] INFO: Spider opened\r\n2019-06-14 04:05:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2019-06-14 04:05:38 [root] INFO: Using crawlera at http://proxy.crawlera.com:8010 (apikey: foobar)\r\n2019-06-14 04:05:38 [root] INFO: CrawleraMiddleware: disabling download delays on Scrapy side to optimize delays introduced by Crawlera. To avoid this behaviour you can use the CRAWLERA_PRESERVE_DELAY setting but keep in mind that this may slow down the crawl significantly\r\n2019-06-14 04:05:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2019-06-14 04:05:38 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET file:///tmp/foobar.txt> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\r\n2019-06-14 04:05:38 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2019-06-14 04:05:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'crawlera/request': 1,\r\n 'crawlera/request/method/GET': 1,\r\n 'downloader/request_bytes': 257,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 22,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'dupefilter/filtered': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2019, 6, 14, 3, 5, 38, 235584),\r\n 'log_count/DEBUG': 1,\r\n 'log_count/INFO': 11,\r\n 'memusage/max': 54403072,\r\n 'memusage/startup': 54403072,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2019, 6, 14, 3, 5, 38, 229586)}\r\n2019-06-14 04:05:38 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\nSample logs with `scrapy-crawlera==1.5.1`:\r\n```\r\n$ echo -n baz > /tmp/foobar.txt && scrapy runspider test.py\r\n2019-06-14 04:04:58 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\r\n2019-06-14 04:04:58 [scrapy.utils.log] INFO: Versions: lxml 4.3.2.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.0, w3lib 1.20.0, Twisted 18.9.0, Python 3.7.3 (default, Mar 26 2019, 21:43:19) - [GCC 8.2.1 20181127], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Linux-5.1.4-arch1-1-ARCH-x86_64-with-arch\r\n2019-06-14 04:04:58 [scrapy.crawler] INFO: Overridden settings: {'EDITOR': 'vim', 'SPIDER_LOADER_WARN_ONLY': True}\r\n2019-06-14 04:04:58 [scrapy.extensions.telnet] INFO: Telnet Password: 930d9b1988e0e136\r\n2019-06-14 04:04:58 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2019-06-14 04:04:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy_crawlera.CrawleraMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2019-06-14 04:04:58 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2019-06-14 04:04:58 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2019-06-14 04:04:58 [scrapy.core.engine] INFO: Spider opened\r\n2019-06-14 04:04:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2019-06-14 04:04:58 [root] INFO: Using crawlera at http://proxy.crawlera.com:8010 (apikey: foobar)\r\n2019-06-14 04:04:58 [root] INFO: CrawleraMiddleware: disabling download delays on Scrapy side to optimize delays introduced by Crawlera. To avoid this behaviour you can use the CRAWLERA_PRESERVE_DELAY setting but keep in mind that this may slow down the crawl significantly\r\n2019-06-14 04:04:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2019-06-14 04:04:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET file:///tmp/foobar.txt> (referer: None)\r\n2019-06-14 04:04:58 [test] INFO: baz\r\n2019-06-14 04:04:58 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2019-06-14 04:04:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'crawlera/request': 1,\r\n 'crawlera/request/method/GET': 1,\r\n 'crawlera/response': 1,\r\n 'crawlera/response/status/200': 1,\r\n 'downloader/request_bytes': 257,\r\n 'downloader/request_count': 1,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/response_bytes': 22,\r\n 'downloader/response_count': 1,\r\n 'downloader/response_status_count/200': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2019, 6, 14, 3, 4, 58, 793630),\r\n 'log_count/DEBUG': 1,\r\n 'log_count/INFO': 12,\r\n 'memusage/max': 54611968,\r\n 'memusage/startup': 54611968,\r\n 'response_received_count': 1,\r\n 'scheduler/dequeued': 1,\r\n 'scheduler/dequeued/memory': 1,\r\n 'scheduler/enqueued': 1,\r\n 'scheduler/enqueued/memory': 1,\r\n 'start_time': datetime.datetime(2019, 6, 14, 3, 4, 58, 681959)}\r\n2019-06-14 04:04:58 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\nThe same issue may be reproduced in various common environments (different versions of Scrapy, Python, OS, etc.).\r\n\r\n## 3. The Cause\r\n\r\nThis issue is believed to have been caused by improperly designed response handling log. Related code: https://github.com/scrapy-plugins/scrapy-crawlera/blob/v1.6.0/scrapy_crawlera/middleware.py#L178\r\n\r\n## 4. Proposed Changes\r\n\r\n4.1. Return proper boolean values for non-HTTP requests / responses in methods including [`CrawleraMiddleware._should_enable_for_response`](https://github.com/scrapy-plugins/scrapy-crawlera/blob/v1.6.0/scrapy_crawlera/middleware.py#L254) and [`CrawleraMiddleware._is_enabled_for_request`](https://github.com/scrapy-plugins/scrapy-crawlera/blob/v1.6.0/scrapy_crawlera/middleware.py#L257) by checking the URL scheme.\r\n4.2. Other than retrying requests unconditionally on [this line](https://github.com/scrapy-plugins/scrapy-crawlera/blob/v1.6.0/scrapy_crawlera/middleware.py#L179), do it with a maximum retry limit like in the built-in `RetryMiddleware`, or just make use of that middleware directly. Also, `dont_filter=True` may be assigned if a request needs to be retried here.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/57", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/57/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/57/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/57/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/57", "id": 359183634, "node_id": "MDU6SXNzdWUzNTkxODM2MzQ=", "number": 57, "title": "Ignores DOWNLOAD_TIMEOUT setting", "user": {"login": "hcoura", "id": 10451850, "node_id": "MDQ6VXNlcjEwNDUxODUw", "avatar_url": "https://avatars0.githubusercontent.com/u/10451850?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hcoura", "html_url": "https://github.com/hcoura", "followers_url": "https://api.github.com/users/hcoura/followers", "following_url": "https://api.github.com/users/hcoura/following{/other_user}", "gists_url": "https://api.github.com/users/hcoura/gists{/gist_id}", "starred_url": "https://api.github.com/users/hcoura/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hcoura/subscriptions", "organizations_url": "https://api.github.com/users/hcoura/orgs", "repos_url": "https://api.github.com/users/hcoura/repos", "events_url": "https://api.github.com/users/hcoura/events{/privacy}", "received_events_url": "https://api.github.com/users/hcoura/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-11T19:13:20Z", "updated_at": "2019-05-27T13:59:35Z", "closed_at": "2019-05-27T13:59:35Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "scrapy crawlera only looks for CRAWLERA_DOWNLOAD_TIMEOUT and ignores the default way of setting timeouts in scrapy through [DownloadTimeoutMiddleware](https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/56", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/56/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/56/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/56/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/56", "id": 356720938, "node_id": "MDU6SXNzdWUzNTY3MjA5Mzg=", "number": 56, "title": "fetch https pages using scrapy + crawlera", "user": {"login": "amralaaalex", "id": 37495211, "node_id": "MDQ6VXNlcjM3NDk1MjEx", "avatar_url": "https://avatars2.githubusercontent.com/u/37495211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amralaaalex", "html_url": "https://github.com/amralaaalex", "followers_url": "https://api.github.com/users/amralaaalex/followers", "following_url": "https://api.github.com/users/amralaaalex/following{/other_user}", "gists_url": "https://api.github.com/users/amralaaalex/gists{/gist_id}", "starred_url": "https://api.github.com/users/amralaaalex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amralaaalex/subscriptions", "organizations_url": "https://api.github.com/users/amralaaalex/orgs", "repos_url": "https://api.github.com/users/amralaaalex/repos", "events_url": "https://api.github.com/users/amralaaalex/events{/privacy}", "received_events_url": "https://api.github.com/users/amralaaalex/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-09-04T09:14:40Z", "updated_at": "2019-01-22T18:16:18Z", "closed_at": "2019-01-22T18:16:18Z", "author_association": "NONE", "active_lock_reason": null, "body": " in crawlera they wrote:\r\nTo fetch HTTPS pages you will need to download and install the following certificate in your HTTP client or disable SSL certificate verification:\r\nI am getting timeout error when I try to fetch https pages, but it works fine if I disable cralwera or when I use crawlera to fetch http pages\r\nI do not know how to disable SSL certificate verification or how to install cralwera https certificate, I have downloaded cralwera https certificate, double clicked on it, followed the wizard but nothing changed\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/53", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/53/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/53/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/53/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/53", "id": 339643057, "node_id": "MDU6SXNzdWUzMzk2NDMwNTc=", "number": 53, "title": "Spider attributes for the most common headers", "user": {"login": "stummjr", "id": 1170435, "node_id": "MDQ6VXNlcjExNzA0MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1170435?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stummjr", "html_url": "https://github.com/stummjr", "followers_url": "https://api.github.com/users/stummjr/followers", "following_url": "https://api.github.com/users/stummjr/following{/other_user}", "gists_url": "https://api.github.com/users/stummjr/gists{/gist_id}", "starred_url": "https://api.github.com/users/stummjr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stummjr/subscriptions", "organizations_url": "https://api.github.com/users/stummjr/orgs", "repos_url": "https://api.github.com/users/stummjr/repos", "events_url": "https://api.github.com/users/stummjr/events{/privacy}", "received_events_url": "https://api.github.com/users/stummjr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-09T23:35:36Z", "updated_at": "2018-09-20T16:04:54Z", "closed_at": "2018-09-20T16:04:54Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Headers such as `X-Crawlera-Profile`, `X-Crawlera-Debug` and `X-Crawlera-Cookies` are quite often used by Crawlera users. As such it could be easier if this plugin offered builtin support for them as spider attributes, such as:\r\n\r\n```python\r\nclass FooSpider(scrapy.Spider):\r\n    name = 'foo'\r\n    crawlera_profile = 'mobile'\r\n    crawlera_debug = 'ua'\r\n    ...\r\n```\r\n\r\nWithout them, users always have to write their custom Crawlera middlewares or, even worse, pass the headers explicitly in each and every request issued by the spiders.\r\n\r\n**A quite common use case:** you want to quickly debug some Crawlera settings such as user agents, response times, etc. To do that, you have to build a custom middleware for that or modify your spiders in every place where they generate requests. The attributes make it more convenient, as it'd be just a matter of setting `crawlera_debug = 'ua'`.\r\n\r\nWhat do you think about adding support to such attributes in a spider level?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/51", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/51/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/51/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/51/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/51", "id": 300457684, "node_id": "MDU6SXNzdWUzMDA0NTc2ODQ=", "number": 51, "title": "scrapy-crawlera not passing auth credentials on to crawlera", "user": {"login": "justinwiley", "id": 4262, "node_id": "MDQ6VXNlcjQyNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justinwiley", "html_url": "https://github.com/justinwiley", "followers_url": "https://api.github.com/users/justinwiley/followers", "following_url": "https://api.github.com/users/justinwiley/following{/other_user}", "gists_url": "https://api.github.com/users/justinwiley/gists{/gist_id}", "starred_url": "https://api.github.com/users/justinwiley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justinwiley/subscriptions", "organizations_url": "https://api.github.com/users/justinwiley/orgs", "repos_url": "https://api.github.com/users/justinwiley/repos", "events_url": "https://api.github.com/users/justinwiley/events{/privacy}", "received_events_url": "https://api.github.com/users/justinwiley/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-27T01:16:10Z", "updated_at": "2018-02-27T01:28:17Z", "closed_at": "2018-02-27T01:28:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting 407 errors trying to connect to Yahoo via Crawlera after setting up the middleware.\r\n\r\nIn `settings.yml` I added CRAWLERA_USER and CRAWLERA_PASS.  I've verified this information works when using Crawlera via curl.  I've also tried CRAWLERA_APIKEY alone, without the other variables defined.\r\n\r\nI'm using Scrapy 1.5.  Any ideas?  Thanks!\r\n\r\n```\r\n2018-02-26 17:09:16 [root] INFO: Using crawlera at http://proxy.crawlera.com:8010 (user: myusername)\r\n2018-02-26 17:09:16 [root] INFO: CrawleraMiddleware: disabling download delays on Scrapy side to optimize delays introduced by Crawlera. To avoid this behaviour you can use the CRAWLERA_PRESERVE_DELAY setting but keep in mind that this may slow down the crawl significantly\r\n2018-02-26 17:09:16 [scrapy.core.engine] DEBUG: Crawled (407) <GET http://yahoo.com> (referer: None)\r\n```\r\n\r\nThe code in question:\r\n\r\n```\r\nrequest = scrapy.Request(\"http://yahoo.com\", callback=self.parse_search, headers={'user-agent': 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko'})\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/50", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/50/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/50/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/50/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/50", "id": 280707731, "node_id": "MDU6SXNzdWUyODA3MDc3MzE=", "number": 50, "title": "Scrapping is not done via Crawlera (on Docker?)", "user": {"login": "zivori", "id": 10966515, "node_id": "MDQ6VXNlcjEwOTY2NTE1", "avatar_url": "https://avatars2.githubusercontent.com/u/10966515?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zivori", "html_url": "https://github.com/zivori", "followers_url": "https://api.github.com/users/zivori/followers", "following_url": "https://api.github.com/users/zivori/following{/other_user}", "gists_url": "https://api.github.com/users/zivori/gists{/gist_id}", "starred_url": "https://api.github.com/users/zivori/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zivori/subscriptions", "organizations_url": "https://api.github.com/users/zivori/orgs", "repos_url": "https://api.github.com/users/zivori/repos", "events_url": "https://api.github.com/users/zivori/events{/privacy}", "received_events_url": "https://api.github.com/users/zivori/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-09T10:22:29Z", "updated_at": "2018-12-03T15:31:33Z", "closed_at": "2017-12-19T14:18:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Scrapping is not done via Crawlera. Or at least, it doesn't reach Crawlera dashboard.\r\n\r\nSimple curl request according to Crawlera examples does reach their dashboard.\r\nalso from inside Docker container, e.g.:\r\ncurl -U \"083c...dd:\" -x \"http://proxy.crawlera.com:8010\" http://....com\r\n\r\nI'm activating it through Docker (17.09.0-ce), the image starts from python:3.6.2-alpine3.6 ;\r\nScrapy==1.4.0 is used (scrapy-splash==0.7.2 is installed but not in use in the relevant process)\r\n\r\nI've tried on both HTTP and HTTPS pages\r\nTried on both current last stable version (1.2.4) and current GitHub master (2017, Dec 9). Only in the latter, I see there's no \"?noconnect\" in proxy URL.\r\n\r\nI've tried to use both         'CRAWLERA_APIKEY': '083c..',\r\nand         'CRAWLERA_USER': '083..' with 'CRAWLERA_PASS': '',\r\nand of course,         'CRAWLERA_ENABLED': True,\r\n\r\nNo error is seen in the log:\r\n```\r\nINFO:scrapy.core.engine:Spider opened\r\n2017-12-09 07:37:24 [scrapy.core.engine] INFO: Spider opened\r\n2017-12-09 07:37:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-12-09 07:37:24 [root] INFO: Using crawlera at proxy.crawlera.com:8010 (user: 083c...)\r\n2017-12-09 07:37:24 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-12-09 07:37:24 [scrapy.core.engine] DEBUG: Crawled (302) <GET http://www...> (referer: None)\r\n2017-12-09 07:37:24 [..Spider] INFO: Parse function: response.url=http://www....co.../..?.., response.headers={b'Location': [b'https://www....co.../..?..'], b'Cache-Control': [b'private'], b'Content-Type': [b'text/html; charset=UTF-8'], b'P3P': [b'CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"'], b'Date': [b'Sat, 09 Dec 2017 07:37:21 GMT'], b'Server': [b'gws'], b'X-Xss-Protection': [b'1; mode=block'], b'X-Frame-Options': [b'SAMEORIGIN'], b'Set-Cookie': [b'1P_JAR=2017-12-09-07; expires=Mon, 08-Jan-2018 07:37:21 GMT; path=/; domain=....co...', b'NID=119=..-..; expires=Sun, 10-Jun-2018 07:37:21 GMT; path=/; domain=....co...; HttpOnly']}\r\n2017-12-09 07:37:24 [scrapy.core.engine] INFO: Closing spider (finished)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/47", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/47/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/47/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/47/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/47", "id": 225740470, "node_id": "MDU6SXNzdWUyMjU3NDA0NzA=", "number": 47, "title": "[question] using scrapy-crawlera and scrapy imagesPipeline", "user": {"login": "hugocar", "id": 19691106, "node_id": "MDQ6VXNlcjE5NjkxMTA2", "avatar_url": "https://avatars0.githubusercontent.com/u/19691106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hugocar", "html_url": "https://github.com/hugocar", "followers_url": "https://api.github.com/users/hugocar/followers", "following_url": "https://api.github.com/users/hugocar/following{/other_user}", "gists_url": "https://api.github.com/users/hugocar/gists{/gist_id}", "starred_url": "https://api.github.com/users/hugocar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hugocar/subscriptions", "organizations_url": "https://api.github.com/users/hugocar/orgs", "repos_url": "https://api.github.com/users/hugocar/repos", "events_url": "https://api.github.com/users/hugocar/events{/privacy}", "received_events_url": "https://api.github.com/users/hugocar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-05-02T15:56:33Z", "updated_at": "2017-05-09T13:12:44Z", "closed_at": "2017-05-09T13:12:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nThank you for the awesome plugin.\r\n\r\nI m using scrapy-crawlera the way described in the documentation.\r\nI m also using the imagesPipeline described here [doc.scrapy.org/...](https://doc.scrapy.org/en/latest/topics/media-pipeline.html)\r\n\r\nDo the calls from the imagesPipeline go through Crawlera ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/46", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/46/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/46/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/46/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/46", "id": 221977647, "node_id": "MDU6SXNzdWUyMjE5Nzc2NDc=", "number": 46, "title": "Include spider object every time logging", "user": {"login": "vionemc", "id": 6565672, "node_id": "MDQ6VXNlcjY1NjU2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6565672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vionemc", "html_url": "https://github.com/vionemc", "followers_url": "https://api.github.com/users/vionemc/followers", "following_url": "https://api.github.com/users/vionemc/following{/other_user}", "gists_url": "https://api.github.com/users/vionemc/gists{/gist_id}", "starred_url": "https://api.github.com/users/vionemc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vionemc/subscriptions", "organizations_url": "https://api.github.com/users/vionemc/orgs", "repos_url": "https://api.github.com/users/vionemc/repos", "events_url": "https://api.github.com/users/vionemc/events{/privacy}", "received_events_url": "https://api.github.com/users/vionemc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-04-15T22:55:57Z", "updated_at": "2019-05-21T14:07:19Z", "closed_at": "2019-05-21T14:07:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Inside Scrapy, logging looks like this.\r\n\r\n```\r\n\t\t\tlogger.error(\"Error %d: %s\" % (e.args[0], e.args[1]),\r\n\t\t\t            extra={'spider': spider})\r\n```\r\n\r\nNotice the `extra`. It's useful to notice to which log it belongs to. It becomes crucial when `CrawlerProcess` for multiple spiders run is used so people can filter the log by spider name. \r\n\r\nCurrently, Scrapy Crawlera doesn't have it.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/45", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/45/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/45/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/45/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/45", "id": 218060769, "node_id": "MDU6SXNzdWUyMTgwNjA3Njk=", "number": 45, "title": "Scrapy-crawlera breaks SplashRequest", "user": {"login": "eusid", "id": 552127, "node_id": "MDQ6VXNlcjU1MjEyNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/552127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eusid", "html_url": "https://github.com/eusid", "followers_url": "https://api.github.com/users/eusid/followers", "following_url": "https://api.github.com/users/eusid/following{/other_user}", "gists_url": "https://api.github.com/users/eusid/gists{/gist_id}", "starred_url": "https://api.github.com/users/eusid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eusid/subscriptions", "organizations_url": "https://api.github.com/users/eusid/orgs", "repos_url": "https://api.github.com/users/eusid/repos", "events_url": "https://api.github.com/users/eusid/events{/privacy}", "received_events_url": "https://api.github.com/users/eusid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-03-30T02:09:45Z", "updated_at": "2019-05-21T14:08:36Z", "closed_at": "2019-05-21T14:08:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "If you are using Splash and crawlera middleware the splash requests will not work. I have not tracked the requests but I assume that its trying to access my local host through the proxy.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/41", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/41/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/41/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/41/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/41", "id": 216978935, "node_id": "MDU6SXNzdWUyMTY5Nzg5MzU=", "number": 41, "title": "Automate Crawlera certificate setup", "user": {"login": "pablohoffman", "id": 185212, "node_id": "MDQ6VXNlcjE4NTIxMg==", "avatar_url": "https://avatars1.githubusercontent.com/u/185212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pablohoffman", "html_url": "https://github.com/pablohoffman", "followers_url": "https://api.github.com/users/pablohoffman/followers", "following_url": "https://api.github.com/users/pablohoffman/following{/other_user}", "gists_url": "https://api.github.com/users/pablohoffman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pablohoffman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pablohoffman/subscriptions", "organizations_url": "https://api.github.com/users/pablohoffman/orgs", "repos_url": "https://api.github.com/users/pablohoffman/repos", "events_url": "https://api.github.com/users/pablohoffman/events{/privacy}", "received_events_url": "https://api.github.com/users/pablohoffman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-25T14:49:28Z", "updated_at": "2017-03-26T15:59:46Z", "closed_at": "2017-03-26T15:59:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I've heard many Crawlera users complaining about installing the certificate (to make https work with Crawlera) and I was thinking that, since the Crawlera middleware is meant to make things easier on the client side, it could automatically download and install the certificate if it's not present, to reduce another step in the \"crawlera setup process\". Installing the certificate is not a major task, but also not a trivial one, and it varies per platform. But most importantly, it can be automated in things like the Crawlera middleware.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/39", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/39/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/39/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/39/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/39", "id": 215075954, "node_id": "MDU6SXNzdWUyMTUwNzU5NTQ=", "number": 39, "title": "Define banned outcome based on code + message", "user": {"login": "tomasrinke", "id": 11856377, "node_id": "MDQ6VXNlcjExODU2Mzc3", "avatar_url": "https://avatars2.githubusercontent.com/u/11856377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomasrinke", "html_url": "https://github.com/tomasrinke", "followers_url": "https://api.github.com/users/tomasrinke/followers", "following_url": "https://api.github.com/users/tomasrinke/following{/other_user}", "gists_url": "https://api.github.com/users/tomasrinke/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomasrinke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomasrinke/subscriptions", "organizations_url": "https://api.github.com/users/tomasrinke/orgs", "repos_url": "https://api.github.com/users/tomasrinke/repos", "events_url": "https://api.github.com/users/tomasrinke/events{/privacy}", "received_events_url": "https://api.github.com/users/tomasrinke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-17T17:47:54Z", "updated_at": "2019-01-22T18:14:34Z", "closed_at": "2019-01-22T18:14:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "As seen here: https://doc.scrapinghub.com/crawlera.html#errors\r\n\r\n503 could mean multiple errors, not just a ban:\r\n```\r\nX-Crawlera-Error\tResponse Code\tError Message\r\n...\r\nnoslaves\t503\tNo available proxies\r\nslavebanned\t503\tWebsite crawl ban\r\nserverbusy\t503\tServer busy: too many outstanding requests\r\n...\r\n```\r\nscrapy-crawlera only checks for the code, and could be misleading. \r\n  ```\r\n      if response.status == self.ban_code:\r\n            self._bans[key] += 1\r\n            if self._bans[key] > self.maxbans:\r\n                self.crawler.engine.close_spider(spider, 'banned')\r\n            else:\r\n                after = response.headers.get('retry-after')\r\n                if after:\r\n                    self._set_custom_delay(request, float(after))\r\n            self.crawler.stats.inc_value('crawlera/response/banned')\r\n        else:\r\n```\r\n\r\nIMHO it should consider the message of the response as well: HTTP code 503 and \"Proxy has been banned\" \r\n\r\nI discovered that this is the output of scrapy:\r\n```\r\n{'crawlera/request': 410730,\r\n 'crawlera/request/method/GET': 410730,\r\n 'crawlera/response': 410412,\r\n 'crawlera/response/banned': 433,\r\n 'crawlera/response/error': 48,\r\n 'crawlera/response/error/banned': 15,\r\n 'crawlera/response/error/internal_error': 15,\r\n 'crawlera/response/error/timeout': 18,\r\n 'crawlera/response/status/200': 409414,\r\n 'crawlera/response/status/400': 514,\r\n 'crawlera/response/status/403': 3,\r\n 'crawlera/response/status/500': 15,\r\n 'crawlera/response/status/502': 15,\r\n 'crawlera/response/status/503': 433,\r\n 'crawlera/response/status/504': 18,\r\n```\r\n\r\nand Crawlera stats show only 15 errors with 503 and \"Proxy has been banned\" which matches this count  `'crawlera/response/error/banned'`\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/38", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/38/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/38/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/38/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/38", "id": 202452825, "node_id": "MDU6SXNzdWUyMDI0NTI4MjU=", "number": 38, "title": "Custom geo location", "user": {"login": "vimagick", "id": 6779585, "node_id": "MDQ6VXNlcjY3Nzk1ODU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6779585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vimagick", "html_url": "https://github.com/vimagick", "followers_url": "https://api.github.com/users/vimagick/followers", "following_url": "https://api.github.com/users/vimagick/following{/other_user}", "gists_url": "https://api.github.com/users/vimagick/gists{/gist_id}", "starred_url": "https://api.github.com/users/vimagick/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vimagick/subscriptions", "organizations_url": "https://api.github.com/users/vimagick/orgs", "repos_url": "https://api.github.com/users/vimagick/repos", "events_url": "https://api.github.com/users/vimagick/events{/privacy}", "received_events_url": "https://api.github.com/users/vimagick/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-01-23T06:06:29Z", "updated_at": "2017-01-23T14:34:37Z", "closed_at": "2017-01-23T14:34:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Some websites return different contents based on geo location.\r\nIs it possible to use country codes as parameter in headers?\r\nIt tells crawlera which proxies to choose from shared pool.\r\n\r\n```\r\nX-Crawlera-Country: US,UK,CA\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/32", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/32/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/32/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/32/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/32", "id": 194463192, "node_id": "MDU6SXNzdWUxOTQ0NjMxOTI=", "number": 32, "title": "Enable Crawlera on demand", "user": {"login": "pablohoffman", "id": 185212, "node_id": "MDQ6VXNlcjE4NTIxMg==", "avatar_url": "https://avatars1.githubusercontent.com/u/185212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pablohoffman", "html_url": "https://github.com/pablohoffman", "followers_url": "https://api.github.com/users/pablohoffman/followers", "following_url": "https://api.github.com/users/pablohoffman/following{/other_user}", "gists_url": "https://api.github.com/users/pablohoffman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pablohoffman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pablohoffman/subscriptions", "organizations_url": "https://api.github.com/users/pablohoffman/orgs", "repos_url": "https://api.github.com/users/pablohoffman/repos", "events_url": "https://api.github.com/users/pablohoffman/events{/privacy}", "received_events_url": "https://api.github.com/users/pablohoffman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2016-12-08T22:35:36Z", "updated_at": "2019-05-27T14:04:53Z", "closed_at": "2019-05-27T14:04:53Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "It would be great to enable Crawlera on demand once we detect bans from a website.\r\n\r\nWe could add some logic to the middleware to enable Crawlera once it detects the first 403 from the website, as opposed to be always on or off.\r\n\r\nThis would serve two purposes:\r\n\r\n1. save a lot of troubles (and costs) by not forcing the user to decide beforehand if a particular spider should run with crawlera enabled or not. It'd be a matter of setting `CRAWLERA_ENABLED = \"auto\"` and let it enable itself when needed\r\n2. make this middleware (and the scrapy+crawlera integration, in general) more useful and convenient\r\n\r\nWe could also support extending the rule for triggering the activation so as to not only be triggered by 403's.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/31", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/31/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/31/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/31/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/31", "id": 194281154, "node_id": "MDU6SXNzdWUxOTQyODExNTQ=", "number": 31, "title": "use crawlera error", "user": {"login": "ichenfujun", "id": 12792880, "node_id": "MDQ6VXNlcjEyNzkyODgw", "avatar_url": "https://avatars3.githubusercontent.com/u/12792880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ichenfujun", "html_url": "https://github.com/ichenfujun", "followers_url": "https://api.github.com/users/ichenfujun/followers", "following_url": "https://api.github.com/users/ichenfujun/following{/other_user}", "gists_url": "https://api.github.com/users/ichenfujun/gists{/gist_id}", "starred_url": "https://api.github.com/users/ichenfujun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ichenfujun/subscriptions", "organizations_url": "https://api.github.com/users/ichenfujun/orgs", "repos_url": "https://api.github.com/users/ichenfujun/repos", "events_url": "https://api.github.com/users/ichenfujun/events{/privacy}", "received_events_url": "https://api.github.com/users/ichenfujun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-12-08T09:05:13Z", "updated_at": "2017-01-18T10:33:21Z", "closed_at": "2017-01-18T10:33:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "http://proxy.crawlera.com:8010?noconnect (user:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/28", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/28/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/28/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/28/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/28", "id": 187475495, "node_id": "MDU6SXNzdWUxODc0NzU0OTU=", "number": 28, "title": "How to install crawlera-ca.crt in Scrapy", "user": {"login": "vionemc", "id": 6565672, "node_id": "MDQ6VXNlcjY1NjU2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6565672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vionemc", "html_url": "https://github.com/vionemc", "followers_url": "https://api.github.com/users/vionemc/followers", "following_url": "https://api.github.com/users/vionemc/following{/other_user}", "gists_url": "https://api.github.com/users/vionemc/gists{/gist_id}", "starred_url": "https://api.github.com/users/vionemc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vionemc/subscriptions", "organizations_url": "https://api.github.com/users/vionemc/orgs", "repos_url": "https://api.github.com/users/vionemc/repos", "events_url": "https://api.github.com/users/vionemc/events{/privacy}", "received_events_url": "https://api.github.com/users/vionemc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-11-05T04:10:17Z", "updated_at": "2018-09-04T09:11:45Z", "closed_at": "2017-01-18T10:54:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "I can't access https URLs using Scrapy+Crawlera. Crawlera provides the certificate, but how to install it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/26", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/26/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/26/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/26/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/26", "id": 186028010, "node_id": "MDU6SXNzdWUxODYwMjgwMTA=", "number": 26, "title": "Documentation is incomplete: dont_proxy vs X-Crawlera-Max-Retries", "user": {"login": "dchaplinsky", "id": 131186, "node_id": "MDQ6VXNlcjEzMTE4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/131186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dchaplinsky", "html_url": "https://github.com/dchaplinsky", "followers_url": "https://api.github.com/users/dchaplinsky/followers", "following_url": "https://api.github.com/users/dchaplinsky/following{/other_user}", "gists_url": "https://api.github.com/users/dchaplinsky/gists{/gist_id}", "starred_url": "https://api.github.com/users/dchaplinsky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dchaplinsky/subscriptions", "organizations_url": "https://api.github.com/users/dchaplinsky/orgs", "repos_url": "https://api.github.com/users/dchaplinsky/repos", "events_url": "https://api.github.com/users/dchaplinsky/events{/privacy}", "received_events_url": "https://api.github.com/users/dchaplinsky/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-10-28T21:37:09Z", "updated_at": "2016-11-16T17:53:05Z", "closed_at": "2016-11-16T17:53:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Documentation says that X-Crawlera-Max-Retries can control if proxying is enabled or not.\n\nOn the other hand, in the code I can see dont_proxy request meta flag which can be used to disable proxy.\n\nI think X-Crawlera-Max-Retries only affects the proxy server itself, as it's not mentioned in the code. Is setting it to 0 will bypass the proxy?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/18", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/18/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/18/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/18/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/18", "id": 154973758, "node_id": "MDU6SXNzdWUxNTQ5NzM3NTg=", "number": 18, "title": "can't crawler to  visit google", "user": {"login": "xusimin1008", "id": 14155320, "node_id": "MDQ6VXNlcjE0MTU1MzIw", "avatar_url": "https://avatars2.githubusercontent.com/u/14155320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xusimin1008", "html_url": "https://github.com/xusimin1008", "followers_url": "https://api.github.com/users/xusimin1008/followers", "following_url": "https://api.github.com/users/xusimin1008/following{/other_user}", "gists_url": "https://api.github.com/users/xusimin1008/gists{/gist_id}", "starred_url": "https://api.github.com/users/xusimin1008/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xusimin1008/subscriptions", "organizations_url": "https://api.github.com/users/xusimin1008/orgs", "repos_url": "https://api.github.com/users/xusimin1008/repos", "events_url": "https://api.github.com/users/xusimin1008/events{/privacy}", "received_events_url": "https://api.github.com/users/xusimin1008/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-05-16T07:11:20Z", "updated_at": "2016-05-16T11:46:23Z", "closed_at": "2016-05-16T11:46:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "when I use requests with crawler to visit google, such as \n[google trends](http://www.google.com/trends/fetchComponent?q=python&cid=TIMESERIES_GRAPH_0&export=3&hl=en-US), it happens\n\n```\nFile \"/home/vagrant/python/datartery-v3/lib/python2.7/site-packages/celery/app/task.py\", line 420, in __call__\n    return self.run(*args, **kwargs)\n  File \"/home/vagrant/Works/datartery-amazon-v3/dispatcher/dispatcher/__init__.py\", line 163, in fetch\n    verify=config.CRAWLERA_CA)\n  File \"/home/vagrant/python/datartery-v3/lib/python2.7/site-packages/requests/api.py\", line 67, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/home/vagrant/python/datartery-v3/lib/python2.7/site-packages/requests/api.py\", line 53, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/home/vagrant/python/datartery-v3/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/vagrant/python/datartery-v3/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/vagrant/python/datartery-v3/lib/python2.7/site-packages/requests/adapters.py\", line 426, in send\n    raise ConnectionError(err, request=request)\nConnectionError: ('Connection aborted.', BadStatusLine(\"''\",))\n```\n\nIs crawler forbidden by google?\nor the ips of crawler is not scattered, and a part of them are forbidden by google?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/17", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/17/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/17/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/17/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/17", "id": 138398177, "node_id": "MDU6SXNzdWUxMzgzOTgxNzc=", "number": 17, "title": "how to transfer the CRAWLERA headers to the crawlspider?", "user": {"login": "aohan237", "id": 3992281, "node_id": "MDQ6VXNlcjM5OTIyODE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3992281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aohan237", "html_url": "https://github.com/aohan237", "followers_url": "https://api.github.com/users/aohan237/followers", "following_url": "https://api.github.com/users/aohan237/following{/other_user}", "gists_url": "https://api.github.com/users/aohan237/gists{/gist_id}", "starred_url": "https://api.github.com/users/aohan237/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aohan237/subscriptions", "organizations_url": "https://api.github.com/users/aohan237/orgs", "repos_url": "https://api.github.com/users/aohan237/repos", "events_url": "https://api.github.com/users/aohan237/events{/privacy}", "received_events_url": "https://api.github.com/users/aohan237/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-03-04T07:09:35Z", "updated_at": "2016-03-12T02:15:33Z", "closed_at": "2016-03-12T02:15:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "i buy your CRAWLERA PLANS,but the offcial doc said dont to override the parse function,but how to transfer the CRAWLERA headers to the crawlspider\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/15", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/15/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/15/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/15/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/15", "id": 125352527, "node_id": "MDU6SXNzdWUxMjUzNTI1Mjc=", "number": 15, "title": "Distinguishing `CRAWLERA_DOWNLOAD_TIMEOUT` and `X-Crawlera-Timeout`", "user": {"login": "starrify", "id": 388828, "node_id": "MDQ6VXNlcjM4ODgyOA==", "avatar_url": "https://avatars0.githubusercontent.com/u/388828?v=4", "gravatar_id": "", "url": "https://api.github.com/users/starrify", "html_url": "https://github.com/starrify", "followers_url": "https://api.github.com/users/starrify/followers", "following_url": "https://api.github.com/users/starrify/following{/other_user}", "gists_url": "https://api.github.com/users/starrify/gists{/gist_id}", "starred_url": "https://api.github.com/users/starrify/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/starrify/subscriptions", "organizations_url": "https://api.github.com/users/starrify/orgs", "repos_url": "https://api.github.com/users/starrify/repos", "events_url": "https://api.github.com/users/starrify/events{/privacy}", "received_events_url": "https://api.github.com/users/starrify/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-01-07T08:58:58Z", "updated_at": "2018-04-20T13:28:53Z", "closed_at": "2018-04-20T13:28:53Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently we have:\n1. `CRAWLERA_DOWNLOAD_TIMEOUT` to specify the download timeout between a client and Crawlera nodes.\n2. [`X-Crawlera-Timeout`](http://doc.scrapinghub.com/crawlera.html#x-crawlera-timeout) to specify the download timeout between Crawlera nodes and the target websites (not yet supported by this package).\n\nLooks like they are somehow misleading and confusing. As we may not easily change the names, shall we:\n1. Add some kind of notes to explain the difference.\n2. Also support `X-Crawlera-Timeout` in this package.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/7", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/7/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/7/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/7/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/7", "id": 98838752, "node_id": "MDU6SXNzdWU5ODgzODc1Mg==", "number": 7, "title": "Crawlera credentials in README", "user": {"login": "aperezalbela", "id": 753821, "node_id": "MDQ6VXNlcjc1MzgyMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/753821?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aperezalbela", "html_url": "https://github.com/aperezalbela", "followers_url": "https://api.github.com/users/aperezalbela/followers", "following_url": "https://api.github.com/users/aperezalbela/following{/other_user}", "gists_url": "https://api.github.com/users/aperezalbela/gists{/gist_id}", "starred_url": "https://api.github.com/users/aperezalbela/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aperezalbela/subscriptions", "organizations_url": "https://api.github.com/users/aperezalbela/orgs", "repos_url": "https://api.github.com/users/aperezalbela/repos", "events_url": "https://api.github.com/users/aperezalbela/events{/privacy}", "received_events_url": "https://api.github.com/users/aperezalbela/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "eLRuLL", "id": 1459486, "node_id": "MDQ6VXNlcjE0NTk0ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1459486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eLRuLL", "html_url": "https://github.com/eLRuLL", "followers_url": "https://api.github.com/users/eLRuLL/followers", "following_url": "https://api.github.com/users/eLRuLL/following{/other_user}", "gists_url": "https://api.github.com/users/eLRuLL/gists{/gist_id}", "starred_url": "https://api.github.com/users/eLRuLL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eLRuLL/subscriptions", "organizations_url": "https://api.github.com/users/eLRuLL/orgs", "repos_url": "https://api.github.com/users/eLRuLL/repos", "events_url": "https://api.github.com/users/eLRuLL/events{/privacy}", "received_events_url": "https://api.github.com/users/eLRuLL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "eLRuLL", "id": 1459486, "node_id": "MDQ6VXNlcjE0NTk0ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1459486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eLRuLL", "html_url": "https://github.com/eLRuLL", "followers_url": "https://api.github.com/users/eLRuLL/followers", "following_url": "https://api.github.com/users/eLRuLL/following{/other_user}", "gists_url": "https://api.github.com/users/eLRuLL/gists{/gist_id}", "starred_url": "https://api.github.com/users/eLRuLL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eLRuLL/subscriptions", "organizations_url": "https://api.github.com/users/eLRuLL/orgs", "repos_url": "https://api.github.com/users/eLRuLL/repos", "events_url": "https://api.github.com/users/eLRuLL/events{/privacy}", "received_events_url": "https://api.github.com/users/eLRuLL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2015-08-03T21:07:12Z", "updated_at": "2015-10-19T13:09:15Z", "closed_at": "2015-10-19T13:09:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "scrapy-crawlera README tells users to set Crawlera user and password. Haven't we changed Crawlera authentication to be done with an API Key?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/5", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/5/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/5/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/5/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/5", "id": 98800064, "node_id": "MDU6SXNzdWU5ODgwMDA2NA==", "number": 5, "title": "python setup.py bdist_egg", "user": {"login": "nramirezuy", "id": 1042865, "node_id": "MDQ6VXNlcjEwNDI4NjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1042865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nramirezuy", "html_url": "https://github.com/nramirezuy", "followers_url": "https://api.github.com/users/nramirezuy/followers", "following_url": "https://api.github.com/users/nramirezuy/following{/other_user}", "gists_url": "https://api.github.com/users/nramirezuy/gists{/gist_id}", "starred_url": "https://api.github.com/users/nramirezuy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nramirezuy/subscriptions", "organizations_url": "https://api.github.com/users/nramirezuy/orgs", "repos_url": "https://api.github.com/users/nramirezuy/repos", "events_url": "https://api.github.com/users/nramirezuy/events{/privacy}", "received_events_url": "https://api.github.com/users/nramirezuy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "eLRuLL", "id": 1459486, "node_id": "MDQ6VXNlcjE0NTk0ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1459486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eLRuLL", "html_url": "https://github.com/eLRuLL", "followers_url": "https://api.github.com/users/eLRuLL/followers", "following_url": "https://api.github.com/users/eLRuLL/following{/other_user}", "gists_url": "https://api.github.com/users/eLRuLL/gists{/gist_id}", "starred_url": "https://api.github.com/users/eLRuLL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eLRuLL/subscriptions", "organizations_url": "https://api.github.com/users/eLRuLL/orgs", "repos_url": "https://api.github.com/users/eLRuLL/repos", "events_url": "https://api.github.com/users/eLRuLL/events{/privacy}", "received_events_url": "https://api.github.com/users/eLRuLL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "eLRuLL", "id": 1459486, "node_id": "MDQ6VXNlcjE0NTk0ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1459486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eLRuLL", "html_url": "https://github.com/eLRuLL", "followers_url": "https://api.github.com/users/eLRuLL/followers", "following_url": "https://api.github.com/users/eLRuLL/following{/other_user}", "gists_url": "https://api.github.com/users/eLRuLL/gists{/gist_id}", "starred_url": "https://api.github.com/users/eLRuLL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eLRuLL/subscriptions", "organizations_url": "https://api.github.com/users/eLRuLL/orgs", "repos_url": "https://api.github.com/users/eLRuLL/repos", "events_url": "https://api.github.com/users/eLRuLL/events{/privacy}", "received_events_url": "https://api.github.com/users/eLRuLL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2015-08-03T17:37:04Z", "updated_at": "2015-08-03T21:50:25Z", "closed_at": "2015-08-03T21:50:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "`python setup.py bdist_egg` generates an empty egg.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/3", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/3/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/3/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-crawlera/issues/3/events", "html_url": "https://github.com/scrapy-plugins/scrapy-crawlera/issues/3", "id": 98082166, "node_id": "MDU6SXNzdWU5ODA4MjE2Ng==", "number": 3, "title": "Documentation Import error on package name", "user": {"login": "cr8ivecodesmith", "id": 1231112, "node_id": "MDQ6VXNlcjEyMzExMTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1231112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cr8ivecodesmith", "html_url": "https://github.com/cr8ivecodesmith", "followers_url": "https://api.github.com/users/cr8ivecodesmith/followers", "following_url": "https://api.github.com/users/cr8ivecodesmith/following{/other_user}", "gists_url": "https://api.github.com/users/cr8ivecodesmith/gists{/gist_id}", "starred_url": "https://api.github.com/users/cr8ivecodesmith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cr8ivecodesmith/subscriptions", "organizations_url": "https://api.github.com/users/cr8ivecodesmith/orgs", "repos_url": "https://api.github.com/users/cr8ivecodesmith/repos", "events_url": "https://api.github.com/users/cr8ivecodesmith/events{/privacy}", "received_events_url": "https://api.github.com/users/cr8ivecodesmith/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2015-07-30T04:16:17Z", "updated_at": "2015-08-03T14:57:01Z", "closed_at": "2015-08-03T14:57:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Installed from `pypi`. Readme here says to use `crawlera.CrawleraMiddleware` which produces an import error.\n\n[Documentation](http://doc.scrapinghub.com/crawlera.html#using-crawlera-with-scrapy) in ScrapingHub says to use `scrapy_crawlera.CrawleraMiddleware` works.\n\nPerhaps the package in `pypi` is in `scrapy_crawlera`.\n", "performed_via_github_app": null, "score": 1.0}]}