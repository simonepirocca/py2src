{"total_count": 20, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/30", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/30/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/30/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/30/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/30", "id": 678635210, "node_id": "MDU6SXNzdWU2Nzg2MzUyMTA=", "number": 30, "title": "FakerProvider not working ? ", "user": {"login": "SecT0uch", "id": 26085417, "node_id": "MDQ6VXNlcjI2MDg1NDE3", "avatar_url": "https://avatars2.githubusercontent.com/u/26085417?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SecT0uch", "html_url": "https://github.com/SecT0uch", "followers_url": "https://api.github.com/users/SecT0uch/followers", "following_url": "https://api.github.com/users/SecT0uch/following{/other_user}", "gists_url": "https://api.github.com/users/SecT0uch/gists{/gist_id}", "starred_url": "https://api.github.com/users/SecT0uch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SecT0uch/subscriptions", "organizations_url": "https://api.github.com/users/SecT0uch/orgs", "repos_url": "https://api.github.com/users/SecT0uch/repos", "events_url": "https://api.github.com/users/SecT0uch/events{/privacy}", "received_events_url": "https://api.github.com/users/SecT0uch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 156532457, "node_id": "MDU6TGFiZWwxNTY1MzI0NTc=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-08-13T18:03:19Z", "updated_at": "2020-08-15T23:03:20Z", "closed_at": "2020-08-15T16:29:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Thanks a lot for your work! :+1: \r\n\r\nHere is my `settings.py`:\r\n```python\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\r\n    'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\r\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\r\n    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\r\n}\r\nFAKEUSERAGENT_PROVIDERS = [\r\n    # 'scrapy_fake_useragent.providers.FakeUserAgentProvider',  # Depends on http://useragentstring.com which is currently down\r\n    'scrapy_fake_useragent.providers.FakerProvider',  # if FakeUserAgentProvider fails, we'll use faker to generate a user-agent string for us\r\n    'scrapy_fake_useragent.providers.FixedUserAgentProvider',  # fall back to USER_AGENT value\r\n]\r\nUSER_AGENT = \"TEST\"\r\n```\r\nWhen running `scrapy shell` in my project, my fallback USER_AGENT is used:\r\n```\r\nIn [1]: settings.get('USER_AGENT')\r\nOut[1]: 'TEST'\r\n```\r\n\r\nAlso tried to define `FAKER_RANDOM_UA_TYPE = 'user_agent'`\r\n\r\nUsing :\r\n* python 3.8.5\r\n* Scrapy 2.3.0\r\n* faker 4.1.1\r\n* scrapy-fake-useragent 1.4.1\r\n\r\n\r\nSide note: the requirements on the PyPi archive are missing faker", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/29", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/29/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/29/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/29/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/29", "id": 666619958, "node_id": "MDU6SXNzdWU2NjY2MTk5NTg=", "number": 29, "title": "Middleware completely preventing scraper from starting/crawling", "user": {"login": "caffeinatedMike", "id": 22151742, "node_id": "MDQ6VXNlcjIyMTUxNzQy", "avatar_url": "https://avatars1.githubusercontent.com/u/22151742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caffeinatedMike", "html_url": "https://github.com/caffeinatedMike", "followers_url": "https://api.github.com/users/caffeinatedMike/followers", "following_url": "https://api.github.com/users/caffeinatedMike/following{/other_user}", "gists_url": "https://api.github.com/users/caffeinatedMike/gists{/gist_id}", "starred_url": "https://api.github.com/users/caffeinatedMike/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caffeinatedMike/subscriptions", "organizations_url": "https://api.github.com/users/caffeinatedMike/orgs", "repos_url": "https://api.github.com/users/caffeinatedMike/repos", "events_url": "https://api.github.com/users/caffeinatedMike/events{/privacy}", "received_events_url": "https://api.github.com/users/caffeinatedMike/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-27T22:19:48Z", "updated_at": "2020-07-31T14:31:40Z", "closed_at": "2020-07-31T14:31:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using the following settings in order to only use the `faker` provider since it'll always generate a user agent string. However, my spider will not crawl at all now when these settings are applied. I keep seeing 180 second timeouts \r\n\r\n# Code\r\n```python3\r\nclass RedactedSpider(scrapy.Spider):\r\n    name = 'redacted'\r\n    custom_settings = {\r\n        # 'DOWNLOAD_DELAY': 0.075,\r\n        'DOWNLOADER_MIDDLEWARES': {\r\n            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\r\n            'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\r\n            'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\r\n            'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\r\n        },\r\n        'FAKEUSERAGENT_PROVIDERS': [\r\n            'scrapy_fake_useragent.providers.FakerProvider',\r\n            'scrapy_fake_useragent.providers.FixedUserAgentProvider'\r\n        ],\r\n        'USER_AGENT': (\r\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\r\n            'AppleWebKit/537.36 (KHTML, like Gecko) '\r\n            'Chrome/78.0.3904.108 Safari/537.36'\r\n        ),\r\n        'FAKER_RANDOM_UA_TYPE': 'chrome',\r\n        ........\r\n```\r\n\r\n# Logs\r\n```\r\n2020-07-27 18:13:46 [scrapy.utils.log] INFO: Scrapy 2.2.0 started (bot: [redacted])\r\n2020-07-27 18:13:46 [scrapy.utils.log] INFO: Versions: lxml 4.5.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Windows-10-10.0.19041-SP0\r\n2020-07-27 18:13:46 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\r\n2020-07-27 18:13:46 [scrapy.crawler] INFO: Overridden settings:\r\n{'BOT_NAME': '[redacted]',\r\n 'FEED_EXPORT_FIELDS': [TRUNCATED],\r\n 'LOG_FILE': 'runtime.log',\r\n 'NEWSPIDER_MODULE': '[redacted].spiders',\r\n 'ROBOTSTXT_OBEY': True,\r\n 'SPIDER_MODULES': ['[redacted].spiders']}\r\n2020-07-27 18:13:46 [scrapy.extensions.telnet] INFO: Telnet Password: [redacted]\r\n2020-07-27 18:13:46 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2020-07-27 18:13:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware',\r\n 'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2020-07-27 18:13:46 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2020-07-27 18:13:46 [scrapy.middleware] INFO: Enabled item pipelines:\r\n['[redacted].pipelines.LocalFilesPipeline',\r\n '[redacted].pipelines.NutrientMappingPipeline',\r\n '[redacted].pipelines.PartitionedCsvPipeline']\r\n2020-07-27 18:13:46 [scrapy.core.engine] INFO: Spider opened\r\n2020-07-27 18:13:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-07-27 18:13:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n2020-07-27 18:14:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-07-27 18:15:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-07-27 18:16:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2020-07-27 18:16:47 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.[redacted].com/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://www.[redacted].com/robots.txt took longer than 180.0 seconds..\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/27", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/27/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/27/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/27/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/27", "id": 659561269, "node_id": "MDU6SXNzdWU2NTk1NjEyNjk=", "number": 27, "title": "[CRITICAL] useragentstring.com not working anymore", "user": {"login": "0xfede7c8", "id": 11838654, "node_id": "MDQ6VXNlcjExODM4NjU0", "avatar_url": "https://avatars2.githubusercontent.com/u/11838654?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0xfede7c8", "html_url": "https://github.com/0xfede7c8", "followers_url": "https://api.github.com/users/0xfede7c8/followers", "following_url": "https://api.github.com/users/0xfede7c8/following{/other_user}", "gists_url": "https://api.github.com/users/0xfede7c8/gists{/gist_id}", "starred_url": "https://api.github.com/users/0xfede7c8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0xfede7c8/subscriptions", "organizations_url": "https://api.github.com/users/0xfede7c8/orgs", "repos_url": "https://api.github.com/users/0xfede7c8/repos", "events_url": "https://api.github.com/users/0xfede7c8/events{/privacy}", "received_events_url": "https://api.github.com/users/0xfede7c8/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2020-07-17T19:29:15Z", "updated_at": "2020-07-28T11:55:21Z", "closed_at": "2020-07-28T11:55:20Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```bash\r\n2020-07-17 16:21:31 [fake_useragent] DEBUG: Error occurred during fetching http://useragentstring.com/pages/useragentstring.php?name=Chrome\r\n```\r\n\r\nIt is failing to fetch it because the site seems to be down or not working properly. Consider removing it from the list or replacing it with another list.\r\n\r\nThis problem renders the tools useless.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/26", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/26/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/26/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/26/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/26", "id": 644267495, "node_id": "MDU6SXNzdWU2NDQyNjc0OTU=", "number": 26, "title": "User Agent not updating", "user": {"login": "yilu1015", "id": 25930948, "node_id": "MDQ6VXNlcjI1OTMwOTQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/25930948?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yilu1015", "html_url": "https://github.com/yilu1015", "followers_url": "https://api.github.com/users/yilu1015/followers", "following_url": "https://api.github.com/users/yilu1015/following{/other_user}", "gists_url": "https://api.github.com/users/yilu1015/gists{/gist_id}", "starred_url": "https://api.github.com/users/yilu1015/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yilu1015/subscriptions", "organizations_url": "https://api.github.com/users/yilu1015/orgs", "repos_url": "https://api.github.com/users/yilu1015/repos", "events_url": "https://api.github.com/users/yilu1015/events{/privacy}", "received_events_url": "https://api.github.com/users/yilu1015/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-24T02:23:49Z", "updated_at": "2020-07-28T12:16:23Z", "closed_at": "2020-07-28T12:16:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I had a successful pip installation. I put the correct configuration code in the `SETTINGS.py` for the global project. I thought it was as easy as that. But when I asked my `scrapy shell`, it told me that I am still, yours sincerely, `'Scrapy/2.1.0 (+https://scrapy.org)'}`. \r\n\r\nIs there anything I am missing here? Relevant sections of my debugging code are shown as below.\r\n\r\n```\r\n'DOWNLOADER_MIDDLEWARES': {'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,\r\n                            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\r\n                            'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\r\n                            'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401},\r\n 'DOWNLOADER_MIDDLEWARES_BASE': {'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\r\n                                 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\r\n                                 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,\r\n                                 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\r\n                                 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\r\n                                 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\r\n                                 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\r\n                                 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\r\n                                 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\r\n                                 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\r\n                                 'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,\r\n                                 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\r\n                                 'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\r\n                                 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500},\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/22", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/22/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/22/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/22/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/22", "id": 372234571, "node_id": "MDU6SXNzdWUzNzIyMzQ1NzE=", "number": 22, "title": "Fallback causing it always use Fallback user agent even though there is no error", "user": {"login": "vionemc", "id": 6565672, "node_id": "MDQ6VXNlcjY1NjU2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6565672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vionemc", "html_url": "https://github.com/vionemc", "followers_url": "https://api.github.com/users/vionemc/followers", "following_url": "https://api.github.com/users/vionemc/following{/other_user}", "gists_url": "https://api.github.com/users/vionemc/gists{/gist_id}", "starred_url": "https://api.github.com/users/vionemc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vionemc/subscriptions", "organizations_url": "https://api.github.com/users/vionemc/orgs", "repos_url": "https://api.github.com/users/vionemc/repos", "events_url": "https://api.github.com/users/vionemc/events{/privacy}", "received_events_url": "https://api.github.com/users/vionemc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-20T17:32:51Z", "updated_at": "2018-10-20T17:41:11Z", "closed_at": "2018-10-20T17:40:56Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I tried to check what user agents are used by the requests since it seems it still gets blocked, even after using Crawlera. It turned out, it always uses Fallback even though there is no problem in downloading the user agents database from the internet. It's really not the intention of the Fallback. We'd need to address it seriously.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/21", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/21/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/21/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/21/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/21", "id": 366947650, "node_id": "MDU6SXNzdWUzNjY5NDc2NTA=", "number": 21, "title": "User agent pool website is down", "user": {"login": "Hacaw", "id": 6963500, "node_id": "MDQ6VXNlcjY5NjM1MDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/6963500?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hacaw", "html_url": "https://github.com/Hacaw", "followers_url": "https://api.github.com/users/Hacaw/followers", "following_url": "https://api.github.com/users/Hacaw/following{/other_user}", "gists_url": "https://api.github.com/users/Hacaw/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hacaw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hacaw/subscriptions", "organizations_url": "https://api.github.com/users/Hacaw/orgs", "repos_url": "https://api.github.com/users/Hacaw/repos", "events_url": "https://api.github.com/users/Hacaw/events{/privacy}", "received_events_url": "https://api.github.com/users/Hacaw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-04T20:00:55Z", "updated_at": "2018-10-04T20:58:48Z", "closed_at": "2018-10-04T20:58:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nThis is my first time I'm opening an issue to someone else's repository. I see this was not asked / proposed.\r\nI am using your library for my student project. \r\n\r\nBut it looks like the user agents used for rotation from this website [http://useragentstring.com/](url) cannot be generated because the website is not responding.\r\n\r\n\r\n\r\nCan you add some logic to use other websites, or an internal cache in case those websites are down ? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/20", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/20/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/20/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/20/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/20", "id": 266615233, "node_id": "MDU6SXNzdWUyNjY2MTUyMzM=", "number": 20, "title": "Usage of setdefault and retries", "user": {"login": "fpghost", "id": 2691631, "node_id": "MDQ6VXNlcjI2OTE2MzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/2691631?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fpghost", "html_url": "https://github.com/fpghost", "followers_url": "https://api.github.com/users/fpghost/followers", "following_url": "https://api.github.com/users/fpghost/following{/other_user}", "gists_url": "https://api.github.com/users/fpghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/fpghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fpghost/subscriptions", "organizations_url": "https://api.github.com/users/fpghost/orgs", "repos_url": "https://api.github.com/users/fpghost/repos", "events_url": "https://api.github.com/users/fpghost/events{/privacy}", "received_events_url": "https://api.github.com/users/fpghost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 156532459, "node_id": "MDU6TGFiZWwxNTY1MzI0NTk=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-10-18T19:32:33Z", "updated_at": "2019-12-25T21:25:38Z", "closed_at": "2019-12-25T21:25:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Scrapy headers class inherits from `scrapy.utils.datatypes.CaselessDict`. The `setdefault` method is defined as\r\n\r\n    def setdefault(self, key, def_val=None):\r\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))\r\n\r\nSo just like a regular Python dictionary the `setdefault` method sets the value to return when a given key is not found, for example:\r\n\r\n    d = {'User-Agent': 'some fake ua'}\r\n    d.setdefault('User-Agent',  'default ua')\r\n    d['User-Agent']\r\n    Out[10]: 'some fake ua'    \r\n\r\nNote how this wasn't \"default ua\" because there already a 'User-Agent' set.\r\nCompare this to:\r\n   \r\n    d = {}\r\n    d.setdefault('User-Agent', 'default ua')\r\n    d['User-Agent']\r\n    Out[13]: 'default ua'\r\n\r\nThere was nothing set, so in this case the default value was of course returned.\r\n\r\nThis is fine for fresh requests which never had a user-agent set in the headers, but if using the requests retry middleware too, then it means that the user-agent will be the same for every single retry, which might not be desired behaviour (especially if you want a different proxy/ua per request and one combination is failing so you want to retry with a fresh combo).\r\n\r\nIf you just did \r\n\r\n    request.headers['User-Agent'] = ....\r\n\r\nit would solve this problem.\r\n\r\nOr if you don't think the desired behaviour should always be to change the User-Agent upon retries, then there could at least be a setting to allow the user to switch this on/off.\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/19", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/19/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/19/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/19/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/19", "id": 253876812, "node_id": "MDU6SXNzdWUyNTM4NzY4MTI=", "number": 19, "title": "useragentstring.com down, how to change other source ?", "user": {"login": "ttpro1995", "id": 7196876, "node_id": "MDQ6VXNlcjcxOTY4NzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7196876?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ttpro1995", "html_url": "https://github.com/ttpro1995", "followers_url": "https://api.github.com/users/ttpro1995/followers", "following_url": "https://api.github.com/users/ttpro1995/following{/other_user}", "gists_url": "https://api.github.com/users/ttpro1995/gists{/gist_id}", "starred_url": "https://api.github.com/users/ttpro1995/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ttpro1995/subscriptions", "organizations_url": "https://api.github.com/users/ttpro1995/orgs", "repos_url": "https://api.github.com/users/ttpro1995/repos", "events_url": "https://api.github.com/users/ttpro1995/events{/privacy}", "received_events_url": "https://api.github.com/users/ttpro1995/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-30T04:44:09Z", "updated_at": "2017-08-30T15:27:29Z", "closed_at": "2017-08-30T15:27:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "`Error occurred during fetching http://useragentstring.com/pages/useragentstring.php?name=Chrome\r\n\r\nI got that error. \r\nI am new to scrapy and have no idea how to fix. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/15", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/15/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/15/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/15/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/15", "id": 235025577, "node_id": "MDU6SXNzdWUyMzUwMjU1Nzc=", "number": 15, "title": "How to add to project?", "user": {"login": "brunomperes", "id": 1819575, "node_id": "MDQ6VXNlcjE4MTk1NzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1819575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brunomperes", "html_url": "https://github.com/brunomperes", "followers_url": "https://api.github.com/users/brunomperes/followers", "following_url": "https://api.github.com/users/brunomperes/following{/other_user}", "gists_url": "https://api.github.com/users/brunomperes/gists{/gist_id}", "starred_url": "https://api.github.com/users/brunomperes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brunomperes/subscriptions", "organizations_url": "https://api.github.com/users/brunomperes/orgs", "repos_url": "https://api.github.com/users/brunomperes/repos", "events_url": "https://api.github.com/users/brunomperes/events{/privacy}", "received_events_url": "https://api.github.com/users/brunomperes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-10T20:38:40Z", "updated_at": "2017-06-10T21:02:26Z", "closed_at": "2017-06-10T20:58:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm a bit confused, I've never used setuptools, how do I add this to a scrapy project?\r\n\r\nI've found this post (http://www.bertcarremans.be/avoiding-ip-banning-with-the-scrapy-framework/) but it doesn't seem the most up to date way of doing so.\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/14", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/14/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/14/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/14/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/14", "id": 217016587, "node_id": "MDU6SXNzdWUyMTcwMTY1ODc=", "number": 14, "title": "Enabling FALLBACK setting on `settings.py`", "user": {"login": "vionemc", "id": 6565672, "node_id": "MDQ6VXNlcjY1NjU2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6565672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vionemc", "html_url": "https://github.com/vionemc", "followers_url": "https://api.github.com/users/vionemc/followers", "following_url": "https://api.github.com/users/vionemc/following{/other_user}", "gists_url": "https://api.github.com/users/vionemc/gists{/gist_id}", "starred_url": "https://api.github.com/users/vionemc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vionemc/subscriptions", "organizations_url": "https://api.github.com/users/vionemc/orgs", "repos_url": "https://api.github.com/users/vionemc/repos", "events_url": "https://api.github.com/users/vionemc/events{/privacy}", "received_events_url": "https://api.github.com/users/vionemc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 156532459, "node_id": "MDU6TGFiZWwxNTY1MzI0NTk=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 156532460, "node_id": "MDU6TGFiZWwxNTY1MzI0NjA=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/help%20wanted", "name": "help wanted", "color": "159818", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-26T00:39:52Z", "updated_at": "2017-06-16T16:32:47Z", "closed_at": "2017-06-16T16:32:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://pypi.python.org/pypi/fake-useragent\r\n\r\nThere said\r\n\r\n> You can completely disable ANY annoying exception with adding fallback: (version 0.1.4 added)\r\n> ```\r\n> import fake_useragent\r\n> \r\n> ua = fake_useragent.UserAgent(fallback='Your favorite Browser')\r\n> # in case if something went wrong, one more time it is REALLY!!! rare case\r\n> ua.random == 'Your favorite Browser'\r\n> ```\r\n\r\nI want to use that fallback feature easily in Scrapy FakeUserAgent just by setting `FAKEUSERAGENT_FALLBACK` in `settings.py`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/13", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/13/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/13/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/13/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/13", "id": 206870516, "node_id": "MDU6SXNzdWUyMDY4NzA1MTY=", "number": 13, "title": "SSL Certificate expired", "user": {"login": "Slater-Victoroff", "id": 1417499, "node_id": "MDQ6VXNlcjE0MTc0OTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1417499?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Slater-Victoroff", "html_url": "https://github.com/Slater-Victoroff", "followers_url": "https://api.github.com/users/Slater-Victoroff/followers", "following_url": "https://api.github.com/users/Slater-Victoroff/following{/other_user}", "gists_url": "https://api.github.com/users/Slater-Victoroff/gists{/gist_id}", "starred_url": "https://api.github.com/users/Slater-Victoroff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Slater-Victoroff/subscriptions", "organizations_url": "https://api.github.com/users/Slater-Victoroff/orgs", "repos_url": "https://api.github.com/users/Slater-Victoroff/repos", "events_url": "https://api.github.com/users/Slater-Victoroff/events{/privacy}", "received_events_url": "https://api.github.com/users/Slater-Victoroff/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-02-10T18:26:29Z", "updated_at": "2017-02-17T04:04:38Z", "closed_at": "2017-02-17T04:04:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "This library now errors whenever it's being used because the SSL certificate on the herokuapp is expired.\r\n\r\nTraceback below: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/fake_useragent/utils.py\", line 45, in get\r\n    return urlopen(request, timeout=settings.HTTP_TIMEOUT).read()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\", line 223, in urlopen\r\n    return opener.open(url, data, timeout)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\", line 526, in open\r\n    response = self._open(req, data)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\", line 544, in _open\r\n    '_open', req)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\", line 504, in _call_chain\r\n    result = func(*args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\", line 1361, in https_open\r\n    context=self._context, check_hostname=self._check_hostname)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\", line 1320, in do_open\r\n    raise URLError(err)\r\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)>\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/10", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/10/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/10/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/10/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/10", "id": 201350394, "node_id": "MDU6SXNzdWUyMDEzNTAzOTQ=", "number": 10, "title": "Random browser user agents or random user agents?", "user": {"login": "japborst", "id": 1426824, "node_id": "MDQ6VXNlcjE0MjY4MjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1426824?v=4", "gravatar_id": "", "url": "https://api.github.com/users/japborst", "html_url": "https://github.com/japborst", "followers_url": "https://api.github.com/users/japborst/followers", "following_url": "https://api.github.com/users/japborst/following{/other_user}", "gists_url": "https://api.github.com/users/japborst/gists{/gist_id}", "starred_url": "https://api.github.com/users/japborst/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/japborst/subscriptions", "organizations_url": "https://api.github.com/users/japborst/orgs", "repos_url": "https://api.github.com/users/japborst/repos", "events_url": "https://api.github.com/users/japborst/events{/privacy}", "received_events_url": "https://api.github.com/users/japborst/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-01-17T17:16:49Z", "updated_at": "2017-01-17T18:12:39Z", "closed_at": "2017-01-17T17:55:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI got a question. The user agents from useragentstring.com also includes crawlers and bots. Does that mean that those are included in the random generation of user agents or are only browsers included?\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/9", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/9/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/9/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/9/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/9", "id": 200063901, "node_id": "MDU6SXNzdWUyMDAwNjM5MDE=", "number": 9, "title": "scrapy-fake-useragent and cfscrape cloudfare anti bot library", "user": {"login": "reyman", "id": 466329, "node_id": "MDQ6VXNlcjQ2NjMyOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/466329?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reyman", "html_url": "https://github.com/reyman", "followers_url": "https://api.github.com/users/reyman/followers", "following_url": "https://api.github.com/users/reyman/following{/other_user}", "gists_url": "https://api.github.com/users/reyman/gists{/gist_id}", "starred_url": "https://api.github.com/users/reyman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reyman/subscriptions", "organizations_url": "https://api.github.com/users/reyman/orgs", "repos_url": "https://api.github.com/users/reyman/repos", "events_url": "https://api.github.com/users/reyman/events{/privacy}", "received_events_url": "https://api.github.com/users/reyman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-01-11T11:04:54Z", "updated_at": "2017-01-13T16:35:19Z", "closed_at": "2017-01-13T16:35:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nThis is more a question than an issue i suppose but perhaps you can help me.\r\nI'm trying to create a scraper using your extension with `cfscrape`, `privoxy`, and `scrapy_fake_useragent`\r\nI'm using `cfscrape` python extension to bypass cloudfare protection with scrapy. \r\n\r\nTo collect cookie needed by `cfscrape`, i need to redefine the `start_request` function into my spider class, like this : \r\n\r\n```\r\n    def start_requests(self):\r\n        cf_requests = []\r\n        for url in self.start_urls:\r\n            token, agent = cfscrape.get_tokens(url)\r\n            self.logger.info(\"agent = %s\", agent)\r\n            cf_requests.append(scrapy.Request(url=url,\r\n                                              cookies= token,\r\n                                              headers={'User-Agent': agent}))\r\n        return cf_requests\r\n```\r\n\r\nMy problem is that the `user_agent` collected by `start_requests` is not the same that the `user_agent` randomly selected by `scrapy_fake_useragent` , as you can see :\r\n\r\n\r\n```\r\n2017-01-11 11:52:55 [airports] INFO: agent = Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36\r\n2017-01-11 11:52:55 [scrapy.core.engine] INFO: Spider opened\r\n2017-01-11 11:52:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-01-11 11:52:55 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-01-11 11:52:55 [scrapy_fake_useragent.middleware] DEBUG: Assign User-Agent Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36 to Proxy None\r\n```\r\n\r\nI defined my extension in this order : \r\n\r\n```\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\r\n    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\r\n    'flight_project.middlewares.ProxyMiddleware': 100,\r\n    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware':110,\r\n    }\r\n```\r\n\r\nI need the same `user_agent`, so how can i pass the good user agent generated by `scrapy_fake_useragent` into the `start_requests` method ? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/8", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/8/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/8/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/8/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/8", "id": 191595662, "node_id": "MDU6SXNzdWUxOTE1OTU2NjI=", "number": 8, "title": "Merge scrapy-fake-useragent into fake-useragent", "user": {"login": "hellysmile", "id": 1834317, "node_id": "MDQ6VXNlcjE4MzQzMTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1834317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hellysmile", "html_url": "https://github.com/hellysmile", "followers_url": "https://api.github.com/users/hellysmile/followers", "following_url": "https://api.github.com/users/hellysmile/following{/other_user}", "gists_url": "https://api.github.com/users/hellysmile/gists{/gist_id}", "starred_url": "https://api.github.com/users/hellysmile/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hellysmile/subscriptions", "organizations_url": "https://api.github.com/users/hellysmile/orgs", "repos_url": "https://api.github.com/users/hellysmile/repos", "events_url": "https://api.github.com/users/hellysmile/events{/privacy}", "received_events_url": "https://api.github.com/users/hellysmile/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-11-24T22:00:38Z", "updated_at": "2020-07-22T23:49:56Z", "closed_at": "2020-07-22T23:49:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey @alecxe!\r\n\r\nI have proposal to merge our libs.\r\n\r\nWhat do You think about it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/7", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/7/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/7/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/7/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/7", "id": 191395155, "node_id": "MDU6SXNzdWUxOTEzOTUxNTU=", "number": 7, "title": "http://useragentstring.com/ is DOWN ", "user": {"login": "carvajalluis", "id": 674024, "node_id": "MDQ6VXNlcjY3NDAyNA==", "avatar_url": "https://avatars0.githubusercontent.com/u/674024?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carvajalluis", "html_url": "https://github.com/carvajalluis", "followers_url": "https://api.github.com/users/carvajalluis/followers", "following_url": "https://api.github.com/users/carvajalluis/following{/other_user}", "gists_url": "https://api.github.com/users/carvajalluis/gists{/gist_id}", "starred_url": "https://api.github.com/users/carvajalluis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carvajalluis/subscriptions", "organizations_url": "https://api.github.com/users/carvajalluis/orgs", "repos_url": "https://api.github.com/users/carvajalluis/repos", "events_url": "https://api.github.com/users/carvajalluis/events{/privacy}", "received_events_url": "https://api.github.com/users/carvajalluis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 156532459, "node_id": "MDU6TGFiZWwxNTY1MzI0NTk=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 156532460, "node_id": "MDU6TGFiZWwxNTY1MzI0NjA=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/help%20wanted", "name": "help wanted", "color": "159818", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2016-11-23T22:40:09Z", "updated_at": "2017-06-16T16:49:35Z", "closed_at": "2017-06-16T16:49:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "this implementation is completely dependant on the fact that this site es working properly. it should be at least a redundant data source in case site falls the middleware keeps working.\r\n\r\nIn my particular case I have a task running spiders automatically with scrapyd on a daily basis days like today it won't work", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/6", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/6/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/6/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/6/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/6", "id": 187691500, "node_id": "MDU6SXNzdWUxODc2OTE1MDA=", "number": 6, "title": "Restrict User-Agent to Desktop Devices", "user": {"login": "vaulstein", "id": 6698535, "node_id": "MDQ6VXNlcjY2OTg1MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/6698535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vaulstein", "html_url": "https://github.com/vaulstein", "followers_url": "https://api.github.com/users/vaulstein/followers", "following_url": "https://api.github.com/users/vaulstein/following{/other_user}", "gists_url": "https://api.github.com/users/vaulstein/gists{/gist_id}", "starred_url": "https://api.github.com/users/vaulstein/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vaulstein/subscriptions", "organizations_url": "https://api.github.com/users/vaulstein/orgs", "repos_url": "https://api.github.com/users/vaulstein/repos", "events_url": "https://api.github.com/users/vaulstein/events{/privacy}", "received_events_url": "https://api.github.com/users/vaulstein/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 156532459, "node_id": "MDU6TGFiZWwxNTY1MzI0NTk=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 156532460, "node_id": "MDU6TGFiZWwxNTY1MzI0NjA=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/help%20wanted", "name": "help wanted", "color": "159818", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2016-11-07T11:47:01Z", "updated_at": "2020-07-28T12:12:58Z", "closed_at": "2020-07-28T12:12:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was using your middleware for generating fake user-agents with every scrapy request.\r\n\r\nBut the problem is that the user-agents are not limited to Desktop devices only and for user-agents like below (Ipad user-agent), the xpath extraction fails\r\n\r\n    User-agent string: Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.116 Safari/537.36 Mozilla/5.0 (iPad; U; CPU OS 3_2 like Mac OS X; en-us) AppleWebKit/531.21.10 (KHTML, like Gecko) Version/4.0.4 Mobile/7B334b Safari/531.21.10\r\n\r\nThis is probably happening because the xpaths for the fields that I want to extract might change for a Ipad device.\r\nIs there a way to limit the user-agent to **only Desktop devices**?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/4", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/4/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/4/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/4/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/4", "id": 168539273, "node_id": "MDU6SXNzdWUxNjg1MzkyNzM=", "number": 4, "title": "the library doesn't work anymore", "user": {"login": "mayouf", "id": 5865600, "node_id": "MDQ6VXNlcjU4NjU2MDA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5865600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mayouf", "html_url": "https://github.com/mayouf", "followers_url": "https://api.github.com/users/mayouf/followers", "following_url": "https://api.github.com/users/mayouf/following{/other_user}", "gists_url": "https://api.github.com/users/mayouf/gists{/gist_id}", "starred_url": "https://api.github.com/users/mayouf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mayouf/subscriptions", "organizations_url": "https://api.github.com/users/mayouf/orgs", "repos_url": "https://api.github.com/users/mayouf/repos", "events_url": "https://api.github.com/users/mayouf/events{/privacy}", "received_events_url": "https://api.github.com/users/mayouf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-07-31T21:28:04Z", "updated_at": "2016-08-27T05:42:20Z", "closed_at": "2016-08-27T05:42:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "here is the error:\n### `2016-07-31 23:12:31 [twisted] CRITICAL:\n\nTraceback (most recent call last):\nFile \"/home/mic/anaconda2/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1128, in _inlineCallbacks result = g.send(result)\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py\", line 90, in crawl\n    six.reraise(*exc_info)\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py\", line 72, in crawl\n    self.engine = self._create_engine()\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/crawler.py\", line 97, in _create_engine\n    return ExecutionEngine(self, lambda _: self.stop())\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/core/engine.py\", line 68, in __init__\n    self.downloader = downloader_cls(crawler)\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/core/downloader/**init**.py\", line 88, in **init**\n    self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py\", line 58, in from_crawler\n    return cls.from_settings(crawler.settings, crawler)\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/middleware.py\", line 40, in from_settings\n    mw = mwcls()\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/scrapy/downloadermiddlewares/useragent.py\", line 32, in **init**\n    self.ua = UserAgent()\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/fake_useragent/fake.py\", line 10, in **init**\n    self.data = load_cached()\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/fake_useragent/utils.py\", line 141, in load_cached\n    update()\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/fake_useragent/utils.py\", line 136, in update\n    write(load())\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/fake_useragent/utils.py\", line 93, in load\n    browsers_dict[browser_key] = get_browser_versions(browser)\n  File \"/home/mic/anaconda2/lib/python2.7/site-packages/fake_useragent/utils.py\", line 56, in get_browser_versions\n    html = html.split('<div id=\\'liste\\'>')[1]\nIndexError: list index out of range\n`\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/3", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/3/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/3/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/3/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/3", "id": 142627402, "node_id": "MDU6SXNzdWUxNDI2Mjc0MDI=", "number": 3, "title": "Deprecated class warning on disabling UserAgentMiddleware", "user": {"login": "shafaq", "id": 1555830, "node_id": "MDQ6VXNlcjE1NTU4MzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1555830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shafaq", "html_url": "https://github.com/shafaq", "followers_url": "https://api.github.com/users/shafaq/followers", "following_url": "https://api.github.com/users/shafaq/following{/other_user}", "gists_url": "https://api.github.com/users/shafaq/gists{/gist_id}", "starred_url": "https://api.github.com/users/shafaq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shafaq/subscriptions", "organizations_url": "https://api.github.com/users/shafaq/orgs", "repos_url": "https://api.github.com/users/shafaq/repos", "events_url": "https://api.github.com/users/shafaq/events{/privacy}", "received_events_url": "https://api.github.com/users/shafaq/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-03-22T11:50:57Z", "updated_at": "2016-03-22T13:49:28Z", "closed_at": "2016-03-22T13:49:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using this line in settings.py, at running spider a warning for deprecated class is given.\n`'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,`\n\nInstead `scrapy.downloadermiddlewares.useragent.UserAgentMiddleware` should be used. Can you please change the docs to reflect this.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/2", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/2/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/2/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/2/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/2", "id": 109120316, "node_id": "MDU6SXNzdWUxMDkxMjAzMTY=", "number": 2, "title": "Didn't work when used to scrape linkedin", "user": {"login": "vionemc", "id": 6565672, "node_id": "MDQ6VXNlcjY1NjU2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6565672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vionemc", "html_url": "https://github.com/vionemc", "followers_url": "https://api.github.com/users/vionemc/followers", "following_url": "https://api.github.com/users/vionemc/following{/other_user}", "gists_url": "https://api.github.com/users/vionemc/gists{/gist_id}", "starred_url": "https://api.github.com/users/vionemc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vionemc/subscriptions", "organizations_url": "https://api.github.com/users/vionemc/orgs", "repos_url": "https://api.github.com/users/vionemc/repos", "events_url": "https://api.github.com/users/vionemc/events{/privacy}", "received_events_url": "https://api.github.com/users/vionemc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2015-09-30T15:43:56Z", "updated_at": "2015-10-20T14:41:43Z", "closed_at": "2015-10-20T14:41:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I set user-agent normally in scrapy, and I succeed to scrape linkedin.\n\nI thought this library would help me bypass 999 response from linkedin even better, but I got 999 response like I used to.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/1", "repository_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent", "labels_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/1/labels{/name}", "comments_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/1/comments", "events_url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/issues/1/events", "html_url": "https://github.com/alecxe/scrapy-fake-useragent/issues/1", "id": 89370800, "node_id": "MDU6SXNzdWU4OTM3MDgwMA==", "number": 1, "title": "Hosting on pypi", "user": {"login": "Roconda", "id": 1711132, "node_id": "MDQ6VXNlcjE3MTExMzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1711132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Roconda", "html_url": "https://github.com/Roconda", "followers_url": "https://api.github.com/users/Roconda/followers", "following_url": "https://api.github.com/users/Roconda/following{/other_user}", "gists_url": "https://api.github.com/users/Roconda/gists{/gist_id}", "starred_url": "https://api.github.com/users/Roconda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Roconda/subscriptions", "organizations_url": "https://api.github.com/users/Roconda/orgs", "repos_url": "https://api.github.com/users/Roconda/repos", "events_url": "https://api.github.com/users/Roconda/events{/privacy}", "received_events_url": "https://api.github.com/users/Roconda/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 156532459, "node_id": "MDU6TGFiZWwxNTY1MzI0NTk=", "url": "https://api.github.com/repos/alecxe/scrapy-fake-useragent/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alecxe", "id": 1291983, "node_id": "MDQ6VXNlcjEyOTE5ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecxe", "html_url": "https://github.com/alecxe", "followers_url": "https://api.github.com/users/alecxe/followers", "following_url": "https://api.github.com/users/alecxe/following{/other_user}", "gists_url": "https://api.github.com/users/alecxe/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecxe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecxe/subscriptions", "organizations_url": "https://api.github.com/users/alecxe/orgs", "repos_url": "https://api.github.com/users/alecxe/repos", "events_url": "https://api.github.com/users/alecxe/events{/privacy}", "received_events_url": "https://api.github.com/users/alecxe/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2015-06-18T19:02:00Z", "updated_at": "2015-06-21T04:28:55Z", "closed_at": "2015-06-21T04:28:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Would be nice if you could host this on pypi.\n\nThanks in advance!\n", "performed_via_github_app": null, "score": 1.0}]}