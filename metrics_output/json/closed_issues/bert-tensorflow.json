{"total_count": 315, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/google-research/bert/issues/1135", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1135/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1135/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1135/events", "html_url": "https://github.com/google-research/bert/issues/1135", "id": 675246368, "node_id": "MDU6SXNzdWU2NzUyNDYzNjg=", "number": 1135, "title": "module 'tensorflow_estimator.python.estimator.api._v1.estimator.tpu' has no attribute 'CrossShardOptimizer'", "user": {"login": "liuyibox", "id": 24197492, "node_id": "MDQ6VXNlcjI0MTk3NDky", "avatar_url": "https://avatars2.githubusercontent.com/u/24197492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liuyibox", "html_url": "https://github.com/liuyibox", "followers_url": "https://api.github.com/users/liuyibox/followers", "following_url": "https://api.github.com/users/liuyibox/following{/other_user}", "gists_url": "https://api.github.com/users/liuyibox/gists{/gist_id}", "starred_url": "https://api.github.com/users/liuyibox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liuyibox/subscriptions", "organizations_url": "https://api.github.com/users/liuyibox/orgs", "repos_url": "https://api.github.com/users/liuyibox/repos", "events_url": "https://api.github.com/users/liuyibox/events{/privacy}", "received_events_url": "https://api.github.com/users/liuyibox/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-07T20:04:19Z", "updated_at": "2020-08-10T03:26:30Z", "closed_at": "2020-08-10T03:26:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to pretrain a bert from google's pretrained checkpoint from Colab TPU. Until yesterday everything is fine. However, I came across this 'crossshardoptimizer' error for all day today. I am wondering if this caused by any code base change or version migration.\r\n\r\ntf version: 1.15.2\r\npython: 3.6\r\nbert-tensorflow: 1.0.3\r\n\r\n> >\r\n> \r\n> \r\n> INFO:tensorflow:*** Input Files (MSL-128) ***\r\n> INFO:tensorflow:  gs://vbert/input/vmware-docs-2020-reddit_non-wwm_msl-128_vocab-vmware-unused.tfrecord\r\n> INFO:tensorflow:*** Input Files (MSL-512) ***\r\n> INFO:tensorflow:  gs://vbert/input/vmware-docs-2020-reddit_non-wwm_msl-512_vocab-vmware-unused.tfrecord\r\n> WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5054197bf8>) includes params argument, but params are not passed to Estimator.\r\n> INFO:tensorflow:Using config: {'_model_dir': 'gs://vbert/liuyi-vbert-docs-reddit/base/vocab-vmware-unused', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n> cluster_def {\r\n>   job {\r\n>     name: \"worker\"\r\n>     tasks {\r\n>       key: 0\r\n>       value: \"10.47.24.194:8470\"\r\n>     }\r\n>   }\r\n> }\r\n> isolate_session_state: true\r\n> , '_keep_checkpoint_max': 10000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f505413deb8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.47.24.194:8470', '_evaluation_master': 'grpc://10.47.24.194:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=10000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f505413dc50>}\r\n> INFO:tensorflow:_TPUContext: eval_on_tpu True\r\n> INFO:tensorflow:***** Running training *****\r\n> INFO:tensorflow:  Batch size = 32\r\n> INFO:tensorflow:Querying Tensorflow master (grpc://10.47.24.194:8470) for TPU system metadata.\r\n> INFO:tensorflow:Found TPU system:\r\n> INFO:tensorflow:*** Num TPU Cores: 8\r\n> INFO:tensorflow:*** Num TPU Workers: 1\r\n> INFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 18293633603678532293)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 16754746863277155707)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12168993875110325416)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5785133627713800739)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 531464872121750804)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13610383926908237188)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3588204162670013970)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5523440629424163654)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 9311023021754933234)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17907827073552055203)\r\n> INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5163179840106115260)\r\n> INFO:tensorflow:Calling model_fn.\r\n> WARNING:tensorflow:Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f50541971e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\n> WARNING: Entity <function input_fn_builder.<locals>.input_fn.<locals>.<lambda> at 0x7f50541971e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Str'\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:Found small feature: next_sentence_labels [4, 1]\r\n> INFO:tensorflow:*** Features ***\r\n> INFO:tensorflow:  name = input_ids, shape = (4, 128)\r\n> INFO:tensorflow:  name = input_mask, shape = (4, 128)\r\n> INFO:tensorflow:  name = masked_lm_ids, shape = (4, 20)\r\n> INFO:tensorflow:  name = masked_lm_positions, shape = (4, 20)\r\n> INFO:tensorflow:  name = masked_lm_weights, shape = (4, 20)\r\n> INFO:tensorflow:  name = next_sentence_labels, shape = (4, 1)\r\n> INFO:tensorflow:  name = segment_ids, shape = (4, 128)\r\n> INFO:tensorflow:**** Trainable Variables ****\r\n> ERROR:tensorflow:Error recorded from training_loop: module 'tensorflow_estimator.python.estimator.api._v1.estimator.tpu' has no attribute 'CrossShardOptimizer'\r\n> INFO:tensorflow:training_loop marked as finished\r\n> WARNING:tensorflow:Reraising captured error\r\n> \r\n> ---------------------------------------------------------------------------\r\n> \r\n> AttributeError                            Traceback (most recent call last)\r\n> \r\n> <ipython-input-23-5adfa9741e65> in <module>()\r\n>       3 start_time = datetime.now()\r\n>       4 FLAGS.training_start_time = start_time\r\n> ----> 5 main()\r\n>       6 print(\"Pretraining took\", datetime.now() - start_time)\r\n> \r\n> 25 frames\r\n> \r\n> <ipython-input-22-85dd70a98293> in main()\r\n>      93         max_predictions_per_seq=FLAGS.max_predictions_per_seq,\r\n>      94         is_training=True)\r\n> ---> 95     estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps, saving_listeners=[listener])\r\n>      96 \r\n>      97     FLAGS.loop_times = loop_times\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n>    3033     finally:\r\n>    3034       rendezvous.record_done('training_loop')\r\n> -> 3035       rendezvous.raise_errors()\r\n>    3036 \r\n>    3037   def evaluate(self,\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/error_handling.py in raise_errors(self, timeout_sec)\r\n>     134       else:\r\n>     135         logging.warn('Reraising captured error')\r\n> --> 136         six.reraise(typ, value, traceback)\r\n>     137 \r\n>     138     for k, (typ, value, traceback) in kept_errors:\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)\r\n>     701             if value.__traceback__ is not tb:\r\n>     702                 raise value.with_traceback(tb)\r\n> --> 703             raise value\r\n>     704         finally:\r\n>     705             value = None\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n>    3028           steps=steps,\r\n>    3029           max_steps=max_steps,\r\n> -> 3030           saving_listeners=saving_listeners)\r\n>    3031     except Exception:  # pylint: disable=broad-except\r\n>    3032       rendezvous.record_error('training_loop', sys.exc_info())\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n>     368 \r\n>     369       saving_listeners = _check_listeners_type(saving_listeners)\r\n> --> 370       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>     371       logging.info('Loss for final step: %s.', loss)\r\n>     372       return self\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n>    1159       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n>    1160     else:\r\n> -> 1161       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n>    1162 \r\n>    1163   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n>    1189       worker_hooks.extend(input_hooks)\r\n>    1190       estimator_spec = self._call_model_fn(\r\n> -> 1191           features, labels, ModeKeys.TRAIN, self.config)\r\n>    1192       global_step_tensor = training_util.get_global_step(g)\r\n>    1193       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n>    2855       else:\r\n>    2856         return super(TPUEstimator, self)._call_model_fn(features, labels, mode,\r\n> -> 2857                                                         config)\r\n>    2858     else:\r\n>    2859       if mode == _INFERENCE_ON_TPU_MODE:\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n>    1147 \r\n>    1148     logging.info('Calling model_fn.')\r\n> -> 1149     model_fn_results = self._model_fn(features=features, **kwargs)\r\n>    1150     logging.info('Done calling model_fn.')\r\n>    1151 \r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in _model_fn(features, labels, mode, config, params)\r\n>    3157         if mode == model_fn_lib.ModeKeys.TRAIN:\r\n>    3158           compile_op, loss, host_call, scaffold_fn, training_hooks = (\r\n> -> 3159               _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn))\r\n>    3160           if ctx.embedding_config:\r\n>    3161             g = ops.get_default_graph()\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in _train_on_tpu_system(ctx, model_fn_wrapper, dequeue_fn)\r\n>    3602       num_shards=ctx.num_replicas,\r\n>    3603       outputs_from_all_shards=False,\r\n> -> 3604       device_assignment=ctx.device_assignment)\r\n>    3605 \r\n>    3606   loss = loss[0]\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/tpu/tpu.py in split_compile_and_shard(computation, inputs, num_shards, input_shard_axes, outputs_from_all_shards, output_shard_axes, infeed_queue, device_assignment, name)\r\n>    1275       infeed_queue=infeed_queue,\r\n>    1276       device_assignment=device_assignment,\r\n> -> 1277       name=name)\r\n>    1278 \r\n>    1279   # There must be at least one shard since num_shards > 0.\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/tpu/tpu.py in split_compile_and_replicate(***failed resolving arguments***)\r\n>     990       vscope.set_custom_getter(custom_getter)\r\n>     991 \r\n> --> 992       outputs = computation(*computation_inputs)\r\n>     993 \r\n>     994       vscope.set_use_resource(saved_use_resource)\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in multi_tpu_train_steps_on_single_shard(replica_id)\r\n>    3587           lambda i, loss: i < iterations_per_loop_var,\r\n>    3588           lambda i, loss: [i + 1, single_tpu_train_step(i)],\r\n> -> 3589           inputs=[0, _INITIAL_LOSS])\r\n>    3590       return outputs[1:]\r\n>    3591 \r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/tpu/training_loop.py in while_loop(***failed resolving arguments***)\r\n>     176     inputs = [array_ops.constant(0)]\r\n>     177   return control_flow_ops.while_loop(\r\n> --> 178       condition_wrapper, body_wrapper, inputs, name=\"\", parallel_iterations=1)\r\n>     179 \r\n>     180 \r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\r\n>    2751       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n>    2752     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\r\n> -> 2753                                     return_same_structure)\r\n>    2754     if maximum_iterations is not None:\r\n>    2755       return result[1]\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)\r\n>    2243       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access\r\n>    2244         original_body_result, exit_vars = self._BuildLoop(\r\n> -> 2245             pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n>    2246     finally:\r\n>    2247       self.Exit()\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n>    2168         expand_composites=True)\r\n>    2169     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n> -> 2170     body_result = body(*packed_vars_for_body)\r\n>    2171     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n>    2172     if not nest.is_sequence_or_composite(body_result):\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/tpu/training_loop.py in body_wrapper(*inputs)\r\n>     119     else:\r\n>     120       dequeue_ops = []\r\n> --> 121     outputs = body(*(inputs + dequeue_ops))\r\n>     122 \r\n>     123     # If the computation only returned one value, make it a tuple.\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in <lambda>(i, loss)\r\n>    3586       outputs = training_loop.while_loop(\r\n>    3587           lambda i, loss: i < iterations_per_loop_var,\r\n> -> 3588           lambda i, loss: [i + 1, single_tpu_train_step(i)],\r\n>    3589           inputs=[0, _INITIAL_LOSS])\r\n>    3590       return outputs[1:]\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in train_step(step)\r\n>    1713 \r\n>    1714       estimator_spec = self._verify_estimator_spec(\r\n> -> 1715           self._call_model_fn(features, labels))\r\n>    1716       loss, train_op = estimator_spec.loss, estimator_spec.train_op\r\n>    1717 \r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py in _call_model_fn(self, features, labels, is_export_mode)\r\n>    1992       _add_item_to_params(params, _CTX_KEY, user_context)\r\n>    1993 \r\n> -> 1994     estimator_spec = self._model_fn(features=features, **kwargs)\r\n>    1995     if (running_on_cpu and\r\n>    1996         isinstance(estimator_spec, model_fn_lib._TPUEstimatorSpec)):  # pylint: disable=protected-access\r\n> \r\n> <ipython-input-21-bc7abb17e900> in model_fn(features, labels, mode, params)\r\n>      67     if mode == tf.estimator.ModeKeys.TRAIN:\r\n>      68       train_op = optimization.create_optimizer(\r\n> ---> 69           total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n>      70 \r\n>      71       output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/bert/optimization.py in create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu)\r\n>      66 \r\n>      67   if use_tpu:\r\n> ---> 68     optimizer = tf.estimator.tpu.CrossShardOptimizer(optimizer)\r\n>      69 \r\n>      70   tvars = tf.trainable_variables()\r\n> \r\n> /tensorflow-1.15.2/python3.6/tensorflow_core/python/util/module_wrapper.py in __getattr__(self, name)\r\n>     191   def __getattr__(self, name):\r\n>     192     try:\r\n> --> 193       attr = getattr(self._tfmw_wrapped_module, name)\r\n>     194     except AttributeError:\r\n>     195       if not self._tfmw_public_apis:\r\n> \r\n> AttributeError: module 'tensorflow_estimator.python.estimator.api._v1.estimator.tpu' has no attribute 'CrossShardOptimizer'\r\n> \r\nAny insights and discussions are appreciated. Thanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1123", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1123/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1123/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1123/events", "html_url": "https://github.com/google-research/bert/issues/1123", "id": 658520229, "node_id": "MDU6SXNzdWU2NTg1MjAyMjk=", "number": 1123, "title": "Generating multiple ckpt files while finetuning BERT base with SQuAD", "user": {"login": "itsjatinsharma", "id": 64981360, "node_id": "MDQ6VXNlcjY0OTgxMzYw", "avatar_url": "https://avatars2.githubusercontent.com/u/64981360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itsjatinsharma", "html_url": "https://github.com/itsjatinsharma", "followers_url": "https://api.github.com/users/itsjatinsharma/followers", "following_url": "https://api.github.com/users/itsjatinsharma/following{/other_user}", "gists_url": "https://api.github.com/users/itsjatinsharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/itsjatinsharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itsjatinsharma/subscriptions", "organizations_url": "https://api.github.com/users/itsjatinsharma/orgs", "repos_url": "https://api.github.com/users/itsjatinsharma/repos", "events_url": "https://api.github.com/users/itsjatinsharma/events{/privacy}", "received_events_url": "https://api.github.com/users/itsjatinsharma/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-16T20:02:05Z", "updated_at": "2020-07-17T15:55:33Z", "closed_at": "2020-07-17T15:55:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Generating multiple ckpt files in the destination path in Drive while finetuning BERT base with SQuAD in colab with GPU.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1118", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1118/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1118/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1118/events", "html_url": "https://github.com/google-research/bert/issues/1118", "id": 650937202, "node_id": "MDU6SXNzdWU2NTA5MzcyMDI=", "number": 1118, "title": "zero-shot for IsNext and NotNext function", "user": {"login": "radiodee1", "id": 8641916, "node_id": "MDQ6VXNlcjg2NDE5MTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/8641916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/radiodee1", "html_url": "https://github.com/radiodee1", "followers_url": "https://api.github.com/users/radiodee1/followers", "following_url": "https://api.github.com/users/radiodee1/following{/other_user}", "gists_url": "https://api.github.com/users/radiodee1/gists{/gist_id}", "starred_url": "https://api.github.com/users/radiodee1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/radiodee1/subscriptions", "organizations_url": "https://api.github.com/users/radiodee1/orgs", "repos_url": "https://api.github.com/users/radiodee1/repos", "events_url": "https://api.github.com/users/radiodee1/events{/privacy}", "received_events_url": "https://api.github.com/users/radiodee1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-04T18:37:34Z", "updated_at": "2020-07-08T12:28:10Z", "closed_at": "2020-07-08T12:28:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm interested in BERT's pre-trained 'IsNext' and 'NotNext' functionality. How could I, without doing any fine-tuning, get Is/Not Next property from the Large model? I thought I would try out 'extract_features.py' from the bert repository, but in the 'jsonl' file there are clearly too many entries.\r\n\r\nI also tried the 'run_classifier.py' file, but I believe this requires training. I'm looking to use BERT in an almost 'zero-shot' mode.\r\n\r\nEDIT:\r\nI also looked at the run_pretraining.py file. This is what I changed (just one line...):\r\n\r\n```python\r\n\r\ndef model_fn_builder(bert_config, init_checkpoint, learning_rate,\r\n                     num_train_steps, num_warmup_steps, use_tpu,\r\n                     use_one_hot_embeddings):\r\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\r\n\r\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\r\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\r\n\r\n    tf.logging.info(\"*** Features ***\")\r\n    for name in sorted(features.keys()):\r\n      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\r\n\r\n    input_ids = features[\"input_ids\"]\r\n    input_mask = features[\"input_mask\"]\r\n    segment_ids = features[\"segment_ids\"]\r\n    masked_lm_positions = features[\"masked_lm_positions\"]\r\n    masked_lm_ids = features[\"masked_lm_ids\"]\r\n    masked_lm_weights = features[\"masked_lm_weights\"]\r\n    next_sentence_labels = features[\"next_sentence_labels\"]\r\n\r\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n    model = modeling.BertModel(\r\n        config=bert_config,\r\n        is_training=is_training,\r\n        input_ids=input_ids,\r\n        input_mask=input_mask,\r\n        token_type_ids=segment_ids,\r\n        use_one_hot_embeddings=use_one_hot_embeddings)\r\n\r\n    (masked_lm_loss,\r\n     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(\r\n         bert_config, model.get_sequence_output(), model.get_embedding_table(),\r\n         masked_lm_positions, masked_lm_ids, masked_lm_weights)\r\n\r\n    (next_sentence_loss, next_sentence_example_loss,\r\n     next_sentence_log_probs) = get_next_sentence_output(\r\n         bert_config, model.get_pooled_output(), next_sentence_labels)\r\n\r\n    total_loss = masked_lm_loss + next_sentence_loss\r\n\r\n    tvars = tf.trainable_variables()\r\n\r\n    initialized_variable_names = {}\r\n    scaffold_fn = None\r\n    if init_checkpoint:\r\n      (assignment_map, initialized_variable_names\r\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\r\n      if use_tpu:\r\n\r\n        def tpu_scaffold():\r\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n          return tf.train.Scaffold()\r\n\r\n        scaffold_fn = tpu_scaffold\r\n      else:\r\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n\r\n    tf.logging.info(\"**** Trainable Variables ****\")\r\n    for var in tvars:\r\n      init_string = \"\"\r\n      if var.name in initialized_variable_names:\r\n        init_string = \", *INIT_FROM_CKPT*\"\r\n      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\r\n                      init_string)\r\n\r\n    output_spec = None\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n      train_op = optimization.create_optimizer(\r\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n\r\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n          mode=mode,\r\n          loss=total_loss,\r\n          train_op=train_op,\r\n          scaffold_fn=scaffold_fn)\r\n    elif mode == tf.estimator.ModeKeys.EVAL:\r\n\r\n      def metric_fn(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\r\n                    masked_lm_weights, next_sentence_example_loss,\r\n                    next_sentence_log_probs, next_sentence_labels):\r\n        \"\"\"Computes the loss and accuracy of the model.\"\"\"\r\n        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,\r\n                                         [-1, masked_lm_log_probs.shape[-1]])\r\n        masked_lm_predictions = tf.argmax(\r\n            masked_lm_log_probs, axis=-1, output_type=tf.int32)\r\n        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [-1])\r\n        masked_lm_ids = tf.reshape(masked_lm_ids, [-1])\r\n        masked_lm_weights = tf.reshape(masked_lm_weights, [-1])\r\n        masked_lm_accuracy = tf.metrics.accuracy(\r\n            labels=masked_lm_ids,\r\n            predictions=masked_lm_predictions,\r\n            weights=masked_lm_weights)\r\n        masked_lm_mean_loss = tf.metrics.mean(\r\n            values=masked_lm_example_loss, weights=masked_lm_weights)\r\n\r\n        next_sentence_log_probs = tf.reshape(\r\n            next_sentence_log_probs, [-1, next_sentence_log_probs.shape[-1]])\r\n        next_sentence_predictions = tf.argmax(\r\n            next_sentence_log_probs, axis=-1, output_type=tf.int32)\r\n        next_sentence_labels = tf.reshape(next_sentence_labels, [-1])\r\n        next_sentence_accuracy = tf.metrics.accuracy(\r\n            labels=next_sentence_labels, predictions=next_sentence_predictions)\r\n        next_sentence_mean_loss = tf.metrics.mean(\r\n            values=next_sentence_example_loss)\r\n\r\n        return {\r\n            \"masked_lm_accuracy\": masked_lm_accuracy,\r\n            \"masked_lm_loss\": masked_lm_mean_loss,\r\n            \"next_sentence_accuracy\": next_sentence_accuracy,\r\n            \"next_sentence_loss\": next_sentence_mean_loss,\r\n            \"next_sentence_log_probs\": next_sentence_log_probs, ## <-- add this line here\r\n        }\r\n\r\n      eval_metrics = (metric_fn, [\r\n          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,\r\n          masked_lm_weights, next_sentence_example_loss,\r\n          next_sentence_log_probs, next_sentence_labels\r\n      ])\r\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n          mode=mode,\r\n          loss=total_loss,\r\n          eval_metrics=eval_metrics,\r\n          scaffold_fn=scaffold_fn)\r\n    else:\r\n      raise ValueError(\"Only TRAIN and EVAL modes are supported: %s\" % (mode))\r\n\r\n    return output_spec\r\n\r\n  return model_fn\r\n```\r\nI get an error like this:\r\n\r\nTypeError: Values of eval_metric_ops must be (metric_value, update_op) tuples, given: Tensor(\"Reshape_7:0\", shape=(1, 2), dtype=float32) for key: next_sentence_log_probs\r\n\r\nI thought that would do it. How do I get that Tensor to show up as values??? Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1098", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1098/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1098/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1098/events", "html_url": "https://github.com/google-research/bert/issues/1098", "id": 631894180, "node_id": "MDU6SXNzdWU2MzE4OTQxODA=", "number": 1098, "title": "Unhandled Rejection (Error): Unknown layer: BertModelLayer.", "user": {"login": "Decoder3-14", "id": 43238519, "node_id": "MDQ6VXNlcjQzMjM4NTE5", "avatar_url": "https://avatars0.githubusercontent.com/u/43238519?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Decoder3-14", "html_url": "https://github.com/Decoder3-14", "followers_url": "https://api.github.com/users/Decoder3-14/followers", "following_url": "https://api.github.com/users/Decoder3-14/following{/other_user}", "gists_url": "https://api.github.com/users/Decoder3-14/gists{/gist_id}", "starred_url": "https://api.github.com/users/Decoder3-14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Decoder3-14/subscriptions", "organizations_url": "https://api.github.com/users/Decoder3-14/orgs", "repos_url": "https://api.github.com/users/Decoder3-14/repos", "events_url": "https://api.github.com/users/Decoder3-14/events{/privacy}", "received_events_url": "https://api.github.com/users/Decoder3-14/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-05T19:44:59Z", "updated_at": "2020-06-27T20:59:23Z", "closed_at": "2020-06-27T20:59:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello everyone\r\n\r\nI have trained a TensorFlow 2.0 Keras model that extends the BertModelLayer provided by this library. The module has been tested and consumed enough on the typical IPYNB environments like Colab. \r\n\r\nAfter saving it and converting it using tensorflow_converter to be used in the browser, I used the following snippet to load the module in a JS environment:\r\n`import * as tf from '@tensorflow/tfjs'`\r\n`\r\nsync componentDidMount() {\r\n        let model =  await tf.models.modelFromJSON(modelPath);\r\n    }\r\n`\r\n\r\nthe _modelPath_ refers to the local path of the model.json file. The moment my project compiles I am getting the following error:\r\n\r\n> Unhandled Rejection (Error): Unknown layer: BertModelLayer. This may be due to one of the following reasons:\r\n> 1. The layer is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\r\n> 2. The custom layer is defined in JavaScript but is not registered properly with tf.serialization.registerClass().\r\n\r\nI have checked the documentation for TF2.0 and the community in Stackoverflow and here but not many similar problems were there. So if anyone can help me resolve this issue would be very appreciated.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1097", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1097/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1097/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1097/events", "html_url": "https://github.com/google-research/bert/issues/1097", "id": 630440308, "node_id": "MDU6SXNzdWU2MzA0NDAzMDg=", "number": 1097, "title": "I don't know how to properly use fine tuned Bert Model", "user": {"login": "XINZHANG-ops", "id": 47906299, "node_id": "MDQ6VXNlcjQ3OTA2Mjk5", "avatar_url": "https://avatars1.githubusercontent.com/u/47906299?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XINZHANG-ops", "html_url": "https://github.com/XINZHANG-ops", "followers_url": "https://api.github.com/users/XINZHANG-ops/followers", "following_url": "https://api.github.com/users/XINZHANG-ops/following{/other_user}", "gists_url": "https://api.github.com/users/XINZHANG-ops/gists{/gist_id}", "starred_url": "https://api.github.com/users/XINZHANG-ops/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XINZHANG-ops/subscriptions", "organizations_url": "https://api.github.com/users/XINZHANG-ops/orgs", "repos_url": "https://api.github.com/users/XINZHANG-ops/repos", "events_url": "https://api.github.com/users/XINZHANG-ops/events{/privacy}", "received_events_url": "https://api.github.com/users/XINZHANG-ops/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-04T01:27:53Z", "updated_at": "2020-08-08T01:45:38Z", "closed_at": "2020-08-08T01:45:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "1. So what I need to do is train a fine turned model on multi-classes classification and use it locally.\r\n\r\n2. I followed the tutorial from this link:\r\nhttps://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb\r\n\r\n3. I created a bucket to save its outputs, and I load my own data from my drive. After training, I have these files in the bucket.\r\n<img width=\"308\" alt=\"Screen Shot 2020-06-03 at 9 12 01 PM\" src=\"https://user-images.githubusercontent.com/47906299/83704020-fb8f0b80-a5de-11ea-910b-51d337bdd75b.png\">\r\n\r\n4. It works all perfectly fine on the colab.\r\n\r\n5. But I want to use the trained model locally. So I read this article:\r\nhttps://towardsdatascience.com/3-ways-to-optimize-and-export-bert-model-for-online-serving-8f49d774a501\r\n\r\n6. So I download the all stuff generated into the bucket, includes model.ckpt-14226\r\n<img width=\"892\" alt=\"Screen Shot 2020-06-03 at 9 25 00 PM\" src=\"https://user-images.githubusercontent.com/47906299/83704700-dd2a0f80-a5e0-11ea-8242-ce48c002c01a.png\">\r\n\r\n\r\n7. And I follow the steps from the article, when I run this block of codes (These codes are in the article from the above link)\r\n\r\n<img width=\"715\" alt=\"Screen Shot 2020-06-03 at 9 19 35 PM\" src=\"https://user-images.githubusercontent.com/47906299/83704413-072f0200-a5e0-11ea-833e-e93e7aa68376.png\">\r\n\r\n7.  I run into these errors. So anyone can help me with this, thank you!\r\n\r\n\r\nINFO:tensorflow:Restoring parameters from ./EvalOutput/model.ckpt-14226\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1347       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1348       self._extend_graph()\r\n   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _extend_graph(self)\r\n   1387     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1388       tf_session.ExtendSession(self._session)\r\n   1389 \r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'TPUReplicateMetadata' used by {{node TPUReplicateMetadata}}with these attrs: [allow_soft_placement=false, num_cores_per_replica=1, use_tpu=true, num_replicas=8, computation_shape=[], host_compute_core=[], device_assignment=[], padding_map=[], _tpu_replicate=\"cluster\", step_marker_location=\"STEP_MARK_AT_TOP_LEVEL_WHILE_LOOP\", topology=\"\"]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[TPUReplicateMetadata]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py in restore(self, sess, save_path)\r\n   1289         sess.run(self.saver_def.restore_op_name,\r\n-> 1290                  {self.saver_def.filename_tensor_name: save_path})\r\n   1291     except errors.NotFoundError as err:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    955       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 956                          run_metadata_ptr)\r\n    957       if run_metadata:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1179       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1180                              feed_dict_tensor, options, run_metadata)\r\n   1181     else:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1359                            run_metadata)\r\n   1360     else:\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'TPUReplicateMetadata' used by node TPUReplicateMetadata (defined at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [allow_soft_placement=false, num_cores_per_replica=1, use_tpu=true, num_replicas=8, computation_shape=[], host_compute_core=[], device_assignment=[], padding_map=[], _tpu_replicate=\"cluster\", step_marker_location=\"STEP_MARK_AT_TOP_LEVEL_WHILE_LOOP\", topology=\"\"]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[TPUReplicateMetadata]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-2-1189dd9d05b9> in <module>\r\n     10 sess = tf.Session()\r\n     11 imported_meta = tf.train.import_meta_graph(os.path.join(path, 'model.ckpt-14226.meta')) #based on the steps of your fine-tuned model\r\n---> 12 imported_meta.restore(sess, os.path.join(path, 'model.ckpt-14226')) #based on the steps of your fine-tuned model\r\n     13 my_vars = []\r\n     14 for var in tf.all_variables():\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py in restore(self, sess, save_path)\r\n   1324       # We add a more reasonable error message here to help users (b/110263146)\r\n   1325       raise _wrap_restore_error_with_msg(\r\n-> 1326           err, \"a mismatch between the current graph and the graph\")\r\n   1327 \r\n   1328   @staticmethod\r\n\r\nInvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nNo OpKernel was registered to support Op 'TPUReplicateMetadata' used by node TPUReplicateMetadata (defined at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [allow_soft_placement=false, num_cores_per_replica=1, use_tpu=true, num_replicas=8, computation_shape=[], host_compute_core=[], device_assignment=[], padding_map=[], _tpu_replicate=\"cluster\", step_marker_location=\"STEP_MARK_AT_TOP_LEVEL_WHILE_LOOP\", topology=\"\"]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[TPUReplicateMetadata]]\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1096", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1096/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1096/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1096/events", "html_url": "https://github.com/google-research/bert/issues/1096", "id": 629653595, "node_id": "MDU6SXNzdWU2Mjk2NTM1OTU=", "number": 1096, "title": "why dropout at predicting time", "user": {"login": "alexwongdl", "id": 8745241, "node_id": "MDQ6VXNlcjg3NDUyNDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8745241?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexwongdl", "html_url": "https://github.com/alexwongdl", "followers_url": "https://api.github.com/users/alexwongdl/followers", "following_url": "https://api.github.com/users/alexwongdl/following{/other_user}", "gists_url": "https://api.github.com/users/alexwongdl/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexwongdl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexwongdl/subscriptions", "organizations_url": "https://api.github.com/users/alexwongdl/orgs", "repos_url": "https://api.github.com/users/alexwongdl/repos", "events_url": "https://api.github.com/users/alexwongdl/events{/privacy}", "received_events_url": "https://api.github.com/users/alexwongdl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-03T02:58:35Z", "updated_at": "2020-06-04T11:33:10Z", "closed_at": "2020-06-04T11:31:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "While reading modeling.py, I'm wondering why the dropout function always do dropout, no matter it is training or predicting. In my opinion, when the model is used to predict, there should be no dropout, or the result will changes at different time for one certain example.\r\n![image](https://user-images.githubusercontent.com/8745241/83590608-b93adf80-a588-11ea-8316-dda603cba4a0.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1091", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1091/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1091/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1091/events", "html_url": "https://github.com/google-research/bert/issues/1091", "id": 626018243, "node_id": "MDU6SXNzdWU2MjYwMTgyNDM=", "number": 1091, "title": "Export model as .pb file in android", "user": {"login": "adarshrana205", "id": 42955521, "node_id": "MDQ6VXNlcjQyOTU1NTIx", "avatar_url": "https://avatars3.githubusercontent.com/u/42955521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adarshrana205", "html_url": "https://github.com/adarshrana205", "followers_url": "https://api.github.com/users/adarshrana205/followers", "following_url": "https://api.github.com/users/adarshrana205/following{/other_user}", "gists_url": "https://api.github.com/users/adarshrana205/gists{/gist_id}", "starred_url": "https://api.github.com/users/adarshrana205/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adarshrana205/subscriptions", "organizations_url": "https://api.github.com/users/adarshrana205/orgs", "repos_url": "https://api.github.com/users/adarshrana205/repos", "events_url": "https://api.github.com/users/adarshrana205/events{/privacy}", "received_events_url": "https://api.github.com/users/adarshrana205/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-27T20:45:41Z", "updated_at": "2020-06-10T23:25:50Z", "closed_at": "2020-06-10T23:25:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have created a CNN model in jupyter notebook.But i want to export this model in android studio as .pb file. \r\nI used the following code:\r\n\r\n\r\ndef export_model(saver, model, input_node_names, output_node_name):\r\n    tf.io.write_graph(tf.compat.v1.keras.backend.get_session().graph_def, 'out',MODEL_NAME + '_graph.pbtxt')\r\n\r\n    saver.save(tf.compat.v1.keras.backend.get_session(),save_path, 'out/' + MODEL_NAME + '.chkp')\r\n\r\n    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None,False, 'out/' + MODEL_NAME + '.chkp', output_node_name,\r\n        \"save/restore_all\", \"save/Const:0\",'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\r\n\r\n    input_graph_def = tf.GraphDef()\r\n    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\r\n        input_graph_def.ParseFromString(f.read())\r\n\r\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n            input_graph_def, input_node_names, [output_node_name],\r\n            tf.float32.as_datatype_enum)\r\n\r\n    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n\r\n    print(\"graph saved!\")\r\n\r\n\r\n export_model(tf.compat.v1.train.Saver(), model, [\"conv2d_1_input\"], \"dense_2/Softmax\")\r\n    \r\n\r\nBut this is giving me an error.\r\nSo please someone modify it so that i can use it in android.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1088", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1088/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1088/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1088/events", "html_url": "https://github.com/google-research/bert/issues/1088", "id": 621278738, "node_id": "MDU6SXNzdWU2MjEyNzg3Mzg=", "number": 1088, "title": "LayerNorm normalises the batch dimension as well", "user": {"login": "smr97", "id": 18290261, "node_id": "MDQ6VXNlcjE4MjkwMjYx", "avatar_url": "https://avatars0.githubusercontent.com/u/18290261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smr97", "html_url": "https://github.com/smr97", "followers_url": "https://api.github.com/users/smr97/followers", "following_url": "https://api.github.com/users/smr97/following{/other_user}", "gists_url": "https://api.github.com/users/smr97/gists{/gist_id}", "starred_url": "https://api.github.com/users/smr97/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smr97/subscriptions", "organizations_url": "https://api.github.com/users/smr97/orgs", "repos_url": "https://api.github.com/users/smr97/repos", "events_url": "https://api.github.com/users/smr97/events{/privacy}", "received_events_url": "https://api.github.com/users/smr97/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-19T20:55:19Z", "updated_at": "2020-05-19T21:19:30Z", "closed_at": "2020-05-19T21:19:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI am using this repository to extend BERT in scenarios where batch size (at inference) is not constant throughout the network (monotonically reduces as we go from layer0 to layer 11).\r\nI see that the `begin_norm_axis` is set to `-1` in the function `layer_norm` (defined [here](https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L362)).\r\nNow, does this mean that normalisation is done over the batch dimension as well? Consider cases where `return_2d_output=False`. The purpose of layer normalisation in it's original paper was to not normalise over a batch, but over the activations produced by a single sample in the batch. Could you please explain why this diverges to normalise over the batch as well?\r\n\r\nThanks in advance", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1087", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1087/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1087/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1087/events", "html_url": "https://github.com/google-research/bert/issues/1087", "id": 620463393, "node_id": "MDU6SXNzdWU2MjA0NjMzOTM=", "number": 1087, "title": "module 'tensorflow_core._api.v2.train' has no attribute 'Saver'", "user": {"login": "adarshrana205", "id": 42955521, "node_id": "MDQ6VXNlcjQyOTU1NTIx", "avatar_url": "https://avatars3.githubusercontent.com/u/42955521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adarshrana205", "html_url": "https://github.com/adarshrana205", "followers_url": "https://api.github.com/users/adarshrana205/followers", "following_url": "https://api.github.com/users/adarshrana205/following{/other_user}", "gists_url": "https://api.github.com/users/adarshrana205/gists{/gist_id}", "starred_url": "https://api.github.com/users/adarshrana205/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adarshrana205/subscriptions", "organizations_url": "https://api.github.com/users/adarshrana205/orgs", "repos_url": "https://api.github.com/users/adarshrana205/repos", "events_url": "https://api.github.com/users/adarshrana205/events{/privacy}", "received_events_url": "https://api.github.com/users/adarshrana205/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-18T19:49:42Z", "updated_at": "2020-05-27T20:27:07Z", "closed_at": "2020-05-27T20:27:07Z", "author_association": "NONE", "active_lock_reason": null, "body": " train(model, x_train, y_train, x_test, y_test)\r\n export_model(tf.train.Saver(), model, [\"conv2d_1_input\"], \"dense_2/Softmax\")\r\n     \r\nAttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Saver'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1082", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1082/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1082/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1082/events", "html_url": "https://github.com/google-research/bert/issues/1082", "id": 615341590, "node_id": "MDU6SXNzdWU2MTUzNDE1OTA=", "number": 1082, "title": "how to used fine tuned model as initial checkpoint for another task?", "user": {"login": "bohanbo", "id": 23105633, "node_id": "MDQ6VXNlcjIzMTA1NjMz", "avatar_url": "https://avatars0.githubusercontent.com/u/23105633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bohanbo", "html_url": "https://github.com/bohanbo", "followers_url": "https://api.github.com/users/bohanbo/followers", "following_url": "https://api.github.com/users/bohanbo/following{/other_user}", "gists_url": "https://api.github.com/users/bohanbo/gists{/gist_id}", "starred_url": "https://api.github.com/users/bohanbo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bohanbo/subscriptions", "organizations_url": "https://api.github.com/users/bohanbo/orgs", "repos_url": "https://api.github.com/users/bohanbo/repos", "events_url": "https://api.github.com/users/bohanbo/events{/privacy}", "received_events_url": "https://api.github.com/users/bohanbo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-10T08:16:13Z", "updated_at": "2020-05-11T21:33:55Z", "closed_at": "2020-05-11T21:33:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI fined tuned a classification model with 19 classes and then I add several new classes and i want to use the old model as an initial checkpoint to fine tune the new model.  After i point the initial checkpoint in the training command to the previous fined model, i got this error\r\n\r\n```ValueError: Shape of variable loss/output_bias:0 ((23,)) doesn't match with shape of tensor loss/output_bias ([19]) from checkpoint reader.```\r\n\r\nWhat is the correct way to save the fine tuned model in order to accomplish this? Thank you!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1066", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1066/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1066/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1066/events", "html_url": "https://github.com/google-research/bert/issues/1066", "id": 603415869, "node_id": "MDU6SXNzdWU2MDM0MTU4Njk=", "number": 1066, "title": "BERT-Tiny,BERT-Mini,BERT-Small,BERT-Medium - TF 2.0 checkpoints ", "user": {"login": "17patelumang", "id": 6100731, "node_id": "MDQ6VXNlcjYxMDA3MzE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6100731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/17patelumang", "html_url": "https://github.com/17patelumang", "followers_url": "https://api.github.com/users/17patelumang/followers", "following_url": "https://api.github.com/users/17patelumang/following{/other_user}", "gists_url": "https://api.github.com/users/17patelumang/gists{/gist_id}", "starred_url": "https://api.github.com/users/17patelumang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/17patelumang/subscriptions", "organizations_url": "https://api.github.com/users/17patelumang/orgs", "repos_url": "https://api.github.com/users/17patelumang/repos", "events_url": "https://api.github.com/users/17patelumang/events{/privacy}", "received_events_url": "https://api.github.com/users/17patelumang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-20T17:42:37Z", "updated_at": "2020-08-14T19:17:55Z", "closed_at": "2020-08-14T19:17:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi All , \r\n\r\nI am looking at BERT checkpoint here - https://github.com/tensorflow/models/tree/master/official/nlp/bert for TF 2.0 .\r\n\r\nAre checkpoints for BERT-Tiny,BERT-Mini,BERT-Small,BERT-Medium avaialbe in TF 2.0 ?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1054", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1054/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1054/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1054/events", "html_url": "https://github.com/google-research/bert/issues/1054", "id": 598374916, "node_id": "MDU6SXNzdWU1OTgzNzQ5MTY=", "number": 1054, "title": "?", "user": {"login": "ZhengxiangShi", "id": 58726982, "node_id": "MDQ6VXNlcjU4NzI2OTgy", "avatar_url": "https://avatars3.githubusercontent.com/u/58726982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhengxiangShi", "html_url": "https://github.com/ZhengxiangShi", "followers_url": "https://api.github.com/users/ZhengxiangShi/followers", "following_url": "https://api.github.com/users/ZhengxiangShi/following{/other_user}", "gists_url": "https://api.github.com/users/ZhengxiangShi/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhengxiangShi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhengxiangShi/subscriptions", "organizations_url": "https://api.github.com/users/ZhengxiangShi/orgs", "repos_url": "https://api.github.com/users/ZhengxiangShi/repos", "events_url": "https://api.github.com/users/ZhengxiangShi/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhengxiangShi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-11T23:49:44Z", "updated_at": "2020-06-25T15:17:27Z", "closed_at": "2020-06-25T15:16:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1037", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1037/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1037/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1037/events", "html_url": "https://github.com/google-research/bert/issues/1037", "id": 585750691, "node_id": "MDU6SXNzdWU1ODU3NTA2OTE=", "number": 1037, "title": "MRPC Produces Two Vastly Different Eval Accuracy", "user": {"login": "wei-v-wang", "id": 22306846, "node_id": "MDQ6VXNlcjIyMzA2ODQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/22306846?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wei-v-wang", "html_url": "https://github.com/wei-v-wang", "followers_url": "https://api.github.com/users/wei-v-wang/followers", "following_url": "https://api.github.com/users/wei-v-wang/following{/other_user}", "gists_url": "https://api.github.com/users/wei-v-wang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wei-v-wang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wei-v-wang/subscriptions", "organizations_url": "https://api.github.com/users/wei-v-wang/orgs", "repos_url": "https://api.github.com/users/wei-v-wang/repos", "events_url": "https://api.github.com/users/wei-v-wang/events{/privacy}", "received_events_url": "https://api.github.com/users/wei-v-wang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-22T16:00:24Z", "updated_at": "2020-04-08T06:12:40Z", "closed_at": "2020-03-22T16:10:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nI am testing using TF v1.14.0. \r\nI ran for 10 times with run_classifier.py (do_traing=False, do_eval=True), i.e. do inference immediately after loading https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip \r\n\r\nI am seeing two values 0.31617647 & 0.6838235. \r\nI wonder why I am seeing different inference results and why I am seeing exactly these two results. Thanks! \r\n\r\ncheck-pretrained-MRPC-1.log:INFO:tensorflow:  eval_accuracy = 0.31617647\r\ncheck-pretrained-MRPC-10.log:INFO:tensorflow:  eval_accuracy = 0.6838235\r\ncheck-pretrained-MRPC-2.log:INFO:tensorflow:  eval_accuracy = 0.31617647\r\ncheck-pretrained-MRPC-3.log:INFO:tensorflow:  eval_accuracy = 0.6838235\r\ncheck-pretrained-MRPC-4.log:INFO:tensorflow:  eval_accuracy = 0.31617647\r\ncheck-pretrained-MRPC-5.log:INFO:tensorflow:  eval_accuracy = 0.31617647\r\ncheck-pretrained-MRPC-6.log:INFO:tensorflow:  eval_accuracy = 0.31617647\r\ncheck-pretrained-MRPC-7.log:INFO:tensorflow:  eval_accuracy = 0.31617647\r\ncheck-pretrained-MRPC-8.log:INFO:tensorflow:  eval_accuracy = 0.31617647\r\ncheck-pretrained-MRPC-9.log:INFO:tensorflow:  eval_accuracy = 0.31617647", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1032", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1032/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1032/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1032/events", "html_url": "https://github.com/google-research/bert/issues/1032", "id": 581245003, "node_id": "MDU6SXNzdWU1ODEyNDUwMDM=", "number": 1032, "title": "BERT SavedModel  Feature Transformation using TF API inference timings", "user": {"login": "17patelumang", "id": 6100731, "node_id": "MDQ6VXNlcjYxMDA3MzE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6100731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/17patelumang", "html_url": "https://github.com/17patelumang", "followers_url": "https://api.github.com/users/17patelumang/followers", "following_url": "https://api.github.com/users/17patelumang/following{/other_user}", "gists_url": "https://api.github.com/users/17patelumang/gists{/gist_id}", "starred_url": "https://api.github.com/users/17patelumang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/17patelumang/subscriptions", "organizations_url": "https://api.github.com/users/17patelumang/orgs", "repos_url": "https://api.github.com/users/17patelumang/repos", "events_url": "https://api.github.com/users/17patelumang/events{/privacy}", "received_events_url": "https://api.github.com/users/17patelumang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-14T15:06:29Z", "updated_at": "2020-08-14T19:17:51Z", "closed_at": "2020-08-14T19:17:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi ,\r\n\r\nI have BERT Large Model with 512 sequence length which I have converted to savedModel . The Feature transformation has also been converted using TF API and ported inside saved model format.\r\n\r\nDuring the inference time the input is just plain text and output is the score given by BERT.  The savedmodel takes care of converting raw string to feature vector and then passing that vector to BERT Large for inferencing.\r\n\r\nIs there any way within savedModel where we can time separately - the time to convert raw text to features and then time separately  the time to do the prediction from vectors ? \r\n\r\nWill normal adding time logging at appropriate places in featureTransformation code using TF API help ?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1030", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1030/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1030/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1030/events", "html_url": "https://github.com/google-research/bert/issues/1030", "id": 580228714, "node_id": "MDU6SXNzdWU1ODAyMjg3MTQ=", "number": 1030, "title": "BERT Large , 512 sequence length - Allocation of X exceeds Y% of system memory.", "user": {"login": "17patelumang", "id": 6100731, "node_id": "MDQ6VXNlcjYxMDA3MzE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6100731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/17patelumang", "html_url": "https://github.com/17patelumang", "followers_url": "https://api.github.com/users/17patelumang/followers", "following_url": "https://api.github.com/users/17patelumang/following{/other_user}", "gists_url": "https://api.github.com/users/17patelumang/gists{/gist_id}", "starred_url": "https://api.github.com/users/17patelumang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/17patelumang/subscriptions", "organizations_url": "https://api.github.com/users/17patelumang/orgs", "repos_url": "https://api.github.com/users/17patelumang/repos", "events_url": "https://api.github.com/users/17patelumang/events{/privacy}", "received_events_url": "https://api.github.com/users/17patelumang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-03-12T21:16:16Z", "updated_at": "2020-08-14T19:54:25Z", "closed_at": "2020-08-14T19:54:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi ,\r\n\r\nI am trying to run BERT Large model having 512 sequence length on CPU for inference. I have converted checkpoint file from BERT Large to savedModel format which has feature transformation ported to it as well.\r\n\r\nHowever when I do the inference I can see  warning message as \r\n\r\n\r\n**\"tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of X exceeds 10% of system memory.\"**\r\n\r\nCould anyone please help me identifying the root cause. Having read different issue related to this it says that the batch size needs to be reduced however trying that doesn't help. \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1005", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1005/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1005/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1005/events", "html_url": "https://github.com/google-research/bert/issues/1005", "id": 565998172, "node_id": "MDU6SXNzdWU1NjU5OTgxNzI=", "number": 1005, "title": "Can you provide the evaluation for mean average prevision for classification task?", "user": {"login": "CSerxy", "id": 7486478, "node_id": "MDQ6VXNlcjc0ODY0Nzg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7486478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CSerxy", "html_url": "https://github.com/CSerxy", "followers_url": "https://api.github.com/users/CSerxy/followers", "following_url": "https://api.github.com/users/CSerxy/following{/other_user}", "gists_url": "https://api.github.com/users/CSerxy/gists{/gist_id}", "starred_url": "https://api.github.com/users/CSerxy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CSerxy/subscriptions", "organizations_url": "https://api.github.com/users/CSerxy/orgs", "repos_url": "https://api.github.com/users/CSerxy/repos", "events_url": "https://api.github.com/users/CSerxy/events{/privacy}", "received_events_url": "https://api.github.com/users/CSerxy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-17T00:28:44Z", "updated_at": "2020-02-24T16:59:42Z", "closed_at": "2020-02-24T16:59:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I found that even in the official document in TensorFlow, there is no such implementation of mean AP as well. I wonder if you could add this as a measurement for BERT?\r\n\r\nMany thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/1000", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/1000/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/1000/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/1000/events", "html_url": "https://github.com/google-research/bert/issues/1000", "id": 562992213, "node_id": "MDU6SXNzdWU1NjI5OTIyMTM=", "number": 1000, "title": "bert run_classifier key error = '0'", "user": {"login": "agarwalishan", "id": 41884267, "node_id": "MDQ6VXNlcjQxODg0MjY3", "avatar_url": "https://avatars1.githubusercontent.com/u/41884267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/agarwalishan", "html_url": "https://github.com/agarwalishan", "followers_url": "https://api.github.com/users/agarwalishan/followers", "following_url": "https://api.github.com/users/agarwalishan/following{/other_user}", "gists_url": "https://api.github.com/users/agarwalishan/gists{/gist_id}", "starred_url": "https://api.github.com/users/agarwalishan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/agarwalishan/subscriptions", "organizations_url": "https://api.github.com/users/agarwalishan/orgs", "repos_url": "https://api.github.com/users/agarwalishan/repos", "events_url": "https://api.github.com/users/agarwalishan/events{/privacy}", "received_events_url": "https://api.github.com/users/agarwalishan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-11T05:51:45Z", "updated_at": "2020-08-04T06:44:03Z", "closed_at": "2020-08-04T06:43:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "File \"run_classifier.py\", line 981, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\Parveen\\ishan\\bertenv\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\Parveen\\ishan\\bertenv\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\Parveen\\ishan\\bertenv\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"run_classifier.py\", line 942, in main\r\n    predict_file)\r\n  File \"run_classifier.py\", line 490, in file_based_convert_examples_to_features\r\n    max_seq_length, tokenizer)\r\n  File \"run_classifier.py\", line 459, in convert_single_example\r\n    label_id = label_map[example.label]\r\nKeyError: '0'\r\n\r\n\r\nI have changed the labels in the colaProcessor class and my training is successful, I am getting this error during testing. Please help", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/980", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/980/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/980/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/980/events", "html_url": "https://github.com/google-research/bert/issues/980", "id": 544901616, "node_id": "MDU6SXNzdWU1NDQ5MDE2MTY=", "number": 980, "title": "Why the results of pytorch and TensorFlow are inconsistent", "user": {"login": "sychenga", "id": 45419517, "node_id": "MDQ6VXNlcjQ1NDE5NTE3", "avatar_url": "https://avatars1.githubusercontent.com/u/45419517?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sychenga", "html_url": "https://github.com/sychenga", "followers_url": "https://api.github.com/users/sychenga/followers", "following_url": "https://api.github.com/users/sychenga/following{/other_user}", "gists_url": "https://api.github.com/users/sychenga/gists{/gist_id}", "starred_url": "https://api.github.com/users/sychenga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sychenga/subscriptions", "organizations_url": "https://api.github.com/users/sychenga/orgs", "repos_url": "https://api.github.com/users/sychenga/repos", "events_url": "https://api.github.com/users/sychenga/events{/privacy}", "received_events_url": "https://api.github.com/users/sychenga/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-01-03T08:44:27Z", "updated_at": "2020-01-05T09:20:40Z", "closed_at": "2020-01-05T09:20:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "TensorFlow result is lower than pytorch result in my classification experiment.They are all default parameters and same epoch.I'm sure it's not experimental error.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/971", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/971/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/971/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/971/events", "html_url": "https://github.com/google-research/bert/issues/971", "id": 542885619, "node_id": "MDU6SXNzdWU1NDI4ODU2MTk=", "number": 971, "title": "\u8bf7\u95ee\u83b7\u5f97\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\u7684\u503c\uff1f How to get the pretrained embedding_table?", "user": {"login": "WHQ1111", "id": 41694860, "node_id": "MDQ6VXNlcjQxNjk0ODYw", "avatar_url": "https://avatars3.githubusercontent.com/u/41694860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WHQ1111", "html_url": "https://github.com/WHQ1111", "followers_url": "https://api.github.com/users/WHQ1111/followers", "following_url": "https://api.github.com/users/WHQ1111/following{/other_user}", "gists_url": "https://api.github.com/users/WHQ1111/gists{/gist_id}", "starred_url": "https://api.github.com/users/WHQ1111/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WHQ1111/subscriptions", "organizations_url": "https://api.github.com/users/WHQ1111/orgs", "repos_url": "https://api.github.com/users/WHQ1111/repos", "events_url": "https://api.github.com/users/WHQ1111/events{/privacy}", "received_events_url": "https://api.github.com/users/WHQ1111/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-27T14:03:25Z", "updated_at": "2020-02-27T02:51:15Z", "closed_at": "2019-12-28T11:44:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "I wanted to know how to get the word embedding after I pretrain the model. Are there someone kindly tell me?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/968", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/968/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/968/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/968/events", "html_url": "https://github.com/google-research/bert/issues/968", "id": 542424745, "node_id": "MDU6SXNzdWU1NDI0MjQ3NDU=", "number": 968, "title": "is it possible to get less than 2.0 loss when pretraining from scratch on 20gb dataset on the first 30k steps?", "user": {"login": "WissamAntoun", "id": 44616226, "node_id": "MDQ6VXNlcjQ0NjE2MjI2", "avatar_url": "https://avatars0.githubusercontent.com/u/44616226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WissamAntoun", "html_url": "https://github.com/WissamAntoun", "followers_url": "https://api.github.com/users/WissamAntoun/followers", "following_url": "https://api.github.com/users/WissamAntoun/following{/other_user}", "gists_url": "https://api.github.com/users/WissamAntoun/gists{/gist_id}", "starred_url": "https://api.github.com/users/WissamAntoun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WissamAntoun/subscriptions", "organizations_url": "https://api.github.com/users/WissamAntoun/orgs", "repos_url": "https://api.github.com/users/WissamAntoun/repos", "events_url": "https://api.github.com/users/WissamAntoun/events{/privacy}", "received_events_url": "https://api.github.com/users/WissamAntoun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-26T04:02:51Z", "updated_at": "2020-06-20T23:24:44Z", "closed_at": "2020-06-20T23:24:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey, I'm trying to pretrain BERT from scratch on a new language, in my first trial I got a ~2 loss and a 60% MLM accuracy after 800k to 1M steps.\r\n\r\nI tried a new way to segment the text then trained a new vocab model, then generated new pretraining data. Now the loss after just 10k steps is 2.7 then drops to below 2 from 30k onwards.\r\n\r\ni checked the pretraining data and it seems correct, the MLM accuracy is above 64% on all checkpoints between 10k and 100k.\r\n\r\nIs this even possible, and should i continue the training till 1M? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/967", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/967/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/967/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/967/events", "html_url": "https://github.com/google-research/bert/issues/967", "id": 542269041, "node_id": "MDU6SXNzdWU1NDIyNjkwNDE=", "number": 967, "title": "BERT Multi-lingual: similarity score of two different languages?", "user": {"login": "chiragsanghvi10", "id": 45583446, "node_id": "MDQ6VXNlcjQ1NTgzNDQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/45583446?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chiragsanghvi10", "html_url": "https://github.com/chiragsanghvi10", "followers_url": "https://api.github.com/users/chiragsanghvi10/followers", "following_url": "https://api.github.com/users/chiragsanghvi10/following{/other_user}", "gists_url": "https://api.github.com/users/chiragsanghvi10/gists{/gist_id}", "starred_url": "https://api.github.com/users/chiragsanghvi10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chiragsanghvi10/subscriptions", "organizations_url": "https://api.github.com/users/chiragsanghvi10/orgs", "repos_url": "https://api.github.com/users/chiragsanghvi10/repos", "events_url": "https://api.github.com/users/chiragsanghvi10/events{/privacy}", "received_events_url": "https://api.github.com/users/chiragsanghvi10/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-12-25T06:33:20Z", "updated_at": "2020-02-14T09:50:28Z", "closed_at": "2019-12-27T06:02:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nIs it possible to find the similarity score of two different languages?\r\n\r\n### For example:\r\n> Sentence 1 (English) \r\n> Sentence 2 (Translation in Hindi of sentence 1)\r\n> Sentence similarity score.\r\n\r\nBest\r\nChirag Sanghvi \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/959", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/959/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/959/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/959/events", "html_url": "https://github.com/google-research/bert/issues/959", "id": 535827619, "node_id": "MDU6SXNzdWU1MzU4Mjc2MTk=", "number": 959, "title": "How to interprete results", "user": {"login": "raff7", "id": 15360240, "node_id": "MDQ6VXNlcjE1MzYwMjQw", "avatar_url": "https://avatars3.githubusercontent.com/u/15360240?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raff7", "html_url": "https://github.com/raff7", "followers_url": "https://api.github.com/users/raff7/followers", "following_url": "https://api.github.com/users/raff7/following{/other_user}", "gists_url": "https://api.github.com/users/raff7/gists{/gist_id}", "starred_url": "https://api.github.com/users/raff7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raff7/subscriptions", "organizations_url": "https://api.github.com/users/raff7/orgs", "repos_url": "https://api.github.com/users/raff7/repos", "events_url": "https://api.github.com/users/raff7/events{/privacy}", "received_events_url": "https://api.github.com/users/raff7/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-10T16:05:48Z", "updated_at": "2020-06-06T16:47:56Z", "closed_at": "2020-06-06T16:47:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am a bit confused.\r\nAfter the training im left with the following files in the output folder:\r\n\r\ntrain.tf_record\r\nmodel.ckpt-x.meta\r\nmodel.ckpt-x.index\r\nmodel.ckpt-x.data-00000of-x\r\ngraph.pbtxt\r\ncheckpoints.txt\r\nevents.out.tfevents.1575989900.MO-HSK-M-TEC057\r\n\r\ni know i have to use the model.ckpt to load the model when testing it, but how do i use the other files to analyse how the training went? is there any helpful information in them? I set the flag \"do evaluate\" to true, so there should be the results of the evaluations somewhere no? \r\n\r\nthanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/940", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/940/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/940/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/940/events", "html_url": "https://github.com/google-research/bert/issues/940", "id": 528568054, "node_id": "MDU6SXNzdWU1Mjg1NjgwNTQ=", "number": 940, "title": "Question about calculating word embeddings using pretrained model", "user": {"login": "jinj200200", "id": 20244788, "node_id": "MDQ6VXNlcjIwMjQ0Nzg4", "avatar_url": "https://avatars2.githubusercontent.com/u/20244788?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinj200200", "html_url": "https://github.com/jinj200200", "followers_url": "https://api.github.com/users/jinj200200/followers", "following_url": "https://api.github.com/users/jinj200200/following{/other_user}", "gists_url": "https://api.github.com/users/jinj200200/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinj200200/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinj200200/subscriptions", "organizations_url": "https://api.github.com/users/jinj200200/orgs", "repos_url": "https://api.github.com/users/jinj200200/repos", "events_url": "https://api.github.com/users/jinj200200/events{/privacy}", "received_events_url": "https://api.github.com/users/jinj200200/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-26T08:34:48Z", "updated_at": "2020-02-27T02:50:57Z", "closed_at": "2019-12-12T11:25:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "In extract_features.py , word embeddings seems to be calculated by summing token, segment and positon embeddings. So the word embedding of \"apple\" in \"Apple is a very good company\" and \"Apple is a kind of fruit\" should be the same. However, I calculate with extract_features.py, the embeddings are NOT the same. What mistake did I make?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/936", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/936/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/936/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/936/events", "html_url": "https://github.com/google-research/bert/issues/936", "id": 528142902, "node_id": "MDU6SXNzdWU1MjgxNDI5MDI=", "number": 936, "title": "Pretraining - ValueError", "user": {"login": "marwage", "id": 5268710, "node_id": "MDQ6VXNlcjUyNjg3MTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5268710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marwage", "html_url": "https://github.com/marwage", "followers_url": "https://api.github.com/users/marwage/followers", "following_url": "https://api.github.com/users/marwage/following{/other_user}", "gists_url": "https://api.github.com/users/marwage/gists{/gist_id}", "starred_url": "https://api.github.com/users/marwage/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marwage/subscriptions", "organizations_url": "https://api.github.com/users/marwage/orgs", "repos_url": "https://api.github.com/users/marwage/repos", "events_url": "https://api.github.com/users/marwage/events{/privacy}", "received_events_url": "https://api.github.com/users/marwage/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-25T15:00:25Z", "updated_at": "2020-04-11T09:03:02Z", "closed_at": "2019-12-08T20:04:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello BERT community!\r\n\r\nI would like to run the pretraining but I am stuck with an error.\r\n\r\nMy procedure is the following.\r\n1. download the Wikipedia dump\r\n2. use the wikiextractor\r\n3. use Spacy to get files with a sentence for each line\r\n```\r\nimport spacy\r\nimport re\r\nimport os\r\n\r\n\r\ndef segment_dir(nlp, dir):\r\n    for file in os.scandir(dir):\r\n        if file.is_file() and file.name != \".DS_Store\":\r\n            print(\"file \" + str(file.name))\r\n            with open(file, \"r\") as text_file:\r\n                text = text_file.read()\r\n\r\n            text = re.sub(r\"[\\n]{2,}\", \"\\n\", text)\r\n            text = re.sub(r\"<.*>\\n\", \"\", text)\r\n            text = re.sub(r\"^[^\\s]+\\n\", \"\", text)\r\n\r\n            doc = nlp(text, disable=[\"tagger\", \"ner\"])\r\n            with open(dir.path + \"/sample_text.txt\", \"a\") as out_file:\r\n                for sent in doc.sents:\r\n                    out_file.write(re.sub(r\"\\n\", \"\", str(sent)) + \"\\n\")\r\n\r\n\r\ndef main():\r\n    nlp = spacy.load(\"en_core_web_lg\")\r\n    nlp.max_length = 1500000\r\n\r\n    for dir in os.scandir():\r\n        if dir.is_dir():\r\n            print(\"dir \" + str(dir.name))\r\n            segment_dir(nlp, dir)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n4. create tf_record for each sample_text using `create_pretraining_data.py`\r\n5. run `run_pretraining.py`\r\n\r\nThen I get the following error\r\n```\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] INFO:tensorflow:*** Input Files ***\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f22940faae8>) includes params argument, but params are not passed to Estimator.\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] INFO:tensorflow:Using config: {'_model_dir': '/cache/out_dir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] graph_options {\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   rewrite_options {\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     meta_optimizer_iterations: ONE\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   }\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] }\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2192c78160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] INFO:tensorflow:_TPUContext: eval_on_tpu True\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] INFO:tensorflow:***** Running training *****\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] INFO:tensorflow:  Batch size = 32\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] WARNING:tensorflow:From /home/work/user-job-dir/src/bert_kf/run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] Instructions for updating:\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] Use `tf.data.experimental.parallel_interleave(...)`.\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] INFO:tensorflow:Error recorded from training_loop: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(\"arg0:0\", shape=(), dtype=float32, device=/device:CPU:0)'\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] INFO:tensorflow:training_loop marked as finished\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] WARNING:tensorflow:Reraising captured error\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] Traceback (most recent call last):\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/home/work/user-job-dir/src/bert_kf/run_pretraining.py\", line 493, in <module>\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     tf.app.run()\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     _sys.exit(main(argv))\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/home/work/user-job-dir/src/bert_kf/run_pretraining.py\", line 466, in main\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2409, in train\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     rendezvous.raise_errors()\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py\", line 128, in raise_errors\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     six.reraise(typ, value, traceback)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     raise value\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2403, in train\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     saving_listeners=saving_listeners\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     return self._train_model_default(input_fn, hooks, saving_listeners)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1234, in _train_model_default\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1075, in _get_features_and_labels_from_input_fn\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     self._call_input_fn(input_fn, mode))\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2363, in _call_input_fn\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     return input_fn(**kwargs)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/home/work/user-job-dir/src/bert_kf/run_pretraining.py\", line 368, in input_fn\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     cycle_length=cycle_length))\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1190, in apply\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     dataset = transformation_func(self)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/experimental/ops/interleave_ops.py\", line 87, in _apply_fn\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     buffer_output_elements, prefetch_input_elements)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py\", line 134, in __init__\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     cycle_length, block_length)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2714, in __init__\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     super(InterleaveDataset, self).__init__(input_dataset, map_func)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2677, in __init__\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     experimental_nested_dataset_support=True)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1860, in __init__\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     self._function.add_to_graph(ops.get_default_graph())\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     self._create_definition_if_needed()\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     self._create_definition_if_needed_impl()\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     self._capture_by_value, self._caller_device)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 864, in func_graph_from_py_func\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     outputs = func(*func_graph.inputs)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1794, in tf_data_structured_function_wrapper\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     ret = func(*nested_args)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/readers.py\", line 200, in __init__\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     filenames = ops.convert_to_tensor(filenames, dtype=dtypes.string)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1050, in convert_to_tensor\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     as_ref=False)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]   File \"/root/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 983, in _TensorTensorConversionFunction\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr]     (dtype.name, t.dtype.name, str(t)))\r\n[\u001b[1;32m169.254.128.185.10000\u001b[m::stderr] ValueError: Tensor conversion requested dtype string for Tensor with dtype float32: 'Tensor(\"arg0:0\", shape=(), dtype=float32, device=/device:CPU:0)'\r\n```\r\n\r\nCan someone tell me what the issue is?\r\nThank you!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/931", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/931/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/931/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/931/events", "html_url": "https://github.com/google-research/bert/issues/931", "id": 527107194, "node_id": "MDU6SXNzdWU1MjcxMDcxOTQ=", "number": 931, "title": "How to splice the other features needed into Bert and fine-tune them together?", "user": {"login": "zysNLP", "id": 45376689, "node_id": "MDQ6VXNlcjQ1Mzc2Njg5", "avatar_url": "https://avatars2.githubusercontent.com/u/45376689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zysNLP", "html_url": "https://github.com/zysNLP", "followers_url": "https://api.github.com/users/zysNLP/followers", "following_url": "https://api.github.com/users/zysNLP/following{/other_user}", "gists_url": "https://api.github.com/users/zysNLP/gists{/gist_id}", "starred_url": "https://api.github.com/users/zysNLP/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zysNLP/subscriptions", "organizations_url": "https://api.github.com/users/zysNLP/orgs", "repos_url": "https://api.github.com/users/zysNLP/repos", "events_url": "https://api.github.com/users/zysNLP/events{/privacy}", "received_events_url": "https://api.github.com/users/zysNLP/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-22T10:15:07Z", "updated_at": "2019-12-27T10:10:48Z", "closed_at": "2019-12-27T10:10:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "In the process of using bert, we don't want to give up our hand-crafted features, such as the length of the sentence, the number of nouns in the sentence, the average length of the words in the sentence, and so on. I know that bert's create_model function in run_classifier has output_layer = model.get_pooled_output() and the output size is [batch_size, hidden_size]. I wander if  it could directly make my above three features to shape [batch_size, 3], Thus concat into [batch_size, hidden_size+3], and then fine-tuned, the problem is that the create_model function can not control the step size of batch_size.\r\n\r\nI also think that using the extract_features method mentioned in https://github.com/google-research/bert/issues/201 is not so reasonable. This method seems to use the bert to extract the feature vector separately, then concat to three features together, then Training and validing alone, but if this method lost the role of bert fine-tuning?\r\n\r\nIs there anyone realize the concat custom features? Thank you very much!!!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/916", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/916/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/916/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/916/events", "html_url": "https://github.com/google-research/bert/issues/916", "id": 522598538, "node_id": "MDU6SXNzdWU1MjI1OTg1Mzg=", "number": 916, "title": "run_classifier with low GPU Util and low GPU Memory", "user": {"login": "guotong1988", "id": 4702353, "node_id": "MDQ6VXNlcjQ3MDIzNTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4702353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guotong1988", "html_url": "https://github.com/guotong1988", "followers_url": "https://api.github.com/users/guotong1988/followers", "following_url": "https://api.github.com/users/guotong1988/following{/other_user}", "gists_url": "https://api.github.com/users/guotong1988/gists{/gist_id}", "starred_url": "https://api.github.com/users/guotong1988/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guotong1988/subscriptions", "organizations_url": "https://api.github.com/users/guotong1988/orgs", "repos_url": "https://api.github.com/users/guotong1988/repos", "events_url": "https://api.github.com/users/guotong1988/events{/privacy}", "received_events_url": "https://api.github.com/users/guotong1988/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-14T03:19:01Z", "updated_at": "2019-11-14T08:31:22Z", "closed_at": "2019-11-14T08:31:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Memory-Usage: 83MiB /  8123MiB\r\nGPU-Util: 0% \r\n%CPU: 1200 \r\ntensorflow-gpu 1.14.0\r\n\r\nAny suggestion? \r\nThank you very much......", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/889", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/889/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/889/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/889/events", "html_url": "https://github.com/google-research/bert/issues/889", "id": 513301726, "node_id": "MDU6SXNzdWU1MTMzMDE3MjY=", "number": 889, "title": "\"model_fn should return an EstimatorSpec.\" Error when running \"predicting_movie_reviews_with_bert_on_tf_hub.ipynb\" for fine-tuning BERT-Chinese Model", "user": {"login": "xinxu75", "id": 32407823, "node_id": "MDQ6VXNlcjMyNDA3ODIz", "avatar_url": "https://avatars2.githubusercontent.com/u/32407823?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xinxu75", "html_url": "https://github.com/xinxu75", "followers_url": "https://api.github.com/users/xinxu75/followers", "following_url": "https://api.github.com/users/xinxu75/following{/other_user}", "gists_url": "https://api.github.com/users/xinxu75/gists{/gist_id}", "starred_url": "https://api.github.com/users/xinxu75/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xinxu75/subscriptions", "organizations_url": "https://api.github.com/users/xinxu75/orgs", "repos_url": "https://api.github.com/users/xinxu75/repos", "events_url": "https://api.github.com/users/xinxu75/events{/privacy}", "received_events_url": "https://api.github.com/users/xinxu75/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-28T13:25:56Z", "updated_at": "2020-02-01T08:08:57Z", "closed_at": "2019-10-29T18:46:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to run \"predicting_movie_reviews_with_bert_on_tf_hub.ipynb\", but fine-tuning on a Chinese-text csv with corresponding labels. As such I loaded 'https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12/1' model instead of English-model in the original code\r\n\r\nI only made minimum changes from the original jpynb for data preparation, which was running OK. But during training as in \r\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\nthere was an error of \"ValueError: model_fn should return an EstimatorSpec.\" - Note that I didn't make any modification in model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps)\r\n\r\nThe full error log is as follows - it was running on CPU (8GB RAM) with Windows 10 environment:  \r\nTraceback (most recent call last):\r\n  File \"bert_classify.py\", line 352, in <module>\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"C:\\Miniconda_python\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Miniconda_python\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1161, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Miniconda_python\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1191, in _train_model_default\r\n    features, labels, ModeKeys.TRAIN, self.config)\r\n  File \"C:\\Miniconda_python\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1153, in _call_model_fn\r\n    raise ValueError('model_fn should return an EstimatorSpec.')\r\nValueError: model_fn should return an EstimatorSpec.\r\n\r\nJust wondering what went wrong and needs to be done when loading BERT-Chinese model to fine-tune it for text classification task? Much appreciated", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/885", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/885/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/885/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/885/events", "html_url": "https://github.com/google-research/bert/issues/885", "id": 512310231, "node_id": "MDU6SXNzdWU1MTIzMTAyMzE=", "number": 885, "title": "How many \"num_tpu_cores\" be set ? ", "user": {"login": "ffeelers", "id": 18190466, "node_id": "MDQ6VXNlcjE4MTkwNDY2", "avatar_url": "https://avatars1.githubusercontent.com/u/18190466?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ffeelers", "html_url": "https://github.com/ffeelers", "followers_url": "https://api.github.com/users/ffeelers/followers", "following_url": "https://api.github.com/users/ffeelers/following{/other_user}", "gists_url": "https://api.github.com/users/ffeelers/gists{/gist_id}", "starred_url": "https://api.github.com/users/ffeelers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ffeelers/subscriptions", "organizations_url": "https://api.github.com/users/ffeelers/orgs", "repos_url": "https://api.github.com/users/ffeelers/repos", "events_url": "https://api.github.com/users/ffeelers/events{/privacy}", "received_events_url": "https://api.github.com/users/ffeelers/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-25T05:27:02Z", "updated_at": "2020-08-08T12:39:57Z", "closed_at": "2019-10-25T06:19:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "I try to pretraining with **run_pretraining.py** using **tpu-v2-32**\r\nHow many \"num_tpu_cores\" be set ? \r\nWhen tested with tpu-v2-8 worked fine(num_tpu_cores=8).\r\n\r\npython3 run_pretraining.py \\ \r\n  --input_file=gs://... \\\r\n  --output_dir=gs://... \\\r\n  --do_train=True \\\r\n  --do_eval=True \\\r\n  --bert_config_file=/data/workspace/bert/bert_config.json \\\r\n  --train_batch_size=64 \\\r\n  --max_seq_length=128 \\\r\n  --max_predictions_per_seq=19 \\\r\n  --num_train_steps=100 \\\r\n  --num_warmup_steps=70 \\\r\n  --learning_rate=1e-4 \\\r\n  --use_tpu=True \\\r\n  --num_tpu_cores=32 \\\r\n  --tpu_name=grpc://ip:8470 \\\r\n  --tpu_zone=us-central1-a \\\r\n  --gcp_project=myproject\r\n\r\n**This are parameters to run. Is that correct? When i do this, i got an error like this :** \r\n\r\nValueError: TPUConfig.num_shards is not set correctly. According to TPU system metadata for Tensorflow master (grpc://...:8470): num_replicas should be (8), got (32). For non-model-parallelism, num_replicas should be the total num of TPU cores in the system. For model-parallelism, the total number of TPU cores should be num_cores_per_replica * num_replicas. Please set it accordingly or leave it as `None`\r\n\r\n**When i set \"num_tpu_cores=8\", I got the following error :**\r\n\r\nI1025 05:22:42.688320 140065835681600 tpu_estimator.py:557] Init TPU system\r\nERROR:tensorflow:Error recorded from evaluation_loop: From /job:worker/replica:0/task:0:\r\nCloud TPU: Invalid TPU configuration, ensure ClusterResolver is passed to tpu.RunConfig\r\n         [[{{node configure_distributed_tpu/_0}}]]\r\n\r\nAm I missing something else? Or which one should I set?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/882", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/882/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/882/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/882/events", "html_url": "https://github.com/google-research/bert/issues/882", "id": 508626912, "node_id": "MDU6SXNzdWU1MDg2MjY5MTI=", "number": 882, "title": "how to reduce the size of a pretrain bert by delete the adam variable(train by tpu) ", "user": {"login": "RyanHuangNLP", "id": 49582480, "node_id": "MDQ6VXNlcjQ5NTgyNDgw", "avatar_url": "https://avatars1.githubusercontent.com/u/49582480?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RyanHuangNLP", "html_url": "https://github.com/RyanHuangNLP", "followers_url": "https://api.github.com/users/RyanHuangNLP/followers", "following_url": "https://api.github.com/users/RyanHuangNLP/following{/other_user}", "gists_url": "https://api.github.com/users/RyanHuangNLP/gists{/gist_id}", "starred_url": "https://api.github.com/users/RyanHuangNLP/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RyanHuangNLP/subscriptions", "organizations_url": "https://api.github.com/users/RyanHuangNLP/orgs", "repos_url": "https://api.github.com/users/RyanHuangNLP/repos", "events_url": "https://api.github.com/users/RyanHuangNLP/events{/privacy}", "received_events_url": "https://api.github.com/users/RyanHuangNLP/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-17T17:46:50Z", "updated_at": "2019-12-24T03:56:52Z", "closed_at": "2019-10-25T13:28:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "While a finish train a bert by **tpu cluster**, the model size is 1.2G, is much too bigger than the release one, I find the [#99](https://github.com/google-research/bert/issues/99) to reduce the model size, here is my following code:\r\n\r\n```\r\n# tf.__version__ 1.15.0\r\nsess = tf.Session()\r\nimported_meta = tf.train.import_meta_graph('model.ckpt-250000.meta')\r\nimported_meta.restore(sess,  'model.ckpt-250000.data-00000-of-00001') --->> raise exception\r\n\r\nmy_vars = []\r\nfor var in tf.all_variables():\r\n    if 'adam_v' not in var.name and 'adam_m' not in var.name:\r\n        my_vars.append(var)\r\nsaver = tf.train.Saver(my_vars)\r\nsaver.save(sess, './model.ckpt')\r\n```\r\nbut raise error\r\n```\r\nINFO:tensorflow:Restoring parameters from gs://xxxxx/xxxxx/model.ckpt-250000\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n8 frames\r\nInvalidArgumentError: No OpKernel was registered to support Op 'TPUReplicatedInput' used by {{node input0}}with these attrs: [N=8, T=DT_INT32]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[input0]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\nInvalidArgumentError: No OpKernel was registered to support Op 'TPUReplicatedInput' used by node input0 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [N=8, T=DT_INT32]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[input0]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py in restore(self, sess, save_path)\r\n   1324       # We add a more reasonable error message here to help users (b/110263146)\r\n   1325       raise _wrap_restore_error_with_msg(\r\n-> 1326           err, \"a mismatch between the current graph and the graph\")\r\n   1327 \r\n   1328   @staticmethod\r\n\r\nInvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nNo OpKernel was registered to support Op 'TPUReplicatedInput' used by node input0 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) with these attrs: [N=8, T=DT_INT32]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[input0]]\r\n```\r\n\r\nafter that I change another method\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ncheckpoint_path = \"/xxxxx/bert_model.ckpt\"\r\nnew_checkpoint_path = \"/yyyyy/bert_model.ckpt\"\r\n\r\nreader = tf.train.NewCheckpointReader(checkpoint_path)\r\nname_shape_map = reader.get_variable_to_shape_map()\r\n\r\nnew_variable_map = {}\r\nfor var_name in name_shape_map:\r\n    if 'adam_v' not in var_name and 'adam_m' not in var_name:\r\n        tensor = reader.get_tensor(var_name)\r\n        var = tf.Variable(tensor, name=var_name)\r\n        new_variable_map[var_name] = var\r\n\r\nsaver = tf.train.Saver(new_variable_map)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    saver.save(sess,  new_checkpoint_path)\r\n```\r\n\r\nthis way can success reduce the model size and the ckpt file is 400mb, almost the same as the release one, but there is a new problem is that `bert_model.ckpt.meta` file is also 400mb and the release one `meta` file is just 9.6kb\r\n\r\nI doubt that the official bert pretrain model**(train by tpu)** is how to reduce the model size   ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/881", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/881/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/881/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/881/events", "html_url": "https://github.com/google-research/bert/issues/881", "id": 507810727, "node_id": "MDU6SXNzdWU1MDc4MTA3Mjc=", "number": 881, "title": "Can not run Bert with Colab TPU", "user": {"login": "nhocqn", "id": 26921561, "node_id": "MDQ6VXNlcjI2OTIxNTYx", "avatar_url": "https://avatars0.githubusercontent.com/u/26921561?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nhocqn", "html_url": "https://github.com/nhocqn", "followers_url": "https://api.github.com/users/nhocqn/followers", "following_url": "https://api.github.com/users/nhocqn/following{/other_user}", "gists_url": "https://api.github.com/users/nhocqn/gists{/gist_id}", "starred_url": "https://api.github.com/users/nhocqn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nhocqn/subscriptions", "organizations_url": "https://api.github.com/users/nhocqn/orgs", "repos_url": "https://api.github.com/users/nhocqn/repos", "events_url": "https://api.github.com/users/nhocqn/events{/privacy}", "received_events_url": "https://api.github.com/users/nhocqn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-16T12:11:19Z", "updated_at": "2019-10-25T10:00:43Z", "closed_at": "2019-10-25T10:00:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "I try to upload credentials to TPU\r\n`  with open('/content/adc.json', 'r') as f:\r\n    auth_info = json.load(f)\r\n  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\r\n`\r\nI got this error :\r\n> InternalError: From /job:tpu_worker/replica:0/task:0:\r\n> The filesystem registered under the 'gs://' scheme was not a tensorflow::RetryingGcsFileSystem*.\r\n> \t [[{{node GcsConfigureCredentials}}]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> InternalError                             Traceback (most recent call last)\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n>    1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n>    1383                     'disable_meta_optimizer = True')\r\n> -> 1384       raise type(e)(node_def, op, message)\r\n>    1385", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/878", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/878/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/878/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/878/events", "html_url": "https://github.com/google-research/bert/issues/878", "id": 506734793, "node_id": "MDU6SXNzdWU1MDY3MzQ3OTM=", "number": 878, "title": "how to realize the tokenization of BERT model in c++", "user": {"login": "lytum", "id": 38668257, "node_id": "MDQ6VXNlcjM4NjY4MjU3", "avatar_url": "https://avatars3.githubusercontent.com/u/38668257?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lytum", "html_url": "https://github.com/lytum", "followers_url": "https://api.github.com/users/lytum/followers", "following_url": "https://api.github.com/users/lytum/following{/other_user}", "gists_url": "https://api.github.com/users/lytum/gists{/gist_id}", "starred_url": "https://api.github.com/users/lytum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lytum/subscriptions", "organizations_url": "https://api.github.com/users/lytum/orgs", "repos_url": "https://api.github.com/users/lytum/repos", "events_url": "https://api.github.com/users/lytum/events{/privacy}", "received_events_url": "https://api.github.com/users/lytum/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2019-10-14T15:45:49Z", "updated_at": "2019-11-22T08:29:58Z", "closed_at": "2019-11-22T08:29:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Thanks for your work.\r\n\r\nIf I want to use tensorflow c++ api to import the pretrained BERT model, how could I process the txt data in C++, including tokenization of BERT? is there c++ wrapper for Bert? or does tensorfow c++ api provide the tokenization of Bert? Or do I need to implement the same tokenization.py in c++?\r\n\r\nThanks for any information.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/869", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/869/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/869/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/869/events", "html_url": "https://github.com/google-research/bert/issues/869", "id": 501705000, "node_id": "MDU6SXNzdWU1MDE3MDUwMDA=", "number": 869, "title": "Multi Task Learning Clarification ", "user": {"login": "Dragon615", "id": 31970390, "node_id": "MDQ6VXNlcjMxOTcwMzkw", "avatar_url": "https://avatars2.githubusercontent.com/u/31970390?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dragon615", "html_url": "https://github.com/Dragon615", "followers_url": "https://api.github.com/users/Dragon615/followers", "following_url": "https://api.github.com/users/Dragon615/following{/other_user}", "gists_url": "https://api.github.com/users/Dragon615/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dragon615/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dragon615/subscriptions", "organizations_url": "https://api.github.com/users/Dragon615/orgs", "repos_url": "https://api.github.com/users/Dragon615/repos", "events_url": "https://api.github.com/users/Dragon615/events{/privacy}", "received_events_url": "https://api.github.com/users/Dragon615/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-10-02T20:19:52Z", "updated_at": "2019-10-02T20:20:57Z", "closed_at": "2019-10-02T20:20:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/866", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/866/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/866/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/866/events", "html_url": "https://github.com/google-research/bert/issues/866", "id": 498694739, "node_id": "MDU6SXNzdWU0OTg2OTQ3Mzk=", "number": 866, "title": "How to get the origin word embedding in training for ditillation?", "user": {"login": "SefaZeng", "id": 13918630, "node_id": "MDQ6VXNlcjEzOTE4NjMw", "avatar_url": "https://avatars2.githubusercontent.com/u/13918630?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SefaZeng", "html_url": "https://github.com/SefaZeng", "followers_url": "https://api.github.com/users/SefaZeng/followers", "following_url": "https://api.github.com/users/SefaZeng/following{/other_user}", "gists_url": "https://api.github.com/users/SefaZeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/SefaZeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SefaZeng/subscriptions", "organizations_url": "https://api.github.com/users/SefaZeng/orgs", "repos_url": "https://api.github.com/users/SefaZeng/repos", "events_url": "https://api.github.com/users/SefaZeng/events{/privacy}", "received_events_url": "https://api.github.com/users/SefaZeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-26T06:52:39Z", "updated_at": "2019-09-26T09:06:23Z", "closed_at": "2019-09-26T09:06:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want to use BERT as a teacher to do knowledge distillation for my machine translation model. So I get the output of bert and the output of MT model encoder, but they have different sentence length because of different bpe.\r\nSo I try to merge the wordpiece of origin word with reduce_mean to get the origin word embeddings. And I try to use scatted_update to keep the vector after reduce_mean. But it returns the error message **LookupError: No gradient defined for operation 'while_5/ScatterUpdate' (op type: ScatterUpdate)**\r\nSo, how can I get the origin word embeddings or how can I make it trainable?\r\nThe example code is like this.\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\n#tf.enable_eager_execution()\r\ndef fn(time_global, a, index_1, index_2, n_global, var_global):\r\n    n = n_global\r\n    temp = a\r\n    index_first = index_1\r\n    index_second = index_2\r\n    time = tf.constant(0)\r\n    zero_local = lambda: tf.zeros(shape=[tf.shape(indice)[1], 2], dtype=tf.float32)\r\n    out_temp = tf.get_variable(name='temp',dtype=tf.float32, initializer=zero_local, validate_shape=False)\r\n    def loop_fn(t, x, ind_1, ind_2, n, temp_out):\r\n        vec = tf.reduce_mean(tf.cast(x[ind_1[t]:ind_2[t]], tf.float32), axis=0)\r\n        temp_out = tf.scatter_update(out_temp, t, vec)\r\n        #print(out_temp)\r\n        print_op = tf.print('vec:', [vec,ind_1, ind_2, temp_out], output_stream=sys.stdout)\r\n        with tf.control_dependencies([print_op,out_temp, temp_out]):\r\n            return t+1, x, ind_1, ind_2, n, temp_out\r\n\r\n    out = tf.while_loop(lambda t, *_: t < n[time_global], loop_fn,\r\n                        (time, temp[time_global],\r\n                         index_first[time_global],\r\n                         index_second[time_global],\r\n                         n[time_global],\r\n                         out_temp))\r\n    out_temp = out[-1]\r\n    print_op_0 = tf.print('temp print:', out_temp, output_stream=sys.stdout)\r\n    out_temp = tf.Print(out_temp, [out_temp], message='temp Print')\r\n    zero = lambda: tf.zeros(shape=[tf.shape(n)[0],tf.shape(indice)[1],2], dtype=tf.float32)\r\n    var_global = tf.get_variable(name='var_global',dtype=tf.float32, initializer=zero, validate_shape=False)\r\n    var_global_temp = tf.scatter_update(var_global, time_global, out_temp)\r\n    with tf.control_dependencies([print_op_0, out_temp, var_global]):\r\n        return time_global+1, a, index_1, index_2, n_global, var_global_temp\r\n\r\na = tf.constant([[[i] * 2 for i in range(7)]]*3, dtype=tf.float32)\r\nindice = tf.constant([[1,3,6,0],[1,4,6,0],[2,4,6,0]])\r\nn = [tf.shape(indice)[1] - 1]\r\nn = tf.tile(n,[tf.shape(indice)[0]])\r\ntag_for_sent = tf.range(tf.shape(a)[0])\r\nindex_1 = indice[:,:-1]\r\nindex_2 = indice[:,1:]\r\n#out_tas = tf.tile(out_ta, [tf.shape(indice)[0]])\r\n#a_temp = tf.zeros([tf.shape(a)[0], tf.shape(indice)[1]-1, 5], dtype=tf.float32)\r\ntime_global = tf.constant(0)\r\nzero = tf.zeros(shape=[tf.shape(a)[0],tf.shape(indice)[1],2], dtype=tf.float32)\r\nwith tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\r\n    var_global = tf.get_variable(name='var_global',dtype=tf.float32, initializer=zero, validate_shape=False)\r\nout = tf.while_loop(lambda t_global, *_: t_global < tf.shape(n)[0], fn, (time_global, a, index_1, index_2, n, var_global))\r\nsys.stdout.flush()\r\nprint(out[0])\r\n\r\nwith tf.Session() as sess:\r\n    tf.global_variables_initializer().run()\r\n    #print(sess.run(a))\r\n    out = sess.run(out)\r\n    print(out[-1])\r\n    #print(out[1])\r\n    #print(out[2])\r\n    #print(out[3])\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/863", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/863/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/863/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/863/events", "html_url": "https://github.com/google-research/bert/issues/863", "id": 496733985, "node_id": "MDU6SXNzdWU0OTY3MzM5ODU=", "number": 863, "title": "My classification always get one same result.", "user": {"login": "Kitiro", "id": 23329089, "node_id": "MDQ6VXNlcjIzMzI5MDg5", "avatar_url": "https://avatars0.githubusercontent.com/u/23329089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kitiro", "html_url": "https://github.com/Kitiro", "followers_url": "https://api.github.com/users/Kitiro/followers", "following_url": "https://api.github.com/users/Kitiro/following{/other_user}", "gists_url": "https://api.github.com/users/Kitiro/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kitiro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kitiro/subscriptions", "organizations_url": "https://api.github.com/users/Kitiro/orgs", "repos_url": "https://api.github.com/users/Kitiro/repos", "events_url": "https://api.github.com/users/Kitiro/events{/privacy}", "received_events_url": "https://api.github.com/users/Kitiro/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-22T05:00:32Z", "updated_at": "2019-09-22T10:45:23Z", "closed_at": "2019-09-22T08:03:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I use bert to classify some chinese news. It's a multi-classification and i just make up a new DataProcessor then feed my news and label into it. No matter how many epoch i set or the learning rate is, i still get one same result. It's always the second label gets the most possibilities.\r\nCan you tell me what the possible reason is and how to fix it ?\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/857", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/857/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/857/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/857/events", "html_url": "https://github.com/google-research/bert/issues/857", "id": 493992774, "node_id": "MDU6SXNzdWU0OTM5OTI3NzQ=", "number": 857, "title": "BERT returns different embedding for same sentence", "user": {"login": "rshah1990", "id": 37735152, "node_id": "MDQ6VXNlcjM3NzM1MTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/37735152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rshah1990", "html_url": "https://github.com/rshah1990", "followers_url": "https://api.github.com/users/rshah1990/followers", "following_url": "https://api.github.com/users/rshah1990/following{/other_user}", "gists_url": "https://api.github.com/users/rshah1990/gists{/gist_id}", "starred_url": "https://api.github.com/users/rshah1990/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rshah1990/subscriptions", "organizations_url": "https://api.github.com/users/rshah1990/orgs", "repos_url": "https://api.github.com/users/rshah1990/repos", "events_url": "https://api.github.com/users/rshah1990/events{/privacy}", "received_events_url": "https://api.github.com/users/rshah1990/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-16T11:29:43Z", "updated_at": "2019-09-16T17:12:37Z", "closed_at": "2019-09-16T17:10:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using pre-trained BERT for creating features, for same sentence it produces different result in two different runs. Do we have to set some random state to produce consistent result?  I am using **pytorch-transformers** for reading pre-trained model.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/850", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/850/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/850/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/850/events", "html_url": "https://github.com/google-research/bert/issues/850", "id": 492051538, "node_id": "MDU6SXNzdWU0OTIwNTE1Mzg=", "number": 850, "title": "Using hub.Module() to import downloaded BERT in disk", "user": {"login": "DreamerDeo", "id": 9781506, "node_id": "MDQ6VXNlcjk3ODE1MDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/9781506?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DreamerDeo", "html_url": "https://github.com/DreamerDeo", "followers_url": "https://api.github.com/users/DreamerDeo/followers", "following_url": "https://api.github.com/users/DreamerDeo/following{/other_user}", "gists_url": "https://api.github.com/users/DreamerDeo/gists{/gist_id}", "starred_url": "https://api.github.com/users/DreamerDeo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DreamerDeo/subscriptions", "organizations_url": "https://api.github.com/users/DreamerDeo/orgs", "repos_url": "https://api.github.com/users/DreamerDeo/repos", "events_url": "https://api.github.com/users/DreamerDeo/events{/privacy}", "received_events_url": "https://api.github.com/users/DreamerDeo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-11T06:53:01Z", "updated_at": "2019-09-11T07:21:56Z", "closed_at": "2019-09-11T07:21:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Due to the network condition, `https://tfhub.dev/google/bert_chinese_L-12_H-768_A-12/1` is unreachable even VPN is used. Thus, I have downloaded `bert_chinese_L-12_H-768_A-12` in my disk.  How can I import it using `hub.Module()`?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/849", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/849/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/849/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/849/events", "html_url": "https://github.com/google-research/bert/issues/849", "id": 490870513, "node_id": "MDU6SXNzdWU0OTA4NzA1MTM=", "number": 849, "title": "Problem: Large Document Classification with BERT", "user": {"login": "garyshincc", "id": 15809516, "node_id": "MDQ6VXNlcjE1ODA5NTE2", "avatar_url": "https://avatars0.githubusercontent.com/u/15809516?v=4", "gravatar_id": "", "url": "https://api.github.com/users/garyshincc", "html_url": "https://github.com/garyshincc", "followers_url": "https://api.github.com/users/garyshincc/followers", "following_url": "https://api.github.com/users/garyshincc/following{/other_user}", "gists_url": "https://api.github.com/users/garyshincc/gists{/gist_id}", "starred_url": "https://api.github.com/users/garyshincc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/garyshincc/subscriptions", "organizations_url": "https://api.github.com/users/garyshincc/orgs", "repos_url": "https://api.github.com/users/garyshincc/repos", "events_url": "https://api.github.com/users/garyshincc/events{/privacy}", "received_events_url": "https://api.github.com/users/garyshincc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-09-09T04:31:15Z", "updated_at": "2019-09-12T00:20:55Z", "closed_at": "2019-09-12T00:20:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi all,\r\n\r\nI'm trying to experiment with document classification with a BERT model.\r\nAre there conventional methods to handle documents with say, 1000+ tokens / document and how do they handle memory usage?\r\n\r\nThanks,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/848", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/848/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/848/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/848/events", "html_url": "https://github.com/google-research/bert/issues/848", "id": 490809197, "node_id": "MDU6SXNzdWU0OTA4MDkxOTc=", "number": 848, "title": "How to run bert classifier pb file ", "user": {"login": "anmol4210", "id": 23002352, "node_id": "MDQ6VXNlcjIzMDAyMzUy", "avatar_url": "https://avatars1.githubusercontent.com/u/23002352?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anmol4210", "html_url": "https://github.com/anmol4210", "followers_url": "https://api.github.com/users/anmol4210/followers", "following_url": "https://api.github.com/users/anmol4210/following{/other_user}", "gists_url": "https://api.github.com/users/anmol4210/gists{/gist_id}", "starred_url": "https://api.github.com/users/anmol4210/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anmol4210/subscriptions", "organizations_url": "https://api.github.com/users/anmol4210/orgs", "repos_url": "https://api.github.com/users/anmol4210/repos", "events_url": "https://api.github.com/users/anmol4210/events{/privacy}", "received_events_url": "https://api.github.com/users/anmol4210/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-08T21:34:56Z", "updated_at": "2020-02-17T03:06:18Z", "closed_at": "2019-10-03T09:24:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI am trying to run bert classifier. I have fine tuned the model on my data. I have also converted ckpt files to pb files. Now I want to do predictions using this pb files. Could anyone guide me how can I do it. What changes do I have to make in the code to load pb file instead of the ckpt files. Any help would be great.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/842", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/842/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/842/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/842/events", "html_url": "https://github.com/google-research/bert/issues/842", "id": 490191705, "node_id": "MDU6SXNzdWU0OTAxOTE3MDU=", "number": 842, "title": "How to fine tune bert on small labeled twitter dataset?", "user": {"login": "RoderickGu", "id": 44223191, "node_id": "MDQ6VXNlcjQ0MjIzMTkx", "avatar_url": "https://avatars1.githubusercontent.com/u/44223191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RoderickGu", "html_url": "https://github.com/RoderickGu", "followers_url": "https://api.github.com/users/RoderickGu/followers", "following_url": "https://api.github.com/users/RoderickGu/following{/other_user}", "gists_url": "https://api.github.com/users/RoderickGu/gists{/gist_id}", "starred_url": "https://api.github.com/users/RoderickGu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RoderickGu/subscriptions", "organizations_url": "https://api.github.com/users/RoderickGu/orgs", "repos_url": "https://api.github.com/users/RoderickGu/repos", "events_url": "https://api.github.com/users/RoderickGu/events{/privacy}", "received_events_url": "https://api.github.com/users/RoderickGu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-06T08:13:35Z", "updated_at": "2020-05-16T09:16:12Z", "closed_at": "2020-05-16T09:16:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Most people test bert on large dataset, but when it comes to small dataset, I assume the fine tune process and batch size maybe different. Besides, the dataset domain is twitter domain, which is kind of different from BERT pretrained corpus.\r\nCould anyone gives some suggestions on finetuning BERT on small labeled twitter dataset? Thanks in advance for any help.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/833", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/833/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/833/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/833/events", "html_url": "https://github.com/google-research/bert/issues/833", "id": 487892521, "node_id": "MDU6SXNzdWU0ODc4OTI1MjE=", "number": 833, "title": "Exporting best model", "user": {"login": "smr97", "id": 18290261, "node_id": "MDQ6VXNlcjE4MjkwMjYx", "avatar_url": "https://avatars0.githubusercontent.com/u/18290261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smr97", "html_url": "https://github.com/smr97", "followers_url": "https://api.github.com/users/smr97/followers", "following_url": "https://api.github.com/users/smr97/following{/other_user}", "gists_url": "https://api.github.com/users/smr97/gists{/gist_id}", "starred_url": "https://api.github.com/users/smr97/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smr97/subscriptions", "organizations_url": "https://api.github.com/users/smr97/orgs", "repos_url": "https://api.github.com/users/smr97/repos", "events_url": "https://api.github.com/users/smr97/events{/privacy}", "received_events_url": "https://api.github.com/users/smr97/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-01T14:28:35Z", "updated_at": "2020-05-05T13:19:51Z", "closed_at": "2020-05-05T13:19:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am serving a  BERT model. My constraint is that I want to export the model with the best accuracy on the evaluation dataset. I want to do this automatically (using the BestExporter class from TF). However, the problem is that this repository uses the TPUEstimator API, which does not allow me to pass in this BestExporter object.\r\n\r\nI tried to work around by passing in the estimator object in the BestExporter call to export(), but this somehow does not work. There is no error, but I get no output in the export_dir path that I give to my BestExporter object.\r\n\r\nCan someone please help?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/824", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/824/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/824/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/824/events", "html_url": "https://github.com/google-research/bert/issues/824", "id": 486176758, "node_id": "MDU6SXNzdWU0ODYxNzY3NTg=", "number": 824, "title": "Use cross entropy loss function in the classification task.", "user": {"login": "Realvincentyuan", "id": 26101303, "node_id": "MDQ6VXNlcjI2MTAxMzAz", "avatar_url": "https://avatars1.githubusercontent.com/u/26101303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Realvincentyuan", "html_url": "https://github.com/Realvincentyuan", "followers_url": "https://api.github.com/users/Realvincentyuan/followers", "following_url": "https://api.github.com/users/Realvincentyuan/following{/other_user}", "gists_url": "https://api.github.com/users/Realvincentyuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Realvincentyuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Realvincentyuan/subscriptions", "organizations_url": "https://api.github.com/users/Realvincentyuan/orgs", "repos_url": "https://api.github.com/users/Realvincentyuan/repos", "events_url": "https://api.github.com/users/Realvincentyuan/events{/privacy}", "received_events_url": "https://api.github.com/users/Realvincentyuan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-28T06:04:06Z", "updated_at": "2020-07-20T10:47:26Z", "closed_at": "2020-07-20T10:47:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Background\r\nI am doing a binary classification task with BERT and I noticed that in the run_classifier.py file the loss function was not cross entropy. So I am re-writing the loss function with cross entropy but met minor issues of the dimension.\r\n\r\n## Codes concerning creating model\r\n```\r\ndef custom_create_model(bert_config, is_training, input_ids, input_mask, segment_ids,labels, num_labels, use_one_hot_embeddings):\r\n      model = modeling.BertModel(\r\n      config=bert_config,\r\n      is_training=is_training,\r\n      input_ids=input_ids,\r\n      input_mask=input_mask,\r\n      token_type_ids=segment_ids,\r\n      use_one_hot_embeddings=use_one_hot_embeddings)\r\n     \r\n    output_layer = model.get_pooled_output()\r\n\r\n    hidden_size = output_layer.shape[-1].value\r\n\r\n    output_weights = tf.get_variable(\r\n      \"output_weights\", [num_labels, hidden_size],\r\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n\r\n    output_bias = tf.get_variable(\r\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\r\n\r\n    with tf.variable_scope(\"loss\"):\r\n        if is_training:\r\n              # I.e., 0.1 dropout\r\n            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\r\n\r\n        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\r\n        logits = tf.nn.bias_add(logits, output_bias)\r\n        probabilities = tf.nn.softmax(logits, axis=-1)\r\n\r\n#       Custom cross entropy loss function\r\n        \r\n        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\r\n        \r\n        print('Labels shape', tf.shape(labels))\r\n        print('logits shape', tf.shape(logits))\r\n        \r\n        per_example_loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits)\r\n        loss = tf.reduce_mean(per_example_loss)\r\n        return (loss, per_example_loss, logits, probabilities)\r\n```\r\n\r\n## Questions\r\n- why does BERT not use cross entropy in the classification task?\r\n- I want to penalizIe the false positives and false negatives so I used class weight calculator in Sklearn libraries to calculate the weights and broadcast to the loss\r\n\r\n```\r\nfrom sklearn.utils.class_weight import compute_class_weight\r\nfrom sklearn.utils import class_weight\r\nclass_weights = class_weight.compute_class_weight('balanced',\r\n                                                 np.unique(df['CII'].tolist()),\r\n                                                 df['CII'].tolist())\r\n...# in the create_model func:\r\nper_example_loss = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_labels, logits=logits)\r\nweighted_losses = per_example_loss * weights\r\n    \r\nloss = tf.reduce_mean(weighted_losses)\r\n```\r\n\r\nThe weights become a parameter to be tuned anyway, not sure if anybody has some tactics on the tuning?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/811", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/811/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/811/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/811/events", "html_url": "https://github.com/google-research/bert/issues/811", "id": 481964031, "node_id": "MDU6SXNzdWU0ODE5NjQwMzE=", "number": 811, "title": "Non deterministic results in movie review notebook", "user": {"login": "rotkert", "id": 7586587, "node_id": "MDQ6VXNlcjc1ODY1ODc=", "avatar_url": "https://avatars1.githubusercontent.com/u/7586587?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rotkert", "html_url": "https://github.com/rotkert", "followers_url": "https://api.github.com/users/rotkert/followers", "following_url": "https://api.github.com/users/rotkert/following{/other_user}", "gists_url": "https://api.github.com/users/rotkert/gists{/gist_id}", "starred_url": "https://api.github.com/users/rotkert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rotkert/subscriptions", "organizations_url": "https://api.github.com/users/rotkert/orgs", "repos_url": "https://api.github.com/users/rotkert/repos", "events_url": "https://api.github.com/users/rotkert/events{/privacy}", "received_events_url": "https://api.github.com/users/rotkert/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-18T07:33:54Z", "updated_at": "2020-03-13T06:36:48Z", "closed_at": "2019-08-20T08:44:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I ran notebook predicting_movie_reviews_with_bert_on_tf_hub.ipynb and every time I execute evaluation and predict steps I receive different output. \r\nIs it smth I should expect, cause when I used pytorch implementation, the model was deterministic? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/796", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/796/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/796/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/796/events", "html_url": "https://github.com/google-research/bert/issues/796", "id": 478129161, "node_id": "MDU6SXNzdWU0NzgxMjkxNjE=", "number": 796, "title": "run_classifier.py input_fn required edits for large datasets", "user": {"login": "mukhinv", "id": 46717607, "node_id": "MDQ6VXNlcjQ2NzE3NjA3", "avatar_url": "https://avatars1.githubusercontent.com/u/46717607?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mukhinv", "html_url": "https://github.com/mukhinv", "followers_url": "https://api.github.com/users/mukhinv/followers", "following_url": "https://api.github.com/users/mukhinv/following{/other_user}", "gists_url": "https://api.github.com/users/mukhinv/gists{/gist_id}", "starred_url": "https://api.github.com/users/mukhinv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mukhinv/subscriptions", "organizations_url": "https://api.github.com/users/mukhinv/orgs", "repos_url": "https://api.github.com/users/mukhinv/repos", "events_url": "https://api.github.com/users/mukhinv/events{/privacy}", "received_events_url": "https://api.github.com/users/mukhinv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-07T20:39:48Z", "updated_at": "2019-09-19T19:52:56Z", "closed_at": "2019-09-19T19:52:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to run BERT on a custom large set of data (~1 million rows). I am not running BERT on TPU, but rather a K80. I seem to get the following error after building the estimator object and calling train on it:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/projects/p30412/incidentals-vlad/bert_quest.py\", line 257, in <module>\r\n>     estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n>     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n>     return self._train_model_default(input_fn, hooks, saving_listeners)\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1234, in _train_model_default\r\n>     input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1075, in _get_features_and_labels_from_input_fn\r\n>     self._call_input_fn(input_fn, mode))\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1162, in _call_input_fn\r\n>     return input_fn(**kwargs)\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/bert/run_classifier.py\", line 740, in input_fn\r\n>     dtype=tf.int32),\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 208, in constant\r\n>     value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n>   File \"/projects/p30412/python-venvs/vlad_DL_36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 497, in make_tensor_proto\r\n>     (shape_size, nparray.size))\r\n> ValueError: Too many elements provided. Needed at most 174662400, but received 272910000\r\n\r\n\r\nHere is what estimator object is:\r\n\r\n> run_config = tf.estimator.RunConfig(\r\n>     model_dir=OUTPUT_DIR,\r\n>     save_summary_steps=SAVE_SUMMARY_STEPS,\r\n>     save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\r\n> \r\n> model_fn = model_fn_builder(\r\n>   num_labels=len(label_list),\r\n>   learning_rate=LEARNING_RATE,\r\n>   num_train_steps=num_train_steps,\r\n>   num_warmup_steps=num_warmup_steps)\r\n> \r\n> estimator = tf.estimator.Estimator(\r\n>   model_fn=model_fn,\r\n>   config=run_config,\r\n>   params={\"batch_size\": BATCH_SIZE})\r\n\r\ntrain_input_fn is the input_fn_builder output from the run_classifier.py in this repo.\r\n\r\nAfter digging through the source code to find the issue, I have found the following in the run_classifier.py input_fn (lines 733-755):\r\n\r\n`    # This is for demo purposes and does NOT scale to large data sets. We do\r\n    # not use Dataset.from_generator() because that uses tf.py_func which is\r\n    # not TPU compatible. The right way to load data is with TFRecordReader.`\r\n\r\nI assume the error is exactly because tf.data.Dataset.from_tensor_slices() is used in the input_fn(). Could you please provide instructions on what should I change in the source code of input_fn for it to work with an output of input_fn_builder so that it scales to large datasets? I tried searching for this but was not able to find an answer to this. \r\n\r\nThanks!\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/789", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/789/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/789/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/789/events", "html_url": "https://github.com/google-research/bert/issues/789", "id": 473822730, "node_id": "MDU6SXNzdWU0NzM4MjI3MzA=", "number": 789, "title": "Null Return in model_predict function when BERT predicts on a single record", "user": {"login": "Realvincentyuan", "id": 26101303, "node_id": "MDQ6VXNlcjI2MTAxMzAz", "avatar_url": "https://avatars1.githubusercontent.com/u/26101303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Realvincentyuan", "html_url": "https://github.com/Realvincentyuan", "followers_url": "https://api.github.com/users/Realvincentyuan/followers", "following_url": "https://api.github.com/users/Realvincentyuan/following{/other_user}", "gists_url": "https://api.github.com/users/Realvincentyuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Realvincentyuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Realvincentyuan/subscriptions", "organizations_url": "https://api.github.com/users/Realvincentyuan/orgs", "repos_url": "https://api.github.com/users/Realvincentyuan/repos", "events_url": "https://api.github.com/users/Realvincentyuan/events{/privacy}", "received_events_url": "https://api.github.com/users/Realvincentyuan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-29T02:38:59Z", "updated_at": "2019-07-29T07:05:10Z", "closed_at": "2019-07-29T07:05:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Function definition is pretty much like that in BERT End to End (Fine-tuning + Predicting) with Cloud TPU: Sentence and Sentence-Pair Classification Tasks\r\n\r\n![image](https://user-images.githubusercontent.com/26101303/62018167-29036f80-b1ec-11e9-8492-3459e36d3b2e.png)\r\n\r\nBased on this some returned values were added for further analysis. I actually want to implement a local model explainer on a single record basis. But the returned value was always null when running on a single record. Also no predictions is seen in the log, do not have a clue.\r\n\r\nRunning clause was like \r\n`prediction_probability_2 = model_predict(estimator_from_checkpoints,[predict_InputExamples[1503]])`\r\n\r\nCan anybody help? Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/765", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/765/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/765/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/765/events", "html_url": "https://github.com/google-research/bert/issues/765", "id": 468537767, "node_id": "MDU6SXNzdWU0Njg1Mzc3Njc=", "number": 765, "title": "Negative Probabilities for sentiment classification", "user": {"login": "grecosalvatore", "id": 45233784, "node_id": "MDQ6VXNlcjQ1MjMzNzg0", "avatar_url": "https://avatars1.githubusercontent.com/u/45233784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/grecosalvatore", "html_url": "https://github.com/grecosalvatore", "followers_url": "https://api.github.com/users/grecosalvatore/followers", "following_url": "https://api.github.com/users/grecosalvatore/following{/other_user}", "gists_url": "https://api.github.com/users/grecosalvatore/gists{/gist_id}", "starred_url": "https://api.github.com/users/grecosalvatore/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/grecosalvatore/subscriptions", "organizations_url": "https://api.github.com/users/grecosalvatore/orgs", "repos_url": "https://api.github.com/users/grecosalvatore/repos", "events_url": "https://api.github.com/users/grecosalvatore/events{/privacy}", "received_events_url": "https://api.github.com/users/grecosalvatore/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-16T09:11:25Z", "updated_at": "2019-07-26T09:30:11Z", "closed_at": "2019-07-26T09:30:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to do sentiment classification on IMDB dataset using the [official notebook](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=j0a4mTk9o1Qg) .\r\nWhy probabilities for both labels are negative? The two probabilities don't need to be [0,1] and the sum =1 ?\r\nthe last layer added is a softmax.\r\nthanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/760", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/760/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/760/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/760/events", "html_url": "https://github.com/google-research/bert/issues/760", "id": 467574362, "node_id": "MDU6SXNzdWU0Njc1NzQzNjI=", "number": 760, "title": "Docs: Very wrong assertion that Wikipedia size correlates with number of speakers", "user": {"login": "brendano", "id": 1677, "node_id": "MDQ6VXNlcjE2Nzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brendano", "html_url": "https://github.com/brendano", "followers_url": "https://api.github.com/users/brendano/followers", "following_url": "https://api.github.com/users/brendano/following{/other_user}", "gists_url": "https://api.github.com/users/brendano/gists{/gist_id}", "starred_url": "https://api.github.com/users/brendano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brendano/subscriptions", "organizations_url": "https://api.github.com/users/brendano/orgs", "repos_url": "https://api.github.com/users/brendano/repos", "events_url": "https://api.github.com/users/brendano/events{/privacy}", "received_events_url": "https://api.github.com/users/brendano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "slavpetrov", "id": 1657200, "node_id": "MDQ6VXNlcjE2NTcyMDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1657200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slavpetrov", "html_url": "https://github.com/slavpetrov", "followers_url": "https://api.github.com/users/slavpetrov/followers", "following_url": "https://api.github.com/users/slavpetrov/following{/other_user}", "gists_url": "https://api.github.com/users/slavpetrov/gists{/gist_id}", "starred_url": "https://api.github.com/users/slavpetrov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slavpetrov/subscriptions", "organizations_url": "https://api.github.com/users/slavpetrov/orgs", "repos_url": "https://api.github.com/users/slavpetrov/repos", "events_url": "https://api.github.com/users/slavpetrov/events{/privacy}", "received_events_url": "https://api.github.com/users/slavpetrov/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "slavpetrov", "id": 1657200, "node_id": "MDQ6VXNlcjE2NTcyMDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1657200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/slavpetrov", "html_url": "https://github.com/slavpetrov", "followers_url": "https://api.github.com/users/slavpetrov/followers", "following_url": "https://api.github.com/users/slavpetrov/following{/other_user}", "gists_url": "https://api.github.com/users/slavpetrov/gists{/gist_id}", "starred_url": "https://api.github.com/users/slavpetrov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/slavpetrov/subscriptions", "organizations_url": "https://api.github.com/users/slavpetrov/orgs", "repos_url": "https://api.github.com/users/slavpetrov/repos", "events_url": "https://api.github.com/users/slavpetrov/events{/privacy}", "received_events_url": "https://api.github.com/users/slavpetrov/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2019-07-12T19:49:04Z", "updated_at": "2019-07-16T01:40:26Z", "closed_at": "2019-07-16T01:40:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently `bert/multilingual.md` states:\r\n\r\n\"... the size of the Wikipedia for a given language varies greatly, and therefore low-resource languages may be \"under-represented\" in terms of the neural network model ...\r\nHowever, the size of a Wikipedia also correlates with the number of speakers of a language\"\r\n\r\nThis claim is wrong. For example, Hindi and English have roughly the same number of native speakers (300-400M?), but Hindi Wikipedia is 40x smaller than English Wikipedia.  Even if there is a correlation (do the BERT authors have any evidence of this?), it's a weak enough correlation to miss out on a resource issue for one of the most widely spoken languages in the world.\r\n\r\nThe size of a language's Wikipedia may be dependent on many other factors, including historical sociopolitical and institutional power, history of colonization and imperialism, etc. It's naive and anti-egalitarian to assume  technical/linguistic resources already exist in proportion to social good.\r\n\r\nThis assertion is additionally disappointing since it's used to excuse possible weaknesses in the model for low-resource languages, implying an assumption that it's OK to screw over speakers of rare languages. But even if you think so, the uneven distribution of Wikipedia sizes can cause a model to fail to serve massive portions of humanity.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/759", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/759/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/759/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/759/events", "html_url": "https://github.com/google-research/bert/issues/759", "id": 467550873, "node_id": "MDU6SXNzdWU0Njc1NTA4NzM=", "number": 759, "title": "Exporting probabilities over the learned vocabulary", "user": {"login": "lioutasb", "id": 9558061, "node_id": "MDQ6VXNlcjk1NTgwNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/9558061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lioutasb", "html_url": "https://github.com/lioutasb", "followers_url": "https://api.github.com/users/lioutasb/followers", "following_url": "https://api.github.com/users/lioutasb/following{/other_user}", "gists_url": "https://api.github.com/users/lioutasb/gists{/gist_id}", "starred_url": "https://api.github.com/users/lioutasb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lioutasb/subscriptions", "organizations_url": "https://api.github.com/users/lioutasb/orgs", "repos_url": "https://api.github.com/users/lioutasb/repos", "events_url": "https://api.github.com/users/lioutasb/events{/privacy}", "received_events_url": "https://api.github.com/users/lioutasb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-12T18:44:11Z", "updated_at": "2019-07-16T15:32:33Z", "closed_at": "2019-07-16T15:32:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently, the `extract_features.py` file supports extracting representations before the last output layer but I want to extract the final probabilities over the vocabulary. I modified the code with the following addition but it doesn't seem to work (the probabilities I'm getting are all very small).\r\n\r\n```\r\nmodel_output = model.get_sequence_output()\r\n\r\nbatch_size = tf.shape(model_output)[0]\r\nlength = tf.shape(model_output)[1]\r\nhidden_size = tf.shape(model_output)[2]\r\n\r\nmodel_output = tf.reshape(inputs, [-1, hidden_size])\r\nlogits = tf.matmul(model_output, model.get_embedding_table(), transpose_b=True)\r\nprobs = tf.nn.softmax(logits, axis=-1)\r\nprobs = tf.reshape(probs, [batch_size, length, bert_config.vocab_size])\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/754", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/754/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/754/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/754/events", "html_url": "https://github.com/google-research/bert/issues/754", "id": 465848374, "node_id": "MDU6SXNzdWU0NjU4NDgzNzQ=", "number": 754, "title": "GPU usage for prediction tasks", "user": {"login": "mfeblowitz", "id": 6854939, "node_id": "MDQ6VXNlcjY4NTQ5Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/6854939?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfeblowitz", "html_url": "https://github.com/mfeblowitz", "followers_url": "https://api.github.com/users/mfeblowitz/followers", "following_url": "https://api.github.com/users/mfeblowitz/following{/other_user}", "gists_url": "https://api.github.com/users/mfeblowitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfeblowitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfeblowitz/subscriptions", "organizations_url": "https://api.github.com/users/mfeblowitz/orgs", "repos_url": "https://api.github.com/users/mfeblowitz/repos", "events_url": "https://api.github.com/users/mfeblowitz/events{/privacy}", "received_events_url": "https://api.github.com/users/mfeblowitz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-09T15:29:48Z", "updated_at": "2019-08-22T14:58:24Z", "closed_at": "2019-08-22T14:58:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Follow-on from #667.\r\n\r\nOn 2-gpu hosts, BERT with tensorflow takes control of both GPUs but makes use of only one. \r\n\r\nIt is not clear how best to pass along instructions to tensorflow to select one specific gpu and also to split the memory equally for each gpu (e.g., to run two separate predict tasks on the same host, one on each gpu).\r\n\r\nI have looked at two horovod treatments and they appear not to improve use of gpus for predict tasks. They do may use of both gpus, but perform the exact same task in each gpu.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/731", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/731/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/731/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/731/events", "html_url": "https://github.com/google-research/bert/issues/731", "id": 462003353, "node_id": "MDU6SXNzdWU0NjIwMDMzNTM=", "number": 731, "title": "How to GIT pull latest BERT code", "user": {"login": "JimAva", "id": 39627219, "node_id": "MDQ6VXNlcjM5NjI3MjE5", "avatar_url": "https://avatars0.githubusercontent.com/u/39627219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JimAva", "html_url": "https://github.com/JimAva", "followers_url": "https://api.github.com/users/JimAva/followers", "following_url": "https://api.github.com/users/JimAva/following{/other_user}", "gists_url": "https://api.github.com/users/JimAva/gists{/gist_id}", "starred_url": "https://api.github.com/users/JimAva/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JimAva/subscriptions", "organizations_url": "https://api.github.com/users/JimAva/orgs", "repos_url": "https://api.github.com/users/JimAva/repos", "events_url": "https://api.github.com/users/JimAva/events{/privacy}", "received_events_url": "https://api.github.com/users/JimAva/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-28T12:07:54Z", "updated_at": "2019-06-30T18:32:49Z", "closed_at": "2019-06-30T18:32:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "What is the correct commands for Git pull the latest BERT code and using it?  I have run into an issue with do_predict that I've seen others suggesting pulling the latest code base to potentially resolve such issues.\r\n\r\nI'm using Colab and have the Git pull command figured out but not sure what I should do next.\r\n\r\nhttps://github.com/google-research/bert.git", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/726", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/726/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/726/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/726/events", "html_url": "https://github.com/google-research/bert/issues/726", "id": 461500518, "node_id": "MDU6SXNzdWU0NjE1MDA1MTg=", "number": 726, "title": "Cannot use trained BERT model from a trained checkpoint", "user": {"login": "JeevaTM", "id": 46955031, "node_id": "MDQ6VXNlcjQ2OTU1MDMx", "avatar_url": "https://avatars3.githubusercontent.com/u/46955031?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JeevaTM", "html_url": "https://github.com/JeevaTM", "followers_url": "https://api.github.com/users/JeevaTM/followers", "following_url": "https://api.github.com/users/JeevaTM/following{/other_user}", "gists_url": "https://api.github.com/users/JeevaTM/gists{/gist_id}", "starred_url": "https://api.github.com/users/JeevaTM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JeevaTM/subscriptions", "organizations_url": "https://api.github.com/users/JeevaTM/orgs", "repos_url": "https://api.github.com/users/JeevaTM/repos", "events_url": "https://api.github.com/users/JeevaTM/events{/privacy}", "received_events_url": "https://api.github.com/users/JeevaTM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-06-27T12:29:11Z", "updated_at": "2019-07-10T04:08:50Z", "closed_at": "2019-07-08T11:15:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "I trained the BERT and got the model.ckpt.data, model.ckpt.meta. model.ckpt.index in the output directory along with predictions.json, etc.\r\n```\r\npython run_squad.py \\\r\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\r\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\r\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\r\n  --do_train=True \\\r\n  --train_file=$SQUAD_DIR/train-v2.0.json \\\r\n  --do_predict=True \\\r\n  --predict_file=$SQUAD_DIR/dev-v2.0.json \\\r\n  --train_batch_size=24 \\\r\n  --learning_rate=3e-5 \\\r\n  --num_train_epochs=2.0 \\\r\n  --max_seq_length=384 \\\r\n  --doc_stride=128 \\\r\n  --output_dir=gs://some_bucket/squad_large/ \\\r\n  --use_tpu=True \\\r\n  --tpu_name=$TPU_NAME \\\r\n  --version_2_with_negative=True\r\n```\r\nI tried to copy the model.ckpt.meta, model.ckpt.index, model.ckpt.data to the BERT directory and changed the run_squad.py flags as follows to only predict the answer and not train using a dataset:\r\n```\r\npython run_squad.py \\\r\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\r\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\r\n  --init_checkpoint=$BERT_LARGE_DIR/model.ckpt \\\r\n  --do_train=False \\\r\n  --train_file=$SQUAD_DIR/train-v2.0.json \\\r\n  --do_predict=True \\\r\n  --predict_file=$SQUAD_DIR/dev-v2.0.json \\\r\n  --train_batch_size=24 \\\r\n  --learning_rate=3e-5 \\\r\n  --num_train_epochs=2.0 \\\r\n  --max_seq_length=384 \\\r\n  --doc_stride=128 \\\r\n  --output_dir=gs://some_bucket/squad_large/ \\\r\n  --use_tpu=True \\\r\n  --tpu_name=$TPU_NAME \\\r\n  --version_2_with_negative=True\r\n```\r\nIt throws *bucket directory*/model.ckpt does not exist error.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/724", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/724/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/724/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/724/events", "html_url": "https://github.com/google-research/bert/issues/724", "id": 461353692, "node_id": "MDU6SXNzdWU0NjEzNTM2OTI=", "number": 724, "title": "Error when running run_classifier.py ", "user": {"login": "zyupup", "id": 28803680, "node_id": "MDQ6VXNlcjI4ODAzNjgw", "avatar_url": "https://avatars2.githubusercontent.com/u/28803680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zyupup", "html_url": "https://github.com/zyupup", "followers_url": "https://api.github.com/users/zyupup/followers", "following_url": "https://api.github.com/users/zyupup/following{/other_user}", "gists_url": "https://api.github.com/users/zyupup/gists{/gist_id}", "starred_url": "https://api.github.com/users/zyupup/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zyupup/subscriptions", "organizations_url": "https://api.github.com/users/zyupup/orgs", "repos_url": "https://api.github.com/users/zyupup/repos", "events_url": "https://api.github.com/users/zyupup/events{/privacy}", "received_events_url": "https://api.github.com/users/zyupup/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-27T07:03:28Z", "updated_at": "2019-06-27T07:23:23Z", "closed_at": "2019-06-27T07:23:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "![image](https://user-images.githubusercontent.com/28803680/60244282-a6d21380-98ec-11e9-9a8d-6503e77d88d8.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/719", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/719/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/719/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/719/events", "html_url": "https://github.com/google-research/bert/issues/719", "id": 461091640, "node_id": "MDU6SXNzdWU0NjEwOTE2NDA=", "number": 719, "title": "ValueError: Tensor not found in checkpoint", "user": {"login": "julia320", "id": 31576485, "node_id": "MDQ6VXNlcjMxNTc2NDg1", "avatar_url": "https://avatars0.githubusercontent.com/u/31576485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/julia320", "html_url": "https://github.com/julia320", "followers_url": "https://api.github.com/users/julia320/followers", "following_url": "https://api.github.com/users/julia320/following{/other_user}", "gists_url": "https://api.github.com/users/julia320/gists{/gist_id}", "starred_url": "https://api.github.com/users/julia320/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/julia320/subscriptions", "organizations_url": "https://api.github.com/users/julia320/orgs", "repos_url": "https://api.github.com/users/julia320/repos", "events_url": "https://api.github.com/users/julia320/events{/privacy}", "received_events_url": "https://api.github.com/users/julia320/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-26T17:22:20Z", "updated_at": "2020-04-26T07:22:29Z", "closed_at": "2019-06-28T14:35:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am trying to add new layers to BERT and freeze all other layers to only train my layers on top of the checkpoint, however I am running into problems with adding the layers. As far as I can tell, the model gets built correctly, but when I try to run run_pretraining.py I get the following error: \r\n![image](https://user-images.githubusercontent.com/31576485/60200356-f395eb80-9813-11e9-9a45-60964942511b.png)\r\n![image](https://user-images.githubusercontent.com/31576485/60200403-0c060600-9814-11e9-9b41-9cdc3143cea7.png)\r\n![image](https://user-images.githubusercontent.com/31576485/60200447-22ac5d00-9814-11e9-95ca-760574b99c2b.png)\r\n\r\nThe problem is that it doesn't know what to do with the adapter layers since they aren't found in the checkpoint file - how can I work around this or get BERT to recognize that I want to add them in?\r\n\r\nFor reference, this is how I am running the script (I've modified it slightly to include adding my new layers as a flag):\r\n```bash\r\npython run_pretraining.py \\  \r\n--adapter=True \\  \r\n--input_file=/path/to/tfrecord/pretrained_iob2.tfrecord \\  \r\n--output_dir=/usr/bert/adapter \\  \r\n--do_train=True \\  \r\n--do_eval=True \\  \r\n--bert_config_file=/path/to/bert/multi_cased_L-12_H-768_A-12/bert_config.json \\  \r\n--init_checkpoint=/path/to/bert/multi_cased_L-12_H-768_A-12/bert_model.ckpt \\  \r\n--train_batch_size=16         \r\n```                                                                                      \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/716", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/716/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/716/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/716/events", "html_url": "https://github.com/google-research/bert/issues/716", "id": 460181205, "node_id": "MDU6SXNzdWU0NjAxODEyMDU=", "number": 716, "title": "How to run prediction on text classification task on GPU", "user": {"login": "Biaocsu", "id": 45280752, "node_id": "MDQ6VXNlcjQ1MjgwNzUy", "avatar_url": "https://avatars1.githubusercontent.com/u/45280752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Biaocsu", "html_url": "https://github.com/Biaocsu", "followers_url": "https://api.github.com/users/Biaocsu/followers", "following_url": "https://api.github.com/users/Biaocsu/following{/other_user}", "gists_url": "https://api.github.com/users/Biaocsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Biaocsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Biaocsu/subscriptions", "organizations_url": "https://api.github.com/users/Biaocsu/orgs", "repos_url": "https://api.github.com/users/Biaocsu/repos", "events_url": "https://api.github.com/users/Biaocsu/events{/privacy}", "received_events_url": "https://api.github.com/users/Biaocsu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-25T02:46:52Z", "updated_at": "2019-07-10T01:09:05Z", "closed_at": "2019-07-10T01:09:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "I used the fine-tuned model to predict txt, but I seems like to run on CPU, for it takes 5s on each txt(which have nearly 2000 words). and I see log like this below, is there something wrong I do.\r\n\r\nInstructions for updating:\r\nUse keras.layers.dense instead.\r\n2019-06-25 10:27:33.731101: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-25 10:27:33.866396: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5626e7cf6f70 executing computations on platform CUDA. Devices:\r\n2019-06-25 10:27:33.866448: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Quadro P5000, Compute Capability 6.1\r\n2019-06-25 10:27:33.870521: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2793620000 Hz\r\n2019-06-25 10:27:33.871123: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5626e7d600f0 executing computations on platform Host. Devices:\r\n2019-06-25 10:27:33.871162: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-25 10:27:33.871906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: Quadro P5000 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 15.90GiB freeMemory: 15.78GiB\r\n2019-06-25 10:27:33.871942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-06-25 10:27:33.873527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-25 10:27:33.873560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-06-25 10:27:33.873572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-06-25 10:27:33.874258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15349 MB memory) -> physical GPU (device: 0, name: Quadro P5000, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2019-06-25 10:27:35.508488: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/707", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/707/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/707/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/707/events", "html_url": "https://github.com/google-research/bert/issues/707", "id": 457965494, "node_id": "MDU6SXNzdWU0NTc5NjU0OTQ=", "number": 707, "title": "Minor clarifications", "user": {"login": "RuiPChaves", "id": 33401801, "node_id": "MDQ6VXNlcjMzNDAxODAx", "avatar_url": "https://avatars2.githubusercontent.com/u/33401801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RuiPChaves", "html_url": "https://github.com/RuiPChaves", "followers_url": "https://api.github.com/users/RuiPChaves/followers", "following_url": "https://api.github.com/users/RuiPChaves/following{/other_user}", "gists_url": "https://api.github.com/users/RuiPChaves/gists{/gist_id}", "starred_url": "https://api.github.com/users/RuiPChaves/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RuiPChaves/subscriptions", "organizations_url": "https://api.github.com/users/RuiPChaves/orgs", "repos_url": "https://api.github.com/users/RuiPChaves/repos", "events_url": "https://api.github.com/users/RuiPChaves/events{/privacy}", "received_events_url": "https://api.github.com/users/RuiPChaves/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-06-19T11:46:16Z", "updated_at": "2019-06-20T12:10:08Z", "closed_at": "2019-06-20T12:10:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/706", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/706/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/706/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/706/events", "html_url": "https://github.com/google-research/bert/issues/706", "id": 457825009, "node_id": "MDU6SXNzdWU0NTc4MjUwMDk=", "number": 706, "title": "Couldn't train BERT with SQUAD 1.1", "user": {"login": "JeevaTM", "id": 46955031, "node_id": "MDQ6VXNlcjQ2OTU1MDMx", "avatar_url": "https://avatars3.githubusercontent.com/u/46955031?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JeevaTM", "html_url": "https://github.com/JeevaTM", "followers_url": "https://api.github.com/users/JeevaTM/followers", "following_url": "https://api.github.com/users/JeevaTM/following{/other_user}", "gists_url": "https://api.github.com/users/JeevaTM/gists{/gist_id}", "starred_url": "https://api.github.com/users/JeevaTM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JeevaTM/subscriptions", "organizations_url": "https://api.github.com/users/JeevaTM/orgs", "repos_url": "https://api.github.com/users/JeevaTM/repos", "events_url": "https://api.github.com/users/JeevaTM/events{/privacy}", "received_events_url": "https://api.github.com/users/JeevaTM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-19T06:13:50Z", "updated_at": "2019-06-20T04:27:16Z", "closed_at": "2019-06-20T04:27:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "I created VM (n1-standard-2) and Cloud TPU (v3-8) using ctpu tool.\r\n\r\nI have created a Storage bucket and mounted it in VM using GCSfuse.\r\n\r\nTried to run it. Failed.\r\n\r\n```\r\npython run_squad.py \\\r\n> --vocab_file=$BERT_BASE_DIR/vocab.txt \\\r\n> --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\r\n> --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\r\n> --do_train=True \\\r\n> --train_file=$SQUAD_DIR/train-v1.1.json \\\r\n> --do_predict=True \\\r\n> --predict_file=$SQUAD_DIR/dev-v1.1.json \\\r\n> --train_batch_size=12 \\\r\n> --learning_rate=3e-5 \\\r\n> --num_train_epochs=2.0 \\\r\n> --max_seq_length=384 \\\r\n> --doc_stride=128 \\\r\n> output_dir=$output\r\nTraceback (most recent call last):\r\n  File \"run_squad.py\", line 1283, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", l\r\nine 119, in run\r\n    argv = flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/flags.py\",\r\n line 112, in __call__\r\n    return self.__dict__['__wrapped'].__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/flags/_flagvalues.py\", line 636,\r\n in __call__\r\n    self._assert_all_validators()\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/flags/_flagvalues.py\", line 510,\r\n in _assert_all_validators\r\n    self._assert_validators(all_validators)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/flags/_flagvalues.py\", line 531,\r\n in _assert_validators\r\n    raise _exceptions.IllegalFlagValueError('%s: %s' % (message, str(e)))\r\nabsl.flags._exceptions.IllegalFlagValueError: flag --output_dir=None: Flag --output_\r\ndir must have a value other than None.\r\n```\r\n\r\n`$output` is `/output: Is a directory`.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/705", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/705/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/705/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/705/events", "html_url": "https://github.com/google-research/bert/issues/705", "id": 457741148, "node_id": "MDU6SXNzdWU0NTc3NDExNDg=", "number": 705, "title": "Why reshape here?", "user": {"login": "eduOS", "id": 8926535, "node_id": "MDQ6VXNlcjg5MjY1MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8926535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eduOS", "html_url": "https://github.com/eduOS", "followers_url": "https://api.github.com/users/eduOS/followers", "following_url": "https://api.github.com/users/eduOS/following{/other_user}", "gists_url": "https://api.github.com/users/eduOS/gists{/gist_id}", "starred_url": "https://api.github.com/users/eduOS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eduOS/subscriptions", "organizations_url": "https://api.github.com/users/eduOS/orgs", "repos_url": "https://api.github.com/users/eduOS/repos", "events_url": "https://api.github.com/users/eduOS/events{/privacy}", "received_events_url": "https://api.github.com/users/eduOS/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-06-18T23:35:20Z", "updated_at": "2019-07-03T15:12:21Z", "closed_at": "2019-07-03T15:12:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Nevermind. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/702", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/702/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/702/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/702/events", "html_url": "https://github.com/google-research/bert/issues/702", "id": 457416963, "node_id": "MDU6SXNzdWU0NTc0MTY5NjM=", "number": 702, "title": "ValueError: For training, each question should have exactly 1 answer.", "user": {"login": "JeevaTM", "id": 46955031, "node_id": "MDQ6VXNlcjQ2OTU1MDMx", "avatar_url": "https://avatars3.githubusercontent.com/u/46955031?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JeevaTM", "html_url": "https://github.com/JeevaTM", "followers_url": "https://api.github.com/users/JeevaTM/followers", "following_url": "https://api.github.com/users/JeevaTM/following{/other_user}", "gists_url": "https://api.github.com/users/JeevaTM/gists{/gist_id}", "starred_url": "https://api.github.com/users/JeevaTM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JeevaTM/subscriptions", "organizations_url": "https://api.github.com/users/JeevaTM/orgs", "repos_url": "https://api.github.com/users/JeevaTM/repos", "events_url": "https://api.github.com/users/JeevaTM/events{/privacy}", "received_events_url": "https://api.github.com/users/JeevaTM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-18T11:29:20Z", "updated_at": "2019-07-10T04:47:12Z", "closed_at": "2019-06-20T04:25:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried\r\n\r\n> Try with argument \"--version_2_with_negative True\"\r\n\r\nI tried this argument. I still have the issue. \r\n\r\n```\r\npython run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LAR\r\nGE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v2.0.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v2.0.json --train_batch_size=24 --learning_rate=\r\n3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://example/neural-output --use_tpu=True \\ --tpu_name=$TPU_NAME \\ --version_2_with_neg\r\native True\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\nTraceback (most recent call last):\r\n  File \"run_squad.py\", line 1283, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_squad.py\", line 1159, in main\r\n    input_file=FLAGS.train_file, is_training=True)\r\n  File \"run_squad.py\", line 268, in read_squad_examples\r\n    \"For training, each question should have exactly 1 answer.\")\r\nValueError: For training, each question should have exactly 1 answer.\r\n```\r\n\r\nand\r\n\r\n```\r\npython run_squad.py --vocab_file=$BERT_LARGE_DIR/vocab.txt --bert_config_file=$BERT_LARGE_DIR/bert_config.json --init_checkpoint=$BERT_LAR\r\nGE_DIR/bert_model.ckpt --do_train=True --train_file=$SQUAD_DIR/train-v2.0.json --do_predict=True --predict_file=$SQUAD_DIR/dev-v2.0.json --train_batch_size=24 --learning_rate=\r\n3e-5 --num_train_epochs=2.0 --max_seq_length=384 --doc_stride=128 --output_dir=gs://example/neural-output --use_tpu=True \\ --tpu_name=$TPU_NAME \\ --version_2_with_neg\r\native=True\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\nTraceback (most recent call last):\r\n  File \"run_squad.py\", line 1283, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_squad.py\", line 1159, in main\r\n    input_file=FLAGS.train_file, is_training=True)\r\n  File \"run_squad.py\", line 268, in read_squad_examples\r\n    \"For training, each question should have exactly 1 answer.\")\r\nValueError: For training, each question should have exactly 1 answer.\r\n```\r\n\r\n_Originally posted by @JeevaTM in https://github.com/google-research/bert/issues/392#issuecomment-503063565_", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/701", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/701/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/701/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/701/events", "html_url": "https://github.com/google-research/bert/issues/701", "id": 456893168, "node_id": "MDU6SXNzdWU0NTY4OTMxNjg=", "number": 701, "title": "$BERT_DIR\\bert_config.json could not be found", "user": {"login": "JeevaTM", "id": 46955031, "node_id": "MDQ6VXNlcjQ2OTU1MDMx", "avatar_url": "https://avatars3.githubusercontent.com/u/46955031?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JeevaTM", "html_url": "https://github.com/JeevaTM", "followers_url": "https://api.github.com/users/JeevaTM/followers", "following_url": "https://api.github.com/users/JeevaTM/following{/other_user}", "gists_url": "https://api.github.com/users/JeevaTM/gists{/gist_id}", "starred_url": "https://api.github.com/users/JeevaTM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JeevaTM/subscriptions", "organizations_url": "https://api.github.com/users/JeevaTM/orgs", "repos_url": "https://api.github.com/users/JeevaTM/repos", "events_url": "https://api.github.com/users/JeevaTM/events{/privacy}", "received_events_url": "https://api.github.com/users/JeevaTM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-17T11:53:36Z", "updated_at": "2020-01-24T12:21:12Z", "closed_at": "2019-06-20T04:28:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "OS: Windows 2012 R2\r\nPython: 3.7.3\r\n\r\nI tried to run run_squad.py using Cloud TPU. I have all the files in compute engine - Windows VM (SQUAD 2.0, Bert-master, BERT_Large_cased).\r\n\r\nNote: I tried to set the environment variables for SQUAD and BERT_large_cased and the files where in Google storage bucket. Example: `set $SQUAD_DIR=gs://test/squad_dir`, it  throws an error saying cannot access file. All though I gave the VM service account both view and write object role for the bucket.\r\n\r\nI set the environment variables for squad directory and bert_large_cased and tried to run using the code provided in the home page of bert in github.\r\n\r\nHere's the code:\r\n```\r\nC:\\Users\\Desktop\\bert-master>python run_squad.py \\ --vocab_file=$BERT\r\n_LARGE_DIR/vocab.txt \\ --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\ ini\r\nt_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\ --do_train=True \\ --train_file=$S\r\nQUAD_DIR/train-v2.0.json \\ --do_predict=True \\ --predict_file=$SQUAD_DIR/dev-v2.\r\n0.json \\ --train_batch_size=24 \\ --learning_rate=3e-5 \\ --num_train_epochs=2.0 \\\r\n --max_seq_length=384 \\ --doc_stride=128 \\ --output_dir=gs://uniquetestbucket/sq\r\nuad_large/ \\ --use_tpu=True \\ --tpu_name=$TPU_NAME \\ --version_2_with_negative=T\r\nrue\r\nTraceback (most recent call last):\r\n  File \"run_squad.py\", line 1283, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pack\r\nages\\tensorflow\\python\\platform\\app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_squad.py\", line 1129, in main\r\n    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\r\n  File \"C:\\Users\\Desktop\\bert-master\\modeling.py\", line 94, in from_j\r\nson_file\r\n    text = reader.read()\r\n  File \"C:\\Users\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pack\r\nages\\tensorflow\\python\\lib\\io\\file_io.py\", line 125, in read\r\n    self._preread_check()\r\n  File \"C:\\Users\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pack\r\nages\\tensorflow\\python\\lib\\io\\file_io.py\", line 85, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"C:\\Users\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pack\r\nages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile faile\r\nd to Create/Open: $BERT_LARGE_DIR/bert_config.json : The system cannot find the\r\npath specified.\r\n; No such process\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/696", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/696/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/696/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/696/events", "html_url": "https://github.com/google-research/bert/issues/696", "id": 455642218, "node_id": "MDU6SXNzdWU0NTU2NDIyMTg=", "number": 696, "title": "End to End (Fine-tuning + Predicting) with Cloud TPU has issu", "user": {"login": "foye501", "id": 36092981, "node_id": "MDQ6VXNlcjM2MDkyOTgx", "avatar_url": "https://avatars3.githubusercontent.com/u/36092981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/foye501", "html_url": "https://github.com/foye501", "followers_url": "https://api.github.com/users/foye501/followers", "following_url": "https://api.github.com/users/foye501/following{/other_user}", "gists_url": "https://api.github.com/users/foye501/gists{/gist_id}", "starred_url": "https://api.github.com/users/foye501/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/foye501/subscriptions", "organizations_url": "https://api.github.com/users/foye501/orgs", "repos_url": "https://api.github.com/users/foye501/repos", "events_url": "https://api.github.com/users/foye501/events{/privacy}", "received_events_url": "https://api.github.com/users/foye501/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-13T09:47:46Z", "updated_at": "2020-06-30T10:36:34Z", "closed_at": "2020-06-30T10:36:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "InvalidArgumentError: Error executing an HTTP request: HTTP response code 400 with body '{\r\n \"error\": {\r\n  \"errors\": [\r\n   {\r\n    \"domain\": \"global\",\r\n    \"reason\": \"invalid\",\r\n    \"message\": \"Invalid bucket name: 'YOUR_BUCKET'\"\r\n   }\r\n  ],\r\n  \"code\": 400,\r\n  \"message\": \"Invalid bucket name: 'YOUR_BUCKET'\"\r\n }\r\n}\r\n'\r\n\t when reading metadata of gs://YOUR_BUCKET/bert-tfhub/models/MRPC", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/695", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/695/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/695/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/695/events", "html_url": "https://github.com/google-research/bert/issues/695", "id": 455581039, "node_id": "MDU6SXNzdWU0NTU1ODEwMzk=", "number": 695, "title": "1", "user": {"login": "sbmark", "id": 16055453, "node_id": "MDQ6VXNlcjE2MDU1NDUz", "avatar_url": "https://avatars3.githubusercontent.com/u/16055453?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbmark", "html_url": "https://github.com/sbmark", "followers_url": "https://api.github.com/users/sbmark/followers", "following_url": "https://api.github.com/users/sbmark/following{/other_user}", "gists_url": "https://api.github.com/users/sbmark/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbmark/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbmark/subscriptions", "organizations_url": "https://api.github.com/users/sbmark/orgs", "repos_url": "https://api.github.com/users/sbmark/repos", "events_url": "https://api.github.com/users/sbmark/events{/privacy}", "received_events_url": "https://api.github.com/users/sbmark/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-06-13T07:34:52Z", "updated_at": "2019-06-13T07:43:57Z", "closed_at": "2019-06-13T07:38:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "1", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/694", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/694/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/694/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/694/events", "html_url": "https://github.com/google-research/bert/issues/694", "id": 455542380, "node_id": "MDU6SXNzdWU0NTU1NDIzODA=", "number": 694, "title": "Interpretation of fine tuning output", "user": {"login": "optstat", "id": 15718380, "node_id": "MDQ6VXNlcjE1NzE4Mzgw", "avatar_url": "https://avatars2.githubusercontent.com/u/15718380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/optstat", "html_url": "https://github.com/optstat", "followers_url": "https://api.github.com/users/optstat/followers", "following_url": "https://api.github.com/users/optstat/following{/other_user}", "gists_url": "https://api.github.com/users/optstat/gists{/gist_id}", "starred_url": "https://api.github.com/users/optstat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/optstat/subscriptions", "organizations_url": "https://api.github.com/users/optstat/orgs", "repos_url": "https://api.github.com/users/optstat/repos", "events_url": "https://api.github.com/users/optstat/events{/privacy}", "received_events_url": "https://api.github.com/users/optstat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-13T05:32:41Z", "updated_at": "2019-06-25T18:39:32Z", "closed_at": "2019-06-25T18:39:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am fine tuning BERT large model on a CPU with 20 cores Xeon v2 processor with 96 GB of ram.  My GPU is a Titan Z with only 6GB memory so it is not suitable for the large model.  I am aware that this will take a long time to train.  The options I used are as follows\r\nexport BERT_LARGE_DIR=/home/panos/Dropbox/SENSEE/Google-Bert/model/cased_L-24_H-1024_A-16\r\nexport SQUAD_DIR=/home/panos/Dropbox/SENSEE/Google-Bert/use_cases/squad\r\n\r\n\r\npython3 run_squad.py \\\r\n  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\\r\n  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\\r\n  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\\r\n  --do_train=True \\\r\n  --train_file=$SQUAD_DIR/train-v2.0.json \\\r\n  --do_predict=True \\\r\n  --predict_file=$SQUAD_DIR/dev-v2.0.json \\\r\n  --train_batch_size=24 \\\r\n  --learning_rate=3e-5 \\\r\n  --num_train_epochs=2.0 \\\r\n  --max_seq_length=384 \\\r\n  --doc_stride=128 \\\r\n  --do_lower_case=False \\\r\n  --version_2_with_negative True \\\r\n  --output_dir=/home/panos/Dropbox/SENSEE/Google-Bert/use_cases/learn\\\r\n  --use_tpu=False \r\n\r\nMy question is how to interpret the output.  Here are the last few lines \r\nINFO:tensorflow:Saving checkpoints for 2000 into /home/panos/Dropbox/SENSEE/Google-Bert/use_cases/learn/model.ckpt.\r\nINFO:tensorflow:global_step/sec: 0.00791887\r\nINFO:tensorflow:examples/sec: 0.190053\r\nINFO:tensorflow:global_step/sec: 0.00797603\r\nINFO:tensorflow:examples/sec: 0.191425\r\nzINFO:tensorflow:global_step/sec: 0.00796088\r\nINFO:tensorflow:examples/sec: 0.191061\r\nHow do I know how far the training has progresses as a fraction of the total?  Is the number 2000 above refer to the number of steps taken so far?  If so how many total steps are there?\r\n\r\nMany thanks!\r\n--Panos", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/688", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/688/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/688/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/688/events", "html_url": "https://github.com/google-research/bert/issues/688", "id": 454139158, "node_id": "MDU6SXNzdWU0NTQxMzkxNTg=", "number": 688, "title": "Getting same probability of labels in all the test set examples", "user": {"login": "Rotramartya", "id": 28892962, "node_id": "MDQ6VXNlcjI4ODkyOTYy", "avatar_url": "https://avatars2.githubusercontent.com/u/28892962?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rotramartya", "html_url": "https://github.com/Rotramartya", "followers_url": "https://api.github.com/users/Rotramartya/followers", "following_url": "https://api.github.com/users/Rotramartya/following{/other_user}", "gists_url": "https://api.github.com/users/Rotramartya/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rotramartya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rotramartya/subscriptions", "organizations_url": "https://api.github.com/users/Rotramartya/orgs", "repos_url": "https://api.github.com/users/Rotramartya/repos", "events_url": "https://api.github.com/users/Rotramartya/events{/privacy}", "received_events_url": "https://api.github.com/users/Rotramartya/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-10T12:09:47Z", "updated_at": "2019-06-12T13:32:13Z", "closed_at": "2019-06-12T13:32:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "The number of labels is 32. It gives 0.25 probability of label '0' in all the test set examples. Similarly, it gives 0.017 probability for label '1' in the test set examples. And so on. Please help", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/682", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/682/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/682/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/682/events", "html_url": "https://github.com/google-research/bert/issues/682", "id": 452819419, "node_id": "MDU6SXNzdWU0NTI4MTk0MTk=", "number": 682, "title": "Can Bert be fine-tuned on unlabeled data?", "user": {"login": "Wanjun0511", "id": 19729970, "node_id": "MDQ6VXNlcjE5NzI5OTcw", "avatar_url": "https://avatars1.githubusercontent.com/u/19729970?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wanjun0511", "html_url": "https://github.com/Wanjun0511", "followers_url": "https://api.github.com/users/Wanjun0511/followers", "following_url": "https://api.github.com/users/Wanjun0511/following{/other_user}", "gists_url": "https://api.github.com/users/Wanjun0511/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wanjun0511/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wanjun0511/subscriptions", "organizations_url": "https://api.github.com/users/Wanjun0511/orgs", "repos_url": "https://api.github.com/users/Wanjun0511/repos", "events_url": "https://api.github.com/users/Wanjun0511/events{/privacy}", "received_events_url": "https://api.github.com/users/Wanjun0511/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-06T03:54:28Z", "updated_at": "2019-06-15T11:43:24Z", "closed_at": "2019-06-06T08:24:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Elmo can fine tune on unlabeled data, so we might need less domain labeled data to do training.  So is it possible on Bert?  Do you think it's a useful way to make model focus on domain data? \r\n\r\n![image](https://user-images.githubusercontent.com/19729970/59005663-0a6aa300-8851-11e9-8aed-0704ec2ad845.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/677", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/677/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/677/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/677/events", "html_url": "https://github.com/google-research/bert/issues/677", "id": 452213347, "node_id": "MDU6SXNzdWU0NTIyMTMzNDc=", "number": 677, "title": "Performance on cpu+gpu host for run_squad: only 1 CPU used for CPU part; only 1 GPU for GPU part", "user": {"login": "mfeblowitz", "id": 6854939, "node_id": "MDQ6VXNlcjY4NTQ5Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/6854939?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfeblowitz", "html_url": "https://github.com/mfeblowitz", "followers_url": "https://api.github.com/users/mfeblowitz/followers", "following_url": "https://api.github.com/users/mfeblowitz/following{/other_user}", "gists_url": "https://api.github.com/users/mfeblowitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfeblowitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfeblowitz/subscriptions", "organizations_url": "https://api.github.com/users/mfeblowitz/orgs", "repos_url": "https://api.github.com/users/mfeblowitz/repos", "events_url": "https://api.github.com/users/mfeblowitz/events{/privacy}", "received_events_url": "https://api.github.com/users/mfeblowitz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-04T21:20:09Z", "updated_at": "2019-07-09T15:18:36Z", "closed_at": "2019-06-19T13:13:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "Following up from #500, I've noticed that I've had to wait quite some time for a predict-only use of run_squad for squad 2.\r\n\r\n**Specifics**\r\nRunning bert_squad for 2M+ questions against multi-paragraph document. Running on cloud host with 2 v100 gpus, 16 cpus. For parts of processing  performed pre- and post- core run_squad processing (where processing occurs only on cpus) only one cpu is used. For core processing on gpus, both gpus configured and assigned to the task, but only one showing evidence of processing.\r\n\r\n**Background**\r\nI tried using progressively bigger/faster hosts, but noticed less improvement in overall processing duration than expected. What I'm using now is a host that's configured with 2 x V100 GPUs,  16 CPUs, and 120 GB RAM.\r\n\r\nGranted, the task I'm trying to perform is not small - in some cases I'm asking a total of 2.5 million question/paragraph pairs (that is, a smaller set of questions, each asked against many paragraphs).\r\n\r\n**Observations**\r\nOne key observation is that most of the processing appears to be single threaded. For much of the CPU-only code, only one core is used. I believe that this is happening during the creation of the eval.tf_record file. Interestingly, the work changes from core to core, leading me to wonder whether some of that could be parallelized.\r\n\r\nI've also noticed that, for the processing that's being done on gpu, the work appears to be performed only on a single gpu, in spite of the fact that the host I'm running on has two gpus, and the log trace indicates that both are being recognized by tensorflow.\r\n\r\nI've noticed that the work being done to create the eval.tf_record file, the GPUs are not in use. That's the part that appears to hop around from cpu to cpu, apparently changing for each of the 19 example sets. Also, the code for determining and writing nbest to disk runs on a single cpu.\r\n\r\nAs for the GPU, the log traces show that both GPUs are configured for use in tensorflow, and also nvidia-smi shows both gpu 0 and gpu 1 are assigned to the same python process  PID. gpu 0 shows most of its memory in use, gpu 1 shows very little. And the relative processor temperatures reflect one being active and the other idle. This appears to be happening during the \"predict\" work :\r\n```\r\nINFO:tensorflow:Processing example: 283000\r\nINFO:tensorflow:Processing example: 284000\r\nINFO:tensorflow:Processing example: 285000\r\nINFO:tensorflow:Processing example: 286000\r\nINFO:tensorflow:Processing example: 287000\r\nINFO:tensorflow:Processing example: 288000\r\nINFO:tensorflow:Processing example: 289000\r\n```\r\n\r\nAm I interpreting this correctly? Does it indicate a problem or is it simply made necessary due to the nature of the processing?\r\n\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/674", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/674/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/674/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/674/events", "html_url": "https://github.com/google-research/bert/issues/674", "id": 451810368, "node_id": "MDU6SXNzdWU0NTE4MTAzNjg=", "number": 674, "title": "Not compatible with tensorflow 2.0", "user": {"login": "makaveli10", "id": 39617050, "node_id": "MDQ6VXNlcjM5NjE3MDUw", "avatar_url": "https://avatars3.githubusercontent.com/u/39617050?v=4", "gravatar_id": "", "url": "https://api.github.com/users/makaveli10", "html_url": "https://github.com/makaveli10", "followers_url": "https://api.github.com/users/makaveli10/followers", "following_url": "https://api.github.com/users/makaveli10/following{/other_user}", "gists_url": "https://api.github.com/users/makaveli10/gists{/gist_id}", "starred_url": "https://api.github.com/users/makaveli10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/makaveli10/subscriptions", "organizations_url": "https://api.github.com/users/makaveli10/orgs", "repos_url": "https://api.github.com/users/makaveli10/repos", "events_url": "https://api.github.com/users/makaveli10/events{/privacy}", "received_events_url": "https://api.github.com/users/makaveli10/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-06-04T05:55:20Z", "updated_at": "2019-09-24T16:50:07Z", "closed_at": "2019-06-16T06:42:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is Bert not compatible with tensorflow 2.0 ?\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-1b957e7a053a> in <module>()\r\n      1 import modeling\r\n----> 2 import optimization\r\n      3 import run_classifier\r\n      4 import run_classifier_with_tfhub\r\n      5 import tokenization\r\n/content/bert_repo/optimization.py in <module>()\r\n     85 \r\n     86 \r\n---> 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):\r\n     88   \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\r\n     89 \r\n\r\nAttributeError: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/672", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/672/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/672/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/672/events", "html_url": "https://github.com/google-research/bert/issues/672", "id": 451554052, "node_id": "MDU6SXNzdWU0NTE1NTQwNTI=", "number": 672, "title": "Bert with ner -> run_classifier", "user": {"login": "NameBrez", "id": 25982352, "node_id": "MDQ6VXNlcjI1OTgyMzUy", "avatar_url": "https://avatars0.githubusercontent.com/u/25982352?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NameBrez", "html_url": "https://github.com/NameBrez", "followers_url": "https://api.github.com/users/NameBrez/followers", "following_url": "https://api.github.com/users/NameBrez/following{/other_user}", "gists_url": "https://api.github.com/users/NameBrez/gists{/gist_id}", "starred_url": "https://api.github.com/users/NameBrez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NameBrez/subscriptions", "organizations_url": "https://api.github.com/users/NameBrez/orgs", "repos_url": "https://api.github.com/users/NameBrez/repos", "events_url": "https://api.github.com/users/NameBrez/events{/privacy}", "received_events_url": "https://api.github.com/users/NameBrez/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-06-03T15:40:08Z", "updated_at": "2020-02-23T09:00:20Z", "closed_at": "2019-06-12T10:07:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI've been tackling NER with BERT problem, using the run classifier and custom processor. I've pretrained the model on my language and would like to try something like: In given text I would like o recognize book titles. I've created a training set in this matter: I've splitted the text into words and put them in one collumn in the next colunm I've put the label (O-other, TS-title start, TM-title middle). The problem Im anticipating is the number of occurances of TS and TM in my text as even in a sentence book titles are usualy less than 10% of a sentence thus Im afraid the model wont learn recognizing book titles but rather classify everyting as O and get good accuracy. \r\n\r\nIs this even a way to tacle such problem? I've seen some implementations here but Im wondergin how you've done it as you are mentioning using bert and ner in the article. Is there any samples like the squad one?\r\nBest", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/669", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/669/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/669/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/669/events", "html_url": "https://github.com/google-research/bert/issues/669", "id": 450271634, "node_id": "MDU6SXNzdWU0NTAyNzE2MzQ=", "number": 669, "title": "Training BERT on v2-8 TPU Google Cloud - Allocation of 93763584 exceeds 10% of system memory.", "user": {"login": "RubensZimbres", "id": 20270054, "node_id": "MDQ6VXNlcjIwMjcwMDU0", "avatar_url": "https://avatars0.githubusercontent.com/u/20270054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RubensZimbres", "html_url": "https://github.com/RubensZimbres", "followers_url": "https://api.github.com/users/RubensZimbres/followers", "following_url": "https://api.github.com/users/RubensZimbres/following{/other_user}", "gists_url": "https://api.github.com/users/RubensZimbres/gists{/gist_id}", "starred_url": "https://api.github.com/users/RubensZimbres/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RubensZimbres/subscriptions", "organizations_url": "https://api.github.com/users/RubensZimbres/orgs", "repos_url": "https://api.github.com/users/RubensZimbres/repos", "events_url": "https://api.github.com/users/RubensZimbres/events{/privacy}", "received_events_url": "https://api.github.com/users/RubensZimbres/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-30T11:59:48Z", "updated_at": "2019-05-31T15:00:55Z", "closed_at": "2019-05-31T15:00:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to train BERT for 3 epochs on a Google Cloud TPU v2-8 running Tensorflow 1.13 with the following command:\r\n\r\n```\r\npython run_classifier.py --task_name=cola --do_train=True -\u2013do_eval=False --do_predict=False --data_dir=/home/BERT/data --vocab_file=/home/BERT/uncased_L-12_H-768_A-12/vocab.txt --bert_config_file=/home/BERT/uncased_L-12_H-768_A-12/bert_config.json --init_checkpoint=/home/BERT/uncased_L-12_H-768_A-12/bert_model.ckpt --max_seq_length=50  --output_dir=/home/BERT/bert_output --do_lower_case=True --save_checkpoints_steps=1 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3 --use_tpu=False --master=\"\" --iterations_per_loop=1000 --tpu_cluster_resolver=None --eval_batch_size=32 --predict_batch_size=32 --warmup_proportion=0.1 --num_tpu_cores=8`\r\n```\r\n\r\nThe whole notebook runs fast, but unfortunately the script stalls and get killed after the following error:\r\n\r\n```\r\nINFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = output_weights:0, shape = (2, 768)\r\nINFO:tensorflow:  name = output_bias:0, shape = (2,)\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2019-05-30 08:48:33.679756: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-30 08:48:33.721412: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-05-30 08:48:33.722776: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55cd8fb471f0 executing computations on platform Host. Devices:\r\n2019-05-30 08:48:33.722799: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-30 08:48:34.879614: W tensorflow/core/framework/allocator.cc:124] Allocation of 93763584 exceeds 10% of system memory.\r\n2019-05-30 08:48:35.002163: W tensorflow/core/framework/allocator.cc:124] Allocation of 93763584 exceeds 10% of system memory.\r\n2019-05-30 08:48:35.429507: W tensorflow/core/framework/allocator.cc:124] Allocation of 93763584 exceeds 10% of system memory.\r\n2019-05-30 08:48:35.502813: W tensorflow/core/framework/allocator.cc:124] Allocation of 93763584 exceeds 10% of system memory.\r\n2019-05-30 08:48:40.231155: W tensorflow/core/framework/allocator.cc:124] Allocation of 93763584 exceeds 10% of system memory.\r\nKilled\r\n```\r\nIf I state `--use_tpu=True` I get this error:\r\n\r\n```\r\nRuntimeError: Cannot find any TPU cores in the system. Please double check Tensorflow master address and TPU worker(s). Available devices are [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 11095850074158087647), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9589057225258435761)].\r\n```\r\n\r\n\r\nI reduced batch size to 1 in training, eval and predict without success. Any ideas on how I can overcome this issue ? As long as I know, next step will be:\r\n\r\n```\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into ./model.ckpt.\r\n```\r\n\r\nFor ```--do_train=False do_eval=False --do_predict=True```, stalls at:\r\n\r\n``` \r\nINFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = output_weights:0, shape = (2, 768)\r\nINFO:tensorflow:  name = output_bias:0, shape = (2,)\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Graph was finalized.\r\n2019-05-30 09:41:14.430582: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-30 09:41:14.476754: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-05-30 09:41:14.478446: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5589ceee9970 executing computations on platform Host. Devices:\r\n2019-05-30 09:41:14.478701: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-30 09:41:18.139314: W tensorflow/core/framework/allocator.cc:124] Allocation of 93763584 exceeds 10% of system memory.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n``` ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/640", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/640/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/640/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/640/events", "html_url": "https://github.com/google-research/bert/issues/640", "id": 443215216, "node_id": "MDU6SXNzdWU0NDMyMTUyMTY=", "number": 640, "title": "How could BERT-Large fit into TPUs?", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-13T05:46:01Z", "updated_at": "2019-05-20T03:26:28Z", "closed_at": "2019-05-20T03:26:28Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "The BERT [README](https://github.com/google-research/bert/blob/master/README.md#out-of-memory-issues) says **a 12 GB RAM Titan X can NOT hold even a single sample of length 384 for BERT-Large**.\r\n\r\nBut calculation shows that per-sample memory usage of TPUs is much less than that of GPUs:\r\n\r\nThe BERT paper says, a batch size of 256 and a maximum sequence length of 512 is used to pre-train BERT-Large on 64 TPU chips, i.e.: 128 TPU cores. Put it another way, 2 samples can sit in a single TPU core. Even TPUv3 (16 GM HBM per core) is used for pre-training, the above calculation indicates 16 GB memory can hold 2 samples, or equivalently 8 GB TPU memory is able to hold 1 sample. **How could 8 GB TPU memory (or 4 GB memory if TPUv2 is used) holds a sample of length 512 while a 12 GB GPU CAN NOT hold a single sample of length 384?**\r\n\r\nDoes this mean memory consumption of TF is highly optimized for TPU? Or did I make some mistakes in the calculation?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/638", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/638/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/638/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/638/events", "html_url": "https://github.com/google-research/bert/issues/638", "id": 443047306, "node_id": "MDU6SXNzdWU0NDMwNDczMDY=", "number": 638, "title": "module 'tokenization' has no attribute 'FullTokenizer'", "user": {"login": "1337-Pete", "id": 43712596, "node_id": "MDQ6VXNlcjQzNzEyNTk2", "avatar_url": "https://avatars2.githubusercontent.com/u/43712596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/1337-Pete", "html_url": "https://github.com/1337-Pete", "followers_url": "https://api.github.com/users/1337-Pete/followers", "following_url": "https://api.github.com/users/1337-Pete/following{/other_user}", "gists_url": "https://api.github.com/users/1337-Pete/gists{/gist_id}", "starred_url": "https://api.github.com/users/1337-Pete/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/1337-Pete/subscriptions", "organizations_url": "https://api.github.com/users/1337-Pete/orgs", "repos_url": "https://api.github.com/users/1337-Pete/repos", "events_url": "https://api.github.com/users/1337-Pete/events{/privacy}", "received_events_url": "https://api.github.com/users/1337-Pete/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-05-11T23:52:43Z", "updated_at": "2020-02-28T12:17:34Z", "closed_at": "2019-05-12T02:43:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm importing tokenization, have installed via pip, and cannot instantiate the tokenizer. I'm using the following code below and continue to get an error message of \"module 'tokenization' has no attribute 'FullTokenizer'\".\r\n\r\nAnyone have a sense as to why?\r\n\r\n`tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/635", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/635/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/635/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/635/events", "html_url": "https://github.com/google-research/bert/issues/635", "id": 442399431, "node_id": "MDU6SXNzdWU0NDIzOTk0MzE=", "number": 635, "title": "Padding sent1 hurts the performance of sentence pair tasks", "user": {"login": "Akella17", "id": 16236287, "node_id": "MDQ6VXNlcjE2MjM2Mjg3", "avatar_url": "https://avatars3.githubusercontent.com/u/16236287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Akella17", "html_url": "https://github.com/Akella17", "followers_url": "https://api.github.com/users/Akella17/followers", "following_url": "https://api.github.com/users/Akella17/following{/other_user}", "gists_url": "https://api.github.com/users/Akella17/gists{/gist_id}", "starred_url": "https://api.github.com/users/Akella17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Akella17/subscriptions", "organizations_url": "https://api.github.com/users/Akella17/orgs", "repos_url": "https://api.github.com/users/Akella17/repos", "events_url": "https://api.github.com/users/Akella17/events{/privacy}", "received_events_url": "https://api.github.com/users/Akella17/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-05-09T19:38:48Z", "updated_at": "2019-11-20T15:02:25Z", "closed_at": "2019-05-10T09:41:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Padding the first sentence to make it a fixed-sized vector hurts the accuracy of sentence pair tasks. My input representation for BERT is as follows,\r\n```python\r\nInput_ids   : <CLS><...Sent_1_tokens...><SEP><PAD>...<PAD> | <SEP><...Sent_2_tokens...><SEP><PAD>...<PAD>\r\nSegment Ids :  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 | 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\r\nInput Mask  :  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 | 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\r\n```\r\nWhen I do not pad **sent1** and directly append it with a **separator token** followed by **sent2** (as suggested in the paper), I am getting great accuracy score. However, the inputs to BERT for my specific usecase are already padded, which puts me in a tough spot. Is there any reason why fine tuning might not be working for this representation and can you suggest a solution to this problem?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/633", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/633/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/633/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/633/events", "html_url": "https://github.com/google-research/bert/issues/633", "id": 442114122, "node_id": "MDU6SXNzdWU0NDIxMTQxMjI=", "number": 633, "title": "Is multilingual model cross-lingual?", "user": {"login": "wanicca", "id": 22728527, "node_id": "MDQ6VXNlcjIyNzI4NTI3", "avatar_url": "https://avatars0.githubusercontent.com/u/22728527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wanicca", "html_url": "https://github.com/wanicca", "followers_url": "https://api.github.com/users/wanicca/followers", "following_url": "https://api.github.com/users/wanicca/following{/other_user}", "gists_url": "https://api.github.com/users/wanicca/gists{/gist_id}", "starred_url": "https://api.github.com/users/wanicca/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wanicca/subscriptions", "organizations_url": "https://api.github.com/users/wanicca/orgs", "repos_url": "https://api.github.com/users/wanicca/repos", "events_url": "https://api.github.com/users/wanicca/events{/privacy}", "received_events_url": "https://api.github.com/users/wanicca/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-09T08:30:12Z", "updated_at": "2019-06-04T08:04:11Z", "closed_at": "2019-06-04T08:04:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I wonder if bert multilingual representations can perform like other multilingual embeddings obtained by aligning monolingual embeddings (like [fastText multilingual](https://github.com/Babylonpartners/fastText_multilingual) )? That is to say, do the synonyms in a parallel sentence in different languages have analogous vector representations? Is the pretrained bert multilingual model cross-lingual, or just a multilingual model that can receive tokens in different languages as input?\r\n\r\nI read the Multilingual README, and didn't find any clue about cross-lingual setting. Does the pretrained model just use corpora in different languages, which means the representations in final layer are not promised to be cross-lingual? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/632", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/632/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/632/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/632/events", "html_url": "https://github.com/google-research/bert/issues/632", "id": 442102114, "node_id": "MDU6SXNzdWU0NDIxMDIxMTQ=", "number": 632, "title": "How do I continue training from checkpoint?", "user": {"login": "kkkppp", "id": 4020633, "node_id": "MDQ6VXNlcjQwMjA2MzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4020633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kkkppp", "html_url": "https://github.com/kkkppp", "followers_url": "https://api.github.com/users/kkkppp/followers", "following_url": "https://api.github.com/users/kkkppp/following{/other_user}", "gists_url": "https://api.github.com/users/kkkppp/gists{/gist_id}", "starred_url": "https://api.github.com/users/kkkppp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kkkppp/subscriptions", "organizations_url": "https://api.github.com/users/kkkppp/orgs", "repos_url": "https://api.github.com/users/kkkppp/repos", "events_url": "https://api.github.com/users/kkkppp/events{/privacy}", "received_events_url": "https://api.github.com/users/kkkppp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-09T08:01:25Z", "updated_at": "2019-05-10T06:00:36Z", "closed_at": "2019-05-10T06:00:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "I did initial fine-tuning, created checkpoint, in my case it's model.ckpt-838\r\nThen I created new dataset from incorrect predictions, annotated by user\r\n\r\nNow I want to continue fine-tuning from where I stopped on first attempt\r\nSo I run run_classifier with value --init_checkpoint=./output/result_dir/model.ckpt-838\r\n\r\nHowever It doesn't do what I expected:\r\n\r\nINFO:tensorflow:***** Running training*****\r\nINFO:tensorflow:  Num examples = 10\r\nINFO:tensorflow:  Batch size = 24\r\nINFO:tensorflow:  Num steps = 4\r\nINFO:tensorflow:Skipping training since max_steps has already saved.\r\n\r\nWhat is correct way to resume training?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/629", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/629/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/629/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/629/events", "html_url": "https://github.com/google-research/bert/issues/629", "id": 442023218, "node_id": "MDU6SXNzdWU0NDIwMjMyMTg=", "number": 629, "title": "Really well commented code. thank you!", "user": {"login": "eracah", "id": 5356173, "node_id": "MDQ6VXNlcjUzNTYxNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5356173?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eracah", "html_url": "https://github.com/eracah", "followers_url": "https://api.github.com/users/eracah/followers", "following_url": "https://api.github.com/users/eracah/following{/other_user}", "gists_url": "https://api.github.com/users/eracah/gists{/gist_id}", "starred_url": "https://api.github.com/users/eracah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eracah/subscriptions", "organizations_url": "https://api.github.com/users/eracah/orgs", "repos_url": "https://api.github.com/users/eracah/repos", "events_url": "https://api.github.com/users/eracah/events{/privacy}", "received_events_url": "https://api.github.com/users/eracah/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-09T02:39:59Z", "updated_at": "2019-05-09T02:40:02Z", "closed_at": "2019-05-09T02:40:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was curious to how you went from (batch_size,seq_len,hidden_size) to (batch_size,hidden_size). I found the comments here https://github.com/google-research/bert/blob/master/modeling.py#L219L226 to be very helpful. \r\n\r\nThank you so much for commenting!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/628", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/628/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/628/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/628/events", "html_url": "https://github.com/google-research/bert/issues/628", "id": 441982672, "node_id": "MDU6SXNzdWU0NDE5ODI2NzI=", "number": 628, "title": "Not able to run BERT-Large with either SQuAD 2.0/SQuAD 1.1 on GCP with TPU", "user": {"login": "praneethnooli", "id": 22989516, "node_id": "MDQ6VXNlcjIyOTg5NTE2", "avatar_url": "https://avatars1.githubusercontent.com/u/22989516?v=4", "gravatar_id": "", "url": "https://api.github.com/users/praneethnooli", "html_url": "https://github.com/praneethnooli", "followers_url": "https://api.github.com/users/praneethnooli/followers", "following_url": "https://api.github.com/users/praneethnooli/following{/other_user}", "gists_url": "https://api.github.com/users/praneethnooli/gists{/gist_id}", "starred_url": "https://api.github.com/users/praneethnooli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/praneethnooli/subscriptions", "organizations_url": "https://api.github.com/users/praneethnooli/orgs", "repos_url": "https://api.github.com/users/praneethnooli/repos", "events_url": "https://api.github.com/users/praneethnooli/events{/privacy}", "received_events_url": "https://api.github.com/users/praneethnooli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-05-08T23:14:09Z", "updated_at": "2019-05-17T15:31:07Z", "closed_at": "2019-05-17T15:13:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I am new to GCP and I followed the instructions of mnist training in https://cloud.google.com/tpu/docs/tutorials/mnist to run run_squad.py with SQuAD 2.0 in my vm with the following script:\r\n\r\npython run_squad.py\r\n  --vocab_file=${STORAGE_BUCKET}/BERT_LARGE_DIR/vocab.txt \\\r\n  --bert_config_file=${STORAGE_BUCKET}/BERT_LARGE_DIR/bert_config.json \\\r\n  --init_checkpoint=${STORAGE_BUCKET}/BERT_LARGE_DIR/bert_model.ckpt \\\r\n  --do_train=True \\\r\n  --train_file=${STORAGE_BUCKET}/SQUAD_DIR/train-v2.0.json \\\r\n  --do_predict=True \\\r\n  --predict_file=${STORAGE_BUCKET}/SQUAD_DIR/dev-v2.0.json \\\r\n  --train_batch_size=24 \\\r\n  --learning_rate=3e-5 \\\r\n  --num_train_epochs=2.0 \\\r\n  --max_seq_length=384 \\\r\n  --doc_stride=128 \\\r\n  --output_dir=${STORAGE_BUCKET}/result/ \\\r\n  --use_tpu=True \\\r\n  --tpu_name=$TPU_NAME \\\r\n  --version_2_with_negative=True\r\n\r\nbut I always end up with the following issue after executing the script for some time\r\n\r\nRuntimeError: Cannot find any TPU cores in the system. Please double check Tensorflow master address and TPU worker(s). Available devices are \r\n[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 12323948552096263132), _DeviceAttributes(/job:localhost/replica:0/t\r\nask:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 12521347734421362015)].\r\n\r\n**I'm facing the same issue even if I run BERT-Large with SQuAD 1.1**\r\n\r\nI don't know if I'm missing anything, can anyone please help me out. \r\nThanks in advance!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/625", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/625/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/625/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/625/events", "html_url": "https://github.com/google-research/bert/issues/625", "id": 441029848, "node_id": "MDU6SXNzdWU0NDEwMjk4NDg=", "number": 625, "title": "Can the use of [SEP] reduce the information extraction between the sentences?", "user": {"login": "RomanShen", "id": 23472425, "node_id": "MDQ6VXNlcjIzNDcyNDI1", "avatar_url": "https://avatars0.githubusercontent.com/u/23472425?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RomanShen", "html_url": "https://github.com/RomanShen", "followers_url": "https://api.github.com/users/RomanShen/followers", "following_url": "https://api.github.com/users/RomanShen/following{/other_user}", "gists_url": "https://api.github.com/users/RomanShen/gists{/gist_id}", "starred_url": "https://api.github.com/users/RomanShen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RomanShen/subscriptions", "organizations_url": "https://api.github.com/users/RomanShen/orgs", "repos_url": "https://api.github.com/users/RomanShen/repos", "events_url": "https://api.github.com/users/RomanShen/events{/privacy}", "received_events_url": "https://api.github.com/users/RomanShen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-07T05:08:59Z", "updated_at": "2019-05-20T00:06:32Z", "closed_at": "2019-05-20T00:06:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello. I know that [CLS] means the start of a sentence and [SEP] makes BERT know the second sentence has begun. [SEP] can\u2019t stop one sentence from extracting information from another sentence. However, I have a question.\r\nIf I have 2 sentences, which are s1 and s2, and our fine-tuning task is the same. In one way, I add special tokens and the input looks like [CLS]+s1+[SEP] + s2 + [SEP]. In another, I make the input look like [CLS] + s1 + s2 + [SEP]. When I input them to BERT respectively, what is the difference between them? Will the s1 in second one integrate more information from s2 than the s1 in first one does? Will the token embeddings change a lot between the 2 methods?\r\nThanks for any help!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/624", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/624/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/624/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/624/events", "html_url": "https://github.com/google-research/bert/issues/624", "id": 440735141, "node_id": "MDU6SXNzdWU0NDA3MzUxNDE=", "number": 624, "title": "Pretraining a PyTorch model of BERT", "user": {"login": "nadavborenstein", "id": 15877500, "node_id": "MDQ6VXNlcjE1ODc3NTAw", "avatar_url": "https://avatars2.githubusercontent.com/u/15877500?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nadavborenstein", "html_url": "https://github.com/nadavborenstein", "followers_url": "https://api.github.com/users/nadavborenstein/followers", "following_url": "https://api.github.com/users/nadavborenstein/following{/other_user}", "gists_url": "https://api.github.com/users/nadavborenstein/gists{/gist_id}", "starred_url": "https://api.github.com/users/nadavborenstein/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nadavborenstein/subscriptions", "organizations_url": "https://api.github.com/users/nadavborenstein/orgs", "repos_url": "https://api.github.com/users/nadavborenstein/repos", "events_url": "https://api.github.com/users/nadavborenstein/events{/privacy}", "received_events_url": "https://api.github.com/users/nadavborenstein/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-06T14:44:12Z", "updated_at": "2019-05-06T15:04:11Z", "closed_at": "2019-05-06T15:04:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "We want to pretrain BERT on a costume dataset and then use this trained model with the pytorch_pretrained_bert package.\r\nWe saw that your repository contains a PyTorch model of pretrained sci-BERT, and wondered which tools did you use to create this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/623", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/623/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/623/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/623/events", "html_url": "https://github.com/google-research/bert/issues/623", "id": 440425400, "node_id": "MDU6SXNzdWU0NDA0MjU0MDA=", "number": 623, "title": "error when running bert on GCP with tpu", "user": {"login": "RomanShen", "id": 23472425, "node_id": "MDQ6VXNlcjIzNDcyNDI1", "avatar_url": "https://avatars0.githubusercontent.com/u/23472425?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RomanShen", "html_url": "https://github.com/RomanShen", "followers_url": "https://api.github.com/users/RomanShen/followers", "following_url": "https://api.github.com/users/RomanShen/following{/other_user}", "gists_url": "https://api.github.com/users/RomanShen/gists{/gist_id}", "starred_url": "https://api.github.com/users/RomanShen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RomanShen/subscriptions", "organizations_url": "https://api.github.com/users/RomanShen/orgs", "repos_url": "https://api.github.com/users/RomanShen/repos", "events_url": "https://api.github.com/users/RomanShen/events{/privacy}", "received_events_url": "https://api.github.com/users/RomanShen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-05-05T09:52:38Z", "updated_at": "2019-07-17T17:49:51Z", "closed_at": "2019-05-08T11:46:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I am new to google cloud and I follow the instruction of mnist training in https://cloud.google.com/tpu/docs/tutorials/mnist and everything seems fine.\r\nHowever, when I try to run run_pretraining.py in my vm, it seems to have some problems:\r\n\r\n Cannot find any TPU cores in the system. Please double check Tensorflow master address and TPU worker(s). Available devices are [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 18332812683785243352), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 7966451089330435898)].\r\n\r\nHere is my script:\r\npython run_pretraining.py \\\r\n  --input_file=${STORAGE_BUCKET}/tf_examples.tfrecord \\\r\n  --output_dir=${STORAGE_BUCKET}/pretraining_output \\\r\n  --do_train=True \\\r\n  --do_eval=True \\\r\n  --bert_config_file=${STORAGE_BUCKET}/uncased_L-12_H-768_A-12/bert_config.json \\\r\n  --init_checkpoint=${STORAGE_BUCKET}/uncased_L-12_H-768_A-12/bert_model.ckpt \\\r\n  --train_batch_size=32 \\\r\n  --max_seq_length=512 \\\r\n  --max_predictions_per_seq=77 \\\r\n  --num_train_steps=10000 \\\r\n  --num_warmup_steps=10 \\\r\n  --learning_rate=2e-5 \\\r\n  --tpu=${TPU_NAME} \\\r\n  --use_tpu=True \r\n\r\nevery file needed is in my storage_bucket.\r\n\r\nctpu status returned:\r\nYour cluster is running!\r\nCompute Engine VM:  RUNNING\r\nCloud TPU:          RUNNING\r\n\r\nIs there anything I didn't get in the tutorial?\r\nThanks for any help!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/620", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/620/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/620/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/620/events", "html_url": "https://github.com/google-research/bert/issues/620", "id": 440259399, "node_id": "MDU6SXNzdWU0NDAyNTkzOTk=", "number": 620, "title": "Bert Context Based QA", "user": {"login": "rohan636", "id": 8437306, "node_id": "MDQ6VXNlcjg0MzczMDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/8437306?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan636", "html_url": "https://github.com/rohan636", "followers_url": "https://api.github.com/users/rohan636/followers", "following_url": "https://api.github.com/users/rohan636/following{/other_user}", "gists_url": "https://api.github.com/users/rohan636/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan636/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan636/subscriptions", "organizations_url": "https://api.github.com/users/rohan636/orgs", "repos_url": "https://api.github.com/users/rohan636/repos", "events_url": "https://api.github.com/users/rohan636/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan636/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-03T23:41:04Z", "updated_at": "2019-05-17T17:47:41Z", "closed_at": "2019-05-17T17:47:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Very new to NLP, still learning at the moment. Could I get a little guidance understanding on how to implement a context-based QA system with gives results in natural language?\r\n\r\nExample: \r\nContext: Its 7pm in New York. \r\nQuestion: What is the time in New York?\r\nAnswer: Its 7 in the evening in New York\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/618", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/618/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/618/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/618/events", "html_url": "https://github.com/google-research/bert/issues/618", "id": 440039376, "node_id": "MDU6SXNzdWU0NDAwMzkzNzY=", "number": 618, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError:", "user": {"login": "Dhanachandra", "id": 10828657, "node_id": "MDQ6VXNlcjEwODI4NjU3", "avatar_url": "https://avatars1.githubusercontent.com/u/10828657?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dhanachandra", "html_url": "https://github.com/Dhanachandra", "followers_url": "https://api.github.com/users/Dhanachandra/followers", "following_url": "https://api.github.com/users/Dhanachandra/following{/other_user}", "gists_url": "https://api.github.com/users/Dhanachandra/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dhanachandra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dhanachandra/subscriptions", "organizations_url": "https://api.github.com/users/Dhanachandra/orgs", "repos_url": "https://api.github.com/users/Dhanachandra/repos", "events_url": "https://api.github.com/users/Dhanachandra/events{/privacy}", "received_events_url": "https://api.github.com/users/Dhanachandra/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-03T13:12:09Z", "updated_at": "2019-05-03T13:13:07Z", "closed_at": "2019-05-03T13:13:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/616", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/616/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/616/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/616/events", "html_url": "https://github.com/google-research/bert/issues/616", "id": 439911808, "node_id": "MDU6SXNzdWU0Mzk5MTE4MDg=", "number": 616, "title": "650M corpus was splited to 40 files pass to create_pretraining_data, but it takes too long time", "user": {"login": "loveJasmine", "id": 18051187, "node_id": "MDQ6VXNlcjE4MDUxMTg3", "avatar_url": "https://avatars2.githubusercontent.com/u/18051187?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loveJasmine", "html_url": "https://github.com/loveJasmine", "followers_url": "https://api.github.com/users/loveJasmine/followers", "following_url": "https://api.github.com/users/loveJasmine/following{/other_user}", "gists_url": "https://api.github.com/users/loveJasmine/gists{/gist_id}", "starred_url": "https://api.github.com/users/loveJasmine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loveJasmine/subscriptions", "organizations_url": "https://api.github.com/users/loveJasmine/orgs", "repos_url": "https://api.github.com/users/loveJasmine/repos", "events_url": "https://api.github.com/users/loveJasmine/events{/privacy}", "received_events_url": "https://api.github.com/users/loveJasmine/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-05-03T06:55:00Z", "updated_at": "2019-05-04T01:02:28Z", "closed_at": "2019-05-04T01:02:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "after 24 hours, it still runing.\r\n\r\npython create_pretraining_data.py --input_file=BERT_BASE_DIR/all_file_merge.copyblock_notitle_linesplit.txt-6400* --output_file=BERT_BASE_DIR/tf_examples-64.tfrecord --vocab_file=chn_vocab --do_lower_case=True --max_seq_length=64 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\r\n\r\n\r\nall_file_merge.copyblock_notitle_linesplit.txt-6400* is all_file_merge.copyblock_notitle_linesplit.txt-640001 to all_file_merge.copyblock_notitle_linesplit.txt-640039\r\n\r\neach file less than 20M", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/614", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/614/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/614/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/614/events", "html_url": "https://github.com/google-research/bert/issues/614", "id": 439423591, "node_id": "MDU6SXNzdWU0Mzk0MjM1OTE=", "number": 614, "title": "how to infer in python ", "user": {"login": "jinamshah", "id": 40169508, "node_id": "MDQ6VXNlcjQwMTY5NTA4", "avatar_url": "https://avatars2.githubusercontent.com/u/40169508?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinamshah", "html_url": "https://github.com/jinamshah", "followers_url": "https://api.github.com/users/jinamshah/followers", "following_url": "https://api.github.com/users/jinamshah/following{/other_user}", "gists_url": "https://api.github.com/users/jinamshah/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinamshah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinamshah/subscriptions", "organizations_url": "https://api.github.com/users/jinamshah/orgs", "repos_url": "https://api.github.com/users/jinamshah/repos", "events_url": "https://api.github.com/users/jinamshah/events{/privacy}", "received_events_url": "https://api.github.com/users/jinamshah/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2019-05-02T04:32:53Z", "updated_at": "2019-07-22T12:23:38Z", "closed_at": "2019-07-22T04:41:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi all!\r\nBefore I start I would like to mention that I am very new to Tensorflow, so please pardon me if my question is very basic.\r\nI am trying to infer BERT model after exporting it using the following piece of code:\r\n\r\n# Export the model\r\ndef serving_input_fn():\r\n  with tf.variable_scope(\"sorting_hat\"):\r\n    feature_spec = {\r\n        \"input_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\r\n        \"input_mask\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\r\n        \"segment_ids\": tf.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\r\n        \"label_ids\": tf.FixedLenFeature([], tf.int64),\r\n      }\r\n    serialized_tf_example = tf.placeholder(dtype=tf.string,\r\n                                           shape=[None],\r\n                                           name='input_example_tensor')\r\n    receiver_tensors = {'examples': serialized_tf_example}\r\n    features = tf.parse_example(serialized_tf_example, feature_spec)\r\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\r\n\r\nEXPORT_DIR = 'gs://{}/export/{}'.format(BUCKET, TASK_VERSION)\r\nestimator._export_to_tpu = False  # this is important\r\npath = estimator.export_savedmodel(EXPORT_DIR, serving_input_fn)\r\nprint(path)\r\n\r\n\r\n\r\nI want to know how can the saved model.pb file be used for inference in python code(not using the saved_model_cli )", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/611", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/611/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/611/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/611/events", "html_url": "https://github.com/google-research/bert/issues/611", "id": 439367811, "node_id": "MDU6SXNzdWU0MzkzNjc4MTE=", "number": 611, "title": "License of the pretrained models", "user": {"login": "xuhdev", "id": 325476, "node_id": "MDQ6VXNlcjMyNTQ3Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/325476?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuhdev", "html_url": "https://github.com/xuhdev", "followers_url": "https://api.github.com/users/xuhdev/followers", "following_url": "https://api.github.com/users/xuhdev/following{/other_user}", "gists_url": "https://api.github.com/users/xuhdev/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuhdev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuhdev/subscriptions", "organizations_url": "https://api.github.com/users/xuhdev/orgs", "repos_url": "https://api.github.com/users/xuhdev/repos", "events_url": "https://api.github.com/users/xuhdev/events{/privacy}", "received_events_url": "https://api.github.com/users/xuhdev/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-01T23:25:05Z", "updated_at": "2019-05-02T18:38:53Z", "closed_at": "2019-05-02T18:38:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "What is the license of the pretrained models? The downloaded archives do not seem to declare license.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/600", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/600/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/600/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/600/events", "html_url": "https://github.com/google-research/bert/issues/600", "id": 437315348, "node_id": "MDU6SXNzdWU0MzczMTUzNDg=", "number": 600, "title": "BERT Issues with run_squad.py", "user": {"login": "TheGiorg", "id": 29436337, "node_id": "MDQ6VXNlcjI5NDM2MzM3", "avatar_url": "https://avatars1.githubusercontent.com/u/29436337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheGiorg", "html_url": "https://github.com/TheGiorg", "followers_url": "https://api.github.com/users/TheGiorg/followers", "following_url": "https://api.github.com/users/TheGiorg/following{/other_user}", "gists_url": "https://api.github.com/users/TheGiorg/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheGiorg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheGiorg/subscriptions", "organizations_url": "https://api.github.com/users/TheGiorg/orgs", "repos_url": "https://api.github.com/users/TheGiorg/repos", "events_url": "https://api.github.com/users/TheGiorg/events{/privacy}", "received_events_url": "https://api.github.com/users/TheGiorg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-04-25T17:31:41Z", "updated_at": "2020-01-15T15:47:38Z", "closed_at": "2019-05-22T20:26:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello!\r\nI was trying to run the SQuAD 1.1 example (run_squad.py)\r\nI followed the instructions from README.md in [Google BERT GitHub repository](https://github.com/google-research/bert#squad-11).\r\n\r\nMy configuration code is the following: \r\n\r\n```\r\nexport BERT_BASE_DIR=\"$/mnt/c/Users/MyUsername/bert/multi_cased_L-12_H-768_A-12\"\r\nexport SQUAD_DIR=\"$/mnt/c/Users/MyUsername/bert/squad\"\r\npython -u run_squad.py \\\r\n   --vocab_file=$BERT_BASE_DIR/vocab.txt \\\r\n   --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\r\n   --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\r\n   --do_train=True \\\r\n   --train_file=$SQUAD_DIR/train-v1.1.json \\\r\n   --do_predict=True \\\r\n   --predict_file=$SQUAD_DIR/dev-v1.1.json \\\r\n   --train_batch_size=12 \\\r\n   --learning_rate=3e-5 \\\r\n   --num_train_epochs=2.0 \\\r\n   --max_seq_length=384 \\\r\n   --doc_stride=128 \\\r\n   --version_2_with_negative True \\\r\n   --output_dir=/tmp/squad_base/\r\n```\r\nAnd then I got this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"run_squad.py\", line 1283, in <module>\r\n    tf.app.run()\r\n  File \"/home/MyUsername/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_squad.py\", line 1129, in main\r\n    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\r\n  File \"/mnt/c/Users/MyUsername/bert/modeling.py\", line 93, in from_json_file\r\n    text = reader.read()\r\n  File \"/home/MyUsername/.local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 125, in read\r\n    self._preread_check()\r\n  File \"/home/MyUsername/.local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 85, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"/home/MyUsername/.local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\n/bert_config.json; No such file or directoryoundError: $/mnt/c/Users/MyUsername/bert/multi_cased_L-12_H-768_A-12\r\n```\r\nFiles in \"multi_cased_L-12_H-768_A-12\" folder :\r\n```\r\n.../MyUsername/bert/multi_cased_L-12_H-768_A-12$/bert/multi_cased_L-12_H-768_A-12$ ls\r\nbert_config.json  bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.index  bert_model.ckpt.meta  vocab.txt\r\n```\r\n\r\nWhat am I doing wrong? Can someone help me?\r\nThank you in advance.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/580", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/580/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/580/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/580/events", "html_url": "https://github.com/google-research/bert/issues/580", "id": 433181347, "node_id": "MDU6SXNzdWU0MzMxODEzNDc=", "number": 580, "title": "Model Hyper Parameters to change after pretraining on the custom dataset", "user": {"login": "aswin-giridhar", "id": 11817160, "node_id": "MDQ6VXNlcjExODE3MTYw", "avatar_url": "https://avatars0.githubusercontent.com/u/11817160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aswin-giridhar", "html_url": "https://github.com/aswin-giridhar", "followers_url": "https://api.github.com/users/aswin-giridhar/followers", "following_url": "https://api.github.com/users/aswin-giridhar/following{/other_user}", "gists_url": "https://api.github.com/users/aswin-giridhar/gists{/gist_id}", "starred_url": "https://api.github.com/users/aswin-giridhar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aswin-giridhar/subscriptions", "organizations_url": "https://api.github.com/users/aswin-giridhar/orgs", "repos_url": "https://api.github.com/users/aswin-giridhar/repos", "events_url": "https://api.github.com/users/aswin-giridhar/events{/privacy}", "received_events_url": "https://api.github.com/users/aswin-giridhar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-15T09:27:05Z", "updated_at": "2019-04-16T10:20:47Z", "closed_at": "2019-04-16T10:20:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "I had run the pretraining code of bert on a custom dataset and now i want to know which arguments i should change based on the pretrained model. The only arguments which I have changed among the three arguments(vocab_file,config_file,init_checkpoint) is the init_checkpoint which I have given the latest checkpoint created by the pretraining code. But when I tried to run it I was getting the following error.\r\n![image](https://user-images.githubusercontent.com/11817160/56122194-6fabc180-5f8f-11e9-9c2e-a3fa0fee522b.png). \r\n\r\nSo i tried changing the vocab_size in bert_config.json and tried to run it. This is the error which I am getting now.\r\n![image](https://user-images.githubusercontent.com/11817160/56122536-450e3880-5f90-11e9-9ee3-c6575c214225.png)\r\n\r\nCould you tell me the reason why am getting this issue?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/578", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/578/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/578/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/578/events", "html_url": "https://github.com/google-research/bert/issues/578", "id": 432997924, "node_id": "MDU6SXNzdWU0MzI5OTc5MjQ=", "number": 578, "title": "Performance for multi-label classification is so low?", "user": {"login": "XSilverBullet", "id": 9512807, "node_id": "MDQ6VXNlcjk1MTI4MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9512807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XSilverBullet", "html_url": "https://github.com/XSilverBullet", "followers_url": "https://api.github.com/users/XSilverBullet/followers", "following_url": "https://api.github.com/users/XSilverBullet/following{/other_user}", "gists_url": "https://api.github.com/users/XSilverBullet/gists{/gist_id}", "starred_url": "https://api.github.com/users/XSilverBullet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XSilverBullet/subscriptions", "organizations_url": "https://api.github.com/users/XSilverBullet/orgs", "repos_url": "https://api.github.com/users/XSilverBullet/repos", "events_url": "https://api.github.com/users/XSilverBullet/events{/privacy}", "received_events_url": "https://api.github.com/users/XSilverBullet/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-14T16:12:25Z", "updated_at": "2019-10-13T02:09:41Z", "closed_at": "2019-07-19T07:12:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "eval_accuracy = 0.05\r\neval_loss = 0.1684", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/572", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/572/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/572/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/572/events", "html_url": "https://github.com/google-research/bert/issues/572", "id": 431924240, "node_id": "MDU6SXNzdWU0MzE5MjQyNDA=", "number": 572, "title": "bert\u8bcd\u8868vocab.txt\u4e2d\u6c49\u8bed\u7684##\u8868\u793a\u662f\u4ec0\u4e48\u610f\u601d\uff1f\u4e2d\u6587\u8bed\u6599\u5728tokenization\u4e4b\u540e\u597d\u50cf\u4ece\u672a\u51fa\u73b0\u8fc7\u7528##\u524d\u7f00\u8868\u793a\u7684\u5440", "user": {"login": "zhuyuuyuhz", "id": 7614530, "node_id": "MDQ6VXNlcjc2MTQ1MzA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7614530?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuyuuyuhz", "html_url": "https://github.com/zhuyuuyuhz", "followers_url": "https://api.github.com/users/zhuyuuyuhz/followers", "following_url": "https://api.github.com/users/zhuyuuyuhz/following{/other_user}", "gists_url": "https://api.github.com/users/zhuyuuyuhz/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuyuuyuhz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuyuuyuhz/subscriptions", "organizations_url": "https://api.github.com/users/zhuyuuyuhz/orgs", "repos_url": "https://api.github.com/users/zhuyuuyuhz/repos", "events_url": "https://api.github.com/users/zhuyuuyuhz/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuyuuyuhz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-11T09:34:38Z", "updated_at": "2020-06-03T09:34:04Z", "closed_at": "2019-04-16T11:19:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/567", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/567/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/567/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/567/events", "html_url": "https://github.com/google-research/bert/issues/567", "id": 430951834, "node_id": "MDU6SXNzdWU0MzA5NTE4MzQ=", "number": 567, "title": "words from 768 dim output", "user": {"login": "radiodee1", "id": 8641916, "node_id": "MDQ6VXNlcjg2NDE5MTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/8641916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/radiodee1", "html_url": "https://github.com/radiodee1", "followers_url": "https://api.github.com/users/radiodee1/followers", "following_url": "https://api.github.com/users/radiodee1/following{/other_user}", "gists_url": "https://api.github.com/users/radiodee1/gists{/gist_id}", "starred_url": "https://api.github.com/users/radiodee1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/radiodee1/subscriptions", "organizations_url": "https://api.github.com/users/radiodee1/orgs", "repos_url": "https://api.github.com/users/radiodee1/repos", "events_url": "https://api.github.com/users/radiodee1/events{/privacy}", "received_events_url": "https://api.github.com/users/radiodee1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-04-09T12:55:39Z", "updated_at": "2019-04-16T15:16:33Z", "closed_at": "2019-04-16T15:16:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm interested in the smaller bert download. I see that tokenizing input is fairly easy. I would like to get words as output. I'm working on a simple chat-bot type application. If the output is 768  long, do I have to add my own linear layer in order to get one of the 30,000 words in the input vocabulary? Somehow I expect you have faced this type of problem in the past. What is the recomended course of action? Thank you,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/557", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/557/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/557/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/557/events", "html_url": "https://github.com/google-research/bert/issues/557", "id": 429991523, "node_id": "MDU6SXNzdWU0Mjk5OTE1MjM=", "number": 557, "title": "masked_lm_accuracy is low at 0.51, but next_sentence_accuracy is high at 0.93", "user": {"login": "loveJasmine", "id": 18051187, "node_id": "MDQ6VXNlcjE4MDUxMTg3", "avatar_url": "https://avatars2.githubusercontent.com/u/18051187?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loveJasmine", "html_url": "https://github.com/loveJasmine", "followers_url": "https://api.github.com/users/loveJasmine/followers", "following_url": "https://api.github.com/users/loveJasmine/following{/other_user}", "gists_url": "https://api.github.com/users/loveJasmine/gists{/gist_id}", "starred_url": "https://api.github.com/users/loveJasmine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loveJasmine/subscriptions", "organizations_url": "https://api.github.com/users/loveJasmine/orgs", "repos_url": "https://api.github.com/users/loveJasmine/repos", "events_url": "https://api.github.com/users/loveJasmine/events{/privacy}", "received_events_url": "https://api.github.com/users/loveJasmine/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-04-06T03:11:54Z", "updated_at": "2019-10-24T11:56:38Z", "closed_at": "2019-04-12T03:05:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "how to explain that,\r\n\r\nmy training set about 1M line, runing 50000 steps, batchsize is 32", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/556", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/556/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/556/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/556/events", "html_url": "https://github.com/google-research/bert/issues/556", "id": 429952294, "node_id": "MDU6SXNzdWU0Mjk5NTIyOTQ=", "number": 556, "title": "Cannot import run_classifier_with_tfhub", "user": {"login": "sddoliver", "id": 46798062, "node_id": "MDQ6VXNlcjQ2Nzk4MDYy", "avatar_url": "https://avatars2.githubusercontent.com/u/46798062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sddoliver", "html_url": "https://github.com/sddoliver", "followers_url": "https://api.github.com/users/sddoliver/followers", "following_url": "https://api.github.com/users/sddoliver/following{/other_user}", "gists_url": "https://api.github.com/users/sddoliver/gists{/gist_id}", "starred_url": "https://api.github.com/users/sddoliver/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sddoliver/subscriptions", "organizations_url": "https://api.github.com/users/sddoliver/orgs", "repos_url": "https://api.github.com/users/sddoliver/repos", "events_url": "https://api.github.com/users/sddoliver/events{/privacy}", "received_events_url": "https://api.github.com/users/sddoliver/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-05T22:07:09Z", "updated_at": "2019-04-08T15:45:56Z", "closed_at": "2019-04-08T15:43:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm running the tutorial without any changes as posted here, https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb .\r\nThe line `'import run_classifier_with_tfhub'` does not run, and gives a module not found error: 'ModuleNotFoundError: No module named 'run_classifier_with_tfhub''\r\n\r\n\r\nSince I've run the earlier lines without error:\r\n`import sys\r\n\r\n!test -d bert_repo || git clone https://github.com/google-research/bert/blob/master bert_repo\r\nif not 'bert_repo' in sys.path:\r\n  sys.path += ['bert_repo']`\r\n\r\n\r\nI've checked help(bert_repo), and while it does list all of the other functions being imported in here, it does not list run_classifier_with_tfhub. As this issue was in #420 I tried to update bert_repo, `pip install bert_repo --upgrade`, and bert, `pip install bert --upgrade`, but both still give module not found.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/555", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/555/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/555/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/555/events", "html_url": "https://github.com/google-research/bert/issues/555", "id": 429431609, "node_id": "MDU6SXNzdWU0Mjk0MzE2MDk=", "number": 555, "title": "Reproducing README pre-training results", "user": {"login": "jeandalf", "id": 11234341, "node_id": "MDQ6VXNlcjExMjM0MzQx", "avatar_url": "https://avatars2.githubusercontent.com/u/11234341?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeandalf", "html_url": "https://github.com/jeandalf", "followers_url": "https://api.github.com/users/jeandalf/followers", "following_url": "https://api.github.com/users/jeandalf/following{/other_user}", "gists_url": "https://api.github.com/users/jeandalf/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeandalf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeandalf/subscriptions", "organizations_url": "https://api.github.com/users/jeandalf/orgs", "repos_url": "https://api.github.com/users/jeandalf/repos", "events_url": "https://api.github.com/users/jeandalf/events{/privacy}", "received_events_url": "https://api.github.com/users/jeandalf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-04-04T18:53:43Z", "updated_at": "2019-04-12T16:59:13Z", "closed_at": "2019-04-12T16:59:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI am trying to reproduce the pre-training results described in the README (using sample_text  and vocab.txt from uncased_L-12_H-768_A-12). Unfortunately, I don't see the good numbers reported in the README.\r\n\r\nHere are my numbers:\r\nINFO:tensorflow:***** Eval results *****\r\nINFO:tensorflow:  global_step = 20\r\nINFO:tensorflow:  loss = 9.203541\r\nINFO:tensorflow:  masked_lm_accuracy = 0.06743769\r\nINFO:tensorflow:  masked_lm_loss = 8.508136\r\nINFO:tensorflow:  next_sentence_accuracy = 0.53625\r\nINFO:tensorflow:  next_sentence_loss = 0.6883839\r\n\r\nMy commands:\r\n1)  root:~/bert# python3 bert/create_pretraining_data.py --input_file=sample_text.txt --output_file=tf_examples.tfrecord --vocab_file=uncased_L-12_H-768_A-12/vo\r\ncab.txt --do_lower_case=True --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5\r\n\r\n2)  root:~/bert# python3 bert/run_pretraining.py --input_file=tf_examples.tfrecord --output_dir=pretraining_output --do_train=True --do_eval=True --bert_config_file=uncased_L-12_H-768_A-12/bert_config.json --init_checkpoint=uncased_L-12_H-768_A-12/bert_model.ckpt.index --train_batch_size=32 --max_seq_length=128 --max_predictions_per_seq=20 --num_train_steps=20 --num_warmup_steps=10 --learning_rate=2e-5\r\n\r\nWhat am I missing?\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/547", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/547/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/547/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/547/events", "html_url": "https://github.com/google-research/bert/issues/547", "id": 429066836, "node_id": "MDU6SXNzdWU0MjkwNjY4MzY=", "number": 547, "title": "AttributeError: module 'tensorflow.contrib.tpu' has no attribute 'InputPipelineConfig'", "user": {"login": "djh123", "id": 13375703, "node_id": "MDQ6VXNlcjEzMzc1NzAz", "avatar_url": "https://avatars2.githubusercontent.com/u/13375703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/djh123", "html_url": "https://github.com/djh123", "followers_url": "https://api.github.com/users/djh123/followers", "following_url": "https://api.github.com/users/djh123/following{/other_user}", "gists_url": "https://api.github.com/users/djh123/gists{/gist_id}", "starred_url": "https://api.github.com/users/djh123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/djh123/subscriptions", "organizations_url": "https://api.github.com/users/djh123/orgs", "repos_url": "https://api.github.com/users/djh123/repos", "events_url": "https://api.github.com/users/djh123/events{/privacy}", "received_events_url": "https://api.github.com/users/djh123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-04T03:30:31Z", "updated_at": "2020-03-08T03:04:09Z", "closed_at": "2019-04-04T03:34:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "AttributeError: module 'tensorflow.contrib.tpu' has no attribute 'InputPipelineConfig' \r\n\r\nin tensorflow-gpu==1.7 ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/524", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/524/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/524/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/524/events", "html_url": "https://github.com/google-research/bert/issues/524", "id": 425267771, "node_id": "MDU6SXNzdWU0MjUyNjc3NzE=", "number": 524, "title": "How to get the previous layers output of the BERT using tf_hub?", "user": {"login": "saikrishna9494", "id": 38318115, "node_id": "MDQ6VXNlcjM4MzE4MTE1", "avatar_url": "https://avatars1.githubusercontent.com/u/38318115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saikrishna9494", "html_url": "https://github.com/saikrishna9494", "followers_url": "https://api.github.com/users/saikrishna9494/followers", "following_url": "https://api.github.com/users/saikrishna9494/following{/other_user}", "gists_url": "https://api.github.com/users/saikrishna9494/gists{/gist_id}", "starred_url": "https://api.github.com/users/saikrishna9494/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saikrishna9494/subscriptions", "organizations_url": "https://api.github.com/users/saikrishna9494/orgs", "repos_url": "https://api.github.com/users/saikrishna9494/repos", "events_url": "https://api.github.com/users/saikrishna9494/events{/privacy}", "received_events_url": "https://api.github.com/users/saikrishna9494/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-03-26T07:40:29Z", "updated_at": "2020-02-07T12:55:04Z", "closed_at": "2020-02-07T12:55:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I need the last 4 layers of the bert output to get the embeddings of each token. I can get those from the source code. Is there a way to get these outputs from the Tf_hub as described in the collab? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/511", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/511/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/511/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/511/events", "html_url": "https://github.com/google-research/bert/issues/511", "id": 423810204, "node_id": "MDU6SXNzdWU0MjM4MTAyMDQ=", "number": 511, "title": "Best performance on concatenated layers: which dimension?", "user": {"login": "BramVanroy", "id": 2779410, "node_id": "MDQ6VXNlcjI3Nzk0MTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/2779410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BramVanroy", "html_url": "https://github.com/BramVanroy", "followers_url": "https://api.github.com/users/BramVanroy/followers", "following_url": "https://api.github.com/users/BramVanroy/following{/other_user}", "gists_url": "https://api.github.com/users/BramVanroy/gists{/gist_id}", "starred_url": "https://api.github.com/users/BramVanroy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BramVanroy/subscriptions", "organizations_url": "https://api.github.com/users/BramVanroy/orgs", "repos_url": "https://api.github.com/users/BramVanroy/repos", "events_url": "https://api.github.com/users/BramVanroy/events{/privacy}", "received_events_url": "https://api.github.com/users/BramVanroy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-03-21T16:01:23Z", "updated_at": "2020-05-06T14:59:13Z", "closed_at": "2019-03-22T10:55:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "In [your paper ](https://arxiv.org/abs/1810.04805) (section 5.4, table 7) you indicate that concatenating the last four layers gave the best performance but details are scarce. I am not sure how to see this, i.e. over which axis the concatenation would take place.\r\n\r\nAssume the output is of a layer is `batch_size, seq_length, hidden`. For a batch of one sentence with 8 tokens, that would be `1, 8, 1024`. When you concatenate them over a default dim=0 that would lead to `batch_size*layers, seq_length, hidden`, where layers is the amount of layers that you concatenate. But concatenating over the first dimensions doesn't seem to make sense. So my question is, on which dimension do you concatenate? My guess would be the last one, leaving you with `batch_size, seq_length, hidden*layers`. Is this correct?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/509", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/509/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/509/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/509/events", "html_url": "https://github.com/google-research/bert/issues/509", "id": 423616664, "node_id": "MDU6SXNzdWU0MjM2MTY2NjQ=", "number": 509, "title": "Error recorded from training_loop: Found Inf or NaN global norm. : Tensor had NaN values", "user": {"login": "zjwzcn07", "id": 15607152, "node_id": "MDQ6VXNlcjE1NjA3MTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/15607152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zjwzcn07", "html_url": "https://github.com/zjwzcn07", "followers_url": "https://api.github.com/users/zjwzcn07/followers", "following_url": "https://api.github.com/users/zjwzcn07/following{/other_user}", "gists_url": "https://api.github.com/users/zjwzcn07/gists{/gist_id}", "starred_url": "https://api.github.com/users/zjwzcn07/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zjwzcn07/subscriptions", "organizations_url": "https://api.github.com/users/zjwzcn07/orgs", "repos_url": "https://api.github.com/users/zjwzcn07/repos", "events_url": "https://api.github.com/users/zjwzcn07/events{/privacy}", "received_events_url": "https://api.github.com/users/zjwzcn07/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-03-21T08:32:22Z", "updated_at": "2019-05-16T11:01:48Z", "closed_at": "2019-05-12T05:27:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n**I try to run run_classifier.sh  in cpu, it runs ok. But when I run in gpu, sometimes good, sometimes bad.** I think it might have something to do with seq_max_length and batch_size. But it's useless to reduce seq_max_length and batch_size. (I've tried it.)\r\n\r\nfollowing is my run_classifier.sh\r\n> python3.5 run_classifier.py \\\r\n  --task_name=sim \\\r\n  --do_train=true \\\r\n  --do_eval=true \\\r\n  --do_predict=true \\\r\n  --data_dir=$MY_DATASET \\\r\n  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\r\n  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\r\n  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\r\n  --max_seq_length=128 \\\r\n  --train_batch_size=32 \\\r\n  --learning_rate=2e-5 \\\r\n  --num_train_epochs=2.0 \\\r\n\r\n\r\nfollowing is error log\r\n\r\n> > 2019-03-21 14:21:12.342706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-03-21 14:21:13.673814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-03-21 14:21:13.673891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 2 3\r\n2019-03-21 14:21:13.673904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N N N N\r\n2019-03-21 14:21:13.673912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N N N N\r\n2019-03-21 14:21:13.673918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 2:   N N N N\r\n2019-03-21 14:21:13.673925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 3:   N N N N\r\n2019-03-21 14:21:13.675250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15119 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pc\r\ni bus id: 0000:00:07.0, compute capability: 6.0)\r\n2019-03-21 14:21:13.676003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 15119 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pc\r\ni bus id: 0000:00:08.0, compute capability: 6.0)\r\n2019-03-21 14:21:13.676420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 15119 MB memory) -> physical GPU (device: 2, name: Tesla P100-PCIE-16GB, pc\r\ni bus id: 0000:00:09.0, compute capability: 6.0)\r\n2019-03-21 14:21:13.676764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 15119 MB memory) -> physical GPU (device: 3, name: Tesla P100-PCIE-16GB, pc\r\ni bus id: 0000:00:0a.0, compute capability: 6.0)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into ./mayi_model_2/model.ckpt.\r\n2019-03-21 14:22:44.995671: E tensorflow/core/kernels/check_numerics_op.cc:185] abnormal_detected_host @0x1085900ab00 = {1, 0} Found Inf or NaN global norm.\r\nINFO:tensorflow:Error recorded from training_loop: Found Inf or NaN global norm. : Tensor had NaN values\r\n         [[node VerifyFinite/CheckNumerics (defined at /var/log//bert_chi/optimization.py:74)  = CheckNumerics[T=DT_FLOAT, message=\"Found Inf or NaN global norm.\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](global_n\r\norm/global_norm)]]", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/500", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/500/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/500/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/500/events", "html_url": "https://github.com/google-research/bert/issues/500", "id": 421179478, "node_id": "MDU6SXNzdWU0MjExNzk0Nzg=", "number": 500, "title": "Poor performance on squad 2 evaluation performed outside of the GPU(s)", "user": {"login": "mfeblowitz", "id": 6854939, "node_id": "MDQ6VXNlcjY4NTQ5Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/6854939?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfeblowitz", "html_url": "https://github.com/mfeblowitz", "followers_url": "https://api.github.com/users/mfeblowitz/followers", "following_url": "https://api.github.com/users/mfeblowitz/following{/other_user}", "gists_url": "https://api.github.com/users/mfeblowitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfeblowitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfeblowitz/subscriptions", "organizations_url": "https://api.github.com/users/mfeblowitz/orgs", "repos_url": "https://api.github.com/users/mfeblowitz/repos", "events_url": "https://api.github.com/users/mfeblowitz/events{/privacy}", "received_events_url": "https://api.github.com/users/mfeblowitz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-14T18:29:21Z", "updated_at": "2019-06-04T19:36:44Z", "closed_at": "2019-06-04T19:36:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have noticed that a significant amount of the bert-squad 2 processing that occurs outside of the GPU (lead-up to GPU-based tf processing) occurs on a single core and can take a great deal of time.  Monitoring the GPU and CPU utilization,  I can see the GPU being inactive and a single CPU being pegged. That eventually switches to GPU usage with nearly no CPU utilization. \r\n\r\nI've tried profiling both my preparatory code and the bert run_squad.py code, and can only confirm that it occurs in tf. \r\n\r\nI am guessing that tf is executing the cpu-side code in a single thread. Also, I note that some hosts have very wimpy cores, adding even more time to the cumulative time.\r\nIs there any user-configurable/tunable setting that can change this behavior? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/497", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/497/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/497/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/497/events", "html_url": "https://github.com/google-research/bert/issues/497", "id": 420319792, "node_id": "MDU6SXNzdWU0MjAzMTk3OTI=", "number": 497, "title": "xnli fin-tunining downlink invalid", "user": {"login": "yelunightroad", "id": 10142040, "node_id": "MDQ6VXNlcjEwMTQyMDQw", "avatar_url": "https://avatars2.githubusercontent.com/u/10142040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yelunightroad", "html_url": "https://github.com/yelunightroad", "followers_url": "https://api.github.com/users/yelunightroad/followers", "following_url": "https://api.github.com/users/yelunightroad/following{/other_user}", "gists_url": "https://api.github.com/users/yelunightroad/gists{/gist_id}", "starred_url": "https://api.github.com/users/yelunightroad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yelunightroad/subscriptions", "organizations_url": "https://api.github.com/users/yelunightroad/orgs", "repos_url": "https://api.github.com/users/yelunightroad/repos", "events_url": "https://api.github.com/users/yelunightroad/events{/privacy}", "received_events_url": "https://api.github.com/users/yelunightroad/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-13T05:09:14Z", "updated_at": "2019-07-19T08:25:38Z", "closed_at": "2019-07-19T08:25:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "i want test fine tuning example at https://github.com/google-research/bert/blob/master/multilingual.md, howerer the dataset download link https://s3.amazonaws.com/xnli/XNLI-1.0.zip doesn't work, where can i get the data, i download the xnli at https://github.com/facebookresearch/XNLI , it now required.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/494", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/494/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/494/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/494/events", "html_url": "https://github.com/google-research/bert/issues/494", "id": 420047282, "node_id": "MDU6SXNzdWU0MjAwNDcyODI=", "number": 494, "title": "How to test QA model using own input? (Single prediction question)", "user": {"login": "SSaishruthi", "id": 22990801, "node_id": "MDQ6VXNlcjIyOTkwODAx", "avatar_url": "https://avatars0.githubusercontent.com/u/22990801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SSaishruthi", "html_url": "https://github.com/SSaishruthi", "followers_url": "https://api.github.com/users/SSaishruthi/followers", "following_url": "https://api.github.com/users/SSaishruthi/following{/other_user}", "gists_url": "https://api.github.com/users/SSaishruthi/gists{/gist_id}", "starred_url": "https://api.github.com/users/SSaishruthi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SSaishruthi/subscriptions", "organizations_url": "https://api.github.com/users/SSaishruthi/orgs", "repos_url": "https://api.github.com/users/SSaishruthi/repos", "events_url": "https://api.github.com/users/SSaishruthi/events{/privacy}", "received_events_url": "https://api.github.com/users/SSaishruthi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-12T15:22:29Z", "updated_at": "2019-05-10T11:03:56Z", "closed_at": "2019-03-12T18:19:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was able to use `run_squad.py` for dev dataset. Now, how to use it for single user question input?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/487", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/487/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/487/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/487/events", "html_url": "https://github.com/google-research/bert/issues/487", "id": 418837627, "node_id": "MDU6SXNzdWU0MTg4Mzc2Mjc=", "number": 487, "title": "BERT Sentence Classification Error: Read less bytes than requested", "user": {"login": "abhimishra91", "id": 27291199, "node_id": "MDQ6VXNlcjI3MjkxMTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/27291199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhimishra91", "html_url": "https://github.com/abhimishra91", "followers_url": "https://api.github.com/users/abhimishra91/followers", "following_url": "https://api.github.com/users/abhimishra91/following{/other_user}", "gists_url": "https://api.github.com/users/abhimishra91/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhimishra91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhimishra91/subscriptions", "organizations_url": "https://api.github.com/users/abhimishra91/orgs", "repos_url": "https://api.github.com/users/abhimishra91/repos", "events_url": "https://api.github.com/users/abhimishra91/events{/privacy}", "received_events_url": "https://api.github.com/users/abhimishra91/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-03-08T15:36:45Z", "updated_at": "2019-05-01T07:47:02Z", "closed_at": "2019-05-01T07:47:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I have been trying to use Bert for a multi-class classification problem but running error.\r\n\r\nI have tried to scale up the capacity of my AWS instance, reduce the batch size, sequence length as suggested on the github to resolve the issue if it was due to memory or machine capacity. But it did not help.\r\n\r\nThe error message below:\r\n\r\n```\r\nINFO:tensorflow:Error recorded from training_loop: Read less bytes than requested \r\n[[{{node checkpoint_initializer_120}} = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](checkpoint_initializer/prefix, checkpoint_initializer_120/tensor_names, checkpoint_initializer/shape_and_slices)]]\r\n```\r\n\r\nThe complete stack trace from the execution is:\r\n```\r\nWARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f7b12aafd08>) includes params argument, but params are not passed to Estimator.\r\nINFO:tensorflow:Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7b138d1b38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': None}\r\nINFO:tensorflow:_TPUContext: eval_on_tpu True\r\nWARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\r\nINFO:tensorflow:Writing example 0 of 59336\r\nINFO:tensorflow:*** Example ***\r\nINFO:tensorflow:guid: train-0\r\nINFO:tensorflow:tokens: [CLS] i ##hs mark ##it ##wire implementation form request [SEP]\r\nINFO:tensorflow:input_ids: 101 1045 7898 2928 4183 20357 7375 2433 5227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:label: 0 (id = 0)\r\nINFO:tensorflow:*** Example ***\r\nINFO:tensorflow:guid: train-1\r\nINFO:tensorflow:tokens: [CLS] rev ##oke mark ##it ##wire access request jp ##m reference 2018 ##0 ##7 ##19 ##6 ##29 ##47 68 [SEP]\r\nINFO:tensorflow:input_ids: 101 7065 11045 2928 4183 20357 3229 5227 16545 2213 4431 2760 2692 2581 16147 2575 24594 22610 6273 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:label: 1 (id = 1)\r\nINFO:tensorflow:*** Example ***\r\nINFO:tensorflow:guid: train-2\r\nINFO:tensorflow:tokens: [CLS] credentials required in mw ua ##t en ##v [SEP]\r\nINFO:tensorflow:input_ids: 101 22496 3223 1999 12464 25423 2102 4372 2615 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:label: 2 (id = 2)\r\nINFO:tensorflow:*** Example ***\r\nINFO:tensorflow:guid: train-3\r\nINFO:tensorflow:tokens: [CLS] incorrect mic code regulatory reporting failure i [SEP]\r\nINFO:tensorflow:input_ids: 101 16542 23025 3642 10738 7316 4945 1045 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:label: 3 (id = 3)\r\nINFO:tensorflow:*** Example ***\r\nINFO:tensorflow:guid: train-4\r\nINFO:tensorflow:tokens: [CLS] book access [SEP]\r\nINFO:tensorflow:input_ids: 101 2338 3229 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:input_mask: 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\r\nINFO:tensorflow:label: 4 (id = 4)\r\nINFO:tensorflow:Writing example 10000 of 59336\r\nINFO:tensorflow:Writing example 20000 of 59336\r\nINFO:tensorflow:Writing example 30000 of 59336\r\nINFO:tensorflow:Writing example 40000 of 59336\r\nINFO:tensorflow:Writing example 50000 of 59336\r\nINFO:tensorflow:***** Running training *****\r\nINFO:tensorflow:  Num examples = 59336\r\nINFO:tensorflow:  Batch size = 64\r\nINFO:tensorflow:  Num steps = 2781\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Running train on CPU\r\nINFO:tensorflow:*** Features ***\r\nINFO:tensorflow:  name = input_ids, shape = (64, 64)\r\nINFO:tensorflow:  name = input_mask, shape = (64, 64)\r\nINFO:tensorflow:  name = is_real_example, shape = (64,)\r\nINFO:tensorflow:  name = label_ids, shape = (64,)\r\nINFO:tensorflow:  name = segment_ids, shape = (64, 64)\r\nINFO:tensorflow:**** Trainable Variables ****\r\nINFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\nINFO:tensorflow:  name = output_weights:0, shape = (15, 768)\r\nINFO:tensorflow:  name = output_bias:0, shape = (15,)\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n2019-03-08 09:33:58.848883: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-03-08 09:34:05.402211: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.403029: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.404072: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.404207: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.404717: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.405412: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.405733: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.407607: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.407979: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.408082: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.410115: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.410560: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.410977: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.412421: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.412653: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.412719: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.413482: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.414399: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.415459: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.416538: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.416960: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.418507: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.418619: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.420532: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.420627: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.421148: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.421602: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.423226: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.423243: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.423430: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.425294: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.425699: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.426459: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.428057: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.428145: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.428263: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.429847: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.429847: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.430076: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.431549: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.431814: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.437967: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.439428: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.439656: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.442647: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.442879: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.443639: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.445226: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.445347: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.445415: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.447350: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.448589: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.448920: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.449484: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.449929: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.450607: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.452014: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.452417: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.452782: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.452962: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.456138: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.456401: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.456657: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.457917: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.458289: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.458471: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.459966: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.460290: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.460732: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.461697: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.463086: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.463342: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.464554: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.465380: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.465428: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.466314: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.466902: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.467497: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\n2019-03-08 09:34:05.467929: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read less bytes than requested\r\nINFO:tensorflow:Error recorded from training_loop: Read less bytes than requested\r\n\t [[{{node checkpoint_initializer_120}} = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](checkpoint_initializer/prefix, checkpoint_initializer_120/tensor_names, checkpoint_initializer/shape_and_slices)]]\r\n\r\nCaused by op 'checkpoint_initializer_120', defined at:\r\n  File \"run_classifier.py\", line 981, in <module>\r\n    tf.app.run()\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_classifier.py\", line 880, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2394, in train\r\n    saving_listeners=saving_listeners\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1211, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2186, in _call_model_fn\r\n    features, labels, mode, config)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1169, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2470, in _model_fn\r\n    features, labels, is_export_mode=is_export_mode)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1250, in call_without_tpu\r\n    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1524, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"run_classifier.py\", line 661, in model_fn\r\n    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 187, in init_from_checkpoint\r\n    _init_from_checkpoint, ckpt_dir_or_file, assignment_map)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 1040, in merge_call\r\n    return self._merge_call(merge_fn, *args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 1048, in _merge_call\r\n    return merge_fn(self._distribution_strategy, *args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 231, in _init_from_checkpoint\r\n    _set_variable_or_list_initializer(var, ckpt_file, tensor_name_in_ckpt)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 355, in _set_variable_or_list_initializer\r\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 309, in _set_checkpoint_initializer\r\n    ckpt_file, [tensor_name], [slice_spec], [base_type], name=name)[0]\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): Read less bytes than requested\r\n\t [[{{node checkpoint_initializer_120}} = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](checkpoint_initializer/prefix, checkpoint_initializer_120/tensor_names, checkpoint_initializer/shape_and_slices)]]\r\n\r\nINFO:tensorflow:training_loop marked as finished\r\nWARNING:tensorflow:Reraising captured error\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested\r\n\t [[{{node checkpoint_initializer_120}} = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](checkpoint_initializer/prefix, checkpoint_initializer_120/tensor_names, checkpoint_initializer/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"run_classifier.py\", line 981, in <module>\r\n    tf.app.run()\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_classifier.py\", line 880, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2400, in train\r\n    rendezvous.raise_errors()\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py\", line 128, in raise_errors\r\n    six.reraise(typ, value, traceback)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2394, in train\r\n    saving_listeners=saving_listeners\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1215, in _train_model_default\r\n    saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1406, in _train_with_estimator_spec\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 566, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py\", line 287, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested\r\n\t [[{{node checkpoint_initializer_120}} = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](checkpoint_initializer/prefix, checkpoint_initializer_120/tensor_names, checkpoint_initializer/shape_and_slices)]]\r\n\r\nCaused by op 'checkpoint_initializer_120', defined at:\r\n  File \"run_classifier.py\", line 981, in <module>\r\n    tf.app.run()\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"run_classifier.py\", line 880, in main\r\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2394, in train\r\n    saving_listeners=saving_listeners\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1211, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2186, in _call_model_fn\r\n    features, labels, mode, config)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1169, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 2470, in _model_fn\r\n    features, labels, is_export_mode=is_export_mode)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1250, in call_without_tpu\r\n    return self._call_model_fn(features, labels, is_export_mode=is_export_mode)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py\", line 1524, in _call_model_fn\r\n    estimator_spec = self._model_fn(features=features, **kwargs)\r\n  File \"run_classifier.py\", line 661, in model_fn\r\n    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 187, in init_from_checkpoint\r\n    _init_from_checkpoint, ckpt_dir_or_file, assignment_map)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 1040, in merge_call\r\n    return self._merge_call(merge_fn, *args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 1048, in _merge_call\r\n    return merge_fn(self._distribution_strategy, *args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 231, in _init_from_checkpoint\r\n    _set_variable_or_list_initializer(var, ckpt_file, tensor_name_in_ckpt)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 355, in _set_variable_or_list_initializer\r\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 309, in _set_checkpoint_initializer\r\n    ckpt_file, [tensor_name], [slice_spec], [base_type], name=name)[0]\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1466, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): Read less bytes than requested\r\n\t [[{{node checkpoint_initializer_120}} = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](checkpoint_initializer/prefix, checkpoint_initializer_120/tensor_names, checkpoint_initializer/shape_and_slices)]]\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/480", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/480/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/480/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/480/events", "html_url": "https://github.com/google-research/bert/issues/480", "id": 417592085, "node_id": "MDU6SXNzdWU0MTc1OTIwODU=", "number": 480, "title": "Trying to run SQuAD with accumulated gradients but get killed automatically", "user": {"login": "lxylxyoo", "id": 33095332, "node_id": "MDQ6VXNlcjMzMDk1MzMy", "avatar_url": "https://avatars1.githubusercontent.com/u/33095332?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lxylxyoo", "html_url": "https://github.com/lxylxyoo", "followers_url": "https://api.github.com/users/lxylxyoo/followers", "following_url": "https://api.github.com/users/lxylxyoo/following{/other_user}", "gists_url": "https://api.github.com/users/lxylxyoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/lxylxyoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lxylxyoo/subscriptions", "organizations_url": "https://api.github.com/users/lxylxyoo/orgs", "repos_url": "https://api.github.com/users/lxylxyoo/repos", "events_url": "https://api.github.com/users/lxylxyoo/events{/privacy}", "received_events_url": "https://api.github.com/users/lxylxyoo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-03-06T02:25:07Z", "updated_at": "2019-03-07T09:53:33Z", "closed_at": "2019-03-07T09:53:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "I try to revise the code in optimization.create_optimizer as:\r\n\r\n```python\r\n  grads = optimizer.compute_gradients(loss, tvars)\r\n  accum_vars =  [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) \r\n                  for tv in tvars\r\n                ]\r\n  no_ops = []\r\n  accum_ops = [accum_vars[i].assign_add(gv[0]) \r\n                for i, gv in enumerate(grads) \r\n                if gv[0] is not None  and gv[1] is not None\r\n              ]\r\n  train_step = optimizer.apply_gradients(\r\n              [\r\n                (accum_vars[i].assign_add(gv[0]), gv[1]) / 3\r\n                for i, gv in enumerate(grads)\r\n                if gv[0] is not None and gv[1] is not None\r\n              ],\r\n              global_step=global_step)\r\n\r\n  with tf.get_default_graph().control_dependencies(train_step):\r\n    zero_ops = [tv.assign(tf.zeros_like(tv)) \r\n                for tv in accum_vars]\r\n\r\n  for _ in range(len(zero_ops) + len(train_step) - len(accum_ops)):\r\n    no_ops.append(accum_vars[0].assign_add(tf.zeros_like(accum_vars[0])))\r\n\r\n  new_global_step = global_step + 1\r\n\r\n  train_op = tf.group(\r\n              tf.cond(tf.equal((global_step + 1)% 3, 0), \r\n                        lambda :train_step + zero_ops, \r\n                        lambda: accum_ops + no_ops), \r\n            [global_step.assign(new_global_step)])\r\n```\r\n\r\n\r\nand after saving checkpoints for 0 into model.ckpt, it seems get stuck and killed after a while without any error information .\r\nCould anyone tell me what happened and how to revise it?\r\nThanks!  \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/google-research/bert/issues/479", "repository_url": "https://api.github.com/repos/google-research/bert", "labels_url": "https://api.github.com/repos/google-research/bert/issues/479/labels{/name}", "comments_url": "https://api.github.com/repos/google-research/bert/issues/479/comments", "events_url": "https://api.github.com/repos/google-research/bert/issues/479/events", "html_url": "https://github.com/google-research/bert/issues/479", "id": 417545094, "node_id": "MDU6SXNzdWU0MTc1NDUwOTQ=", "number": 479, "title": "Tensor wrong shape (swag)", "user": {"login": "klxiao", "id": 35232167, "node_id": "MDQ6VXNlcjM1MjMyMTY3", "avatar_url": "https://avatars2.githubusercontent.com/u/35232167?v=4", "gravatar_id": "", "url": "https://api.github.com/users/klxiao", "html_url": "https://github.com/klxiao", "followers_url": "https://api.github.com/users/klxiao/followers", "following_url": "https://api.github.com/users/klxiao/following{/other_user}", "gists_url": "https://api.github.com/users/klxiao/gists{/gist_id}", "starred_url": "https://api.github.com/users/klxiao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/klxiao/subscriptions", "organizations_url": "https://api.github.com/users/klxiao/orgs", "repos_url": "https://api.github.com/users/klxiao/repos", "events_url": "https://api.github.com/users/klxiao/events{/privacy}", "received_events_url": "https://api.github.com/users/klxiao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-05T23:11:21Z", "updated_at": "2019-03-20T09:54:41Z", "closed_at": "2019-03-07T17:52:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried following the instructions in [@38](https://github.com/google-research/bert/issues/38) but ran into the following bug when computing the tensors:\r\nValueError: Cannot reshape a tensor with 128 elements to shape [8,4] (32 elements) for 'loss/Reshape' (op: 'Reshape') with input shapes: [32,4], [2] and with input tensors computed as partial shapes: input[1] = [8,4].\r\n\r\nTo note, my input_ids/input_masks/segment_id's all have shape [4,max_seq_length], my model_fn rehapes to [32, max_seq_length], and I used the exact code for computing logits as run_classifier.py except that I reshaped logits to be [8,4], which resulted in the aforementioned error.", "performed_via_github_app": null, "score": 1.0}]}