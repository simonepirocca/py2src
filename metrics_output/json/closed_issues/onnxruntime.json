{"total_count": 1123, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5082", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5082/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5082/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5082/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/5082", "id": 696083724, "node_id": "MDU6SXNzdWU2OTYwODM3MjQ=", "number": 5082, "title": "ONNXRuntime java library should include libgomp.so.1", "user": {"login": "frankfliu", "id": 24425551, "node_id": "MDQ6VXNlcjI0NDI1NTUx", "avatar_url": "https://avatars1.githubusercontent.com/u/24425551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frankfliu", "html_url": "https://github.com/frankfliu", "followers_url": "https://api.github.com/users/frankfliu/followers", "following_url": "https://api.github.com/users/frankfliu/following{/other_user}", "gists_url": "https://api.github.com/users/frankfliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/frankfliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frankfliu/subscriptions", "organizations_url": "https://api.github.com/users/frankfliu/orgs", "repos_url": "https://api.github.com/users/frankfliu/repos", "events_url": "https://api.github.com/users/frankfliu/events{/privacy}", "received_events_url": "https://api.github.com/users/frankfliu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-09-08T18:22:55Z", "updated_at": "2020-09-08T19:14:14Z", "closed_at": "2020-09-08T19:14:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI'm using onnxruntime java 1.4.0 on ubuntu 16.04. My system doesn't have gcc installed.\r\nI got UnsatisfiedLinkError due to missing libgomp.so.1 file. I have to manually install gcc first before I can use onnxruntime.\r\n\r\n**Urgency**\r\nThis makes it hard to redistribute the jar with onnxruntime. Extra requirement/documentation is required for the project depends on onnxruntime.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- ONNX Runtime installed from: maven central\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: N/A\r\n- Visual Studio version (if applicable): N/A\r\n- GCC/Compiler version (if compiling from source): Not installed\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**To Reproduce**\r\nOn a fresh ubuntu 16.04 machine or use ubuntu:16.04 docker container, create a java project depends on onnxrutime. Runt java project will failed to run: OrtEnvironment.getEnvironment()\r\n\r\n**Expected behavior**\r\nThe java program should run at any environment without extra installation of GCC\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5076", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5076/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5076/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5076/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/5076", "id": 694973043, "node_id": "MDU6SXNzdWU2OTQ5NzMwNDM=", "number": 5076, "title": "Inference by C++ GPU", "user": {"login": "zsy9512", "id": 30277703, "node_id": "MDQ6VXNlcjMwMjc3NzAz", "avatar_url": "https://avatars3.githubusercontent.com/u/30277703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zsy9512", "html_url": "https://github.com/zsy9512", "followers_url": "https://api.github.com/users/zsy9512/followers", "following_url": "https://api.github.com/users/zsy9512/following{/other_user}", "gists_url": "https://api.github.com/users/zsy9512/gists{/gist_id}", "starred_url": "https://api.github.com/users/zsy9512/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zsy9512/subscriptions", "organizations_url": "https://api.github.com/users/zsy9512/orgs", "repos_url": "https://api.github.com/users/zsy9512/repos", "events_url": "https://api.github.com/users/zsy9512/events{/privacy}", "received_events_url": "https://api.github.com/users/zsy9512/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-09-07T10:46:06Z", "updated_at": "2020-09-10T03:00:44Z", "closed_at": "2020-09-10T03:00:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi. By referring to your example, I have successfully run my C++ inference demo in CPU mode.But I can't very understand how to run my cuda demo by using onnx model.Could you please  provided me some good examples or told me how to find the API documents to acheive my target? And do you provide the multi-image test on onnx func? I can find these infomation in the web ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5048", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5048/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5048/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5048/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/5048", "id": 691687740, "node_id": "MDU6SXNzdWU2OTE2ODc3NDA=", "number": 5048, "title": "c++ complie and run", "user": {"login": "chen1399", "id": 20791416, "node_id": "MDQ6VXNlcjIwNzkxNDE2", "avatar_url": "https://avatars1.githubusercontent.com/u/20791416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chen1399", "html_url": "https://github.com/chen1399", "followers_url": "https://api.github.com/users/chen1399/followers", "following_url": "https://api.github.com/users/chen1399/following{/other_user}", "gists_url": "https://api.github.com/users/chen1399/gists{/gist_id}", "starred_url": "https://api.github.com/users/chen1399/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chen1399/subscriptions", "organizations_url": "https://api.github.com/users/chen1399/orgs", "repos_url": "https://api.github.com/users/chen1399/repos", "events_url": "https://api.github.com/users/chen1399/events{/privacy}", "received_events_url": "https://api.github.com/users/chen1399/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-09-03T06:38:48Z", "updated_at": "2020-09-03T06:59:58Z", "closed_at": "2020-09-03T06:59:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nc++ complie and run error\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu\r\n- ONNX Runtime installed from (source or binary):onnxruntime-linux-x64-1.4.0.tgz\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0\r\n**To Reproduce**\r\ncomplie: g++ eval.cpp -o app -Lonnxruntime-linux-x64-1.4.0/lib/libonnxruntime.so -Ionnxruntime-linux-x64-1.4.0/include/\r\nthere is error: eval.cpp:(.text+0x203): undefined reference to `OrtGetApiBase'\r\nthen complie: g++ eval.cpp -o app -Lonnxruntime-linux-x64-1.4.0/lib/libonnxruntime.so -Ionnxruntime-linux-x64-1.4.0/include/ -lm -D EXCLUDE_REFERENCE_TO_ORT_DLL\r\ncomplie no error, but run error: Segmentation fault.\r\n``` \r\n#include <assert.h>\r\n#include <vector>\r\n#include \"onnxruntime_cxx_api.h\"\r\n\r\nint main(int argc, char* argv[]){\r\n    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"test\");\r\n    // Ort::SessionOptions session_options;\r\n    // session_options.SetIntraOpNumThreads(1);\r\n\r\n    // session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);\r\n    // const char* model_path = \"encoder.onnx\";\r\n    // Ort::Session session(env, model_path, session_options);\r\n\r\n    // Ort::AllocatorWithDefaultOptions allocator;\r\n\r\n    // // print number of model input nodes\r\n    // size_t num_input_nodes = session.GetInputCount();\r\n    // std::vector<const char*> input_node_names(num_input_nodes);\r\n    // std::vector<int64_t> input_node_dims;  // simplify... this model has only 1 input node {1, 3, 224, 224}.\r\n    //                                         // Otherwise need vector<vector<>>\r\n    // printf(\"Number of inputs = %zu\\n\", num_input_nodes);\r\n\r\n    return 0;\r\n}\r\n```\r\nerror from 'Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"test\");'\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5047", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5047/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5047/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5047/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/5047", "id": 691618459, "node_id": "MDU6SXNzdWU2OTE2MTg0NTk=", "number": 5047, "title": "Export set provider API in C++", "user": {"login": "tkclimb", "id": 14227047, "node_id": "MDQ6VXNlcjE0MjI3MDQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/14227047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tkclimb", "html_url": "https://github.com/tkclimb", "followers_url": "https://api.github.com/users/tkclimb/followers", "following_url": "https://api.github.com/users/tkclimb/following{/other_user}", "gists_url": "https://api.github.com/users/tkclimb/gists{/gist_id}", "starred_url": "https://api.github.com/users/tkclimb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tkclimb/subscriptions", "organizations_url": "https://api.github.com/users/tkclimb/orgs", "repos_url": "https://api.github.com/users/tkclimb/repos", "events_url": "https://api.github.com/users/tkclimb/events{/privacy}", "received_events_url": "https://api.github.com/users/tkclimb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-09-03T04:05:49Z", "updated_at": "2020-09-03T18:02:05Z", "closed_at": "2020-09-03T18:02:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently I began to use ort in C++. I expected to use it with Tensorrt provider but I couldn't find the api for enabling the provider from C++ function.\r\nI checked the [doc](https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/TensorRT-ExecutionProvider.md#cc) and it seems to need `onnxruntime::InferenceSession` not 'Ort::Session', which is  exported to users in `onnxruntime_cxx_api.h`.\r\nI think it is thought users normally use this header file to integrate ort into their own project as well as I'm doing.\r\nIn this sense, I think `Ort::Session` would be better to have the provider activate function. What do you think??\r\n\r\nCurrent solution seems that only adding the function to `onnxruntime_c_api.cc` and exporting them from the headers (c_api and cxx_api), of course it needs to rebuild ort but it's not user friendly.\r\nIs there another solution ??", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5029", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5029/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5029/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5029/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/5029", "id": 691318499, "node_id": "MDU6SXNzdWU2OTEzMTg0OTk=", "number": 5029, "title": "Cannot specify link options for target \"protoc\" which is not built by this project.", "user": {"login": "xkszltl", "id": 5203025, "node_id": "MDQ6VXNlcjUyMDMwMjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/5203025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xkszltl", "html_url": "https://github.com/xkszltl", "followers_url": "https://api.github.com/users/xkszltl/followers", "following_url": "https://api.github.com/users/xkszltl/following{/other_user}", "gists_url": "https://api.github.com/users/xkszltl/gists{/gist_id}", "starred_url": "https://api.github.com/users/xkszltl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xkszltl/subscriptions", "organizations_url": "https://api.github.com/users/xkszltl/orgs", "repos_url": "https://api.github.com/users/xkszltl/repos", "events_url": "https://api.github.com/users/xkszltl/events{/privacy}", "received_events_url": "https://api.github.com/users/xkszltl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-09-02T18:43:35Z", "updated_at": "2020-09-03T22:20:02Z", "closed_at": "2020-09-03T22:20:02Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\nhttps://github.com/microsoft/onnxruntime/blob/1f69a58105f320e9b79008698b4275fa3f113f2d/cmake/CMakeLists.txt#L450\r\nShould check `onnxruntime_PREFER_SYSTEM_LIB` as well.\r\n\r\n<img width=\"769\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5203025/92023249-16fce480-ed8f-11ea-9620-2671b0b9cb0f.png\">\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: 1.4.0 and master\r\n- GCC/Compiler version (if compiling from source): 8", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5008", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5008/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5008/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/5008/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/5008", "id": 690434496, "node_id": "MDU6SXNzdWU2OTA0MzQ0OTY=", "number": 5008, "title": "error: 'once_flag' in namespace 'std' does not name a type", "user": {"login": "xkszltl", "id": 5203025, "node_id": "MDQ6VXNlcjUyMDMwMjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/5203025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xkszltl", "html_url": "https://github.com/xkszltl", "followers_url": "https://api.github.com/users/xkszltl/followers", "following_url": "https://api.github.com/users/xkszltl/following{/other_user}", "gists_url": "https://api.github.com/users/xkszltl/gists{/gist_id}", "starred_url": "https://api.github.com/users/xkszltl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xkszltl/subscriptions", "organizations_url": "https://api.github.com/users/xkszltl/orgs", "repos_url": "https://api.github.com/users/xkszltl/repos", "events_url": "https://api.github.com/users/xkszltl/events{/privacy}", "received_events_url": "https://api.github.com/users/xkszltl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-09-01T20:43:26Z", "updated_at": "2020-09-02T07:47:00Z", "closed_at": "2020-09-02T07:47:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\n<img width=\"1064\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5203025/91903805-70eda380-ecd6-11ea-983d-08934a4df620.png\">\r\nPlease include mutex as suggested.\r\nhttps://en.cppreference.com/w/cpp/thread/once_flag\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: master\r\n- GCC/Compiler version (if compiling from source): gcc-8\r\n- CUDA/cuDNN version: 11.0/8\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4999", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4999/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4999/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4999/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4999", "id": 689927747, "node_id": "MDU6SXNzdWU2ODk5Mjc3NDc=", "number": 4999, "title": "using float16 onnx model gives wrong predict results.", "user": {"login": "zhuxiaoxuhit", "id": 32813150, "node_id": "MDQ6VXNlcjMyODEzMTUw", "avatar_url": "https://avatars2.githubusercontent.com/u/32813150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuxiaoxuhit", "html_url": "https://github.com/zhuxiaoxuhit", "followers_url": "https://api.github.com/users/zhuxiaoxuhit/followers", "following_url": "https://api.github.com/users/zhuxiaoxuhit/following{/other_user}", "gists_url": "https://api.github.com/users/zhuxiaoxuhit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuxiaoxuhit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuxiaoxuhit/subscriptions", "organizations_url": "https://api.github.com/users/zhuxiaoxuhit/orgs", "repos_url": "https://api.github.com/users/zhuxiaoxuhit/repos", "events_url": "https://api.github.com/users/zhuxiaoxuhit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuxiaoxuhit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-09-01T08:13:47Z", "updated_at": "2020-09-02T00:26:21Z", "closed_at": "2020-09-02T00:26:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nSuccessful compile and run float16 model, but gives wrong predict results.\uff08float32 model all right\uff09.\r\n\r\n**System information**\r\n- OS Platform and Distribution:CentOS Linux release 7.5.1804\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: master\r\n- Python version: 3.7\r\n- Visual Studio version (if applicable): None\r\n- GCC/Compiler version: gcc (GCC) 4.8.5 \r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**To Reproduce**\r\n```c++\r\nvoid BFloat16ToFloat(const uint16_t* src, float* dst, size_t size) \r\n{\r\n    const uint16_t* p = reinterpret_cast<const uint16_t*>(src);\r\n    uint16_t* q = reinterpret_cast<uint16_t*>(dst);\r\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\r\n  for (; size != 0; p++, q += 2, size--) \r\n  {\r\n    q[0] = *p; \r\n    q[1] = 0;\r\n  }\r\n#else\r\n  for (; size != 0; p++, q += 2, size--) \r\n  {\r\n    q[0] = 0;\r\n    q[1] = *p; \r\n  }\r\n#endif\r\n}\r\n\r\nvoid doinference()\r\n{\r\n    // PREPARE INPUTS AND CREATE ENVS,SESSION\r\n    ......\r\n    // SESSION RUN\r\n    auto output_values = pp_session_->Run(Ort::RunOptions{nullptr}, input_node_names.data(), input_tensors, 3, output_node_names.data(), 1);\r\n\r\n    // output frame size    \r\n    auto out_info = output_values[0].GetTensorTypeAndShapeInfo();\r\n    ONNXTensorElementDataType type_ = out_info.GetElementType();\r\n    printf(\"--------------------type=%d\\n\", type_);    // got 10(float 16)\r\n    acoustic_len_ = out_info.GetShape()[1];\r\n\r\n    uint16_t* outputs_16 = output_values[0].GetTensorMutableData<uint16_t>();\r\n    outputs_ = new float [22*acoustic_len_];\r\n    BFloat16ToFloat(outputs_16, outputs_, 22*acoustic_len_);\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nGet right predict results.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4998", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4998/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4998/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4998/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4998", "id": 689925618, "node_id": "MDU6SXNzdWU2ODk5MjU2MTg=", "number": 4998, "title": "Loading onnx model failed with Fatal error: max_pool2d is not a registered function/op", "user": {"login": "eRaul", "id": 3354725, "node_id": "MDQ6VXNlcjMzNTQ3MjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eRaul", "html_url": "https://github.com/eRaul", "followers_url": "https://api.github.com/users/eRaul/followers", "following_url": "https://api.github.com/users/eRaul/following{/other_user}", "gists_url": "https://api.github.com/users/eRaul/gists{/gist_id}", "starred_url": "https://api.github.com/users/eRaul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eRaul/subscriptions", "organizations_url": "https://api.github.com/users/eRaul/orgs", "repos_url": "https://api.github.com/users/eRaul/repos", "events_url": "https://api.github.com/users/eRaul/events{/privacy}", "received_events_url": "https://api.github.com/users/eRaul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1220611565, "node_id": "MDU6TGFiZWwxMjIwNjExNTY1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:converter", "name": "component:converter", "color": "303a93", "default": false, "description": "related to ONNX converters"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-09-01T08:10:49Z", "updated_at": "2020-09-01T08:41:33Z", "closed_at": "2020-09-01T08:35:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI have exported a pytorch model squeezenet1.1 (which is traced with torch.jit.trace). This model comes from torchvison model zone. Now I want to use it to inference with onnxruntime, but failed:\r\n```\r\nsess = rt.InferenceSession(model)\r\n```\r\nException below was throwed:\r\n```\r\nonnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Fatal error: max_pool2d is not a registered function/op\r\n```\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- ONNX Runtime installed from (source or binary): binary (pip3 install onnxruntime==1.2.0)\r\n- ONNX Runtime version: 1.2.0 (1.4.0 also tried)\r\n- Python version: 3.6\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4997", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4997/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4997/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4997/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4997", "id": 689895295, "node_id": "MDU6SXNzdWU2ODk4OTUyOTU=", "number": 4997, "title": "Unable to load model generated by tf2onnx ", "user": {"login": "bedapisl", "id": 10141878, "node_id": "MDQ6VXNlcjEwMTQxODc4", "avatar_url": "https://avatars1.githubusercontent.com/u/10141878?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bedapisl", "html_url": "https://github.com/bedapisl", "followers_url": "https://api.github.com/users/bedapisl/followers", "following_url": "https://api.github.com/users/bedapisl/following{/other_user}", "gists_url": "https://api.github.com/users/bedapisl/gists{/gist_id}", "starred_url": "https://api.github.com/users/bedapisl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bedapisl/subscriptions", "organizations_url": "https://api.github.com/users/bedapisl/orgs", "repos_url": "https://api.github.com/users/bedapisl/repos", "events_url": "https://api.github.com/users/bedapisl/events{/privacy}", "received_events_url": "https://api.github.com/users/bedapisl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1220611565, "node_id": "MDU6TGFiZWwxMjIwNjExNTY1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:converter", "name": "component:converter", "color": "303a93", "default": false, "description": "related to ONNX converters"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-09-01T07:26:43Z", "updated_at": "2020-09-01T08:27:29Z", "closed_at": "2020-09-01T08:27:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nWhen loading model generated by tf2onnx I got following error: \r\n```\r\nonnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Node (generic_loop_Loop__44) Op (Loop) [TypeInferenceError] Graph attribute inferencing failed: Node (generic_loop_Loop__25) Op (Loop) [TypeInferenceError] Graph attribute inferencing failed: Type Error: Type parameter (T) bound to different types (tensor(int64) and tensor(int32) in node (loop_over_batch/while/loop_over_classes/while/Equal).\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- ONNX Runtime installed from (source or binary): binary (pip)\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.6.9\r\n\r\n**To Reproduce**\r\nTry loading model this model [decode_detections.onnx.zip](https://github.com/microsoft/onnxruntime/files/5154554/decode_detections.onnx.zip) with \r\n```\r\n>>> import onnxruntime\r\n>>> onnxruntime.InferenceSession(\"decode_detections.onnx\")\r\n```\r\n\r\nIs this a problem of onnxruntime or tf2onnx?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4993", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4993/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4993/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4993/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4993", "id": 689784745, "node_id": "MDU6SXNzdWU2ODk3ODQ3NDU=", "number": 4993, "title": "NNAPI on Android gives different output from the one that CPU gives", "user": {"login": "s94285", "id": 23016451, "node_id": "MDQ6VXNlcjIzMDE2NDUx", "avatar_url": "https://avatars2.githubusercontent.com/u/23016451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/s94285", "html_url": "https://github.com/s94285", "followers_url": "https://api.github.com/users/s94285/followers", "following_url": "https://api.github.com/users/s94285/following{/other_user}", "gists_url": "https://api.github.com/users/s94285/gists{/gist_id}", "starred_url": "https://api.github.com/users/s94285/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/s94285/subscriptions", "organizations_url": "https://api.github.com/users/s94285/orgs", "repos_url": "https://api.github.com/users/s94285/repos", "events_url": "https://api.github.com/users/s94285/events{/privacy}", "received_events_url": "https://api.github.com/users/s94285/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2036741918, "node_id": "MDU6TGFiZWwyMDM2NzQxOTE4", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:NNAPI", "name": "ep:NNAPI", "color": "bfdadc", "default": false, "description": "questions/issues related to NNAPI EP"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-09-01T03:51:54Z", "updated_at": "2020-09-02T04:19:48Z", "closed_at": "2020-09-02T03:27:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI added armv7l into initOsArch and rebuild AAR to fix #4976 \r\nI then test [Ultra-Light-Fast-Generic-Face-Detector-1MB](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/models/onnx/version-RFB-320_simplified.onnx).\r\nNNAPI gives different output on every inference with same input data, while CPU gives same output which should be correct.\r\nThe first two column of outputs is the score of the box, which was produced by softmax function.\r\nDespite that output changes every time, the sum of the two columns gives 1.\r\nThus the output part should be correct imo.\r\n\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.7.5\r\n- GCC/Compiler version (if compiling from source): 6.5.0\r\n\r\n**To Reproduce**\r\nBuild AAR using following command\r\n`./build.sh --android --android_sdk_path /home/kevin/Android/Sdk --android_ndk_path /home/kevin/Android/Sdk/ndk/21.2.6472646 --android_abi armeabi-v7a --android_api 28 --use_nnapi --build_java`\r\n\r\nMy testing app, screenshots and output results\r\nhttps://github.com/s94285/ONNX-Test/tree/nnapiIncorrectOutput\r\n\r\n**Expected behavior**\r\nSame outputs for NNAPI and CPU\r\n\r\n**Additional context**\r\noutputs\r\n```\r\nscores            boxes\r\nNNAPI\r\n0.000000 1.000000 -9721483286680020000000000000000.000000 37482082545802430000000000000000.000000 106805191827844450000000000000000.000000 27817414541403927000000000000000.000000\r\n0.000000 1.000000 79688540809186700000000000000000.000000 56770527847676790000000000000000.000000 -56594841911867110000000000000000.000000 -67892248860462540000000000000000.000000\r\n0.000000 1.000000 113178561706193770000000000000000.000000 159783570835960630000000000000000.000000 -76747968988369190000000000000000.000000 -14819046546759074000000000000000.000000\r\n1.000000 0.000000 -28403378468319500000000000000000.000000 -12232165317282937000000000000000.000000 25499437099836310000000000000000.000000 -28504855701617950000000000000000.000000\r\n0.000000 1.000000 -31206425305749290000000000000000.000000 23600514450824994000000000000000.000000 -17805072769468274000000000000000.000000 -43845592425166965000000000000000.000000\r\n1.000000 0.000000 -25969718915072960000000000000000.000000 75570189933571110000000000000000.000000 -31958556086570890000000000000000.000000 -17601597255843116000000000000000.000000\r\n0.941594 0.058406 -154.381699 -307.089142 -783.662109 -490.694550\r\n0.939313 0.060687 -204.694305 -49.444698 -198.451538 -41.679722\r\n0.937129 0.062871 -254.715500 -32.978069 -234.318695 -141.584808\r\n1.000000 0.000000 5852403848125322000000000000000.000000 -12818769974882904000000000000000.000000 9270502576616430000000000000000.000000 -33799733184882496000000000000000.000000\r\n0.000000 1.000000 -7184477500817905000000000000000.000000 17264709524468567000000000000000.000000 -52583389092925130000000000000000.000000 -56310517066203585000000000000000.000000\r\n1.000000 0.000000 -10670045774838796000000000000000.000000 83584434826883360000000000000000.000000 -46172112136575980000000000000000.000000 -9066530194479411000000000000000.000000\r\n0.000000 1.000000 -9721483286680020000000000000000.000000 37482082545802430000000000000000.000000 106805191827844450000000000000000.000000 27817414541403927000000000000000.000000\r\n0.000000 1.000000 79688540809186700000000000000000.000000 56770527847676790000000000000000.000000 -56594841911867110000000000000000.000000 -67892248860462540000000000000000.000000\r\n0.000000 1.000000 113178561706193770000000000000000.000000 159783570835960630000000000000000.000000 -76747968988369190000000000000000.000000 -14819046546759074000000000000000.000000\r\n1.000000 0.000000 -28403378468319500000000000000000.000000 -12232165317282937000000000000000.000000 25499437099836310000000000000000.000000 -28504855701617950000000000000000.000000\r\n0.000000 1.000000 -31206425305749290000000000000000.000000 23600514450824994000000000000000.000000 -17805072769468274000000000000000.000000 -43845592425166965000000000000000.000000\r\n1.000000 0.000000 -25969718915072960000000000000000.000000 75570189933571110000000000000000.000000 -31958556086570890000000000000000.000000 -17601597255843116000000000000000.000000\r\n0.948166 0.051835 -63.734016 -387.792114 -569.298767 -460.760010\r\n0.944951 0.055049 19.591267 -98.694290 121.338081 -13.369317\r\n0.941335 0.058665 45.873062 -55.803158 -4.560786 -133.835815\r\n1.000000 0.000000 5852403848125322000000000000000.000000 -12818769974882904000000000000000.000000 9270502576616430000000000000000.000000 -33799733184882496000000000000000.000000\r\n\r\nCPU\r\n0.943954 0.056046 546.441895 199.126144 -339.588074 70.642044\r\n0.932864 0.067136 663.946411 813.192017 -67.176483 366.294922\r\n0.924546 0.075454 502.542908 753.034119 -411.762146 140.737015\r\n0.947124 0.052876 -372.240356 111.347778 -456.431305 -54.801693\r\n0.938151 0.061849 -394.491943 723.942993 -219.131943 275.616882\r\n0.931809 0.068191 -264.284424 719.915588 -439.095795 136.958084\r\n0.949984 0.050016 40.844231 46.047665 -1071.941772 -423.784668\r\n0.948004 0.051996 -37.008282 291.401093 -360.245667 -8.781137\r\n0.948927 0.051073 -232.510651 351.550140 -232.721848 25.473518\r\n0.947411 0.052589 -168.603821 -245.925522 -982.257141 -494.832184\r\n0.946205 0.053795 121.196175 136.661102 -411.463470 -61.700977\r\n0.947822 0.052178 145.699097 242.051498 -368.328308 -18.955723\r\n0.950891 0.049109 245.020126 37.207867 -997.234558 -550.028503\r\n0.947889 0.052111 139.870041 282.246582 -330.454407 -30.775051\r\n0.949342 0.050658 18.845734 287.226349 -329.962219 -45.978249\r\n0.948421 0.051579 29.227474 171.604523 -676.300354 -387.206726\r\n0.945300 0.054700 334.688538 437.656189 -120.102249 -13.328161\r\n0.944995 0.055005 320.806396 453.638916 -92.753052 0.245833\r\n0.951835 0.048165 161.016876 -100.445564 -388.817993 -216.044601\r\n0.947542 0.052458 435.896912 352.136108 156.850311 19.001419\r\n0.944383 0.055617 355.845612 599.699097 196.363297 71.831528\r\n0.951963 0.048037 239.960190 117.357056 -320.302155 -112.081520\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4976", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4976/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4976/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4976/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4976", "id": 688855709, "node_id": "MDU6SXNzdWU2ODg4NTU3MDk=", "number": 4976, "title": "Using AAR on android gives java.lang.IllegalStateException: Unsupported arch:armv7l exception", "user": {"login": "s94285", "id": 23016451, "node_id": "MDQ6VXNlcjIzMDE2NDUx", "avatar_url": "https://avatars2.githubusercontent.com/u/23016451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/s94285", "html_url": "https://github.com/s94285", "followers_url": "https://api.github.com/users/s94285/followers", "following_url": "https://api.github.com/users/s94285/following{/other_user}", "gists_url": "https://api.github.com/users/s94285/gists{/gist_id}", "starred_url": "https://api.github.com/users/s94285/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/s94285/subscriptions", "organizations_url": "https://api.github.com/users/s94285/orgs", "repos_url": "https://api.github.com/users/s94285/repos", "events_url": "https://api.github.com/users/s94285/events{/privacy}", "received_events_url": "https://api.github.com/users/s94285/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1805781160, "node_id": "MDU6TGFiZWwxODA1NzgxMTYw", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:Java", "name": "api:Java", "color": "0e8a16", "default": false, "description": "related to the Java API"}, {"id": 2314849695, "node_id": "MDU6TGFiZWwyMzE0ODQ5Njk1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:android", "name": "platform:android", "color": "50ed7f", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-31T02:37:20Z", "updated_at": "2020-09-02T07:48:08Z", "closed_at": "2020-09-02T07:48:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nUsing AAR on android gives java.lang.IllegalStateException: Unsupported arch:armv7l exception.\r\n\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.7.5\r\n- GCC/Compiler version (if compiling from source): 6.5.0\r\n\r\n**To Reproduce**\r\nBuild AAR using following command\r\n`./build.sh --android --android_sdk_path /home/kevin/Android/Sdk --android_ndk_path /home/kevin/Android/Sdk/ndk/21.2.6472646 --android_abi armeabi-v7a --android_api 28 --use_nnapi --build_java`\r\nIn android app:\r\n```java\r\ntry(OrtEnvironment env = OrtEnvironment.getEnvironment();\r\n    OrtSession.SessionOptions opts = new OrtSession.SessionOptions()){\r\n    opts.setOptimizationLevel(OrtSession.SessionOptions.OptLevel.BASIC_OPT);\r\n    opts.addNnapi();\r\n} catch (OrtException e) {\r\n    e.printStackTrace();\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nI checked the OnnxRuntime.class where the exception threw.\r\nSeems like it support for amd64/x86_64/x86 only.\r\nWhat about other arch like aarch64 and armv7l ?\r\n\r\n**Additional context**\r\n```\r\n2020-08-31 10:13:20.611 2992-2992/com.example.onnxtest E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.onnxtest, PID: 2992\r\n    java.lang.ExceptionInInitializerError\r\n        at ai.onnxruntime.OnnxRuntime.init(Unknown Source:0)\r\n        at ai.onnxruntime.OrtEnvironment.<clinit>(OrtEnvironment.java:24)\r\n        at ai.onnxruntime.OrtEnvironment.getEnvironment(OrtEnvironment.java:45)\r\n        at com.example.onnxtest.MainActivity.onCreate(MainActivity.java:34)\r\n        at android.app.Activity.performCreate(Activity.java:7136)\r\n        at android.app.Activity.performCreate(Activity.java:7127)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1271)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2922)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3090)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1822)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6751)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\r\n     Caused by: java.lang.IllegalStateException: Unsupported arch:armv7l\r\n        at ai.onnxruntime.OnnxRuntime.initOsArch(OnnxRuntime.java:65)\r\n        at ai.onnxruntime.OnnxRuntime.<clinit>(OnnxRuntime.java:36)\r\n        at ai.onnxruntime.OnnxRuntime.init(Unknown Source:0)\u00a0\r\n        at ai.onnxruntime.OrtEnvironment.<clinit>(OrtEnvironment.java:24)\u00a0\r\n        at ai.onnxruntime.OrtEnvironment.getEnvironment(OrtEnvironment.java:45)\u00a0\r\n        at com.example.onnxtest.MainActivity.onCreate(MainActivity.java:34)\u00a0\r\n        at android.app.Activity.performCreate(Activity.java:7136)\u00a0\r\n        at android.app.Activity.performCreate(Activity.java:7127)\u00a0\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1271)\u00a0\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2922)\u00a0\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3090)\u00a0\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)\u00a0\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)\u00a0\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)\u00a0\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1822)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\u00a0\r\n        at android.os.Looper.loop(Looper.java:193)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:6751)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:493)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)\u00a0\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4974", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4974/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4974/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4974/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4974", "id": 688716815, "node_id": "MDU6SXNzdWU2ODg3MTY4MTU=", "number": 4974, "title": "onnx inferencesession load hangs with quartznet asr config. ", "user": {"login": "darraghdog", "id": 7209588, "node_id": "MDQ6VXNlcjcyMDk1ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/7209588?v=4", "gravatar_id": "", "url": "https://api.github.com/users/darraghdog", "html_url": "https://github.com/darraghdog", "followers_url": "https://api.github.com/users/darraghdog/followers", "following_url": "https://api.github.com/users/darraghdog/following{/other_user}", "gists_url": "https://api.github.com/users/darraghdog/gists{/gist_id}", "starred_url": "https://api.github.com/users/darraghdog/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/darraghdog/subscriptions", "organizations_url": "https://api.github.com/users/darraghdog/orgs", "repos_url": "https://api.github.com/users/darraghdog/repos", "events_url": "https://api.github.com/users/darraghdog/events{/privacy}", "received_events_url": "https://api.github.com/users/darraghdog/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-30T13:14:01Z", "updated_at": "2020-08-30T13:20:01Z", "closed_at": "2020-08-30T13:20:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nThe quartznet onnx exported model cannot be loaded for inference. The code snippet below runs fine up until `encoder_session = onnxruntime.InferenceSession(filename)`, at that point the code just hangs, no error but hangs. I left it for ~ 10mins.    \r\nSee code snippet below. The model generated from the first config works for me, the model generated by the second config does not work for me. \r\n\r\n**Urgency**\r\nWe would like to use this in a product being launched later this year. Speeding up inference would be very helpful. \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- ONNX Runtime installed from (source or binary): binary\r\n- ONNX Runtime version: \r\n```\r\nonnx.__version__ -- > 1.7.0\r\nonnxruntime.__version__ --> 1.3.0\r\n```\r\n- Python version: Python 3.7.6\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source): N/A... testing on CPU\r\n- CUDA/cuDNN version: N/A... testing on CPU\r\n- GPU model and memory: N/A... testing on CPU\r\n\r\n**To Reproduce**\r\n```\r\nimport os\r\nimport tempfile\r\nimport torch\r\nimport onnx\r\nimport pytest\r\nfrom omegaconf import DictConfig\r\nfrom nemo.collections.asr.modules import ConvASRDecoder, ConvASREncoder\r\n   \r\n# This config can be loaded via the Inference Session\r\nencoder_dict = {\r\n    'cls': 'nemo.collections.asr.modules.ConvASREncoder',\r\n    'params': {\r\n        'feat_in': 64,\r\n        'activation': 'relu',\r\n        'conv_mask': True,\r\n        'jasper': [\r\n            {'filters': 1024, 'repeat': 1, 'kernel': [1], 'stride': [1], 'dilation': [1],'dropout': 0.0,'residual': False, 'separable': True,'se': True, 'se_context_size': -1}\r\n        ],\r\n    },\r\n}\r\n   \r\n# This config cannot be loaded via the Inference Session\r\nencoder_dict = {\r\n    'cls': 'nemo.collections.asr.modules.ConvASREncoder', \r\n    'params': {\r\n        'feat_in': 64, \r\n        'activation': 'relu', \r\n        'conv_mask': True, \r\n        'jasper': \r\n            [{'filters': 256, 'repeat': 1, 'kernel': [33], 'stride': [2], 'dilation': [1], 'dropout': 0.0, 'residual': False, 'separable': True}, \r\n             {'filters': 256, 'repeat': 5, 'kernel': [33], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 256, 'repeat': 5, 'kernel': [33], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 256, 'repeat': 5, 'kernel': [33], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 256, 'repeat': 5, 'kernel': [39], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 256, 'repeat': 5, 'kernel': [39], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 256, 'repeat': 5, 'kernel': [39], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True},\r\n             {'filters': 512, 'repeat': 5, 'kernel': [51], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [51], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [51], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [63], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [63], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [63], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [75], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [75], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 5, 'kernel': [75], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': True, 'separable': True}, \r\n             {'filters': 512, 'repeat': 1, 'kernel': [87], 'stride': [1], 'dilation': [2], 'dropout': 0.0, 'residual': False, 'separable': True}, \r\n             {'filters': 1024, 'repeat': 1, 'kernel': [1], 'stride': [1], 'dilation': [1], 'dropout': 0.0, 'residual': False}\r\n             ]\r\n            }\r\n        }\r\n\r\nINPATH = '/Users/dhanley/Documents/gazeapi/ai-tasks/gazeproto/experiment'\r\nfilename  = os.path.join(INPATH, 'qn_encoder.onnx')\r\nencoder_instance = ConvASREncoder.from_config_dict(DictConfig(encoder_dict))\r\nencoder_instance.export(output=filename)\r\n# Create dummy inputs\r\ndummy_processed_signal  = torch.randn([1, 64, 5104])\r\ndummy_processed_signal_len =  torch.tensor([5103])\r\n# Check model\r\nquartznet_encoder = onnx.load(filename)\r\nonnx.checker.check_model(quartznet_encoder)\r\n# Load inference session\r\nimport onnxruntime\r\nencoder_session = onnxruntime.InferenceSession(filename)\r\n```\r\n\r\n**Expected behavior**\r\nThe inference session for the second config should load, just as well as the first session. \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4972", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4972/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4972/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4972/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4972", "id": 688532711, "node_id": "MDU6SXNzdWU2ODg1MzI3MTE=", "number": 4972, "title": "/usr/bin/ld: cannot find -lonnxruntime", "user": {"login": "Bartekkz", "id": 43574448, "node_id": "MDQ6VXNlcjQzNTc0NDQ4", "avatar_url": "https://avatars1.githubusercontent.com/u/43574448?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bartekkz", "html_url": "https://github.com/Bartekkz", "followers_url": "https://api.github.com/users/Bartekkz/followers", "following_url": "https://api.github.com/users/Bartekkz/following{/other_user}", "gists_url": "https://api.github.com/users/Bartekkz/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bartekkz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bartekkz/subscriptions", "organizations_url": "https://api.github.com/users/Bartekkz/orgs", "repos_url": "https://api.github.com/users/Bartekkz/repos", "events_url": "https://api.github.com/users/Bartekkz/events{/privacy}", "received_events_url": "https://api.github.com/users/Bartekkz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-29T13:25:15Z", "updated_at": "2020-08-29T14:16:06Z", "closed_at": "2020-08-29T14:16:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nTrying to compile squeezenet derived C++ inference sample with onnxruntime 1.4.0 I get \r\n/usr/bin/ld: cannot find -lonnxruntime\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- Linux Ubuntu 18.04\r\n- ONNX Runtime installed from binary\r\n- ONNX Runtime version: 1.4.0\r\n\r\n**To Reproduce**\r\nCode:\r\n```\r\n#include <assert.h>\r\n#include <vector>\r\n#include <iostream>\r\n#include <onnxruntime_cxx_api.h>\r\n\r\nint main(int argc, char* argv[]) {\r\n\r\n    const OrtApi* g_ort = OrtGetApiBase()->GetApi(ORT_API_VERSION);\r\n\r\n    std::cout << \"Passed\" << '\\n';\r\n    return 0;\r\n}\r\n```\r\n\r\nLibraries installed:\r\n```\r\n$ sudo ldconfig -p | grep onnx\r\n        libonnxruntime.so.1.4.0 (libc6,x86-64) => /usr/local/lib/libonnxruntime.so.1.4.0\r\n        libonnxruntime.so.1.4.0 (libc6,x86-64) => /lib/x86_64-linux-gnu/libonnxruntime.so.1.4.0\r\n        libonnxifi_dummy.so (libc6,x86-64) => /usr/local/lib/libonnxifi_dummy.so\r\n        libonnxifi.so (libc6,x86-64) => /usr/local/lib/libonnxifi.so\r\n```\r\nCompilation:\r\n```\r\ng++ api_sample.cpp -o api_sample -I/home/bearnard/onnxruntime/include/onnxruntime/core/session -lonnxruntime\r\n```\r\n\r\n**Expected behavior**\r\nCompilation without errors", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4962", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4962/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4962/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4962/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4962", "id": 688328760, "node_id": "MDU6SXNzdWU2ODgzMjg3NjA=", "number": 4962, "title": "Add support for sessions to share a global threadpool", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": {"login": "pranavsharma", "id": 2732907, "node_id": "MDQ6VXNlcjI3MzI5MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2732907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pranavsharma", "html_url": "https://github.com/pranavsharma", "followers_url": "https://api.github.com/users/pranavsharma/followers", "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}", "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions", "organizations_url": "https://api.github.com/users/pranavsharma/orgs", "repos_url": "https://api.github.com/users/pranavsharma/repos", "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}", "received_events_url": "https://api.github.com/users/pranavsharma/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "pranavsharma", "id": 2732907, "node_id": "MDQ6VXNlcjI3MzI5MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2732907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pranavsharma", "html_url": "https://github.com/pranavsharma", "followers_url": "https://api.github.com/users/pranavsharma/followers", "following_url": "https://api.github.com/users/pranavsharma/following{/other_user}", "gists_url": "https://api.github.com/users/pranavsharma/gists{/gist_id}", "starred_url": "https://api.github.com/users/pranavsharma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pranavsharma/subscriptions", "organizations_url": "https://api.github.com/users/pranavsharma/orgs", "repos_url": "https://api.github.com/users/pranavsharma/repos", "events_url": "https://api.github.com/users/pranavsharma/events{/privacy}", "received_events_url": "https://api.github.com/users/pranavsharma/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-28T20:07:10Z", "updated_at": "2020-08-28T21:26:59Z", "closed_at": "2020-08-28T21:20:11Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Is your feature request related to a problem? Please describe.**\r\nWe have to run 2 sessions, but not in parallel.\r\nDoes that mean we may also benefit from [this PR](https://github.com/microsoft/onnxruntime/pull/3177)?\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): 1.4\r\n\r\n**Describe the solution you'd like**\r\nExpose the threadpool sharing to C#, so we can test\r\n\r\n**Describe alternatives you've considered**\r\nUse C++ code\r\n\r\n**Additional context**\r\nThe design discussion does not explain the rationale, or exactly how it helps OCR. So we do not know if this also benefits us.\r\nIs it possible to discuss this in open and explain the issue?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4956", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4956/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4956/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4956/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4956", "id": 687871416, "node_id": "MDU6SXNzdWU2ODc4NzE0MTY=", "number": 4956, "title": "Build by source, import onnxruntime, ModuleNotFoundError: No module named 'onnxruntime.capi'", "user": {"login": "zhuguiqian", "id": 7693110, "node_id": "MDQ6VXNlcjc2OTMxMTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/7693110?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuguiqian", "html_url": "https://github.com/zhuguiqian", "followers_url": "https://api.github.com/users/zhuguiqian/followers", "following_url": "https://api.github.com/users/zhuguiqian/following{/other_user}", "gists_url": "https://api.github.com/users/zhuguiqian/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuguiqian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuguiqian/subscriptions", "organizations_url": "https://api.github.com/users/zhuguiqian/orgs", "repos_url": "https://api.github.com/users/zhuguiqian/repos", "events_url": "https://api.github.com/users/zhuguiqian/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuguiqian/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-28T07:29:35Z", "updated_at": "2020-08-28T18:19:51Z", "closed_at": "2020-08-28T18:19:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nWhen i use source code to build onnxruntime, build OK by the command './build.sh --config RelWithDebInfo --build_shared_lib --parallel', but when import onnxruntime, return error \"ModuleNotFoundError: No module named 'onnxruntime.capi'\", if any action need to do after building?\r\n\r\n**Urgency**\r\nIf there are particular important use cases blocked by this or strict project-related timelines, please share more information and dates. If there are no hard deadlines, please specify none.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- ONNX Runtime installed from (source or binary): source source \r\n- ONNX Runtime version: 1.3.0\r\n- Python version: Python 3.6.11\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\nbuild OK\r\n![image](https://user-images.githubusercontent.com/7693110/91533676-df330e80-e942-11ea-85eb-4d3a5a4321f9.png)\r\nimport error\r\n![image](https://user-images.githubusercontent.com/7693110/91533702-ea863a00-e942-11ea-83af-9c07d3e7d560.png)\r\n\r\n**Additional context**\r\nAdd any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4952", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4952/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4952/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4952/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4952", "id": 687678332, "node_id": "MDU6SXNzdWU2ODc2NzgzMzI=", "number": 4952, "title": "GPT2 quantization", "user": {"login": "carter54", "id": 26741594, "node_id": "MDQ6VXNlcjI2NzQxNTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26741594?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carter54", "html_url": "https://github.com/carter54", "followers_url": "https://api.github.com/users/carter54/followers", "following_url": "https://api.github.com/users/carter54/following{/other_user}", "gists_url": "https://api.github.com/users/carter54/gists{/gist_id}", "starred_url": "https://api.github.com/users/carter54/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carter54/subscriptions", "organizations_url": "https://api.github.com/users/carter54/orgs", "repos_url": "https://api.github.com/users/carter54/repos", "events_url": "https://api.github.com/users/carter54/events{/privacy}", "received_events_url": "https://api.github.com/users/carter54/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1607058914, "node_id": "MDU6TGFiZWwxNjA3MDU4OTE0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/feature:quantization", "name": "feature:quantization", "color": "9073d1", "default": false, "description": "related to quantization of models or running quantized models"}], "state": "closed", "locked": false, "assignee": {"login": "yufenglee", "id": 30486710, "node_id": "MDQ6VXNlcjMwNDg2NzEw", "avatar_url": "https://avatars2.githubusercontent.com/u/30486710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yufenglee", "html_url": "https://github.com/yufenglee", "followers_url": "https://api.github.com/users/yufenglee/followers", "following_url": "https://api.github.com/users/yufenglee/following{/other_user}", "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}", "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions", "organizations_url": "https://api.github.com/users/yufenglee/orgs", "repos_url": "https://api.github.com/users/yufenglee/repos", "events_url": "https://api.github.com/users/yufenglee/events{/privacy}", "received_events_url": "https://api.github.com/users/yufenglee/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yufenglee", "id": 30486710, "node_id": "MDQ6VXNlcjMwNDg2NzEw", "avatar_url": "https://avatars2.githubusercontent.com/u/30486710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yufenglee", "html_url": "https://github.com/yufenglee", "followers_url": "https://api.github.com/users/yufenglee/followers", "following_url": "https://api.github.com/users/yufenglee/following{/other_user}", "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}", "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions", "organizations_url": "https://api.github.com/users/yufenglee/orgs", "repos_url": "https://api.github.com/users/yufenglee/repos", "events_url": "https://api.github.com/users/yufenglee/events{/privacy}", "received_events_url": "https://api.github.com/users/yufenglee/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-08-28T03:13:24Z", "updated_at": "2020-09-08T23:17:47Z", "closed_at": "2020-09-08T23:17:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI have tried to quantize GPT2-small(124M) model with the following code, and it works very well\r\n```\r\nmodel = model_class.from_pretrained(model_name_or_path)\r\nmodel = QuantizeHelper.quantize_torch_model(model)\r\nmodel.to(device)\r\n```\r\nbut if I try to save the quantized model and reload it by\r\n```\r\nmodel.save_pretrained(quantized_model_path)\r\nmodel = model_class.from_pretrain(quantized_model_path)\r\nmodel.to(device)\r\n```\r\nthe saved qunatized model size is about half of the initial model as it only quantized Conv1D/Linear layer.\r\nBut the quantized model generated very strange results which has none sense...\r\nDo you know any possible reason?\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- ONNX Runtime installed from (source or binary): pip\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.7.4\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4944", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4944/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4944/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4944/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4944", "id": 687532617, "node_id": "MDU6SXNzdWU2ODc1MzI2MTc=", "number": 4944, "title": "EinSum CPU provider rejects equation ZY,YX->ZX", "user": {"login": "fdwr", "id": 1809166, "node_id": "MDQ6VXNlcjE4MDkxNjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1809166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fdwr", "html_url": "https://github.com/fdwr", "followers_url": "https://api.github.com/users/fdwr/followers", "following_url": "https://api.github.com/users/fdwr/following{/other_user}", "gists_url": "https://api.github.com/users/fdwr/gists{/gist_id}", "starred_url": "https://api.github.com/users/fdwr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fdwr/subscriptions", "organizations_url": "https://api.github.com/users/fdwr/orgs", "repos_url": "https://api.github.com/users/fdwr/repos", "events_url": "https://api.github.com/users/fdwr/events{/privacy}", "received_events_url": "https://api.github.com/users/fdwr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2014185961, "node_id": "MDU6TGFiZWwyMDE0MTg1OTYx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:coreruntime", "name": "component:coreruntime", "color": "303a93", "default": false, "description": "related to core runtime"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-08-27T20:52:11Z", "updated_at": "2020-08-29T22:18:22Z", "closed_at": "2020-08-29T22:18:22Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\n`numpy.einsum()`, which is the reference implementation for ONNX\u2019s [EinSum](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Einsum), accepts \u201cZY,YX->ZX\u201d whereas ORT CPU fails on the equation. Turns out that ORT only accepts lowercase \u201czy,yx->zx\u201d (that\u2019s the only test case of 29 that failed when writing tests for the DML EP).\r\n\r\n**Urgency**\r\nNot blocked on 1.5, as we just disabled the test case for the CPU EP on our side.\r\n\r\n    {\r\n      \"op_type\": \"Einsum\",\r\n      \"graph_name\": \"Einsum ZY,YX->ZX matmul 2D uppercase float32\",\r\n      \"equation\": \"ZY,YX->ZX\",\r\n      \"data_0\": [[0,1],[2,3]],\r\n      \"data_1\": [[1,2],[3,4]],\r\n      \"output\":  [[3,4],[11,16]],\r\n      \"T\": \"float32\",\r\n      \"verification\": { \"executor_filter\": \".*Cpu\" } // ORT rejects uppercase letters, even though the ONNX reference numpy accepts them.\r\n    },\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: master\r\n- Python version: NA\r\n- Visual Studio version (if applicable): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior: Run CPU EinSum with \"ZY,YX->ZX\".\r\n\r\n**Expected behavior**\r\nEinSum reads a-z and A-Z. Note a != A. So `iK,Kk->ik` is not equivalent to `ik,kk->ik`, but rather equivalent to `ij,jk->ik`.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4941", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4941/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4941/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4941/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4941", "id": 687504229, "node_id": "MDU6SXNzdWU2ODc1MDQyMjk=", "number": 4941, "title": "Expose DirectML to C#", "user": {"login": "jbrownkramer", "id": 4051645, "node_id": "MDQ6VXNlcjQwNTE2NDU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4051645?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbrownkramer", "html_url": "https://github.com/jbrownkramer", "followers_url": "https://api.github.com/users/jbrownkramer/followers", "following_url": "https://api.github.com/users/jbrownkramer/following{/other_user}", "gists_url": "https://api.github.com/users/jbrownkramer/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbrownkramer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbrownkramer/subscriptions", "organizations_url": "https://api.github.com/users/jbrownkramer/orgs", "repos_url": "https://api.github.com/users/jbrownkramer/repos", "events_url": "https://api.github.com/users/jbrownkramer/events{/privacy}", "received_events_url": "https://api.github.com/users/jbrownkramer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1758308176, "node_id": "MDU6TGFiZWwxNzU4MzA4MTc2", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C%23", "name": "api:C#", "color": "0e8a16", "default": false, "description": "related to the C# API"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-08-27T20:01:46Z", "updated_at": "2020-09-04T21:53:01Z", "closed_at": "2020-08-29T22:18:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Is your feature request related to a problem? Please describe.**\r\nI need to be able to run inference on an integrated Intel GPU from C#\r\n\r\n**System information**\r\nMicrosoft.ML.OnnxRuntime.DirectML v1.4.0 nuget package.\r\n\r\n**Describe the solution you'd like**\r\nThere seems to be no ability to select the DirectML provider from the C# api, even when using the DirectML build.  I would like that to be exposed.\r\n\r\n**Describe alternatives you've considered**\r\nI have considered using the OpenVINO provider, which apparently recently added C# support.  However, there is no nuget package, and I have not yet been able to make it through the installation and build process to see if that works.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4929", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4929/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4929/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4929/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4929", "id": 686706600, "node_id": "MDU6SXNzdWU2ODY3MDY2MDA=", "number": 4929, "title": "Onnx input size", "user": {"login": "AD-HO", "id": 59181393, "node_id": "MDQ6VXNlcjU5MTgxMzkz", "avatar_url": "https://avatars0.githubusercontent.com/u/59181393?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AD-HO", "html_url": "https://github.com/AD-HO", "followers_url": "https://api.github.com/users/AD-HO/followers", "following_url": "https://api.github.com/users/AD-HO/following{/other_user}", "gists_url": "https://api.github.com/users/AD-HO/gists{/gist_id}", "starred_url": "https://api.github.com/users/AD-HO/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AD-HO/subscriptions", "organizations_url": "https://api.github.com/users/AD-HO/orgs", "repos_url": "https://api.github.com/users/AD-HO/repos", "events_url": "https://api.github.com/users/AD-HO/events{/privacy}", "received_events_url": "https://api.github.com/users/AD-HO/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-26T22:30:23Z", "updated_at": "2020-08-27T00:51:08Z", "closed_at": "2020-08-27T00:51:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Everyone,\r\nI would like to know how to unfreeze the input size of an exported Onnx model?\r\nExample: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html\r\n\r\nSuper-Resolution is an image-to-image translation, i can\u2019t see how i would consume this model without resizing the input to 224 x 224 , and this mean the input quality will decrease\r\nthank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4910", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4910/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4910/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4910/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4910", "id": 685457165, "node_id": "MDU6SXNzdWU2ODU0NTcxNjU=", "number": 4910, "title": "Raspberry Pi OnnxRuntime error: call of overloaded \u2018abs(__gnu_cxx::__alloc_traits<std::allocator<float> >::value_type)\u2019 is ambiguous ASSERT_TRUE(std::abs(values_y[i] - f[i]) < 1e-6)", "user": {"login": "NagarajSMurthy", "id": 35525360, "node_id": "MDQ6VXNlcjM1NTI1MzYw", "avatar_url": "https://avatars0.githubusercontent.com/u/35525360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NagarajSMurthy", "html_url": "https://github.com/NagarajSMurthy", "followers_url": "https://api.github.com/users/NagarajSMurthy/followers", "following_url": "https://api.github.com/users/NagarajSMurthy/following{/other_user}", "gists_url": "https://api.github.com/users/NagarajSMurthy/gists{/gist_id}", "starred_url": "https://api.github.com/users/NagarajSMurthy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NagarajSMurthy/subscriptions", "organizations_url": "https://api.github.com/users/NagarajSMurthy/orgs", "repos_url": "https://api.github.com/users/NagarajSMurthy/repos", "events_url": "https://api.github.com/users/NagarajSMurthy/events{/privacy}", "received_events_url": "https://api.github.com/users/NagarajSMurthy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "snnn", "id": 856316, "node_id": "MDQ6VXNlcjg1NjMxNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/856316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snnn", "html_url": "https://github.com/snnn", "followers_url": "https://api.github.com/users/snnn/followers", "following_url": "https://api.github.com/users/snnn/following{/other_user}", "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snnn/subscriptions", "organizations_url": "https://api.github.com/users/snnn/orgs", "repos_url": "https://api.github.com/users/snnn/repos", "events_url": "https://api.github.com/users/snnn/events{/privacy}", "received_events_url": "https://api.github.com/users/snnn/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "snnn", "id": 856316, "node_id": "MDQ6VXNlcjg1NjMxNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/856316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snnn", "html_url": "https://github.com/snnn", "followers_url": "https://api.github.com/users/snnn/followers", "following_url": "https://api.github.com/users/snnn/following{/other_user}", "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snnn/subscriptions", "organizations_url": "https://api.github.com/users/snnn/orgs", "repos_url": "https://api.github.com/users/snnn/repos", "events_url": "https://api.github.com/users/snnn/events{/privacy}", "received_events_url": "https://api.github.com/users/snnn/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-08-25T12:47:16Z", "updated_at": "2020-09-02T03:45:23Z", "closed_at": "2020-09-02T03:45:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI'm following the steps given [here](https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/README.md#arm-32v7) to build onnxruntime on Raspberry Pi. The installation fails with the below error message. This issue has been discussed in #4393 but the platform was Linux and the answer couldn't clear the issue in my case. \r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution: Raspbian Stretch\r\n- ONNX Runtime installed from (source or binary): Source\r\n- ONNX Runtime version: master branch\r\n- Python version: 3.5.3\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source): 6.3.0\r\n\r\n**To Reproduce**\r\nAt step: RUN ./build.sh --use_openmp ${BUILDARGS} --build_shared_lib from the Dockerfile.arm32v7 file\r\nThe error output is: \r\n[100%] Building CXX object CMakeFiles/onnxruntime_global_thread_pools_test.dir/code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc.o\r\n\r\nIn file included from /code/onnxruntime/cmake/external/googletest/googletest/include/gtest/gtest.h:62:0,\r\n                 from /code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc:14:\r\n/code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc: In instantiation of \u2018void RunSession(OrtAllocator&, Ort::Session&, std::vector<Input>&, const char*, const std::vector<long long int>&, const std::vector<T>&, Ort::Value*) [with OutT = float; OrtAllocator = OrtAllocator]\u2019:\r\n/code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc:108:19:   required from \u2018void TestInference(Ort::Session&, std::vector<Input>&, const char*, const std::vector<long long int>&, const std::vector<OutT>&) [with T = const char*; OutT = float]\u2019\r\n/code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc:157:109:   required from here\r\n**/code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc:61:20: error: call of overloaded \u2018abs(__gnu_cxx::__alloc_traits<std::allocator<float> >::value_type)\u2019 is ambiguous\r\n     ASSERT_TRUE(std::abs(values_y[i] - f[i]) < 1e-6);**\r\n                 ~~~^~~~~~~~~~~~~~~~~~~~\r\nIn file included from /usr/include/c++/6/cstdlib:75:0,\r\n                 from /usr/include/c++/6/ext/string_conversions.h:41,\r\n                 from /usr/include/c++/6/bits/basic_string.h:5417,\r\n                 from /usr/include/c++/6/string:52,\r\n                 from /usr/include/c++/6/stdexcept:39,\r\n                 from /usr/include/c++/6/array:39,\r\n                 from /usr/include/c++/6/tuple:39,\r\n                 from /usr/include/c++/6/functional:55,\r\n                 from /usr/include/c++/6/memory:79,\r\n                 from /code/onnxruntime/include/onnxruntime/core/common/make_unique.h:28,\r\n                 from /code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc:4:\r\n/usr/include/stdlib.h:735:12: note: candidate: int abs(int)\r\n extern int abs (int __x) __THROW __attribute__ ((__const__)) __wur;\r\n            ^~~\r\nIn file included from /usr/include/c++/6/ext/string_conversions.h:41:0,\r\n                 from /usr/include/c++/6/bits/basic_string.h:5417,\r\n                 from /usr/include/c++/6/string:52,\r\n                 from /usr/include/c++/6/stdexcept:39,\r\n                 from /usr/include/c++/6/array:39,\r\n                 from /usr/include/c++/6/tuple:39,\r\n                 from /usr/include/c++/6/functional:55,\r\n                 from /usr/include/c++/6/memory:79,\r\n                 from /code/onnxruntime/include/onnxruntime/core/common/make_unique.h:28,\r\n                 from /code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc:4:\r\n/usr/include/c++/6/cstdlib:180:3: note: candidate: long long int std::abs(long long int)\r\n   abs(long long __x) { return __builtin_llabs (__x); }\r\n   ^~~\r\n/usr/include/c++/6/cstdlib:172:3: note: candidate: long int std::abs(long int)\r\n   abs(long __i) { return __builtin_labs(__i); }\r\n   ^~~\r\nCMakeFiles/onnxruntime_global_thread_pools_test.dir/build.make:75: recipe for target 'CMakeFiles/onnxruntime_global_thread_pools_test.dir/code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc.o' failed\r\nmake[2]: *** [CMakeFiles/onnxruntime_global_thread_pools_test.dir/code/onnxruntime/onnxruntime/test/global_thread_pools/test_inference.cc.o] Error 1\r\nCMakeFiles/Makefile2:771: recipe for target 'CMakeFiles/onnxruntime_global_thread_pools_test.dir/all' failed\r\nmake[1]: *** [CMakeFiles/onnxruntime_global_thread_pools_test.dir/all] Error 2\r\nmake: *** [all] Error 2\r\nMakefile:140: recipe for target 'all' failed\r\nTraceback (most recent call last):\r\n  File \"/code/onnxruntime/tools/ci_build/build.py\", line 1707, in <module>\r\n    sys.exit(main())\r\n  File \"/code/onnxruntime/tools/ci_build/build.py\", line 1662, in main\r\n    build_targets(args, cmake_path, build_dir, configs, args.parallel)\r\n  File \"/code/onnxruntime/tools/ci_build/build.py\", line 910, in build_targets\r\n    run_subprocess(cmd_args, env=env)\r\n  File \"/code/onnxruntime/tools/ci_build/build.py\", line 416, in run_subprocess\r\n    completed_process = subprocess.run(\r\n  File \"/usr/local/lib/python3.8/subprocess.py\", line 512, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['/usr/local/bin/cmake', '--build', '/code/onnxruntime/build/Linux/MinSizeRel', '--config', 'MinSizeRel']' returned non-zero exit status 2.\r\nThe command '/bin/sh -c ./build.sh --use_openmp ${BUILDARGS} --build_shared_lib' returned a non-zero code: 1\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4909", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4909/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4909/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4909/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4909", "id": 685397623, "node_id": "MDU6SXNzdWU2ODUzOTc2MjM=", "number": 4909, "title": "Install onnxruntime via pip on raspberry pi 3 fails", "user": {"login": "sbuschjaeger", "id": 32978972, "node_id": "MDQ6VXNlcjMyOTc4OTcy", "avatar_url": "https://avatars0.githubusercontent.com/u/32978972?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbuschjaeger", "html_url": "https://github.com/sbuschjaeger", "followers_url": "https://api.github.com/users/sbuschjaeger/followers", "following_url": "https://api.github.com/users/sbuschjaeger/following{/other_user}", "gists_url": "https://api.github.com/users/sbuschjaeger/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbuschjaeger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbuschjaeger/subscriptions", "organizations_url": "https://api.github.com/users/sbuschjaeger/orgs", "repos_url": "https://api.github.com/users/sbuschjaeger/repos", "events_url": "https://api.github.com/users/sbuschjaeger/events{/privacy}", "received_events_url": "https://api.github.com/users/sbuschjaeger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-25T11:15:02Z", "updated_at": "2020-08-25T20:47:00Z", "closed_at": "2020-08-25T20:47:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI cannot install onnxruntime on a raspberry pi 3 with berryconda (https://github.com/jjhelmus/berryconda) environment. \r\n\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian GNU/Linux 9 (stretch)\r\n- ONNX Runtime installed from (source or binary): not installed\r\n- ONNX Runtime version: not installed\r\n- Python version: 3.6.6\r\n\r\n**To Reproduce**\r\n\r\nTrying to install onnxruntime via pip on a pi 3 gives the following output:\r\n\r\n    (fi) fiactions@ls8rsp01:~ $ pip install onnxruntime\r\n    Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\n    ERROR: Could not find a version that satisfies the requirement onnxruntime (from versions: none)\r\n    ERROR: No matching distribution found for onnxruntime\r\n\r\nMy minimal conda environment file:\r\n\r\n    name: fi\r\n    channels:\r\n          - rpi\r\n    dependencies:\r\n          - scikit-learn\r\n          - numpy\r\n          - pip\r\n\r\nI manually updated pip just in case:\r\n\r\n    (fi) fiactions@ls8rsp01:~ $ pip --version\r\n    pip 20.2.2 from /home/fiactions/berryconda3/envs/fi/lib/python3.6/site-packages/pip (python 3.6)\r\n\r\nNote: I am aware of the fact that berryconda is inactive. However, the latest commits are from march this year so for now I deem it recent enough for my use-case. \r\n\r\n**Expected behavior**\r\nOnnxruntime should install\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4898", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4898/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4898/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4898/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4898", "id": 684960683, "node_id": "MDU6SXNzdWU2ODQ5NjA2ODM=", "number": 4898, "title": "Optimizer changes dynamic shape to constant", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2190555590, "node_id": "MDU6TGFiZWwyMTkwNTU1NTkw", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:optimizer", "name": "component:optimizer", "color": "303a93", "default": false, "description": "related to optimizers in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "askhade", "id": 6475296, "node_id": "MDQ6VXNlcjY0NzUyOTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6475296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/askhade", "html_url": "https://github.com/askhade", "followers_url": "https://api.github.com/users/askhade/followers", "following_url": "https://api.github.com/users/askhade/following{/other_user}", "gists_url": "https://api.github.com/users/askhade/gists{/gist_id}", "starred_url": "https://api.github.com/users/askhade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/askhade/subscriptions", "organizations_url": "https://api.github.com/users/askhade/orgs", "repos_url": "https://api.github.com/users/askhade/repos", "events_url": "https://api.github.com/users/askhade/events{/privacy}", "received_events_url": "https://api.github.com/users/askhade/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "askhade", "id": 6475296, "node_id": "MDQ6VXNlcjY0NzUyOTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6475296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/askhade", "html_url": "https://github.com/askhade", "followers_url": "https://api.github.com/users/askhade/followers", "following_url": "https://api.github.com/users/askhade/following{/other_user}", "gists_url": "https://api.github.com/users/askhade/gists{/gist_id}", "starred_url": "https://api.github.com/users/askhade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/askhade/subscriptions", "organizations_url": "https://api.github.com/users/askhade/orgs", "repos_url": "https://api.github.com/users/askhade/repos", "events_url": "https://api.github.com/users/askhade/events{/privacy}", "received_events_url": "https://api.github.com/users/askhade/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2020-08-24T20:56:57Z", "updated_at": "2020-08-31T20:14:04Z", "closed_at": "2020-08-31T20:14:03Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Describe the bug**\r\n1. We rely heavily on tensors with dynamic shape.\r\n2. We rely on `sess_options.optimized_model_filepath` and `ORT_ENABLE_BASIC` for basic optimizations, that would basically eliminate unused initializers (because onnx optimizer crashes)\r\n\r\nBefore optimization\r\n\r\n![image](https://user-images.githubusercontent.com/873905/91094541-cdcec700-e60f-11ea-81c2-0c9783fdfb8d.png)\r\n\r\nAfter optimization\r\n\r\n![image](https://user-images.githubusercontent.com/873905/91094986-72510900-e610-11ea-9aba-1779607484b3.png)\r\n\r\n\r\n**Urgency**\r\nWe have a workaround that avoids this optimizer mistake, but is an ugly hack. Plus, I am not sure if other places in the model might have the same issue.\r\nFirst we saw issue #4853 related to the exported model\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- ONNX Runtime installed from (source or binary): pip\r\n- ONNX Runtime version: 1.4\r\n- Python version: 3.6\r\n\r\n**To Reproduce**\r\n\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n[test.zip](https://github.com/microsoft/onnxruntime/files/5120158/test.zip)\r\n\r\n```python\r\nimport onnxruntime as rt\r\nsess_options = rt.SessionOptions()\r\nsess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_BASIC\r\nsess_options.optimized_model_filepath = \"test_opt.onnx\"\r\n\r\nsess = rt.InferenceSession(\"test.onnx\", sess_options)\r\n```\r\n\r\noutputs this onnx file:\r\n[test_opt.zip](https://github.com/microsoft/onnxruntime/files/5120198/test_opt.zip)\r\n\r\n\r\n**Expected behavior**\r\nOptimizer should not change dynamic shapes to constants.\r\n\r\n**Additional context**\r\nInitially we suspected `torch.full_like` is not generating `ConstantOfShape` and created [this issue](https://github.com/pytorch/pytorch/issues/43283) for exporter.\r\n\r\nHere is the code that generates the onnx file (PyTorch 1.6). It is a little piece of code that is part of a bigger model.\r\n\r\n```python\r\nimport torch\r\nimport onnxruntime as rt\r\n\r\ndef _align_box(im_h, im_w, bbs,\r\n               enlargement_factor, target_size, min_len, align='SHORTEST'):\r\n    zero = torch.as_tensor(0.0).float().to(bbs.device)\r\n\r\n    # enlarge the bounding box\r\n    w = (bbs[:, 2] * enlargement_factor).ceil()\r\n    h = (bbs[:, 3] * enlargement_factor).ceil()\r\n\r\n    # CXCYWH to XYWH\r\n    input_x = (bbs[:, 0] - w / 2).floor()\r\n    cond = input_x < 0\r\n    idx = torch.nonzero(cond).view(-1)\r\n    # TODO: use scatter_add when fixed: https://github.com/pytorch/pytorch/issues/26472\r\n    w = torch.scatter(w, 0, idx, w[idx] + input_x[idx])  # w[idx] += input_x[idx]\r\n    input_x = torch.where(cond, zero, input_x)  # input_x[idx] = 0\r\n    input_y = (bbs[:, 1] - h / 2).floor()\r\n    cond = input_y < 0\r\n    idx = torch.nonzero(cond).view(-1)\r\n    # TODO: use scatter_add when fixed: https://github.com/pytorch/pytorch/issues/26472\r\n    h = torch.scatter(h, 0, idx, h[idx] + input_y[idx])  # h[idx] += input_y[idx]\r\n    input_y = torch.where(cond, zero, input_y)  # input_y[idx] = 0\r\n\r\n    w = w.clamp(min=min_len)  # w[w < min_len] = min_len\r\n    h = h.clamp(min=min_len)  # h[h < min_len] = min_len\r\n\r\n    input_width = im_w - input_x\r\n    input_width = torch.where(w < input_width, w, input_width)  # input_width[idx=w < input_width] = w[idx]\r\n    input_height = im_h - input_y\r\n    input_height = torch.where(h < input_height, h, input_height)  # input_height[idx=h < input_height] = h[idx]\r\n    del w, h\r\n\r\n    # If too far to the right shift the coordinates\r\n    idx = input_width < min_len\r\n    input_width = torch.where(idx, min_len, input_width)  # input_width[idx] = min_len\r\n    input_x = torch.where(idx, im_w - min_len, input_x)  # input_x[idx] = im_w - min_len\r\n    idx = input_height < min_len\r\n    input_height = torch.where(idx, min_len, input_height)  # input_height[idx] = min_len\r\n    input_y = torch.where(idx, im_h - min_len, input_y)  # input_y[idx] = im_h - min_len\r\n\r\n    target_width = torch.full_like(input_width, target_size)\r\n    target_height = torch.full_like(input_height, target_size)\r\n\r\n    # align with constraint\r\n    if align == 'SHORTEST':\r\n        # Avoid `<=` because ONNX will export that as `Not` and `>` then we need another `Not`\r\n        #  instead, first compute `>` and use `~` to be exported as `Not`\r\n        nidx = input_width > input_height\r\n        idx = ~nidx  # this requires patch_logical_not_opset9 until bitwise_not for bool is in PyTorch\r\n        idx = torch.nonzero(idx).view(-1)\r\n        target_height = torch.scatter(target_height, 0, idx, target_size * input_height[idx] / input_width[idx])\r\n\r\n        idx = torch.nonzero(nidx).view(-1)\r\n        target_width = torch.scatter(target_width, 0, idx, target_size * input_width[idx] / input_height[idx])\r\n    elif align == 'LONGEST':\r\n        idx = input_width > input_height\r\n        nidx = ~idx  # this requires patch_logical_not_opset9 until bitwise_not for bool is in PyTorch\r\n        idx = torch.nonzero(idx).view(-1)\r\n        target_height = torch.scatter(target_height, 0, idx, target_size * input_height[idx] / input_width[idx])\r\n\r\n        idx = torch.nonzero(nidx).view(-1)\r\n        target_width = torch.scatter(target_width, 0, idx, target_size * input_width[idx] / input_height[idx])\r\n    else:\r\n        assert align == 'BOTH'\r\n\r\n    return input_x, input_y, input_width, input_height, target_width, target_height\r\n\r\nclass Module(torch.nn.Module):\r\n    def forward(self, img, bbs):\r\n        _, _, height, width = img.shape[0], img.shape[1], img.shape[2], img.shape[3]\r\n        # When tracing (for onnx export this is Tensor)\r\n        if not isinstance(height, torch.Tensor):\r\n            height, width = torch.as_tensor(height).to(img.device), torch.as_tensor(width).to(img.device)\r\n\r\n        input_x, input_y, input_width, input_height, target_width, target_height = _align_box(\r\n            height.float(), width.float(), bbs, 1.5, 256, torch.as_tensor(3).float()\r\n        )\r\n        return input_x, input_y, input_width, input_height, target_width, target_height\r\n\r\nmodel = Module()\r\nimg = torch.empty((1,3,256,512))\r\nbbs = torch.as_tensor([[0, 0, 100, 100], [0, 0, 500, 100]]).float()\r\n\r\ntargets = ['input_x', 'input_y', 'input_width', 'input_height', 'target_width', 'target_height']\r\ntorch.onnx.export(model, (img, bbs), 'test.onnx',\r\n                  verbose=True,\r\n                  input_names=['img', 'bbs'],\r\n                  output_names=targets,\r\n                  opset_version=11,\r\n                  dynamic_axes={'img': {2: 'h', 3: 'w'}, 'bbs': {0: 'rpn'}})\r\n\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4883", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4883/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4883/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4883/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4883", "id": 683525671, "node_id": "MDU6SXNzdWU2ODM1MjU2NzE=", "number": 4883, "title": "ONNXRuntime StandardScaler distinctions", "user": {"login": "ladodc", "id": 10833738, "node_id": "MDQ6VXNlcjEwODMzNzM4", "avatar_url": "https://avatars0.githubusercontent.com/u/10833738?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ladodc", "html_url": "https://github.com/ladodc", "followers_url": "https://api.github.com/users/ladodc/followers", "following_url": "https://api.github.com/users/ladodc/following{/other_user}", "gists_url": "https://api.github.com/users/ladodc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ladodc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ladodc/subscriptions", "organizations_url": "https://api.github.com/users/ladodc/orgs", "repos_url": "https://api.github.com/users/ladodc/repos", "events_url": "https://api.github.com/users/ladodc/events{/privacy}", "received_events_url": "https://api.github.com/users/ladodc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": {"login": "xadupre", "id": 22452781, "node_id": "MDQ6VXNlcjIyNDUyNzgx", "avatar_url": "https://avatars1.githubusercontent.com/u/22452781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xadupre", "html_url": "https://github.com/xadupre", "followers_url": "https://api.github.com/users/xadupre/followers", "following_url": "https://api.github.com/users/xadupre/following{/other_user}", "gists_url": "https://api.github.com/users/xadupre/gists{/gist_id}", "starred_url": "https://api.github.com/users/xadupre/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xadupre/subscriptions", "organizations_url": "https://api.github.com/users/xadupre/orgs", "repos_url": "https://api.github.com/users/xadupre/repos", "events_url": "https://api.github.com/users/xadupre/events{/privacy}", "received_events_url": "https://api.github.com/users/xadupre/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "xadupre", "id": 22452781, "node_id": "MDQ6VXNlcjIyNDUyNzgx", "avatar_url": "https://avatars1.githubusercontent.com/u/22452781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xadupre", "html_url": "https://github.com/xadupre", "followers_url": "https://api.github.com/users/xadupre/followers", "following_url": "https://api.github.com/users/xadupre/following{/other_user}", "gists_url": "https://api.github.com/users/xadupre/gists{/gist_id}", "starred_url": "https://api.github.com/users/xadupre/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xadupre/subscriptions", "organizations_url": "https://api.github.com/users/xadupre/orgs", "repos_url": "https://api.github.com/users/xadupre/repos", "events_url": "https://api.github.com/users/xadupre/events{/privacy}", "received_events_url": "https://api.github.com/users/xadupre/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-08-21T12:27:57Z", "updated_at": "2020-08-27T10:42:00Z", "closed_at": "2020-08-27T10:41:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\nI have saved my pipeline with scikit-learn StandardScaler into ONNX Format. After that I have inferenced this model with ONNXRuntime and got a bit different results as in scikit-learn. See my example. Why is it so?\r\n\r\n\r\n[ONNXRuntime StandardScaler distinctions.zip](https://github.com/microsoft/onnxruntime/files/5108748/ONNXRuntime.StandardScaler.distinctions.zip)\r\n\r\n**System information**\r\n- OS Platform and Distribution (Win10\r\n- ONNX Runtime version: 1.3\r\n\r\n\r\n- Python version: 3.7\r\n\r\n**To Reproduce**\r\n\r\n```\r\nimport numpy\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\ndata = load_iris()\r\nX = data.data[:5, :2]\r\ny = data.target\r\n\r\npipeline = Pipeline([('scaler', StandardScaler())])\r\npipeline.fit(X, y)\r\nprint(X)\r\nprint(pipe.transform(X)\r\n```\r\n[[5.1 3.5]\r\n [4.9 3. ]\r\n [4.7 3.2]\r\n [4.6 3.1]\r\n [5.  3.6]]\r\n[[ 1.29399**328**  0.950255**27**]\r\n [ 0.21566555 -1.2094158 ]\r\n [-0.86266219 -0.34554737]\r\n [-1.40182605 -0.77748158]\r\n [ 0.75482941  1.38218948]]\r\n```\r\nfrom onnxmltools import convert_sklearn\r\nfrom onnxmltools.utils import save_model\r\nfrom onnxmltools.convert.common.data_types import FloatTensorType, StringTensorType\r\n\r\nonnx_model = convert_sklearn(pipeline, 'data preprocessing',\r\n                            [('X1', FloatTensorType([None, 1])),\r\n                            ('X2', FloatTensorType([None, 1]))])\r\n\r\n#Save data preprocessing model\r\nsave_model(onnx_model, 'preprocessing.onnx')\r\n\r\nimport onnxruntime as rt\r\n\r\nsess = rt.InferenceSession(\"preprocessing.onnx\")\r\n\r\nlabel_name = sess.get_outputs()[0].name\r\nresult = sess.run([label_name], {\"X1\": X[:,0].reshape(5,1).astype(numpy.float32),\"X2\": X[:,1].reshape(5,1).astype(numpy.float32)})[0]\r\nprint(result)\r\n```\r\n[[ 1.29399**2**    0.950255**4** ]\r\n [ 0.21566534 -1.2094157 ]\r\n [-0.8626639  -0.34554705]\r\n [-1.4018273  -0.77748185]\r\n [ 0.7548287   1.3821892 ]]\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4876", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4876/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4876/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4876/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4876", "id": 683083264, "node_id": "MDU6SXNzdWU2ODMwODMyNjQ=", "number": 4876, "title": "Separate Nugets for different execution providers", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1758308176, "node_id": "MDU6TGFiZWwxNzU4MzA4MTc2", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C%23", "name": "api:C#", "color": "0e8a16", "default": false, "description": "related to the C# API"}, {"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 2212485427, "node_id": "MDU6TGFiZWwyMjEyNDg1NDI3", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "askhade", "id": 6475296, "node_id": "MDQ6VXNlcjY0NzUyOTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6475296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/askhade", "html_url": "https://github.com/askhade", "followers_url": "https://api.github.com/users/askhade/followers", "following_url": "https://api.github.com/users/askhade/following{/other_user}", "gists_url": "https://api.github.com/users/askhade/gists{/gist_id}", "starred_url": "https://api.github.com/users/askhade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/askhade/subscriptions", "organizations_url": "https://api.github.com/users/askhade/orgs", "repos_url": "https://api.github.com/users/askhade/repos", "events_url": "https://api.github.com/users/askhade/events{/privacy}", "received_events_url": "https://api.github.com/users/askhade/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "askhade", "id": 6475296, "node_id": "MDQ6VXNlcjY0NzUyOTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6475296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/askhade", "html_url": "https://github.com/askhade", "followers_url": "https://api.github.com/users/askhade/followers", "following_url": "https://api.github.com/users/askhade/following{/other_user}", "gists_url": "https://api.github.com/users/askhade/gists{/gist_id}", "starred_url": "https://api.github.com/users/askhade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/askhade/subscriptions", "organizations_url": "https://api.github.com/users/askhade/orgs", "repos_url": "https://api.github.com/users/askhade/repos", "events_url": "https://api.github.com/users/askhade/events{/privacy}", "received_events_url": "https://api.github.com/users/askhade/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-08-20T20:42:04Z", "updated_at": "2020-08-20T23:27:55Z", "closed_at": "2020-08-20T23:27:22Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Is your feature request related to a problem? Please describe.**\r\nWe want to try different execution providers, perhaps chose the best for each model.\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): 1.4\r\n\r\n**Describe the solution you'd like**\r\nProvide *extra* nugets for `Microsoft.ML.OnnxRuntime.Dnnl`, ... from nuget.org that we can install together with `Microsoft.ML.OnnxRuntime`.\r\n\r\n\r\n**Describe alternatives you've considered**\r\nBuild from source is not an option for us\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4869", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4869/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4869/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4869/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4869", "id": 682365604, "node_id": "MDU6SXNzdWU2ODIzNjU2MDQ=", "number": 4869, "title": "About hyperthreading and openmp on the CPU", "user": {"login": "Peppa-cs", "id": 60772185, "node_id": "MDQ6VXNlcjYwNzcyMTg1", "avatar_url": "https://avatars0.githubusercontent.com/u/60772185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Peppa-cs", "html_url": "https://github.com/Peppa-cs", "followers_url": "https://api.github.com/users/Peppa-cs/followers", "following_url": "https://api.github.com/users/Peppa-cs/following{/other_user}", "gists_url": "https://api.github.com/users/Peppa-cs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Peppa-cs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Peppa-cs/subscriptions", "organizations_url": "https://api.github.com/users/Peppa-cs/orgs", "repos_url": "https://api.github.com/users/Peppa-cs/repos", "events_url": "https://api.github.com/users/Peppa-cs/events{/privacy}", "received_events_url": "https://api.github.com/users/Peppa-cs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-20T03:14:46Z", "updated_at": "2020-08-21T04:13:44Z", "closed_at": "2020-08-21T04:13:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi. I found If you use OpenMP on the CPU, using hyperthreading can hardly bring any improvement. I test a simple vgg16 model on the CPU. The following is a comparison of my experiment.\r\n\r\n1. using a single thread, latency is 10.37s.\r\n2. using 48 threads with hyperthreading, latency is 0.47s.\r\n3. using 24 threads without hyperthreading, latency is 0.53s.\r\n\r\nIt seems that if I use 48 threads with hyperthreading, it only brings 22x improvement. The 22x improvement is half of the num of threads almost. And if I use 24 threads without hyperthreading, it brings 19x improvement. The 19x improvement is not much different from the num of threads.\r\n\r\n**System information**\r\n- OS Platform and Distribution  Linux Ubuntu 16.04:\r\n- ONNX Runtime installed from binary\r\n- ONNX Runtime version:1.4.0\r\n- Python version: 3.6.5\r\n- CPU (i have break hyperthreading):\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                56\r\nOn-line CPU(s) list:   0-27\r\nOff-line CPU(s) list:  28-55\r\nThread(s) per core:    1\r\nCore(s) per socket:    14\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 63\r\nModel name:            Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00GHz\r\nStepping:              2\r\nCPU MHz:               1200.202\r\nCPU max MHz:           3000.0000\r\nCPU min MHz:           1200.0000\r\nBogoMIPS:              4001.81\r\nVirtualization:        VT-x\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              35840K\r\nNUMA node0 CPU(s):     0-13\r\nNUMA node1 CPU(s):     14-27\r\n\r\n\r\n**To Reproduce**\r\nuse_openmp = True/Flase\r\nif use_openmp:\r\n    os.environ[\"OMP_NUM_THREADS\"] = str(nthreads)\r\nelse:\r\n    os.environ[\"OMP_NUM_THREADS\"] = '1'\r\n\r\nimport onnxruntime\r\nsession = onnxruntime.InferenceSession(inputfile)\r\nsession.set_providers(['CPUExecutionProvider'])\r\ninput_name = session.get_inputs()[0].name  \r\n\r\nsessionOptions = onnxruntime.SessionOptions()\r\nif use_openmp == False:\r\n    sessionOptions.intra_op_num_threads = 1\r\n    sessionOptions.inter_op_num_threads = 1\r\n    sessionOptions.execution_mode = onnxruntime.ExecutionMode.ORT_SEQUENTIAL\r\n\r\nWhen use_openmp is True, I use multi-threads by setting \"OMP_NUM_THREADS\". And a single thread is set by \"OMP_NUM_THREADS\" =1 and intra_op_num_threads/inter_op_num_threads  =1.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4853", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4853/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4853/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4853/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4853", "id": 681472449, "node_id": "MDU6SXNzdWU2ODE0NzI0NDk=", "number": 4853, "title": " INVALID_ARGUMENT : Non-zero status code returned while running ScatterElements node. Name:'ScatterElements_102' Status Message: Indices dim=24 at pos=0 is greater than input dim=2", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-19T00:59:51Z", "updated_at": "2020-08-19T19:29:43Z", "closed_at": "2020-08-19T19:29:42Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Describe the bug**\r\nI get the error with this description\r\n\r\n```text\r\n INVALID_ARGUMENT : Non-zero status code returned while running ScatterElements node. Name:'roi_pool_ScatterElements_102' Status Message: Indices dim=24 at pos=0 is greater than input dim=2\r\n```\r\n`torch.scatter` does not have this restriction. Previous version of scatter also was fine, but it is now deprecated and `ScatterElements` seems to require this condition?\r\n\r\nAs long as individual element inside the index are in range scatter should work\r\n\r\n**Urgency**\r\nIf there are particular important use cases blocked by this or strict project-related timelines, please share more information and dates. If there are no hard deadlines, please specify none.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- ONNX Runtime installed from (source or binary): pip\r\n- ONNX Runtime version: 1.2\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n**Expected behavior**\r\nBehave same as torch.scatter\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4839", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4839/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4839/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4839/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4839", "id": 680909304, "node_id": "MDU6SXNzdWU2ODA5MDkzMDQ=", "number": 4839, "title": "No Op registered for ArrayFeatureExtractor with domain_version of 11", "user": {"login": "Nirmal-Neel", "id": 32963085, "node_id": "MDQ6VXNlcjMyOTYzMDg1", "avatar_url": "https://avatars3.githubusercontent.com/u/32963085?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nirmal-Neel", "html_url": "https://github.com/Nirmal-Neel", "followers_url": "https://api.github.com/users/Nirmal-Neel/followers", "following_url": "https://api.github.com/users/Nirmal-Neel/following{/other_user}", "gists_url": "https://api.github.com/users/Nirmal-Neel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nirmal-Neel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nirmal-Neel/subscriptions", "organizations_url": "https://api.github.com/users/Nirmal-Neel/orgs", "repos_url": "https://api.github.com/users/Nirmal-Neel/repos", "events_url": "https://api.github.com/users/Nirmal-Neel/events{/privacy}", "received_events_url": "https://api.github.com/users/Nirmal-Neel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-18T10:35:09Z", "updated_at": "2020-08-18T12:01:07Z", "closed_at": "2020-08-18T12:00:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI am using onnxruntime 1.3.0. When I try to start an InferenceSession with an onnx model (a KNNRegressor model created using skl2onnx 1.6.0) with ir_version 6 and opset version 11, I get the following error - \r\n`[ONNXRuntimeError] : 10 : INVALID_GRAPH : This is an invalid model. Error in Node:Ar_ArrayFeatureExtractor : No Op registered for ArrayFeatureExtractor with domain_version of 11`.\r\n\r\nCould you help me in resolving this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4821", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4821/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4821/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4821/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4821", "id": 680227084, "node_id": "MDU6SXNzdWU2ODAyMjcwODQ=", "number": 4821, "title": "\"Segmentation fault\" (core dumped) ", "user": {"login": "zhuxiaoxuhit", "id": 32813150, "node_id": "MDQ6VXNlcjMyODEzMTUw", "avatar_url": "https://avatars2.githubusercontent.com/u/32813150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuxiaoxuhit", "html_url": "https://github.com/zhuxiaoxuhit", "followers_url": "https://api.github.com/users/zhuxiaoxuhit/followers", "following_url": "https://api.github.com/users/zhuxiaoxuhit/following{/other_user}", "gists_url": "https://api.github.com/users/zhuxiaoxuhit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuxiaoxuhit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuxiaoxuhit/subscriptions", "organizations_url": "https://api.github.com/users/zhuxiaoxuhit/orgs", "repos_url": "https://api.github.com/users/zhuxiaoxuhit/repos", "events_url": "https://api.github.com/users/zhuxiaoxuhit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuxiaoxuhit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1395147764, "node_id": "MDU6TGFiZWwxMzk1MTQ3NzY0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C/C++", "name": "api:C/C++", "color": "0e8a16", "default": false, "description": "related to the  public API(and ABI) for onnxruntime"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-08-17T13:14:59Z", "updated_at": "2020-08-18T03:25:14Z", "closed_at": "2020-08-18T03:25:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI convert pb file to onnx , it has two inputs and one output. Then I successful compiled cpp file. \r\nSo far everything is normal\uff0cno error or warning. When i execute the out file(using gnu compiler),got this message.\r\n///////////////////////////////////////////\r\nUsing Onnxruntime C++ API\r\nNumber of inputs = 2\r\nInput 0 : name=text_len:0\r\nInput 0 : type=6\r\nInput 0 : num_dims=1\r\nInput 0 : dim 0=1\r\nInput 1 : name=text:0\r\nInput 1 : type=6\r\nInput 1 : num_dims=2\r\nInput 1 : dim 0=1\r\nInput 1 : dim 1=66\r\nSegmentation fault (core dumped)\r\n//////////////////////////////////////////\r\nusing gdb to locate this error,got this:\r\n![image](https://user-images.githubusercontent.com/32813150/90401570-b738e500-e0d0-11ea-95db-aed768dce45e.png)\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.5.1804\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: master\r\n- Python version: 3.6\r\n\r\n**To Reproduce**\r\n```c++\r\n#include <assert.h>\r\n#include <vector>\r\n#include \"onnxruntime_cxx_api.h\"\r\n\r\nint main(int argc, char* argv[]) { Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"test\"); \r\n  Ort::SessionOptions session_options;\r\n  session_options.SetIntraOpNumThreads(1); \r\n  session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED); const char* model_path = \"model.onnx\"; printf(\"Using Onnxruntime C++ API\\n\");\r\n  Ort::Session session(env, model_path, session_options);\r\n  \r\n  Ort::AllocatorWithDefaultOptions allocator;\r\n\r\n  // print model input layer (node names, types, shape etc.) Ort::AllocatorWithDefaultOptions allocator; \r\n  // print number of model input nodes\r\n  size_t num_input_nodes = session.GetInputCount();\r\n  std::vector<const char*> input_node_names(num_input_nodes);\r\n  std::vector<std::vector<int64_t> > input_node_dims;\r\n\r\n  printf(\"Number of inputs = %zu\\n\", num_input_nodes);\r\n\r\n  // iterate over all input nodes\r\n  for (int i = 0; i < num_input_nodes; i++) {\r\n    // print input node names\r\n    char* input_name = session.GetInputName(i, allocator);\r\n    printf(\"Input %d : name=%s\\n\", i, input_name);\r\n    input_node_names[i] = input_name;\r\n\r\n    // print input node types\r\n    Ort::TypeInfo type_info = session.GetInputTypeInfo(i);\r\n    auto tensor_info = type_info.GetTensorTypeAndShapeInfo();\r\n\r\n    ONNXTensorElementDataType type = tensor_info.GetElementType();\r\n    printf(\"Input %d : type=%d\\n\", i, type);\r\n\r\n    // print input shapes/dims\r\n    input_node_dims.push_back(tensor_info.GetShape());\r\n    printf(\"Input %d : num_dims=%zu\\n\", i, input_node_dims.size());\r\n    for (int j = 0; j < input_node_dims[i].size(); j++)\r\n    \u250a printf(\"Input %d : dim %d=%jd\\n\", i, j, input_node_dims[i][j]);\r\n  }\r\n  \r\n  size_t input_tensor_size_0 = 1;  \r\n  size_t input_tensor_size_1 = 66; \r\n  std::vector<int64_t> input_tensor_values_0 = {66};\r\n  std::vector<int64_t> input_tensor_values_1 = {{3,   89,  75,  11,   4,  74,  62,  12,   7,  94,  82,  12,   4,  66,  62, 13,  5, 115,  60, 13,  4,  98,  82,  10,   5,  87, 78,  12,   4, 98,  92, 11, 4,\\ \r\n                                            \u250a   \u250a66,  67,  14,   6, 114,  75,  13,   4, 115,  99,  13,   5,  97,  92,  13,  4, 93,  79,  13,  4, 85,  75,  10,   5,  73,  92, 13,   4,  88, 59,  12,  3, 1}};\r\n  std::vector<const char*> output_node_names = {\"output_minmax:0\"};\r\n\r\n  // create input tensor object from data values\r\n  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\r\n  Ort::Value input_tensor_0 = Ort::Value::CreateTensor<int64_t>(memory_info, input_tensor_values_0.data(), input_tensor_size_0, input_node_dims[0].data(), 1); \r\n  Ort::Value input_tensor_1 = Ort::Value::CreateTensor<int64_t>(memory_info, input_tensor_values_1.data(), input_tensor_size_1, input_node_dims[1].data(), 2); \r\n  Ort::Value* input_tensors[input_node_names.size()];\r\n  input_tensors[0] = &input_tensor_0; \r\n  input_tensors[1] = &input_tensor_1; \r\n\r\n  auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_node_names.data(), (const Ort::Value*)input_tensors, 2, output_node_names.data(), 1);\r\n  return 0;\r\n}\r\n\r\n```\r\n\r\n**Expected behavior**\r\nNO ERROR.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4820", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4820/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4820/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4820/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4820", "id": 680133148, "node_id": "MDU6SXNzdWU2ODAxMzMxNDg=", "number": 4820, "title": "Ort::Env cannot be static", "user": {"login": "mbelloc", "id": 35163952, "node_id": "MDQ6VXNlcjM1MTYzOTUy", "avatar_url": "https://avatars1.githubusercontent.com/u/35163952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mbelloc", "html_url": "https://github.com/mbelloc", "followers_url": "https://api.github.com/users/mbelloc/followers", "following_url": "https://api.github.com/users/mbelloc/following{/other_user}", "gists_url": "https://api.github.com/users/mbelloc/gists{/gist_id}", "starred_url": "https://api.github.com/users/mbelloc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mbelloc/subscriptions", "organizations_url": "https://api.github.com/users/mbelloc/orgs", "repos_url": "https://api.github.com/users/mbelloc/repos", "events_url": "https://api.github.com/users/mbelloc/events{/privacy}", "received_events_url": "https://api.github.com/users/mbelloc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-17T10:39:37Z", "updated_at": "2020-08-17T17:29:05Z", "closed_at": "2020-08-17T17:10:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nIt seems Ort::Env cannot be a static member. I want to parallelize inference using a class that handles a static Ort::Env * and non static Ort::Session. When I define the static member like so in the .cpp file:\r\n`Ort::Env* unique_env = new Ort::Env(ORT_LOGGING_LEVEL_WARNING, \"onnx_unique_env\")`\r\nthe compilation is ok but when I run my program it crashes instantly because the .dll could not be open (class that handles Ort::Env and session is exported and used by multiple other libraries).\r\n\r\nAs a workaround I just initialize the Ort::Env to nullptr in the .cpp file (so it is not constructed) and the class constructor check if the static variable is nullptr in order to initialize it only once. It seems to delay enough the construction of the Ort::Env and solve the dll problem.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- ONNX Runtime installed from (source or binary): binary\r\n- ONNX Runtime version: 1.4.0\r\n- Python version:\r\n- Visual Studio version (if applicable): VS 2017\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\n\r\n```\r\n// .h\r\n\r\nstatic Ort::Env* _env;\r\n```\r\n```\r\n// .cpp\r\n\r\n// outside of any class method\r\nOrt::Env* MyClass::_env = new Ort::Env(ORT_LOGGING_LEVEL_WARNING, \"onnx_unique_env\")\r\n\r\n// Then in some class method\r\nthis->_session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);\r\nthis->_session = new Ort::Session(*OnnxRuntime::_env, \"/some/model.onnx\", this->_session_options);\r\n```\r\n\r\n\r\n**Expected behavior**\r\nOrt::Env can be created at the very beginning of the program and thus can be used as a static variable without manually delaying its construction.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4818", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4818/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4818/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4818/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4818", "id": 680089905, "node_id": "MDU6SXNzdWU2ODAwODk5MDU=", "number": 4818, "title": "Got error onnxruntime c++ api with two inputs", "user": {"login": "zhuxiaoxuhit", "id": 32813150, "node_id": "MDQ6VXNlcjMyODEzMTUw", "avatar_url": "https://avatars2.githubusercontent.com/u/32813150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuxiaoxuhit", "html_url": "https://github.com/zhuxiaoxuhit", "followers_url": "https://api.github.com/users/zhuxiaoxuhit/followers", "following_url": "https://api.github.com/users/zhuxiaoxuhit/following{/other_user}", "gists_url": "https://api.github.com/users/zhuxiaoxuhit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuxiaoxuhit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuxiaoxuhit/subscriptions", "organizations_url": "https://api.github.com/users/zhuxiaoxuhit/orgs", "repos_url": "https://api.github.com/users/zhuxiaoxuhit/repos", "events_url": "https://api.github.com/users/zhuxiaoxuhit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuxiaoxuhit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-17T09:32:06Z", "updated_at": "2020-08-17T12:58:51Z", "closed_at": "2020-08-17T12:58:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nRun with two input nodes and one output node, got error message:\r\n///////////////////////////////////////////////////////////////////////////\r\nmodel.cpp: In function \u2018int main(int, char**)\u2019:\r\nmodel.cpp:67:158: error: no matching function for call to \u2018Ort::Session::Run(Ort::RunOptions, const char**, const OrtValue* const*, int, const char**, int)\u2019\r\n   auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_node_names.data(), (const OrtValue* const*)&input_tensors, 2, output_node_names.data(), 1);                                                                                                                          ^\r\nmodel.cpp:67:158: note: candidates are:\r\nIn file included from onnxruntime_cxx_api.h:389:0,\r\n                 from model.cpp:4:\r\nonnxruntime_cxx_inline.h:240:27: note: std::vector<Ort::Value> Ort::Session::Run(const Ort::RunOptions&, const char* const*, const Ort::Value*, size_t, const char* const*, size_t)\r\n inline std::vector<Value> Session::Run(const RunOptions& run_options, const char* const* input_names, const Value* input_values, size_t input_count,\r\n                           ^\r\nonnxruntime_cxx_inline.h:240:27: note:   no known conversion for argument 3 from \u2018const OrtValue* const*\u2019 to \u2018const Ort::Value*\u2019\r\nonnxruntime_cxx_inline.h:249:13: note: void Ort::Session::Run(const Ort::RunOptions&, const char* const*, const Ort::Value*, size_t, const char* const*, Ort::Value*, size_t)\r\n inline void Session::Run(const RunOptions& run_options, const char* const* input_names, const Value* input_values, size_t input_count,\r\n             ^\r\nonnxruntime_cxx_inline.h:249:13: note:   candidate expects 7 arguments, 6 provided\r\n\r\n//////////////////////////////////////////////////////////////////////////\r\n\r\n\r\n**Screenshots**\r\nAt onnxruntime_cxx_inline.h, I find the api of what i used(first verison)\uff1a\r\n![image](https://user-images.githubusercontent.com/32813150/90380551-c6a73680-e0ae-11ea-9099-a792e858fc23.png)\r\n\r\n**inputs and outputs**\r\nUsing Onnxruntime C++ API\r\nNumber of inputs = 2\r\nInput 0 : name=text_len:0\r\nInput 0 : type=6\r\nInput 0 : num_dims=1\r\nInput 0 : dim 0=1\r\nInput 1 : name=text:0\r\nInput 1 : type=6\r\nInput 1 : num_dims=2\r\nInput 1 : dim 0=1\r\nInput 1 : dim 1=-1\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution : CentOS Linux release 7.5.1804\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version:  master\r\n- Python version: 3.6\r\n\r\n**To Reproduce**\r\n#include <assert.h>\r\n#include <vector>\r\n//#include <onnxruntime_cxx_api.h>\r\n#include \"onnxruntime_cxx_api.h\"\r\n\r\nint main(int argc, char* argv[]) {\r\n  Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"test\");\r\n\r\n  Ort::SessionOptions session_options;\r\n  session_options.SetIntraOpNumThreads(1); session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED); const char* model_path = \"model.onnx\";\r\n\r\n  printf(\"Using Onnxruntime C++ API\\n\");\r\n  Ort::Session session(env, model_path, session_options);\r\n\r\n  // print model input layer (node names, types, shape etc.)\r\n  Ort::AllocatorWithDefaultOptions allocator;\r\n\r\n  // print number of model input nodes\r\n  size_t num_input_nodes = session.GetInputCount();\r\n  std::vector<const char*> input_node_names(num_input_nodes);\r\n  std::vector<std::vector<int64_t> > input_node_dims;\r\n\r\n  printf(\"Number of inputs = %zu\\n\", num_input_nodes);\r\n\r\n  // iterate over all input nodes\r\n  for (int i = 0; i < num_input_nodes; i++) {\r\n    // print input node names\r\n    char* input_name = session.GetInputName(i, allocator);\r\n    printf(\"Input %d : name=%s\\n\", i, input_name);\r\n    input_node_names[i] = input_name;\r\n\r\n    // print input node types\r\n    Ort::TypeInfo type_info = session.GetInputTypeInfo(i);\r\n    auto tensor_info = type_info.GetTensorTypeAndShapeInfo();\r\n\r\n    ONNXTensorElementDataType type = tensor_info.GetElementType();\r\n    printf(\"Input %d : type=%d\\n\", i, type);\r\n\r\n    // print input shapes/dims\r\n    input_node_dims.push_back(tensor_info.GetShape());\r\n    printf(\"Input %d : num_dims=%zu\\n\", i, input_node_dims.size());\r\n    for (int j = 0; j < input_node_dims[i].size(); j++)\r\n    \u250a printf(\"Input %d : dim %d=%jd\\n\", i, j, input_node_dims[i][j]);\r\n  }\r\n  \r\n  size_t input_tensor_size_0 = 1;  \r\n  size_t input_tensor_size_1 = 16; \r\n  std::vector<int64_t> input_tensor_values_0 = {16};\r\n  std::vector<int64_t> input_tensor_values_1 = {3, 89, 75, 11, 4, 74, 62, 12, 7, 94, 82, 12, 4, 66, 3, 1}; \r\n  \r\n  std::vector<const char*> output_node_names = {\"output_minmax:0\"};\r\n\r\n  // create input tensor object from data values\r\n  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\r\n  Ort::Value input_tensor_0 = Ort::Value::CreateTensor<int64_t>(memory_info, input_tensor_values_0.data(), input_tensor_size_0, input_node_dims[0].data(), 1); \r\n  Ort::Value input_tensor_1 = Ort::Value::CreateTensor<int64_t>(memory_info, input_tensor_values_1.data(), input_tensor_size_1, input_node_dims[1].data(), 2);\r\n  OrtValue* input_tensors[input_node_names.size()];\r\n  input_tensors[0] = (OrtValue*)&input_tensor_0;\r\n  input_tensors[1] = (OrtValue*)&input_tensor_1;\r\n\r\n  //score model & input tensor, get back output tensor\r\n  auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_node_names.data(), (const OrtValue* const*)&input_tensors, 2, output_node_names.data(), 1);\r\n  assert(output_tensors.size() == 1 && output_tensors.front().IsTensor());\r\n\r\n\r\n**Expected behavior**\r\nNo error.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4816", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4816/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4816/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4816/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4816", "id": 679490538, "node_id": "MDU6SXNzdWU2Nzk0OTA1Mzg=", "number": 4816, "title": "No performance different between ORT_ENABLE_ALL and ORT_DISABLE_ALL ", "user": {"login": "sapjunior", "id": 987946, "node_id": "MDQ6VXNlcjk4Nzk0Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/987946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sapjunior", "html_url": "https://github.com/sapjunior", "followers_url": "https://api.github.com/users/sapjunior/followers", "following_url": "https://api.github.com/users/sapjunior/following{/other_user}", "gists_url": "https://api.github.com/users/sapjunior/gists{/gist_id}", "starred_url": "https://api.github.com/users/sapjunior/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sapjunior/subscriptions", "organizations_url": "https://api.github.com/users/sapjunior/orgs", "repos_url": "https://api.github.com/users/sapjunior/repos", "events_url": "https://api.github.com/users/sapjunior/events{/privacy}", "received_events_url": "https://api.github.com/users/sapjunior/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-15T03:26:16Z", "updated_at": "2020-08-18T13:22:39Z", "closed_at": "2020-08-18T13:22:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nNo performance gain at all compared between ORT_ENABLE_ALL and ORT_DISABLE_ALL\r\nLink to model: https://drive.google.com/file/d/14Lv1dDiOU9B_FeKe7_6mkh8j_XpnlKZg/view?usp=sharing\r\n\r\nHere is the speed comparison code\r\n\r\n```\r\nimport onnxruntime as rt\r\nimport numpy as np\r\nimport time\r\n\r\nsessOptionsA = rt.SessionOptions()\r\nsessOptionsA.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL    \r\nrtA = rt.InferenceSession('t10-exp.onnx', sessOptionsA)\r\n\r\nsessOptionsB = rt.SessionOptions()\r\nsessOptionsB.graph_optimization_level = rt.GraphOptimizationLevel.ORT_DISABLE_ALL    \r\nrtB = rt.InferenceSession('t10-exp.onnx', sessOptionsB)\r\n\r\ninputNodeName = rtA.get_inputs()[0].name\r\ndummyData = np.zeros((1,3,112,112),dtype=np.float32)\r\n\r\n\r\navgTimeA = 0\r\nfor i in range(20):\r\n    st = time.time()\r\n    rtA.run([], {inputNodeName: dummyData})[0]\r\n    en= time.time()\r\n    avgTimeA+=(en-st)\r\n\r\n\r\navgTimeB = 0\r\nfor i in range(20):\r\n    st = time.time()\r\n    rtB.run([], {inputNodeName: dummyData})[0]\r\n    en= time.time()\r\n    avgTimeB+=(en-st)\r\n\r\n\r\nprint('ENABLE',avgTimeA/20,'DISABLE',avgTimeB/20)\r\n```\r\n\r\n`ENABLE 0.08494850397109985 DISABLE 0.08426977396011352`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4809", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4809/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4809/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4809/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4809", "id": 679342790, "node_id": "MDU6SXNzdWU2NzkzNDI3OTA=", "number": 4809, "title": "CUDAExecutionProvider", "user": {"login": "TheBigTicket02", "id": 52916777, "node_id": "MDQ6VXNlcjUyOTE2Nzc3", "avatar_url": "https://avatars1.githubusercontent.com/u/52916777?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheBigTicket02", "html_url": "https://github.com/TheBigTicket02", "followers_url": "https://api.github.com/users/TheBigTicket02/followers", "following_url": "https://api.github.com/users/TheBigTicket02/following{/other_user}", "gists_url": "https://api.github.com/users/TheBigTicket02/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheBigTicket02/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheBigTicket02/subscriptions", "organizations_url": "https://api.github.com/users/TheBigTicket02/orgs", "repos_url": "https://api.github.com/users/TheBigTicket02/repos", "events_url": "https://api.github.com/users/TheBigTicket02/events{/privacy}", "received_events_url": "https://api.github.com/users/TheBigTicket02/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-14T18:52:15Z", "updated_at": "2020-08-14T20:14:01Z", "closed_at": "2020-08-14T20:14:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nHi!. I\u2019ve just started using onnxruntime for DistilBert optimization on GPU. And from `ort.get_available_providers()` only `['CPUExecutionProvider']`.\r\n**Urgency**\r\nMedium\r\n\r\n**System information**\r\n- Kaggle and Colab Notebooks\r\n\r\n**To Reproduce**\r\nI used this command `!pip install onnxruntime-gpu onnxruntime onnxruntime-tools`, and tried this `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"`\r\n\r\n**Expected behavior**\r\nCan somebody explain how to get 'CUDAExecutionProvider'?\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4806", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4806/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4806/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4806/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4806", "id": 679282600, "node_id": "MDU6SXNzdWU2NzkyODI2MDA=", "number": 4806, "title": "onnxruntime-gpu ImportError: cannot import name 'get_all_providers'", "user": {"login": "lex3001", "id": 12215450, "node_id": "MDQ6VXNlcjEyMjE1NDUw", "avatar_url": "https://avatars0.githubusercontent.com/u/12215450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lex3001", "html_url": "https://github.com/lex3001", "followers_url": "https://api.github.com/users/lex3001/followers", "following_url": "https://api.github.com/users/lex3001/following{/other_user}", "gists_url": "https://api.github.com/users/lex3001/gists{/gist_id}", "starred_url": "https://api.github.com/users/lex3001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lex3001/subscriptions", "organizations_url": "https://api.github.com/users/lex3001/orgs", "repos_url": "https://api.github.com/users/lex3001/repos", "events_url": "https://api.github.com/users/lex3001/events{/privacy}", "received_events_url": "https://api.github.com/users/lex3001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1311608287, "node_id": "MDU6TGFiZWwxMzExNjA4Mjg3", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:documentation", "name": "component:documentation", "color": "303a93", "default": false, "description": "related to documentation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-14T16:56:10Z", "updated_at": "2020-08-18T19:59:48Z", "closed_at": "2020-08-18T19:59:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI am trying to test onnxruntime-gpu.\r\nOn \"import onnxruntime as rt\" I get an error:\r\n(onnx-gpu) C:\\Users\\l\\OneDrive\\AF\\mut1ny\\face_segmentaion_onnx>python test.py\r\nC:\\Users\\l\\.conda\\envs\\onnx-gpu\\lib\\site-packages\\onnxruntime\\capi\\_pybind_state.py:14: UserWarning: Cannot load onnxruntime.capi. Error: 'DLL load failed: The specified module could not be found.'.\r\n  warnings.warn(\"Cannot load onnxruntime.capi. Error: '{0}'.\".format(str(e)))\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 8, in <module>\r\n    import onnxruntime as rt\r\n  File \"C:\\Users\\l\\.conda\\envs\\onnx-gpu\\lib\\site-packages\\onnxruntime\\__init__.py\", line 13, in <module>\r\n    from onnxruntime.capi._pybind_state import get_all_providers, get_available_providers, get_device, set_seed, \\\r\nImportError: cannot import name 'get_all_providers'\r\n\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- Wndows 10\r\n- pip install onnxruntime-gpu\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.6.10 (Anaconda)\r\n- Visual Studio version (if applicable): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia GeForce GTX 1660 Ti\r\n\r\n**To Reproduce**\r\nimport onnxruntime as rt\r\n\r\n**Expected behavior**\r\nn/a\r\nload onnx runtime for GPU\r\n\r\n**Screenshots**\r\nn/a\r\n\r\n**Additional context**\r\nI have successfully used onnxruntime (not \"-gpu\") so maybe this is something related to my Nvidia drivers and CUDA setup? The error message is hard to understand. My Nvidia system info says NVCUDA64.DLL is an 11.0.208 driver. Maybe I need CUDA 11? Any ideas on how to get this working would be great. The inference I want to use takes about 4 seconds with CPU, should be much faster with GPU.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4802", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4802/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4802/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4802/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4802", "id": 679049434, "node_id": "MDU6SXNzdWU2NzkwNDk0MzQ=", "number": 4802, "title": "A question on DispatchOnTensorType()", "user": {"login": "antkillerfarm", "id": 10249981, "node_id": "MDQ6VXNlcjEwMjQ5OTgx", "avatar_url": "https://avatars3.githubusercontent.com/u/10249981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/antkillerfarm", "html_url": "https://github.com/antkillerfarm", "followers_url": "https://api.github.com/users/antkillerfarm/followers", "following_url": "https://api.github.com/users/antkillerfarm/following{/other_user}", "gists_url": "https://api.github.com/users/antkillerfarm/gists{/gist_id}", "starred_url": "https://api.github.com/users/antkillerfarm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/antkillerfarm/subscriptions", "organizations_url": "https://api.github.com/users/antkillerfarm/orgs", "repos_url": "https://api.github.com/users/antkillerfarm/repos", "events_url": "https://api.github.com/users/antkillerfarm/events{/privacy}", "received_events_url": "https://api.github.com/users/antkillerfarm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-14T10:03:27Z", "updated_at": "2020-08-15T14:25:50Z", "closed_at": "2020-08-15T14:25:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "In include/onnxruntime/core/framework/data_types_internal.h: Line 114:\r\n\r\n```\r\ncase ONNX_NAMESPACE::TensorProto_DataType_UINT8:              \\\r\n      function<uint32_t>(__VA_ARGS__);                            \\\r\n      break;\r\n```\r\nWhy map `ONNX_NAMESPACE::TensorProto_DataType_UINT8` to `uint32_t`?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4801", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4801/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4801/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4801/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4801", "id": 679045995, "node_id": "MDU6SXNzdWU2NzkwNDU5OTU=", "number": 4801, "title": "How to use yolov5 onnx file in C#", "user": {"login": "ricklina90", "id": 53546552, "node_id": "MDQ6VXNlcjUzNTQ2NTUy", "avatar_url": "https://avatars2.githubusercontent.com/u/53546552?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ricklina90", "html_url": "https://github.com/ricklina90", "followers_url": "https://api.github.com/users/ricklina90/followers", "following_url": "https://api.github.com/users/ricklina90/following{/other_user}", "gists_url": "https://api.github.com/users/ricklina90/gists{/gist_id}", "starred_url": "https://api.github.com/users/ricklina90/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ricklina90/subscriptions", "organizations_url": "https://api.github.com/users/ricklina90/orgs", "repos_url": "https://api.github.com/users/ricklina90/repos", "events_url": "https://api.github.com/users/ricklina90/events{/privacy}", "received_events_url": "https://api.github.com/users/ricklina90/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1758308176, "node_id": "MDU6TGFiZWwxNzU4MzA4MTc2", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C%23", "name": "api:C#", "color": "0e8a16", "default": false, "description": "related to the C# API"}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-14T09:57:29Z", "updated_at": "2020-08-14T21:55:58Z", "closed_at": "2020-08-14T21:55:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n     According to this sample https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data. I convert training data(.pt) to ONNX file.\r\n(https://github.com/ultralytics/yolov5/issues/412)\r\nIf I need to use C# and onnxruntime, is there any sample like https://github.com/microsoft/onnxruntime/tree/master/csharp/sample/Microsoft.ML.OnnxRuntime.FasterRcnnSample I can reference to? THX.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4798", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4798/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4798/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4798/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4798", "id": 678977449, "node_id": "MDU6SXNzdWU2Nzg5Nzc0NDk=", "number": 4798, "title": "add new op but register test failed", "user": {"login": "freedenS", "id": 26213470, "node_id": "MDQ6VXNlcjI2MjEzNDcw", "avatar_url": "https://avatars2.githubusercontent.com/u/26213470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/freedenS", "html_url": "https://github.com/freedenS", "followers_url": "https://api.github.com/users/freedenS/followers", "following_url": "https://api.github.com/users/freedenS/following{/other_user}", "gists_url": "https://api.github.com/users/freedenS/gists{/gist_id}", "starred_url": "https://api.github.com/users/freedenS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/freedenS/subscriptions", "organizations_url": "https://api.github.com/users/freedenS/orgs", "repos_url": "https://api.github.com/users/freedenS/repos", "events_url": "https://api.github.com/users/freedenS/events{/privacy}", "received_events_url": "https://api.github.com/users/freedenS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-14T07:59:06Z", "updated_at": "2020-08-20T09:37:53Z", "closed_at": "2020-08-20T09:37:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI have added a new op following this way(Contributing the op to ONNXRuntime).\r\nmodify: \r\nonnxruntime/contrib_ops/cpu/newop.h \r\nonnxruntime/contrib_ops/cpu/newop.cc\r\nonnxruntime/contrib_ops/cpu/cpu_contrib_kernels.cc class(newop) BuildKernelCreateInfo(newop) same to SampleOp\r\nonnxruntime/core/graph/contrib_ops/contrib_defs.cc ONNX_CONTRIB_OPERATOR_SCHEMA(newop) same to SampleOp\r\ntest with following code in onnxruntime/test/contrib_ops/op_reg_test.cc:\r\n```\r\nauto op1 = OpSchemaRegistry::Schema(\"newop\");\r\nEXPECT_TRUE(nullptr != op);\r\n```\r\nbut failed.\r\n\r\n**Urgency**\r\nNone.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version:\r\n- Python version:\r\n- Visual Studio version (if applicable): 2017\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4796", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4796/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4796/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4796/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4796", "id": 678855960, "node_id": "MDU6SXNzdWU2Nzg4NTU5NjA=", "number": 4796, "title": "Quantized tiny_yolov4 model ,inference speed from 47ms to 83ms.What should I do speed up inference speed? ", "user": {"login": "NeekHua", "id": 32665609, "node_id": "MDQ6VXNlcjMyNjY1NjA5", "avatar_url": "https://avatars3.githubusercontent.com/u/32665609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NeekHua", "html_url": "https://github.com/NeekHua", "followers_url": "https://api.github.com/users/NeekHua/followers", "following_url": "https://api.github.com/users/NeekHua/following{/other_user}", "gists_url": "https://api.github.com/users/NeekHua/gists{/gist_id}", "starred_url": "https://api.github.com/users/NeekHua/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NeekHua/subscriptions", "organizations_url": "https://api.github.com/users/NeekHua/orgs", "repos_url": "https://api.github.com/users/NeekHua/repos", "events_url": "https://api.github.com/users/NeekHua/events{/privacy}", "received_events_url": "https://api.github.com/users/NeekHua/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1607058914, "node_id": "MDU6TGFiZWwxNjA3MDU4OTE0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/feature:quantization", "name": "feature:quantization", "color": "9073d1", "default": false, "description": "related to quantization of models or running quantized models"}, {"id": 1273765791, "node_id": "MDU6TGFiZWwxMjczNzY1Nzkx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:performance", "name": "type:performance", "color": "a2eeef", "default": false, "description": "performance related issues and questions"}], "state": "closed", "locked": false, "assignee": {"login": "yufenglee", "id": 30486710, "node_id": "MDQ6VXNlcjMwNDg2NzEw", "avatar_url": "https://avatars2.githubusercontent.com/u/30486710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yufenglee", "html_url": "https://github.com/yufenglee", "followers_url": "https://api.github.com/users/yufenglee/followers", "following_url": "https://api.github.com/users/yufenglee/following{/other_user}", "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}", "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions", "organizations_url": "https://api.github.com/users/yufenglee/orgs", "repos_url": "https://api.github.com/users/yufenglee/repos", "events_url": "https://api.github.com/users/yufenglee/events{/privacy}", "received_events_url": "https://api.github.com/users/yufenglee/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yufenglee", "id": 30486710, "node_id": "MDQ6VXNlcjMwNDg2NzEw", "avatar_url": "https://avatars2.githubusercontent.com/u/30486710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yufenglee", "html_url": "https://github.com/yufenglee", "followers_url": "https://api.github.com/users/yufenglee/followers", "following_url": "https://api.github.com/users/yufenglee/following{/other_user}", "gists_url": "https://api.github.com/users/yufenglee/gists{/gist_id}", "starred_url": "https://api.github.com/users/yufenglee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yufenglee/subscriptions", "organizations_url": "https://api.github.com/users/yufenglee/orgs", "repos_url": "https://api.github.com/users/yufenglee/repos", "events_url": "https://api.github.com/users/yufenglee/events{/privacy}", "received_events_url": "https://api.github.com/users/yufenglee/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-08-14T02:43:29Z", "updated_at": "2020-08-24T06:40:21Z", "closed_at": "2020-08-24T06:40:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nIf inscribe\r\n\r\n**Urgency**\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 Professional edition\r\n- ONNX Runtime installed from (source or binary):source\r\n- ONNX Runtime version:1.4.0\r\n- Python version:3.6.5\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4787", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4787/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4787/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4787/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4787", "id": 678528480, "node_id": "MDU6SXNzdWU2Nzg1Mjg0ODA=", "number": 4787, "title": "unclear error message: INVALID_ARGUMENT : Unexpected input data type. Actual: (N11onnxruntime17PrimitiveDataTypeIlEE) , expected: (N11onnxruntime17PrimitiveDataTypeIiEE)", "user": {"login": "Zhen-hao", "id": 10957195, "node_id": "MDQ6VXNlcjEwOTU3MTk1", "avatar_url": "https://avatars3.githubusercontent.com/u/10957195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zhen-hao", "html_url": "https://github.com/Zhen-hao", "followers_url": "https://api.github.com/users/Zhen-hao/followers", "following_url": "https://api.github.com/users/Zhen-hao/following{/other_user}", "gists_url": "https://api.github.com/users/Zhen-hao/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zhen-hao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zhen-hao/subscriptions", "organizations_url": "https://api.github.com/users/Zhen-hao/orgs", "repos_url": "https://api.github.com/users/Zhen-hao/repos", "events_url": "https://api.github.com/users/Zhen-hao/events{/privacy}", "received_events_url": "https://api.github.com/users/Zhen-hao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1311608287, "node_id": "MDU6TGFiZWwxMzExNjA4Mjg3", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:documentation", "name": "component:documentation", "color": "303a93", "default": false, "description": "related to documentation"}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2020-08-13T15:28:26Z", "updated_at": "2020-08-17T09:13:43Z", "closed_at": "2020-08-17T09:13:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nI have a fine-tuned BERT model that has the following shape.\r\n\r\n```\r\ngraph_name: tf_bert_for_multi_classification\r\ndomain: onnxmltools\r\ndescription: \r\ninput 0: \"attention_mask\" [\"N\", 7] Int32\r\ninput 1: \"input_ids\" [\"N\", 7] Int32\r\ninput 2: \"token_type_ids\" [\"N\", 7] Int32\r\noutput 0: \"output_1\" [\"N\", 4404, 1] Float\r\n```\r\n\r\nwhen I try to run the model with\r\n```\r\nresults = session.run(None, inputs_onnx)\r\n```\r\nwhere `inputs_onnx` is \r\n```\r\n{'input_ids': array([ 101,  146, 1169, 1631, 1103, 3974,  117, 1169, 1128,  136,  102]),\r\n 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\r\n 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\r\n```\r\nI got the following error.\r\n\r\n```\r\n>>> results = session.run(None, inputs_onnx)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/nix/store/xws61xnjc03fjiwfh7ci5cwgg1chmp3l-python3.7-onnxruntime-1.4.0/lib/python3.7/site-packages/onnxruntime/capi/session.py\", line 110, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (N11onnxruntime17PrimitiveDataTypeIlEE) , expected: (N11onnxruntime17PrimitiveDataTypeIiEE)\r\n```\r\n\r\nIt is really hard for me to debug because it is not clear from the error message what's wrong.\r\nI can't find source code covering `N11onnxruntime17PrimitiveDataTypeIiEE` or `N11onnxruntime17PrimitiveDataTypeIlEE`\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4782", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4782/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4782/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4782/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4782", "id": 678438732, "node_id": "MDU6SXNzdWU2Nzg0Mzg3MzI=", "number": 4782, "title": "Error when buliding sample MNIST with visual studio 2019", "user": {"login": "zhuxiaoxuhit", "id": 32813150, "node_id": "MDQ6VXNlcjMyODEzMTUw", "avatar_url": "https://avatars2.githubusercontent.com/u/32813150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuxiaoxuhit", "html_url": "https://github.com/zhuxiaoxuhit", "followers_url": "https://api.github.com/users/zhuxiaoxuhit/followers", "following_url": "https://api.github.com/users/zhuxiaoxuhit/following{/other_user}", "gists_url": "https://api.github.com/users/zhuxiaoxuhit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuxiaoxuhit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuxiaoxuhit/subscriptions", "organizations_url": "https://api.github.com/users/zhuxiaoxuhit/orgs", "repos_url": "https://api.github.com/users/zhuxiaoxuhit/repos", "events_url": "https://api.github.com/users/zhuxiaoxuhit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuxiaoxuhit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-13T13:30:07Z", "updated_at": "2020-08-14T03:36:36Z", "closed_at": "2020-08-14T03:36:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nSamle path: onnxruntime/samples/c_cxx/MNIST\r\n\r\nWhen building it, got error:\r\n$ build.bat //this command\r\n\r\nwhen I built MNIST, got this message:(told me onnxruntime.lib with 64bit, but sample MNIST need 32bit lib)\r\n/////////////////////////////////////////////////////////////\r\nE:\\onnxruntime\\samples\\c_cxx\\MNIST>cl MNIST.cpp /Zi /EHsc /I......\\include\\onnxruntime\\core\\session /link /LIBPATH:......\\build\\Windows\\Debug\\Debug\r\n\u7528\u4e8e x86 \u7684 Microsoft (R) C/C++ \u4f18\u5316\u7f16\u8bd1\u5668 19.27.29111 \u7248\r\n\u7248\u6743\u6240\u6709(C) Microsoft Corporation\u3002\u4fdd\u7559\u6240\u6709\u6743\u5229\u3002\r\n\r\nMNIST.cpp\r\nMicrosoft (R) Incremental Linker Version 14.27.29111.0\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n/out:MNIST.exe\r\n/debug\r\n/LIBPATH:......\\build\\Windows\\Debug\\Debug\r\nMNIST.obj\r\nMNIST.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 _OrtGetApiBase@0\uff0c\u51fd\u6570 \"void _cdecl `dynamic initializer for 'public: static struct OrtApi const * const Ort::Global::api''(void)\" (??_E?api@?$Global@X@Ort@@2PBUOrtApi@@b@@YAXXZ) \u4e2d\u5f15\u7528\u4e86\u8be5\u7b26\u53f7\r\n......\\build\\Windows\\Debug\\Debug\\onnxruntime.lib : warning LNK4272:\u5e93\u8ba1\u7b97\u673a\u7c7b\u578b\u201cx64\u201d\u4e0e\u76ee\u6807 \u8ba1\u7b97\u673a\u7c7b\u578b\u201cx86\u201d\u51b2\u7a81\r\nMNIST.exe : fatal error LNK1120: 1 \u4e2a\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u547d\u4ee4\r\n//////////////////////////////////////////////////////////////\r\n\r\n**System information**\r\nWindows 10\r\nONNX Runtime installed from source\r\nPython version: 3.8.5\r\nVisual Studio version (if applicable): Microsoft Visual C++ 2019 16.7.1\r\nGCC/Compiler version (if compiling from source): Microsoft (R) Windows (R) Resource Compiler Version 10.0.10011.16384\r\nCUDA/cuDNN version: just cpu version\r\nGPU model and memory: just cpu version\r\n\r\n**Expected behavior**\r\nno errors", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4770", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4770/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4770/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4770/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4770", "id": 677948438, "node_id": "MDU6SXNzdWU2Nzc5NDg0Mzg=", "number": 4770, "title": "Document what are the defaults", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1311608287, "node_id": "MDU6TGFiZWwxMzExNjA4Mjg3", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:documentation", "name": "component:documentation", "color": "303a93", "default": false, "description": "related to documentation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-08-12T20:12:03Z", "updated_at": "2020-08-12T21:30:03Z", "closed_at": "2020-08-12T21:30:03Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Describe the bug**\r\nReading [this](https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Graph_Optimizations.md#graph-optimization-levels) document I am not still sure what is the default value of `GraphOptimizationLevel`.\r\nIf I create `InferenceSession` without passing any `SessionOptions`, what will be the default?\r\nIf I create `SessionOptions` instance without passing anything for `GraphOptimizationLevel` what is the default?\r\n\r\n**Urgency**\r\nNeed to understand this to debug perf issue\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- ONNX Runtime installed from (source or binary): nuget\r\n- ONNX Runtime version: 1.4\r\n- Visual Studio version (if applicable): 2019\r\n\r\n**Expected behavior**\r\nDocument to mention defaults clearly\r\n\r\n**Additional context**\r\nWhy is `ORT_ENABLE_ALL` not the default, if it is not.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4766", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4766/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4766/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4766/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4766", "id": 677516065, "node_id": "MDU6SXNzdWU2Nzc1MTYwNjU=", "number": 4766, "title": "Error when buliding sample MNIST with visual studio 2019", "user": {"login": "zhuxiaoxuhit", "id": 32813150, "node_id": "MDQ6VXNlcjMyODEzMTUw", "avatar_url": "https://avatars2.githubusercontent.com/u/32813150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhuxiaoxuhit", "html_url": "https://github.com/zhuxiaoxuhit", "followers_url": "https://api.github.com/users/zhuxiaoxuhit/followers", "following_url": "https://api.github.com/users/zhuxiaoxuhit/following{/other_user}", "gists_url": "https://api.github.com/users/zhuxiaoxuhit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhuxiaoxuhit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhuxiaoxuhit/subscriptions", "organizations_url": "https://api.github.com/users/zhuxiaoxuhit/orgs", "repos_url": "https://api.github.com/users/zhuxiaoxuhit/repos", "events_url": "https://api.github.com/users/zhuxiaoxuhit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhuxiaoxuhit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-12T08:48:51Z", "updated_at": "2020-08-13T13:19:02Z", "closed_at": "2020-08-13T04:46:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nSamle path:     onnxruntime/samples/c_cxx/MNIST\r\n\r\nWhen  building it, got error:\r\n$ build.bat   //this command\r\n\r\n//////////////////////////\r\nMNIST.cpp\r\nMNIST.cpp(44): error C2039: \"max_element\": \u4e0d\u662f \"std\" \u7684\u6210\u5458\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\include\\cmath(683): note: \u53c2\u89c1 \u201cstd\u201d\u7684\u58f0\u660e\r\nMNIST.cpp(44): error C3861: \u201cmax_element\u201d: \u627e\u4e0d\u5230\u6807\u8bc6\u7b26\r\nMNIST.cpp(188): error C2039: \"min_element\": \u4e0d\u662f \"std\" \u7684\u6210\u5458\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\include\\cmath(683): note: \u53c2\u89c1 \u201cstd\u201d\u7684\u58f0\u660e\r\nMNIST.cpp(188): error C3861: \u201cmin_element\u201d: \u627e\u4e0d\u5230\u6807\u8bc6\u7b26\r\nMNIST.cpp(192): error C3536: \u201crange\u201d: \u521d\u59cb\u5316\u4e4b\u524d\u65e0\u6cd5\u4f7f\u7528\r\n/////////////////////////\r\n\r\n**System information**\r\n- Windows 10\r\n- ONNX Runtime installed from source\r\n- Python version:  3.8.5\r\n- Visual Studio version (if applicable): Microsoft Visual C++ 2019   16.7.1\r\n- GCC/Compiler version (if compiling from source): Microsoft (R) Windows (R) Resource Compiler Version 10.0.10011.16384\r\n- CUDA/cuDNN version: just cpu version\r\n- GPU model and memory: just cpu version\r\n\r\n**Expected behavior**\r\nno errors\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4755", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4755/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4755/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4755/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4755", "id": 676787374, "node_id": "MDU6SXNzdWU2NzY3ODczNzQ=", "number": 4755, "title": "ONNXRuntime JAVA JNI build failure ", "user": {"login": "quantum-fusion", "id": 10088099, "node_id": "MDQ6VXNlcjEwMDg4MDk5", "avatar_url": "https://avatars3.githubusercontent.com/u/10088099?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quantum-fusion", "html_url": "https://github.com/quantum-fusion", "followers_url": "https://api.github.com/users/quantum-fusion/followers", "following_url": "https://api.github.com/users/quantum-fusion/following{/other_user}", "gists_url": "https://api.github.com/users/quantum-fusion/gists{/gist_id}", "starred_url": "https://api.github.com/users/quantum-fusion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quantum-fusion/subscriptions", "organizations_url": "https://api.github.com/users/quantum-fusion/orgs", "repos_url": "https://api.github.com/users/quantum-fusion/repos", "events_url": "https://api.github.com/users/quantum-fusion/events{/privacy}", "received_events_url": "https://api.github.com/users/quantum-fusion/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1805781160, "node_id": "MDU6TGFiZWwxODA1NzgxMTYw", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:Java", "name": "api:Java", "color": "0e8a16", "default": false, "description": "related to the Java API"}, {"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-11T11:07:08Z", "updated_at": "2020-08-11T17:54:24Z", "closed_at": "2020-08-11T17:54:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nThe problem is that the build for the java JNI run time is failing.\r\n\r\nI used the instructions from ./java/README.txt\r\n### Building\r\nUse the main project's [build instructions](../BUILD.md) with the `--build_java` option.\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS \r\n- ONNX Runtime installed from (source or binary): source \r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.8.3\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n./build.sh --config RelWithDebInfo --build_shared_lib --parallel --build_java\r\n\r\n(venv) (base) MacBook-Pro:~/onnxruntime quantum-fusion$ ./build.sh --config RelWithDebInfo --build_shared_lib --parallel --build_java\r\n2020-08-11 06:11:39,314 Build [DEBUG] - Defaulting to running update, build [and test for native builds].\r\n2020-08-11 06:11:39,315 Build [INFO] - Build started\r\n2020-08-11 06:11:39,315 Build [DEBUG] - Running subprocess in '/Users/hottelet/onnxruntime'\r\n['git', 'submodule', 'sync', '--recursive']\r\nSynchronizing submodule url for 'cmake/external/FeaturizersLibrary'\r\nSynchronizing submodule url for 'cmake/external/FeaturizersLibrary/src/3rdParty/eigen'\r\nSynchronizing submodule url for 'cmake/external/FeaturizersLibrary/src/3rdParty/re2'\r\nSynchronizing submodule url for 'cmake/external/SafeInt/safeint'\r\nSynchronizing submodule url for 'cmake/external/cub'\r\nSynchronizing submodule url for 'cmake/external/cxxopts'\r\nSynchronizing submodule url for 'cmake/external/date'\r\nSynchronizing submodule url for 'cmake/external/eigen'\r\nSynchronizing submodule url for 'cmake/external/gemmlowp'\r\nSynchronizing submodule url for 'cmake/external/googletest'\r\nSynchronizing submodule url for 'cmake/external/horovod'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/HTTPRequest'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/assert'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/config'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/core'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/detail'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/iterator'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/lockfree'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/mpl'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/parameter'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/predef'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/preprocessor'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/static_assert'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/type_traits'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/boost/utility'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/eigen'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/flatbuffers'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/gloo'\r\nSynchronizing submodule url for 'cmake/external/horovod/third_party/lbfgs'\r\nSynchronizing submodule url for 'cmake/external/json'\r\nSynchronizing submodule url for 'cmake/external/libprotobuf-mutator'\r\nSynchronizing submodule url for 'cmake/external/mimalloc'\r\nSynchronizing submodule url for 'cmake/external/nsync'\r\nSynchronizing submodule url for 'cmake/external/onnx'\r\nSynchronizing submodule url for 'cmake/external/onnx/third_party/benchmark'\r\nSynchronizing submodule url for 'cmake/external/onnx/third_party/pybind11'\r\nSynchronizing submodule url for 'cmake/external/onnx/third_party/pybind11/tools/clang'\r\nSynchronizing submodule url for 'cmake/external/onnx-tensorrt'\r\nSynchronizing submodule url for 'cmake/external/onnx-tensorrt/third_party/onnx'\r\nSynchronizing submodule url for 'cmake/external/onnx-tensorrt/third_party/onnx/third_party/benchmark'\r\nSynchronizing submodule url for 'cmake/external/onnx-tensorrt/third_party/onnx/third_party/pybind11'\r\nSynchronizing submodule url for 'cmake/external/onnx-tensorrt/third_party/onnx/third_party/pybind11/tools/clang'\r\nSynchronizing submodule url for 'cmake/external/protobuf'\r\nSynchronizing submodule url for 'cmake/external/protobuf/third_party/benchmark'\r\nSynchronizing submodule url for 'cmake/external/protobuf/third_party/googletest'\r\nSynchronizing submodule url for 'cmake/external/re2'\r\nSynchronizing submodule url for 'cmake/external/tensorboard'\r\nSynchronizing submodule url for 'cmake/external/tvm'\r\nSynchronizing submodule url for 'cmake/external/tvm/3rdparty/HalideIR'\r\nSynchronizing submodule url for 'cmake/external/tvm/3rdparty/dlpack'\r\nSynchronizing submodule url for 'cmake/external/tvm/3rdparty/dmlc-core'\r\nSynchronizing submodule url for 'cmake/external/tvm/3rdparty/rang'\r\nSynchronizing submodule url for 'cmake/external/wil'\r\nSynchronizing submodule url for 'server/external/spdlog'\r\n2020-08-11 06:11:40,885 Build [DEBUG] - Subprocess completed. Return code=0\r\n2020-08-11 06:11:40,885 Build [DEBUG] - Running subprocess in '/Users/hottelet/onnxruntime'\r\n['git', 'submodule', 'update', '--init', '--recursive']\r\n2020-08-11 06:11:46,857 Build [DEBUG] - Subprocess completed. Return code=0\r\n2020-08-11 06:11:46,857 Build [INFO] - Generating CMake build tree\r\n2020-08-11 06:11:46,857 Build [DEBUG] - Running subprocess in '/Users/hottelet/onnxruntime/build/Linux/RelWithDebInfo'\r\n['/usr/local/Cellar/cmake/3.17.0_1/bin/cmake', '/Users/hottelet/onnxruntime/cmake', '-Donnxruntime_RUN_ONNX_TESTS=OFF', '-Donnxruntime_BUILD_WINML_TESTS=ON', '-Donnxruntime_GENERATE_TEST_REPORTS=ON', '-Donnxruntime_DEV_MODE=ON', '-DPYTHON_EXECUTABLE=/Users/hottelet/venv/bin/python3', '-Donnxruntime_USE_CUDA=OFF', '-Donnxruntime_CUDNN_HOME=', '-Donnxruntime_USE_FEATURIZERS=OFF', '-Donnxruntime_CUDA_HOME=', '-Donnxruntime_USE_JEMALLOC=OFF', '-Donnxruntime_USE_MIMALLOC_STL_ALLOCATOR=OFF', '-Donnxruntime_USE_MIMALLOC_ARENA_ALLOCATOR=OFF', '-Donnxruntime_ENABLE_PYTHON=OFF', '-Donnxruntime_BUILD_CSHARP=OFF', '-Donnxruntime_BUILD_JAVA=ON', '-Donnxruntime_BUILD_NODEJS=OFF', '-Donnxruntime_BUILD_SHARED_LIB=ON', '-Donnxruntime_USE_EIGEN_FOR_BLAS=ON', '-Donnxruntime_USE_OPENBLAS=OFF', '-Donnxruntime_USE_DNNL=OFF', '-Donnxruntime_USE_MKLML=OFF', '-Donnxruntime_USE_NGRAPH=OFF', '-Donnxruntime_USE_OPENVINO=OFF', '-Donnxruntime_USE_OPENVINO_MYRIAD=OFF', '-Donnxruntime_USE_OPENVINO_GPU_FP32=OFF', '-Donnxruntime_USE_OPENVINO_GPU_FP16=OFF', '-Donnxruntime_USE_OPENVINO_CPU_FP32=OFF', '-Donnxruntime_USE_OPENVINO_VAD_M=OFF', '-Donnxruntime_USE_OPENVINO_VAD_F=OFF', '-Donnxruntime_USE_OPENVINO_BINARY=OFF', '-Donnxruntime_USE_NNAPI_BUILTIN=OFF', '-Donnxruntime_USE_RKNPU=OFF', '-Donnxruntime_USE_OPENMP=OFF', '-Donnxruntime_USE_TVM=OFF', '-Donnxruntime_USE_LLVM=OFF', '-Donnxruntime_ENABLE_MICROSOFT_INTERNAL=OFF', '-Donnxruntime_USE_VITISAI=OFF', '-Donnxruntime_USE_NUPHAR=OFF', '-Donnxruntime_USE_TENSORRT=OFF', '-Donnxruntime_TENSORRT_HOME=', '-Donnxruntime_USE_MIGRAPHX=OFF', '-Donnxruntime_MIGRAPHX_HOME=', '-Donnxruntime_CROSS_COMPILING=OFF', '-Donnxruntime_DISABLE_CONTRIB_OPS=OFF', '-Donnxruntime_DISABLE_ML_OPS=OFF', '-Donnxruntime_DISABLE_RTTI=OFF', '-Donnxruntime_MSVC_STATIC_RUNTIME=OFF', '-Donnxruntime_ENABLE_LANGUAGE_INTEROP_OPS=OFF', '-Donnxruntime_USE_DML=OFF', '-Donnxruntime_USE_WINML=OFF', '-Donnxruntime_BUILD_FOR_WINDOWS_STORE=OFF', '-Donnxruntime_USE_TELEMETRY=OFF', '-Donnxruntime_ENABLE_LTO=OFF', '-Donnxruntime_USE_ACL=OFF', '-Donnxruntime_USE_ACL_1902=OFF', '-Donnxruntime_USE_ACL_1905=OFF', '-Donnxruntime_USE_ACL_1908=OFF', '-Donnxruntime_USE_ARMNN=OFF', '-Donnxruntime_ARMNN_RELU_USE_CPU=ON', '-Donnxruntime_ARMNN_BN_USE_CPU=ON', '-Donnxruntime_ENABLE_NVTX_PROFILE=OFF', '-Donnxruntime_ENABLE_TRAINING=OFF', '-Donnxruntime_USE_HOROVOD=OFF', '-Donnxruntime_BUILD_BENCHMARKS=OFF', '-Donnxruntime_PYBIND_EXPORT_OPSCHEMA=OFF', '-Donnxruntime_ENABLE_MEMLEAK_CHECKER=OFF', '-DCMAKE_BUILD_TYPE=RelWithDebInfo']\r\nAdding flags for Mac builds\r\nUse gtest from submodule\r\n-- Found PythonInterp: /Users/hottelet/venv/bin/python3 (found version \"3.8.3\") \r\n-- Found PythonInterp: /Users/hottelet/venv/bin/python3 (found suitable version \"3.8.3\", minimum required is \"3.4\") \r\nUse protobuf from submodule\r\n-- \r\n-- 3.11.3.0\r\n-- Java Build is enabled\r\n-- Using gradle: /usr/local/bin/gradle\r\n-- Running Java tests\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /Users/hottelet/onnxruntime/build/Linux/RelWithDebInfo\r\n2020-08-11 06:11:49,416 Build [DEBUG] - Subprocess completed. Return code=0\r\n2020-08-11 06:11:49,416 Build [INFO] - Building targets for RelWithDebInfo configuration\r\n2020-08-11 06:11:49,417 Build [DEBUG] - Running subprocess in '/Users/hottelet/onnxruntime'\r\n['/usr/local/Cellar/cmake/3.17.0_1/bin/cmake', '--build', '/Users/hottelet/onnxruntime/build/Linux/RelWithDebInfo', '--config', 'RelWithDebInfo', '--', '-j16']\r\nScanning dependencies of target pep8_check\r\nChecking python scripts for PEP8 conformance using flake8\r\n[  0%] Built target onnxruntime_generate_def\r\n[  0%] Built target gtest\r\n[  1%] Generating /Users/hottelet/onnxruntime/java/build/libs/onnxruntime.jar\r\n[  1%] Built target onnxruntime_mocked_allocator\r\n[  2%] Built target custom_op_library\r\n[  3%] Built target re2\r\n[ 10%] Built target libprotobuf-lite\r\n[ 11%] Built target onnxruntime_mlas\r\n[ 15%] Built target nsync_cpp\r\n[ 23%] Built target libprotobuf\r\n[ 23%] Built target gmock\r\n[ 33%] Built target libprotoc\r\n[ 33%] Built target protoc\r\n[ 34%] Built target onnx_proto\r\nScanning dependencies of target onnx_test_data_proto\r\nScanning dependencies of target onnxruntime_util\r\nScanning dependencies of target onnxruntime_common\r\nScanning dependencies of target onnxruntime_session\r\nScanning dependencies of target onnxruntime_test_utils\r\nScanning dependencies of target onnxruntime_graph\r\nScanning dependencies of target onnxruntime_optimizer\r\nScanning dependencies of target onnxruntime_framework\r\n[ 34%] Building CXX object CMakeFiles/onnx_test_data_proto.dir/tml.pb.cc.o\r\n[ 34%] Building CXX object onnx/CMakeFiles/onnx.dir/__/external/onnx/onnx/checker.cc.o\r\n[ 34%] Building CXX object onnx/CMakeFiles/onnx.dir/__/external/onnx/onnx/common/assertions.cc.o\r\n[ 35%] Building CXX object onnx/CMakeFiles/onnx.dir/__/external/onnx/onnx/common/interned_strings.cc.o\r\n[ 35%] Building CXX object onnx/CMakeFiles/onnx.dir/__/external/onnx/onnx/common/ir_pb_converter.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_test_utils.dir/Users/hottelet/onnxruntime/onnxruntime/test/util/compare_ortvalue.cc.o\r\n[ 35%] Building CXX object onnx/CMakeFiles/onnx.dir/__/external/onnx/onnx/common/status.cc.o\r\n[ 35%] Building CXX object onnx/CMakeFiles/onnx.dir/__/external/onnx/onnx/common/model_helpers.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_common.dir/Users/hottelet/onnxruntime/onnxruntime/core/common/cpuid_info.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_util.dir/Users/hottelet/onnxruntime/onnxruntime/core/profile/profile.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_graph.dir/Users/hottelet/onnxruntime/onnxruntime/core/graph/contrib_ops/attn_lstm_schema_defs.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_session.dir/Users/hottelet/onnxruntime/onnxruntime/core/session/IOBinding.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_optimizer.dir/Users/hottelet/onnxruntime/onnxruntime/core/optimizer/attention_fusion.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_framework.dir/Users/hottelet/onnxruntime/onnxruntime/core/framework/allocation_planner.cc.o\r\n[ 35%] Building CXX object CMakeFiles/onnxruntime_util.dir/Users/hottelet/onnxruntime/onnxruntime/core/util/gemmlowp_common.cc.o\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* Where:\r\nBuild file '/Users/hottelet/onnxruntime/java/build.gradle' line: 32\r\n\r\n* What went wrong:\r\nA problem occurred evaluating root project 'onnxruntime'.\r\n> Could not find method java() for arguments [build_d2sga7li9vj1uors92zmxp804$_run_closure2@48a46518] on root project 'onnxruntime' of type org.gradle.api.Project.\r\n\r\n\r\n**Additional context**\r\nAdd any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4745", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4745/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4745/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4745/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4745", "id": 676345096, "node_id": "MDU6SXNzdWU2NzYzNDUwOTY=", "number": 4745, "title": "InferenceSession(GraphOptimizationLevel.ORT_ENABLE_BASIC) inserts Cast nodes with hard-coded name cause name conflict", "user": {"login": "JianhuiD", "id": 47045227, "node_id": "MDQ6VXNlcjQ3MDQ1MjI3", "avatar_url": "https://avatars2.githubusercontent.com/u/47045227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JianhuiD", "html_url": "https://github.com/JianhuiD", "followers_url": "https://api.github.com/users/JianhuiD/followers", "following_url": "https://api.github.com/users/JianhuiD/following{/other_user}", "gists_url": "https://api.github.com/users/JianhuiD/gists{/gist_id}", "starred_url": "https://api.github.com/users/JianhuiD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JianhuiD/subscriptions", "organizations_url": "https://api.github.com/users/JianhuiD/orgs", "repos_url": "https://api.github.com/users/JianhuiD/repos", "events_url": "https://api.github.com/users/JianhuiD/events{/privacy}", "received_events_url": "https://api.github.com/users/JianhuiD/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "jcwchen", "id": 14194980, "node_id": "MDQ6VXNlcjE0MTk0OTgw", "avatar_url": "https://avatars0.githubusercontent.com/u/14194980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcwchen", "html_url": "https://github.com/jcwchen", "followers_url": "https://api.github.com/users/jcwchen/followers", "following_url": "https://api.github.com/users/jcwchen/following{/other_user}", "gists_url": "https://api.github.com/users/jcwchen/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcwchen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcwchen/subscriptions", "organizations_url": "https://api.github.com/users/jcwchen/orgs", "repos_url": "https://api.github.com/users/jcwchen/repos", "events_url": "https://api.github.com/users/jcwchen/events{/privacy}", "received_events_url": "https://api.github.com/users/jcwchen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jcwchen", "id": 14194980, "node_id": "MDQ6VXNlcjE0MTk0OTgw", "avatar_url": "https://avatars0.githubusercontent.com/u/14194980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcwchen", "html_url": "https://github.com/jcwchen", "followers_url": "https://api.github.com/users/jcwchen/followers", "following_url": "https://api.github.com/users/jcwchen/following{/other_user}", "gists_url": "https://api.github.com/users/jcwchen/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcwchen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcwchen/subscriptions", "organizations_url": "https://api.github.com/users/jcwchen/orgs", "repos_url": "https://api.github.com/users/jcwchen/repos", "events_url": "https://api.github.com/users/jcwchen/events{/privacy}", "received_events_url": "https://api.github.com/users/jcwchen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-10T18:55:34Z", "updated_at": "2020-08-27T19:38:43Z", "closed_at": "2020-08-27T19:38:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nONNXRT  InferenceSession(GraphOptimizationLevel.ORT_ENABLE_BASIC) inserted Cast nodes with hard-coded name(both node name and output ) cause name conflict\r\n\r\n**Urgency**\r\nHigh\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- ONNX Runtime installed from (source or binary): binary\r\n- ONNX Runtime version:  1.4.0\r\n- Python version:  3.6.8\r\n- Visual Studio version (if applicable): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: M/A\r\n\r\n**To Reproduce**\r\n- Run InferenceSession(GraphOptimizationLevel.ORT_ENABLE_BASIC, as below)  on the attached model twice  - the 2nd runs on the generated-model  from 1st run.  2nd run emitted the error \"onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : This is an invalid model. Error: Duplicate definition of name (CastDef_0)\" \r\n    ```\r\n    so = onnxrt.SessionOptions()\r\n    so.graph_optimization_level = onnxrt.GraphOptimizationLevel.ORT_ENABLE_BASIC\r\n    so.optimized_model_filepath = model_cf_filename\r\n    onnxrt.InferenceSession(inputmodel, sess_options=so)\r\n   ```\r\n   The attached model contains Float16 Data, The root of this issue is that InferenceSession try to  insert Cast nodes, however, the inserted node's name(output) is hard-coded which cause name conflict.  And OnnxRT NOT not assume there is no \"CastDef_X\" output used in the original model.\r\n\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n[test_model.zip](https://github.com/microsoft/onnxruntime/files/5052702/test_model.zip)\r\n\r\n**Expected behavior**\r\nShall not see this failure\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4741", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4741/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4741/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4741/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4741", "id": 676036716, "node_id": "MDU6SXNzdWU2NzYwMzY3MTY=", "number": 4741, "title": "Failed to build onnxruntime with TensorRT on Windows 10", "user": {"login": "cocoyen1995", "id": 37019005, "node_id": "MDQ6VXNlcjM3MDE5MDA1", "avatar_url": "https://avatars3.githubusercontent.com/u/37019005?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cocoyen1995", "html_url": "https://github.com/cocoyen1995", "followers_url": "https://api.github.com/users/cocoyen1995/followers", "following_url": "https://api.github.com/users/cocoyen1995/following{/other_user}", "gists_url": "https://api.github.com/users/cocoyen1995/gists{/gist_id}", "starred_url": "https://api.github.com/users/cocoyen1995/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cocoyen1995/subscriptions", "organizations_url": "https://api.github.com/users/cocoyen1995/orgs", "repos_url": "https://api.github.com/users/cocoyen1995/repos", "events_url": "https://api.github.com/users/cocoyen1995/events{/privacy}", "received_events_url": "https://api.github.com/users/cocoyen1995/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 2204061391, "node_id": "MDU6TGFiZWwyMjA0MDYxMzkx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:TensorRT", "name": "ep:TensorRT", "color": "bfdadc", "default": false, "description": "questions/issues related to TensorRT EP"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-08-10T10:36:51Z", "updated_at": "2020-08-20T06:33:00Z", "closed_at": "2020-08-20T06:33:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI'm trying to build onnxruntime with tensorRT, and I'm getting errors as I will show.\r\n(Similar to [this](https://github.com/microsoft/onnxruntime/issues/3630#issue-604657058) issue)\r\n\r\n**Urgency**\r\nnone\r\n(But I've been working on this issue for about a week, but still can't figure it out....)\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows10 \r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.7.0\r\n- Visual Studio version (if applicable): VS2019 Community 16.6.5\r\n- GCC/Compiler version (if compiling from source): - \r\n- CUDA/cuDNN version: 10.2/ 7.6.5\r\n- TensorRT version: 7.0.0.11 with CUDA10.2\r\n- GPU model and memory: Nvidia RTX 2080 Ti 11G\r\n\r\n**To Reproduce**\r\nGo to onnxruntime's directory, open terminal and type command as below:\r\n`build.bat --config Release --parallel --build_shared_lib --use_cuda --cuda_version 10.2 --cudnn_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\" --cuda_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\" --use_tensorrt --tensorrt_home \"D:\\Coco\\Libs\\TensorRT-7.0.0.11_cuda10.2\" --cmake_generator \"Visual Studio 16 2019\"`\r\n\r\n**Expected behavior**\r\nFinish the build successfully.\r\n\r\n**Screenshots**\r\n![image](https://user-images.githubusercontent.com/37019005/89774383-593c5880-db38-11ea-9f8d-43e15eae34a0.png)\r\n\r\n\r\n**Additional context**\r\nI've tried to build with CUDA10.0 succefully without build with tensorRT with the command below in build-in terminal:\r\n`build.bat --config Release --build_shared_lib --parallel --use_cuda --cuda_version 10.0 --cuda_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\" --cudnn_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\" --cmake_generator \"Visual Studio 15 2017\"`\r\n(The building also failed if with tensorRT)\r\n\r\nDue to the usage of TensorRT, I can build onnxruntime with CUDA10.2 with VS2019 by similar command mentioned above.\r\nBut I cannot build successfully if it's with TensorRT. I also tried to clone the repo with --recursive, but the error seems the same...\r\nHere's some log file during the build:\r\n[log_with_trt_vs2019.txt](https://drive.google.com/file/d/10Rg3kHeVEl_oBiwYwe73LAT0qd6fqxBT/view?usp=sharing)\r\n\r\nThanks in advance for any help!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4740", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4740/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4740/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4740/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4740", "id": 675906580, "node_id": "MDU6SXNzdWU2NzU5MDY1ODA=", "number": 4740, "title": "Mem pattern should be disabled when using DML execution provider.", "user": {"login": "YunYang1994", "id": 30433053, "node_id": "MDQ6VXNlcjMwNDMzMDUz", "avatar_url": "https://avatars3.githubusercontent.com/u/30433053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YunYang1994", "html_url": "https://github.com/YunYang1994", "followers_url": "https://api.github.com/users/YunYang1994/followers", "following_url": "https://api.github.com/users/YunYang1994/following{/other_user}", "gists_url": "https://api.github.com/users/YunYang1994/gists{/gist_id}", "starred_url": "https://api.github.com/users/YunYang1994/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YunYang1994/subscriptions", "organizations_url": "https://api.github.com/users/YunYang1994/orgs", "repos_url": "https://api.github.com/users/YunYang1994/repos", "events_url": "https://api.github.com/users/YunYang1994/events{/privacy}", "received_events_url": "https://api.github.com/users/YunYang1994/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2185567573, "node_id": "MDU6TGFiZWwyMTg1NTY3NTcz", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/status:more-info-needed", "name": "status:more-info-needed", "color": "45b2d3", "default": false, "description": "more information is requested to continue investigation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-10T06:37:49Z", "updated_at": "2020-08-14T08:22:37Z", "closed_at": "2020-08-14T08:22:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\n**Urgency**\r\nIf there are particular important use cases blocked by this or strict project-related timelines, please share more information and dates. If there are no hard deadlines, please specify none.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- ONNX Runtime installed from (source or binary):\r\n- ONNX Runtime version:\r\n- Python version:\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here. If the issue is about a particular model, please share the model details as well to facilitate debugging.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4735", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4735/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4735/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4735/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4735", "id": 674932421, "node_id": "MDU6SXNzdWU2NzQ5MzI0MjE=", "number": 4735, "title": "ONNXRuntime fails to build on MAC ", "user": {"login": "quantum-fusion", "id": 10088099, "node_id": "MDQ6VXNlcjEwMDg4MDk5", "avatar_url": "https://avatars3.githubusercontent.com/u/10088099?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quantum-fusion", "html_url": "https://github.com/quantum-fusion", "followers_url": "https://api.github.com/users/quantum-fusion/followers", "following_url": "https://api.github.com/users/quantum-fusion/following{/other_user}", "gists_url": "https://api.github.com/users/quantum-fusion/gists{/gist_id}", "starred_url": "https://api.github.com/users/quantum-fusion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quantum-fusion/subscriptions", "organizations_url": "https://api.github.com/users/quantum-fusion/orgs", "repos_url": "https://api.github.com/users/quantum-fusion/repos", "events_url": "https://api.github.com/users/quantum-fusion/events{/privacy}", "received_events_url": "https://api.github.com/users/quantum-fusion/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-08-07T10:48:00Z", "updated_at": "2020-08-11T10:00:57Z", "closed_at": "2020-08-11T10:00:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\nMACOS fails to build the ONNXruntime .JAR file.  It fails after executing the intructions to build on MAC located here (https://github.com/autodeployai/ai-serving#onnx-runtime-configuration)\r\n\r\n**System information**\r\n- OS Platform and Distribution MACOS 10.15.6 \r\n- ONNX Runtime installed from (source or binary): source \r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.7.6\r\n- Cmake version: 3.17.0\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n./build.sh --config RelWithDebInfo --build_shared_lib --parallel\r\n2020-08-07 06:42:18,070 Build [DEBUG] - Defaulting to running update, build [and test for native builds].\r\n2020-08-07 06:42:18,074 Build [INFO] - Build started\r\n2020-08-07 06:42:18,074 Build [DEBUG] - Running subprocess in '/Users/hottelet/onnxruntime-master'\r\n['git', 'submodule', 'sync', '--recursive']\r\n2020-08-07 06:42:18,262 Build [DEBUG] - Subprocess completed. Return code=0\r\n2020-08-07 06:42:18,262 Build [DEBUG] - Running subprocess in '/Users/hottelet/onnxruntime-master'\r\n['git', 'submodule', 'update', '--init', '--recursive']\r\n2020-08-07 06:42:18,377 Build [DEBUG] - Subprocess completed. Return code=0\r\n2020-08-07 06:42:18,377 Build [INFO] - Generating CMake build tree\r\n2020-08-07 06:42:18,378 Build [DEBUG] - Running subprocess in '/Users/hottelet/onnxruntime-master/build/Linux/RelWithDebInfo'\r\n['/usr/local/Cellar/cmake/3.17.0_1/bin/cmake', '/Users/hottelet/onnxruntime-master/cmake', '-Donnxruntime_RUN_ONNX_TESTS=OFF', '-Donnxruntime_BUILD_WINML_TESTS=ON', '-Donnxruntime_GENERATE_TEST_REPORTS=ON', '-Donnxruntime_DEV_MODE=ON', '-DPYTHON_EXECUTABLE=/Library/Frameworks/Python.framework/Versions/3.8/bin/python3', '-Donnxruntime_USE_CUDA=OFF', '-Donnxruntime_CUDNN_HOME=', '-Donnxruntime_USE_FEATURIZERS=OFF', '-Donnxruntime_CUDA_HOME=', '-Donnxruntime_USE_JEMALLOC=OFF', '-Donnxruntime_USE_MIMALLOC_STL_ALLOCATOR=OFF', '-Donnxruntime_USE_MIMALLOC_ARENA_ALLOCATOR=OFF', '-Donnxruntime_ENABLE_PYTHON=OFF', '-Donnxruntime_BUILD_CSHARP=OFF', '-Donnxruntime_BUILD_JAVA=OFF', '-Donnxruntime_BUILD_NODEJS=OFF', '-Donnxruntime_BUILD_SHARED_LIB=ON', '-Donnxruntime_USE_EIGEN_FOR_BLAS=ON', '-Donnxruntime_USE_OPENBLAS=OFF', '-Donnxruntime_USE_DNNL=OFF', '-Donnxruntime_USE_MKLML=OFF', '-Donnxruntime_USE_NGRAPH=OFF', '-Donnxruntime_USE_OPENVINO=OFF', '-Donnxruntime_USE_OPENVINO_MYRIAD=OFF', '-Donnxruntime_USE_OPENVINO_GPU_FP32=OFF', '-Donnxruntime_USE_OPENVINO_GPU_FP16=OFF', '-Donnxruntime_USE_OPENVINO_CPU_FP32=OFF', '-Donnxruntime_USE_OPENVINO_VAD_M=OFF', '-Donnxruntime_USE_OPENVINO_VAD_F=OFF', '-Donnxruntime_USE_OPENVINO_BINARY=OFF', '-Donnxruntime_USE_NNAPI_BUILTIN=OFF', '-Donnxruntime_USE_RKNPU=OFF', '-Donnxruntime_USE_OPENMP=OFF', '-Donnxruntime_USE_TVM=OFF', '-Donnxruntime_USE_LLVM=OFF', '-Donnxruntime_ENABLE_MICROSOFT_INTERNAL=OFF', '-Donnxruntime_USE_VITISAI=OFF', '-Donnxruntime_USE_NUPHAR=OFF', '-Donnxruntime_USE_TENSORRT=OFF', '-Donnxruntime_TENSORRT_HOME=', '-Donnxruntime_USE_MIGRAPHX=OFF', '-Donnxruntime_MIGRAPHX_HOME=', '-Donnxruntime_CROSS_COMPILING=OFF', '-Donnxruntime_DISABLE_CONTRIB_OPS=OFF', '-Donnxruntime_DISABLE_ML_OPS=OFF', '-Donnxruntime_DISABLE_RTTI=OFF', '-Donnxruntime_MSVC_STATIC_RUNTIME=OFF', '-Donnxruntime_ENABLE_LANGUAGE_INTEROP_OPS=OFF', '-Donnxruntime_USE_DML=OFF', '-Donnxruntime_USE_WINML=OFF', '-Donnxruntime_BUILD_FOR_WINDOWS_STORE=OFF', '-Donnxruntime_USE_TELEMETRY=OFF', '-Donnxruntime_ENABLE_LTO=OFF', '-Donnxruntime_USE_ACL=OFF', '-Donnxruntime_USE_ACL_1902=OFF', '-Donnxruntime_USE_ACL_1905=OFF', '-Donnxruntime_USE_ACL_1908=OFF', '-Donnxruntime_USE_ARMNN=OFF', '-Donnxruntime_ARMNN_RELU_USE_CPU=ON', '-Donnxruntime_ARMNN_BN_USE_CPU=ON', '-Donnxruntime_ENABLE_NVTX_PROFILE=OFF', '-Donnxruntime_ENABLE_TRAINING=OFF', '-Donnxruntime_USE_HOROVOD=OFF', '-Donnxruntime_BUILD_BENCHMARKS=OFF', '-Donnxruntime_PYBIND_EXPORT_OPSCHEMA=OFF', '-Donnxruntime_ENABLE_MEMLEAK_CHECKER=OFF', '-DCMAKE_BUILD_TYPE=RelWithDebInfo']\r\nAdding flags for Mac builds\r\nUse gtest from submodule\r\nCMake Error at CMakeLists.txt:346 (add_subdirectory):\r\n  The source directory\r\n\r\n    /Users/hottelet/onnxruntime-master/cmake/external/googletest\r\n\r\n  does not contain a CMakeLists.txt file.\r\n\r\n\r\nCMake Error at CMakeLists.txt:347 (set_target_properties):\r\n  set_target_properties Can not find target to add properties to: gmock\r\n\r\n\r\nCMake Error at CMakeLists.txt:348 (set_target_properties):\r\n  set_target_properties Can not find target to add properties to: gmock_main\r\n\r\n\r\nCMake Error at CMakeLists.txt:349 (set_target_properties):\r\n  set_target_properties Can not find target to add properties to: gtest\r\n\r\n\r\nCMake Error at CMakeLists.txt:350 (set_target_properties):\r\n  set_target_properties Can not find target to add properties to: gtest_main\r\n\r\n\r\nCMake Error at CMakeLists.txt:351 (add_library):\r\n  add_library cannot create ALIAS target \"GTest::gmock\" because target\r\n  \"gmock\" does not already exist.\r\n\r\n\r\nCMake Error at CMakeLists.txt:352 (add_library):\r\n  add_library cannot create ALIAS target \"GTest::gmock_main\" because target\r\n  \"gmock_main\" does not already exist.\r\n\r\n\r\nCMake Error at CMakeLists.txt:353 (add_library):\r\n  add_library cannot create ALIAS target \"GTest::gtest\" because target\r\n  \"gtest\" does not already exist.\r\n\r\n\r\nCMake Error at CMakeLists.txt:354 (add_library):\r\n  add_library cannot create ALIAS target \"GTest::gtest_main\" because target\r\n  \"gtest_main\" does not already exist.\r\n\r\n\r\nCMake Error at CMakeLists.txt:394 (add_subdirectory):\r\n  The source directory\r\n\r\n    /Users/hottelet/onnxruntime-master/cmake/external/nsync\r\n\r\n  does not contain a CMakeLists.txt file.\r\n\r\n\r\nUse protobuf from submodule\r\nCMake Error at CMakeLists.txt:433 (add_subdirectory):\r\n  add_subdirectory given source\r\n  \"/Users/hottelet/onnxruntime-master/cmake/external/protobuf/cmake\" which is\r\n  not an existing directory.\r\n\r\n\r\nCMake Error at CMakeLists.txt:445 (add_library):\r\n  add_library cannot create ALIAS target \"protobuf::libprotobuf\" because\r\n  target \"libprotobuf-lite\" does not already exist.\r\n\r\n\r\nCMake Error at CMakeLists.txt:494 (add_subdirectory):\r\n  The source directory\r\n\r\n    /Users/hottelet/onnxruntime-master/cmake/external/date\r\n\r\n  does not contain a CMakeLists.txt file.\r\n\r\n\r\nCMake Error at CMakeLists.txt:505 (add_subdirectory):\r\n  The source directory\r\n\r\n    /Users/hottelet/onnxruntime-master/cmake/external/re2\r\n\r\n  does not contain a CMakeLists.txt file.\r\n\r\n\r\nCMake Error at CMakeLists.txt:506 (set_target_properties):\r\n  set_target_properties Can not find target to add properties to: re2\r\n\r\n\r\nCMake Error at CMakeLists.txt:507 (add_library):\r\n  add_library cannot create ALIAS target \"re2::re2\" because target \"re2\" does\r\n  not already exist.\r\n\r\n\r\nCMake Error at onnx/CMakeLists.txt:38 (list):\r\n  list sub-command REMOVE_ITEM requires two or more arguments.\r\n\r\n\r\nCMake Warning at flake8.cmake:19 (message):\r\n  Could not find 'flake8' to check python scripts.  Please install flake8\r\n  using pip.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:1212 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/Users/hottelet/onnxruntime-master/build/Linux/RelWithDebInfo/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/Users/hottelet/onnxruntime-master/build/Linux/RelWithDebInfo/CMakeFiles/CMakeError.log\".\r\nTraceback (most recent call last):\r\n  File \"/Users/hottelet/onnxruntime-master/tools/ci_build/build.py\", line 1691, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/hottelet/onnxruntime-master/tools/ci_build/build.py\", line 1635, in main\r\n    generate_build_tree(\r\n  File \"/Users/hottelet/onnxruntime-master/tools/ci_build/build.py\", line 867, in generate_build_tree\r\n    run_subprocess(\r\n  File \"/Users/hottelet/onnxruntime-master/tools/ci_build/build.py\", line 419, in run_subprocess\r\n    completed_process = subprocess.run(\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py\", line 512, in run\r\n    raise CalledProcessError(retcode, process.args,\r\nsubprocess.CalledProcessError: Command '['/usr/local/Cellar/cmake/3.17.0_1/bin/cmake', '/Users/hottelet/onnxruntime-master/cmake', '-Donnxruntime_RUN_ONNX_TESTS=OFF', '-Donnxruntime_BUILD_WINML_TESTS=ON', '-Donnxruntime_GENERATE_TEST_REPORTS=ON', '-Donnxruntime_DEV_MODE=ON', '-DPYTHON_EXECUTABLE=/Library/Frameworks/Python.framework/Versions/3.8/bin/python3', '-Donnxruntime_USE_CUDA=OFF', '-Donnxruntime_CUDNN_HOME=', '-Donnxruntime_USE_FEATURIZERS=OFF', '-Donnxruntime_CUDA_HOME=', '-Donnxruntime_USE_JEMALLOC=OFF', '-Donnxruntime_USE_MIMALLOC_STL_ALLOCATOR=OFF', '-Donnxruntime_USE_MIMALLOC_ARENA_ALLOCATOR=OFF', '-Donnxruntime_ENABLE_PYTHON=OFF', '-Donnxruntime_BUILD_CSHARP=OFF', '-Donnxruntime_BUILD_JAVA=OFF', '-Donnxruntime_BUILD_NODEJS=OFF', '-Donnxruntime_BUILD_SHARED_LIB=ON', '-Donnxruntime_USE_EIGEN_FOR_BLAS=ON', '-Donnxruntime_USE_OPENBLAS=OFF', '-Donnxruntime_USE_DNNL=OFF', '-Donnxruntime_USE_MKLML=OFF', '-Donnxruntime_USE_NGRAPH=OFF', '-Donnxruntime_USE_OPENVINO=OFF', '-Donnxruntime_USE_OPENVINO_MYRIAD=OFF', '-Donnxruntime_USE_OPENVINO_GPU_FP32=OFF', '-Donnxruntime_USE_OPENVINO_GPU_FP16=OFF', '-Donnxruntime_USE_OPENVINO_CPU_FP32=OFF', '-Donnxruntime_USE_OPENVINO_VAD_M=OFF', '-Donnxruntime_USE_OPENVINO_VAD_F=OFF', '-Donnxruntime_USE_OPENVINO_BINARY=OFF', '-Donnxruntime_USE_NNAPI_BUILTIN=OFF', '-Donnxruntime_USE_RKNPU=OFF', '-Donnxruntime_USE_OPENMP=OFF', '-Donnxruntime_USE_TVM=OFF', '-Donnxruntime_USE_LLVM=OFF', '-Donnxruntime_ENABLE_MICROSOFT_INTERNAL=OFF', '-Donnxruntime_USE_VITISAI=OFF', '-Donnxruntime_USE_NUPHAR=OFF', '-Donnxruntime_USE_TENSORRT=OFF', '-Donnxruntime_TENSORRT_HOME=', '-Donnxruntime_USE_MIGRAPHX=OFF', '-Donnxruntime_MIGRAPHX_HOME=', '-Donnxruntime_CROSS_COMPILING=OFF', '-Donnxruntime_DISABLE_CONTRIB_OPS=OFF', '-Donnxruntime_DISABLE_ML_OPS=OFF', '-Donnxruntime_DISABLE_RTTI=OFF', '-Donnxruntime_MSVC_STATIC_RUNTIME=OFF', '-Donnxruntime_ENABLE_LANGUAGE_INTEROP_OPS=OFF', '-Donnxruntime_USE_DML=OFF', '-Donnxruntime_USE_WINML=OFF', '-Donnxruntime_BUILD_FOR_WINDOWS_STORE=OFF', '-Donnxruntime_USE_TELEMETRY=OFF', '-Donnxruntime_ENABLE_LTO=OFF', '-Donnxruntime_USE_ACL=OFF', '-Donnxruntime_USE_ACL_1902=OFF', '-Donnxruntime_USE_ACL_1905=OFF', '-Donnxruntime_USE_ACL_1908=OFF', '-Donnxruntime_USE_ARMNN=OFF', '-Donnxruntime_ARMNN_RELU_USE_CPU=ON', '-Donnxruntime_ARMNN_BN_USE_CPU=ON', '-Donnxruntime_ENABLE_NVTX_PROFILE=OFF', '-Donnxruntime_ENABLE_TRAINING=OFF', '-Donnxruntime_USE_HOROVOD=OFF', '-Donnxruntime_BUILD_BENCHMARKS=OFF', '-Donnxruntime_PYBIND_EXPORT_OPSCHEMA=OFF', '-Donnxruntime_ENABLE_MEMLEAK_CHECKER=OFF', '-DCMAKE_BUILD_TYPE=RelWithDebInfo']' returned non-zero exit status 1.\r\n\r\n\r\n(base) MacBook-Pro:~/onnxruntime-master quantum-fusion$ export ONNX_ML=1\r\n(base) MacBook-Pro:~/onnxruntime-master quantum-fusion$ python3 setup.py bdist_wheel\r\nrunning bdist_wheel\r\nrunning build\r\nrunning build_py\r\ncreating build/lib\r\ncreating build/lib/onnxruntime\r\ncopying onnxruntime/__init__.py -> build/lib/onnxruntime\r\nerror: package directory 'onnxruntime/backend' does not exist\r\n(base) MacBook-Pro:~/onnxruntime-master quantum-fusion$ pip3 install --upgrade dist/*.whl\r\nWARNING: Requirement 'dist/*.whl' looks like a filename, but the file does not exist\r\nERROR: *.whl is not a valid wheel filename.\r\nWARNING: You are using pip version 20.2; however, version 20.2.1 is available.\r\nYou should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\r\n\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nIt should build the .JAR file \r\n\r\nPIP list: \r\n\r\npip list\r\nPackage                    Version\r\n-------------------------- ---------\r\nabsl-py                    0.9.0\r\naiohttp                    3.6.2\r\nalembic                    1.4.2\r\nappnope                    0.1.0\r\nasn1crypto                 1.4.0\r\nastroid                    2.4.2\r\nastunparse                 1.6.3\r\nasync-timeout              3.0.1\r\nattrs                      19.3.0\r\nazure-common               1.1.25\r\nazure-core                 1.7.0\r\nazure-storage-blob         12.3.2\r\nbackcall                   0.2.0\r\nBentoML                    0.8.3\r\nboto3                      1.14.31\r\nbotocore                   1.17.31\r\ncachetools                 4.1.1\r\nCerberus                   1.3.2\r\ncertifi                    2020.6.20\r\ncffi                       1.14.1\r\nchardet                    3.0.4\r\nclick                      7.1.2\r\nconfigparser               5.0.0\r\ncryptography               2.9.2\r\ncycler                     0.10.0\r\ndecorator                  4.4.2\r\ndefusedxml                 0.6.0\r\ndocker                     4.2.2\r\ndocutils                   0.15.2\r\nenum-compat                0.0.3\r\nfire                       0.3.1\r\nFlask                      1.1.2\r\nfuture                     0.18.2\r\ngast                       0.3.3\r\ngoogle-auth                1.20.0\r\ngoogle-auth-oauthlib       0.4.1\r\ngoogle-pasta               0.2.0\r\ngraphviz                   0.8.4\r\ngrpcio                     1.27.2\r\ngunicorn                   20.0.4\r\nh5py                       2.10.0\r\nhumanfriendly              8.2\r\nidna                       2.9\r\nimageio                    2.9.0\r\nipykernel                  5.3.4\r\nipython                    7.16.1\r\nipython-genutils           0.2.0\r\nisodate                    0.6.0\r\nisort                      4.3.21\r\nitsdangerous               1.1.0\r\njedi                       0.17.2\r\nJinja2                     2.11.2\r\njmespath                   0.10.0\r\njoblib                     0.16.0\r\njupyter-client             6.1.6\r\njupyter-core               4.6.3\r\nKeras-Applications         1.0.6\r\nKeras-Preprocessing        1.1.2\r\nkeras2onnx                 1.7.1\r\nkiwisolver                 1.2.0\r\nlazy-object-proxy          1.4.3\r\nMako                       1.1.3\r\nMarkdown                   3.2.2\r\nMarkupSafe                 1.1.1\r\nmatplotlib                 3.3.0\r\nmccabe                     0.6.1\r\nmock                       4.0.2\r\nmodel-archiver             1.0.3\r\nmsrest                     0.6.18\r\nmultidict                  4.7.6\r\nmxnet                      1.6.0\r\nmxnet-model-server         1.0.8\r\nnetworkx                   2.4\r\nnumpy                      1.18.4\r\noauthlib                   3.1.0\r\nonnx                       1.7.0\r\nonnxconverter-common       1.7.0\r\nonnxmltools                1.7.0\r\nonnxruntime                1.4.0\r\nopt-einsum                 3.3.0\r\noscrypto                   1.2.1\r\npackaging                  20.4\r\npandas                     1.1.0\r\nparso                      0.7.1\r\npexpect                    4.8.0\r\npickleshare                0.7.5\r\nPillow                     7.1.2\r\npip                        20.2\r\nply                        3.11\r\nprometheus-client          0.8.0\r\nprompt-toolkit             3.0.5\r\nprotobuf                   3.12.4\r\npsutil                     5.7.2\r\nptyprocess                 0.6.0\r\npy-zipkin                  0.20.0\r\npyasn1                     0.4.8\r\npyasn1-modules             0.2.8\r\npycparser                  2.20\r\npycryptodomex              3.9.8\r\nPygments                   2.6.1\r\nPyJWT                      1.7.1\r\npylint                     2.5.3\r\npyOpenSSL                  19.1.0\r\npyparsing                  2.4.7\r\npyserial                   3.4\r\npython-dateutil            2.8.0\r\npython-editor              1.0.4\r\npython-json-logger         0.1.11\r\npytz                       2020.1\r\nPyYAML                     5.3.1\r\npyzmq                      19.0.1\r\nrequests                   2.23.0\r\nrequests-oauthlib          1.3.0\r\nrsa                        4.6\r\nruamel.yaml                0.16.10\r\nruamel.yaml.clib           0.2.0\r\ns3transfer                 0.3.3\r\nscikit-learn               0.23.1\r\nscipy                      1.4.1\r\nsetuptools                 48.0.0\r\nsix                        1.15.0\r\nskl2onnx                   1.7.0\r\nsklearn                    0.0\r\nsnowflake-connector-python 2.2.10\r\nSQLAlchemy                 1.3.18\r\nSQLAlchemy-Utils           0.36.8\r\ntabulate                   0.8.7\r\ntensorboard                2.3.0\r\ntensorboard-plugin-wit     1.7.0\r\ntensorflow                 2.3.0\r\ntensorflow-estimator       2.3.0\r\ntermcolor                  1.1.0\r\ntest-generator             0.1.1\r\ntf2onnx                    1.7.0\r\nthreadpoolctl              2.1.0\r\nthriftpy2                  0.4.11\r\ntoml                       0.10.1\r\ntorch                      1.5.0\r\ntorchvision                0.6.0\r\ntornado                    6.0.4\r\ntraitlets                  4.3.3\r\ntyping-extensions          3.7.4.2\r\nurllib3                    1.25.10\r\nwcwidth                    0.2.5\r\nwebsocket-client           0.57.0\r\nWerkzeug                   1.0.1\r\nwheel                      0.34.2\r\nwrapt                      1.12.1\r\nyarl                       1.5.0\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4723", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4723/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4723/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4723/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4723", "id": 674071495, "node_id": "MDU6SXNzdWU2NzQwNzE0OTU=", "number": 4723, "title": "Building onnxruntime on Jetson TX2 with python 3.7.8 and jetpack 4.4", "user": {"login": "tarunraj", "id": 1688508, "node_id": "MDQ6VXNlcjE2ODg1MDg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1688508?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tarunraj", "html_url": "https://github.com/tarunraj", "followers_url": "https://api.github.com/users/tarunraj/followers", "following_url": "https://api.github.com/users/tarunraj/following{/other_user}", "gists_url": "https://api.github.com/users/tarunraj/gists{/gist_id}", "starred_url": "https://api.github.com/users/tarunraj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tarunraj/subscriptions", "organizations_url": "https://api.github.com/users/tarunraj/orgs", "repos_url": "https://api.github.com/users/tarunraj/repos", "events_url": "https://api.github.com/users/tarunraj/events{/privacy}", "received_events_url": "https://api.github.com/users/tarunraj/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 2233218084, "node_id": "MDU6TGFiZWwyMjMzMjE4MDg0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:jetson", "name": "platform:jetson", "color": "c0ef7f", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-06T06:59:42Z", "updated_at": "2020-08-18T03:05:56Z", "closed_at": "2020-08-18T03:05:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n I am trying to build it onnxruntime on Jetson TX2 with python 3.7.8 and jetpack 4.4. While the build was successful, I ran into the following issue while importing it\r\n\r\n```\r\nPython 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:52)\r\n[GCC 7.5.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import onnxruntime\r\n/home/qureai/.local/lib/python3.7/site-packages/onnxruntime/capi/_pybind_state.py:14: UserWarning: Cannot load onnxruntime.capi. Error: 'Python version mismatch: module was compiled for Python 3.6, but the interpreter version is incompatible: 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:52)\r\n[GCC 7.5.0].'.\r\n  warnings.warn(\"Cannot load onnxruntime.capi. Error: '{0}'.\".format(str(e)))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/qureai/.local/lib/python3.7/site-packages/onnxruntime/__init__.py\", line 13, in <module>\r\n    from onnxruntime.capi._pybind_state import get_all_providers, get_available_providers, get_device, set_seed, \\\r\nImportError: cannot import name 'get_all_providers' from 'onnxruntime.capi._pybind_state' (/home/qureai/.local/lib/python3.7/site-packages/onnxruntime/capi/_pybind_state.py)\r\n```\r\n\r\nI was able to build onnxruntime on the same board with python 3.6.9 and it ran successfully.\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution \r\n```\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 18.04.4 LTS\r\nRelease:        18.04\r\nCodename:       bionic\r\n```\r\n\r\n```\r\nNVIDIA Jetson TX2 - Jetpack 4.4 [L4T 32.4.3]                                                                                                       - Version: 3.0.1                 \r\n - Jetpack:        4.4 [L4T 32.4.3]                      \r\n - Board:                                                                                                                                          \r\n   * Type:           TX2                                                                 \r\n   * SOC Family:     tegra186     ID: 24                                                      \r\n   * Module:         P3310-1000   Board: UNKNOWN                                              \r\n   * Code Name:      quill                                                                    \r\n   * Cuda ARCH:      6.2                                                          \r\n   * Board ids:      3310:0000:A0                                                                                                                                                                                                             \r\n - Libraries:                                           \r\n   * CUDA:         10.2.89                                          \r\n   * OpenCV:       4.1.1  compiled CUDA: NO                     \r\n   * TensorRT:     7.1.3.0                                        \r\n   * VPI:          0.3.7                                                                      \r\n   * VisionWorks:  1.6.0.501                                                                  \r\n   * Vulkan:       1.2.70                                                                     \r\n   * cuDNN:        8.0.0.180                                                                                                                                                     \r\n```\r\n\r\n- ONNX Runtime installed from : Source\r\n- ONNX Runtime version: 1.4.0 https://github.com/microsoft/onnxruntime/commit/eb0f57f0e47120ce1d98797e2f0a1c15eb2cd050\r\n- Python version: 3.7.8\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 10.2.89/ 8.0.0.180  \r\n\r\n**To Reproduce**\r\n\r\nI have built onnxruntime for python 3.6 using the instructions [here](https://github.com/microsoft/onnxruntime/blob/master/BUILD.md#nvidia-jetson-tx1tx2nanoxavier) and [here](https://github.com/microsoft/onnxruntime/blob/master/dockerfiles/README.md#nvidia-jetson-tx1tx2nanoxavier)\r\n\r\nTo build it for python3.7, I installed miniforge from [here](https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh) and created a python3.7 environment\r\n\r\n```\r\n\r\n# remove line 895,896 in https://github.com/microsoft/onnxruntime/blob/master/cmake/CMakeLists.txt\r\n        - string (APPEND CMAKE_CUDA_FLAGS \"-gencode=arch=compute_53,code=sm_53 -gencode=arch=compute_62,code=sm_62\") #nano, TX1, TX2 \r\n        - string (APPEND CMAKE_CUDA_FLAGS \"-gencode=arch=compute_72,code=sm_72\") # Jetson N\r\n        + string (APPEND CMAKE_CUDA_FLAGS \"-gencode=arch=compute_62,code=sm_62\") # 62 for tx2\r\n$ ./build.sh --update --config Release --build --build_wheel \\\r\n   --use_cuda --cuda_home /usr/local/cuda --cudnn_home /usr/lib/aarch64-linux-gnu\r\n```\r\n\r\n\r\n**Expected behavior**\r\nBuild will be successful and a wheel file will be created. When you install the wheel file and import onnxruntime, you'll encounter this error\r\n```.local/lib/python3.7/site-packages/onnxruntime/capi/_pybind_state.py:14: UserWarning: Cannot load onnxruntime.capi. Error: 'Python version mismatch: module was compiled for Python 3.6, but the interpreter version is incompatible: 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:52)\r\n[GCC 7.5.0].'.\r\n  warnings.warn(\"Cannot load onnxruntime.capi. Error: '{0}'.\".format(str(e)))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/qureai/.local/lib/python3.7/site-packages/onnxruntime/__init__.py\", line 13, in <module>\r\n    from onnxruntime.capi._pybind_state import get_all_providers, get_available_providers, get_device, set_seed, \\\r\nImportError: cannot import name 'get_all_providers' from 'onnxruntime.capi._pybind_state' (/home/qureai/.local/lib/python3.7/site-packages/onnxruntime/capi/_pybind_state.py)\r\n```\r\n\r\nI think the problem may be with pybind11 which is somehow picking python3.6 instead of 3.7.\r\n\r\ncmake output\r\n```\r\n[ 97%] Built target onnxruntime_test_all                                                                                                                                                                                                      \r\n[ 97%] Performing update step for 'pybind11'                                                                                                                                                                                                  \r\n[ 97%] No patch step for 'pybind11'                                                                                                                                                                                                           \r\n[ 97%] No configure step for 'pybind11'                                                                                                                                                                                                       \r\n[ 97%] No build step for 'pybind11'                                                                                                                                                                                                           \r\n[ 97%] No install step for 'pybind11'                                                                                                                                                                                                         \r\n[ 97%] Completed 'pybind11'                                                                                                                                                                                                                   \r\n[ 98%] Built target pybind11                                                                                                                                                                                                                  \r\n[100%] Built target onnxruntime_pybind11_state \r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4711", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4711/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4711/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4711/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4711", "id": 673517178, "node_id": "MDU6SXNzdWU2NzM1MTcxNzg=", "number": 4711, "title": "Onnxruntimeserver Jupiter notebook has missing dependencies ", "user": {"login": "quantum-fusion", "id": 10088099, "node_id": "MDQ6VXNlcjEwMDg4MDk5", "avatar_url": "https://avatars3.githubusercontent.com/u/10088099?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quantum-fusion", "html_url": "https://github.com/quantum-fusion", "followers_url": "https://api.github.com/users/quantum-fusion/followers", "following_url": "https://api.github.com/users/quantum-fusion/following{/other_user}", "gists_url": "https://api.github.com/users/quantum-fusion/gists{/gist_id}", "starred_url": "https://api.github.com/users/quantum-fusion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quantum-fusion/subscriptions", "organizations_url": "https://api.github.com/users/quantum-fusion/orgs", "repos_url": "https://api.github.com/users/quantum-fusion/repos", "events_url": "https://api.github.com/users/quantum-fusion/events{/privacy}", "received_events_url": "https://api.github.com/users/quantum-fusion/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-05T12:54:22Z", "updated_at": "2020-08-05T16:58:31Z", "closed_at": "2020-08-05T16:58:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nThe OnnxRuntimeServerSSDModel Jupiter notebook requires two dependencies.\r\nhttps://github.com/onnx/tutorials/blob/master/tutorials/OnnxRuntimeServerSSDModel.ipynb\r\n\r\nThese dependencies are missing:\r\nimport assets.onnx_ml_pb2 as onnx_ml_pb2\r\nimport assets.predict_pb2 as predict_pb2\r\n\r\nExecute the ipynb script\r\n\r\nModuleNotFoundError: No module named 'onnx_ml_pb2'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4709", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4709/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4709/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4709/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4709", "id": 673309929, "node_id": "MDU6SXNzdWU2NzMzMDk5Mjk=", "number": 4709, "title": "Concrete C# samples", "user": {"login": "marcusturewicz", "id": 24448509, "node_id": "MDQ6VXNlcjI0NDQ4NTA5", "avatar_url": "https://avatars0.githubusercontent.com/u/24448509?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marcusturewicz", "html_url": "https://github.com/marcusturewicz", "followers_url": "https://api.github.com/users/marcusturewicz/followers", "following_url": "https://api.github.com/users/marcusturewicz/following{/other_user}", "gists_url": "https://api.github.com/users/marcusturewicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/marcusturewicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marcusturewicz/subscriptions", "organizations_url": "https://api.github.com/users/marcusturewicz/orgs", "repos_url": "https://api.github.com/users/marcusturewicz/repos", "events_url": "https://api.github.com/users/marcusturewicz/events{/privacy}", "received_events_url": "https://api.github.com/users/marcusturewicz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1758308176, "node_id": "MDU6TGFiZWwxNzU4MzA4MTc2", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C%23", "name": "api:C#", "color": "0e8a16", "default": false, "description": "related to the C# API"}, {"id": 1267486731, "node_id": "MDU6TGFiZWwxMjY3NDg2NzMx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/status:contributions-welcome", "name": "status:contributions-welcome", "color": "45b2d3", "default": false, "description": "contributions welcome; lower priority items for the core teams on this project"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-05T07:10:45Z", "updated_at": "2020-08-07T20:36:37Z", "closed_at": "2020-08-07T20:36:37Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Whilst the [C# Inference Tutorial](https://github.com/microsoft/onnxruntime/blob/master/docs/CSharp_API.md) demonstrates the C# API surface, the C# samples are lacking and could be complimented with more concrete ones, like the [Python Samples](https://github.com/microsoft/onnxruntime/tree/master/samples#Python):\r\n\r\n![image](https://user-images.githubusercontent.com/24448509/89382530-5dcece80-d73e-11ea-9ec5-183597d4b0fd.png)\r\n\r\nWe could start with a C# version of the ResNet50 example and build from there. If in agreeance, I'd be happy to take this on, having just written [blog post](https://dev.to/marcusturewicz/serverless-image-classification-with-onnx-net-and-azure-functions-1h7l) about doing image classification on Azure Functions with the C# API. Let me know what you think.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4707", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4707/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4707/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4707/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4707", "id": 673139777, "node_id": "MDU6SXNzdWU2NzMxMzk3Nzc=", "number": 4707, "title": "Can't export GPT2-XL to ONNX: ModelProto exceeds maximum protobuf size of 2GB", "user": {"login": "klimentij", "id": 3247669, "node_id": "MDQ6VXNlcjMyNDc2Njk=", "avatar_url": "https://avatars0.githubusercontent.com/u/3247669?v=4", "gravatar_id": "", "url": "https://api.github.com/users/klimentij", "html_url": "https://github.com/klimentij", "followers_url": "https://api.github.com/users/klimentij/followers", "following_url": "https://api.github.com/users/klimentij/following{/other_user}", "gists_url": "https://api.github.com/users/klimentij/gists{/gist_id}", "starred_url": "https://api.github.com/users/klimentij/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/klimentij/subscriptions", "organizations_url": "https://api.github.com/users/klimentij/orgs", "repos_url": "https://api.github.com/users/klimentij/repos", "events_url": "https://api.github.com/users/klimentij/events{/privacy}", "received_events_url": "https://api.github.com/users/klimentij/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-04T23:11:52Z", "updated_at": "2020-08-08T15:06:29Z", "closed_at": "2020-08-08T15:06:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI use `onnxruntime/onnxruntime/python/tools/transformers/benchmark_gpt2.py` script to benchmark and export GPT2-XL (1.5B) to ONNX and apply optimizations:\r\n\r\n```\r\npython benchmark_gpt2.py \\\r\n--model_name \"gpt2-xl\" \\\r\n--cache_dir \"./cache_models\" \\\r\n--onnx_dir=\"./gpt2_xl_onnx_past\" \\\r\n--test_times 10 \\\r\n--precision fp16 \\\r\n--optimize_onnx \\\r\n--use_gpu \\\r\n--batch_sizes \"1\" \\\r\n--past_sequence_lengths 1000 \\\r\n--result_csv gpt2_results.csv\r\n```\r\n\r\nWhen I use it for `gpt2-large`, it works without a problem. When I switch `model_name` to `gpt2-xl`, it shows that optimizations are being applied, but fails to save optimized model to disk:\r\n\r\n```\r\n...\r\nOutput model to ./gpt2_xl_onnx_past/_past_fp16.onnx\r\nTraceback (most recent call last):\r\n  File \"benchmark_gpt2.py\", line 258, in <module>\r\n    main()\r\n  File \"benchmark_gpt2.py\", line 152, in main\r\n    model.config.num_attention_heads, model.config.hidden_size)\r\n  File \"/home/jupyter/onnxruntime/onnxruntime/python/tools/transformers/gpt2_helper.py\", line 252, in optimize_onnx\r\n    m.save_model_to_file(optimized_model_path)\r\n  File \"/home/jupyter/onnxruntime/onnxruntime/python/tools/transformers/onnx_model.py\", line 668, in save_model_to_file\r\n    save_model(self.model, output_path, format=None)\r\n  File \"/opt/conda/lib/python3.7/site-packages/onnx/__init__.py\", line 186, in save_model\r\n    s = _serialize(proto)\r\n  File \"/opt/conda/lib/python3.7/site-packages/onnx/__init__.py\", line 67, in _serialize\r\n    result = proto.SerializeToString()\r\nValueError: Message ONNX_REL_1_7.ModelProto exceeds maximum protobuf size of 2GB: 3276141735 \r\n```\r\n\r\nI also added `use_external_data_format=True` to `torch.onnx.export()` method in `gpt2_helper.py` and expected that it would help, but it did not. I can't use other scripts (`benchmark.py`) because I need GPT2 to be exported with the past state support.\r\n\r\n**Urgency**\r\nI'm blocked on my current GPT2 deployment project because of this issue. The model is approximately 4x more expensive and slow in our production without ONNX optimizations.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9\r\n- ONNX Runtime installed from (source or binary): Binary\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.7.6\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia T4 16Gb + 51Gb RAM\r\n- Pytorch version: 1.5.0+cu101 (has to support `use_external_data_format` flag in `torch.onnx.export()`)\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n1. Add `use_external_data_format=True` to `torch.onnx.export()` method in `gpt2_helper.py`\r\n2. Add `gpt2-xl` to `PRETRAINED_MODELS` list in `benchmark_gpt2.py` \r\n3. Run:\r\n```\r\npython benchmark_gpt2.py \\\r\n--model_name \"gpt2-xl\" \\\r\n--cache_dir \"./cache_models\" \\\r\n--onnx_dir=\"./gpt2_xl_onnx_past\" \\\r\n--test_times 10 \\\r\n--precision fp16 \\\r\n--optimize_onnx \\\r\n--use_gpu \\\r\n--batch_sizes \"1\" \\\r\n--past_sequence_lengths 1000 \\\r\n--result_csv gpt2_results.csv\r\n```\r\n\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\nhttps://huggingface.co/gpt2-xl\r\n\r\n**Expected behavior**\r\ngpt2-xl is benchmarked and exported to ONNX without any error.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4698", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4698/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4698/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4698/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4698", "id": 672391278, "node_id": "MDU6SXNzdWU2NzIzOTEyNzg=", "number": 4698, "title": "ConvTranspose output shape incorrect when auto_pad attribute is set.", "user": {"login": "jwfromm", "id": 9874242, "node_id": "MDQ6VXNlcjk4NzQyNDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/9874242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwfromm", "html_url": "https://github.com/jwfromm", "followers_url": "https://api.github.com/users/jwfromm/followers", "following_url": "https://api.github.com/users/jwfromm/following{/other_user}", "gists_url": "https://api.github.com/users/jwfromm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwfromm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwfromm/subscriptions", "organizations_url": "https://api.github.com/users/jwfromm/orgs", "repos_url": "https://api.github.com/users/jwfromm/repos", "events_url": "https://api.github.com/users/jwfromm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwfromm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2114490130, "node_id": "MDU6TGFiZWwyMTE0NDkwMTMw", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:operator", "name": "component:operator", "color": "303a93", "default": false, "description": "related to specific ONNX operator support"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-03T23:00:49Z", "updated_at": "2020-08-04T22:33:58Z", "closed_at": "2020-08-04T22:33:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nThe [Onnx specification for ConvTranspose](https://github.com/onnx/onnx/blob/master/docs/Operators.md#ConvTranspose) states that only one of the attributes `auto_pad` and `padding` may be applied. Constructing a graph with `auto_pad='SAME_UPPER'` performs proper shape inference, however, onnxruntime produces an incorrect output as it assumes that `padding=[0, 0]` and overrides the auto_pad value.\r\n\r\n**Urgency**\r\nMedium priority, this should be causing issues for onnx graphs that use auto_pad for ConvTranspose operators.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- ONNX Runtime installed from (source or binary): binary\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.7.7\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\n```\r\nimport onnx\r\nfrom onnx import helper, TensorProto\r\nimport onnxruntime.backend\r\nimport numpy as np\r\n\r\nx_shape = [1, 1, 5, 5]\r\nw_shape = [1, 1, 3, 3]\r\ny_shape = [1, 1, 9, 9]\r\nnode = helper.make_node(\r\n    'ConvTranspose',\r\n    inputs=['x', 'W'],\r\n    outputs=['y'],\r\n    kernel_shape=[3, 3],\r\n    strides=[2, 2],\r\n    dilations=[1, 1],\r\n    auto_pad='SAME_UPPER')\r\n\r\ngraph = helper.make_graph(\r\n    [node],\r\n    'convtranspose_test',\r\n    inputs=[helper.make_tensor_value_info(\"x\", TensorProto.FLOAT, list(x_shape)), helper.make_tensor_value_info(\"W\", TensorProto.FLOAT, list(w_shape))],\r\n    outputs=[helper.make_tensor_value_info(\"y\", TensorProto.FLOAT, list(y_shape))])\r\n\r\nmodel = helper.make_model(graph, producer_name='convtranspose_test')\r\n\r\nrep = onnxruntime.backend.prepare(model, 'CPU')\r\nx = np.random.uniform(size=x_shape).astype('float32')\r\nW = np.random.uniform(size=w_shape).astype('float32')\r\nord_out = rep.run([x, W])[0]\r\nprint(ord_out.shape)\r\n```\r\n**Expected behavior**\r\nThe output shape should be `[1, 1, 9, 9]` however running the above script produces a shape of `[1, 1, 11, 11]`, which would be the expected shape if we had set the padding attribute to `[0, 0]`. The output of onnxruntime directly contradicts the onnx graph we constructed.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4689", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4689/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4689/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4689/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4689", "id": 671939302, "node_id": "MDU6SXNzdWU2NzE5MzkzMDI=", "number": 4689, "title": "SetOutputs of graph in transformer not working as expected", "user": {"login": "Dudeldu", "id": 18610449, "node_id": "MDQ6VXNlcjE4NjEwNDQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/18610449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dudeldu", "html_url": "https://github.com/Dudeldu", "followers_url": "https://api.github.com/users/Dudeldu/followers", "following_url": "https://api.github.com/users/Dudeldu/following{/other_user}", "gists_url": "https://api.github.com/users/Dudeldu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dudeldu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dudeldu/subscriptions", "organizations_url": "https://api.github.com/users/Dudeldu/orgs", "repos_url": "https://api.github.com/users/Dudeldu/repos", "events_url": "https://api.github.com/users/Dudeldu/events{/privacy}", "received_events_url": "https://api.github.com/users/Dudeldu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2014185961, "node_id": "MDU6TGFiZWwyMDE0MTg1OTYx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:coreruntime", "name": "component:coreruntime", "color": "303a93", "default": false, "description": "related to core runtime"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-03T09:32:55Z", "updated_at": "2020-09-04T08:51:20Z", "closed_at": "2020-09-04T08:51:20Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I wrote a custom graph transformer, that has to add a new output to a graph. For that, I created new Nodes and tried to set the output of the graph with its `SetOutputs` method.  \r\nNevertheless the outputs aren't added as expected as seen when using `GetOutputName` via C++ API. **BUT** the optimized model, that gets exported with `SetOptimizedModelFilePath` session option, contains the added nodes and outputs as expected and after loading this one into ORT everything works fine.  \r\nSo it seems to me, the graph isn't correctly *rebuilt* after the transformers ran?! Maybe there is another flag, that must be set aside from the `modified` value?\r\n\r\n**System information**\r\n- commit: e70e9e2f6751e3ce14e7aeca1e21af46e5ad6350\r\n\r\n**To Reproduce**\r\nBasic implementation of transformer, that simply adds an ABS node on top of the first existing output and adds the new output to the graph as well (minimal sample).\r\n```cpp\r\n    const std::vector<const NodeArg*>& graphOutputs = graph.GetOutputs();\r\n\r\n    NodeArg* out = new NodeArg(graph.GenerateNodeArgName(\"NEW OUTPUT\"),\r\n                                 &ONNX_NAMESPACE::Utils::DataTypeUtils::ToTypeProto(\r\n                                     ONNX_NAMESPACE::Utils::DataTypeUtils::ToType(\"tensor(int64)\")));\r\n    Node& serializerNode = graph.AddNode(graph.GenerateNodeName(\"TEST\"),\r\n                                                 \"Abs\",\r\n                                                 \"TEST\",\r\n                                                 std::vector<NodeArg*>{const_cast<NodeArg*>(graphOutputs[0])},\r\n                                                 std::vector<NodeArg*>{out},\r\n                                                 nullptr,\r\n                                                 kOnnxDomain);\r\n    serializerNode.SetExecutionProviderType(kCpuExecutionProvider);\r\n\r\n    std::vector<const NodeArg*> newOutputs{};\r\n\r\n    for(const NodeArg* arg: graphOutputs){\r\n        newOutputs.push_back(arg);\r\n    }\r\n    newOutputs.push_back(out);\r\n\r\n    graph.SetOutputs(newOutputs);\r\n    modified = true;\r\n```\r\n\r\n**Expected behavior**\r\nI would expect to directly apply the changes, and not only after serializing to ONNX and reloading, or, if this behaviour was expected, I would wish to rename/hide the `SetOuputs` function.  \r\nOne workaround i'm currently using for just appending a node to an existing output is by mutating the NodeArg the *const* pointer of the graph outputs points towards, which works, but isn't the best way either.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4685", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4685/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4685/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4685/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4685", "id": 670739082, "node_id": "MDU6SXNzdWU2NzA3MzkwODI=", "number": 4685, "title": "PyTorch FPN segmentation model differs in inference between C++ in Windows and Python on Linux", "user": {"login": "samhodge", "id": 86946, "node_id": "MDQ6VXNlcjg2OTQ2", "avatar_url": "https://avatars0.githubusercontent.com/u/86946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samhodge", "html_url": "https://github.com/samhodge", "followers_url": "https://api.github.com/users/samhodge/followers", "following_url": "https://api.github.com/users/samhodge/following{/other_user}", "gists_url": "https://api.github.com/users/samhodge/gists{/gist_id}", "starred_url": "https://api.github.com/users/samhodge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samhodge/subscriptions", "organizations_url": "https://api.github.com/users/samhodge/orgs", "repos_url": "https://api.github.com/users/samhodge/repos", "events_url": "https://api.github.com/users/samhodge/events{/privacy}", "received_events_url": "https://api.github.com/users/samhodge/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-08-01T10:24:38Z", "updated_at": "2020-08-03T02:08:04Z", "closed_at": "2020-08-03T02:08:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\nInference in C++ in Windows produces a poor result compared to Python on Linux\r\n\r\n**Urgency**\r\nI have already started to see if inference is possible with libtorch \r\n\r\nThe client is confused why we have a mIOU of 93% but the result is so poor\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- ONNX Runtime installed from (source or binary): pip\r\n- ONNX Runtime version: 1.4\r\n- Python version:3.6\r\n- Visual Studio version (if applicable):2019\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1 7.6\r\n- GPU model and memory: GTX 1060\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n\r\nFollow the cam vid tutorial here https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb\r\n\r\nUsing a FPN model with Resnext 50 backbone\r\n\r\nsee:\r\nhttps://github.com/samhodge/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb\r\n\r\nExport to opset 11\r\n\r\nProduce inference in C++ on Windows using onnx runtime\r\nTBA:\r\n\r\nProduce inference in Python on Linux using onnx runtime \r\nsee:\r\nhttps://github.com/samhodge/segmentation_models.pytorch/blob/master/examples/example_PRY_onnx_.0.jpg\r\n\r\n- https://github.com/samhodge/segmentation_models.pytorch/blob/master/examples/best_model.onnx.zip\r\n\r\n**Expected behavior**\r\n\r\nInference should be identical \r\n\r\n**Screenshots**\r\n\r\nhttps://github.com/samhodge/segmentation_models.pytorch/blob/master/examples/example_PRY_onnx_.0.jpg\r\n\r\n**Additional context**\r\nThe actual model and data are client sensitive but I will reproduce with CamVid dataset\r\n\r\nUnder WinML there are errors with MaxPool and ceil attribute missing\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4683", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4683/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4683/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4683/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4683", "id": 670386589, "node_id": "MDU6SXNzdWU2NzAzODY1ODk=", "number": 4683, "title": "C# how to mock Run's result", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-08-01T00:50:12Z", "updated_at": "2020-08-06T04:53:27Z", "closed_at": "2020-08-06T04:53:27Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Is your feature request related to a problem? Please describe.**\r\n1. In my unit tests I want to mock the result of `Run` from a session, and then test the logic that deals with the results\r\n2. Due to performance I do not want to convert tensors to arrays, and so would rather keep tensors in `IDisposableReadOnlyCollection<DisposableNamedOnnxValue>`\r\n3. I know we should not mock 3rd party, but this is a little different, and my C#-fu is not great\r\n\r\n\r\nCurrently I have to wrap `IDisposableReadOnlyCollection` as well as `DisposableNamedOnnxValue` because I cannot create any of them from normal tensors. \r\n\r\nWhat is the preferred/suggested way to avoid all these wrapping? Specially because some times I want to pass the result of one model directly to another and thus need to keep  `DisposableNamedOnnxValue` just as-is, to avoid data copy.\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): 1.4.0\r\n\r\n**Describe the solution you'd like**\r\nA public constructor for `DisposableNamedOnnxValue`, or a way to convert `NamedOnnxValue` to `DisposableNamedOnnxValue`. Internally it could ignore actually calling `Dispose` if the tensor is created like that.\r\n\r\n**Describe alternatives you've considered**\r\nWrapping\r\n\r\n```C#\r\n    internal class TensorCollection : IDisposableReadOnlyCollection<TensorResult>\r\n    {\r\n        private readonly IDisposableReadOnlyCollection<DisposableNamedOnnxValue> _results;\r\n        private readonly List<NamedOnnxValue> _tensorList;\r\n        private bool _disposed; // To detect redundant calls\r\n...\r\n    }\r\n\r\n    internal sealed class TensorResult : IDisposable\r\n    {\r\n        private readonly DisposableNamedOnnxValue _disposableOnnxValue;\r\n        private readonly NamedOnnxValue _onnxValue;\r\n        private bool _disposed; // To detect redundant calls\r\n\r\n        public string Name { get; }\r\n\r\n        /// <summary>\r\n        /// Initializes a new instance of the <see cref=\"TensorResult\"/> class.\r\n        /// Wrap external type.\r\n        /// </summary>\r\n        public TensorResult(DisposableNamedOnnxValue disposableOnnxValue)\r\n        {\r\n            Contracts.AssertValue(disposableOnnxValue);\r\n\r\n            _disposableOnnxValue = disposableOnnxValue;\r\n            Name = disposableOnnxValue.Name;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Initializes a new instance of the <see cref=\"TensorResult\"/> class.\r\n        /// Internal constructor, for mocking.\r\n        /// </summary>\r\n        internal TensorResult(NamedOnnxValue onnxValue)\r\n        {\r\n            Contracts.AssertValue(onnxValue);\r\n\r\n            _disposableOnnxValue = null;\r\n            _onnxValue = onnxValue;\r\n            Name = onnxValue.Name;\r\n        }\r\n\r\n        public Tensor<T> AsTensor<T>()\r\n        {\r\n            if (_disposableOnnxValue != null)\r\n            {\r\n                return _disposableOnnxValue.AsTensor<T>();\r\n            }\r\n            return _onnxValue.AsTensor<T>();\r\n        }\r\n\r\n        public void Dispose()\r\n        {\r\n            if (!_disposed)\r\n            {\r\n                _disposableOnnxValue?.Dispose();\r\n            }\r\n            _disposed = true;\r\n        }\r\n    }\r\n\r\n```\r\n\r\n**Additional context**\r\nIs there any suggestions or samples or generic advice? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4665", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4665/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4665/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4665/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4665", "id": 668381176, "node_id": "MDU6SXNzdWU2NjgzODExNzY=", "number": 4665, "title": "Failed to build ORT 1.4.0 with CUDA provider", "user": {"login": "GuanLuo", "id": 41310872, "node_id": "MDQ6VXNlcjQxMzEwODcy", "avatar_url": "https://avatars0.githubusercontent.com/u/41310872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GuanLuo", "html_url": "https://github.com/GuanLuo", "followers_url": "https://api.github.com/users/GuanLuo/followers", "following_url": "https://api.github.com/users/GuanLuo/following{/other_user}", "gists_url": "https://api.github.com/users/GuanLuo/gists{/gist_id}", "starred_url": "https://api.github.com/users/GuanLuo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GuanLuo/subscriptions", "organizations_url": "https://api.github.com/users/GuanLuo/orgs", "repos_url": "https://api.github.com/users/GuanLuo/repos", "events_url": "https://api.github.com/users/GuanLuo/events{/privacy}", "received_events_url": "https://api.github.com/users/GuanLuo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-30T05:20:16Z", "updated_at": "2020-07-30T20:47:24Z", "closed_at": "2020-07-30T20:47:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nORT build failed with the following error:\r\n```\r\nnvcc error   : 'ptxas' died due to signal 11 (Invalid memory reference)\r\nnvcc error   : 'ptxas' core dumped\r\nCMakeFiles/onnxruntime_providers_cuda.dir/build.make:1089: recipe for target 'CMakeFiles/onnxruntime_providers_cuda.dir/workspace/onnxruntime/onnxruntime/core/providers/cuda/math/topk_impl.cu.o' failed\r\nmake[2]: *** [CMakeFiles/onnxruntime_providers_cuda.dir/workspace/onnxruntime/onnxruntime/core/providers/cuda/math/topk_impl.cu.o] Error 139\r\nCMakeFiles/Makefile2:732: recipe for target 'CMakeFiles/onnxruntime_providers_cuda.dir/all' failed\r\nmake[1]: *** [CMakeFiles/onnxruntime_providers_cuda.dir/all] Error 2\r\nmake: *** [all] Error 2\r\n```\r\n\r\n\r\n**Urgency**\r\nWe would like to update the ORT version that we use for next release\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.6\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0.167 / 8.0.0.180\r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\nRun the following dockerfile to build the docker image (the base image is handle as it has CUDA / CUDNN version that we need)\r\n```\r\nARG BASE_IMAGE=nvcr.io/nvidia/tritonserver:20.06-py3\r\n\r\n############################################################################\r\n## Onnx Runtime stage: Build Onnx Runtime on CUDA 11, CUDNN 8\r\n############################################################################\r\nFROM ${BASE_IMAGE} AS tritonserver_onnx\r\n\r\n# Onnx Runtime release version\r\nARG ONNX_RUNTIME_VERSION=1.4.0\r\n\r\nWORKDIR /workspace\r\n\r\n# Get release version of Onnx Runtime\r\nRUN apt-get update && \\\r\n    apt-get install -y --no-install-recommends git && \\\r\n    rm -rf /var/lib/apt/lists/*\r\n\r\nRUN git clone -b rel-${ONNX_RUNTIME_VERSION} --recursive https://github.com/Microsoft/onnxruntime && \\\r\n    (cd onnxruntime && \\\r\n            git submodule update --init --recursive)\r\n\r\nARG SCRIPT_DIR=/workspace/onnxruntime/tools/ci_build/github/linux/docker/scripts\r\n\r\nRUN sed -i \"s/backend-test-tools.*//\" ${SCRIPT_DIR}/install_onnx.sh\r\nRUN cp -r ${SCRIPT_DIR} /tmp/scripts && \\\r\n    ${SCRIPT_DIR}/install_ubuntu.sh -p 3.6 -o 18.04 && ${SCRIPT_DIR}/install_deps.sh -p 3.6\r\n\r\nENV PATH /usr/bin:$PATH\r\nRUN cmake --version\r\n\r\n# Allow configure to pick up GDK and CuDNN where it expects it.\r\n# (Note: $CUDNN_VERSION is defined by NVidia's base image)\r\nRUN _CUDNN_VERSION=$(echo $CUDNN_VERSION | cut -d. -f1-2) && \\\r\n    mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/include && \\\r\n    ln -s /usr/include/cudnn.h /usr/local/cudnn-$_CUDNN_VERSION/cuda/include/cudnn.h && \\\r\n    mkdir -p /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64 && \\\r\n    ln -s /etc/alternatives/libcudnn_so /usr/local/cudnn-$_CUDNN_VERSION/cuda/lib64/libcudnn.so\r\n\r\n# Build files will be in /workspace/build\r\nARG COMMON_BUILD_ARGS=\"--skip_submodule_sync --parallel --build_shared_lib --use_openmp\"\r\nRUN mkdir -p /workspace/build\r\nRUN python3 /workspace/onnxruntime/tools/ci_build/build.py --build_dir /workspace/build \\\r\n            --config Release $COMMON_BUILD_ARGS \\\r\n            --use_cuda \\\r\n            --cuda_home /usr/local/cuda \\\r\n            --cudnn_home /usr/local/cudnn-$(echo $CUDNN_VERSION | cut -d. -f1-2)/cuda \\\r\n            --update \\\r\n            --build\r\n```\r\n\r\n**Expected behavior**\r\nBuild finishes successfully", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4655", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4655/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4655/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4655/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4655", "id": 667736977, "node_id": "MDU6SXNzdWU2Njc3MzY5Nzc=", "number": 4655, "title": "Windows Version Requirements", "user": {"login": "finsker", "id": 10171596, "node_id": "MDQ6VXNlcjEwMTcxNTk2", "avatar_url": "https://avatars1.githubusercontent.com/u/10171596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/finsker", "html_url": "https://github.com/finsker", "followers_url": "https://api.github.com/users/finsker/followers", "following_url": "https://api.github.com/users/finsker/following{/other_user}", "gists_url": "https://api.github.com/users/finsker/gists{/gist_id}", "starred_url": "https://api.github.com/users/finsker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/finsker/subscriptions", "organizations_url": "https://api.github.com/users/finsker/orgs", "repos_url": "https://api.github.com/users/finsker/repos", "events_url": "https://api.github.com/users/finsker/events{/privacy}", "received_events_url": "https://api.github.com/users/finsker/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-29T10:36:12Z", "updated_at": "2020-07-29T10:51:36Z", "closed_at": "2020-07-29T10:51:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello All,\r\n\r\ni could not unfortunately find windows version requirements. Can i run onnxruntime on system with window 7 via C#?  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4653", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4653/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4653/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4653/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4653", "id": 667668411, "node_id": "MDU6SXNzdWU2Njc2Njg0MTE=", "number": 4653, "title": "loading a model in ort throws error: onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_62' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:43 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{22,256}, requested shape:{56,1,256}", "user": {"login": "natangold85", "id": 24711661, "node_id": "MDQ6VXNlcjI0NzExNjYx", "avatar_url": "https://avatars2.githubusercontent.com/u/24711661?v=4", "gravatar_id": "", "url": "https://api.github.com/users/natangold85", "html_url": "https://github.com/natangold85", "followers_url": "https://api.github.com/users/natangold85/followers", "following_url": "https://api.github.com/users/natangold85/following{/other_user}", "gists_url": "https://api.github.com/users/natangold85/gists{/gist_id}", "starred_url": "https://api.github.com/users/natangold85/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/natangold85/subscriptions", "organizations_url": "https://api.github.com/users/natangold85/orgs", "repos_url": "https://api.github.com/users/natangold85/repos", "events_url": "https://api.github.com/users/natangold85/events{/privacy}", "received_events_url": "https://api.github.com/users/natangold85/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-29T08:51:06Z", "updated_at": "2020-07-31T07:40:28Z", "closed_at": "2020-07-31T07:40:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "i'm converting a pytorch model to onnx this model is exported as onnx model.\r\nwhen i tried to load it using onnxruntime it returns this error:\r\n\r\nonnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'Reshape_62' Status Message: /onnxruntime_src/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:43 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&, std::vector<long int>&) gsl::narrow_cast<int64_t>(input_shape.Size()) == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{22,256}, requested shape:{56,1,256}\r\n\r\ni isolated the problematic line to a stand alone model. \r\n\r\n`\r\nimport torch\r\nimport onnx\r\nimport onnxruntime\r\n\r\nclass Test(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\r\n        x[:y.shape[0], 0, :] = y\r\n        return x\r\n\r\nx = torch.zeros((56,6,256))\r\ny = torch.rand((22,256))\r\n\r\nm = Test()\r\ntorch_outputs = m(x, y)\r\n\r\npath = \"/tmp/m.onnx\"\r\ntorch.onnx.export(m, (x, y), path,\r\n                    do_constant_folding=False,\r\n                    opset_version=12,\r\n                    input_names=[\"x\", \"y\"],\r\n                    output_names=[\"z\"])\r\n\r\nonnx_model = onnx.load_model(path)\r\nonnx.checker.check_model(onnx_model)\r\ninferred_model = onnx.shape_inference.infer_shapes(onnx_model)\r\nonnx.save(inferred_model, path)\r\n\r\nort_model = onnxruntime.InferenceSession(path)\r\n\r\nort_outputs = ort_model.run([\"z\"], {\"x\": x.numpy(), \"y\": y.numpy()})\r\n`\r\n\r\n**System information**\r\ni'm running on Linux Ubuntu 16.04 with python 3.6.8\r\npytorch: 1.6.0, onnxruntime: 1.4.0, onnx: 1.7.0\r\n\r\n\r\n**To Reproduce**\r\nrun the attached code\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4644", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4644/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4644/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4644/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4644", "id": 667338709, "node_id": "MDU6SXNzdWU2NjczMzg3MDk=", "number": 4644, "title": "Multinomial float16 is broken since a recent change to the Cast transformer", "user": {"login": "PatriceVignola", "id": 5880386, "node_id": "MDQ6VXNlcjU4ODAzODY=", "avatar_url": "https://avatars1.githubusercontent.com/u/5880386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PatriceVignola", "html_url": "https://github.com/PatriceVignola", "followers_url": "https://api.github.com/users/PatriceVignola/followers", "following_url": "https://api.github.com/users/PatriceVignola/following{/other_user}", "gists_url": "https://api.github.com/users/PatriceVignola/gists{/gist_id}", "starred_url": "https://api.github.com/users/PatriceVignola/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PatriceVignola/subscriptions", "organizations_url": "https://api.github.com/users/PatriceVignola/orgs", "repos_url": "https://api.github.com/users/PatriceVignola/repos", "events_url": "https://api.github.com/users/PatriceVignola/events{/privacy}", "received_events_url": "https://api.github.com/users/PatriceVignola/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2114490130, "node_id": "MDU6TGFiZWwyMTE0NDkwMTMw", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:operator", "name": "component:operator", "color": "303a93", "default": false, "description": "related to specific ONNX operator support"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-28T20:05:12Z", "updated_at": "2020-07-30T06:32:02Z", "closed_at": "2020-07-30T06:32:02Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\nMultinomial float16 throws an exception since a [recent change to the Cast transformer](https://github.com/microsoft/onnxruntime/pull/4523)\r\n\r\n**To Reproduce**\r\nRun inference the following model: [multinomial_float16.zip](https://github.com/microsoft/onnxruntime/files/4990832/multinomial_float16.zip)\r\n\r\n**Expected behavior**\r\nThe model completes inference without throwing an exception\r\n\r\n**Additional context**\r\nThe change to the Cast transformer sets the output types of nodes who have a \"dtype\" attribute to float, but Multinomial is a special case that outputs only int32 or int64 tensors. Therefore, an exception is thrown during inference.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4643", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4643/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4643/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4643/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4643", "id": 667129198, "node_id": "MDU6SXNzdWU2NjcxMjkxOTg=", "number": 4643, "title": "cmake error", "user": {"login": "cqray1990", "id": 32585434, "node_id": "MDQ6VXNlcjMyNTg1NDM0", "avatar_url": "https://avatars0.githubusercontent.com/u/32585434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cqray1990", "html_url": "https://github.com/cqray1990", "followers_url": "https://api.github.com/users/cqray1990/followers", "following_url": "https://api.github.com/users/cqray1990/following{/other_user}", "gists_url": "https://api.github.com/users/cqray1990/gists{/gist_id}", "starred_url": "https://api.github.com/users/cqray1990/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cqray1990/subscriptions", "organizations_url": "https://api.github.com/users/cqray1990/orgs", "repos_url": "https://api.github.com/users/cqray1990/repos", "events_url": "https://api.github.com/users/cqray1990/events{/privacy}", "received_events_url": "https://api.github.com/users/cqray1990/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-28T14:40:13Z", "updated_at": "2020-07-30T23:34:37Z", "closed_at": "2020-07-30T23:34:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "when i cmake the code as follows:\r\n#include <experimental_onnxruntime_cxx_api.h>\r\n#include <onnxruntime_c_api.h>\r\n#include <opencv2/core/core.hpp>\r\n#include <opencv2/opencv.hpp>\r\n#include <opencv2/highgui/highgui.hpp>\r\n#include <opencv2/imgproc/imgproc.hpp>\r\n#include <vector>\r\n#include <stdlib.h>\r\n#include <iostream>\r\n\r\nusing namespace cv;\r\nusing namespace std;\r\n\r\nstd::string print_shape(const std::vector<int64_t>& v) {\r\n  std::stringstream ss(\"\");\r\n  for (size_t i = 0; i < v.size() - 1; i++)\r\n    ss << v[i] << \"x\";\r\n  ss << v[v.size() - 1];\r\n  return ss.str();\r\n}\r\n\r\nint calculate_product(const std::vector<int64_t>& v) {\r\n  int total = 1;\r\n  for (auto& i : v) total *= i;\r\n  return total;\r\n}\r\n\r\n\r\nvoid resizeimg(cv::Mat org,cv::Mat dst)\r\n{\r\n    int h = org.rows;\r\n    int w = org.cols;\r\n\r\n    float scale;\r\n    int crnn_h = 32;\r\n    int crnn_w_target;\r\n    int crnn_h_target;\r\n    scale = crnn_h*1.0/h;\r\n    crnn_w_target = int(w * scale);\r\n\r\n    resize(org,dst,Size(crnn_w_target,crnn_h));\r\n    \r\n}\r\n\r\nint main(int argc, const char** argv) \r\n{\r\n\r\n//   if (argc != 2) {\r\n//     cout << \"Usage: ./onnx-api-example <onnx_model.onnx>\" << endl;\r\n//     return -1;\r\n//   }\r\n  char * modelpath= \"/home/lgx/myprj/OCRdetectionreg/crnn_huocepiao/crnn_Rec_done_3_371686.onnx\";\r\n  const char * imgpath = \"/home/lgx/myprj/OCRdetectionreg/crnn_huocepiao/1.jpg\";\r\n  \r\n  cv::Mat img = cv::imread(imgpath);\r\n  cv::Mat dst;\r\n  resizeimg(img,dst);\r\n\r\n  \r\n\r\n  // onnxruntime setup\r\n  Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"example-model-explorer\");\r\n  Ort::SessionOptions session_options;\r\n\r\n  Ort::Experimental::Session session = Ort::Experimental::Session(env, modelpath, session_options); \r\n  std::vector<std::string> input_names = session.GetInputNames();\r\n  std::vector<std::vector<int64_t> > input_shapes = session.GetInputShapes();\r\n  cout << \"Input Node Name/Shape (\" << input_names.size() << \"):\" << endl;\r\n  for (size_t i = 0; i < input_names.size(); i++) {\r\n    cout << \"\\t\" << input_names[i] << \" : \" << print_shape(input_shapes[i]) << endl;\r\n  }\r\n\r\n  std::vector<std::string> output_names = session.GetOutputNames();\r\n  std::vector<std::vector<int64_t> > output_shapes = session.GetOutputShapes();\r\n  cout << \"Output Node Name/Shape (\" << output_names.size() << \"):\" << endl;\r\n  for (size_t i = 0; i < output_names.size(); i++) {\r\n    cout << \"\\t\" << output_names[i] << \" : \" << print_shape(output_shapes[i]) << endl;\r\n  }\r\n\r\n\r\n    assert(input_names.size() == 1 && output_names.size() == 1);\r\n    auto input_shape = input_shapes[0];\r\n    int total_number_elements = calculate_product(input_shape);\r\n\r\n     std::vector<float> input_tensor_values(total_number_elements);\r\n  std::generate(input_tensor_values.begin(), input_tensor_values.end(), [&] { return rand() % 255; });  // generate random numbers in the range [0, 255]\r\n  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\r\n  std::vector<Ort::Value> input_tensors;\r\n  input_tensors.push_back(Ort::Value::CreateTensor<float>(memory_info, input_tensor_values.data(), input_tensor_values.size(), input_shape.data(), input_shape.size()));\r\n\r\n  // double-check the dimensions of the input tensor\r\n  assert(input_tensors[0].IsTensor() &&\r\n         input_tensors[0].GetTensorTypeAndShapeInfo().GetShape() == input_shape);\r\n  cout << \"\\ninput_tensor shape: \" << print_shape(input_tensors[0].GetTensorTypeAndShapeInfo().GetShape()) << endl;\r\n\r\n  // pass data through model\r\n  cout << \"Running model...\";\r\n  try {\r\n    auto output_tensors = session.Run(session.GetInputNames(), input_tensors, session.GetOutputNames());\r\n    cout << \"done\" << endl;\r\n\r\n    // double-check the dimensions of the output tensors\r\n    // NOTE: the number of output tensors is equal to the number of output nodes specifed in the Run() call\r\n    assert(output_tensors.size() == session.GetOutputNames().size() &&\r\n           output_tensors[0].IsTensor());\r\n    cout << \"output_tensor_shape: \" << print_shape(output_tensors[0].GetTensorTypeAndShapeInfo().GetShape()) << endl;\r\n\r\n  } catch (const Ort::Exception& exception) {\r\n    cout << \"ERROR running model inference: \" << exception.what() << endl;\r\n    exit(-1);\r\n  }\r\n\r\n    return 0;\r\n}\r\n\r\nit got the errors:\r\n\r\n/usr/include/c++/5/bits/stl_tree.h:731:7: error: \u2018pair\u2019 does not name a type\r\n       pair<_Base_ptr, _Base_ptr>\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:735:7: error: \u2018pair\u2019 does not name a type\r\n       pair<_Base_ptr, _Base_ptr>\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:854:32: error: \u2018_Node_allocator\u2019 has not been declared\r\n       _Rb_tree(_Rb_tree&& __x, _Node_allocator&& __a);\r\n                                ^\r\n/usr/include/c++/5/bits/stl_tree.h:884:7: error: \u2018reverse_iterator\u2019 does not name a type\r\n       reverse_iterator\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:888:7: error: \u2018const_reverse_iterator\u2019 does not name a type\r\n       const_reverse_iterator\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:892:7: error: \u2018reverse_iterator\u2019 does not name a type\r\n       reverse_iterator\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:896:7: error: \u2018const_reverse_iterator\u2019 does not name a type\r\n       const_reverse_iterator\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:914:36: error: \u2018_Alloc_traits\u2019 has not been declared\r\n       swap(_Rb_tree& __t) noexcept(_Alloc_traits::_S_nothrow_swap());\r\n                                    ^\r\n/usr/include/c++/5/bits/stl_tree.h:922:9: error: \u2018pair\u2019 does not name a type\r\n         pair<iterator, bool>\r\n         ^\r\n/usr/include/c++/5/bits/stl_tree.h:954:2: error: \u2018pair\u2019 does not name a type\r\n  pair<iterator, bool>\r\n  ^\r\n/usr/include/c++/5/bits/stl_tree.h:1105:7: error: \u2018pair\u2019 does not name a type\r\n       pair<iterator, iterator>\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:1108:7: error: \u2018pair\u2019 does not name a type\r\n       pair<const_iterator, const_iterator>\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:1248:38: error: \u2018_Alloc_traits\u2019 has not been declared\r\n       operator=(_Rb_tree&&) noexcept(_Alloc_traits::_S_nothrow_move());\r\n                                      ^\r\n/usr/include/c++/5/bits/stl_tree.h:1261:36: error: \u2018Ort::Ort::std::true_type\u2019 has not been declared\r\n       _M_move_data(_Rb_tree&, std::true_type);\r\n                                    ^\r\n/usr/include/c++/5/bits/stl_tree.h:1266:36: error: \u2018Ort::Ort::std::false_type\u2019 has not been declared\r\n       _M_move_data(_Rb_tree&, std::false_type);\r\n                                    ^\r\n/usr/include/c++/5/bits/stl_tree.h:1266:7: error: \u2018void Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_move_data(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>&, int)\u2019 cannot be overloaded\r\n       _M_move_data(_Rb_tree&, std::false_type);\r\n       ^\r\n/usr/include/c++/5/bits/stl_tree.h:1261:7: error: with \u2018void Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_move_data(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>&, int)\u2019\r\n       _M_move_data(_Rb_tree&, std::true_type);\r\n       ^\r\nIn file included from /usr/include/c++/5/bits/stl_pair.h:59:0,\r\n                 from /usr/include/c++/5/bits/stl_algobase.h:64,\r\n                 from /usr/include/c++/5/bits/char_traits.h:39,\r\n                 from /usr/include/c++/5/string:40,\r\n                 from /usr/include/c++/5/stdexcept:39,\r\n                 from /usr/include/c++/5/array:38,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/experimental_onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:1:\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018Ort::Ort::std::_Rb_tree_node<_Val>* Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Reuse_or_alloc_node::operator()(_Arg&&)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:398:34: error: \u2018forward\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n   _M_t._M_construct_node(__node, _GLIBCXX_FORWARD(_Arg, __arg));\r\n                                  ^\r\n/usr/include/c++/5/bits/stl_tree.h:398:34: note: suggested alternative:\r\n/usr/include/c++/5/bits/move.h:87:5: note:   \u2018std::forward\u2019\r\n     forward(typename std::remove_reference<_Tp>::type&& __t) noexcept\r\n     ^\r\n/usr/include/c++/5/bits/stl_tree.h:398:34: error: expected primary-expression before \u2018>\u2019 token\r\n   _M_t._M_construct_node(__node, _GLIBCXX_FORWARD(_Arg, __arg));\r\n                                  ^\r\n/usr/include/c++/5/bits/stl_tree.h:402:33: error: \u2018forward\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n      return _M_t._M_create_node(_GLIBCXX_FORWARD(_Arg, __arg));\r\n                                 ^\r\n/usr/include/c++/5/bits/stl_tree.h:402:33: note: suggested alternative:\r\n/usr/include/c++/5/bits/move.h:87:5: note:   \u2018std::forward\u2019\r\n     forward(typename std::remove_reference<_Tp>::type&& __t) noexcept\r\n     ^\r\n/usr/include/c++/5/bits/stl_tree.h:402:33: error: expected primary-expression before \u2018>\u2019 token\r\n      return _M_t._M_create_node(_GLIBCXX_FORWARD(_Arg, __arg));\r\n                                 ^\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018Ort::Ort::std::_Rb_tree_node<_Val>* Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Alloc_node::operator()(_Arg&&) const\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:459:33: error: \u2018forward\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n    { return _M_t._M_create_node(_GLIBCXX_FORWARD(_Arg, __arg)); }\r\n                                 ^\r\n/usr/include/c++/5/bits/stl_tree.h:459:33: note: suggested alternative:\r\n/usr/include/c++/5/bits/move.h:87:5: note:   \u2018std::forward\u2019\r\n     forward(typename std::remove_reference<_Tp>::type&& __t) noexcept\r\n     ^\r\n/usr/include/c++/5/bits/stl_tree.h:459:33: error: expected primary-expression before \u2018>\u2019 token\r\n    { return _M_t._M_create_node(_GLIBCXX_FORWARD(_Arg, __arg)); }\r\n                                 ^\r\nIn file included from /usr/include/c++/5/map:60:0,\r\n                 from /usr/local/include/opencv2/flann/params.h:36,\r\n                 from /usr/local/include/opencv2/flann/flann_base.hpp:40,\r\n                 from /usr/local/include/opencv2/flann.hpp:48,\r\n                 from /usr/local/include/opencv2/opencv.hpp:62,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:4:\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::allocator_type Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::get_allocator() const\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:486:53: error: there are no arguments to \u2018_M_get_Node_allocator\u2019 that depend on a template parameter, so a declaration of \u2018_M_get_Node_allocator\u2019 must be available [-fpermissive]\r\n       { return allocator_type(_M_get_Node_allocator()); }\r\n                                                     ^\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018Ort::Ort::std::_Rb_tree_node<_Val>* Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_get_node()\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:491:16: error: \u2018_Alloc_traits\u2019 has not been declared\r\n       { return _Alloc_traits::allocate(_M_get_Node_allocator(), 1); }\r\n                ^\r\n/usr/include/c++/5/bits/stl_tree.h:491:62: error: there are no arguments to \u2018_M_get_Node_allocator\u2019 that depend on a template parameter, so a declaration of \u2018_M_get_Node_allocator\u2019 must be available [-fpermissive]\r\n       { return _Alloc_traits::allocate(_M_get_Node_allocator(), 1); }\r\n                                                              ^\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018void Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_put_node(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Link_type)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:495:9: error: \u2018_Alloc_traits\u2019 has not been declared\r\n       { _Alloc_traits::deallocate(_M_get_Node_allocator(), __p, 1); }\r\n         ^\r\n/usr/include/c++/5/bits/stl_tree.h:495:57: error: there are no arguments to \u2018_M_get_Node_allocator\u2019 that depend on a template parameter, so a declaration of \u2018_M_get_Node_allocator\u2019 must be available [-fpermissive]\r\n       { _Alloc_traits::deallocate(_M_get_Node_allocator(), __p, 1); }\r\n                                                         ^\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018void Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_construct_node(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Link_type, _Args&& ...)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:529:8: error: \u2018_Alloc_traits\u2019 has not been declared\r\n        _Alloc_traits::construct(_M_get_Node_allocator(),\r\n        ^\r\n/usr/include/c++/5/bits/stl_tree.h:529:55: error: there are no arguments to \u2018_M_get_Node_allocator\u2019 that depend on a template parameter, so a declaration of \u2018_M_get_Node_allocator\u2019 must be available [-fpermissive]\r\n        _Alloc_traits::construct(_M_get_Node_allocator(),\r\n                                                       ^\r\n/usr/include/c++/5/bits/stl_tree.h:531:12: error: \u2018forward\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n            std::forward<_Args>(__args)...);\r\n            ^\r\n/usr/include/c++/5/bits/stl_tree.h:531:12: note: suggested alternative:\r\nIn file included from /usr/include/c++/5/bits/stl_pair.h:59:0,\r\n                 from /usr/include/c++/5/bits/stl_algobase.h:64,\r\n                 from /usr/include/c++/5/bits/char_traits.h:39,\r\n                 from /usr/include/c++/5/string:40,\r\n                 from /usr/include/c++/5/stdexcept:39,\r\n                 from /usr/include/c++/5/array:38,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/experimental_onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:1:\r\n/usr/include/c++/5/bits/move.h:87:5: note:   \u2018std::forward\u2019\r\n     forward(typename std::remove_reference<_Tp>::type&& __t) noexcept\r\n     ^\r\nIn file included from /usr/include/c++/5/map:60:0,\r\n                 from /usr/local/include/opencv2/flann/params.h:36,\r\n                 from /usr/local/include/opencv2/flann/flann_base.hpp:40,\r\n                 from /usr/local/include/opencv2/flann.hpp:48,\r\n                 from /usr/local/include/opencv2/opencv.hpp:62,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:4:\r\n/usr/include/c++/5/bits/stl_tree.h:531:30: error: expected primary-expression before \u2018>\u2019 token\r\n            std::forward<_Args>(__args)...);\r\n                              ^\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018Ort::Ort::std::_Rb_tree_node<_Val>* Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_create_node(_Args&& ...)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:546:29: error: \u2018forward\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n    _M_construct_node(__tmp, std::forward<_Args>(__args)...);\r\n                             ^\r\n/usr/include/c++/5/bits/stl_tree.h:546:29: note: suggested alternative:\r\nIn file included from /usr/include/c++/5/bits/stl_pair.h:59:0,\r\n                 from /usr/include/c++/5/bits/stl_algobase.h:64,\r\n                 from /usr/include/c++/5/bits/char_traits.h:39,\r\n                 from /usr/include/c++/5/string:40,\r\n                 from /usr/include/c++/5/stdexcept:39,\r\n                 from /usr/include/c++/5/array:38,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/experimental_onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:1:\r\n/usr/include/c++/5/bits/move.h:87:5: note:   \u2018std::forward\u2019\r\n     forward(typename std::remove_reference<_Tp>::type&& __t) noexcept\r\n     ^\r\nIn file included from /usr/include/c++/5/map:60:0,\r\n                 from /usr/local/include/opencv2/flann/params.h:36,\r\n                 from /usr/local/include/opencv2/flann/flann_base.hpp:40,\r\n                 from /usr/local/include/opencv2/flann.hpp:48,\r\n                 from /usr/local/include/opencv2/opencv.hpp:62,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:4:\r\n/usr/include/c++/5/bits/stl_tree.h:546:47: error: expected primary-expression before \u2018>\u2019 token\r\n    _M_construct_node(__tmp, std::forward<_Args>(__args)...);\r\n                                               ^\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018void Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_destroy_node(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Link_type)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:553:2: error: \u2018_Alloc_traits\u2019 has not been declared\r\n  _Alloc_traits::destroy(_M_get_Node_allocator(), __p->_M_valptr());\r\n  ^\r\n/usr/include/c++/5/bits/stl_tree.h:553:47: error: there are no arguments to \u2018_M_get_Node_allocator\u2019 that depend on a template parameter, so a declaration of \u2018_M_get_Node_allocator\u2019 must be available [-fpermissive]\r\n  _Alloc_traits::destroy(_M_get_Node_allocator(), __p->_M_valptr());\r\n                                               ^\r\n/usr/include/c++/5/bits/stl_tree.h: In constructor \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree_impl<_Key_compare, <anonymous> >::_Rb_tree_impl()\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:587:6: error: class \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree_impl<_Key_compare, <anonymous> >\u2019 does not have any field named \u2018_Node_allocator\u2019\r\n    : _Node_allocator(), _M_key_compare(), _M_header(),\r\n      ^\r\n/usr/include/c++/5/bits/stl_tree.h: In constructor \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree_impl<_Key_compare, <anonymous> >::_Rb_tree_impl(const _Key_compare&, const int&)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:592:6: error: class \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree_impl<_Key_compare, <anonymous> >\u2019 does not have any field named \u2018_Node_allocator\u2019\r\n    : _Node_allocator(__a), _M_key_compare(__comp), _M_header(),\r\n      ^\r\n/usr/include/c++/5/bits/stl_tree.h: In constructor \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree_impl<_Key_compare, <anonymous> >::_Rb_tree_impl(const _Key_compare&, int&&)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:598:6: error: class \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree_impl<_Key_compare, <anonymous> >\u2019 does not have any field named \u2018_Node_allocator\u2019\r\n    : _Node_allocator(std::move(__a)), _M_key_compare(__comp),\r\n      ^\r\n/usr/include/c++/5/bits/stl_tree.h:598:22: error: \u2018move\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n    : _Node_allocator(std::move(__a)), _M_key_compare(__comp),\r\n                      ^\r\n/usr/include/c++/5/bits/stl_tree.h:598:22: note: suggested alternative:\r\nIn file included from /usr/include/c++/5/bits/char_traits.h:39:0,\r\n                 from /usr/include/c++/5/string:40,\r\n                 from /usr/include/c++/5/stdexcept:39,\r\n                 from /usr/include/c++/5/array:38,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/experimental_onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:1:\r\n/usr/include/c++/5/bits/stl_algobase.h:495:5: note:   \u2018std::move\u2019\r\n     move(_II __first, _II __last, _OI __result)\r\n     ^\r\nIn file included from /usr/include/c++/5/map:60:0,\r\n                 from /usr/local/include/opencv2/flann/params.h:36,\r\n                 from /usr/local/include/opencv2/flann/flann_base.hpp:40,\r\n                 from /usr/local/include/opencv2/flann.hpp:48,\r\n                 from /usr/local/include/opencv2/opencv.hpp:62,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:4:\r\n/usr/include/c++/5/bits/stl_tree.h: In copy constructor \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree(const Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>&)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:815:10: error: \u2018_Alloc_traits\u2019 has not been declared\r\n          _Alloc_traits::_S_select_on_copy(__x._M_get_Node_allocator()))\r\n          ^\r\n/usr/include/c++/5/bits/stl_tree.h: In constructor \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>&&)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:847:22: error: \u2018true_type\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n    _M_move_data(__x, std::true_type());\r\n                      ^\r\n/usr/include/c++/5/bits/stl_tree.h:847:22: note: suggested alternative:\r\nIn file included from /usr/include/c++/5/bits/move.h:57:0,\r\n                 from /usr/include/c++/5/bits/stl_pair.h:59,\r\n                 from /usr/include/c++/5/bits/stl_algobase.h:64,\r\n                 from /usr/include/c++/5/bits/char_traits.h:39,\r\n                 from /usr/include/c++/5/string:40,\r\n                 from /usr/include/c++/5/stdexcept:39,\r\n                 from /usr/include/c++/5/array:38,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/experimental_onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:1:\r\n/usr/include/c++/5/type_traits:87:45: note:   \u2018std::true_type\u2019\r\n   typedef integral_constant<bool, true>     true_type;\r\n                                             ^\r\nIn file included from /usr/include/c++/5/map:60:0,\r\n                 from /usr/local/include/opencv2/flann/params.h:36,\r\n                 from /usr/local/include/opencv2/flann/flann_base.hpp:40,\r\n                 from /usr/local/include/opencv2/flann.hpp:48,\r\n                 from /usr/local/include/opencv2/opencv.hpp:62,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:4:\r\n/usr/include/c++/5/bits/stl_tree.h: In constructor \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_Rb_tree(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>&&, const allocator_type&)\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:851:18: error: \u2018move\u2019 is not a member of \u2018Ort::Ort::std\u2019\r\n       : _Rb_tree(std::move(__x), _Node_allocator(__a))\r\n                  ^\r\n/usr/include/c++/5/bits/stl_tree.h:851:18: note: suggested alternative:\r\nIn file included from /usr/include/c++/5/bits/char_traits.h:39:0,\r\n                 from /usr/include/c++/5/string:40,\r\n                 from /usr/include/c++/5/stdexcept:39,\r\n                 from /usr/include/c++/5/array:38,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/../include/onnxruntime/core/session/experimental_onnxruntime_cxx_api.h:18,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:1:\r\n/usr/include/c++/5/bits/stl_algobase.h:495:5: note:   \u2018std::move\u2019\r\n     move(_II __first, _II __last, _OI __result)\r\n     ^\r\nIn file included from /usr/include/c++/5/map:60:0,\r\n                 from /usr/local/include/opencv2/flann/params.h:36,\r\n                 from /usr/local/include/opencv2/flann/flann_base.hpp:40,\r\n                 from /usr/local/include/opencv2/flann.hpp:48,\r\n                 from /usr/local/include/opencv2/opencv.hpp:62,\r\n                 from /home/lgx/onnxruntime/onnruntimeprj/src/main.cpp:4:\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::size_type Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::max_size() const\u2019:\r\n/usr/include/c++/5/bits/stl_tree.h:910:16: error: \u2018_Alloc_traits\u2019 has not been declared\r\n       { return _Alloc_traits::max_size(_M_get_Node_allocator()); }\r\n                ^\r\n/usr/include/c++/5/bits/stl_tree.h:910:62: error: there are no arguments to \u2018_M_get_Node_allocator\u2019 that depend on a template parameter, so a declaration of \u2018_M_get_Node_allocator\u2019 must be available [-fpermissive]\r\n       { return _Alloc_traits::max_size(_M_get_Node_allocator()); }\r\n                                                              ^\r\n/usr/include/c++/5/bits/stl_tree.h: In member function \u2018Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::iterator Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Alloc>::_M_insert_unique_(Ort::Ort::std::_Rb_tree<_Key, _Val, _KeyOfValue, _Compare, _Al\r\n\r\n\r\ni think #include <experimental_onnxruntime_cxx_api.h> is something wrong but i don't know why", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4641", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4641/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4641/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4641/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4641", "id": 666800364, "node_id": "MDU6SXNzdWU2NjY4MDAzNjQ=", "number": 4641, "title": "Non-zero status code returned while running Concat node. Name:'Concat_582'. Axis 0 has mismatched dimensions of 5 and 24", "user": {"login": "briliantnugraha", "id": 9381132, "node_id": "MDQ6VXNlcjkzODExMzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/9381132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/briliantnugraha", "html_url": "https://github.com/briliantnugraha", "followers_url": "https://api.github.com/users/briliantnugraha/followers", "following_url": "https://api.github.com/users/briliantnugraha/following{/other_user}", "gists_url": "https://api.github.com/users/briliantnugraha/gists{/gist_id}", "starred_url": "https://api.github.com/users/briliantnugraha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/briliantnugraha/subscriptions", "organizations_url": "https://api.github.com/users/briliantnugraha/orgs", "repos_url": "https://api.github.com/users/briliantnugraha/repos", "events_url": "https://api.github.com/users/briliantnugraha/events{/privacy}", "received_events_url": "https://api.github.com/users/briliantnugraha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-28T06:17:26Z", "updated_at": "2020-07-31T02:24:20Z", "closed_at": "2020-07-31T02:24:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi guys,\r\nI have encountered a strange issue here, where the pytorch-ONNX's output (and output shape) is fine. But I encountered an error as described in my title when I want to use the converted model (onnx) in onnxruntime. FYI, the detectron2 model is working fine in Pytorch. Could anyone help with it? T,T\r\n\r\nWhen I debug it in Spyder, I could see that the first dimension/dim-0 and the output from Pytorch and ONNX seems correct (Picture 3), i.e. inputs are (24,1) and (24,4), and the concatenated output with dim-0 is (24,5). However, when I am trying to feed proposals and features in onnxruntime, the error as in the title (Picture 1) came.\r\n- - - - \r\n### Picture 1 (ONNXRuntime)\r\n![image](https://user-images.githubusercontent.com/9381132/88624192-1fe7ff80-d0d9-11ea-937d-5f59e4c50589.png)\r\n- - - - \r\n\r\nWhen I traced the error from ONNXRuntime->Netron->onnx export->Pytorch. I see that the error pointed to 'Concat_582' (**Picture 1**). Then I checked in Netron (**Picture 2**), We could see that the concat node has two inputs, which are **613** and **609**. Then I traced it back to the torch.onnx.export with verbose=True (Picture 3) and check those nodes **(the blue mark ones)**, we could see that the shape is correct for dim=0.\r\n- - - - \r\n### Picture 2 (Netron)\r\n![image](https://user-images.githubusercontent.com/9381132/88624256-3b530a80-d0d9-11ea-8058-433602335c72.png)\r\n- - - - \r\n### Picture 3 (Spyder debug, please see the blue marks)\r\n![image](https://user-images.githubusercontent.com/9381132/88624276-4017be80-d0d9-11ea-8ac6-25d6a8363e60.png)\r\n- - - - \r\n\r\nIf we traced it to the code as in [here](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/poolers.py#L75-L77), we could see that it will try to fit the size w.r.t the tensor. Thus, ideally, the result should be the same size (24). Then the concatenation as in [here](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/poolers.py#L78) should be fine. FYI, the line is not matching in the Picture 3 due to my debugging scripts, but the links that I pointed out are of the same source.\r\n\r\nCode to reproduce the error\r\n- To check onnx.export: [Use detectron2, and save only the Roi heads part](https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/meta_arch/rcnn.py#L206). You are gonna need to tweak the I/O a bit so that the 'instances conversion error' with [torch.onnx.export](https://pytorch.org/docs/stable/onnx.html) does not happen.\r\n- To check the error in onnxruntime , [please use this files](https://drive.google.com/file/d/1ZYIsGjJsxjkKYzzhQJzwHnk_cQjjAsqa/view?usp=sharing) with the codes below.\r\n\r\n\r\n```\r\nimport onnxruntime as rt\r\nimport numpy as np, cv2, os, onnx\r\nfrom onnx import helper\r\nfrom onnx import TensorProto\r\nimport torch\r\nfrom PIL import Image\r\nfrom time import time\r\n\r\n\r\nprint('a. Define input...')\r\ntorchpath = r'new_proposals.pt'\r\ninptorch = torch.load(torchpath)\r\n\r\nprint('b. Define model paths...')\r\npathmodel = 'detectron2_2.onnx' # put your frozen onnx model here\r\n\r\nprint('c. Check the model...')\r\nmodel = onnx.load(pathmodel)\r\nonnx.checker.check_model(model)\r\n\r\n\r\nprint('d. Define session onnxruntime...')\r\nstart = time()\r\nsess = rt.InferenceSession(pathmodel)\r\nend = time()\r\nprint('Loadtime of onnxruntime: {:.2f}s'.format(end-start))\r\n\r\n\r\nprint('\\n\\nCheck Input list: ')\r\n[print(i, a.name, a.shape, a.type) for i,a in enumerate(sess.get_inputs())]\r\n\r\n#ORT inputs\r\ninput0 = sess.get_inputs()[0].name\r\ninput1 = sess.get_inputs()[1].name\r\ninput2 = sess.get_inputs()[2].name\r\ninput3 = sess.get_inputs()[3].name\r\ninput4 = sess.get_inputs()[4].name\r\ninput5 = sess.get_inputs()[5].name\r\ninput6 = sess.get_inputs()[6].name\r\ninput7 = sess.get_inputs()[7].name\r\n\r\n#Pytorch-Detectron2 inputs for Roi heads\r\ninp0 = inptorch[0]['p2'].cpu().numpy()\r\ninp1 = inptorch[0]['p3'].cpu().numpy()\r\ninp2 = inptorch[0]['p4'].cpu().numpy()\r\ninp3 = inptorch[0]['p5'].cpu().numpy()\r\ninp4 = inptorch[0]['p6'].cpu().numpy()\r\ninp5 = inptorch[1][0].cpu().numpy()\r\ninp6 = inptorch[1][1].cpu().numpy()\r\ninp7 = inptorch[1][2].cpu().numpy()\r\n\r\nprint('-------\\nNumpy input0:', inp0.shape, inp0.dtype)\r\nprint('Numpy input1:', inp1.shape, inp1.dtype)\r\nprint('Numpy input2:', inp2.shape, inp2.dtype)\r\nprint('Numpy input3:', inp3.shape, inp3.dtype)\r\nprint('Numpy input4:', inp4.shape, inp4.dtype)\r\nprint('Numpy input5:', inp5.shape, inp5.dtype)\r\nprint('Numpy input6:', inp6.shape, inp6.dtype)\r\nprint('Numpy input7:', inp7.shape, inp7.dtype)\r\n\r\n\r\nprint('Test inference...\\n\\n')\r\n# This will give the erro as in the Picture 1\r\noutputs = sess.run(None, {input0: inp0, input1: inp1, input2: inp2, \\\r\n                                          input3: inp3, input4: inp4, input5: inp5, \\\r\n                                          input6: inp6, input7: inp7})#[0]\r\n```\r\n\r\n\r\nSystem info:\r\n- OS Platform and Distribution: Windows 10 ver1909\r\n- ONNX Runtime installed from (source or binary): Binary\r\n- ONNX Runtime version: 1.3.0\r\n- Python version: 3.6.5\r\n- Visual Studio version (if applicable): VS 2017 Community Edition\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: GTX 1080 8GB\r\n[Urgent] If possible, please help me to solve it within this week, your help will be much appreciated.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4635", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4635/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4635/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4635/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4635", "id": 666638900, "node_id": "MDU6SXNzdWU2NjY2Mzg5MDA=", "number": 4635, "title": "C API Error in model input dimensions with yolov4 model", "user": {"login": "AmirElAttar", "id": 30228248, "node_id": "MDQ6VXNlcjMwMjI4MjQ4", "avatar_url": "https://avatars0.githubusercontent.com/u/30228248?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AmirElAttar", "html_url": "https://github.com/AmirElAttar", "followers_url": "https://api.github.com/users/AmirElAttar/followers", "following_url": "https://api.github.com/users/AmirElAttar/following{/other_user}", "gists_url": "https://api.github.com/users/AmirElAttar/gists{/gist_id}", "starred_url": "https://api.github.com/users/AmirElAttar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AmirElAttar/subscriptions", "organizations_url": "https://api.github.com/users/AmirElAttar/orgs", "repos_url": "https://api.github.com/users/AmirElAttar/repos", "events_url": "https://api.github.com/users/AmirElAttar/events{/privacy}", "received_events_url": "https://api.github.com/users/AmirElAttar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1395147764, "node_id": "MDU6TGFiZWwxMzk1MTQ3NzY0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C/C++", "name": "api:C/C++", "color": "0e8a16", "default": false, "description": "related to the  public API(and ABI) for onnxruntime"}, {"id": 1311608287, "node_id": "MDU6TGFiZWwxMzExNjA4Mjg3", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:documentation", "name": "component:documentation", "color": "303a93", "default": false, "description": "related to documentation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-27T23:08:02Z", "updated_at": "2020-08-02T23:02:42Z", "closed_at": "2020-08-02T23:02:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nusing the code in sample: C_Api_Sample.cpp \r\nwith the zoo model Yolov4: https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/yolov4\r\nfirst input dimension is reported as **(-1)** instead of **(1)** from function: **g_ort->GetDimensions**\r\n\r\n**Urgency**\r\nnot blocking but good to resolve for my case I already know the model i/p o/p but may not be for others\r\n\r\n**System information**\r\n- Linux Ubuntu 16.04\r\n- ONNX Runtime installed from (source ):\r\n- ONNX Runtime version: git status: branch 530117c\r\n- C API\r\n\r\n- GCC/Compiler version (if compiling from source):\r\ngcc (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\r\n\r\n\r\n**To Reproduce**\r\n-just run C_Api_Sample.cpp  with the listed model above\r\n\r\n**Expected behavior**\r\nNumber of inputs = 1\r\nInput 0 : name=input_1:0\r\nInput 0 : type=1\r\nInput 0 : num_dims=4\r\nInput 0 : dim 0=**1**\r\nInput 0 : dim 1=416\r\nInput 0 : dim 2=416\r\nInput 0 : dim 3=3\r\n\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n![image](https://user-images.githubusercontent.com/30228248/88600523-ba5f2900-d06e-11ea-9d18-25a1149f621c.png)\r\n\r\n\r\n**Additional context**\r\nnone\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4628", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4628/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4628/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4628/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4628", "id": 666106009, "node_id": "MDU6SXNzdWU2NjYxMDYwMDk=", "number": 4628, "title": "when input shape  too big, the result of GRU is different with Numpy.", "user": {"login": "Channingss", "id": 12471701, "node_id": "MDQ6VXNlcjEyNDcxNzAx", "avatar_url": "https://avatars1.githubusercontent.com/u/12471701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Channingss", "html_url": "https://github.com/Channingss", "followers_url": "https://api.github.com/users/Channingss/followers", "following_url": "https://api.github.com/users/Channingss/following{/other_user}", "gists_url": "https://api.github.com/users/Channingss/gists{/gist_id}", "starred_url": "https://api.github.com/users/Channingss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Channingss/subscriptions", "organizations_url": "https://api.github.com/users/Channingss/orgs", "repos_url": "https://api.github.com/users/Channingss/repos", "events_url": "https://api.github.com/users/Channingss/events{/privacy}", "received_events_url": "https://api.github.com/users/Channingss/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2014185961, "node_id": "MDU6TGFiZWwyMDE0MTg1OTYx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:coreruntime", "name": "component:coreruntime", "color": "303a93", "default": false, "description": "related to core runtime"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-07-27T08:48:31Z", "updated_at": "2020-07-31T09:10:34Z", "closed_at": "2020-07-31T09:10:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nwhen input shape  too big, and  there are both positive and negative numbers in \u2019X\u2018 or 'W', the result of GRU is wrong.\r\n\r\n**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 16.04):\r\n- ONNX Runtime installed from (pip install onnxruntime==1.4.0):\r\n- ONNX Runtime version: 1.4.0\r\n- Python version:3.7\r\n\r\n**To Reproduce**\r\n```\r\nimport numpy as np\r\nimport onnx\r\nfrom collections import namedtuple\r\nimport onnxruntime as rt\r\nfrom onnx import helper\r\nfrom onnx import AttributeProto, TensorProto, GraphProto\r\n\r\ndef _extract_value_info(arr, name):  # type: (np.ndarray, Text) -> onnx.ValueInfoProto\r\n    return onnx.helper.make_tensor_value_info(\r\n            name=name,\r\n                    elem_type=onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype],\r\n                            shape=arr.shape)\r\n\r\ndef expect(nodes,  # type: onnx.NodeProto\r\n           inputs,  # type: Sequence[np.ndarray]\r\n           outputs,  # type: Sequence[np.ndarray]\r\n           name,  # type: Text\r\n           opset,\r\n           ):  # type: (...) -> None\r\n    print('op_type:{}, op_set:{}'.format(node.op_type, opset))\r\n    present_inputs = [x for x in node.input if (x != '')]\r\n    present_outputs = [x for x in node.output if (x != '')]\r\n    inputs_vi = [_extract_value_info(arr, arr_name)\r\n                 for arr, arr_name in zip(inputs, present_inputs)]\r\n    outputs_vi = [_extract_value_info(arr, arr_name)\r\n                  for arr, arr_name in zip(outputs, present_outputs)]\r\n    graph = onnx.helper.make_graph(\r\n        nodes=nodes,\r\n        name=name,\r\n        inputs=inputs_vi,\r\n        outputs=outputs_vi)\r\n    opset_imports = [onnx.helper.make_opsetid(\"\", opset)]\r\n    model = onnx.helper.make_model(graph, producer_name='backend-test', opset_imports=opset_imports)\r\n    onnx.save_model(model, 'GRU-1.onnx')\r\n    sess = rt.InferenceSession('GRU-1.onnx')\r\n    inputs_dict = {}\r\n    for i in range(0, len(inputs)):\r\n        inputs_dict[sess.get_inputs()[i].name] = inputs[i]\r\n    res_onnx = sess.run(None,input_feed=inputs_dict)\r\n    for i in range(0, len(outputs)):\r\n        output = outputs[i].flatten()\r\n        output_onnx = res_onnx[i].flatten()\r\n        diff = output - output_onnx\r\n        max_elem = max(np.max(np.fabs(output)) , np.max(np.fabs(output_onnx)))\r\n        print('max absolute diff: ', np.max(np.fabs(diff)))\r\n        print('max relative diff: ', np.max(np.fabs(diff))/max_elem)\r\n\r\nclass GRU_Helper():\r\n   def __init__(self, **params):  # type: (*Any) -> None\r\n       # GRU Input Names\r\n       X = str('X')\r\n       W = str('W')\r\n       R = str('R')\r\n       B = str('B')\r\n       H_0 = str('initial_h')\r\n       LBR = str('linear_before_reset')\r\n       number_of_gates = 3\r\n\r\n       required_inputs = [X, W, R]\r\n       for i in required_inputs:\r\n           assert i in params, \"Missing Required Input: {0}\".format(i)\r\n\r\n       self.num_directions = params[W].shape[0]\r\n\r\n       if self.num_directions == 1:\r\n           for k in params.keys():\r\n               if k != X:\r\n                   params[k] = np.squeeze(params[k], axis=0)\r\n\r\n           hidden_size = params[R].shape[-1]\r\n           batch_size = params[X].shape[1]\r\n\r\n           b = params[B] if B in params else np.zeros(2 * number_of_gates * hidden_size)\r\n           h_0 = params[H_0] if H_0 in params else np.zeros((batch_size, hidden_size))\r\n           lbr = params[LBR] if LBR in params else 0\r\n\r\n           self.X = params[X]\r\n           self.W = params[W]\r\n           self.R = params[R]\r\n           self.B = b\r\n           self.H_0 = h_0\r\n           self.LBR = lbr\r\n\r\n       else:\r\n           raise NotImplementedError()\r\n\r\n   def f(self, x):  # type: (np.ndarray) -> np.ndarray\r\n       return 1 / (1 + np.exp(-x))\r\n\r\n   def g(self, x):  # type: (np.ndarray) -> np.ndarray\r\n       return np.tanh(x)\r\n\r\n   def step(self):  # type: () -> Tuple[np.ndarray, np.ndarray]\r\n       h_list = []\r\n       [w_z, w_r, w_h] = np.split(self.W, 3)\r\n       [r_z, r_r, r_h] = np.split(self.R, 3)\r\n       [w_bz, w_br, w_bh, r_bz, r_br, r_bh] = np.split(self.B, 6)\r\n       gates_w = np.transpose(np.concatenate((w_z, w_r)))\r\n       gates_r = np.transpose(np.concatenate((r_z, r_r)))\r\n       gates_b = np.add(np.concatenate((w_bz, w_br)), np.concatenate((r_bz, r_br)))\r\n\r\n       H_t = self.H_0\r\n       for x in np.split(self.X, self.X.shape[0], axis=0):\r\n           gates = np.dot(x, gates_w) + np.dot(H_t, gates_r) + gates_b\r\n           #print(np.dot(x, gates_w), np.dot(x, gates_w).shape)\r\n           z, r = np.split(gates, 2, -1)\r\n           z = self.f(z)\r\n           r = self.f(r)\r\n           h_default = self.g(np.dot(x, np.transpose(w_h)) + np.dot(r * H_t, np.transpose(r_h)) + w_bh + r_bh)\r\n           h_linear = self.g(np.dot(x, np.transpose(w_h)) + r * (np.dot(H_t, np.transpose(r_h)) + r_bh) + w_bh)\r\n           #print(np.dot(x,  np.transpose(w_h)), np.dot(x,  np.transpose(w_h)).shape)\r\n           h = h_linear if self.LBR else h_default\r\n           H = (1 - z) * h + z * H_t\r\n           h_list.append(H)\r\n           H_t = H\r\n       concatenated = np.concatenate(h_list)\r\n       if self.num_directions == 1:\r\n           output = np.expand_dims(concatenated, 1)\r\n       return output, h_list[-1]\r\n\r\n\r\ninput_size = 512\r\nhidden_size = 128\r\nweight_scale = 0.1\r\nnumber_of_gates = 3\r\nnum_direct = 1\r\nseq_len = 10\r\nbatch_size = 10\r\ninput = np.random.random((seq_len, batch_size , input_size)).astype(np.float32) - 0.5\r\nW = weight_scale * np.random.random((num_direct, number_of_gates * hidden_size, input_size)).astype(np.float32)\r\nR = weight_scale * np.random.random((num_direct, number_of_gates * hidden_size, hidden_size)).astype(np.float32)\r\nB = weight_scale * np.random.random((num_direct, 2*number_of_gates * hidden_size)).astype(np.float32)\r\n\r\nw = onnx.helper.make_node(\r\n    'Constant',\r\n    inputs=[],\r\n    outputs=['w'],\r\n    value=onnx.helper.make_tensor(\r\n        name='const_tensor',\r\n        data_type=onnx.TensorProto.FLOAT,\r\n        dims=W.shape,\r\n        vals=W.flatten().astype(float),\r\n    ),\r\n)\r\n\r\nr = onnx.helper.make_node(\r\n    'Constant',\r\n    inputs=[],\r\n    outputs=['r'],\r\n    value=onnx.helper.make_tensor(\r\n        name='const_tensor',\r\n        data_type=onnx.TensorProto.FLOAT,\r\n        dims=R.shape,\r\n        vals=R.flatten().astype(float),\r\n    )\r\n)\r\n\r\nb = onnx.helper.make_node(\r\n    'Constant',\r\n    inputs=[],\r\n    outputs=['b'],\r\n    value=onnx.helper.make_tensor(\r\n        name='const_tensor',\r\n        data_type=onnx.TensorProto.FLOAT,\r\n        dims=B.shape,\r\n        vals=B.flatten().astype(float),\r\n    ),\r\n)\r\n\r\nnode = onnx.helper.make_node(\r\n    'GRU',\r\n    inputs=['X', 'w', 'r', 'b'],\r\n    outputs=['Y', 'Y_h'],\r\n    hidden_size=hidden_size,\r\n    #linear_before_reset=1,\r\n    #direction='bidirectional'\r\n)\r\n\r\nY = weight_scale * np.ones((1, 1, 3, hidden_size)).astype(np.float32)\r\nY_h = weight_scale * np.ones((1, 3, hidden_size)).astype(np.float32)\r\ngru = GRU_Helper(X=input, W=W, R=R)\r\nY, Y_h = gru.step()\r\nexpect([w,r,b,node], inputs=[input], outputs=[Y.astype(np.float32), Y_h.astype(np.float32)], name='test_gru_defaults', opset=10)\r\n```\r\n**Expected behavior**\r\ndiff should small than 10e-4?\r\n\r\n**Screenshots**\r\n![image](https://user-images.githubusercontent.com/12471701/88522450-f079ba00-d028-11ea-8a6a-126af005bb8a.png)\r\n\r\n**Additional context**\r\nthe expect function & GRU_Helper class is copyed form Test module in onnxruntime.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4624", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4624/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4624/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4624/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4624", "id": 665877698, "node_id": "MDU6SXNzdWU2NjU4Nzc2OTg=", "number": 4624, "title": "Error using output of one model as input for another C#", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1758308176, "node_id": "MDU6TGFiZWwxNzU4MzA4MTc2", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C%23", "name": "api:C#", "color": "0e8a16", "default": false, "description": "related to the C# API"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-26T23:06:46Z", "updated_at": "2020-08-02T22:39:54Z", "closed_at": "2020-08-02T22:39:53Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Describe the bug**\r\nI would like to use the output of one model as input to another, and avoid data marshaling/copy of large tensors.\r\nSimilar to issue #2983, and I get exception\r\n\r\n> Exception thrown: 'System.OverflowException' in System.Private.CoreLib.dll\r\n\r\nThe difference is that I re-create the NamedOnnxValue because I want to give it a different name.\r\n\r\n**Urgency**\r\nI am now going to export with a different name and try passing the output as-is without change, hopefully it works otherwise I will update the issue.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- ONNX Runtime installed from (source or binary): nuget\r\n- ONNX Runtime version: 1.4.0\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n\r\n```C#\r\nNamedOnnxValue inputIds = NamedOnnxValue.CreateFromTensor(\"input\", val.AsTensor<long>());\r\n```\r\n\r\nWhere `val` is `DisposableNamedOnnxValue` with name \"output\". I want to change that to \"input\".\r\n\r\n**Expected behavior**\r\nEasy way to work with intermediate tensors. It is also hard to unit test with \"DisposableNamedOnnxValue\" and I currently wrap everything.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4623", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4623/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4623/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4623/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4623", "id": 665872189, "node_id": "MDU6SXNzdWU2NjU4NzIxODk=", "number": 4623, "title": "Test failure on Windows", "user": {"login": "xkszltl", "id": 5203025, "node_id": "MDQ6VXNlcjUyMDMwMjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/5203025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xkszltl", "html_url": "https://github.com/xkszltl", "followers_url": "https://api.github.com/users/xkszltl/followers", "following_url": "https://api.github.com/users/xkszltl/following{/other_user}", "gists_url": "https://api.github.com/users/xkszltl/gists{/gist_id}", "starred_url": "https://api.github.com/users/xkszltl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xkszltl/subscriptions", "organizations_url": "https://api.github.com/users/xkszltl/orgs", "repos_url": "https://api.github.com/users/xkszltl/repos", "events_url": "https://api.github.com/users/xkszltl/events{/privacy}", "received_events_url": "https://api.github.com/users/xkszltl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2233102485, "node_id": "MDU6TGFiZWwyMjMzMTAyNDg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:windows", "name": "platform:windows", "color": "97e572", "default": false, "description": ""}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2020-07-26T22:27:48Z", "updated_at": "2020-08-05T15:50:51Z", "closed_at": "2020-08-02T22:39:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\n```\r\n[ RUN      ] TensorOpTest.CastFromString\r\nD:\\roaster-scratch\\onnxruntime\\onnxruntime\\test\\providers\\provider_test_utils.cc(153): error: The difference between expected[i] and output[i] is nan, which exceeds threshold, where\r\nexpected[i] evaluates to -nan(ind),\r\noutput[i] evaluates to nan, and\r\nthreshold evaluates to 9.9999997473787516e-05.\r\ni:6, provider_type: CPUExecutionProvider\r\nD:\\roaster-scratch\\onnxruntime\\onnxruntime\\test\\providers\\provider_test_utils.cc(153): error: The difference between expected[i] and output[i] is nan, which exceeds threshold, where\r\nexpected[i] evaluates to -nan(ind),\r\noutput[i] evaluates to nan, and\r\nthreshold evaluates to 9.9999997473787516e-05.\r\ni:7, provider_type: CPUExecutionProvider\r\n[  FAILED  ] TensorOpTest.CastFromString (134 ms)\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win Server 2019\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.8.5\r\n- Visual Studio version (if applicable): 2019\r\n\r\n**Screenshots**\r\n\r\n<img width=\"1194\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5203025/88491000-014b1100-cfd2-11ea-835a-04b78dbae9e0.png\">\r\n<img width=\"577\" alt=\"image\" src=\"https://user-images.githubusercontent.com/5203025/88491013-217ad000-cfd2-11ea-870f-53c7e1bfbaa6.png\">\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4622/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4622", "id": 665743593, "node_id": "MDU6SXNzdWU2NjU3NDM1OTM=", "number": 4622, "title": "Does onnxruntime use data parallelism or model parallelism?", "user": {"login": "Peppa-cs", "id": 60772185, "node_id": "MDQ6VXNlcjYwNzcyMTg1", "avatar_url": "https://avatars0.githubusercontent.com/u/60772185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Peppa-cs", "html_url": "https://github.com/Peppa-cs", "followers_url": "https://api.github.com/users/Peppa-cs/followers", "following_url": "https://api.github.com/users/Peppa-cs/following{/other_user}", "gists_url": "https://api.github.com/users/Peppa-cs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Peppa-cs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Peppa-cs/subscriptions", "organizations_url": "https://api.github.com/users/Peppa-cs/orgs", "repos_url": "https://api.github.com/users/Peppa-cs/repos", "events_url": "https://api.github.com/users/Peppa-cs/events{/privacy}", "received_events_url": "https://api.github.com/users/Peppa-cs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-26T09:20:44Z", "updated_at": "2020-07-28T16:47:48Z", "closed_at": "2020-07-28T16:45:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI build onnxruntime with \u2018openmp\u2019 on Nvidia AGX Xaiver. When i run onnx model (resnet) on cpu with onnxruntime, multiple threads are created to perform code. If I set four cpu cores, four threads will be created. The follow pictures are the result of running \u2018pythoncifar10.py\u2019. So I have two questions: 1. Is the num of threads producing by \u2018openmp\u2019 equal to cpu cores? 2. When onnxruntime produces multiple threads, whether it uses data parallelism or model parallelism?\r\n\r\nThanks!\r\n\r\n\r\n\r\n ONNX Runtime version :\r\n\r\n  onnxruntime 1.3.0\r\n  onnx 1.7.0\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/60772185/88475619-4b95a900-cf64-11ea-8837-10be6cfb4063.png)\r\n\r\n![image](https://user-images.githubusercontent.com/60772185/88475622-4e909980-cf64-11ea-922f-473a8dfb90d1.png)\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4620", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4620/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4620/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4620/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4620", "id": 665618372, "node_id": "MDU6SXNzdWU2NjU2MTgzNzI=", "number": 4620, "title": "Input and output tensors data info", "user": {"login": "TratsiakY", "id": 56832989, "node_id": "MDQ6VXNlcjU2ODMyOTg5", "avatar_url": "https://avatars0.githubusercontent.com/u/56832989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TratsiakY", "html_url": "https://github.com/TratsiakY", "followers_url": "https://api.github.com/users/TratsiakY/followers", "following_url": "https://api.github.com/users/TratsiakY/following{/other_user}", "gists_url": "https://api.github.com/users/TratsiakY/gists{/gist_id}", "starred_url": "https://api.github.com/users/TratsiakY/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TratsiakY/subscriptions", "organizations_url": "https://api.github.com/users/TratsiakY/orgs", "repos_url": "https://api.github.com/users/TratsiakY/repos", "events_url": "https://api.github.com/users/TratsiakY/events{/privacy}", "received_events_url": "https://api.github.com/users/TratsiakY/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-25T16:53:13Z", "updated_at": "2020-08-02T22:38:37Z", "closed_at": "2020-08-02T22:38:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello to everybody. I've done the C++ class to predicting with my onnx model. My model has two outputs: they are int(0 or 1, label) and map(output with probability). The first output(label) read without any problems. How could I get the data from the second layer (probability, map form)? (The layer with probabilities is described as a map<int, float> in Netron).\r\nI build the output tensor as follows:\r\n\r\n> auto output_tensor = ses.Run(RunOptions{ nullptr }, parameters.InputLayerName.data(), input_tensor.data(), parameters.Ninp, parameters.OututLayerName.data(), parameters.NumOfOutputs);\r\n\r\nwhere parameters is the structure with data about model from onnx file. output_tensor is a vector<Value>, where the first element is an int value (label) and the second is a map. \r\nThe model is in [this message](https://github.com/microsoft/onnxruntime/issues/4528#issuecomment-661736788)  \r\n\r\nYet one question. What do mean the int values from the session.GetOutputTypeInfo(layer).GetONNXType()? I mean the link between values and types. In source code is mentioned only\r\n\"Return OnnxType from OrtTypeInfo\"\r\nIs it true, that 1 only used for sequences and 2  for maps only? Or I misunderstood? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4617", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4617/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4617/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4617/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4617", "id": 665457467, "node_id": "MDU6SXNzdWU2NjU0NTc0Njc=", "number": 4617, "title": "Cmake fail to create symbolic link", "user": {"login": "tsaizhenling", "id": 1062331, "node_id": "MDQ6VXNlcjEwNjIzMzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1062331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tsaizhenling", "html_url": "https://github.com/tsaizhenling", "followers_url": "https://api.github.com/users/tsaizhenling/followers", "following_url": "https://api.github.com/users/tsaizhenling/following{/other_user}", "gists_url": "https://api.github.com/users/tsaizhenling/gists{/gist_id}", "starred_url": "https://api.github.com/users/tsaizhenling/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tsaizhenling/subscriptions", "organizations_url": "https://api.github.com/users/tsaizhenling/orgs", "repos_url": "https://api.github.com/users/tsaizhenling/repos", "events_url": "https://api.github.com/users/tsaizhenling/events{/privacy}", "received_events_url": "https://api.github.com/users/tsaizhenling/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2233218084, "node_id": "MDU6TGFiZWwyMjMzMjE4MDg0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/platform:jetson", "name": "platform:jetson", "color": "c0ef7f", "default": false, "description": ""}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "jywu-msft", "id": 43355415, "node_id": "MDQ6VXNlcjQzMzU1NDE1", "avatar_url": "https://avatars0.githubusercontent.com/u/43355415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jywu-msft", "html_url": "https://github.com/jywu-msft", "followers_url": "https://api.github.com/users/jywu-msft/followers", "following_url": "https://api.github.com/users/jywu-msft/following{/other_user}", "gists_url": "https://api.github.com/users/jywu-msft/gists{/gist_id}", "starred_url": "https://api.github.com/users/jywu-msft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jywu-msft/subscriptions", "organizations_url": "https://api.github.com/users/jywu-msft/orgs", "repos_url": "https://api.github.com/users/jywu-msft/repos", "events_url": "https://api.github.com/users/jywu-msft/events{/privacy}", "received_events_url": "https://api.github.com/users/jywu-msft/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jywu-msft", "id": 43355415, "node_id": "MDQ6VXNlcjQzMzU1NDE1", "avatar_url": "https://avatars0.githubusercontent.com/u/43355415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jywu-msft", "html_url": "https://github.com/jywu-msft", "followers_url": "https://api.github.com/users/jywu-msft/followers", "following_url": "https://api.github.com/users/jywu-msft/following{/other_user}", "gists_url": "https://api.github.com/users/jywu-msft/gists{/gist_id}", "starred_url": "https://api.github.com/users/jywu-msft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jywu-msft/subscriptions", "organizations_url": "https://api.github.com/users/jywu-msft/orgs", "repos_url": "https://api.github.com/users/jywu-msft/repos", "events_url": "https://api.github.com/users/jywu-msft/events{/privacy}", "received_events_url": "https://api.github.com/users/jywu-msft/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-24T22:53:09Z", "updated_at": "2020-08-02T22:38:21Z", "closed_at": "2020-08-02T22:38:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "trying to build onnxruntime on the nvidia xavier nx, with jetpack 4.2 on a microsd\r\nfollowing instructions here https://github.com/microsoft/onnxruntime/issues/2684#issuecomment-568548387\r\nI used `compute_72` and `sm_72` instead\r\n\r\nit seems to have mostly completed, but have problems with linking\r\n```\r\n[ 98%] Built target onnxruntime_test_framework_session_without_environment_standalone\r\n[100%] Linking CXX shared library libnvonnxparser.so\r\nCMake Error: failed to create symbolic link 'libnvonnxparser.so.6': function not implemented\r\nCMake Error: cmake_symlink_library: System Error: Function not implemented\r\nCMake Error: failed to create symbolic link 'libnvonnxparser.so': function not implemented\r\nCMake Error: cmake_symlink_library: System Error: Function not implemented\r\nexternal/onnx-tensorrt/CMakeFiles/nvonnxparser.dir/build.make:183: recipe for target 'external/onnx-tensorrt/libnvonnxparser.so.6.0.1' failed\r\nmake[2]: *** [external/onnx-tensorrt/libnvonnxparser.so.6.0.1] Error 1\r\nmake[2]: *** Deleting file 'external/onnx-tensorrt/libnvonnxparser.so.6.0.1'\r\nCMakeFiles/Makefile2:2158: recipe for target 'external/onnx-tensorrt/CMakeFiles/nvonnxparser.dir/all' failed\r\nmake[1]: *** [external/onnx-tensorrt/CMakeFiles/nvonnxparser.dir/all] Error 2\r\nMakefile:182: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\nTraceback (most recent call last):\r\n  File \"/media/aaeon/6463-6563/onnxruntime/tools/ci_build/build.py\", line 1043, in <module>\r\n    sys.exit(main())\r\n  File \"/media/aaeon/6463-6563/onnxruntime/tools/ci_build/build.py\", line 975, in main\r\n    build_targets(cmake_path, build_dir, configs, args.parallel)\r\n  File \"/media/aaeon/6463-6563/onnxruntime/tools/ci_build/build.py\", line 415, in build_targets\r\n    run_subprocess(cmd_args)\r\n  File \"/media/aaeon/6463-6563/onnxruntime/tools/ci_build/build.py\", line 197, in run_subprocess\r\n    completed_process = subprocess.run(args, cwd=cwd, check=True, stdout=stdout, stderr=stderr, env=my_env, shell=shell)\r\n  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\r\n    output=stdout, stderr=stderr)\r\nsubprocess.CalledProcessError: Command '['/usr/local/bin/cmake', '--build', '/media/aaeon/6463-6563/onnxruntime/build/Linux/Release', '--config', 'Release']' returned non-zero exit status 2.\r\n```\r\n\r\nhow do I fix the linking error?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4611", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4611/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4611/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4611/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4611", "id": 665260795, "node_id": "MDU6SXNzdWU2NjUyNjA3OTU=", "number": 4611, "title": "Inference on GPU is not deterministic", "user": {"login": "erikbrntsn", "id": 7210405, "node_id": "MDQ6VXNlcjcyMTA0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/7210405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erikbrntsn", "html_url": "https://github.com/erikbrntsn", "followers_url": "https://api.github.com/users/erikbrntsn/followers", "following_url": "https://api.github.com/users/erikbrntsn/following{/other_user}", "gists_url": "https://api.github.com/users/erikbrntsn/gists{/gist_id}", "starred_url": "https://api.github.com/users/erikbrntsn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erikbrntsn/subscriptions", "organizations_url": "https://api.github.com/users/erikbrntsn/orgs", "repos_url": "https://api.github.com/users/erikbrntsn/repos", "events_url": "https://api.github.com/users/erikbrntsn/events{/privacy}", "received_events_url": "https://api.github.com/users/erikbrntsn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-24T15:53:51Z", "updated_at": "2020-07-24T15:58:19Z", "closed_at": "2020-07-24T15:58:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nIn C#:\r\nTwo instances of the same model running on the GPU does not produce the same result.\r\nThe results are numerically identical when using the CPU however.\r\n\r\n**Urgency**\r\nSome urgency.\r\nWe would like to use ORT to run some ML components in a large computer vision application. Non-determinism makes our development more difficult and therefore ORT less appealing.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Build 18362\r\n- ONNX Runtime installed from (source or binary): NuGet\r\n- ONNX Runtime version: Microsoft.ML.OnnxRuntime.Gpu 1.3, Microsoft.ML.OnnxRuntime.Managed 1.3\r\n- Python version: 3.7.3\r\n- Visual Studio version (if applicable): VS2019 Version 16.4.5\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce 2080 Ti - driver 441.22\r\n\r\n**To Reproduce**\r\nA small model that reproduces the problem can be created using the following code:\r\n```\r\nimport torch\r\nfrom torch import nn\r\n\r\n\r\nclass Model(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        c = 10\r\n        self.relu = torch.nn.ReLU(inplace=True)\r\n        self.conv1 = torch.nn.Conv2d(1, c, kernel_size=3, stride=1, padding=1)\r\n        self.conv2 = torch.nn.Conv2d(c, 1, kernel_size=3, stride=1, padding=1)\r\n        self.batchNorm = torch.nn.BatchNorm2d(c)\r\n\r\n    def forward(self, data):\r\n        return self.conv2(self.batchNorm(self.relu(self.conv1(data))))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    import sys\r\n\r\n    if len(sys.argv) != 2:\r\n        print(\"Usage: <path/to/output>\")\r\n        exit()\r\n\r\n    model = Model()\r\n    model_input = torch.zeros([1, 1, 123, 234])\r\n    input_names = [\"input\"]\r\n\r\n    with open(sys.argv[1], \"wb\") as fh:\r\n        torch.onnx.export(model,\r\n                          model_input,\r\n                          fh,\r\n                          opset_version=11,\r\n                          do_constant_folding=True,\r\n                          input_names=input_names,\r\n                          output_names=[\"output\"],\r\n                          dynamic_axes={name: {2: \"rows\", 3: \"cols\"} for name in input_names})\r\n```\r\nTo reproduce the non-deterministic behavior run the following in C# (eg. in a unit test)\r\n```\r\nvar modelPath = @\"path\\to\\model\";\r\nvar options = SessionOptions.MakeSessionOptionWithCudaProvider();\r\nusing (var model1 = new InferenceSession(modelPath, options))\r\nusing (var model2 = new InferenceSession(modelPath, options))\r\n{\r\n    int rowsIn = 123;\r\n    int colsIn = 234;\r\n    int nChannelsIn = 1;\r\n    var dims = new[] { 1, nChannelsIn, rowsIn, colsIn };\r\n    var rnd = new Random();\r\n    var data = new float[nChannelsIn * colsIn * rowsIn];\r\n    for (int i = 0; i < data.Length; i++)\r\n        data[i] = (float) (rnd.NextDouble() - 0.5);\r\n    var input = new DenseTensor<float>(data, dims);\r\n\r\n    var inputORT = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor(model1.InputMetadata.Keys.First(), input) };\r\n    var output1 = model1.Run(inputORT).First().AsTensor<float>();\r\n    var output2 = model2.Run(inputORT).First().AsTensor<float>();\r\n\r\n    int rowsOut = 123;\r\n    int colsOut = 234;\r\n    int nChannelsOut = 1;\r\n    for (int c = 0; c < nChannelsOut; c++)\r\n    for (int row = 0; row < rowsOut; row++)\r\n    for (int col = 0; col < colsOut; col++)\r\n        Assert.AreEqual(output1[0, c, row, col], output2[0, c, row, col]);\r\n}\r\n```\r\n\r\n**Expected behavior**\r\nThe outputs of the two models are identical\r\n\r\n**Additional context**\r\nAs mentioned, two models created on the CPU produce identical results.\r\nPassing the same input twice to the same model also produces identical results.\r\nNote that you might have to run the test code a few times to reproduce the problem. For me the two results are different roughly every second time I run it.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4609", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4609/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4609/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4609/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4609", "id": 664984282, "node_id": "MDU6SXNzdWU2NjQ5ODQyODI=", "number": 4609, "title": "Cmakelist.txt  in samples", "user": {"login": "cqray1990", "id": 32585434, "node_id": "MDQ6VXNlcjMyNTg1NDM0", "avatar_url": "https://avatars0.githubusercontent.com/u/32585434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cqray1990", "html_url": "https://github.com/cqray1990", "followers_url": "https://api.github.com/users/cqray1990/followers", "following_url": "https://api.github.com/users/cqray1990/following{/other_user}", "gists_url": "https://api.github.com/users/cqray1990/gists{/gist_id}", "starred_url": "https://api.github.com/users/cqray1990/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cqray1990/subscriptions", "organizations_url": "https://api.github.com/users/cqray1990/orgs", "repos_url": "https://api.github.com/users/cqray1990/repos", "events_url": "https://api.github.com/users/cqray1990/events{/privacy}", "received_events_url": "https://api.github.com/users/cqray1990/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-24T07:51:02Z", "updated_at": "2020-07-24T08:38:41Z", "closed_at": "2020-07-24T08:38:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "when i try to run examples in sample dir,and exectue cmake it comes the follow error:\r\n\r\n/c_cxx/fns_candy_style_transfer/fns_candy_style_transfer.c:77:3: error: unknown type name \u2018png_image\u2019\r\n   png_image image; /* The control structure used by libpng */\r\n   ^\r\n/home/lgx/onnxruntime/samples/c_cxx/fns_candy_style_transfer/fns_candy_style_transfer.c:80:8: error: request for member \u2018version\u2019 in something not a structure or union\r\n   image.version = PNG_IMAGE_VERSION;\r\n        ^\r\n/home/lgx/onnxruntime/samples/c_cxx/fns_candy_style_transfer/fns_candy_style_transfer.c:80:19: error: \u2018PNG_IMAGE_VERSION\u2019 undeclared (first use in this function)\r\n   image.version = PNG_IMAGE_VERSION;", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4598", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4598/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4598/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4598/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4598", "id": 664374853, "node_id": "MDU6SXNzdWU2NjQzNzQ4NTM=", "number": 4598, "title": "Incompatibility Openvino-vadm Model  Error", "user": {"login": "Ken-LiuL", "id": 7971987, "node_id": "MDQ6VXNlcjc5NzE5ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7971987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ken-LiuL", "html_url": "https://github.com/Ken-LiuL", "followers_url": "https://api.github.com/users/Ken-LiuL/followers", "following_url": "https://api.github.com/users/Ken-LiuL/following{/other_user}", "gists_url": "https://api.github.com/users/Ken-LiuL/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ken-LiuL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ken-LiuL/subscriptions", "organizations_url": "https://api.github.com/users/Ken-LiuL/orgs", "repos_url": "https://api.github.com/users/Ken-LiuL/repos", "events_url": "https://api.github.com/users/Ken-LiuL/events{/privacy}", "received_events_url": "https://api.github.com/users/Ken-LiuL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1624868154, "node_id": "MDU6TGFiZWwxNjI0ODY4MTU0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:OpenVINO", "name": "ep:OpenVINO", "color": "bfdadc", "default": false, "description": "questions/issues related to OpenVINO EP"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-23T10:44:29Z", "updated_at": "2020-07-29T08:38:28Z", "closed_at": "2020-07-29T08:38:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nWhen I use Openvino-vadm  version onnxruntime to create inference session, there are errors thrown, though the model could run perfectly in standard onnxruntime environment\r\n**Urgency**\r\nNone\r\n\r\n**System information**\r\n- Ubuntu\r\n- ONNX Runtime installed from (source or binary): mcr.microsoft.com/azureml/onnxruntime:latest-openvino-vadm\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\nJust use onnxruntime.InferenceSession with onnxruntime.capi._pybind_state.set_openvino_device(\"VAD-M_FP16\")\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n\r\n[model.onnx.zip](https://github.com/microsoft/onnxruntime/files/4965380/model.onnx.zip)\r\n\r\n**Expected behavior**\r\nI would expect a success instantiation of the inference session.\r\n\r\n**Screenshots**\r\n![image](https://user-images.githubusercontent.com/7971987/88278099-6eca1980-cd14-11ea-8c8f-86ef9383799e.png)\r\n![image](https://user-images.githubusercontent.com/7971987/88278140-7c7f9f00-cd14-11ea-971e-7eb13d7ab6a6.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4589", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4589/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4589/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4589/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4589", "id": 664049540, "node_id": "MDU6SXNzdWU2NjQwNDk1NDA=", "number": 4589, "title": "ONNXRuntime outputs are different from PyTorch outputs", "user": {"login": "dtch1997", "id": 25474937, "node_id": "MDQ6VXNlcjI1NDc0OTM3", "avatar_url": "https://avatars1.githubusercontent.com/u/25474937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dtch1997", "html_url": "https://github.com/dtch1997", "followers_url": "https://api.github.com/users/dtch1997/followers", "following_url": "https://api.github.com/users/dtch1997/following{/other_user}", "gists_url": "https://api.github.com/users/dtch1997/gists{/gist_id}", "starred_url": "https://api.github.com/users/dtch1997/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dtch1997/subscriptions", "organizations_url": "https://api.github.com/users/dtch1997/orgs", "repos_url": "https://api.github.com/users/dtch1997/repos", "events_url": "https://api.github.com/users/dtch1997/events{/privacy}", "received_events_url": "https://api.github.com/users/dtch1997/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-22T21:05:09Z", "updated_at": "2020-07-23T09:22:55Z", "closed_at": "2020-07-23T09:22:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nAfter exporting a PyTorch model to ONNX and performing inference on the same data using `onnxruntime.InferenceSession`, the model output is different from the output of the original PyTorch model. \r\n\r\n**Urgency**\r\nNone\r\n\r\n**System information**\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: 1.4.0\r\n\r\nMinimal example is provided in Colab so I am uncertain of the hardware details. \r\n\r\n**To Reproduce**\r\nPlease refer to this [Colab notebook](https://colab.research.google.com/drive/1cThbqXLHfkdXzeTtD6fxC3YkVZRc9RC-) for a minimal example. Run the cells in sequence. \r\n\r\n**Expected behavior**\r\nONNX Runtime should produce the same outputs as PyTorch to a high degree of precision (around 1e-16 for a float32 model).. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4570", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4570/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4570/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4570/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4570", "id": 663107194, "node_id": "MDU6SXNzdWU2NjMxMDcxOTQ=", "number": 4570, "title": " there is always a \"import error\" in a new win7 system: \"ImportError: cannot import name 'get_all_providers'\"", "user": {"login": "lightfate", "id": 41222646, "node_id": "MDQ6VXNlcjQxMjIyNjQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/41222646?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lightfate", "html_url": "https://github.com/lightfate", "followers_url": "https://api.github.com/users/lightfate/followers", "following_url": "https://api.github.com/users/lightfate/following{/other_user}", "gists_url": "https://api.github.com/users/lightfate/gists{/gist_id}", "starred_url": "https://api.github.com/users/lightfate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lightfate/subscriptions", "organizations_url": "https://api.github.com/users/lightfate/orgs", "repos_url": "https://api.github.com/users/lightfate/repos", "events_url": "https://api.github.com/users/lightfate/events{/privacy}", "received_events_url": "https://api.github.com/users/lightfate/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "snnn", "id": 856316, "node_id": "MDQ6VXNlcjg1NjMxNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/856316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snnn", "html_url": "https://github.com/snnn", "followers_url": "https://api.github.com/users/snnn/followers", "following_url": "https://api.github.com/users/snnn/following{/other_user}", "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snnn/subscriptions", "organizations_url": "https://api.github.com/users/snnn/orgs", "repos_url": "https://api.github.com/users/snnn/repos", "events_url": "https://api.github.com/users/snnn/events{/privacy}", "received_events_url": "https://api.github.com/users/snnn/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "snnn", "id": 856316, "node_id": "MDQ6VXNlcjg1NjMxNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/856316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snnn", "html_url": "https://github.com/snnn", "followers_url": "https://api.github.com/users/snnn/followers", "following_url": "https://api.github.com/users/snnn/following{/other_user}", "gists_url": "https://api.github.com/users/snnn/gists{/gist_id}", "starred_url": "https://api.github.com/users/snnn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snnn/subscriptions", "organizations_url": "https://api.github.com/users/snnn/orgs", "repos_url": "https://api.github.com/users/snnn/repos", "events_url": "https://api.github.com/users/snnn/events{/privacy}", "received_events_url": "https://api.github.com/users/snnn/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2020-07-21T15:29:13Z", "updated_at": "2020-07-23T17:49:58Z", "closed_at": "2020-07-23T17:49:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "webwxgetmsgimg\r\nDescribe the bug\r\nwhen I install onnxruntime in a new  windows 7 sp1 system, there is always a \"import error\" . I have already installed \u201cvs 2019 runtime\u201d \uff08VC_redist.x64.exe\uff09\r\n\r\nPackage           Version\r\n----------------- -------\r\nnumpy             1.19.0\r\nonnx              1.7.0\r\nonnxruntime       1.4.0\r\npip               20.1.1\r\nprotobuf          3.12.2\r\nsetuptools        39.0.1\r\nsix               1.15.0\r\ntyping-extensions 3.7.4.2\r\nwheel             0.34.2\r\n\r\nC:\\Users\\zhjs>python\r\nPython 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD6\r\n4)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import onnxruntime\r\nC:\\Users\\zhjs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\onnxrunti\r\nme\\capi\\_pybind_state.py:14: UserWarning: Cannot load onnxruntime.capi. Error: '\r\nDLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u7a0b\u5e8f\u3002'.\r\n  warnings.warn(\"Cannot load onnxruntime.capi. Error: '{0}'.\".format(str(e)))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\zhjs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\o\r\nnnxruntime\\__init__.py\", line 13, in <module>\r\n    from onnxruntime.capi._pybind_state import get_all_providers, get_available_\r\nproviders, get_device, set_seed, \\\r\nImportError: cannot import name 'get_all_providers'\r\n>>>\r\n\r\nUrgency\r\nHigh, our project is based on onnxruntime, and now it cannot run in a new win7 system\uff0cand we already installed \u201cVC_redist.x64.exe\u201d\uff0cwe don\u2018t know what's missing\uff1f\r\n\r\nSystem information\r\n\r\nWin7:\r\npip insatll onnxruntime:\r\nonnxruntime 1.4.0/1.3.0:\r\npython 3.6.6\r\nuse cpu:\r\nTo Reproduce\r\n it cannot run in a new win7 system\uff0cand we already installed \u201cVC_redist.x64.exe\u201d\uff0cwe don\u2018t know what's missing\uff1f\r\n\r\nExpected behavior\r\nno import error\r\n\r\nScreenshots\r\n\r\n![error](https://user-images.githubusercontent.com/41222646/88074312-d1eb6d00-cba9-11ea-9d8b-c2d1b935e69b.JPG)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4568", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4568/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4568/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4568/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4568", "id": 662952023, "node_id": "MDU6SXNzdWU2NjI5NTIwMjM=", "number": 4568, "title": "Access to custom metadata of an .onnx model when creating an InferenceSession in a C# WPF application.", "user": {"login": "DanielMemmel", "id": 68106736, "node_id": "MDQ6VXNlcjY4MTA2NzM2", "avatar_url": "https://avatars0.githubusercontent.com/u/68106736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DanielMemmel", "html_url": "https://github.com/DanielMemmel", "followers_url": "https://api.github.com/users/DanielMemmel/followers", "following_url": "https://api.github.com/users/DanielMemmel/following{/other_user}", "gists_url": "https://api.github.com/users/DanielMemmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/DanielMemmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DanielMemmel/subscriptions", "organizations_url": "https://api.github.com/users/DanielMemmel/orgs", "repos_url": "https://api.github.com/users/DanielMemmel/repos", "events_url": "https://api.github.com/users/DanielMemmel/events{/privacy}", "received_events_url": "https://api.github.com/users/DanielMemmel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493981, "node_id": "MDU6TGFiZWwxMTIyNDkzOTgx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:enhancement", "name": "type:enhancement", "color": "a2eeef", "default": false, "description": "request for unsupported feature or enhancement"}], "state": "closed", "locked": false, "assignee": {"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2020-07-21T12:16:18Z", "updated_at": "2020-08-25T18:13:50Z", "closed_at": "2020-08-25T18:13:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Is your feature request related to a problem? Please describe.**\r\nNo.\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): Microsoft.ML.OnnxRuntime Nuget Package v1.4.0\r\n- .Net Framework 4.7.2\r\n\r\n**Describe the solution you'd like**\r\nI want to access the custom metadata of my .onnx file in the C# application. It is important for my application to know the model type, which version, numerical range of the input data, etc.\r\n\r\n**Describe alternatives you've considered**\r\nIf the custom metadata is not accessible from the model file I will be forced to create a separate file that contains the needed information. This would be very annoying to always load two files and harder to keep track of.\r\n\r\n**Additional context**\r\nI am working with image segmentation and the user will be able to choose with model to use for his problem. So I need to provide additional info about the different models. \r\nFrom the `InputMetadata` of the `InferenceSession` I can get the image size and dtype that is needed but the `ModelMetadata `property is always empty. \r\n\r\nI populate the .onnx model with custom metadata in python with the onnxmltools (version 1.6.1) library:\r\n\r\n```\r\nimport onnxmltools\r\n\r\nmodel = onnxmltools.load_model(\"../model.onnx\")\r\nmeta = model.metadata_props.add()\r\nmeta.key = \"version\"\r\nmeta.value = \"0.0.1\"\r\nonnxmltools.utils.save_model(model, \"../model_1.onnx\")\r\n\r\n```\r\nAfter saving the model again I can see the custom properties e.g. in [Netron](https://lutzroeder.github.io/netron/) but the C# api does not show them to me. \r\nIs there a way to have the custom data I put into the .onnx model read when creating an InferenceSession?\r\n\r\nThanks in advance!\r\n\r\nDaniel", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4565", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4565/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4565/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4565/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4565", "id": 662704842, "node_id": "MDU6SXNzdWU2NjI3MDQ4NDI=", "number": 4565, "title": "Add `optimizer_cli` support for `gpt2-medium`", "user": {"login": "evgerher", "id": 15091718, "node_id": "MDQ6VXNlcjE1MDkxNzE4", "avatar_url": "https://avatars2.githubusercontent.com/u/15091718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/evgerher", "html_url": "https://github.com/evgerher", "followers_url": "https://api.github.com/users/evgerher/followers", "following_url": "https://api.github.com/users/evgerher/following{/other_user}", "gists_url": "https://api.github.com/users/evgerher/gists{/gist_id}", "starred_url": "https://api.github.com/users/evgerher/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/evgerher/subscriptions", "organizations_url": "https://api.github.com/users/evgerher/orgs", "repos_url": "https://api.github.com/users/evgerher/repos", "events_url": "https://api.github.com/users/evgerher/events{/privacy}", "received_events_url": "https://api.github.com/users/evgerher/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-21T07:44:38Z", "updated_at": "2020-07-21T11:47:33Z", "closed_at": "2020-07-21T11:47:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n`onnxruntime-tools` optimization does not support transformation of models where `hidden_size % num_heads != 0`.  \r\nFor example, `gpt2-medium` has hidden_size=1024 and heads=24. After applying `optimizer_cli` on a model, it fails during runtime due to assertion violation: `onnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Attention node. Name: 'Attention_1' Status Message: Input 0 dimension 2 should be divisable by value of the num_heads attribute.`  \r\n\r\nAn error appears after optimization with _attention layers fusion_ applied (--opt_level=99 or --use_gpu);  \r\nFor the testing purposes I do not utilize past, only `input_ids`.  \r\n\r\n**Urgency** \r\nNone  \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 / Ubuntu 16.04\r\n- ONNX Runtime installed from (source or binary): binary (via pip)\r\n- ONNX Runtime version: 1.4.0\r\n- ONNX-tools version: 1.4.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: None / CUDA=10.1\r\n- GPU model and memory: None / V100, 16GB\r\n\r\nI reproduced this problem on both win 10 (non-gpu) and ubuntu 16.04 environments.\r\n\r\n**To Reproduce**\r\n\r\n### Convert to .onnx\r\n```python=\r\nfrom transformers.convert_graph_to_onnx import convert\r\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\r\nimport psutil\r\nimport os\r\nimport torch\r\n\r\nuse_openmp = False\r\n\r\nif use_openmp:\r\n\tos.environ['OMP_NUM_THREADS'] = str(psutil.cpu_count(logical=True))\r\nelse:\r\n\tos.environ['OMP_NUM_THREADS'] = '1'\r\n\r\nos.environ['OMP_WAIT_POLICY'] = 'ACTIVE'\r\n\r\nimport onnx\r\nimport onnxruntime\r\n\r\ndef convert(model: GPT2LMHeadModel, path:str):\r\n\twith torch.no_grad():\r\n\t\tmodel.to('cpu')\r\n\t\tdummy_input = torch.randint(0, 50000, (1,64), device='cpu').long()\r\n\t\tinputs = {\r\n\t\t\t'input_ids': dummy_input\r\n\t\t}\r\n\r\n\t\tnum_layer = model.config.n_layer\r\n\t\tpresent_names = [f'present_{i}' for i in range(num_layer)]\r\n\t\toutput_names = ['last_state'] + present_names\r\n\t\tdynamic_axes = {\r\n\t\t\t\t'input_ids': {0: 'batch', 1: 'sequence'},\r\n\t\t\t\t'last_state': {0: 'batch', 1: 'sequence'}\r\n\t\t}\r\n\r\n\t\ttorch.onnx.export(model,\r\n\t\t\t\targs=tuple(dummy_input,),\r\n\t\t\t\tf=path,\r\n\t\t\t\topset_version=12,\r\n\t\t\t\tdo_constant_folding=True,\r\n\t\t\t\tinput_names=['input_ids'],\r\n\t\t\t\toutput_names=output_names,\r\n\t\t\t\tdynamic_axes=dynamic_axes\r\n\t\t\t)\r\n\t\tprint(f'Model exported at : {path}')\r\n\r\npath = './model/gpt2.onnx'\r\nmodel_config = GPT2Config.from_pretrained('gpt2-medium')\r\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\r\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium', config=model_config)\r\n\r\n# model_config.save_pretrained('./save/')\r\n# tokenizer.save_pretrained('./save/')\r\n# model.save_pretrained('./save/')\r\n\r\nconvert(model, path)\r\n```\r\n### Optimize\r\n\r\n`python -m onnxruntime_tools.optimizer_cli --input gpt2.onnx --output gpt2-fp32.onnx --num_heads 24 --hidden_size 1024 --model_type gpt2`\r\n```optimize_by_onnxruntime: Save optimized model by onnxruntime to gpt2_o1_cpu.onnx\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\evger\\miniconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\evger\\miniconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\evger\\miniconda3\\lib\\site-packages\\onnxruntime_tools\\optimizer_cli.py\", line 4, in <module>\r\n    _main()\r\n  File \"C:\\Users\\evger\\miniconda3\\lib\\site-packages\\onnxruntime_tools\\transformers\\optimizer.py\", line 309, in main\r\n    only_onnxruntime=args.only_onnxruntime)\r\n  File \"C:\\Users\\evger\\miniconda3\\lib\\site-packages\\onnxruntime_tools\\transformers\\optimizer.py\", line 280, in optimize_model\r\n    optimizer = optimizer_class(model, num_heads, hidden_size)\r\n  File \"C:\\Users\\evger\\miniconda3\\lib\\site-packages\\onnxruntime_tools\\transformers\\onnx_model_bert.py\", line 41, in __init__\r\n    assert hidden_size % num_heads == 0\r\nAssertionError```\r\n\r\n\r\n**Expected behavior**\r\n`onnxruntime-tools` supports optimization for models with `hidden_size % num_heads != 0`  with fused layers (attentions, etc.)  \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4563", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4563/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4563/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4563/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4563", "id": 662565768, "node_id": "MDU6SXNzdWU2NjI1NjU3Njg=", "number": 4563, "title": "Unknown model file format version with example dataset in Python", "user": {"login": "ankane", "id": 220358, "node_id": "MDQ6VXNlcjIyMDM1OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/220358?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankane", "html_url": "https://github.com/ankane", "followers_url": "https://api.github.com/users/ankane/followers", "following_url": "https://api.github.com/users/ankane/following{/other_user}", "gists_url": "https://api.github.com/users/ankane/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankane/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankane/subscriptions", "organizations_url": "https://api.github.com/users/ankane/orgs", "repos_url": "https://api.github.com/users/ankane/repos", "events_url": "https://api.github.com/users/ankane/events{/privacy}", "received_events_url": "https://api.github.com/users/ankane/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-21T04:23:55Z", "updated_at": "2020-07-22T07:37:42Z", "closed_at": "2020-07-22T06:51:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\nHi, this is a small one, but the `mul_1.onnx` example dataset in Python looks be broken or outdated. It fails to load with:\r\n\r\n```txt\r\nonnxruntime.capi.onnxruntime_pybind11_state.InvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Failed to load model with error: Unknown model file format version.\r\n```\r\n\r\n**Urgency**\r\n\r\nNone\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.15\r\n- ONNX Runtime installed from (source or binary): binary\r\n- ONNX Runtime version: 1.4.0\r\n- Python version: 3.8.4\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\n\r\n```python\r\nimport onnxruntime as rt\r\nfrom onnxruntime import datasets\r\n\r\npath = datasets.get_example('mul_1.onnx')\r\nprint(path)\r\n\r\nsess = rt.InferenceSession(path)\r\nprint(sess.get_inputs()[0].name)\r\n```\r\n\r\n**Expected behavior**\r\n\r\nModel loads", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4559", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4559/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4559/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4559/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4559", "id": 662287891, "node_id": "MDU6SXNzdWU2NjIyODc4OTE=", "number": 4559, "title": "Segmentation fault with zero-dim tensor", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-07-20T21:34:54Z", "updated_at": "2020-07-22T00:57:48Z", "closed_at": "2020-07-22T00:57:48Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Describe the bug**\r\nThis is very much related to issue #1879 which was fixed for the ops we used before.\r\n\r\n**Urgency**\r\nFound a workaround to avoid the faulty ops (code is attached later)\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- ONNX Runtime installed from (source or binary): pip\r\n- ONNX Runtime version: pip 1.4.0 (same crash on 1.2.0)\r\n- Python version: 3.6.9\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n\r\n```python\r\nimport onnxruntime as rt\r\nimport torch\r\nfrom torch import nn\r\n\r\nclass CrashRepro(nn.Module):\r\n    def __init__(self):\r\n        super(CrashRepro, self).__init__()\r\n        self.img_embedding = nn.Linear(1030, 768, bias=True)\r\n        \r\n    def forward(self, img_feats):    \r\n        return self.img_embedding(img_feats)\r\n\r\nmodel = CrashRepro()\r\nimg_feats = torch.ones(1, 19, 1030)\r\n\r\ndynamic_axes = {\r\n    'img_feats': {1: 'box_spatial'},\r\n}\r\nonnxfile = \"/mnt/output/gr/cap/crash.onnx\"\r\ntargets = [\"result\"]\r\ntorch.onnx.export(model, img_feats, onnxfile,\r\n                  verbose=True,\r\n                  input_names=[\r\n                      'img_feats'\r\n                  ],\r\n                  dynamic_axes=dynamic_axes,\r\n                  output_names=targets,\r\n                  opset_version=11)\r\n\r\n# works\r\nsess = rt.InferenceSession(onnxfile)\r\nresults = sess.run(targets, {\r\n    'img_feats': img_feats.numpy()\r\n})\r\n\r\n# crashes\r\nimg_feats = torch.ones(1, 0, 1030)\r\nresults = sess.run(targets, {\r\n    'img_feats': img_feats.numpy()\r\n})\r\n\r\n```\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\n\r\n![image](https://user-images.githubusercontent.com/873905/87988412-205b1780-ca95-11ea-9571-a105cbde32d8.png)\r\n\r\n\r\n**Expected behavior**\r\nNo segmentation fault\r\n\r\n**Screenshots**\r\nMatMulis causing crash:\r\n![image](https://user-images.githubusercontent.com/873905/87988412-205b1780-ca95-11ea-9571-a105cbde32d8.png)\r\n\r\n**Additional context**\r\nThis is the workaround:\r\n\r\n```python\r\nclass CrashRepro(nn.Module):\r\n    def __init__(self):\r\n        super(CrashRepro, self).__init__()\r\n        self.img_embedding = nn.Linear(1030, 768, bias=True)\r\n        \r\n    def forward(self, img_feats):    \r\n        return self.img_embedding(img_feats.squeeze(0)).unsqueeze(0)\r\n```\r\n\r\nwhich exports a different ops (not sure why!) but this ops does not crash with empty tensor.\r\n\r\n![image](https://user-images.githubusercontent.com/873905/87988588-716b0b80-ca95-11ea-96dd-1b413b2f02d1.png)\r\n\r\nChanging the size exporting a different model is another issue but I guess I have to open that in PyTorch ONNX Export being flimsy.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4554", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4554/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4554/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4554/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4554", "id": 661007444, "node_id": "MDU6SXNzdWU2NjEwMDc0NDQ=", "number": 4554, "title": "Unable to build Onnxruntime 1.4.0 with OpenVINO 2020.3 on Windows 10", "user": {"login": "danielecazzari", "id": 37703014, "node_id": "MDQ6VXNlcjM3NzAzMDE0", "avatar_url": "https://avatars0.githubusercontent.com/u/37703014?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielecazzari", "html_url": "https://github.com/danielecazzari", "followers_url": "https://api.github.com/users/danielecazzari/followers", "following_url": "https://api.github.com/users/danielecazzari/following{/other_user}", "gists_url": "https://api.github.com/users/danielecazzari/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielecazzari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielecazzari/subscriptions", "organizations_url": "https://api.github.com/users/danielecazzari/orgs", "repos_url": "https://api.github.com/users/danielecazzari/repos", "events_url": "https://api.github.com/users/danielecazzari/events{/privacy}", "received_events_url": "https://api.github.com/users/danielecazzari/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 1624868154, "node_id": "MDU6TGFiZWwxNjI0ODY4MTU0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:OpenVINO", "name": "ep:OpenVINO", "color": "bfdadc", "default": false, "description": "questions/issues related to OpenVINO EP"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-07-19T19:02:28Z", "updated_at": "2020-07-28T00:08:23Z", "closed_at": "2020-07-28T00:08:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI'm trying to build  latest release of Onnxruntime 1.4.0 with OpenVINO EP (2020.3) with the following parameters:\r\n--config Release --build_shared_lib --parallel --cmake_generator \"Visual Studio 16 2019\" --enable_pybind --build_wheel --use_openvino CPU_FP32 --skip_submodule_sync --skip_tests\r\n\r\nDuring compilation I receive this warning treated as error and compilation end:\r\n```\r\nD:\\onnx\\onnxruntime-1.4.0\\onnxruntime\\core\\providers\\openvino\\backends\\vadm_backend.cc(67,25): error C2220: the followi\r\nng warning is treated as an error [D:\\onnx\\onnxruntime-1.4.0\\build\\Windows\\Release\\onnxruntime_providers_openvino.vcxpr\r\noj]\r\nD:\\onnx\\onnxruntime-1.4.0\\onnxruntime\\core\\providers\\openvino\\backends\\vadm_backend.cc(67,25): warning C4456: declarati\r\non of 'i' hides previous local declaration [D:\\onnx\\onnxruntime-1.4.0\\build\\Windows\\Release\\onnxruntime_providers_openv\r\nino.vcxproj]\r\n```\r\n\r\nCould you help me understand what I'm doing wrong?\r\n\r\nRegards,\r\n\r\nDaniele\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4553", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4553/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4553/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4553/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4553", "id": 660690209, "node_id": "MDU6SXNzdWU2NjA2OTAyMDk=", "number": 4553, "title": "NNAPI build doesn't provide libonnxruntime library", "user": {"login": "Hramchenko", "id": 1009779, "node_id": "MDQ6VXNlcjEwMDk3Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1009779?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hramchenko", "html_url": "https://github.com/Hramchenko", "followers_url": "https://api.github.com/users/Hramchenko/followers", "following_url": "https://api.github.com/users/Hramchenko/following{/other_user}", "gists_url": "https://api.github.com/users/Hramchenko/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hramchenko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hramchenko/subscriptions", "organizations_url": "https://api.github.com/users/Hramchenko/orgs", "repos_url": "https://api.github.com/users/Hramchenko/repos", "events_url": "https://api.github.com/users/Hramchenko/events{/privacy}", "received_events_url": "https://api.github.com/users/Hramchenko/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-19T08:25:30Z", "updated_at": "2020-07-21T03:58:30Z", "closed_at": "2020-07-21T03:58:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello. \r\nAfter building the library according to the instructions, I got a set of.a files in the onnxruntime/build/Linux/Debug/ folder(the list is given below).\r\n`\r\nonnx/libonnx.a\r\nonnx/libonnx_proto.a\r\nlibonnxruntime_common.a\r\nlibonnxruntime_framework.a\r\nlibonnxruntime_graph.a\r\nlibonnxruntime_mlas.a\r\nlibonnxruntime_optimizer.a\r\nlibonnxruntime_providers.a\r\nlibonnxruntime_providers_nnapi.a\r\nlibonnxruntime_session.a\r\nlibonnxruntime_test_utils.a\r\nlibonnxruntime_util.a\r\nlibonnx_test_data_proto.a\r\nlibonnx_test_runner_common.a\r\n`\r\nThe C++ examples shows that developers need to use onnxruntime library, but I don't have it. Please tell me what I should specify in CMakeLists.txt to build my application?\r\n\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- OS Linux Ubuntu 20.04\r\n- ONNX Runtime installed from source\r\n- ONNX Runtime version from GIT\r\n- GCC version 9.3.0\r\n\r\n**To Reproduce**\r\n- Build onnxruntime with command\r\n`./build.sh --android --android_sdk_path <android sdk path> --android_ndk_path <android ndk path> --use_dnnlibrary`\r\n\r\n**Expected behavior**\r\nBuild folder contains an onnxruntime library or library provide CMakeLists.txt example.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4540", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4540/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4540/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4540/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4540", "id": 659088885, "node_id": "MDU6SXNzdWU2NTkwODg4ODU=", "number": 4540, "title": "Impossible to Compile an source code to run onnx with Arm Compute Library", "user": {"login": "kuroro20", "id": 418105, "node_id": "MDQ6VXNlcjQxODEwNQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/418105?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kuroro20", "html_url": "https://github.com/kuroro20", "followers_url": "https://api.github.com/users/kuroro20/followers", "following_url": "https://api.github.com/users/kuroro20/following{/other_user}", "gists_url": "https://api.github.com/users/kuroro20/gists{/gist_id}", "starred_url": "https://api.github.com/users/kuroro20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kuroro20/subscriptions", "organizations_url": "https://api.github.com/users/kuroro20/orgs", "repos_url": "https://api.github.com/users/kuroro20/repos", "events_url": "https://api.github.com/users/kuroro20/events{/privacy}", "received_events_url": "https://api.github.com/users/kuroro20/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-17T09:51:29Z", "updated_at": "2020-08-01T05:58:42Z", "closed_at": "2020-08-01T05:57:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "OS: Ubuntu 18.04\r\nTarget : Cortex A-53\r\nCompiler : gcc version 7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04)\r\nACL Version 19.08\r\nOnnxruntime: master branch\r\n- ONNX Runtime installed from source:\r\n\r\nI'm trying to use onnxruntime on ARM.\r\nGiven the sample model-explorer.cpp , i ran sucessfully my model on ARM without ACL.\r\n\r\nThen I built Onnx with Arm Compute Library.\r\n\r\nFollowing this README : https://github.com/microsoft/onnxruntime/blob/master/docs/execution_providers/ACL-ExecutionProvider.md\r\n\r\nI tried to run my model with InferenceSession but when i try to include \"inference_session.h\" I have an error from gcc (attached file) and i don't know if i'm not doing the right includes or if i missed something during the build ?\r\n\r\nIt looks like a bad include but it looks like the right include file to me.\r\n\r\n[log.txt](https://github.com/microsoft/onnxruntime/files/4936981/log.txt)\r\n\r\nThe error starts with this :\r\n`In file included from ../../cmake/external/onnx/onnx/defs/shape_inference.h:4:0,\r\n                 from ../../cmake/external/onnx/onnx/defs/schema.h:24,\r\n                 from ../../include/onnxruntime/core/graph/onnx_protobuf.h:36,\r\n                 from ../../include/onnxruntime/core/framework/data_types.h:16,\r\n                 from ../../include/onnxruntime/core/framework/tensor.h:16,\r\n                 from ../../include/onnxruntime/core/framework/execution_provider.h:11,\r\n                 from ../../onnxruntime/core/framework/execution_providers.h:12,\r\n                 from ../../onnxruntime/core/session/inference_session.h:13,\r\n                 from model-explorer.cpp:38:\r\n../../cmake/external/onnx/onnx/proto_utils.h: In function \u2018std::vector<T> ONNX_NAMESPACE::RetrieveValues(const ONNX_NAMESPACE::AttributeProto&) [with T = long int]\u2019:\r\n../../cmake/external/onnx/onnx/proto_utils.h:40:11: error: invalid use of incomplete type \u2018const class ONNX_NAMESPACE::AttributeProto\u2019\r\n   return {attr.ints().begin(), attr.ints().end()};\r\n           ^~~~\r\nIn file included from ../../include/onnxruntime/core/framework/fence.h:7:0,\r\n                 from ../../include/onnxruntime/core/framework/allocator.h:15,\r\n                 from ../../include/onnxruntime/core/framework/tensor.h:13,\r\n                 from ../../include/onnxruntime/core/framework/execution_provider.h:11,\r\n                 from ../../onnxruntime/core/framework/execution_providers.h:12,\r\n                 from ../../onnxruntime/core/session/inference_session.h:13,\r\n                 from model-explorer.cpp:38:\r\n../../include/onnxruntime/core/graph/basic_types.h:14:7: note: forward declaration of \u2018class ONNX_NAMESPACE::AttributeProto\u2019\r\n class AttributeProto;`\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4539", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4539/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4539/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4539/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4539", "id": 659026791, "node_id": "MDU6SXNzdWU2NTkwMjY3OTE=", "number": 4539, "title": "In function __cxx_global_var_init undefined reference to `OrtGetApiBase'", "user": {"login": "AliceSchaw", "id": 29159540, "node_id": "MDQ6VXNlcjI5MTU5NTQw", "avatar_url": "https://avatars1.githubusercontent.com/u/29159540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AliceSchaw", "html_url": "https://github.com/AliceSchaw", "followers_url": "https://api.github.com/users/AliceSchaw/followers", "following_url": "https://api.github.com/users/AliceSchaw/following{/other_user}", "gists_url": "https://api.github.com/users/AliceSchaw/gists{/gist_id}", "starred_url": "https://api.github.com/users/AliceSchaw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AliceSchaw/subscriptions", "organizations_url": "https://api.github.com/users/AliceSchaw/orgs", "repos_url": "https://api.github.com/users/AliceSchaw/repos", "events_url": "https://api.github.com/users/AliceSchaw/events{/privacy}", "received_events_url": "https://api.github.com/users/AliceSchaw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-17T08:43:11Z", "updated_at": "2020-07-19T22:40:49Z", "closed_at": "2020-07-19T22:40:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\nI built the static libraries for android, all of things were fine, but failed to link.  In function `__cxx_global_var_init':\r\n/Downloads/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:53: undefined reference to `OrtGetApiBase'\r\n\r\nnm -g --defined-only libonnxruntime_session.a  \uff08the function exists\uff09\r\n0000000000000000 T OrtGetApiBase\r\n\r\ncmake build command\r\n`cmake -GNinja -Donnxruntime_RUN_ONNX_TESTS=OFF -Donnxruntime_BUILD_WINML_TESTS=OFF -Donnxruntime_GENERATE_TEST_REPORTS=OFF -Donnxruntime_DEV_MODE=OFF -DPYTHON_EXECUTABLE=/usr/local/opt/python/bin/python3.7 -Donnxruntime_USE_CUDA=OFF  -Donnxruntime_USE_FEATURIZERS=OFF -Donnxruntime_USE_JEMALLOC=OFF -Donnxruntime_USE_MIMALLOC_STL_ALLOCATOR=OFF -Donnxruntime_USE_MIMALLOC_ARENA_ALLOCATOR=OFF -Donnxruntime_ENABLE_PYTHON=OFF -Donnxruntime_BUILD_CSHARP=OFF -Donnxruntime_BUILD_JAVA=OFF -Donnxruntime_BUILD_NODEJS=OFF -Donnxruntime_BUILD_SHARED_LIB=OFF -Donnxruntime_USE_EIGEN_FOR_BLAS=ON -Donnxruntime_USE_OPENBLAS=OFF -Donnxruntime_USE_DNNL=OFF -Donnxruntime_USE_MKLML=OFF -Donnxruntime_USE_NGRAPH=OFF -Donnxruntime_USE_OPENVINO=OFF -Donnxruntime_USE_OPENVINO_MYRIAD=OFF -Donnxruntime_USE_OPENVINO_GPU_FP32=OFF -Donnxruntime_USE_OPENVINO_GPU_FP16=OFF -Donnxruntime_USE_OPENVINO_CPU_FP32=OFF -Donnxruntime_USE_OPENVINO_VAD_M=OFF -Donnxruntime_USE_OPENVINO_VAD_F=OFF -Donnxruntime_USE_OPENVINO_BINARY=OFF -Donnxruntime_USE_NNAPI_DNNLIBRARY=ON -Donnxruntime_USE_NNAPI_BUILTIN=OFF -Donnxruntime_USE_RKNPU=OFF -Donnxruntime_USE_OPENMP=OFF -Donnxruntime_USE_TVM=OFF -Donnxruntime_USE_LLVM=OFF -Donnxruntime_ENABLE_MICROSOFT_INTERNAL=OFF -Donnxruntime_USE_VITISAI=OFF -Donnxruntime_USE_NUPHAR=OFF -Donnxruntime_USE_TENSORRT=OFF -Donnxruntime_TENSORRT_HOME= -Donnxruntime_USE_MIGRAPHX=OFF -Donnxruntime_MIGRAPHX_HOME= -Donnxruntime_CROSS_COMPILING=OFF -Donnxruntime_DISABLE_CONTRIB_OPS=OFF -Donnxruntime_DISABLE_ML_OPS=OFF -Donnxruntime_DISABLE_RTTI=OFF -Donnxruntime_MSVC_STATIC_RUNTIME=OFF -Donnxruntime_ENABLE_LANGUAGE_INTEROP_OPS=OFF -Donnxruntime_USE_DML=OFF -Donnxruntime_USE_WINML=OFF -Donnxruntime_BUILD_FOR_WINDOWS_STORE=OFF -Donnxruntime_USE_TELEMETRY=OFF -Donnxruntime_ENABLE_LTO=OFF -Donnxruntime_USE_ACL=OFF -Donnxruntime_USE_ACL_1902=OFF -Donnxruntime_USE_ACL_1905=OFF -Donnxruntime_USE_ACL_1908=OFF -Donnxruntime_USE_ARMNN=OFF -Donnxruntime_ARMNN_RELU_USE_CPU=ON -Donnxruntime_ENABLE_NVTX_PROFILE=OFF -Donnxruntime_ENABLE_TRAINING=OFF -Donnxruntime_USE_HOROVOD=OFF -Donnxruntime_BUILD_BENCHMARKS=OFF -DCMAKE_TOOLCHAIN_FILE=android-sdk/ndk/21.3.6528147/build/cmake/android.toolchain.cmake -DANDROID_PLATFORM=android-23 -DANDROID_ABI=arm64-v8a -DONNX_CUSTOM_PROTOC_EXECUTABLE=onnxruntime/build/Linux/host_protoc/protoc -Donnxruntime_PYBIND_EXPORT_OPSCHEMA=OFF -Donnxruntime_ENABLE_MEMLEAK_CHECKER=ON -DCMAKE_BUILD_TYPE=Release -DCMAKE_ANDROID_STL_TYPE=c++_shared ..`\r\n\r\n```\r\nBuild command failed.\r\nError while executing process  /mytools/android-sdk/cmake/3.10.2.4988404/bin/ninja with arguments {-C  /app/.cxx/cmake/release/arm64-v8a assistence}\r\nninja: Entering directory ` /app/.cxx/cmake/release/arm64-v8a'\r\n[1/1] Linking CXX shared library  /app/build/intermediates/cmake/release/obj/arm64-v8a/libassistence.so\r\nFAILED:  /app/build/intermediates/cmake/release/obj/arm64-v8a/libassistence.so \r\n: &&  /mytools/android-sdk/ndk/21.3.6528147/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++ --target=aarch64-none-linux-android24 --gcc-toolchain= /mytools/android-sdk/ndk/21.3.6528147/toolchains/llvm/prebuilt/darwin-x86_64 --sysroot= /mytools/android-sdk/ndk/21.3.6528147/toolchains/llvm/prebuilt/darwin-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -std=c++14 -O2 -DNDEBUG  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libgcc_real.a -Wl,--exclude-libs,libatomic.a -Wl,--build-id -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -shared -Wl,-soname,libassistence.so -o  /app/build/intermediates/cmake/release/obj/arm64-v8a/libassistence.so CMakeFiles/assistence.dir/helper/JNIHelp.cpp.o CMakeFiles/assistence.dir/helper/JniConstants.cpp.o CMakeFiles/assistence.dir/assistence.cpp.o   /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_calib3d.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_core.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_dnn.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_features2d.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_flann.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_gapi.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_highgui.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_imgcodecs.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_imgproc.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_ml.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_objdetect.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_photo.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_stitching.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_video.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_videoio.a  /Downloads/onnxruntime/cmake/mybuild/libonnxruntime_session.a  /app/src/main/cpp/../../../../dynamiclibs/arm64-v8a/libMNN.so libinference.a -landroid -ljnigraphics  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/liblibprotobuf.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/libade.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_imgcodecs.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/liblibjpeg-turbo.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/liblibwebp.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/liblibpng.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/liblibtiff.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/liblibjasper.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/libIlmImf.a -landroid -llog -lmediandk  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/libquirc.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_calib3d.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_features2d.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_flann.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_imgproc.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/staticlibs/arm64-v8a/libopencv_core.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/libtbb.a -lc -lz  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/libcpufeatures.a  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/libittnotify.a -ldl -lm -llog  /mytools/OpenCV-android-sdk4.3.0/sdk/native/3rdparty/libs/arm64-v8a/libtegra_hal.a -latomic -lm && :\r\nlibinference.a(OnnxInference.cpp.o): In function `__cxx_global_var_init':\r\n /Downloads/onnxruntime/include/onnxruntime/core/session/onnxruntime_cxx_api.h:53: undefined reference to `OrtGetApiBase'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nninja: build stopped: subcommand failed.\r\n```\r\n\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (MAC build for android):\r\n- ONNX Runtime installed from source:\r\n- ONNX Runtime version: 1.3.1\r\n- Python version: python3.7\r\n- Compiler version  android clang\r\n\r\n\r\n**To Reproduce**\r\nfailed to link. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4528", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4528/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4528/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4528/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4528", "id": 657879974, "node_id": "MDU6SXNzdWU2NTc4Nzk5NzQ=", "number": 4528, "title": "Tensor Creation from data", "user": {"login": "TratsiakY", "id": 56832989, "node_id": "MDQ6VXNlcjU2ODMyOTg5", "avatar_url": "https://avatars0.githubusercontent.com/u/56832989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TratsiakY", "html_url": "https://github.com/TratsiakY", "followers_url": "https://api.github.com/users/TratsiakY/followers", "following_url": "https://api.github.com/users/TratsiakY/following{/other_user}", "gists_url": "https://api.github.com/users/TratsiakY/gists{/gist_id}", "starred_url": "https://api.github.com/users/TratsiakY/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TratsiakY/subscriptions", "organizations_url": "https://api.github.com/users/TratsiakY/orgs", "repos_url": "https://api.github.com/users/TratsiakY/repos", "events_url": "https://api.github.com/users/TratsiakY/events{/privacy}", "received_events_url": "https://api.github.com/users/TratsiakY/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1395147764, "node_id": "MDU6TGFiZWwxMzk1MTQ3NzY0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C/C++", "name": "api:C/C++", "color": "0e8a16", "default": false, "description": "related to the  public API(and ABI) for onnxruntime"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2020-07-16T06:11:01Z", "updated_at": "2020-07-25T16:05:41Z", "closed_at": "2020-07-25T16:05:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi.\r\nI've exported my classificator to .onnx and have checked it in python with onnxruntime. All works perfect. \r\nSo, I am trying to run my model in c++ now but, unfortunatelly, I haven't got an enought experience in c++. I've studied examples and try to write my own code, but the moment with transform of vector to tensor is not clear for me. \r\nin the case of vector {m}, where m is a number of elements the code below pass the compilation\r\n\r\n\r\n> \t\tvector<int64_t> input_node_dims = { 0, 2 };\r\n> \t\tvector<vector<float>> init_data = { { 1.5, 30.0 } }; //toy datafor test\r\n> \t\tauto memory_info = MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\r\n> \t\tValue input_tensor = Value::CreateTensor<float>(memory_info, init_data.data(), init_data.size(), input_node_dims.data(), 2);\r\n> \t\tassert(input_tensor.IsTensor());\r\n\r\n\r\n(meanwhile, I am not sure that used dims for init_data is correct, but I am not about that). \r\n\r\nThe main problem is:\r\nHow should be written the CreateTensor() in the case of vector{m, n}, where m - rows, n - cols (for example for, vector<vector<float>> init_data = {{a1, b1}, {a, b} })? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4516", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4516/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4516/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4516/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4516", "id": 657366808, "node_id": "MDU6SXNzdWU2NTczNjY4MDg=", "number": 4516, "title": "loading a model in ort throws error: onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Node (Reshape_46) Op (Reshape) [ShapeInferenceError] Invalid position of 0 ", "user": {"login": "natangold85", "id": 24711661, "node_id": "MDQ6VXNlcjI0NzExNjYx", "avatar_url": "https://avatars2.githubusercontent.com/u/24711661?v=4", "gravatar_id": "", "url": "https://api.github.com/users/natangold85", "html_url": "https://github.com/natangold85", "followers_url": "https://api.github.com/users/natangold85/followers", "following_url": "https://api.github.com/users/natangold85/following{/other_user}", "gists_url": "https://api.github.com/users/natangold85/gists{/gist_id}", "starred_url": "https://api.github.com/users/natangold85/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/natangold85/subscriptions", "organizations_url": "https://api.github.com/users/natangold85/orgs", "repos_url": "https://api.github.com/users/natangold85/repos", "events_url": "https://api.github.com/users/natangold85/events{/privacy}", "received_events_url": "https://api.github.com/users/natangold85/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2185567573, "node_id": "MDU6TGFiZWwyMTg1NTY3NTcz", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/status:more-info-needed", "name": "status:more-info-needed", "color": "45b2d3", "default": false, "description": "more information is requested to continue investigation"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": {"login": "jcwchen", "id": 14194980, "node_id": "MDQ6VXNlcjE0MTk0OTgw", "avatar_url": "https://avatars0.githubusercontent.com/u/14194980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcwchen", "html_url": "https://github.com/jcwchen", "followers_url": "https://api.github.com/users/jcwchen/followers", "following_url": "https://api.github.com/users/jcwchen/following{/other_user}", "gists_url": "https://api.github.com/users/jcwchen/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcwchen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcwchen/subscriptions", "organizations_url": "https://api.github.com/users/jcwchen/orgs", "repos_url": "https://api.github.com/users/jcwchen/repos", "events_url": "https://api.github.com/users/jcwchen/events{/privacy}", "received_events_url": "https://api.github.com/users/jcwchen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jcwchen", "id": 14194980, "node_id": "MDQ6VXNlcjE0MTk0OTgw", "avatar_url": "https://avatars0.githubusercontent.com/u/14194980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcwchen", "html_url": "https://github.com/jcwchen", "followers_url": "https://api.github.com/users/jcwchen/followers", "following_url": "https://api.github.com/users/jcwchen/following{/other_user}", "gists_url": "https://api.github.com/users/jcwchen/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcwchen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcwchen/subscriptions", "organizations_url": "https://api.github.com/users/jcwchen/orgs", "repos_url": "https://api.github.com/users/jcwchen/repos", "events_url": "https://api.github.com/users/jcwchen/events{/privacy}", "received_events_url": "https://api.github.com/users/jcwchen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2020-07-15T13:50:52Z", "updated_at": "2020-07-20T15:08:34Z", "closed_at": "2020-07-20T15:08:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "i'm trying to convert a model from pytorch to onnxruntime. one part of the model is roi align algorithm.\r\ni converted the roi align function to onnx and its also pass onnx.checker.\r\nwhen i'm trying to load this model using onnxruntime i get this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/vzqgyc/projects/uc_workspace/ultracruise/nn_product/python_converter/roi_align_test.py\", line 91, in <module>\r\n    ort_model = onnxruntime.InferenceSession(path)\r\n  File \"/usr/local/lib/python3.6/dist-packages/onnxruntime/capi/session.py\", line 158, in __init__\r\n    self._load_model(providers)\r\n  File \"/usr/local/lib/python3.6/dist-packages/onnxruntime/capi/session.py\", line 177, in _load_model\r\n    self._sess.load_model(providers)\r\nonnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Node (Reshape_29) Op (Reshape) [ShapeInferenceError] Invalid position of 0\r\n\r\ni'm on linux os with python 3.6.8, pytorch 1.5, onnx 1.3.0, onnxruntime 1.7.0\r\nthis is my roi align class:\r\n\r\n`class RoIAlignTensorOpImp(torch.nn.Module):\r\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\r\n        super(RoIAlignTensorOpImp, self).__init__()\r\n\r\n        self.aligned_width = int(aligned_width)\r\n        self.aligned_height = int(aligned_height)\r\n        self.spatial_scale = float(spatial_scale)\r\n\r\n        self.pw_all = torch.arange(self.aligned_width)\r\n        self.ph_all = torch.arange(self.aligned_height)\r\n\r\n    def get_slice(self, ten: torch.Tensor, n: int, h_idx: torch.Tensor, w_idx: torch.Tensor):\r\n        d4t_tmp = ten[n, :, h_idx, :]\r\n        return d4t_tmp[:, :, :, w_idx]\r\n\r\n    def forward(self, features: torch.Tensor, rois: torch.Tensor) -> torch.Tensor:\r\n        num_rois = rois.size(0)\r\n        # Number of channels\r\n        channels = features.size(1)\r\n\r\n        output = torch.zeros((num_rois, channels, self.aligned_height, self.aligned_width))\r\n\r\n        for n in range(num_rois):\r\n            roi_batch_ind = int(rois[n, 0].item())\r\n            roi_start_w = rois[n, 1].item() * self.spatial_scale\r\n            roi_start_h = rois[n, 2].item() * self.spatial_scale\r\n            roi_end_w = rois[n, 3].item() * self.spatial_scale\r\n            roi_end_h = rois[n, 4].item() * self.spatial_scale\r\n            roi_width = max(roi_end_w - roi_start_w, 1.0)\r\n            roi_height = max(roi_end_h - roi_start_h, 1.0)\r\n\r\n            bin_size_h = roi_height / float(self.aligned_height)\r\n            bin_size_w = roi_width / float(self.aligned_width)\r\n\r\n            hstart = self.ph_all * bin_size_h + roi_start_h\r\n            hend = hstart + bin_size_h\r\n\r\n            hsize = (hend - hstart).int()\r\n\r\n            hlow_start = hstart.int()\r\n            hhigh_start = hstart.ceil().int()\r\n\r\n            wstart = self.pw_all * bin_size_w + roi_start_w\r\n            wend = wstart + bin_size_w\r\n            wsize = (wend - wstart).int()\r\n\r\n            wleft_start = wstart.int()\r\n            wright_start = wstart.ceil().int()\r\n\r\n            beta = ((wstart - wstart.floor()) / (wstart.ceil() - wstart.floor()))\r\n            beta_nan = ((wstart.ceil() - wstart.floor()) == 0).nonzero().squeeze(0).long()\r\n            beta[beta_nan] = torch.tensor(0.5, dtype=torch.float32)\r\n\r\n\r\n            alpha = (hstart - hstart.floor()) / (hstart.ceil() - hstart.floor())\r\n            alpha_nan = ((hstart.ceil() - hstart.floor()) == 0).nonzero().squeeze(0).long()\r\n            alpha[alpha_nan.long()] = torch.tensor(0.5, dtype=torch.float32)\r\n\r\n            # todo see what needs to do for different sizes of wsize or hsize\r\n            h_base = torch.arange(hsize[0] + 1).unsqueeze(0).repeat(self.aligned_height, 1)\r\n            w_base = torch.arange(wsize[0] + 1).unsqueeze(0).repeat(self.aligned_width, 1)\r\n\r\n            hlow_idx = h_base + hlow_start.unsqueeze(1)\r\n            hhigh_idx = h_base + hhigh_start.unsqueeze(1)\r\n\r\n            wleft_idx = w_base + wleft_start.unsqueeze(1)\r\n            wright_idx = w_base + wright_start.unsqueeze(1)\r\n\r\n            d1_all = self.get_slice(features, roi_batch_ind, hlow_idx, wleft_idx)\r\n            d2_all = self.get_slice(features, roi_batch_ind, hhigh_idx, wleft_idx)\r\n            d3_all = self.get_slice(features, roi_batch_ind, hlow_idx, wright_idx)\r\n            d4_all = self.get_slice(features, roi_batch_ind, hhigh_idx, wright_idx)\r\n\r\n            alpha_all = alpha.reshape(1, alpha.size()[0], 1, 1, 1)\r\n            beta_all = beta.reshape(1, 1, 1, beta.size()[0], 1)\r\n            values_all = (1 - alpha_all) * (1 - beta_all) * d1_all + \\\r\n                         alpha_all * (1 - beta_all) * d2_all + \\\r\n                         (1 - alpha_all) * beta_all * d3_all + \\\r\n                         alpha_all * beta_all * d4_all\r\n\r\n            out_sum_all = values_all.sum(4).sum(2)\r\n            bin_area = bin_size_h * bin_size_w\r\n\r\n            output[n, :, :, :] = out_sum_all / bin_area\r\n\r\n        return output`\r\n\r\nand this is the code to convert it and load it:\r\n\r\n`width = 7\r\nheight = 7\r\nscale = 0.125\r\n\r\nroi_align = RoIAlignTensorOpImp(height, width, scale)\r\n\r\n\r\nfeatures = torch.rand((1,16,96,170))\r\nrois = torch.rand((9,5))\r\n\r\npath = \"/tmp/t.onnx\"\r\n\r\noutput_keys = [\"outputs\"]\r\ntorch.onnx.export(roi_align, (features, rois), path,\r\n                  do_constant_folding=False,\r\n                  opset_version=12,\r\n                  input_names=[\"features\", \"rois\"],\r\n                  output_names=output_keys)\r\n\r\n\r\n\r\nmodel = onnx.load_model(path)\r\nonnx.shape_inference.infer_shapes(model)\r\nonnx.checker.check_model(model)\r\nprint(\"finish check onnx model\")\r\n\r\nort_model = onnxruntime.InferenceSession(path)`\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4515", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4515/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4515/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4515/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4515", "id": 657074694, "node_id": "MDU6SXNzdWU2NTcwNzQ2OTQ=", "number": 4515, "title": "How to register a new domain?", "user": {"login": "dongzhen123", "id": 29719811, "node_id": "MDQ6VXNlcjI5NzE5ODEx", "avatar_url": "https://avatars1.githubusercontent.com/u/29719811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dongzhen123", "html_url": "https://github.com/dongzhen123", "followers_url": "https://api.github.com/users/dongzhen123/followers", "following_url": "https://api.github.com/users/dongzhen123/following{/other_user}", "gists_url": "https://api.github.com/users/dongzhen123/gists{/gist_id}", "starred_url": "https://api.github.com/users/dongzhen123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dongzhen123/subscriptions", "organizations_url": "https://api.github.com/users/dongzhen123/orgs", "repos_url": "https://api.github.com/users/dongzhen123/repos", "events_url": "https://api.github.com/users/dongzhen123/events{/privacy}", "received_events_url": "https://api.github.com/users/dongzhen123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-15T05:37:47Z", "updated_at": "2020-07-15T07:12:51Z", "closed_at": "2020-07-15T07:12:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Which files need to be modified?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4513", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4513/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4513/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4513/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4513", "id": 656928148, "node_id": "MDU6SXNzdWU2NTY5MjgxNDg=", "number": 4513, "title": "Question about Tensorflow_Keras_Bert-Squad_OnnxRuntime_CPU.ipynb", "user": {"login": "652994331", "id": 51428350, "node_id": "MDQ6VXNlcjUxNDI4MzUw", "avatar_url": "https://avatars3.githubusercontent.com/u/51428350?v=4", "gravatar_id": "", "url": "https://api.github.com/users/652994331", "html_url": "https://github.com/652994331", "followers_url": "https://api.github.com/users/652994331/followers", "following_url": "https://api.github.com/users/652994331/following{/other_user}", "gists_url": "https://api.github.com/users/652994331/gists{/gist_id}", "starred_url": "https://api.github.com/users/652994331/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/652994331/subscriptions", "organizations_url": "https://api.github.com/users/652994331/orgs", "repos_url": "https://api.github.com/users/652994331/repos", "events_url": "https://api.github.com/users/652994331/events{/privacy}", "received_events_url": "https://api.github.com/users/652994331/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1974915201, "node_id": "MDU6TGFiZWwxOTc0OTE1MjAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/model:BERT", "name": "model:BERT", "color": "aee888", "default": false, "description": "BERT Optimization"}, {"id": 1273765791, "node_id": "MDU6TGFiZWwxMjczNzY1Nzkx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:performance", "name": "type:performance", "color": "a2eeef", "default": false, "description": "performance related issues and questions"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-14T22:08:33Z", "updated_at": "2020-08-25T18:39:16Z", "closed_at": "2020-08-25T18:39:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, i am following this tutorial and have 2 questions. The first one is , after i finished this part:\r\n![image](https://user-images.githubusercontent.com/51428350/87481022-1a160700-c661-11ea-8672-c46f595ce21a.png)\r\nthe result i got was length 512 = 486ms not 94ms.\r\n\r\nThe other one is, i have a question, why the average inference time for original tensorflow(100 sentenses) is 94 ms, but the inference time for onnx is 300+ms ~ 600+ms. i thought onnxruntime's a oprimization method so it should have better result right? \r\nThank you\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4507", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4507/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4507/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4507/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4507", "id": 656669767, "node_id": "MDU6SXNzdWU2NTY2Njk3Njc=", "number": 4507, "title": "remove_initializer_from_input.py not removing all ininitializers from inputs", "user": {"login": "cjtrueeb", "id": 63647197, "node_id": "MDQ6VXNlcjYzNjQ3MTk3", "avatar_url": "https://avatars3.githubusercontent.com/u/63647197?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cjtrueeb", "html_url": "https://github.com/cjtrueeb", "followers_url": "https://api.github.com/users/cjtrueeb/followers", "following_url": "https://api.github.com/users/cjtrueeb/following{/other_user}", "gists_url": "https://api.github.com/users/cjtrueeb/gists{/gist_id}", "starred_url": "https://api.github.com/users/cjtrueeb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cjtrueeb/subscriptions", "organizations_url": "https://api.github.com/users/cjtrueeb/orgs", "repos_url": "https://api.github.com/users/cjtrueeb/repos", "events_url": "https://api.github.com/users/cjtrueeb/events{/privacy}", "received_events_url": "https://api.github.com/users/cjtrueeb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2185567573, "node_id": "MDU6TGFiZWwyMTg1NTY3NTcz", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/status:more-info-needed", "name": "status:more-info-needed", "color": "45b2d3", "default": false, "description": "more information is requested to continue investigation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-14T14:45:23Z", "updated_at": "2020-07-16T09:50:23Z", "closed_at": "2020-07-16T09:50:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using my exported TF model I am getting the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Workspaces/ML/Zahnsegmentierung_KI/Model_Training/TestScript/TestOnnx.py\", line 51, in <module>\r\n    'labels_pl:0': np.random.randn(1, 2000).astype(np.int32), 'is_training_pl:0': np.array(False)})\r\n  File \"C:\\Anaconda3\\envs\\py37\\lib\\site-packages\\onnxruntime\\capi\\session.py\", line 111, in run\r\n    return self._sess.run(output_names, input_feed, run_options)\r\nonnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Mul node. Name:'conv0/batch_normalization/mul_3' Status Message: D:\\2\\s\\include\\onnxruntime\\core/framework/op_kernel.h:88 onnxruntime::OpKernelContext::Input Missing Input: conv0/batch_normalization/cond_3/Merge:0\r\n```\r\nTherefore I have tried to use onnxruntime/tools/python/remove_initializer_from_input.py to clean up the inputs. After running the script and starting a session with the cleaned up model, I am still getting the same error. The warnings reveal that the model inputs still contain some initializers (most of them have been removed):\r\n\r\n```\r\n2020-07-14 15:44:10.7294799 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__49:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7295972 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__800 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7296120 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__799 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7298176 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__73:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7298704 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__814 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7298824 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__813 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7300587 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__97:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7301062 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__810 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7301178 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__809 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7303060 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__121:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7303546 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__806 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7303652 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__805 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7305517 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__812 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7305633 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__811 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7306118 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__808 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7319337 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__807 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7322793 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__169:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7323384 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__816 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7323516 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__815 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7325435 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__193:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7326140 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__802 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7326268 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__801 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7328374 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__217:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7329132 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__804 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7329604 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__803 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7331998 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer sub_graph_ending_node_Identity__241:0 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7333040 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__818 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7333171 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer const_fold_opt__817 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.7336573 [W:onnxruntime:, graph.cc:814 onnxruntime::Graph::Graph] Initializer cond__457 appears in graph inputs and will not be treated as constant value/weight. This may fail some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\r\n2020-07-14 15:44:10.8512426 [E:onnxruntime:, sequential_executor.cc:281 onnxruntime::SequentialExecutor::Execute] Non-zero status code returned while running Mul node. Name:'conv0/batch_normalization/mul_3' Status Message: D:\\2\\s\\include\\onnxruntime\\core/framework/op_kernel.h:88 onnxruntime::OpKernelContext::Input Missing Input: conv0/batch_normalization/cond_3/Merge:0\r\n```\r\n\r\n\r\n**System information**\r\n- Windows 10\r\n- onnxruntime 1.3.0\r\n- model: tf2onnx 1.6.2 --opset 9\r\n\r\nUnfortunately I cannot share the model.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4506", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4506/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4506/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4506/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4506", "id": 656611437, "node_id": "MDU6SXNzdWU2NTY2MTE0Mzc=", "number": 4506, "title": "GPU Memory Release Problem ONNRuntime C++", "user": {"login": "anirudha16101", "id": 46598161, "node_id": "MDQ6VXNlcjQ2NTk4MTYx", "avatar_url": "https://avatars3.githubusercontent.com/u/46598161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anirudha16101", "html_url": "https://github.com/anirudha16101", "followers_url": "https://api.github.com/users/anirudha16101/followers", "following_url": "https://api.github.com/users/anirudha16101/following{/other_user}", "gists_url": "https://api.github.com/users/anirudha16101/gists{/gist_id}", "starred_url": "https://api.github.com/users/anirudha16101/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anirudha16101/subscriptions", "organizations_url": "https://api.github.com/users/anirudha16101/orgs", "repos_url": "https://api.github.com/users/anirudha16101/repos", "events_url": "https://api.github.com/users/anirudha16101/events{/privacy}", "received_events_url": "https://api.github.com/users/anirudha16101/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1273765791, "node_id": "MDU6TGFiZWwxMjczNzY1Nzkx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:performance", "name": "type:performance", "color": "a2eeef", "default": false, "description": "performance related issues and questions"}], "state": "closed", "locked": false, "assignee": {"login": "HectorSVC", "id": 29932710, "node_id": "MDQ6VXNlcjI5OTMyNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/29932710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HectorSVC", "html_url": "https://github.com/HectorSVC", "followers_url": "https://api.github.com/users/HectorSVC/followers", "following_url": "https://api.github.com/users/HectorSVC/following{/other_user}", "gists_url": "https://api.github.com/users/HectorSVC/gists{/gist_id}", "starred_url": "https://api.github.com/users/HectorSVC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HectorSVC/subscriptions", "organizations_url": "https://api.github.com/users/HectorSVC/orgs", "repos_url": "https://api.github.com/users/HectorSVC/repos", "events_url": "https://api.github.com/users/HectorSVC/events{/privacy}", "received_events_url": "https://api.github.com/users/HectorSVC/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "HectorSVC", "id": 29932710, "node_id": "MDQ6VXNlcjI5OTMyNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/29932710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HectorSVC", "html_url": "https://github.com/HectorSVC", "followers_url": "https://api.github.com/users/HectorSVC/followers", "following_url": "https://api.github.com/users/HectorSVC/following{/other_user}", "gists_url": "https://api.github.com/users/HectorSVC/gists{/gist_id}", "starred_url": "https://api.github.com/users/HectorSVC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HectorSVC/subscriptions", "organizations_url": "https://api.github.com/users/HectorSVC/orgs", "repos_url": "https://api.github.com/users/HectorSVC/repos", "events_url": "https://api.github.com/users/HectorSVC/events{/privacy}", "received_events_url": "https://api.github.com/users/HectorSVC/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2020-07-14T13:27:05Z", "updated_at": "2020-08-19T06:42:15Z", "closed_at": "2020-08-19T06:42:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nHi, Currently I am using ONNX C++ Api and when I analysis the GPU Memory Usage.  I find that Onnx is not Releasing the Whole memory until its .exe got killed. Can Somebody Explain me why it is not releasing the memory completely. I have used gort->ReleaseSession(session);, g_ort->ReleaseSessionOptions(session_options); g_ort->ReleaseEnv(env);\r\n code at the last to release the memory.\r\n\r\nFor Reference:---\r\nHere memory is not released It is still occupying 7% of GPU Memory. but after some time when I close the exe it goes down to 0\r\nPlease Help!! me How can I release the memory completely without killing the .exe.\r\n![github](https://user-images.githubusercontent.com/46598161/87430844-6a5e7c00-c603-11ea-9e21-56e08ca186e1.PNG)\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4492", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4492/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4492/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4492/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4492", "id": 655816283, "node_id": "MDU6SXNzdWU2NTU4MTYyODM=", "number": 4492, "title": "How can I get a reference to an already running environment on the same thread?", "user": {"login": "brnd42", "id": 68229816, "node_id": "MDQ6VXNlcjY4MjI5ODE2", "avatar_url": "https://avatars3.githubusercontent.com/u/68229816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brnd42", "html_url": "https://github.com/brnd42", "followers_url": "https://api.github.com/users/brnd42/followers", "following_url": "https://api.github.com/users/brnd42/following{/other_user}", "gists_url": "https://api.github.com/users/brnd42/gists{/gist_id}", "starred_url": "https://api.github.com/users/brnd42/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brnd42/subscriptions", "organizations_url": "https://api.github.com/users/brnd42/orgs", "repos_url": "https://api.github.com/users/brnd42/repos", "events_url": "https://api.github.com/users/brnd42/events{/privacy}", "received_events_url": "https://api.github.com/users/brnd42/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1395147764, "node_id": "MDU6TGFiZWwxMzk1MTQ3NzY0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C/C++", "name": "api:C/C++", "color": "0e8a16", "default": false, "description": "related to the  public API(and ABI) for onnxruntime"}, {"id": 1122493981, "node_id": "MDU6TGFiZWwxMTIyNDkzOTgx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:enhancement", "name": "type:enhancement", "color": "a2eeef", "default": false, "description": "request for unsupported feature or enhancement"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-13T12:36:12Z", "updated_at": "2020-07-22T19:29:56Z", "closed_at": "2020-07-22T19:29:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am writing a plugin that uses ONNX Runtime for neural network inference. This plugin is a dynamic library on MacOs or Windows and it can be opened on the same thread with other plugins using ONNX Runtime. To run our inference I need to create an environment when the plugin is opened. If two plugins are opened the second one will throw an exception, since there can only be one environment on one thread at a time.\r\nWhat is the best way to solve this? Do I need shared memory between the plugins to hold a pointer to an environment or should I call another dynamic library that holds the environment? Is there maybe some ONNX Runtime API call to get a pointer to existing environments on a thread?\r\n\r\nI do not have any control over the threading by the plugin host and I cannot exchange information over the host.\r\n\r\nI am using the c++ api", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4488", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4488/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4488/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4488/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4488", "id": 655139452, "node_id": "MDU6SXNzdWU2NTUxMzk0NTI=", "number": 4488, "title": "Different Output when Inference on CPU / GPU in some case", "user": {"login": "hirokic5", "id": 19792127, "node_id": "MDQ6VXNlcjE5NzkyMTI3", "avatar_url": "https://avatars2.githubusercontent.com/u/19792127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hirokic5", "html_url": "https://github.com/hirokic5", "followers_url": "https://api.github.com/users/hirokic5/followers", "following_url": "https://api.github.com/users/hirokic5/following{/other_user}", "gists_url": "https://api.github.com/users/hirokic5/gists{/gist_id}", "starred_url": "https://api.github.com/users/hirokic5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hirokic5/subscriptions", "organizations_url": "https://api.github.com/users/hirokic5/orgs", "repos_url": "https://api.github.com/users/hirokic5/repos", "events_url": "https://api.github.com/users/hirokic5/events{/privacy}", "received_events_url": "https://api.github.com/users/hirokic5/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2186357781, "node_id": "MDU6TGFiZWwyMTg2MzU3Nzgx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:CUDA", "name": "ep:CUDA", "color": "bfdadc", "default": false, "description": "questions/issues related to CUDA EP"}], "state": "closed", "locked": false, "assignee": {"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hariharans29", "id": 9969784, "node_id": "MDQ6VXNlcjk5Njk3ODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hariharans29", "html_url": "https://github.com/hariharans29", "followers_url": "https://api.github.com/users/hariharans29/followers", "following_url": "https://api.github.com/users/hariharans29/following{/other_user}", "gists_url": "https://api.github.com/users/hariharans29/gists{/gist_id}", "starred_url": "https://api.github.com/users/hariharans29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hariharans29/subscriptions", "organizations_url": "https://api.github.com/users/hariharans29/orgs", "repos_url": "https://api.github.com/users/hariharans29/repos", "events_url": "https://api.github.com/users/hariharans29/events{/privacy}", "received_events_url": "https://api.github.com/users/hariharans29/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-07-11T05:29:14Z", "updated_at": "2020-08-20T17:04:29Z", "closed_at": "2020-08-18T09:09:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nThe output of onnx model with GPU seems to different from that with CPU in some case(in my case, segmentation! ).\r\n\r\n**Urgency**\r\nASAP\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10 64bit\r\n- ONNX Runtime installed from (source or binary):binary (by pip install onnxruntime-gpu)\r\n- ONNX Runtime version:1.3.0\r\n- Python version:3.7.2\r\n- Visual Studio version (if applicable):2019\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:CUDA 10.1 cuDNN 7.6.5\r\n- GPU model and memory:1080Ti \r\n\r\n**To Reproduce**\r\nI trained segmentation model in keras, and using [keras2onnx](https://github.com/onnx/keras-onnx/tree/master/keras2onnx), convert keras model to onnx model.   \r\nBut there seems to be non-negligible difference outputs of these models. \r\nHere is an example. Dog Segmentation. \r\n\r\n![onnx_segmentation_gpu](https://user-images.githubusercontent.com/19792127/87217259-2f99e100-c382-11ea-8bbd-28d93303f5ab.png)\r\n\r\n![onnx_segmentation_cpu](https://user-images.githubusercontent.com/19792127/87217293-830c2f00-c382-11ea-95f6-0b0f1b2073a5.png)\r\n\r\nThe output of onnxruntime on **CPU** is almost same as that of keras, but Output of onnxruntime on GPU seems to be different !!! \r\n\r\nBy the way, for classification model , the output of onnxruntime on GPU is almost same as that of keras... \r\n\r\n![onnx_classification_gpu](https://user-images.githubusercontent.com/19792127/87217305-ae8f1980-c382-11ea-8478-3e2e1cbd2957.png)\r\n\r\nAnd [here](https://drive.google.com/drive/folders/1Ph8KC1zj17OUGsxMOEvnAaKf8r0_Hmvv?usp=sharing) is source code & weights & image. \r\n\r\nDid I have mistake when convert keras model , or when inference on GPU ? \r\n\r\nThank you.\r\n\r\n**Expected behavior**\r\nthe outputs are almost same in keras / onnxruntime on CPU / onnxruntime on GPU \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4478", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4478/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4478/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4478/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4478", "id": 654715065, "node_id": "MDU6SXNzdWU2NTQ3MTUwNjU=", "number": 4478, "title": "Assertion failed: f.pitches[i] >= s", "user": {"login": "GrzegorzKarchNV", "id": 45092119, "node_id": "MDQ6VXNlcjQ1MDkyMTE5", "avatar_url": "https://avatars3.githubusercontent.com/u/45092119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GrzegorzKarchNV", "html_url": "https://github.com/GrzegorzKarchNV", "followers_url": "https://api.github.com/users/GrzegorzKarchNV/followers", "following_url": "https://api.github.com/users/GrzegorzKarchNV/following{/other_user}", "gists_url": "https://api.github.com/users/GrzegorzKarchNV/gists{/gist_id}", "starred_url": "https://api.github.com/users/GrzegorzKarchNV/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GrzegorzKarchNV/subscriptions", "organizations_url": "https://api.github.com/users/GrzegorzKarchNV/orgs", "repos_url": "https://api.github.com/users/GrzegorzKarchNV/repos", "events_url": "https://api.github.com/users/GrzegorzKarchNV/events{/privacy}", "received_events_url": "https://api.github.com/users/GrzegorzKarchNV/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2204061391, "node_id": "MDU6TGFiZWwyMjA0MDYxMzkx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:TensorRT", "name": "ep:TensorRT", "color": "bfdadc", "default": false, "description": "questions/issues related to TensorRT EP"}, {"id": 1122493979, "node_id": "MDU6TGFiZWwxMTIyNDkzOTc5", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:bug", "name": "type:bug", "color": "a2eeef", "default": false, "description": "potential bug in ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-10T11:30:08Z", "updated_at": "2020-08-10T09:07:13Z", "closed_at": "2020-08-10T09:07:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\nWhen loading an ONNX model in python with onnxruntime.InferenceSession(), I get the `Assertion failed: f.pitches[i] >= s`.\r\nThe model was generated using `torch.onnx.export`, opset=12, in PyTorch container `nvcr.io/nvidia/pytorch:20.06-py3`\r\n**Urgency**\r\nI have a model in ONNX format that I would like to test.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04:\r\n- ONNX Runtime installed from docker: https://github.com/microsoft/onnxruntime/blob/v1.3.1/dockerfiles/Dockerfile.tensorrt\r\n- ONNX Runtime version: 1.3.1\r\n- Python version: 3.7\r\n- CUDA version: 10.2.89\r\n- CUDA driver version: 440.33.01\r\n- CUDNN version: 7.6.5.32\r\n- GPU model and memory: NVIDIA TitanV 12gb\r\n\r\n**To Reproduce**\r\nFollow instructions to build onnxruntime-trt\r\nhttps://github.com/microsoft/onnxruntime/tree/master/dockerfiles#tensorrt\r\nIn the docker, with accessible ONNX model: https://drive.google.com/file/d/1lPN2iANiI7mjm1V2XkguFPds2Q0KSzb8/view?usp=sharing\r\n```bash\r\n# python3.7\r\nin interpreter:\r\n>>> import onnxruntime as ort\r\n>>> sess = ort.InferenceSession(\"waveglow.onnx\")\r\n```\r\n- ONNX model: \r\n\r\n**Expected behavior**\r\nSuccessful model loading\r\n\r\n**Additional context**\r\ncommand and full error:\r\n```bash\r\n>import onnxruntime as ort\r\n>sess = ort.InferenceSession(\"triton_models/waveglow/1/waveglow.onnx\")\r\n2020-07-10 09:56:30.231246592 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:56:30 WARNING] /code/onnxruntime/cmake/external/onnx-tensorrt/onnx2trt_utils.cpp:235: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\r\n2020-07-10 09:56:30.231284361 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:56:30 WARNING] /code/onnxruntime/cmake/external/onnx-tensorrt/onnx2trt_utils.cpp:261: One or more weights outside the range of INT32 was clamped\r\n...\r\n2020-07-10 09:56:32.539329200 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:56:32 WARNING] /code/onnxruntime/cmake/external/onnx-tensorrt/onnx2trt_utils.cpp:261: One or more weights outside the range of INT32 was clamped\r\n2020-07-10 09:57:11.654768766 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:57:11     BUG] Assertion failed: f.pitches[i] >= s\r\n../builder/tacticOptimizer.cpp:617\r\nAborting...\r\n\r\n2020-07-10 09:57:11.664716702 [W:onnxruntime:Default, tensorrt_execution_provider.h:36 log] [2020-07-10 09:57:11   ERROR] ../builder/tacticOptimizer.cpp (617) - Assertion Error in shrink: 0 (f.pitches[i] >= s)\r\n---------------------------------------------------------------------------\r\nEPFail                                    Traceback (most recent call last)\r\n<ipython-input-2-9e2b45df1a75> in <module>\r\n----> 1 sess = ort.InferenceSession(\"triton_models/waveglow/1/waveglow.onnx\")\r\n\r\n/opt/miniconda/lib/python3.7/site-packages/onnxruntime/capi/session.py in __init__(self, path_or_bytes, sess_options, providers)\r\n    156         self._path_or_bytes = path_or_bytes\r\n    157         self._sess_options = sess_options\r\n--> 158         self._load_model(providers or [])\r\n    159         self._enable_fallback = True\r\n    160         Session.__init__(self, self._sess)\r\n\r\n/opt/miniconda/lib/python3.7/site-packages/onnxruntime/capi/session.py in _load_model(self, providers)\r\n    175             raise TypeError(\"Unable to load from type '{0}'\".format(type(self._path_or_bytes)))\r\n    176\r\n--> 177         self._sess.load_model(providers)\r\n    178\r\n    179         self._sess_options = self._sess.session_options\r\n\r\nEPFail: [ONNXRuntimeError] : 11 : EP_FAIL : TensorRT EP could not build Engine for fused node: TensorrtExecutionProvider_TRTKernel_graph_torch-jit-export_0_0\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4477", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4477/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4477/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4477/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4477", "id": 654625498, "node_id": "MDU6SXNzdWU2NTQ2MjU0OTg=", "number": 4477, "title": "Question about default multi-thread of mlas", "user": {"login": "feiyuvl", "id": 13388702, "node_id": "MDQ6VXNlcjEzMzg4NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/13388702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/feiyuvl", "html_url": "https://github.com/feiyuvl", "followers_url": "https://api.github.com/users/feiyuvl/followers", "following_url": "https://api.github.com/users/feiyuvl/following{/other_user}", "gists_url": "https://api.github.com/users/feiyuvl/gists{/gist_id}", "starred_url": "https://api.github.com/users/feiyuvl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/feiyuvl/subscriptions", "organizations_url": "https://api.github.com/users/feiyuvl/orgs", "repos_url": "https://api.github.com/users/feiyuvl/repos", "events_url": "https://api.github.com/users/feiyuvl/events{/privacy}", "received_events_url": "https://api.github.com/users/feiyuvl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2014185961, "node_id": "MDU6TGFiZWwyMDE0MTg1OTYx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:coreruntime", "name": "component:coreruntime", "color": "303a93", "default": false, "description": "related to core runtime"}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-10T08:52:01Z", "updated_at": "2020-07-13T22:45:22Z", "closed_at": "2020-07-13T22:45:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, what's the default multi-thread version of mlas, Openmp or thread-pool defined by onnxruntime?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4466", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4466/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4466/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4466/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4466", "id": 654194887, "node_id": "MDU6SXNzdWU2NTQxOTQ4ODc=", "number": 4466, "title": "Running models with dynamic output shapes (C++)", "user": {"login": "tna0y", "id": 22504374, "node_id": "MDQ6VXNlcjIyNTA0Mzc0", "avatar_url": "https://avatars1.githubusercontent.com/u/22504374?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tna0y", "html_url": "https://github.com/tna0y", "followers_url": "https://api.github.com/users/tna0y/followers", "following_url": "https://api.github.com/users/tna0y/following{/other_user}", "gists_url": "https://api.github.com/users/tna0y/gists{/gist_id}", "starred_url": "https://api.github.com/users/tna0y/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tna0y/subscriptions", "organizations_url": "https://api.github.com/users/tna0y/orgs", "repos_url": "https://api.github.com/users/tna0y/repos", "events_url": "https://api.github.com/users/tna0y/events{/privacy}", "received_events_url": "https://api.github.com/users/tna0y/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-09T16:29:48Z", "updated_at": "2020-07-09T19:56:38Z", "closed_at": "2020-07-09T19:56:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a model which accepts and returns tensors with dynamic axes (variable input/output shape).\r\n\r\nI run models via C++ onnxruntime SDK. The problem is that according to all examples and docs I managed to find you have to preallocate input and output tensors. While input tensors are fine it is still unclear how do you preallocate output tensors if their shape is unknown. \r\n\r\nBelow is an example how I run models. It lacks a few details but they should be deductible. Note that output tensors are already allocated. \r\n```c++\r\ntemplate<typename T>\r\nstd::vector<std::unique_ptr<Tensor>>\r\n        run(std::vector<std::unique_ptr<Tensor>> input, std::vector<std::unique_ptr<Tensor>> output) {\r\n        \r\n        auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);\r\n\r\n        // setup inputs\r\n     \r\n        std::vector<const char*> input_names(input.size());\r\n        \r\n        std::vector<Ort::Value> input_tensors;\r\n        input_tensors.reserve(input.size());\r\n\r\n        for (uint32_t i = 0; i < input.size(); ++i) {\r\n            input_names[i] = this->session->GetInputName(i, Ort::AllocatorWithDefaultOptions());\r\n            input_tensors.push_back(\r\n                    Ort::Value::CreateTensor<T>(\r\n                            memory_info,\r\n                            reinterpret_cast<T*>(const_cast<char*>(input[i]->Ptr())),\r\n                            input[i]->Size(),\r\n                            this->meta->InputShape[i].data(),\r\n                            this->meta->InputShape[i].size())\r\n                    );\r\n        }\r\n\r\n        std::vector<const char*> output_names(output.size());\r\n\r\n        std::vector<Ort::Value> output_tensors;\r\n        output_tensors.reserve(output.size());\r\n\r\n        for (uint32_t i = 0; i < output.size(); ++i) {\r\n            output_names[i] = this->session->GetOutputName(i, Ort::AllocatorWithDefaultOptions());\r\n            output_tensors.push_back(\r\n                    Ort::Value::CreateTensor<T>(\r\n                            memory_info,\r\n                            reinterpret_cast<T*>(const_cast<char*>(output[i]->Ptr())),\r\n                            output[i]->Size(),\r\n                            this->meta->OutputShape[i].data(),\r\n                            this->meta->OutputShape[i].size())\r\n            );\r\n        }\r\n\r\n        Ort::RunOptions run_options;\r\n        // run_options.SetRunLogSeverityLevel(0);\r\n\r\n        this->session->Run(run_options,\r\n                input_names.data(), input_tensors.data(), input_tensors.size(),\r\n                output_names.data(), output_tensors.data(), output_tensors.size());\r\n\r\n        return std::move(output);\r\n}\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4450", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4450/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4450/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4450/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4450", "id": 653098549, "node_id": "MDU6SXNzdWU2NTMwOTg1NDk=", "number": 4450, "title": "How to import  onnxruntime when I build from source?", "user": {"login": "dongzhen123", "id": 29719811, "node_id": "MDQ6VXNlcjI5NzE5ODEx", "avatar_url": "https://avatars1.githubusercontent.com/u/29719811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dongzhen123", "html_url": "https://github.com/dongzhen123", "followers_url": "https://api.github.com/users/dongzhen123/followers", "following_url": "https://api.github.com/users/dongzhen123/following{/other_user}", "gists_url": "https://api.github.com/users/dongzhen123/gists{/gist_id}", "starred_url": "https://api.github.com/users/dongzhen123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dongzhen123/subscriptions", "organizations_url": "https://api.github.com/users/dongzhen123/orgs", "repos_url": "https://api.github.com/users/dongzhen123/repos", "events_url": "https://api.github.com/users/dongzhen123/events{/privacy}", "received_events_url": "https://api.github.com/users/dongzhen123/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-08T08:28:23Z", "updated_at": "2020-07-09T02:21:38Z", "closed_at": "2020-07-09T02:21:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have build the onnxruntime use \"./build.sh --config RelWithDebInfo --build_shared_lib --parallel\" successully.\r\n\r\n\r\nThen how to import onnxruntime? or before I import onnxruntime what else should I do?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4445", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4445/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4445/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4445/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4445", "id": 652502963, "node_id": "MDU6SXNzdWU2NTI1MDI5NjM=", "number": 4445, "title": "Build with OpenVINO", "user": {"login": "oliveirara", "id": 12023276, "node_id": "MDQ6VXNlcjEyMDIzMjc2", "avatar_url": "https://avatars2.githubusercontent.com/u/12023276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oliveirara", "html_url": "https://github.com/oliveirara", "followers_url": "https://api.github.com/users/oliveirara/followers", "following_url": "https://api.github.com/users/oliveirara/following{/other_user}", "gists_url": "https://api.github.com/users/oliveirara/gists{/gist_id}", "starred_url": "https://api.github.com/users/oliveirara/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oliveirara/subscriptions", "organizations_url": "https://api.github.com/users/oliveirara/orgs", "repos_url": "https://api.github.com/users/oliveirara/repos", "events_url": "https://api.github.com/users/oliveirara/events{/privacy}", "received_events_url": "https://api.github.com/users/oliveirara/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 1624868154, "node_id": "MDU6TGFiZWwxNjI0ODY4MTU0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:OpenVINO", "name": "ep:OpenVINO", "color": "bfdadc", "default": false, "description": "questions/issues related to OpenVINO EP"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2020-07-07T17:42:21Z", "updated_at": "2020-09-07T23:26:38Z", "closed_at": "2020-09-07T23:26:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to build onnxruntime with OpenVINO following the procedures from [BUILD.md#openvino](https://github.com/microsoft/onnxruntime/blob/master/BUILD.md#openvino) but when the compiler tries `Linking CXX executable onnxruntime_test_all`, `undefined references` appears and end it up with errors:\r\n\r\n```bash\r\n[ 81%] [Building CXX object]\r\nCMakeFiles/onnxruntime_test_all.dir/share/storage1/TNG/onnxruntime/onnxruntime/test/contrib_ops/inverse_test.cc.o\u001b[0m\r\nlibonnxruntime_providers_openvino.a(backend_manager.cc.o): In function `onnxruntime::openvino_ep::GlobalContext::GlobalContext()':\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/contexts.h:12: undefined reference to `InferenceEngine::Core::Core(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::details::extract_exception(InferenceEngine::StatusCode, char*)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/details/ie_exception_conversion.hpp:62: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::InferRequest::GetBlob(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:114: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:113: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:110: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::InferRequest::StartAsync()':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:215: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o):/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:114: more undefined references to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)' follow\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `onnxruntime::openvino_ep::BasicBackend::BasicBackend(onnx::ModelProto const&, onnxruntime::openvino_ep::GlobalContext&, onnxruntime::openvino_ep::SubGraphContext const&)':\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/backends/basic_backend.cc:39: undefined reference to `InferenceEngine::Core::LoadNetwork(InferenceEngine::CNNNetwork, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::ExecutableNetwork::CreateInferRequestPtr()':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_executable_network.hpp:122: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::InferRequest::InferRequest(std::shared_ptr<InferenceEngine::IInferRequest>, InferenceEngine::details::SOPointer<InferenceEngine::IInferencePlugin, InferenceEngine::details::SharedObjectLoader>)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:205: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `InferenceEngine::ExecutableNetwork::CreateInferRequestPtr()':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_executable_network.hpp:122: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `InferenceEngine::InferRequest::InferRequest(std::shared_ptr<InferenceEngine::IInferRequest>, InferenceEngine::details::SOPointer<InferenceEngine::IInferencePlugin, InferenceEngine::details::SharedObjectLoader>)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:205: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `InferenceEngine::InferRequest::GetBlob(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:113: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o):/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:110: more undefined references to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)' follow\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `onnxruntime::openvino_ep::VADMBackend::VADMBackend(onnx::ModelProto const&, onnxruntime::openvino_ep::GlobalContext&, onnxruntime::openvino_ep::SubGraphContext const&)':\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/backends/vadm_backend.cc:58: undefined reference to `InferenceEngine::Core::LoadNetwork(InferenceEngine::CNNNetwork, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)'\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/backends/vadm_backend.cc:88: undefined reference to `InferenceEngine::Core::LoadNetwork(InferenceEngine::CNNNetwork, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o): In function `InferenceEngine::InputInfo::setPrecision(InferenceEngine::Precision)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:60: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o): In function `InferenceEngine::InputInfo::getTensorDesc() const':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:131: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o): In function `InferenceEngine::InputInfo::setLayout(InferenceEngine::Layout)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o):/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: more undefined references to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)' follow\r\ncollect2: error: ld returned 1 exit status\r\ngmake[2]: *** [onnx_test_runner] Error 1\r\ngmake[1]: *** [CMakeFiles/onnx_test_runner.dir/all] Error 2\r\n```\r\n\r\nand\r\n\r\n```bash\r\n[ 95%] [Linking CXX executable onnxruntime_test_all]\r\nlibonnxruntime_providers_openvino.a(backend_manager.cc.o): In function `onnxruntime::openvino_ep::GlobalContext::GlobalContext()':\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/contexts.h:12: undefined reference to `InferenceEngine::Core::Core(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::details::extract_exception(InferenceEngine::StatusCode, char*)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/details/ie_exception_conversion.hpp:62: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::InferRequest::GetBlob(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:114: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:113: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:110: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::InferRequest::StartAsync()':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:215: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o):/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:114: more undefined references to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)' follow\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `onnxruntime::openvino_ep::BasicBackend::BasicBackend(onnx::ModelProto const&, onnxruntime::openvino_ep::GlobalContext&, onnxruntime::openvino_ep::SubGraphContext const&)':\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/backends/basic_backend.cc:39: undefined reference to `InferenceEngine::Core::LoadNetwork(InferenceEngine::CNNNetwork, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::ExecutableNetwork::CreateInferRequestPtr()':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_executable_network.hpp:122: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(basic_backend.cc.o): In function `InferenceEngine::InferRequest::InferRequest(std::shared_ptr<InferenceEngine::IInferRequest>, InferenceEngine::details::SOPointer<InferenceEngine::IInferencePlugin, InferenceEngine::details::SharedObjectLoader>)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:205: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `InferenceEngine::ExecutableNetwork::CreateInferRequestPtr()':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_executable_network.hpp:122: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `InferenceEngine::InferRequest::InferRequest(std::shared_ptr<InferenceEngine::IInferRequest>, InferenceEngine::details::SOPointer<InferenceEngine::IInferencePlugin, InferenceEngine::details::SharedObjectLoader>)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:205: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `InferenceEngine::InferRequest::GetBlob(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:113: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o):/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/cpp/ie_infer_request.hpp:110: more undefined references to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)' follow\r\nlibonnxruntime_providers_openvino.a(vadm_backend.cc.o): In function `onnxruntime::openvino_ep::VADMBackend::VADMBackend(onnx::ModelProto const&, onnxruntime::openvino_ep::GlobalContext&, onnxruntime::openvino_ep::SubGraphContext const&)':\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/backends/vadm_backend.cc:58: undefined reference to `InferenceEngine::Core::LoadNetwork(InferenceEngine::CNNNetwork, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)'\r\n/share/storage1/TNG/onnxruntime/onnxruntime/core/providers/openvino/backends/vadm_backend.cc:88: undefined reference to `InferenceEngine::Core::LoadNetwork(InferenceEngine::CNNNetwork, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o): In function `InferenceEngine::InputInfo::setPrecision(InferenceEngine::Precision)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:60: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o): In function `InferenceEngine::InputInfo::getTensorDesc() const':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:131: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o): In function `InferenceEngine::InputInfo::setLayout(InferenceEngine::Layout)':\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\n/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: undefined reference to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'\r\nlibonnxruntime_providers_openvino.a(backend_utils.cc.o):/opt/intel/openvino_2020.3.194/deployment_tools/inference_engine/include/ie_input_info.hpp:91: more undefined references to `InferenceEngine::details::InferenceEngineException::InferenceEngineException(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)' follow\r\ncollect2: error: ld returned 1 exit status\r\ngmake[2]: *** [onnxruntime_test_all] Error 1\r\ngmake[1]: *** [CMakeFiles/onnxruntime_test_all.dir/all] Error 2\r\ngmake: *** [all] Error 2\r\nTraceback (most recent call last):\r\n  File \"/share/storage1/TNG/onnxruntime/tools/ci_build/build.py\", line 1891, in <module>\r\n    sys.exit(main())\r\n  File \"/share/storage1/TNG/onnxruntime/tools/ci_build/build.py\", line 1780, in main\r\n    build_targets(args, cmake_path, build_dir, configs, args.parallel)\r\n  File \"/share/storage1/TNG/onnxruntime/tools/ci_build/build.py\", line 920, in build_targets\r\n    run_subprocess(cmd_args, env=env)\r\n  File \"/share/storage1/TNG/onnxruntime/tools/ci_build/build.py\", line 422, in run_subprocess\r\n    env=my_env, shell=shell)\r\n  File \"/share/storage1/TNG/Softwares/Anaconda3-2020.02/lib/python3.7/subprocess.py\", line 512, in run\r\n    output=stdout, stderr=stderr)\r\nsubprocess.CalledProcessError: Command '['/share/storage1/TNG/Softwares/Anaconda3-2020.02/bin/cmake', '--build', '/share/storage1/TNG/onnxruntime/build/Linux/RelWithDebInfo', '--config', 'RelWithDebInfo', '--', '-j32']' returned non-zero exit status 2.]0;\r\n```\r\n\r\nThe complete log file is attached at the end.\r\n\r\n**System information**\r\n- OS Platform and Distribution: CentOS Linux 7\r\n- ONNX Runtime installed from (source or binary): latest, from source\r\n- Python version: 3.7.7\r\n- GCC/Compiler version (if compiling from source): 5.3.0\r\n- OpenVINO version: 2020.03\r\n\r\n**To Reproduce**\r\n[log_onnxruntime.txt](https://github.com/microsoft/onnxruntime/files/4886266/log_onnxruntime.txt)\r\n\r\n1. source <openvino_install_directory>/bin/setupvars.sh\r\n2. git clone --recursive https://github.com/Microsoft/onnxruntime\r\n3. cd onnxruntime\r\n4. ./build.sh --config RelWithDebInfo --build_shared_lib --parallel --use_openvino CPU_FP32", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4431", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4431/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4431/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4431/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4431", "id": 651449163, "node_id": "MDU6SXNzdWU2NTE0NDkxNjM=", "number": 4431, "title": "Why is the onnx version down to 1.2 in the document docs/version.md that chapter is compatibility/version matrix?", "user": {"login": "zxgm", "id": 17983863, "node_id": "MDQ6VXNlcjE3OTgzODYz", "avatar_url": "https://avatars3.githubusercontent.com/u/17983863?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zxgm", "html_url": "https://github.com/zxgm", "followers_url": "https://api.github.com/users/zxgm/followers", "following_url": "https://api.github.com/users/zxgm/following{/other_user}", "gists_url": "https://api.github.com/users/zxgm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zxgm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zxgm/subscriptions", "organizations_url": "https://api.github.com/users/zxgm/orgs", "repos_url": "https://api.github.com/users/zxgm/repos", "events_url": "https://api.github.com/users/zxgm/events{/privacy}", "received_events_url": "https://api.github.com/users/zxgm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1311608287, "node_id": "MDU6TGFiZWwxMzExNjA4Mjg3", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:documentation", "name": "component:documentation", "color": "303a93", "default": false, "description": "related to documentation"}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-06T11:03:41Z", "updated_at": "2020-07-07T02:17:26Z", "closed_at": "2020-07-07T02:17:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Why is the onnx version down to 1.2 in the document docs/version.md that chapter is compatibility/version matrix?If i have pytorch pt model,i want to convert to onnx model,and use torch.onnx.export interface that set `opset_version=10`, what is the onnx release version ?What is the ONNX Runtime release version?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4430", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4430/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4430/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4430/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4430", "id": 651358330, "node_id": "MDU6SXNzdWU2NTEzNTgzMzA=", "number": 4430, "title": "ARMNN build failed due to signature change of CPUAllocator introduced by PR2850", "user": {"login": "clockfly", "id": 2595532, "node_id": "MDQ6VXNlcjI1OTU1MzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2595532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clockfly", "html_url": "https://github.com/clockfly", "followers_url": "https://api.github.com/users/clockfly/followers", "following_url": "https://api.github.com/users/clockfly/following{/other_user}", "gists_url": "https://api.github.com/users/clockfly/gists{/gist_id}", "starred_url": "https://api.github.com/users/clockfly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clockfly/subscriptions", "organizations_url": "https://api.github.com/users/clockfly/orgs", "repos_url": "https://api.github.com/users/clockfly/repos", "events_url": "https://api.github.com/users/clockfly/events{/privacy}", "received_events_url": "https://api.github.com/users/clockfly/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 2036741918, "node_id": "MDU6TGFiZWwyMDM2NzQxOTE4", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/ep:NNAPI", "name": "ep:NNAPI", "color": "bfdadc", "default": false, "description": "questions/issues related to NNAPI EP"}], "state": "closed", "locked": false, "assignee": {"login": "prabhat00155", "id": 7043157, "node_id": "MDQ6VXNlcjcwNDMxNTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7043157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prabhat00155", "html_url": "https://github.com/prabhat00155", "followers_url": "https://api.github.com/users/prabhat00155/followers", "following_url": "https://api.github.com/users/prabhat00155/following{/other_user}", "gists_url": "https://api.github.com/users/prabhat00155/gists{/gist_id}", "starred_url": "https://api.github.com/users/prabhat00155/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prabhat00155/subscriptions", "organizations_url": "https://api.github.com/users/prabhat00155/orgs", "repos_url": "https://api.github.com/users/prabhat00155/repos", "events_url": "https://api.github.com/users/prabhat00155/events{/privacy}", "received_events_url": "https://api.github.com/users/prabhat00155/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "prabhat00155", "id": 7043157, "node_id": "MDQ6VXNlcjcwNDMxNTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7043157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prabhat00155", "html_url": "https://github.com/prabhat00155", "followers_url": "https://api.github.com/users/prabhat00155/followers", "following_url": "https://api.github.com/users/prabhat00155/following{/other_user}", "gists_url": "https://api.github.com/users/prabhat00155/gists{/gist_id}", "starred_url": "https://api.github.com/users/prabhat00155/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prabhat00155/subscriptions", "organizations_url": "https://api.github.com/users/prabhat00155/orgs", "repos_url": "https://api.github.com/users/prabhat00155/repos", "events_url": "https://api.github.com/users/prabhat00155/events{/privacy}", "received_events_url": "https://api.github.com/users/prabhat00155/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-07-06T08:43:36Z", "updated_at": "2020-07-13T23:07:41Z", "closed_at": "2020-07-13T23:07:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nA clear and concise description of what the bug is.\r\n\r\nIn https://github.com/microsoft/onnxruntime/pull/2850, the CPUAllocator signature changed from\r\n\r\n```\r\nCPUAllocator(std::unique_ptr<OrtMemoryInfo> memory_info)\r\n```\r\n\r\nto \r\n\r\n```\r\n CPUAllocator(const OrtMemoryInfo& memory_info) : IDeviceAllocator(memory_info) {}\r\n```\r\n\r\nThis breaks armnn build,\r\n\r\n```\r\n/usr/include/c++/7/bits/unique_ptr.h: In instantiation of 'typename std::_MakeUniq<_Tp>::__single_object std::make_unique(_Args&& ...) [with _Tp = onnxruntime::CPUAllocator; _Args = {std::unique_ptr<OrtMemoryInfo, std::default_delete<OrtMemoryInfo> >}; typename std::_MakeUniq<_Tp>::__single_object = std::unique_ptr<onnxruntime::CPUAllocator, std::default_delete<onnxruntime::CPUAllocator> >]':\r\n/workspace/trtis-build/framework/onnxruntime/src/onnxruntime/onnxruntime/core/providers/armnn/armnn_execution_provider.cc:78:73:   required from here\r\n/usr/include/c++/7/bits/unique_ptr.h:821:30: error: no matching function for call to 'onnxruntime::CPUAllocator::CPUAllocator(std::unique_ptr<OrtMemoryInfo, std::default_delete<OrtMemoryInfo> >)'\r\n     { return unique_ptr<_Tp>(new _Tp(std::forward<_Args>(__args)...)); }\r\n                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from /workspace/trtis-build/framework/onnxruntime/src/onnxruntime/onnxruntime/core/framework/arena.h:9:0,\r\n                 from /workspace/trtis-build/framework/onnxruntime/src/onnxruntime/onnxruntime/core/framework/allocatormgr.h:7,\r\n                 from /workspace/trtis-build/framework/onnxruntime/src/onnxruntime/onnxruntime/core/providers/armnn/armnn_execution_provider.h:7,\r\n                 from /workspace/trtis-build/framework/onnxruntime/src/onnxruntime/onnxruntime/core/providers/armnn/armnn_execution_provider.cc:5:\r\n```\r\n\r\n**Urgency**\r\nnone\r\n\r\n**System information**\r\n- OS Platform and Distribution: ubuntu18 arm64v8\r\n- ONNX Runtime installed from (source or binary): source\r\n- ONNX Runtime version: master\r\n\r\n**To Reproduce**\r\n\r\narmnn provider build.\r\n\r\n**Expected behavior**\r\n\r\nbuild pass.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4428", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4428/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4428/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4428/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4428", "id": 651105059, "node_id": "MDU6SXNzdWU2NTExMDUwNTk=", "number": 4428, "title": "specify install location using build.sh", "user": {"login": "jinz2014", "id": 7799920, "node_id": "MDQ6VXNlcjc3OTk5MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/7799920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinz2014", "html_url": "https://github.com/jinz2014", "followers_url": "https://api.github.com/users/jinz2014/followers", "following_url": "https://api.github.com/users/jinz2014/following{/other_user}", "gists_url": "https://api.github.com/users/jinz2014/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinz2014/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinz2014/subscriptions", "organizations_url": "https://api.github.com/users/jinz2014/orgs", "repos_url": "https://api.github.com/users/jinz2014/repos", "events_url": "https://api.github.com/users/jinz2014/events{/privacy}", "received_events_url": "https://api.github.com/users/jinz2014/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2159809301, "node_id": "MDU6TGFiZWwyMTU5ODA5MzAx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/component:build", "name": "component:build", "color": "303a93", "default": false, "description": "related to builds"}, {"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-07-05T16:52:44Z", "updated_at": "2020-07-25T18:16:34Z", "closed_at": "2020-07-07T16:01:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Please advise how to specify a custom install location using build.sh. I don't have root permission. \r\n\r\nThank you\r\n\r\nInstall the project...\r\n-- Install configuration: \"RelWithDebInfo\"\r\n-- Installing: /usr/local/include/onnxruntime/core/common\r\nCMake Error at cmake_install.cmake:41 (file):\r\n  file INSTALL cannot make directory\r\n  \"/usr/local/include/onnxruntime/core/common\": No such file or directory\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4425", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4425/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4425/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4425/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4425", "id": 650781081, "node_id": "MDU6SXNzdWU2NTA3ODEwODE=", "number": 4425, "title": "Session.Run(inputs) returns null result", "user": {"login": "hanzigs", "id": 30790120, "node_id": "MDQ6VXNlcjMwNzkwMTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/30790120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hanzigs", "html_url": "https://github.com/hanzigs", "followers_url": "https://api.github.com/users/hanzigs/followers", "following_url": "https://api.github.com/users/hanzigs/following{/other_user}", "gists_url": "https://api.github.com/users/hanzigs/gists{/gist_id}", "starred_url": "https://api.github.com/users/hanzigs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hanzigs/subscriptions", "organizations_url": "https://api.github.com/users/hanzigs/orgs", "repos_url": "https://api.github.com/users/hanzigs/repos", "events_url": "https://api.github.com/users/hanzigs/events{/privacy}", "received_events_url": "https://api.github.com/users/hanzigs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 2185567573, "node_id": "MDU6TGFiZWwyMTg1NTY3NTcz", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/status:more-info-needed", "name": "status:more-info-needed", "color": "45b2d3", "default": false, "description": "more information is requested to continue investigation"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-03T22:17:56Z", "updated_at": "2020-07-07T02:26:01Z", "closed_at": "2020-07-07T02:26:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have this code for keras2onnx model prediction\r\n```\r\nvar Session = new InferenceSession(modelPath);\r\nfloat[] data = new float[] { 0.62382546f, -0.37804684f, -0.079906f, -0.45980235f, 0.34740723f, 0.00235355f, -0.29869863f,...};\r\nvar dimensions = new int[] { 1, 25 };\r\nTensor<float> tensorA = new DenseTensor<float>(data, dimensions); \r\nvar inputs = new List<NamedOnnxValue>();\r\nvar t1 = NamedOnnxValue.CreateFromTensor<float>(\"dense_4_input\", tensorA);          \r\ninputs.Add(t1);\r\nusing (IDisposableReadOnlyCollection<DisposableNamedOnnxValue> results = Session.Run(inputs))\r\n            {\r\n                Console.WriteLine(results);\r\n            }\r\n```\r\nresults value should be a float '0.014536', I get null, may I know my issue, Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4424", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4424/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4424/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4424/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4424", "id": 650733872, "node_id": "MDU6SXNzdWU2NTA3MzM4NzI=", "number": 4424, "title": "Session load fails with Protobuf serialization failed", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1122493985, "node_id": "MDU6TGFiZWwxMTIyNDkzOTg1", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:support", "name": "type:support", "color": "a2eeef", "default": false, "description": "need help or have question about ORT"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-03T19:03:00Z", "updated_at": "2020-07-20T22:18:36Z", "closed_at": "2020-07-20T22:18:35Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "**Describe the bug**\r\nExported a modified BERT model.\r\nWhen loading the model I get this error in the log:\r\n\r\n> [libprotobuf ERROR /onnxruntime_src/cmake/external/protobuf/src/google/protobuf/message_lite.cc:379] onnx.ModelProto exceeded maximum protobuf size of 2GB: 2192699607\r\n\r\nAnd then this is the exception:\r\n\r\n> InvalidProtobuf: [ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Protobuf serialization failed.\r\n\r\n**Urgency**\r\nNeeded for new captioning model\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- ONNX Runtime installed from (source or binary): pip\r\n- ONNX Runtime version: 1.2.0\r\n- Python version: 3.6.9\r\n- Visual Studio version (if applicable):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**To Reproduce**\r\n- Describe steps/code to reproduce the behavior.\r\n\r\n```python\r\n    sess_options = rt.SessionOptions()\r\n    sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_BASIC\r\n    sess_options.optimized_model_filepath = onnxfile_optimized\r\n    sess = rt.InferenceSession(onnxfile, sess_options)\r\n```\r\n\r\nresults in \r\n\r\n> InvalidProtobuf: [ONNXRuntimeError] : 7 : INVALID_PROTOBUF : Protobuf serialization failed.\r\n\r\nNormally loading the session however does not show that error:\r\n\r\n```python\r\nsess = rt.InferenceSession(onnxfile)\r\n```\r\n\r\n- Attach the ONNX model to the issue (where applicable) to expedite investigation.\r\nWill send the model\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nThis is BERT but with some changes\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4423", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4423/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4423/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4423/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4423", "id": 650719066, "node_id": "MDU6SXNzdWU2NTA3MTkwNjY=", "number": 4423, "title": "Unable to Perform Inference using ONNX RUNTIME. Invalid Argument Error", "user": {"login": "CosmicHunter", "id": 52317137, "node_id": "MDQ6VXNlcjUyMzE3MTM3", "avatar_url": "https://avatars2.githubusercontent.com/u/52317137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CosmicHunter", "html_url": "https://github.com/CosmicHunter", "followers_url": "https://api.github.com/users/CosmicHunter/followers", "following_url": "https://api.github.com/users/CosmicHunter/following{/other_user}", "gists_url": "https://api.github.com/users/CosmicHunter/gists{/gist_id}", "starred_url": "https://api.github.com/users/CosmicHunter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CosmicHunter/subscriptions", "organizations_url": "https://api.github.com/users/CosmicHunter/orgs", "repos_url": "https://api.github.com/users/CosmicHunter/repos", "events_url": "https://api.github.com/users/CosmicHunter/events{/privacy}", "received_events_url": "https://api.github.com/users/CosmicHunter/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-03T18:10:05Z", "updated_at": "2020-07-09T11:35:59Z", "closed_at": "2020-07-09T11:35:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI have exported a LSTM model from pytorch to onnx . The model takes sequences of length 200. It has hidden state size 256 , number of layers = 2.The forward function takes input size of (batches , sequencelength) as input along with a tuple consisting of hidden state and cell state. I am getting an error while inferencing the model with onnx runtime. hidden state and cell state dimensions are same. \r\n\r\n**Expected behavior**\r\n~~~\r\nioio1 = np.random.rand(1,200)\r\nioio2 = np.zeros((2,1,256),dtype = np.float)\r\npred = runtime_session.run([output_name],{runtime_session.get_inputs()[0].name:ioio1,\r\n                                          runtime_session.get_inputs()[1].name :ioio2,\r\n                                          runtime_session.get_inputs()[2].name : ioio2})\r\n~~~\r\n\r\n~~~ \r\nERROR\r\n\r\nInvalidArgument                           Traceback (most recent call last)\r\n<ipython-input-204-3928823f661e> in <module>()\r\n      1 pred = runtime_session.run([output_name],{runtime_session.get_inputs()[0].name:ioio1,\r\n      2                                           runtime_session.get_inputs()[1].name :ioio2,\r\n----> 3                                           runtime_session.get_inputs()[2].name : ioio2})\r\n\r\n/usr/local/lib/python3.6/dist-packages/onnxruntime/capi/session.py in run(self, output_names, input_feed, run_options)\r\n    109             output_names = [output.name for output in self._outputs_meta]\r\n    110         try:\r\n--> 111             return self._sess.run(output_names, input_feed, run_options)\r\n    112         except C.EPFail as err:\r\n    113             if self._enable_fallback:\r\n\r\nInvalidArgument: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Unexpected input data type. Actual: (N11onnxruntime17PrimitiveDataTypeIdEE) , expected: (N11onnxruntime17PrimitiveDataTypeIlEE)\r\n\r\n~~~\r\n\r\nI have exported the model like this \r\n\r\n~~~\r\ninput_names = [ \"input1\", \"hidden\"]\r\noutput_names = [ \"output\" ]\r\ndummy_input = torch.autograd.Variable(torch.ones(1,200,dtype=torch.long))\r\ndummy_h = torch.autograd.Variable(torch.zeros(2,1,256,dtype = torch.float))\r\ndummy_h = (dummy_h,dummy_h)\r\n\r\ntorch.onnx.export(net ,(dummy_input ,dummy_h),\"lstm_model_5_test.onnx\",input_names = input_names ,output_names = output_names,verbose=True,export_params=True)\r\n~~~\r\n\r\nOn exporting the model the conversion is like the below screenshot\r\n\r\n![exportouts](https://user-images.githubusercontent.com/52317137/86490822-6e2e0b00-bd86-11ea-9a46-88f1a30cb154.PNG)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4422", "repository_url": "https://api.github.com/repos/microsoft/onnxruntime", "labels_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4422/labels{/name}", "comments_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4422/comments", "events_url": "https://api.github.com/repos/microsoft/onnxruntime/issues/4422/events", "html_url": "https://github.com/microsoft/onnxruntime/issues/4422", "id": 650704302, "node_id": "MDU6SXNzdWU2NTA3MDQzMDI=", "number": 4422, "title": "Improve support for tensor element access (C/C++ API)", "user": {"login": "jgbradley1", "id": 654554, "node_id": "MDQ6VXNlcjY1NDU1NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/654554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgbradley1", "html_url": "https://github.com/jgbradley1", "followers_url": "https://api.github.com/users/jgbradley1/followers", "following_url": "https://api.github.com/users/jgbradley1/following{/other_user}", "gists_url": "https://api.github.com/users/jgbradley1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgbradley1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgbradley1/subscriptions", "organizations_url": "https://api.github.com/users/jgbradley1/orgs", "repos_url": "https://api.github.com/users/jgbradley1/repos", "events_url": "https://api.github.com/users/jgbradley1/events{/privacy}", "received_events_url": "https://api.github.com/users/jgbradley1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1395147764, "node_id": "MDU6TGFiZWwxMzk1MTQ3NzY0", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/api:C/C++", "name": "api:C/C++", "color": "0e8a16", "default": false, "description": "related to the  public API(and ABI) for onnxruntime"}, {"id": 1267486731, "node_id": "MDU6TGFiZWwxMjY3NDg2NzMx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/status:contributions-welcome", "name": "status:contributions-welcome", "color": "45b2d3", "default": false, "description": "contributions welcome; lower priority items for the core teams on this project"}, {"id": 1122493981, "node_id": "MDU6TGFiZWwxMTIyNDkzOTgx", "url": "https://api.github.com/repos/microsoft/onnxruntime/labels/type:enhancement", "name": "type:enhancement", "color": "a2eeef", "default": false, "description": "request for unsupported feature or enhancement"}], "state": "closed", "locked": false, "assignee": {"login": "jgbradley1", "id": 654554, "node_id": "MDQ6VXNlcjY1NDU1NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/654554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgbradley1", "html_url": "https://github.com/jgbradley1", "followers_url": "https://api.github.com/users/jgbradley1/followers", "following_url": "https://api.github.com/users/jgbradley1/following{/other_user}", "gists_url": "https://api.github.com/users/jgbradley1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgbradley1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgbradley1/subscriptions", "organizations_url": "https://api.github.com/users/jgbradley1/orgs", "repos_url": "https://api.github.com/users/jgbradley1/repos", "events_url": "https://api.github.com/users/jgbradley1/events{/privacy}", "received_events_url": "https://api.github.com/users/jgbradley1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jgbradley1", "id": 654554, "node_id": "MDQ6VXNlcjY1NDU1NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/654554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgbradley1", "html_url": "https://github.com/jgbradley1", "followers_url": "https://api.github.com/users/jgbradley1/followers", "following_url": "https://api.github.com/users/jgbradley1/following{/other_user}", "gists_url": "https://api.github.com/users/jgbradley1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgbradley1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgbradley1/subscriptions", "organizations_url": "https://api.github.com/users/jgbradley1/orgs", "repos_url": "https://api.github.com/users/jgbradley1/repos", "events_url": "https://api.github.com/users/jgbradley1/events{/privacy}", "received_events_url": "https://api.github.com/users/jgbradley1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-07-03T17:20:09Z", "updated_at": "2020-08-12T01:34:05Z", "closed_at": "2020-08-12T01:34:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently after a tensor has been created with one of the `Ort::Value::CreateTensor()` functions, there is no easy way to check individual elements in a multidimensional tensor to verify the data blob has mapped correctly to an ORT tensor. For example, in [this](https://github.com/microsoft/onnxruntime/issues/2832) issue, the user takes it on faith that mapping OpenCV data to an ORT tensor _just works_ (assuming row-major order of the data).\r\n\r\nI would like to propose a new helper function be added to the C/C++ API that would improve unit testing and inspection of ORT tensors.\r\n\r\n**System information**\r\n- ONNX Runtime version (you are using): master branch\r\n\r\n**Describe the solution you'd like**\r\nPerhaps a function called `At()` that could be used in the following manner:\r\n\r\n```cpp\r\nauto tensor = Ort::Value::CreateTensor(...)\r\n\r\n// allow the user to write unit tests like below\r\nfor(size_t row=0; row<row_count; row++) {\r\n  for(size_t col=0; col<col_count; col++) {\r\n    for(size_t channel=0; channel< 3; channel++) {\r\n      assert( tensor.At<float>({row, col, channel}) == imagedata(row, col, channel) ) // element access would probably be declared as an initializer_list\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nObviously `At()` would extend to any multidimensional tensor access. I only used an image example for clarity.", "performed_via_github_app": null, "score": 1.0}]}