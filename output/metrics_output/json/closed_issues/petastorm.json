{"total_count": 101, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/uber/petastorm/issues/583", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/583/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/583/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/583/events", "html_url": "https://github.com/uber/petastorm/issues/583", "id": 680848250, "node_id": "MDU6SXNzdWU2ODA4NDgyNTA=", "number": 583, "title": "Error running the generate_petastorm_dataset example", "user": {"login": "HannesZei", "id": 69667221, "node_id": "MDQ6VXNlcjY5NjY3MjIx", "avatar_url": "https://avatars3.githubusercontent.com/u/69667221?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HannesZei", "html_url": "https://github.com/HannesZei", "followers_url": "https://api.github.com/users/HannesZei/followers", "following_url": "https://api.github.com/users/HannesZei/following{/other_user}", "gists_url": "https://api.github.com/users/HannesZei/gists{/gist_id}", "starred_url": "https://api.github.com/users/HannesZei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HannesZei/subscriptions", "organizations_url": "https://api.github.com/users/HannesZei/orgs", "repos_url": "https://api.github.com/users/HannesZei/repos", "events_url": "https://api.github.com/users/HannesZei/events{/privacy}", "received_events_url": "https://api.github.com/users/HannesZei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-18T09:34:03Z", "updated_at": "2020-08-19T16:01:18Z", "closed_at": "2020-08-19T16:01:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I followed the instructions of the installation and tried the example from petastorm/examples/hello_world/petastorm_dataset/generate_petastorm_dataset.py \r\nAt first i got this error: \"Failed to locate the winutils binary in the hadoop binary path\"\r\nTo solve this i downloaded hadoop and set the System Environment Variable to the folder.\r\nAnd this did solve the error, however there is another error which i don't quite understand.\r\n\r\nThe files at FILENAME\\_temporary\\0\\_temporary\\attempt_...  are generated and sometimes i get an additional error that those cannot be deleted.\r\n\r\nIs this a known problem? Are there solutions to it or did i maybe missed some step during installation?\r\n\r\n\r\n```\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/Users/Username/AppData/Local/Programs/Python/Python38/Lib/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\r\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\n20/08/18 11:22:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n20/08/18 11:22:44 ERROR Utils: Aborting task                        (0 + 2) / 2]\r\norg.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:643)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: java.io.EOFException\r\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n        ... 22 more\r\n20/08/18 11:22:45 ERROR FileFormatWriter: Job job_20200818112241_0000 aborted.\r\n20/08/18 11:22:45 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\r\norg.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:643)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\r\n        ... 9 more\r\nCaused by: java.io.EOFException\r\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n        ... 22 more\r\n20/08/18 11:22:45 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, DESKTOP-JJJJ2NO, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:830)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:643)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:272)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\r\n        ... 9 more\r\nCaused by: java.io.EOFException\r\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n        ... 22 more\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/570", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/570/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/570/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/570/events", "html_url": "https://github.com/uber/petastorm/issues/570", "id": 657627722, "node_id": "MDU6SXNzdWU2NTc2Mjc3MjI=", "number": 570, "title": "Guidance on How to Tune BatchedDataLoader", "user": {"login": "andrewredd", "id": 48295430, "node_id": "MDQ6VXNlcjQ4Mjk1NDMw", "avatar_url": "https://avatars1.githubusercontent.com/u/48295430?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewredd", "html_url": "https://github.com/andrewredd", "followers_url": "https://api.github.com/users/andrewredd/followers", "following_url": "https://api.github.com/users/andrewredd/following{/other_user}", "gists_url": "https://api.github.com/users/andrewredd/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewredd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewredd/subscriptions", "organizations_url": "https://api.github.com/users/andrewredd/orgs", "repos_url": "https://api.github.com/users/andrewredd/repos", "events_url": "https://api.github.com/users/andrewredd/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewredd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-07-15T20:20:33Z", "updated_at": "2020-07-28T20:56:09Z", "closed_at": "2020-07-28T20:56:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi All,\r\n\r\nI tried reading the docs, issues, and pull requests and haven't felt like I have a good handle on how to optimize a dataloader in pytorch. I'm using Pytorch 1.5, Cloudera HDFS, Nvidia Tesla V100, have 80 cores and 700 gb of memory. \r\n\r\nI'm trying to read a smallish table of 36M rows with ~ 2000 values per row (they are grouped into five array columns). I've made sure the parquet block size is ~256MB\r\n\r\nThis is what I have for code:\r\n\r\n`    dl = BatchedDataLoader(make_batch_reader(file_name, \r\n                                             workers_count=10, \r\n                                             schema_fields=col_names, \r\n                                             hdfs_driver='libhdfs',\r\n                                             reader_pool_type = 'process'), \r\n                           batch_size=batch_size, \r\n                           shuffling_queue_capacity=shuffling_queue_capacity)`\r\n\r\nProblem:\r\nI'm trying to figure out how to keep a single GPU running at full memory capacity from the cluster. The setup above is majorly IO bound. In very basic tuning that it seems like I can get through an epoch faster using a large batch size (500K) and a shuffling_queue_capacity of 10x that. Even with the large batch size I only use up half of the GPU memory 17GB/32GB. This setup seems to process a batch every ~20secs, but then the GPU sits idle for ~30secs as it waits for another batch (I wish I knew how to benchmark all this better) I've experimented with larger and smaller shuffle queues but it seems to burn through the queue super fast and then be bound by IO out of the cluster.\r\n\r\nI've tried a much smaller batch size (64) and petastorm cranks through the batches constantly but the GPU memory is minimally used.\r\n\r\nQuestions: \r\n1) Are large batches the best way to get high throughout out of the cluster?\r\n2) Do I need to add more workers? I've increased the amount to 50 and haven't seen a improvement.\r\n3) I read that the 'process' pool type is superior because I don't use any image c++ -like processing. Is this the right approach?\r\n4) When I added the transform_fn to the batched data loader I'd almost immediately overflowed my gpu. Should I be making my batches smaller if I take this approach?\r\n\r\nWould love to contribute a tutorial when we figure this out if that would be helpful.\r\n\r\nThe next step would be to put this on a much larger multi GPU machine. I need to figure out these throughput questions first. \r\n\r\nThanks for the great work!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/551", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/551/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/551/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/551/events", "html_url": "https://github.com/uber/petastorm/issues/551", "id": 607692993, "node_id": "MDU6SXNzdWU2MDc2OTI5OTM=", "number": 551, "title": "Incorrect order of row groups when reading", "user": {"login": "hig-dev", "id": 7386016, "node_id": "MDQ6VXNlcjczODYwMTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7386016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hig-dev", "html_url": "https://github.com/hig-dev", "followers_url": "https://api.github.com/users/hig-dev/followers", "following_url": "https://api.github.com/users/hig-dev/following{/other_user}", "gists_url": "https://api.github.com/users/hig-dev/gists{/gist_id}", "starred_url": "https://api.github.com/users/hig-dev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hig-dev/subscriptions", "organizations_url": "https://api.github.com/users/hig-dev/orgs", "repos_url": "https://api.github.com/users/hig-dev/repos", "events_url": "https://api.github.com/users/hig-dev/events{/privacy}", "received_events_url": "https://api.github.com/users/hig-dev/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-27T16:41:51Z", "updated_at": "2020-05-06T20:31:26Z", "closed_at": "2020-05-06T20:31:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "My goal is to read the created dataset in the order in which I generated the rows. However if the row group size is set to a value lower than the total size of the dataset, the order when reading the dataset is wrong, despite setting shuffle_row_groups=False.\r\n\r\nPlease look at this demonstration of this problem. I would expect that the exception does not occur.\r\n\r\n```\r\nimport pathlib\r\nimport numpy as np\r\nfrom petastorm import make_reader\r\nfrom petastorm.codecs import ScalarCodec\r\n\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nfrom petastorm.unischema import Unischema, UnischemaField, dict_to_spark_row\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.types import LongType\r\n\r\noutput_directory = pathlib.Path('./_generated_demo_data')\r\noutput_url = output_directory.resolve().as_uri()\r\n\r\nsession_builder = SparkSession \\\r\n    .builder \\\r\n    .appName('Demo')\r\n\r\nspark = session_builder.getOrCreate()\r\nsc = spark.sparkContext\r\n\r\nschema = Unischema('DemoSchema', [\r\n        UnischemaField('timestamp', np.uint64, (), ScalarCodec(LongType()), False),\r\n    ])\r\n\r\n# Generate petastorm with timestamps in order\r\nwith materialize_dataset(spark, output_url, schema, row_group_size_mb=1):\r\n    generator = enumerate(range(1000000))\r\n    rows_dict_generator = map(lambda x: {'timestamp': x[0]}, generator)\r\n    rows_spark_generator = map(lambda x: dict_to_spark_row(schema, x), rows_dict_generator)\r\n    rows_rdd = sc.parallelize(rows_spark_generator)\r\n\r\n    spark.createDataFrame(rows_rdd, schema.as_spark_schema()) \\\r\n        .coalesce(1) \\\r\n        .write \\\r\n        .mode('overwrite') \\\r\n        .parquet(output_url)\r\n\r\n# Read generated petastorm and check timestamps ordering\r\nlast_timestamp = -float(\"inf\")\r\nwith make_reader(output_url,\r\n                 schema_fields=['timestamp'],\r\n                 shuffle_row_groups=False) as reader:\r\n    for row in reader:\r\n        # ensure timestamp ordering or num_epochs handling\r\n        if row.timestamp < last_timestamp:\r\n            raise Exception('Timestamps in petastorm are not in order!')\r\n\r\n        last_timestamp = row.timestamp\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/547", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/547/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/547/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/547/events", "html_url": "https://github.com/uber/petastorm/issues/547", "id": 603178530, "node_id": "MDU6SXNzdWU2MDMxNzg1MzA=", "number": 547, "title": "Non deterministic fail during model training", "user": {"login": "sonNeturo", "id": 45131840, "node_id": "MDQ6VXNlcjQ1MTMxODQw", "avatar_url": "https://avatars1.githubusercontent.com/u/45131840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sonNeturo", "html_url": "https://github.com/sonNeturo", "followers_url": "https://api.github.com/users/sonNeturo/followers", "following_url": "https://api.github.com/users/sonNeturo/following{/other_user}", "gists_url": "https://api.github.com/users/sonNeturo/gists{/gist_id}", "starred_url": "https://api.github.com/users/sonNeturo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sonNeturo/subscriptions", "organizations_url": "https://api.github.com/users/sonNeturo/orgs", "repos_url": "https://api.github.com/users/sonNeturo/repos", "events_url": "https://api.github.com/users/sonNeturo/events{/privacy}", "received_events_url": "https://api.github.com/users/sonNeturo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-20T11:51:28Z", "updated_at": "2020-04-26T21:08:11Z", "closed_at": "2020-04-26T21:08:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "When training a model, it randomly fails with following error message:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/tmp/3e264757a0e444669399e145e55ba71d/training.py\", line 201, in <module>\r\n    test_freq=1,\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/offline_audience/lib/utils.py\", line 205, in fit_with_petastorm\r\n    test_epoch(model, test_path, fields_to_parse, load_batch, **kwargs)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/offline_audience/lib/utils.py\", line 258, in test_epoch\r\n    make_batch_reader(test_path, schema_fields=fields_to_parse, num_epochs=1),\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/petastorm/reader.py\", line 289, in make_batch_reader\r\n    is_batched_reader=True)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/petastorm/reader.py\", line 394, in __init__\r\n    row_groups = dataset_metadata.load_row_groups(self.dataset)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 244, in load_row_groups\r\n    return _split_row_groups_from_footers(dataset)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 334, in _split_row_groups_from_footers\r\n    result = [item for f in futures_list for item in f.result()]\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 334, in <listcomp>\r\n    result = [item for f in futures_list for item in f.result()]\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/concurrent/futures/_base.py\", line 432, in result\r\n    return self.__get_result()\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/concurrent/futures/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/concurrent/futures/thread.py\", line 56, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 316, in _split_piece\r\n    metadata = compat_get_metadata(piece, fs_open)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/petastorm/compat.py\", line 31, in compat_get_metadata\r\n    arrow_metadata = piece.get_metadata()\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 502, in get_metadata\r\n    f = self.open()\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 520, in open\r\n    reader = self.open_file_func(self.path)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1062, in open_file\r\n    common_metadata=self.common_metadata)\r\n  File \"/opt/conda/miniconda3/envs/myenv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 130, in __init__\r\n    self.reader.open(source, use_memory_map=memory_map, metadata=metadata)\r\n  File \"pyarrow/_parquet.pyx\", line 655, in pyarrow._parquet.ParquetReader.open\r\n  File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIOError: Arrow error: IOError: b''\r\n```\r\n\r\nThe error happens randomly. Sometimes an epoch finishes without an issue, but fails in the next one. My code is pretty simple:\r\n\r\n```python\r\n    with DataLoader(\r\n        make_batch_reader(train_path, schema_fields=fields_to_parse, num_epochs=1),\r\n        batch_size=kwargs[\"train_batch_size\"],\r\n    ) as train_loader:\r\n        device = kwargs[\"device\"]\r\n        for batch_data in tqdm(train_loader, total=n_batches):\r\n            x, y = load_batch(batch_data)\r\n            x = x.to(device)\r\n            y = y.to(device)\r\n            y_pred = model(x).squeeze()\r\n\r\n            model.optim.zero_grad()\r\n            loss = model.loss_func(y_pred, y.squeeze(), reduction=\"sum\")\r\n\r\n            loss_epoch += loss.item()\r\n            loss.backward()\r\n            model.optim.step()\r\n```\r\n\r\nCould this error be due to the distributed settings? I'am using a dataproc cluster with 20 workers and a GPU.\r\n\r\nversions:\r\n* petastorm: 0.82\r\n* pytorch: 1.4\r\n* pyarrow: tested with 0.13 and 0.16\r\n\r\nThanks for the help. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/527", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/527/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/527/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/527/events", "html_url": "https://github.com/uber/petastorm/issues/527", "id": 591720341, "node_id": "MDU6SXNzdWU1OTE3MjAzNDE=", "number": 527, "title": "IndexError: list index out of range", "user": {"login": "danielhaviv", "id": 11798914, "node_id": "MDQ6VXNlcjExNzk4OTE0", "avatar_url": "https://avatars2.githubusercontent.com/u/11798914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielhaviv", "html_url": "https://github.com/danielhaviv", "followers_url": "https://api.github.com/users/danielhaviv/followers", "following_url": "https://api.github.com/users/danielhaviv/following{/other_user}", "gists_url": "https://api.github.com/users/danielhaviv/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielhaviv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielhaviv/subscriptions", "organizations_url": "https://api.github.com/users/danielhaviv/orgs", "repos_url": "https://api.github.com/users/danielhaviv/repos", "events_url": "https://api.github.com/users/danielhaviv/events{/privacy}", "received_events_url": "https://api.github.com/users/danielhaviv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-01T08:20:22Z", "updated_at": "2020-04-01T15:42:28Z", "closed_at": "2020-04-01T15:42:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to run the code examples to understand how exactly to use Petastorm with PyTorch but the [generate_petastorm_mnist.py](https://github.com/uber/petastorm/blob/master/examples/mnist/generate_petastorm_mnist.py) script fails with the following error:\r\n`IndexError: list index out of range`\r\n\r\n\r\nThis is the stack trace:\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<command-6237180> in <module>\r\n      1 # Make a temp dir that we'll clean up afterward\r\n      2 download_dir = tempfile.mkdtemp()\r\n----> 3 mnist_data_to_petastorm_dataset(download_dir, \"file:///tmp/daniel.haviv@databricks.com/mnistps\")\r\n      4 \r\n      5 if args.download_dir is None:\r\n\r\n<command-6237171> in mnist_data_to_petastorm_dataset(download_dir, output_url, spark_master, parquet_files_count, mnist_data)\r\n    129                 .write \\\r\n    130                 .option('compression', 'none') \\\r\n--> 131                 .parquet(dset_output_url)\r\n    132 \r\n    133 \r\n\r\n/databricks/python/lib/python3.7/contextlib.py in __exit__(self, type, value, traceback)\r\n    117         if type is None:\r\n    118             try:\r\n--> 119                 next(self.gen)\r\n    120             except StopIteration:\r\n    121                 return False\r\n\r\n/databricks/python/lib/python3.7/site-packages/petastorm/etl/dataset_metadata.py in materialize_dataset(spark, dataset_url, schema, row_group_size_mb, use_summary_metadata, filesystem_factory)\r\n    110         validate_schema=False)\r\n    111 \r\n--> 112     _generate_unischema_metadata(dataset, schema)\r\n    113     if not use_summary_metadata:\r\n    114         _generate_num_row_groups_per_file(dataset, spark.sparkContext, filesystem_factory)\r\n\r\n/databricks/python/lib/python3.7/site-packages/petastorm/etl/dataset_metadata.py in _generate_unischema_metadata(dataset, schema)\r\n    190     assert schema\r\n    191     serialized_schema = pickle.dumps(schema)\r\n--> 192     utils.add_to_dataset_metadata(dataset, UNISCHEMA_KEY, serialized_schema)\r\n    193 \r\n    194 \r\n\r\n/databricks/python/lib/python3.7/site-packages/petastorm/utils.py in add_to_dataset_metadata(dataset, key, value)\r\n    113             arrow_metadata = pyarrow.parquet.read_metadata(f)\r\n    114     else:\r\n--> 115         arrow_metadata = compat_get_metadata(dataset.pieces[0], dataset.fs.open)\r\n    116 \r\n    117     base_schema = arrow_metadata.schema.to_arrow_schema()\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/523", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/523/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/523/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/523/events", "html_url": "https://github.com/uber/petastorm/issues/523", "id": 589198573, "node_id": "MDU6SXNzdWU1ODkxOTg1NzM=", "number": 523, "title": "Error while using pytorch dataloader with petastorm", "user": {"login": "nikitamehrotra12", "id": 20640205, "node_id": "MDQ6VXNlcjIwNjQwMjA1", "avatar_url": "https://avatars1.githubusercontent.com/u/20640205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikitamehrotra12", "html_url": "https://github.com/nikitamehrotra12", "followers_url": "https://api.github.com/users/nikitamehrotra12/followers", "following_url": "https://api.github.com/users/nikitamehrotra12/following{/other_user}", "gists_url": "https://api.github.com/users/nikitamehrotra12/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikitamehrotra12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikitamehrotra12/subscriptions", "organizations_url": "https://api.github.com/users/nikitamehrotra12/orgs", "repos_url": "https://api.github.com/users/nikitamehrotra12/repos", "events_url": "https://api.github.com/users/nikitamehrotra12/events{/privacy}", "received_events_url": "https://api.github.com/users/nikitamehrotra12/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-27T15:06:40Z", "updated_at": "2020-03-31T17:26:37Z", "closed_at": "2020-03-31T17:26:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nCan we use petastorm pytorch to load float array. I tried doing the same but got the following error\r\n\r\n![image](https://user-images.githubusercontent.com/20640205/77770036-a4167180-706a-11ea-9b8f-dcc979ee7cf8.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/515", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/515/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/515/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/515/events", "html_url": "https://github.com/uber/petastorm/issues/515", "id": 586223888, "node_id": "MDU6SXNzdWU1ODYyMjM4ODg=", "number": 515, "title": "_common_metadata file gets corrupted", "user": {"login": "filipski", "id": 1301017, "node_id": "MDQ6VXNlcjEzMDEwMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1301017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/filipski", "html_url": "https://github.com/filipski", "followers_url": "https://api.github.com/users/filipski/followers", "following_url": "https://api.github.com/users/filipski/following{/other_user}", "gists_url": "https://api.github.com/users/filipski/gists{/gist_id}", "starred_url": "https://api.github.com/users/filipski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/filipski/subscriptions", "organizations_url": "https://api.github.com/users/filipski/orgs", "repos_url": "https://api.github.com/users/filipski/repos", "events_url": "https://api.github.com/users/filipski/events{/privacy}", "received_events_url": "https://api.github.com/users/filipski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2020-03-23T13:48:50Z", "updated_at": "2020-04-02T07:50:08Z", "closed_at": "2020-04-02T06:32:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "For some reason the _common_metadata file in my data set has been corrupted. Instead of proper parquet content, it has just 4 bytes (zipped here just due to GitHub limitation for attaching files):\r\n[_common_metadata.zip](https://github.com/uber/petastorm/files/4369339/_common_metadata.zip)\r\nEvery time I try to append to the set now, I get `pyarrow.lib.ArrowIOError: Invalid Parquet file size is 4 bytes, smaller than standard file footer (8 bytes)` exception:\r\n```\r\n2020-03-23 14:33:07,177 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\nTraceback (most recent call last):\r\n  File \"./test_ingest.py\", line 75, in <module>\r\n    main()\r\n  File \"./test_ingest.py\", line 71, in main\r\n    ingest_folder(\"../../annotated_data/ideon_val/images\", spark)\r\n  File \"./test_ingest.py\", line 62, in ingest_folder\r\n    print('DONE')\r\n  File \"/home/hduser/miniconda3/envs/petastorm/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"/home/hduser/projects/petastorm_ingest/petastorm/etl/dataset_metadata.py\", line 113, in materialize_dataset\r\n    validate_schema=False)\r\n  File \"/home/hduser/miniconda3/envs/petastorm/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1037, in __init__\r\n    memory_map=memory_map\r\n  File \"/home/hduser/miniconda3/envs/petastorm/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1503, in read_metadata\r\n    return ParquetFile(where, memory_map=memory_map).metadata\r\n  File \"/home/hduser/miniconda3/envs/petastorm/lib/python3.6/site-packages/pyarrow/parquet.py\", line 137, in __init__\r\n    read_dictionary=read_dictionary, metadata=metadata)\r\n  File \"pyarrow/_parquet.pyx\", line 1036, in pyarrow._parquet.ParquetReader.open\r\n  File \"pyarrow/error.pxi\", line 80, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIOError: Invalid Parquet file size is 4 bytes, smaller than standard file footer (8 bytes)\r\n```\r\n\r\nAny idea why did my metadata got corrupted? Have you experienced the same? How to prevent this? How to fix that broken metadata for the existing data set?\r\n\r\n**The code**\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nimport os, sys\r\nimport numpy as np\r\nfrom glob import glob\r\nfrom datetime import date\r\n\r\nimport cv2\r\n\r\nfrom pyspark.sql import SparkSession, Row\r\nfrom pyspark.sql.types import StructField, StructType, IntegerType, BinaryType, StringType, TimestampType, DateType\r\n\r\nfrom petastorm.fs_utils import FilesystemResolver\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\r\nfrom petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\r\n\r\nROWGROUP_SIZE_MB = 128 # The same as the default HDFS block size\r\n\r\n# The schema defines how the dataset schema looks like\r\nImageSchema = Unischema('ImageSchema', [\r\n    UnischemaField('date', np.datetime64, (), ScalarCodec(DateType()), False),\r\n    UnischemaField('dataset_name', np.string_, (), ScalarCodec(StringType()), False),\r\n    UnischemaField('path', np.string_, (), ScalarCodec(StringType()), False),\r\n    UnischemaField('image', np.uint8, (1080, 1280, 3), CompressedImageCodec('png'), False)\r\n])\r\n\r\n\r\n#output_url = \"file:///tmp/petastorm_ingest_test\"\r\noutput_url = \"hdfs:///data/petastorm_ingestion_tests\"\r\n\r\ndef ingest_folder(images_folder, spark):\r\n\r\n    # List all images in the folder\r\n    image_files = sorted(glob(os.path.join(images_folder, \"*.png\")))\r\n\r\n    # Switching from default libdfs3 to libdfs as the former crashes while storing metadata\r\n    resolver=FilesystemResolver(output_url, spark.sparkContext._jsc.hadoopConfiguration(), hdfs_driver='libhdfs')\r\n    with materialize_dataset(spark, output_url, ImageSchema, ROWGROUP_SIZE_MB, filesystem_factory=resolver.filesystem_factory()):\r\n    #with materialize_dataset(spark, output_url, ImageSchema, ROWGROUP_SIZE_MB):\r\n\r\n        print('STAGE 1')\r\n        input_rdd = spark.sparkContext.parallelize(image_files) \\\r\n            .map(lambda image_path:\r\n                    {ImageSchema.date.name: date.today(),\r\n                     ImageSchema.dataset_name.name: 'sample_set',\r\n                     ImageSchema.path.name: image_path,\r\n                     ImageSchema.image.name: cv2.imread(image_path)})\r\n\r\n        print('STAGE 2')\r\n        rows_rdd = input_rdd.map(lambda r: dict_to_spark_row(ImageSchema, r))\r\n\r\n        print('STAGE 3')\r\n\r\n        spark.createDataFrame(rows_rdd, ImageSchema.as_spark_schema()) \\\r\n                .repartition(1) \\\r\n                .write \\\r\n                .mode('append') \\\r\n                .partitionBy('date', 'dataset_name') \\\r\n                .option('compression', 'none') \\\r\n                .parquet(output_url)\r\n        print('DONE')\r\n\r\ndef main():\r\n\r\n    # Start the Spark session\r\n    spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[*]').getOrCreate()    \r\n    sc = spark.sparkContext\r\n\r\n    # Ingest images and annotations for a given folder\r\n    ingest_folder(\"../images\", spark)\r\n    print('Finished')\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/502", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/502/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/502/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/502/events", "html_url": "https://github.com/uber/petastorm/issues/502", "id": 576232404, "node_id": "MDU6SXNzdWU1NzYyMzI0MDQ=", "number": 502, "title": "Crash (segmentation fault) when storing data to HDFS", "user": {"login": "filipski", "id": 1301017, "node_id": "MDQ6VXNlcjEzMDEwMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1301017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/filipski", "html_url": "https://github.com/filipski", "followers_url": "https://api.github.com/users/filipski/followers", "following_url": "https://api.github.com/users/filipski/following{/other_user}", "gists_url": "https://api.github.com/users/filipski/gists{/gist_id}", "starred_url": "https://api.github.com/users/filipski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/filipski/subscriptions", "organizations_url": "https://api.github.com/users/filipski/orgs", "repos_url": "https://api.github.com/users/filipski/repos", "events_url": "https://api.github.com/users/filipski/events{/privacy}", "received_events_url": "https://api.github.com/users/filipski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-05T12:53:37Z", "updated_at": "2020-03-06T18:04:30Z", "closed_at": "2020-03-06T18:04:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "The simple code to ingest images (listed below) works fine with output_url set to a local file system ('file://') but crashes with just 'Segmentation fault (core dumped)' when trying to save data to HDFS on high-availability Hadoop cluster.\r\n\r\nIt looks like some finalizing code from `materialize_dataset` fails, as the segfault is just after printing out 'DONE', so 'STAGE 3' finishes well.\r\nAll .parquet files are properly created on HDFS (with correct file size, compared with the same output on the local file system), there's also `_SUCCESS` flag file, but the only one which is missing is `_common_metadata` file. Apparently the crash happens when writing this file.\r\n\r\nI tried increasing `spark.driver.memory` up to 8g in:\r\n```python\r\nspark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[*]').getOrCreate()\r\n```\r\nI also tried limiting the CPU to 'local[1]' or 'local[2]', with no effect, it still crashes the same way.\r\n\r\n**Folder content**\r\nHere's what's in the output folder after the crash:\r\n```\r\n$ hdfs dfs -ls /data/petastorm_ingestion_tests\r\nFound 11 items\r\n-rw-r--r--   3 hduser hadoop          0 2020-03-05 15:04 /data/petastorm_ingestion_tests/_SUCCESS\r\n-rw-r--r--   3 hduser hadoop    3204443 2020-03-05 15:04 /data/petastorm_ingestion_tests/part-00000-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    3267115 2020-03-05 15:03 /data/petastorm_ingestion_tests/part-00001-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4943022 2020-03-05 15:03 /data/petastorm_ingestion_tests/part-00002-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4937730 2020-03-05 15:04 /data/petastorm_ingestion_tests/part-00003-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4911543 2020-03-05 15:03 /data/petastorm_ingestion_tests/part-00004-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    3132042 2020-03-05 15:04 /data/petastorm_ingestion_tests/part-00005-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4816829 2020-03-05 15:04 /data/petastorm_ingestion_tests/part-00006-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4835624 2020-03-05 15:04 /data/petastorm_ingestion_tests/part-00007-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    3187349 2020-03-05 15:03 /data/petastorm_ingestion_tests/part-00008-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    6273385 2020-03-05 15:04 /data/petastorm_ingestion_tests/part-00009-e10468ee-72d2-4a8f-b000-f63ed0a6427b-c000.parquet\r\n```\r\n\r\nAnd when I pass `use_summary_metadata=True` to `materialize_dataset()` it still crashes but creates the following files:\r\n\r\n```\r\n$ hdfs dfs -ls /data/petastorm_ingestion_tests\r\nFound 13 items\r\n-rw-r--r--   3 hduser hadoop          0 2020-03-05 15:01 /data/petastorm_ingestion_tests/_SUCCESS\r\n-rw-r--r--   3 hduser hadoop        351 2020-03-05 15:01 /data/petastorm_ingestion_tests/_common_metadata\r\n-rw-r--r--   3 hduser hadoop       3950 2020-03-05 15:01 /data/petastorm_ingestion_tests/_metadata\r\n-rw-r--r--   3 hduser hadoop    3204443 2020-03-05 15:00 /data/petastorm_ingestion_tests/part-00000-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    3267115 2020-03-05 15:00 /data/petastorm_ingestion_tests/part-00001-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4943022 2020-03-05 15:01 /data/petastorm_ingestion_tests/part-00002-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4937730 2020-03-05 15:00 /data/petastorm_ingestion_tests/part-00003-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4911543 2020-03-05 15:00 /data/petastorm_ingestion_tests/part-00004-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    3132042 2020-03-05 15:00 /data/petastorm_ingestion_tests/part-00005-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4816829 2020-03-05 15:00 /data/petastorm_ingestion_tests/part-00006-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    4835624 2020-03-05 15:01 /data/petastorm_ingestion_tests/part-00007-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    3187349 2020-03-05 15:01 /data/petastorm_ingestion_tests/part-00008-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n-rw-r--r--   3 hduser hadoop    6273385 2020-03-05 15:00 /data/petastorm_ingestion_tests/part-00009-daa9b6fc-7cf1-4404-aad5-2870e6e85c76-c000.parquet\r\n```\r\n\r\n**Environment**\r\npetastorm==0.8.2\r\npyspark=2.4.4=py_0\r\nlibhdfs3=2.3.0=h2fca0e8_1\r\n\r\n**Logs**\r\n```\r\n$ ./test_ingest.py\r\n2020-03-05 13:29:29,067 WARN util.Utils: Your hostname, worker1 resolves to a loopback address: 127.0.1.1; using 10.x.x.x instead (on interface enp0s31f6)\r\n2020-03-05 13:29:29,068 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\nSTAGE 1\r\nSTAGE 2\r\nSTAGE 3\r\n2020-03-05 13:29:32,384 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\r\n2020-03-05 13:29:44,585 WARN hadoop.ParquetOutputFormat: Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\r\nDONE\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n**The code**\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nimport os, sys\r\nimport numpy as np\r\nfrom glob import glob\r\nimport cv2\r\n\r\nfrom pyspark.sql import SparkSession, Row\r\nfrom pyspark.sql.types import StructField, StructType, IntegerType, BinaryType, StringType, TimestampType\r\n\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\r\nfrom petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\r\n\r\nROWGROUP_SIZE_MB = 128 # The same as the default HDFS block size\r\n\r\n# The schema defines how the dataset schema looks like\r\nImageSchema = Unischema('ImageSchema', [\r\n    UnischemaField('path', np.string_, (), ScalarCodec(StringType()), False),\r\n    UnischemaField('image', np.uint8, (1080, 1280, 3), CompressedImageCodec('png'), False)\r\n])\r\n\r\n\r\n#output_url = \"file:///tmp/petastorm_ingest_test\"\r\noutput_url = \"hdfs:///data/petastorm_ingestion_tests\"\r\nrows_count = 1\r\n\r\ndef row_generator(x):\r\n    \"\"\"Converts single row into a dictionary\"\"\"\r\n    print(\"Creating row for: \" + x.path)\r\n    return {'path': x.path,\r\n            'image': np.reshape(x.image, (1080,1280,3))}\r\n\r\ndef ingest_folder2(images_folder, spark):\r\n\r\n    # List all images in the folder\r\n    image_files = sorted(glob(os.path.join(images_folder, \"*.png\")))\r\n\r\n    with materialize_dataset(spark, output_url, ImageSchema, ROWGROUP_SIZE_MB):\r\n\r\n        print(\"STAGE 1\")\r\n        input_rdd = spark.sparkContext.parallelize(image_files) \\\r\n            .map(lambda image_path:\r\n                    {ImageSchema.path.name: image_path,\r\n                     ImageSchema.image.name: cv2.imread(image_path)})\r\n\r\n        print(\"STAGE 2\")\r\n        rows_rdd = input_rdd.map(lambda r: dict_to_spark_row(ImageSchema, r))\r\n\r\n        print(\"STAGE 3\")\r\n        spark.createDataFrame(rows_rdd, ImageSchema.as_spark_schema()) \\\r\n            .coalesce(10) \\\r\n            .write \\\r\n            .mode('overwrite') \\\r\n            .option('compression', 'none') \\\r\n            .parquet(output_url)\r\n\r\n        print(\"DONE\")\r\n\r\ndef main():\r\n\r\n    # Start the Spark session\r\n    spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[*]').getOrCreate()    \r\n    sc = spark.sparkContext\r\n\r\n    # Ingest images and annotations for a given folder\r\n    ingest_folder2(\"../images\", spark)\r\n    print(\"Finished\")\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/497", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/497/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/497/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/497/events", "html_url": "https://github.com/uber/petastorm/issues/497", "id": 574105598, "node_id": "MDU6SXNzdWU1NzQxMDU1OTg=", "number": 497, "title": "Best way to load folder of images into petastorm data set?", "user": {"login": "filipski", "id": 1301017, "node_id": "MDQ6VXNlcjEzMDEwMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1301017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/filipski", "html_url": "https://github.com/filipski", "followers_url": "https://api.github.com/users/filipski/followers", "following_url": "https://api.github.com/users/filipski/following{/other_user}", "gists_url": "https://api.github.com/users/filipski/gists{/gist_id}", "starred_url": "https://api.github.com/users/filipski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/filipski/subscriptions", "organizations_url": "https://api.github.com/users/filipski/orgs", "repos_url": "https://api.github.com/users/filipski/repos", "events_url": "https://api.github.com/users/filipski/events{/privacy}", "received_events_url": "https://api.github.com/users/filipski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-03-02T16:49:02Z", "updated_at": "2020-03-05T17:26:49Z", "closed_at": "2020-03-05T06:24:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI have a folder of images in png format and I need to load them into a petastorm data set with given schema. For simplicity let's say it's just a path and encoded image. I tried loading all images into a dataframe with `spark.read.format(\"image\").load(image_files)` and then storing this dataframe with materialize_dataset. Is this the way to go? I'm not sure if the images end up compressed with png in the parquet files. And sometimes I get weird errors, as it seems that petastorm sorts the columns alphabetically, so I need to create Unischema and my final dataframe with columns ordered by name, too.\r\n\r\nHere's a sample code I'm trying now:\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nimport os, sys\r\nimport numpy as np\r\n\r\nfrom pyspark.sql import SparkSession, Row\r\nfrom pyspark.sql.types import StructField, StructType, IntegerType, BinaryType, StringType, TimestampType\r\n\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\r\nfrom petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\r\n\r\nROWGROUP_SIZE_MB = 128 # The same as the default HDFS block size\r\n\r\n# The schema defines how the dataset schema looks like\r\nImageSchema = Unischema('ImageSchema', [\r\n    UnischemaField('path', np.string_, (), ScalarCodec(StringType()), False),\r\n    UnischemaField('image', np.uint8, (1080, 1280, 3), CompressedImageCodec('png'), False)\r\n])\r\n\r\n\r\noutput_url = \"file:///tmp/petastorm_ingest_test\"\r\nrows_count = 1\r\n\r\ndef ingest_folder(images_folder, spark):\r\n\r\n    image_files = \"file:///\"+os.path.abspath(images_folder)+\"/*.png\"\r\n    print(image_files)\r\n    # Read all images at once\r\n    image_df = spark.read.format(\"image\").load(image_files)\r\n\r\n    print('Schema of image_df')\r\n    print('--------------------------')\r\n    image_df.printSchema()\r\n\r\n    with materialize_dataset(spark, output_url, ImageSchema, ROWGROUP_SIZE_MB):\r\n        \r\n        set_df = image_df.select(image_df.image.origin.alias('path'), image_df.image.data.alias('image'))\r\n\r\n        print('Schema of set_df')\r\n        print('--------------------------')\r\n        set_df.printSchema()\r\n        print(ImageSchema.as_spark_schema())\r\n        \r\n        print('Saving to parquet')\r\n\r\n        \"\"\"\r\n        set_df.write \\\r\n                .mode('overwrite') \\\r\n                .parquet(output_url)\r\n        \r\n        \"\"\"\r\n        \r\n        spark.createDataFrame(set_df.rdd, ImageSchema.as_spark_schema()) \\\r\n            .coalesce(10) \\\r\n            .write \\\r\n            .mode('overwrite') \\\r\n            .parquet(output_url)\r\n\r\ndef main():\r\n\r\n    # Start the Spark session\r\n    spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[*]').getOrCreate()    \r\n    sc = spark.sparkContext\r\n\r\n    # Ingest images and annotations for a given folder\r\n    ingest_folder(\"../images/\", spark)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/494", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/494/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/494/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/494/events", "html_url": "https://github.com/uber/petastorm/issues/494", "id": 571259559, "node_id": "MDU6SXNzdWU1NzEyNTk1NTk=", "number": 494, "title": "Segmentation fault importing pytorch Dataloader", "user": {"login": "sonNeturo", "id": 45131840, "node_id": "MDQ6VXNlcjQ1MTMxODQw", "avatar_url": "https://avatars1.githubusercontent.com/u/45131840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sonNeturo", "html_url": "https://github.com/sonNeturo", "followers_url": "https://api.github.com/users/sonNeturo/followers", "following_url": "https://api.github.com/users/sonNeturo/following{/other_user}", "gists_url": "https://api.github.com/users/sonNeturo/gists{/gist_id}", "starred_url": "https://api.github.com/users/sonNeturo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sonNeturo/subscriptions", "organizations_url": "https://api.github.com/users/sonNeturo/orgs", "repos_url": "https://api.github.com/users/sonNeturo/repos", "events_url": "https://api.github.com/users/sonNeturo/events{/privacy}", "received_events_url": "https://api.github.com/users/sonNeturo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-26T11:17:40Z", "updated_at": "2020-02-26T16:46:37Z", "closed_at": "2020-02-26T16:46:37Z", "author_association": "NONE", "active_lock_reason": null, "body": " Hello,\r\n\r\nImporting the pytorch dataloader causes a segmentation fault and crushes. This doesn't happen locally, but only when I run a simple import in on google dataproc.\r\n\r\n```python\r\nimport faulthandler\r\nfaulthandler.enable()\r\n\r\nfrom petastorm.pytorch import DataLoader\r\n```\r\n\r\nI get:\r\n\r\n```python\r\nWaiting for job output...\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007f28e9495700 (most recent call first):\r\n  File \"<frozen importlib._bootstrap>\", line 205 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 922 in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 560 in module_from_spec\r\n  File \"<frozen importlib._bootstrap>\", line 648 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 950 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load\r\n  File \"/opt/conda/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/torch/__init__.py\", line 81 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 205 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 678 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 655 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 950 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 205 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 936 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 205 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 936 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 205 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 936 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load\r\n  File \"/opt/conda/miniconda3/envs/deep-ctr/lib/python3.6/site-packages/petastorm/pytorch.py\", line 22 in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 205 in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap_external>\", line 678 in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 655 in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 950 in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 961 in _find_and_load\r\n```\r\n\r\nPetastorm version: 0.8.2\r\nPytorch version: 1.4.0\r\n\r\n\r\nThanks in advance for the help", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/479", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/479/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/479/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/479/events", "html_url": "https://github.com/uber/petastorm/issues/479", "id": 557684416, "node_id": "MDU6SXNzdWU1NTc2ODQ0MTY=", "number": 479, "title": "Pandas 1.0.0 compatibity", "user": {"login": "abditag2", "id": 2999450, "node_id": "MDQ6VXNlcjI5OTk0NTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2999450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abditag2", "html_url": "https://github.com/abditag2", "followers_url": "https://api.github.com/users/abditag2/followers", "following_url": "https://api.github.com/users/abditag2/following{/other_user}", "gists_url": "https://api.github.com/users/abditag2/gists{/gist_id}", "starred_url": "https://api.github.com/users/abditag2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abditag2/subscriptions", "organizations_url": "https://api.github.com/users/abditag2/orgs", "repos_url": "https://api.github.com/users/abditag2/repos", "events_url": "https://api.github.com/users/abditag2/events{/privacy}", "received_events_url": "https://api.github.com/users/abditag2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-01-30T18:52:14Z", "updated_at": "2020-02-01T17:05:15Z", "closed_at": "2020-02-01T17:05:15Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Yesterday there was a new pandas package released. When using `make_batch_reader` and `DataLoader` for pytorch, we are getting this error:\r\n\r\n```\r\n  File \"/usr/local/lib/python3.6/dist-packages/horovod/spark/torch/remote.py\", line 274, in _train\r\n    row = next(train_loader_iter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/petastorm/pytorch.py\", line 152, in __iter__\r\n    for row in self.reader:\r\n  File \"/usr/local/lib/python3.6/dist-packages/petastorm/reader.py\", line 610, in __next__\r\n    return self._results_queue_reader.read_next(self._workers_pool, self.schema, self.ngram)\r\n  File \"/usr/local/lib/python3.6/dist-packages/petastorm/arrow_reader_worker.py\", line 60, in read_next\r\n    column_as_numpy = column_as_pandas.as_matrix()\r\n  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\", line 5273, in __getattr__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'Series' object has no attribute 'as_matrix'\r\n```\r\n\r\nThe root cause is that new `pandas.Series` does not have `as_matrix` \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/475", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/475/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/475/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/475/events", "html_url": "https://github.com/uber/petastorm/issues/475", "id": 555872296, "node_id": "MDU6SXNzdWU1NTU4NzIyOTY=", "number": 475, "title": "Problem with HelloWorld Example on Front Page of Repo", "user": {"login": "andrewredd", "id": 48295430, "node_id": "MDQ6VXNlcjQ4Mjk1NDMw", "avatar_url": "https://avatars1.githubusercontent.com/u/48295430?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewredd", "html_url": "https://github.com/andrewredd", "followers_url": "https://api.github.com/users/andrewredd/followers", "following_url": "https://api.github.com/users/andrewredd/following{/other_user}", "gists_url": "https://api.github.com/users/andrewredd/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewredd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewredd/subscriptions", "organizations_url": "https://api.github.com/users/andrewredd/orgs", "repos_url": "https://api.github.com/users/andrewredd/repos", "events_url": "https://api.github.com/users/andrewredd/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewredd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2020-01-27T22:28:52Z", "updated_at": "2020-04-02T04:41:07Z", "closed_at": "2020-01-31T16:43:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi I'm running the following code:\r\n\r\n```\r\nfrom petastorm.unischema import Unischema, UnischemaField, dict_to_spark_row\r\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nfrom pyspark.sql.types import IntegerType\r\nimport numpy as np\r\nfrom petastorm.fs_utils import FilesystemResolver\r\n\r\nresolver=FilesystemResolver(output_url + 'test', spark.sparkContext._jsc.hadoopConfiguration(),\r\n                             hdfs_driver='libhdfs')\r\nfact = resolver.filesystem_factory()\r\n\r\nHelloWorldSchema = Unischema('HelloWorldSchema', [\r\n   UnischemaField('id', np.int32, (), ScalarCodec(IntegerType()), False),\r\n   UnischemaField('other_data', np.uint8, (None, 128, 30, None), NdarrayCodec(), False),\r\n])\r\n\r\n\r\ndef row_generator(x):\r\n   \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\r\n   return {'id': x,\r\n           'other_data': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}\r\n\r\ndef generate_hello_world_dataset(output_url, spark, sc):\r\n   rows_count = 1000\r\n   rowgroup_size_mb = 256\r\n\r\n   # Wrap dataset materialization portion. Will take care of setting up spark environment variables as\r\n   # well as save petastorm specific metadata\r\n   with materialize_dataset(spark, url, HelloWorldSchema, rowgroup_size_mb, filesystem_factory=fact):\r\n\r\n       rows_rdd = sc.parallelize(range(rows_count))\\\r\n           .map(row_generator)\\\r\n           .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\r\n\r\n       spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema(), ) \\\r\n           .coalesce(10) \\\r\n           .write \\\r\n           .mode('overwrite') \\\r\n           .parquet(url)\r\n    \r\ngenerate_hello_world_dataset(url, spark, sc)\r\n```\r\n\r\nThis is the only way that I can run with a libhdfs setup. I get the following error.\r\n\r\n```\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/serializers.py\", line 393, in dump_stream\r\n    vs = list(itertools.islice(iterator, batch))\r\n  File \"/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/util.py\", line 99, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/basedir/home/aredd/venvs/prometheus/lib64/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 216, in get_row_group_info\r\n  File \"/basedir/home/aredd/venvs/prometheus/lib64/python3.6/site-packages/petastorm/fs_utils.py\", line 108, in <lambda>\r\n  File \"/basedir/tmp/mapred.tmp1/yarn/nm/usercache/username/appcache/application_1576215002453_189781/container_e15_1576215002453_189781_01_000003/PRO/pro/lib64/python3.6/site-packages/petastorm/hdfs/namenode.py\", line 266, in hdfs_connect_namenode\r\n    return pyarrow.hdfs.connect(hostname, url.port or 8020, driver=driver, user=user)\r\n  File \"/basedir/tmp/mapred.tmp1/yarn/nm/usercache/username/appcache/application_1576215002453_189781/container_e15_1576215002453_189781_01_000003/PRO/pro/lib64/python3.6/site-packages/pyarrow/hdfs.py\", line 215, in connect\r\n    extra_conf=extra_conf)\r\n  File \"/basedir/tmp/mapred.tmp1/yarn/nm/usercache/username/appcache/application_1576215002453_189781/container_e15_1576215002453_189781_01_000003/PRO/pro/lib64/python3.6/site-packages/pyarrow/hdfs.py\", line 40, in __init__\r\n    self._connect(host, port, user, kerb_ticket, driver, extra_conf)\r\n  File \"pyarrow/io-hdfs.pxi\", line 105, in pyarrow.lib.HadoopFileSystem._connect\r\n  File \"pyarrow/error.pxi\", line 80, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIOError: HDFS connection failed\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\n\r\nThanks in advance", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/464", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/464/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/464/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/464/events", "html_url": "https://github.com/uber/petastorm/issues/464", "id": 544295453, "node_id": "MDU6SXNzdWU1NDQyOTU0NTM=", "number": 464, "title": "What is the difference between petastorm and horovod?", "user": {"login": "dclong", "id": 824507, "node_id": "MDQ6VXNlcjgyNDUwNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/824507?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dclong", "html_url": "https://github.com/dclong", "followers_url": "https://api.github.com/users/dclong/followers", "following_url": "https://api.github.com/users/dclong/following{/other_user}", "gists_url": "https://api.github.com/users/dclong/gists{/gist_id}", "starred_url": "https://api.github.com/users/dclong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dclong/subscriptions", "organizations_url": "https://api.github.com/users/dclong/orgs", "repos_url": "https://api.github.com/users/dclong/repos", "events_url": "https://api.github.com/users/dclong/events{/privacy}", "received_events_url": "https://api.github.com/users/dclong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-01T00:54:08Z", "updated_at": "2020-01-07T02:09:07Z", "closed_at": "2020-01-07T02:09:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "Both products are from Uber and both claims to do distributed training. Just curious about it. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/463", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/463/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/463/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/463/events", "html_url": "https://github.com/uber/petastorm/issues/463", "id": 542043501, "node_id": "MDU6SXNzdWU1NDIwNDM1MDE=", "number": 463, "title": "Is there an easy way to transform native Parquet file to Petastorm datasets", "user": {"login": "LiuxyEric", "id": 28945845, "node_id": "MDQ6VXNlcjI4OTQ1ODQ1", "avatar_url": "https://avatars1.githubusercontent.com/u/28945845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LiuxyEric", "html_url": "https://github.com/LiuxyEric", "followers_url": "https://api.github.com/users/LiuxyEric/followers", "following_url": "https://api.github.com/users/LiuxyEric/following{/other_user}", "gists_url": "https://api.github.com/users/LiuxyEric/gists{/gist_id}", "starred_url": "https://api.github.com/users/LiuxyEric/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LiuxyEric/subscriptions", "organizations_url": "https://api.github.com/users/LiuxyEric/orgs", "repos_url": "https://api.github.com/users/LiuxyEric/repos", "events_url": "https://api.github.com/users/LiuxyEric/events{/privacy}", "received_events_url": "https://api.github.com/users/LiuxyEric/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-24T08:29:17Z", "updated_at": "2019-12-29T07:32:52Z", "closed_at": "2019-12-29T07:32:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I have a dataset saved as a native parquet file and I used \"make_batch_reader\" to read it and transform to tensorflow dataset. Everything works fine.\r\n\r\nBut with the native parquet file format, I can't control the number of batch size and I have to generate lots of parquet partition to reduce the number of training data in a batch in order to fix in the GPU memory which is really inefficient. \r\n\r\nI was wondering is there an easy way to transform native Parquet file to Petastorm datasets?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/456", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/456/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/456/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/456/events", "html_url": "https://github.com/uber/petastorm/issues/456", "id": 533311529, "node_id": "MDU6SXNzdWU1MzMzMTE1Mjk=", "number": 456, "title": "results_queue_size defines max number of prefetched row groups?", "user": {"login": "GregAru", "id": 46323816, "node_id": "MDQ6VXNlcjQ2MzIzODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/46323816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregAru", "html_url": "https://github.com/GregAru", "followers_url": "https://api.github.com/users/GregAru/followers", "following_url": "https://api.github.com/users/GregAru/following{/other_user}", "gists_url": "https://api.github.com/users/GregAru/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregAru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregAru/subscriptions", "organizations_url": "https://api.github.com/users/GregAru/orgs", "repos_url": "https://api.github.com/users/GregAru/repos", "events_url": "https://api.github.com/users/GregAru/events{/privacy}", "received_events_url": "https://api.github.com/users/GregAru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-05T12:08:31Z", "updated_at": "2019-12-19T15:06:29Z", "closed_at": "2019-12-19T15:06:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "According to `make_reader` doc:\r\nresults_queue_size: Size of the results queue to store prefetched rows\r\n\r\nHowever when debugging I can see that the `_results_queue` in ThreadPool which is affected by this parameter holds full row groups and not individual rows.\r\n\r\nAm I missing anything?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/447", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/447/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/447/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/447/events", "html_url": "https://github.com/uber/petastorm/issues/447", "id": 525118971, "node_id": "MDU6SXNzdWU1MjUxMTg5NzE=", "number": 447, "title": "Error reading parquet files made by AWS Athena", "user": {"login": "RoelantStegmann", "id": 2260120, "node_id": "MDQ6VXNlcjIyNjAxMjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/2260120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RoelantStegmann", "html_url": "https://github.com/RoelantStegmann", "followers_url": "https://api.github.com/users/RoelantStegmann/followers", "following_url": "https://api.github.com/users/RoelantStegmann/following{/other_user}", "gists_url": "https://api.github.com/users/RoelantStegmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/RoelantStegmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RoelantStegmann/subscriptions", "organizations_url": "https://api.github.com/users/RoelantStegmann/orgs", "repos_url": "https://api.github.com/users/RoelantStegmann/repos", "events_url": "https://api.github.com/users/RoelantStegmann/events{/privacy}", "received_events_url": "https://api.github.com/users/RoelantStegmann/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2019-11-19T16:26:10Z", "updated_at": "2020-01-29T22:34:24Z", "closed_at": "2020-01-29T22:34:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "I made a bunch of parquet files using an amazon athena CTAS query. I downloaded these files to first test locally (the end goal is to access the data from S3).\r\n\r\nIf I run the code below;\r\n\r\n```python\r\nimport s3fs\r\nfrom petastorm.reader import make_batch_reader\r\nfrom petastorm.tf_utils import make_petastorm_dataset\r\n\r\ndataset_url = \"file:///Data/test-parquet\"\r\n\r\nwith make_batch_reader(dataset_url) as reader:\r\n    dataset = make_petastorm_dataset(reader)\r\n    for batch in dataset:\r\n        break\r\nbatch.correct\r\n```\r\n\r\nI receive a lot of warnings and then an error in `for batch in dataset`\r\n\r\n`pyarrow.lib.ArrowIOError: The file only has 1 row groups, requested metadata for row group: 1`\r\n\r\nIf 1 look at dataset.take(1) or something alike, I do see the correct schema of the table. However, I don't seem to be able to access the data. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/441", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/441/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/441/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/441/events", "html_url": "https://github.com/uber/petastorm/issues/441", "id": 510125695, "node_id": "MDU6SXNzdWU1MTAxMjU2OTU=", "number": 441, "title": "Python running out of RAM", "user": {"login": "Mmiglio", "id": 26923771, "node_id": "MDQ6VXNlcjI2OTIzNzcx", "avatar_url": "https://avatars1.githubusercontent.com/u/26923771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mmiglio", "html_url": "https://github.com/Mmiglio", "followers_url": "https://api.github.com/users/Mmiglio/followers", "following_url": "https://api.github.com/users/Mmiglio/following{/other_user}", "gists_url": "https://api.github.com/users/Mmiglio/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mmiglio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mmiglio/subscriptions", "organizations_url": "https://api.github.com/users/Mmiglio/orgs", "repos_url": "https://api.github.com/users/Mmiglio/repos", "events_url": "https://api.github.com/users/Mmiglio/events{/privacy}", "received_events_url": "https://api.github.com/users/Mmiglio/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-10-21T17:05:42Z", "updated_at": "2019-10-22T09:56:06Z", "closed_at": "2019-10-22T09:56:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Problem\r\n\r\nPython process killed while creating a reader because it runs out of RAM.\r\n\r\n## How to reproduce\r\n\r\n### Dataset Generation\r\n\r\nDataset is generated by modifying the schema of the [Hello World](https://github.com/uber/petastorm/blob/master/examples/hello_world/petastorm_dataset/generate_petastorm_dataset.py) example. In my dataset I have two columns:  an integer `id` and a nd array with shape `801x19`.\r\n\r\n```\r\nimport numpy as np\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.types import IntegerType\r\n\r\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nfrom petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\r\n\r\n# The schema defines how the dataset schema looks like\r\nHelloWorldSchema = Unischema('HelloWorldSchema', [\r\n    UnischemaField('id', np.int32, (), ScalarCodec(IntegerType()), False),\r\n    UnischemaField('array', np.float32, (801, 19), NdarrayCodec(), False),\r\n])\r\n\r\n\r\ndef row_generator(x):\r\n    \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\r\n    return {'id': x,\r\n            'array': np.random.randn(801, 19).astype(np.float32)}\r\n\r\n\r\ndef generate_petastorm_dataset(output_url='file:///tmp/hello_world_dataset'):\r\n    rowgroup_size_mb = 256\r\n\r\n    spark = SparkSession.builder.config('spark.driver.memory', '2g').master('local[2]').getOrCreate()\r\n    sc = spark.sparkContext\r\n\r\n    # Wrap dataset materialization portion. Will take care of setting up spark environment variables as\r\n    # well as save petastorm specific metadata\r\n    rows_count = 10000\r\n    with materialize_dataset(spark, output_url, HelloWorldSchema, rowgroup_size_mb):\r\n\r\n        rows_rdd = sc.parallelize(range(rows_count))\\\r\n            .map(row_generator)\\\r\n            .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\r\n\r\n        spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema()) \\\r\n            .write \\\r\n            .mode('overwrite') \\\r\n            .parquet(output_url)\r\n\r\nif __name__ == '__main__':\r\n    generate_petastorm_dataset()\r\n```\r\n\r\n### Issue\r\nIf I run\r\n \r\n```\r\nfrom petastorm import make_reader\r\nreader = make_reader('file:///tmp/hello_world_dataset', schema_fields=['array'])\r\nprint(next(iter))\r\n```\r\n\r\nafter a couple of seconds the process gets killed because it runs out of memory. What is causing it? I tried to play around with reader parameters and `rowgroup_size_mb` during the dataset generation, but I didn't find a solution.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/437", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/437/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/437/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/437/events", "html_url": "https://github.com/uber/petastorm/issues/437", "id": 505580449, "node_id": "MDU6SXNzdWU1MDU1ODA0NDk=", "number": 437, "title": "incompatible with pyarrow==0.15.0", "user": {"login": "abditag2", "id": 2999450, "node_id": "MDQ6VXNlcjI5OTk0NTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2999450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abditag2", "html_url": "https://github.com/abditag2", "followers_url": "https://api.github.com/users/abditag2/followers", "following_url": "https://api.github.com/users/abditag2/following{/other_user}", "gists_url": "https://api.github.com/users/abditag2/gists{/gist_id}", "starred_url": "https://api.github.com/users/abditag2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abditag2/subscriptions", "organizations_url": "https://api.github.com/users/abditag2/orgs", "repos_url": "https://api.github.com/users/abditag2/repos", "events_url": "https://api.github.com/users/abditag2/events{/privacy}", "received_events_url": "https://api.github.com/users/abditag2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-11T00:52:25Z", "updated_at": "2019-10-26T07:30:50Z", "closed_at": "2019-10-26T07:30:50Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Pyarrow might have introduced some breaking changes in 0.15.0\r\n\r\n```\r\nE   Traceback (most recent call last):\r\nE       schema_fields=schema_fields) as train_reader:\r\nE     File \"/tmp/michelangelo-python-client/.tox/py27/local/lib/python2.7/site-packages/petastorm/reader.py\", line 289, in make_batch_reader\r\nE       is_batched_reader=True)\r\nE     File \"/tmp/michelangelo-python-client/.tox/py27/local/lib/python2.7/site-packages/petastorm/reader.py\", line 360, in __init__\r\nE       stored_schema = infer_or_load_unischema(self.dataset)\r\nE     File \"/tmp/michelangelo-python-client/.tox/py27/local/lib/python2.7/site-packages/petastorm/etl/dataset_metadata.py\", line 394, in infer_or_load_unischema\r\nE       return Unischema.from_arrow_schema(dataset)\r\nE     File \"/tmp/michelangelo-python-client/.tox/py27/local/lib/python2.7/site-packages/petastorm/unischema.py\", line 251, in from_arrow_schema\r\nE       meta = parquet_dataset.pieces[0].get_metadata(parquet_dataset.fs.open)\r\nE   TypeError: get_metadata() takes exactly 1 argument (2 given)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/432", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/432/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/432/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/432/events", "html_url": "https://github.com/uber/petastorm/issues/432", "id": 500935005, "node_id": "MDU6SXNzdWU1MDA5MzUwMDU=", "number": 432, "title": "TF Dataset Runs Out Of Data Despite num_epochs=None", "user": {"login": "cupdike", "id": 133214, "node_id": "MDQ6VXNlcjEzMzIxNA==", "avatar_url": "https://avatars3.githubusercontent.com/u/133214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cupdike", "html_url": "https://github.com/cupdike", "followers_url": "https://api.github.com/users/cupdike/followers", "following_url": "https://api.github.com/users/cupdike/following{/other_user}", "gists_url": "https://api.github.com/users/cupdike/gists{/gist_id}", "starred_url": "https://api.github.com/users/cupdike/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cupdike/subscriptions", "organizations_url": "https://api.github.com/users/cupdike/orgs", "repos_url": "https://api.github.com/users/cupdike/repos", "events_url": "https://api.github.com/users/cupdike/events{/privacy}", "received_events_url": "https://api.github.com/users/cupdike/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-10-01T14:27:24Z", "updated_at": "2019-11-13T19:35:23Z", "closed_at": "2019-11-13T19:35:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a petastorm dataset with 10000+ images.  I'm trying to train with 10 epochs and a batch size of 32.  Training runs are failing with:\r\n`Your dataset ran out of data; interrupting training. Make sure that your dataset can generate at least `steps_per_epoch * epochs` batches (in this case, 3390 batches)` \r\n\r\nI am applying the `num_epochs=None` to make_reader which I thought was sufficient to supply \"infinite\" data:\r\n```\r\nwith make_reader(dataset_url, reader_pool_type='dummy', num_epochs=None) as reader:\r\n\r\n    dataset = make_petastorm_dataset(reader)\r\n    dataset = dataset.map(lambda x: tf.numpy_function(func=process_inputs, inp=(x.image1, x.category), Tout=(tf.float32,tf.uint8)))\r\n\r\n    dataset = dataset.batch(bs).shuffle(buffer_size=15000) \r\n```\r\n\r\n Am I doing something wrong here?  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/427", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/427/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/427/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/427/events", "html_url": "https://github.com/uber/petastorm/issues/427", "id": 492029740, "node_id": "MDU6SXNzdWU0OTIwMjk3NDA=", "number": 427, "title": "Fetch a specific batch size", "user": {"login": "priyankaexp", "id": 19281501, "node_id": "MDQ6VXNlcjE5MjgxNTAx", "avatar_url": "https://avatars2.githubusercontent.com/u/19281501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/priyankaexp", "html_url": "https://github.com/priyankaexp", "followers_url": "https://api.github.com/users/priyankaexp/followers", "following_url": "https://api.github.com/users/priyankaexp/following{/other_user}", "gists_url": "https://api.github.com/users/priyankaexp/gists{/gist_id}", "starred_url": "https://api.github.com/users/priyankaexp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/priyankaexp/subscriptions", "organizations_url": "https://api.github.com/users/priyankaexp/orgs", "repos_url": "https://api.github.com/users/priyankaexp/repos", "events_url": "https://api.github.com/users/priyankaexp/events{/privacy}", "received_events_url": "https://api.github.com/users/priyankaexp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-11T05:47:35Z", "updated_at": "2019-09-11T14:16:51Z", "closed_at": "2019-09-11T14:16:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Tensorflow works better when all the training batches are of the same size. While creating reader via make_batch_reader and converting the dataset to tensorflow dataset via make_petastorm_dataset, there is no way to specify the batch size. The batches retrieved via make_batch_reader is dependent on parquet rowgroups. \r\n\r\nIs there a way to specify a batch size and only retrieve those number of records from S3 in one go? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/426", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/426/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/426/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/426/events", "html_url": "https://github.com/uber/petastorm/issues/426", "id": 491921940, "node_id": "MDU6SXNzdWU0OTE5MjE5NDA=", "number": 426, "title": "Error while creating a dataset (ArrowIOError: Invalid parquet file. Corrupt footer) ", "user": {"login": "sgvarsh", "id": 33467893, "node_id": "MDQ6VXNlcjMzNDY3ODkz", "avatar_url": "https://avatars3.githubusercontent.com/u/33467893?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgvarsh", "html_url": "https://github.com/sgvarsh", "followers_url": "https://api.github.com/users/sgvarsh/followers", "following_url": "https://api.github.com/users/sgvarsh/following{/other_user}", "gists_url": "https://api.github.com/users/sgvarsh/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgvarsh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgvarsh/subscriptions", "organizations_url": "https://api.github.com/users/sgvarsh/orgs", "repos_url": "https://api.github.com/users/sgvarsh/repos", "events_url": "https://api.github.com/users/sgvarsh/events{/privacy}", "received_events_url": "https://api.github.com/users/sgvarsh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-09-10T22:07:03Z", "updated_at": "2020-01-09T23:33:15Z", "closed_at": "2020-01-09T23:33:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/uber/petastorm/blob/master/examples/hello_world/petastorm_dataset/generate_petastorm_dataset.py\r\n\r\n\r\nCode: \r\n```\r\ndef row_generator(x):\r\n  \"\"\"Returns a single entry in the generated dataset. Return a bunch of random values as an example.\"\"\"\r\n  return {'id': x,\r\n           'other_data': np.random.randint(0, 255, dtype=np.uint8, size=(4, 128, 30, 3))}\r\n\r\n\r\ndef generate_hello_world_dataset(output_url='file:///tmp/hello_world_dataset'):\r\n  rows_count = 10\r\n  rowgroup_size_mb = 256\r\n  with materialize_dataset(spark, output_url, HelloWorldSchema, rowgroup_size_mb):\r\n    rows_rdd = sc.parallelize(range(rows_count))\\\r\n           .map(row_generator)\\\r\n           .map(lambda x: dict_to_spark_row(HelloWorldSchema, x))\r\n\r\n    spark.createDataFrame(rows_rdd, HelloWorldSchema.as_spark_schema()) \\\r\n           .coalesce(10) \\\r\n           .write \\\r\n           .mode('overwrite') \\\r\n           .parquet(output_url)\r\n```\r\nI am simply trying to create dataset using HelloWorld example, but it fails with below error:\r\n\r\nArrowIOError: Invalid parquet file. Corrupt footer:\r\n```\r\n/databricks/python/lib/python3.6/site-packages/petastorm/utils.py in add_to_dataset_metadata(dataset, key, value)\r\n     96             arrow_metadata = pyarrow.parquet.read_metadata(f)\r\n     97     else:\r\n---> 98         arrow_metadata = dataset.pieces[0].get_metadata(dataset.fs.open)\r\n     99 \r\n    100     base_schema = arrow_metadata.schema.to_arrow_schema()\r\n\r\n/databricks/python/lib/python3.6/site-packages/pyarrow/parquet.py in get_metadata(self, open_file_func)\r\n    498         \"\"\"\r\n    499         if open_file_func is not None:\r\n--> 500             f = self._open(open_file_func)\r\n    501         else:\r\n    502             f = self.open()\r\n\r\n/databricks/python/lib/python3.6/site-packages/pyarrow/parquet.py in _open(self, open_file_func)\r\n    511         reader = open_file_func(self.path)\r\n    512         if not isinstance(reader, ParquetFile):\r\n--> 513             reader = ParquetFile(reader)\r\n    514         return reader\r\n    515 \r\n\r\n/databricks/python/lib/python3.6/site-packages/pyarrow/parquet.py in __init__(self, source, metadata, common_metadata, memory_map)\r\n    128                  memory_map=True):\r\n    129         self.reader = ParquetReader()\r\n--> 130         self.reader.open(source, use_memory_map=memory_map, metadata=metadata)\r\n    131         self.common_metadata = common_metadata\r\n    132         self._nested_paths_by_prefix = self._build_nested_paths()\r\n\r\n/databricks/python/lib/python3.6/site-packages/pyarrow/_parquet.pyx in pyarrow._parquet.ParquetReader.open()\r\n\r\n/databricks/python/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowIOError: Invalid parquet file. Corrupt footer.\r\n```\r\nI saw one similar issue opened up but closed without resolution, I am using Databricks Spark 5.5 ML Runtime which comes preinstall with pyarrow==0.13", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/419", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/419/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/419/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/419/events", "html_url": "https://github.com/uber/petastorm/issues/419", "id": 485438566, "node_id": "MDU6SXNzdWU0ODU0Mzg1NjY=", "number": 419, "title": "Support for GCS", "user": {"login": "dexterfichuk", "id": 8170800, "node_id": "MDQ6VXNlcjgxNzA4MDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/8170800?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dexterfichuk", "html_url": "https://github.com/dexterfichuk", "followers_url": "https://api.github.com/users/dexterfichuk/followers", "following_url": "https://api.github.com/users/dexterfichuk/following{/other_user}", "gists_url": "https://api.github.com/users/dexterfichuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/dexterfichuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dexterfichuk/subscriptions", "organizations_url": "https://api.github.com/users/dexterfichuk/orgs", "repos_url": "https://api.github.com/users/dexterfichuk/repos", "events_url": "https://api.github.com/users/dexterfichuk/events{/privacy}", "received_events_url": "https://api.github.com/users/dexterfichuk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-08-26T20:35:55Z", "updated_at": "2020-01-25T00:06:04Z", "closed_at": "2020-01-25T00:06:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Would be great to also have support to read data from GCS paths, any plans on updating this. I see ludwig actually already has support for this!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/418", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/418/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/418/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/418/events", "html_url": "https://github.com/uber/petastorm/issues/418", "id": 483978383, "node_id": "MDU6SXNzdWU0ODM5NzgzODM=", "number": 418, "title": "pyarrow.lib.ArrowIOError: Prior attempt to load libhdfs3 failed", "user": {"login": "okedoki", "id": 4446172, "node_id": "MDQ6VXNlcjQ0NDYxNzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4446172?v=4", "gravatar_id": "", "url": "https://api.github.com/users/okedoki", "html_url": "https://github.com/okedoki", "followers_url": "https://api.github.com/users/okedoki/followers", "following_url": "https://api.github.com/users/okedoki/following{/other_user}", "gists_url": "https://api.github.com/users/okedoki/gists{/gist_id}", "starred_url": "https://api.github.com/users/okedoki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/okedoki/subscriptions", "organizations_url": "https://api.github.com/users/okedoki/orgs", "repos_url": "https://api.github.com/users/okedoki/repos", "events_url": "https://api.github.com/users/okedoki/events{/privacy}", "received_events_url": "https://api.github.com/users/okedoki/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-08-22T12:56:20Z", "updated_at": "2019-09-04T14:30:06Z", "closed_at": "2019-09-04T14:30:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI have an issue when I'm trying to read a parquet file from HDFS.\r\n\r\nWhen I run an example : \r\nimport tensorflow as tf\r\nfrom petastorm import make_reader\r\nwith make_reader(path) as reader:\r\n\tdataset = make_petastorm_dataset(reader)\r\n\titerator = dataset.make_one_shot_iterator()\r\n\ttensor = iterator.get_next()\r\n\twith tf.Session() as sess:\r\n\t\tsample = sess.run(tensor)\r\n\t\tprint(sample.id)\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/olbico/anaconda2/lib/python2.7/site-packages/petastorm/reader.py\", line 120, in make_reader\r\n    resolver = FilesystemResolver(dataset_url, hdfs_driver=hdfs_driver)\r\n  File \"/home/olbico/anaconda2/lib/python2.7/site-packages/petastorm/fs_utils.py\", line 89, in __init__\r\n    self._filesystem = connector.hdfs_connect_namenode(self._parsed_dataset_url, user=user)\r\n  File \"/home/olbico/anaconda2/lib/python2.7/site-packages/petastorm/hdfs/namenode.py\", line 266, in hdfs_connect_namenode\r\n    return pyarrow.hdfs.connect(hostname, url.port or 8020, driver=driver, user=user)\r\n  File \"/home/olbico/anaconda2/lib/python2.7/site-packages/pyarrow/hdfs.py\", line 215, in connect\r\n    extra_conf=extra_conf)\r\n  File \"/home/olbico/anaconda2/lib/python2.7/site-packages/pyarrow/hdfs.py\", line 40, in __init__\r\n    self._connect(host, port, user, kerb_ticket, driver, extra_conf)\r\n  File \"pyarrow/io-hdfs.pxi\", line 93, in pyarrow.lib.HadoopFileSystem._connect\r\n  File \"pyarrow/error.pxi\", line 87, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIOError: Prior attempt to load libhdfs3 failed\r\n\r\nIf I try to call the function directly from pyarrow I dont have any problems\r\n\r\nfrom pyarrow import hdfs\r\nfs = hdfs.connect()\r\nwith fs.open(path) as f:\r\n\r\nI work in conda environment with installed libhdfs3\r\n\r\nI would appreciate any suggestions to solve this problem.\r\nThanks.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/416", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/416/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/416/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/416/events", "html_url": "https://github.com/uber/petastorm/issues/416", "id": 479928551, "node_id": "MDU6SXNzdWU0Nzk5Mjg1NTE=", "number": 416, "title": "removed_fileds not acknowledged in arrow_reader_worker", "user": {"login": "praateekmahajan", "id": 7589415, "node_id": "MDQ6VXNlcjc1ODk0MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/7589415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/praateekmahajan", "html_url": "https://github.com/praateekmahajan", "followers_url": "https://api.github.com/users/praateekmahajan/followers", "following_url": "https://api.github.com/users/praateekmahajan/following{/other_user}", "gists_url": "https://api.github.com/users/praateekmahajan/gists{/gist_id}", "starred_url": "https://api.github.com/users/praateekmahajan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/praateekmahajan/subscriptions", "organizations_url": "https://api.github.com/users/praateekmahajan/orgs", "repos_url": "https://api.github.com/users/praateekmahajan/repos", "events_url": "https://api.github.com/users/praateekmahajan/events{/privacy}", "received_events_url": "https://api.github.com/users/praateekmahajan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-08-13T02:21:45Z", "updated_at": "2019-08-29T16:19:35Z", "closed_at": "2019-08-29T16:19:35Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Sadly won't be able to reproduce the error quickly :\r\n\r\n```python\r\ntransform = TransformSpec(lambda x : x, removed_fields=[\"columnA\"])\r\nreader = make_batch_reader(dataset_url, transform_spec=transform)\r\n# next(reader) gives an error\r\n```\r\n\r\nI was trying to trace the error and realised that \r\n1. `reader.schema` : doesn't have `columnA`\r\n2. `arrow_reader_worker.py` calls `schema.make_namedtuple(**result_dict)` where `result_dict` comes from worker `workers_pool.get_results()`. \r\nThe `result_dict` seems to have `columnA` in it.\r\n\r\nThe error is following :\r\n\r\n\r\n```python\r\n~/Desktop/github/petastorm/petastorm/reader.py in __next__(self)\r\n    598     def __next__(self):\r\n    599         try:\r\n--> 600             return self._results_queue_reader.read_next(self._workers_pool, self.schema, self.ngram)\r\n    601         except StopIteration:\r\n    602             self.last_row_consumed = True\r\n\r\n~/Desktop/github/petastorm/petastorm/arrow_reader_worker.py in read_next(self, workers_pool, schema, ngram)\r\n     71             print(f\"The schema has the following keys : \\n {schema._fields.keys()}\")\r\n     72             print(f\"The resulting dict has the folowing keys {result_dict.keys()}\")\r\n---> 73             return schema.make_namedtuple(**result_dict)\r\n     74 \r\n     75         except EmptyResultError:\r\n\r\n~/Desktop/github/petastorm/petastorm/unischema.py in make_namedtuple(self, **kargs)\r\n    233                 typed_dict[key] = None\r\n    234         print(typed_dict.keys())\r\n--> 235         return self._get_namedtuple()(**typed_dict)\r\n    236 \r\n    237     def make_namedtuple_tf(self, *args, **kargs):\r\n\r\nTypeError: __new__() got an unexpected keyword argument 'columnA'\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/413", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/413/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/413/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/413/events", "html_url": "https://github.com/uber/petastorm/issues/413", "id": 478060138, "node_id": "MDU6SXNzdWU0NzgwNjAxMzg=", "number": 413, "title": "Nd-array Shape in UnischemaField", "user": {"login": "seranotannason", "id": 16227136, "node_id": "MDQ6VXNlcjE2MjI3MTM2", "avatar_url": "https://avatars0.githubusercontent.com/u/16227136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seranotannason", "html_url": "https://github.com/seranotannason", "followers_url": "https://api.github.com/users/seranotannason/followers", "following_url": "https://api.github.com/users/seranotannason/following{/other_user}", "gists_url": "https://api.github.com/users/seranotannason/gists{/gist_id}", "starred_url": "https://api.github.com/users/seranotannason/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seranotannason/subscriptions", "organizations_url": "https://api.github.com/users/seranotannason/orgs", "repos_url": "https://api.github.com/users/seranotannason/repos", "events_url": "https://api.github.com/users/seranotannason/events{/privacy}", "received_events_url": "https://api.github.com/users/seranotannason/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-08-07T17:48:17Z", "updated_at": "2019-09-10T13:28:56Z", "closed_at": "2019-09-10T13:28:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "When specifying the shape of an Nd-array in the creation of a UnischemaField object, one is allowed to specify None for the columns that have variable dimensions. Is there anything stopping one from listing None for all the columns (e.g. (None, None, None) for an image)? This may be handy when one is unsure whether a column may have variable dimensions.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/410", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/410/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/410/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/410/events", "html_url": "https://github.com/uber/petastorm/issues/410", "id": 466876558, "node_id": "MDU6SXNzdWU0NjY4NzY1NTg=", "number": 410, "title": "Segmentation fault with materialize_dataset and torch", "user": {"login": "mattiasarro", "id": 53719, "node_id": "MDQ6VXNlcjUzNzE5", "avatar_url": "https://avatars0.githubusercontent.com/u/53719?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mattiasarro", "html_url": "https://github.com/mattiasarro", "followers_url": "https://api.github.com/users/mattiasarro/followers", "following_url": "https://api.github.com/users/mattiasarro/following{/other_user}", "gists_url": "https://api.github.com/users/mattiasarro/gists{/gist_id}", "starred_url": "https://api.github.com/users/mattiasarro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mattiasarro/subscriptions", "organizations_url": "https://api.github.com/users/mattiasarro/orgs", "repos_url": "https://api.github.com/users/mattiasarro/repos", "events_url": "https://api.github.com/users/mattiasarro/events{/privacy}", "received_events_url": "https://api.github.com/users/mattiasarro/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-11T12:57:06Z", "updated_at": "2019-11-13T18:47:51Z", "closed_at": "2019-07-11T12:57:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "A segmentation fault occurs when you import `materialize_dataset` before `torch`. It happens inside an `ubuntu:16.04` Docker container but not natively on MacOS.\r\n\r\nThis produces a segfault with `pyarrow==0.13.0` and `pyarrow==0.14.0`, but works with `pyarrow==0.12.1`:\r\n\r\n```\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nimport torch\r\n```\r\n\r\nThis one works with newer versions of pyarrow:\r\n\r\n```\r\nimport torch\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\n```\r\n\r\nInitially it looked like this [pyarrow bug](https://issues.apache.org/jira/browse/ARROW-5130), but the workaround for that (placing import pyarrow in the beginning of a program) didn't work, and it shouldn't have affected `pyarrow==0.14.0`.\r\n\r\nClosing this issue since there is a workaround already. Just wanted to document this unexpected behaviour.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/406", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/406/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/406/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/406/events", "html_url": "https://github.com/uber/petastorm/issues/406", "id": 463440126, "node_id": "MDU6SXNzdWU0NjM0NDAxMjY=", "number": 406, "title": "Error while throwing runtime error in ArrowReaderWorker", "user": {"login": "praateekmahajan", "id": 7589415, "node_id": "MDQ6VXNlcjc1ODk0MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/7589415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/praateekmahajan", "html_url": "https://github.com/praateekmahajan", "followers_url": "https://api.github.com/users/praateekmahajan/followers", "following_url": "https://api.github.com/users/praateekmahajan/following{/other_user}", "gists_url": "https://api.github.com/users/praateekmahajan/gists{/gist_id}", "starred_url": "https://api.github.com/users/praateekmahajan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/praateekmahajan/subscriptions", "organizations_url": "https://api.github.com/users/praateekmahajan/orgs", "repos_url": "https://api.github.com/users/praateekmahajan/repos", "events_url": "https://api.github.com/users/praateekmahajan/events{/privacy}", "received_events_url": "https://api.github.com/users/praateekmahajan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-02T21:18:00Z", "updated_at": "2019-07-03T18:42:06Z", "closed_at": "2019-07-03T18:42:06Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "In ArrowReaderWorker, when trying to load a column of `list` type, where list is of variable length an error needs to be thrown. This error is defined [here](https://github.com/uber/petastorm/blob/ccf738e6efdc90f9643bdb6e20e064c7ba924379/petastorm/arrow_reader_worker.py#L67).\r\n\r\nHowever the error fails while printing \r\n\r\nInstead of \r\n`', '.join({values.shape[0] for value in ....})`\r\n\r\nIt should be \r\n`', '.join({str(values.shape[0]) for value in ....})`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/405", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/405/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/405/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/405/events", "html_url": "https://github.com/uber/petastorm/issues/405", "id": 462954065, "node_id": "MDU6SXNzdWU0NjI5NTQwNjU=", "number": 405, "title": "Loading a batch size i.e not equal to Row Group Size for PyTorch", "user": {"login": "praateekmahajan", "id": 7589415, "node_id": "MDQ6VXNlcjc1ODk0MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/7589415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/praateekmahajan", "html_url": "https://github.com/praateekmahajan", "followers_url": "https://api.github.com/users/praateekmahajan/followers", "following_url": "https://api.github.com/users/praateekmahajan/following{/other_user}", "gists_url": "https://api.github.com/users/praateekmahajan/gists{/gist_id}", "starred_url": "https://api.github.com/users/praateekmahajan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/praateekmahajan/subscriptions", "organizations_url": "https://api.github.com/users/praateekmahajan/orgs", "repos_url": "https://api.github.com/users/praateekmahajan/repos", "events_url": "https://api.github.com/users/praateekmahajan/events{/privacy}", "received_events_url": "https://api.github.com/users/praateekmahajan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-01T23:30:49Z", "updated_at": "2019-08-29T16:19:50Z", "closed_at": "2019-08-29T16:19:50Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Follow up from this issue here https://github.com/uber/petastorm/issues/389\r\n\r\nLooks like Tensorflow has utils such as `tf.data.Dataset.unbatch` that allows to get different batch sizes. However for PyTorch its unclear how to get a batch size that is the same as PyTorch [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\r\n\r\nGenerally for pytorch the following code would suffice :\r\n\r\n```\r\nwith DataLoader(make_batch_reader(petastorm_dataset_url), batch_size=2) as train_loader:\r\n  for i, data in enumerate(train_loader):\r\n      output = model(data)\r\n      .\r\n      .\r\n      . \r\n```\r\n\r\nHowever under this setting, could there be an example provided that helps people on the PyTorch boat decide how to use batches as per the Deep learning convention.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/401", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/401/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/401/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/401/events", "html_url": "https://github.com/uber/petastorm/issues/401", "id": 461452719, "node_id": "MDU6SXNzdWU0NjE0NTI3MTk=", "number": 401, "title": "transform_spec does not work with predicate ", "user": {"login": "GregAru", "id": 46323816, "node_id": "MDQ6VXNlcjQ2MzIzODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/46323816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregAru", "html_url": "https://github.com/GregAru", "followers_url": "https://api.github.com/users/GregAru/followers", "following_url": "https://api.github.com/users/GregAru/following{/other_user}", "gists_url": "https://api.github.com/users/GregAru/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregAru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregAru/subscriptions", "organizations_url": "https://api.github.com/users/GregAru/orgs", "repos_url": "https://api.github.com/users/GregAru/repos", "events_url": "https://api.github.com/users/GregAru/events{/privacy}", "received_events_url": "https://api.github.com/users/GregAru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-27T10:36:21Z", "updated_at": "2019-09-09T09:55:00Z", "closed_at": "2019-09-09T09:55:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I fail to get samples from the Reader when `transform_spec` and `predicate` are provided together in `make_reader` and operate on different fields.\r\n\r\nThe failure happens because the  transform function is activated with an input that does not include the transformed field. If I add a functionality in the transform function to return the input unchanged if the input fields do not include the transform field - everything works fine, but no transformation is done.\r\n\r\nFrom a short investigation:\r\nWith the predicate provided, the transform function is activated when calculating `decoded_predicate_rows` in `PyDictReaderWorker` line 195 and fails since the predicate fields are not the ones I want to transform. Additionally, no transformation is done when calculating the `decoded_other_rows` in line 220.\r\n\r\nThe only test I have found that includes both `predicate` and `transform_spec` is `test_transform_function_with_predicate_batched` in `test_end_to_end.py`\r\nIt passes because both the `predicate` and `transform_spec` operate on the same field.\r\n\r\nI am not providing the code, because the issue can be easily reproduced as long as both `transform_spec` and `predicate` are used, with different fields in each.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/399", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/399/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/399/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/399/events", "html_url": "https://github.com/uber/petastorm/issues/399", "id": 461037308, "node_id": "MDU6SXNzdWU0NjEwMzczMDg=", "number": 399, "title": "`make_batch_reader` returning different numbers of features, labels in the same read", "user": {"login": "srowen", "id": 822522, "node_id": "MDQ6VXNlcjgyMjUyMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/822522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srowen", "html_url": "https://github.com/srowen", "followers_url": "https://api.github.com/users/srowen/followers", "following_url": "https://api.github.com/users/srowen/following{/other_user}", "gists_url": "https://api.github.com/users/srowen/gists{/gist_id}", "starred_url": "https://api.github.com/users/srowen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srowen/subscriptions", "organizations_url": "https://api.github.com/users/srowen/orgs", "repos_url": "https://api.github.com/users/srowen/repos", "events_url": "https://api.github.com/users/srowen/events{/privacy}", "received_events_url": "https://api.github.com/users/srowen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-06-26T15:24:03Z", "updated_at": "2019-06-27T23:58:13Z", "closed_at": "2019-06-27T23:58:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I feel like I'm missing something basic here, so I apologize, but: I have a Parquet file (written by Spark) that simply contains a binary-valued column \"image\" and a int-valued column \"label\". If I run...\r\n\r\n```\r\nwith make_batch_reader(table_path_base_file + \"train/\") as train_reader:\r\n  for i in range(100):\r\n    next_sample = train_reader.next()\r\n    print(str(next_sample[0].shape) + \" \" + str(next_sample[1].shape))\r\n```\r\n\r\nThen I get output like:\r\n```\r\n(62,) (100,)\r\n(62,) (100,)\r\n(62,) (100,)\r\n(62,) (100,)\r\n(62,) (100,)\r\n(62,) (81,)\r\n(62,) (100,)\r\n(62,) (64,)\r\n...\r\n```\r\n\r\nThat is, each element from the Reader seems to have a different number of image and label values within the same read. I get 62 images with 100 labels. I'm pretty puzzled as of course each row is 1 image and 1 label.\r\n\r\nI understand getting different numbers of pairs per read, no problem, but not mismatched counts here. Of course that makes any training on this output fail.\r\n\r\nDid I miss something fundamental about how to use this? thank you for any pointers!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/396", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/396/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/396/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/396/events", "html_url": "https://github.com/uber/petastorm/issues/396", "id": 459325624, "node_id": "MDU6SXNzdWU0NTkzMjU2MjQ=", "number": 396, "title": "iterator from make_reader hangs after 10 epochs even if num_epochs=None", "user": {"login": "sdegryze", "id": 634714, "node_id": "MDQ6VXNlcjYzNDcxNA==", "avatar_url": "https://avatars0.githubusercontent.com/u/634714?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sdegryze", "html_url": "https://github.com/sdegryze", "followers_url": "https://api.github.com/users/sdegryze/followers", "following_url": "https://api.github.com/users/sdegryze/following{/other_user}", "gists_url": "https://api.github.com/users/sdegryze/gists{/gist_id}", "starred_url": "https://api.github.com/users/sdegryze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sdegryze/subscriptions", "organizations_url": "https://api.github.com/users/sdegryze/orgs", "repos_url": "https://api.github.com/users/sdegryze/repos", "events_url": "https://api.github.com/users/sdegryze/events{/privacy}", "received_events_url": "https://api.github.com/users/sdegryze/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-21T18:43:52Z", "updated_at": "2019-06-24T21:12:07Z", "closed_at": "2019-06-24T21:12:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "In the example below, I'm iterating over the MNIST parquet file generated by [the MNIST example](https://github.com/uber/petastorm/blob/master/examples/mnist/generate_petastorm_mnist.py)\r\n\r\n`num_epochs` is set to `None` to get the \"infinite number of epochs\" behavior.\r\n\r\nHowever, the iterator loop hangs after 600 batches. Given that the parquet file contains 60000 records and I requested 1000 as my batch size, this corresponds to 10 epochs.\r\n\r\nCode to reproduce:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom petastorm import make_reader\r\nfrom petastorm.tf_utils import make_petastorm_dataset\r\n\r\n\r\ndef streaming_parser(serialized_example):   \r\n    image_data = tf.cast(tf.reshape(serialized_example.image, [784]), tf.float32)\r\n    label = serialized_example.digit\r\n    return {\"image_data\": image_data}, label\r\n\r\n\r\nwith make_reader(\"s3://path/to/train\",\r\n                 num_epochs=None,\r\n                 cur_shard=0,\r\n                 shard_count=1) as reader:\r\n    exp_dataset = (make_petastorm_dataset(reader)\r\n                   .map(streaming_parser)\r\n                   .batch(1000))\r\n\r\nfeatures, labels = exp_dataset.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.local_variables_initializer())\r\n    sess.run(tf.global_variables_initializer())\r\n    cum_count = 0\r\n    for idx in range(610):\r\n        labels_manifested = sess.run([labels])\r\n        count = labels_manifested[0].shape[0]\r\n        cum_count += count\r\n        print(f\"Batch {idx}, contains {count} records, total records read is {cum_count}\")\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/395", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/395/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/395/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/395/events", "html_url": "https://github.com/uber/petastorm/issues/395", "id": 458577209, "node_id": "MDU6SXNzdWU0NTg1NzcyMDk=", "number": 395, "title": "Configure Petastrom to point to HDFS and SPARK", "user": {"login": "prakashmstpt", "id": 47393706, "node_id": "MDQ6VXNlcjQ3MzkzNzA2", "avatar_url": "https://avatars1.githubusercontent.com/u/47393706?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prakashmstpt", "html_url": "https://github.com/prakashmstpt", "followers_url": "https://api.github.com/users/prakashmstpt/followers", "following_url": "https://api.github.com/users/prakashmstpt/following{/other_user}", "gists_url": "https://api.github.com/users/prakashmstpt/gists{/gist_id}", "starred_url": "https://api.github.com/users/prakashmstpt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prakashmstpt/subscriptions", "organizations_url": "https://api.github.com/users/prakashmstpt/orgs", "repos_url": "https://api.github.com/users/prakashmstpt/repos", "events_url": "https://api.github.com/users/prakashmstpt/events{/privacy}", "received_events_url": "https://api.github.com/users/prakashmstpt/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-20T10:34:53Z", "updated_at": "2019-06-21T21:21:21Z", "closed_at": "2019-06-21T21:21:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "What are the configuration steps to point Hadoop cluster.\r\nJust passing hdfs://nn:8020/file is not working. Do we need any specific setup required in pyarrow?\r\nSimilarly do we need to specify spark-defults.conf\r\n\r\nI appreciate your input\r\n\r\nPS: if you have specific instruction for Cloudera or any other kerberos enabled cluster that will be useful as well.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/391", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/391/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/391/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/391/events", "html_url": "https://github.com/uber/petastorm/issues/391", "id": 456424270, "node_id": "MDU6SXNzdWU0NTY0MjQyNzA=", "number": 391, "title": "Train-Test Dataset Split", "user": {"login": "seranotannason", "id": 16227136, "node_id": "MDQ6VXNlcjE2MjI3MTM2", "avatar_url": "https://avatars0.githubusercontent.com/u/16227136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seranotannason", "html_url": "https://github.com/seranotannason", "followers_url": "https://api.github.com/users/seranotannason/followers", "following_url": "https://api.github.com/users/seranotannason/following{/other_user}", "gists_url": "https://api.github.com/users/seranotannason/gists{/gist_id}", "starred_url": "https://api.github.com/users/seranotannason/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seranotannason/subscriptions", "organizations_url": "https://api.github.com/users/seranotannason/orgs", "repos_url": "https://api.github.com/users/seranotannason/repos", "events_url": "https://api.github.com/users/seranotannason/events{/privacy}", "received_events_url": "https://api.github.com/users/seranotannason/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2019-06-14T20:36:39Z", "updated_at": "2019-10-23T12:13:47Z", "closed_at": "2019-06-21T21:21:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there currently support for splitting a Petastorm dataset into train-test for PyTorch? In PyTorch, one would typically do this to a Dataset class but since Petastorm only has the classes Reader and DataLoader (as below), I wonder if this feature has been implemented.\r\n\r\n`trainloader = DataLoader(make_reader('file://' + filename), batch_size=128)`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/380", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/380/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/380/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/380/events", "html_url": "https://github.com/uber/petastorm/issues/380", "id": 451590720, "node_id": "MDU6SXNzdWU0NTE1OTA3MjA=", "number": 380, "title": "Generating meta data for existing datasets", "user": {"login": "w1nk", "id": 307741, "node_id": "MDQ6VXNlcjMwNzc0MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/307741?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w1nk", "html_url": "https://github.com/w1nk", "followers_url": "https://api.github.com/users/w1nk/followers", "following_url": "https://api.github.com/users/w1nk/following{/other_user}", "gists_url": "https://api.github.com/users/w1nk/gists{/gist_id}", "starred_url": "https://api.github.com/users/w1nk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w1nk/subscriptions", "organizations_url": "https://api.github.com/users/w1nk/orgs", "repos_url": "https://api.github.com/users/w1nk/repos", "events_url": "https://api.github.com/users/w1nk/events{/privacy}", "received_events_url": "https://api.github.com/users/w1nk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-03T17:01:35Z", "updated_at": "2019-06-04T01:27:35Z", "closed_at": "2019-06-04T01:27:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello!\r\n\r\n  I've got an existing dataset of parquet files sitting in HDFS.  I'm trying to load them via petastorm, but I'm receiving errors that the meta data can't be located:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 391, in infer_or_load_unischema\r\n    return get_schema(dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 345, in get_schema\r\n    'Could not find _common_metadata file. Use materialize_dataset(..) in'\r\npetastorm.etl.dataset_metadata.PetastormMetadataError: Could not find _common_metadata file. Use materialize_dataset(..) in petastorm.etl.dataset_metadata.py to generate this file in your ETL code. You can generate it on an existing dataset using petastorm-generate-metadata.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    pytorch_word2vec(dataset_url=\"hdfs://path_to/parquets\")\r\n  File \"test.py\", line 8, in pytorch_word2vec\r\n    with DataLoader(make_batch_reader(dataset_url)) as train_loader:\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/reader.py\", line 350, in make_batch_reader\r\n    transform_spec=transform_spec)\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/reader.py\", line 420, in __init__\r\n    stored_schema = infer_or_load_unischema(self.dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 395, in infer_or_load_unischema\r\n    return Unischema.from_arrow_schema(dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/unischema.py\", line 251, in from_arrow_schema\r\n    meta = parquet_dataset.pieces[0].get_metadata(parquet_dataset.fs.open)\r\nIndexError: list index out of range\r\n```\r\n\r\n\r\nThis makes a bit of sense, as I don't have any petastorm specific metadata at the moment.  I took the advice of the error and tried to run petastorm-generate-metadata.py , however when I do that, I receive the same error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/bin/petastorm-generate-metadata.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/etl/petastorm_generate_metadata.py\", line 155, in main\r\n    _main(sys.argv[1:])\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/etl/petastorm_generate_metadata.py\", line 148, in _main\r\n    hdfs_driver=args.hdfs_driver)\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/etl/petastorm_generate_metadata.py\", line 77, in generate_petastorm_metadata\r\n    schema = get_schema(dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 345, in get_schema\r\n    'Could not find _common_metadata file. Use materialize_dataset(..) in'\r\npetastorm.etl.dataset_metadata.PetastormMetadataError: Could not find _common_metadata file. Use materialize_dataset(..) in petastorm.etl.dataset_metadata.py to generate this file in your ETL code. You can generate it on an existing dataset using petastorm-generate-metadata.py\r\n```\r\n\r\nIs there anyway to create the metadata without regenerating my parquet dataset through the petastorm interface?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/379", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/379/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/379/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/379/events", "html_url": "https://github.com/uber/petastorm/issues/379", "id": 450750816, "node_id": "MDU6SXNzdWU0NTA3NTA4MTY=", "number": 379, "title": "Regarding performance of different read dataset methods", "user": {"login": "panfengfeng", "id": 3929979, "node_id": "MDQ6VXNlcjM5Mjk5Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3929979?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panfengfeng", "html_url": "https://github.com/panfengfeng", "followers_url": "https://api.github.com/users/panfengfeng/followers", "following_url": "https://api.github.com/users/panfengfeng/following{/other_user}", "gists_url": "https://api.github.com/users/panfengfeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/panfengfeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panfengfeng/subscriptions", "organizations_url": "https://api.github.com/users/panfengfeng/orgs", "repos_url": "https://api.github.com/users/panfengfeng/repos", "events_url": "https://api.github.com/users/panfengfeng/events{/privacy}", "received_events_url": "https://api.github.com/users/panfengfeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-31T11:24:18Z", "updated_at": "2019-06-08T06:50:04Z", "closed_at": "2019-06-08T06:50:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I use make_petastorm_dataset API (make_petastorm_dataset(reader)) to scan the dataset, I found that it is much worse than reader loop method (for sample in reader), do you have any ideas about the make_petastorm_dataset API ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/378", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/378/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/378/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/378/events", "html_url": "https://github.com/uber/petastorm/issues/378", "id": 450708498, "node_id": "MDU6SXNzdWU0NTA3MDg0OTg=", "number": 378, "title": "Support to get sample by index", "user": {"login": "un-knight", "id": 8266614, "node_id": "MDQ6VXNlcjgyNjY2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/8266614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/un-knight", "html_url": "https://github.com/un-knight", "followers_url": "https://api.github.com/users/un-knight/followers", "following_url": "https://api.github.com/users/un-knight/following{/other_user}", "gists_url": "https://api.github.com/users/un-knight/gists{/gist_id}", "starred_url": "https://api.github.com/users/un-knight/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/un-knight/subscriptions", "organizations_url": "https://api.github.com/users/un-knight/orgs", "repos_url": "https://api.github.com/users/un-knight/repos", "events_url": "https://api.github.com/users/un-knight/events{/privacy}", "received_events_url": "https://api.github.com/users/un-knight/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-31T09:38:47Z", "updated_at": "2019-05-31T19:21:44Z", "closed_at": "2019-05-31T19:21:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want to get sample by index, but there seems no way to do this, sine `Reader` get samples only by walking through the row. In this case, I can't even shuffle the samples by index.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/377", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/377/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/377/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/377/events", "html_url": "https://github.com/uber/petastorm/issues/377", "id": 450628091, "node_id": "MDU6SXNzdWU0NTA2MjgwOTE=", "number": 377, "title": "Sharding for distribtued ", "user": {"login": "un-knight", "id": 8266614, "node_id": "MDQ6VXNlcjgyNjY2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/8266614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/un-knight", "html_url": "https://github.com/un-knight", "followers_url": "https://api.github.com/users/un-knight/followers", "following_url": "https://api.github.com/users/un-knight/following{/other_user}", "gists_url": "https://api.github.com/users/un-knight/gists{/gist_id}", "starred_url": "https://api.github.com/users/un-knight/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/un-knight/subscriptions", "organizations_url": "https://api.github.com/users/un-knight/orgs", "repos_url": "https://api.github.com/users/un-knight/repos", "events_url": "https://api.github.com/users/un-knight/events{/privacy}", "received_events_url": "https://api.github.com/users/un-knight/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-05-31T05:58:12Z", "updated_at": "2019-08-30T05:10:13Z", "closed_at": "2019-08-30T05:10:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "[Uber petastorm blog](https://eng.uber.com/petastorm) said that petastorm support sharding for distributed training, but I can't find the relative api in [petastorm api document](https://petastorm.readthedocs.io/en/latest/api.html). So will this feature support in the future? \r\n\r\nBesides, I want to know will petastorm support pytorch **sampler functions** and **parallel transformation** in `Dataloader` ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/376", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/376/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/376/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/376/events", "html_url": "https://github.com/uber/petastorm/issues/376", "id": 448080288, "node_id": "MDU6SXNzdWU0NDgwODAyODg=", "number": 376, "title": "Reading Parquet files stored on s3 using petastorm generates connection warnings", "user": {"login": "keurcien", "id": 11570911, "node_id": "MDQ6VXNlcjExNTcwOTEx", "avatar_url": "https://avatars0.githubusercontent.com/u/11570911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keurcien", "html_url": "https://github.com/keurcien", "followers_url": "https://api.github.com/users/keurcien/followers", "following_url": "https://api.github.com/users/keurcien/following{/other_user}", "gists_url": "https://api.github.com/users/keurcien/gists{/gist_id}", "starred_url": "https://api.github.com/users/keurcien/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keurcien/subscriptions", "organizations_url": "https://api.github.com/users/keurcien/orgs", "repos_url": "https://api.github.com/users/keurcien/repos", "events_url": "https://api.github.com/users/keurcien/events{/privacy}", "received_events_url": "https://api.github.com/users/keurcien/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-24T09:51:59Z", "updated_at": "2019-06-17T12:42:02Z", "closed_at": "2019-05-31T18:59:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a Tensorflow model that I would like to feed with parquet files stored on s3. I'm using `petastorm` to query these files from s3 and the result of the query is stored as a Tensorflow dataset thanks to `petastorm.tf_utils.make_petastorm_dataset`.\r\n\r\n````python\r\nimport s3fs\r\nfrom pyarrow.filesystem import S3FSWrapper\r\nfrom petastorm.reader import Reader\r\nfrom petastorm.tf_utils import make_petastorm_dataset\r\n\r\ndataset_url = \"analytics.xxx.xxx\" #s3 bucket name\r\n\r\nfs = s3fs.S3FileSystem()\r\nwrapped_fs = S3FSWrapper(fs)\r\n\r\nwith Reader(pyarrow_filesystem=wrapped_fs, dataset_path=dataset_url) as reader:\r\n    dataset = make_petastorm_dataset(reader)\r\n````\r\n\r\nThis works pretty well, except that it generates 20+ lines of connection warnings:\r\n\r\n```\r\nW0514 18:56:42.779965 140231344908032 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com\r\nW0514 18:56:42.782773 140231311337216 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com\r\nW0514 18:56:42.854569 140232468973312 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com\r\nW0514 18:56:42.868761 140231328122624 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com\r\nW0514 18:56:42.885518 140230816429824 connectionpool.py:274] Connection pool is full, discarding connection: s3.eu-west-1.amazonaws.com\r\n...\r\n```\r\n\r\nAccording to this thread https://stackoverflow.com/questions/53765366/urllib3-connectionpool-connection-pool-is-full-discarding-connection, it's certainly related to `urllib3`, but I can't figure a way to get rid of these warnings.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/375", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/375/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/375/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/375/events", "html_url": "https://github.com/uber/petastorm/issues/375", "id": 446826372, "node_id": "MDU6SXNzdWU0NDY4MjYzNzI=", "number": 375, "title": "dfsclient warning", "user": {"login": "bluesummers1129", "id": 48036759, "node_id": "MDQ6VXNlcjQ4MDM2NzU5", "avatar_url": "https://avatars0.githubusercontent.com/u/48036759?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bluesummers1129", "html_url": "https://github.com/bluesummers1129", "followers_url": "https://api.github.com/users/bluesummers1129/followers", "following_url": "https://api.github.com/users/bluesummers1129/following{/other_user}", "gists_url": "https://api.github.com/users/bluesummers1129/gists{/gist_id}", "starred_url": "https://api.github.com/users/bluesummers1129/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bluesummers1129/subscriptions", "organizations_url": "https://api.github.com/users/bluesummers1129/orgs", "repos_url": "https://api.github.com/users/bluesummers1129/repos", "events_url": "https://api.github.com/users/bluesummers1129/events{/privacy}", "received_events_url": "https://api.github.com/users/bluesummers1129/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-21T21:08:49Z", "updated_at": "2019-05-22T00:26:22Z", "closed_at": "2019-05-22T00:26:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, this may be more of a pyarrow question than a petastorm question, but when I use the make_batch_reader method and read through parquet files, once it reaches the EOF I get warnings like:\r\n\r\n```\r\n[1,49]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,24]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,0]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,55]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,48]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,25]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,23]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,28]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,4]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,59]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,39]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n[1,32]<stderr>:19/05/21 21:07:32 WARN hdfs.DFSClient: zero\r\n```\r\n\r\nWhen I only have a few workers this is not a huge deal, but when I have hundreds of processes using petastorm this adds a lot of spurious output. Is there a way to suppress these warning messages?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/372", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/372/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/372/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/372/events", "html_url": "https://github.com/uber/petastorm/issues/372", "id": 443481716, "node_id": "MDU6SXNzdWU0NDM0ODE3MTY=", "number": 372, "title": "Segmentation fault when using make_batch_reader", "user": {"login": "janhaviag", "id": 50589828, "node_id": "MDQ6VXNlcjUwNTg5ODI4", "avatar_url": "https://avatars2.githubusercontent.com/u/50589828?v=4", "gravatar_id": "", "url": "https://api.github.com/users/janhaviag", "html_url": "https://github.com/janhaviag", "followers_url": "https://api.github.com/users/janhaviag/followers", "following_url": "https://api.github.com/users/janhaviag/following{/other_user}", "gists_url": "https://api.github.com/users/janhaviag/gists{/gist_id}", "starred_url": "https://api.github.com/users/janhaviag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/janhaviag/subscriptions", "organizations_url": "https://api.github.com/users/janhaviag/orgs", "repos_url": "https://api.github.com/users/janhaviag/repos", "events_url": "https://api.github.com/users/janhaviag/events{/privacy}", "received_events_url": "https://api.github.com/users/janhaviag/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-13T16:05:21Z", "updated_at": "2019-05-14T10:17:43Z", "closed_at": "2019-05-14T10:17:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi!\r\n\r\n I am trying to read a non-Petastorm parquet file in pytorch but just get the error:\r\n\r\n>  Segmentation fault (core dumped)\r\n\r\nThe code I am using is from examples/hello_world/external_datasets/pytorch_hello_world.py:\r\n\r\n`\r\n\r\n    with DataLoader(make_batch_reader(file_path_to_my_parquet_file)) as train_loader:\r\n        sample = next(iter(train_loader))\r\n        print(\"id batch: {0}\".format(sample['id']))\r\n`\r\nAny idea why this might be happening?\r\n\r\nThanks,\r\nJanhavi\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/356", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/356/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/356/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/356/events", "html_url": "https://github.com/uber/petastorm/issues/356", "id": 431395895, "node_id": "MDU6SXNzdWU0MzEzOTU4OTU=", "number": 356, "title": "arrow_reader_worker.py: to_pandas() of read_next() move to ArrowReaderWorker.process(*)", "user": {"login": "panfengfeng", "id": 3929979, "node_id": "MDQ6VXNlcjM5Mjk5Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3929979?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panfengfeng", "html_url": "https://github.com/panfengfeng", "followers_url": "https://api.github.com/users/panfengfeng/followers", "following_url": "https://api.github.com/users/panfengfeng/following{/other_user}", "gists_url": "https://api.github.com/users/panfengfeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/panfengfeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panfengfeng/subscriptions", "organizations_url": "https://api.github.com/users/panfengfeng/orgs", "repos_url": "https://api.github.com/users/panfengfeng/repos", "events_url": "https://api.github.com/users/panfengfeng/events{/privacy}", "received_events_url": "https://api.github.com/users/panfengfeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-04-10T09:23:44Z", "updated_at": "2019-04-16T08:39:03Z", "closed_at": "2019-04-16T08:39:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "after getting result from result_queue, petastorm transforms result_table to result_dict, during this transformation, to_pandas will be called. Can we move this transformation into ArrowReaderWorker.process(*) (using multi-thread), and so petastorm can get the final result from the result_queue directly ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/354", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/354/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/354/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/354/events", "html_url": "https://github.com/uber/petastorm/issues/354", "id": 431104899, "node_id": "MDU6SXNzdWU0MzExMDQ4OTk=", "number": 354, "title": "make_batch_reader with nullable field, TypeError: an integer is required", "user": {"login": "working-estimate", "id": 10961759, "node_id": "MDQ6VXNlcjEwOTYxNzU5", "avatar_url": "https://avatars1.githubusercontent.com/u/10961759?v=4", "gravatar_id": "", "url": "https://api.github.com/users/working-estimate", "html_url": "https://github.com/working-estimate", "followers_url": "https://api.github.com/users/working-estimate/followers", "following_url": "https://api.github.com/users/working-estimate/following{/other_user}", "gists_url": "https://api.github.com/users/working-estimate/gists{/gist_id}", "starred_url": "https://api.github.com/users/working-estimate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/working-estimate/subscriptions", "organizations_url": "https://api.github.com/users/working-estimate/orgs", "repos_url": "https://api.github.com/users/working-estimate/repos", "events_url": "https://api.github.com/users/working-estimate/events{/privacy}", "received_events_url": "https://api.github.com/users/working-estimate/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-04-09T18:09:19Z", "updated_at": "2019-04-12T16:40:20Z", "closed_at": "2019-04-12T16:40:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a simple parquet file with the schema:\r\n```\r\n root\r\n |-- id: string (nullable = true)\r\n |-- feat1: array (nullable = true)\r\n |    |-- element: string (containsNull = true)\r\n |-- feat2: string (nullable = true)\r\n |-- feat3: array (nullable = true)\r\n |    |-- element: string (containsNull = true)\r\n |-- feat4: array (nullable = true)\r\n |    |-- element: string (containsNull = true)\r\n |-- feat5: array (nullable = true)\r\n |    |-- element: string (containsNull = true)\r\n |-- feat6: integer (nullable = true) \r\n```\r\n\r\nwhen I try to read it in with the hello world example, I get many instances of the following error:\r\n\r\n```\r\nWorker 2 terminated: unexpected exception:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/workers_pool/thread_pool.py\", line 62, in run\r\n    self._worker_impl.process(*args, **kargs)\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 138, in process\r\n    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/cache.py\", line 39, in get\r\n    return fill_cache_func()\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 138, in <lambda>\r\n    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 152, in _load_rows\r\n    result = self._read_with_shuffle_row_drop(piece, pq_file, column_names, shuffle_row_drop_range)\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 224, in _read_with_shuffle_row_drop\r\n    partitions=self._dataset.partitions\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyarrow/parquet.py\", line 562, in read\r\n    table = reader.read_row_group(self.row_group, **options)\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyarrow/parquet.py\", line 188, in read_row_group\r\n    use_threads=use_threads)\r\n  File \"pyarrow/_parquet.pyx\", line 696, in pyarrow._parquet.ParquetReader.read_row_group\r\nTypeError: an integer is required\r\n```\r\n\r\nfrom several workers. What could be the root cause here?\r\n\r\nHere's the traceback using a dummy pool:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"petastorm_genome.py\", line 19, in <module>\r\n    python_hello_world()\r\n  File \"petastorm_genome.py\", line 12, in python_hello_world\r\n    for schema_view in reader:\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/reader.py\", line 622, in __next__\r\n    return self._results_queue_reader.read_next(self._workers_pool, self.schema, self.ngram)\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 42, in read_next\r\n    result_table = workers_pool.get_results()\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/workers_pool/dummy_pool.py\", line 72, in get_results\r\n    self._worker.process(*args, **kargs)\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 138, in process\r\n    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/cache.py\", line 39, in get\r\n    return fill_cache_func()\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 138, in <lambda>\r\n    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 152, in _load_rows\r\n    result = self._read_with_shuffle_row_drop(piece, pq_file, column_names, shuffle_row_drop_range)\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py\", line 224, in _read_with_shuffle_row_drop\r\n    partitions=self._dataset.partitions\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyarrow/parquet.py\", line 562, in read\r\n    table = reader.read_row_group(self.row_group, **options)\r\n  File \"/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyarrow/parquet.py\", line 188, in read_row_group\r\n    use_threads=use_threads)\r\n  File \"pyarrow/_parquet.pyx\", line 696, in pyarrow._parquet.ParquetReader.read_row_group\r\nTypeError: an integer is required\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/353", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/353/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/353/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/353/events", "html_url": "https://github.com/uber/petastorm/issues/353", "id": 430707270, "node_id": "MDU6SXNzdWU0MzA3MDcyNzA=", "number": 353, "title": "make_batch_reader fails on nested struct", "user": {"login": "working-estimate", "id": 10961759, "node_id": "MDQ6VXNlcjEwOTYxNzU5", "avatar_url": "https://avatars1.githubusercontent.com/u/10961759?v=4", "gravatar_id": "", "url": "https://api.github.com/users/working-estimate", "html_url": "https://github.com/working-estimate", "followers_url": "https://api.github.com/users/working-estimate/followers", "following_url": "https://api.github.com/users/working-estimate/following{/other_user}", "gists_url": "https://api.github.com/users/working-estimate/gists{/gist_id}", "starred_url": "https://api.github.com/users/working-estimate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/working-estimate/subscriptions", "organizations_url": "https://api.github.com/users/working-estimate/orgs", "repos_url": "https://api.github.com/users/working-estimate/repos", "events_url": "https://api.github.com/users/working-estimate/events{/privacy}", "received_events_url": "https://api.github.com/users/working-estimate/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-08T23:56:44Z", "updated_at": "2019-05-16T16:09:13Z", "closed_at": "2019-05-16T16:08:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have an external parquet file that I'm trying to load using the hello world example, but it fails as such:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/341", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/341/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/341/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/341/events", "html_url": "https://github.com/uber/petastorm/issues/341", "id": 427574399, "node_id": "MDU6SXNzdWU0Mjc1NzQzOTk=", "number": 341, "title": "Add shuffle for pytorch dataloader", "user": {"login": "DeliangFan", "id": 8185735, "node_id": "MDQ6VXNlcjgxODU3MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/8185735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DeliangFan", "html_url": "https://github.com/DeliangFan", "followers_url": "https://api.github.com/users/DeliangFan/followers", "following_url": "https://api.github.com/users/DeliangFan/following{/other_user}", "gists_url": "https://api.github.com/users/DeliangFan/gists{/gist_id}", "starred_url": "https://api.github.com/users/DeliangFan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DeliangFan/subscriptions", "organizations_url": "https://api.github.com/users/DeliangFan/orgs", "repos_url": "https://api.github.com/users/DeliangFan/repos", "events_url": "https://api.github.com/users/DeliangFan/events{/privacy}", "received_events_url": "https://api.github.com/users/DeliangFan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-01T08:27:53Z", "updated_at": "2019-08-30T05:04:22Z", "closed_at": "2019-08-30T05:04:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Since the shuffling is not well supported by petastorm pytorch dataloader.  We need to improve the shuffle feature.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/337", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/337/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/337/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/337/events", "html_url": "https://github.com/uber/petastorm/issues/337", "id": 425473829, "node_id": "MDU6SXNzdWU0MjU0NzM4Mjk=", "number": 337, "title": "Dataset generation from a stream", "user": {"login": "GregAru", "id": 46323816, "node_id": "MDQ6VXNlcjQ2MzIzODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/46323816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregAru", "html_url": "https://github.com/GregAru", "followers_url": "https://api.github.com/users/GregAru/followers", "following_url": "https://api.github.com/users/GregAru/following{/other_user}", "gists_url": "https://api.github.com/users/GregAru/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregAru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregAru/subscriptions", "organizations_url": "https://api.github.com/users/GregAru/orgs", "repos_url": "https://api.github.com/users/GregAru/repos", "events_url": "https://api.github.com/users/GregAru/events{/privacy}", "received_events_url": "https://api.github.com/users/GregAru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-03-26T15:04:23Z", "updated_at": "2019-03-31T10:03:44Z", "closed_at": "2019-03-31T10:03:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Not quite an issue but a question.\r\nIs it possible currently to generate a Petastorm Dataset form a streaming data source by using Spark Structured Streaming or any other framework?\r\n\r\n\r\n ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/333", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/333/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/333/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/333/events", "html_url": "https://github.com/uber/petastorm/issues/333", "id": 421192443, "node_id": "MDU6SXNzdWU0MjExOTI0NDM=", "number": 333, "title": "Failure in case of a null value in an integer scalar field", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-03-14T18:59:35Z", "updated_at": "2019-03-28T04:32:40Z", "closed_at": "2019-03-28T04:32:40Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "If a dataset contains a nullable integer field with a null value, reading the dataset fails (using `make_reader`):\r\n\r\n\r\n```\r\nFile \"/lib/python_env/petastorm/workers_pool/dummy_pool.py\", line 72, in get_results\r\n    self._worker.process(*args, **kargs)\r\n  File \"/lib/python_env/petastorm/py_dict_reader_worker.py\", line 142, in process\r\n    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\r\n  File \"/lib/python_env/petastorm/cache.py\", line 39, in get\r\n    return fill_cache_func()\r\n  File \"/lib/python_env/petastorm/py_dict_reader_worker.py\", line 142, in <lambda>\r\n    lambda: self._load_rows(parquet_file, piece, shuffle_row_drop_partition))\r\n  File \"/lib/python_env/petastorm/py_dict_reader_worker.py\", line 161, in _load_rows\r\n    return [transform_func(utils.decode_row(row, self._schema)) for row in all_rows]\r\n  File \"/lib/python_env/petastorm/utils.py\", line 60, in decode_row\r\n    decoded_row[field_name] = codec.decode(schema.fields[field_name], row[field_name])\r\n  File \"/lib/python_env/petastorm/codecs.py\", line 205, in decode\r\n    return unischema_field.numpy_dtype(value)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/332", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/332/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/332/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/332/events", "html_url": "https://github.com/uber/petastorm/issues/332", "id": 421144530, "node_id": "MDU6SXNzdWU0MjExNDQ1MzA=", "number": 332, "title": "Make it easier to specify fields in n-gram", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-14T17:12:30Z", "updated_at": "2019-06-29T04:57:38Z", "closed_at": "2019-06-29T04:57:37Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "`make_reader` takes either a list of UnischemaField objects or regular expressions. Using regular expression in `schema_fields=` argument simplifies user code. `NGram` takes only `UnischemaFields` as a parameter. \r\n\r\nLet's follow the same pattern and allow user to pass a list of regular expressions during `NGram` construction.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/327", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/327/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/327/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/327/events", "html_url": "https://github.com/uber/petastorm/issues/327", "id": 420442381, "node_id": "MDU6SXNzdWU0MjA0NDIzODE=", "number": 327, "title": "UnischemaField name field cannot set `name` value ", "user": {"login": "wangqiaoshi", "id": 2505074, "node_id": "MDQ6VXNlcjI1MDUwNzQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2505074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangqiaoshi", "html_url": "https://github.com/wangqiaoshi", "followers_url": "https://api.github.com/users/wangqiaoshi/followers", "following_url": "https://api.github.com/users/wangqiaoshi/following{/other_user}", "gists_url": "https://api.github.com/users/wangqiaoshi/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangqiaoshi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangqiaoshi/subscriptions", "organizations_url": "https://api.github.com/users/wangqiaoshi/orgs", "repos_url": "https://api.github.com/users/wangqiaoshi/repos", "events_url": "https://api.github.com/users/wangqiaoshi/events{/privacy}", "received_events_url": "https://api.github.com/users/wangqiaoshi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-03-13T11:11:01Z", "updated_at": "2019-04-10T20:40:15Z", "closed_at": "2019-04-10T20:40:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "code:\r\ntest_schema = Unischema('test_schema', [\r\n    ...:         UnischemaField('id', np.int32, (), ScalarCodec(IntegerType()), False),\r\n    ...:         UnischemaField('name', np.string, (), ScalarCodec(StringType()), False)\r\n    ...:     ])\r\n\r\nerror:\r\n 114         # Generates attributes named by the field names as an access syntax sugar.\r\n    115         for f in fields:\r\n--> 116             setattr(self, f.name, f)\r\n    117 \r\n    118     def create_schema_view(self, fields):\r\n\r\nAttributeError: can't set attribute", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/321", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/321/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/321/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/321/events", "html_url": "https://github.com/uber/petastorm/issues/321", "id": 415826343, "node_id": "MDU6SXNzdWU0MTU4MjYzNDM=", "number": 321, "title": "Error while creating a dataset (ArrowIOError: Invalid parquet file. Corrupt footer)", "user": {"login": "fralik", "id": 191278, "node_id": "MDQ6VXNlcjE5MTI3OA==", "avatar_url": "https://avatars0.githubusercontent.com/u/191278?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fralik", "html_url": "https://github.com/fralik", "followers_url": "https://api.github.com/users/fralik/followers", "following_url": "https://api.github.com/users/fralik/following{/other_user}", "gists_url": "https://api.github.com/users/fralik/gists{/gist_id}", "starred_url": "https://api.github.com/users/fralik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fralik/subscriptions", "organizations_url": "https://api.github.com/users/fralik/orgs", "repos_url": "https://api.github.com/users/fralik/repos", "events_url": "https://api.github.com/users/fralik/events{/privacy}", "received_events_url": "https://api.github.com/users/fralik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-02-28T21:14:50Z", "updated_at": "2019-09-10T21:55:23Z", "closed_at": "2019-03-14T12:38:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to make a dataset from existing data, but the process fails with the error message `rrowIOError: Invalid parquet file. Corrupt footer`.\r\n\r\nHere is my setup. I am using Spark on databricks and have loaded a dataframe into variable `a` with the following schema:\r\n```\r\na.printSchema()\r\nroot\r\n |-- DataPointId: long (nullable = true)\r\n |-- Label: integer (nullable = true)\r\n |-- wtc_TORGear_counts: double (nullable = true)\r\n |-- wtc_UPSLowBt_counts: double (nullable = true)\r\n |-- wtc_IO1TRef2_mean: double (nullable = true)\r\n```\r\n\r\nAnd my code:\r\n```\r\nimport petastorm\r\nfrom petastorm.codecs import ScalarCodec, CompressedImageCodec, NdarrayCodec\r\nfrom petastorm.etl.dataset_metadata import materialize_dataset\r\nfrom petastorm.unischema import dict_to_spark_row, Unischema, UnischemaField\r\n\r\nds_url = 'file:///dbfs' + working_dir_spark + 'petastorm-training'\r\n\r\nHelloWorldSchema = petastorm.unischema.Unischema('HelloWorldSchema', [\r\n   petastorm.unischema.UnischemaField('DataPointId', np.long, (), ScalarCodec(LongType()), False),\r\n   petastorm.unischema.UnischemaField('Label', np.int64, (), ScalarCodec(IntegerType()), False),\r\n   UnischemaField('wtc_TORGear_counts', np.double, (), ScalarCodec(DoubleType()), False),\r\n   UnischemaField('wtc_UPSLowBt_counts', np.double, (), ScalarCodec(DoubleType()), False),\r\n   UnischemaField('wtc_IO1TRef2_mean', np.double, (), ScalarCodec(DoubleType()), False),\r\n])\r\n\r\nwith petastorm.etl.dataset_metadata.materialize_dataset(spark, ds_url, HelloWorldSchema):\r\n  a.write.parquet(ds_url, mode='overwrite')\r\n```\r\n\r\nI got the same error if I try to use `make_batch_reader`. I am using Python 3.6, petastorm version is 0.6.0, pyarrow version is 0.12.1.\r\n\r\nDoes anyone know how to make it working?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/312", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/312/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/312/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/312/events", "html_url": "https://github.com/uber/petastorm/issues/312", "id": 409912355, "node_id": "MDU6SXNzdWU0MDk5MTIzNTU=", "number": 312, "title": "make_batch_reader fails with unclear assertion error", "user": {"login": "GregAru", "id": 46323816, "node_id": "MDQ6VXNlcjQ2MzIzODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/46323816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregAru", "html_url": "https://github.com/GregAru", "followers_url": "https://api.github.com/users/GregAru/followers", "following_url": "https://api.github.com/users/GregAru/following{/other_user}", "gists_url": "https://api.github.com/users/GregAru/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregAru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregAru/subscriptions", "organizations_url": "https://api.github.com/users/GregAru/orgs", "repos_url": "https://api.github.com/users/GregAru/repos", "events_url": "https://api.github.com/users/GregAru/events{/privacy}", "received_events_url": "https://api.github.com/users/GregAru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-02-13T17:14:58Z", "updated_at": "2019-02-17T06:39:09Z", "closed_at": "2019-02-17T06:39:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I am experiencing the following error when trying to read an S3 dataset using make_batch_reader.\r\nI am using the latest master. \r\n\r\n_lib/python3.6/site-packages/petastorm/arrow_reader_worker.py\", line 50, in read_next\r\n    assert len(column.data.chunks) == 1\r\nAssertionError_\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/308", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/308/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/308/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/308/events", "html_url": "https://github.com/uber/petastorm/issues/308", "id": 407636436, "node_id": "MDU6SXNzdWU0MDc2MzY0MzY=", "number": 308, "title": "Can't use materialize_dataset with libhdfs", "user": {"login": "maver1ck", "id": 4006010, "node_id": "MDQ6VXNlcjQwMDYwMTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/4006010?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maver1ck", "html_url": "https://github.com/maver1ck", "followers_url": "https://api.github.com/users/maver1ck/followers", "following_url": "https://api.github.com/users/maver1ck/following{/other_user}", "gists_url": "https://api.github.com/users/maver1ck/gists{/gist_id}", "starred_url": "https://api.github.com/users/maver1ck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maver1ck/subscriptions", "organizations_url": "https://api.github.com/users/maver1ck/orgs", "repos_url": "https://api.github.com/users/maver1ck/repos", "events_url": "https://api.github.com/users/maver1ck/events{/privacy}", "received_events_url": "https://api.github.com/users/maver1ck/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2019-02-07T10:41:48Z", "updated_at": "2019-02-20T19:08:02Z", "closed_at": "2019-02-20T19:08:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using materialize_dataset there is no option to set hdfs_driver.\r\nIt's by default set to libhdfs3.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/306", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/306/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/306/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/306/events", "html_url": "https://github.com/uber/petastorm/issues/306", "id": 406223548, "node_id": "MDU6SXNzdWU0MDYyMjM1NDg=", "number": 306, "title": "Using predicate with local cache", "user": {"login": "GregAru", "id": 46323816, "node_id": "MDQ6VXNlcjQ2MzIzODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/46323816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregAru", "html_url": "https://github.com/GregAru", "followers_url": "https://api.github.com/users/GregAru/followers", "following_url": "https://api.github.com/users/GregAru/following{/other_user}", "gists_url": "https://api.github.com/users/GregAru/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregAru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregAru/subscriptions", "organizations_url": "https://api.github.com/users/GregAru/orgs", "repos_url": "https://api.github.com/users/GregAru/repos", "events_url": "https://api.github.com/users/GregAru/events{/privacy}", "received_events_url": "https://api.github.com/users/GregAru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-02-04T08:27:37Z", "updated_at": "2019-02-07T17:56:10Z", "closed_at": "2019-02-07T17:36:57Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\nWhen trying to query a dataset with both predicate and cache (aka: `predicate` and `cache_location`, `cache_type`, `cache_size_limit` and `cache_row_size_estimate`) arguments provided in `make_reader`, I get the following exception:\r\n\r\n_/petastorm/arrow_reader_worker.py\", line 121, in process\r\n    raise RuntimeError('Local cache is not supported together with predicates, '\r\nRuntimeError: Local cache is not supported together with predicates, unless the dataset is partitioned by the column the predicate operates on._\r\n\r\nThis is extremely limiting. \r\nAre there plans to make cache work with the predicate?\r\n \r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/304", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/304/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/304/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/304/events", "html_url": "https://github.com/uber/petastorm/issues/304", "id": 403361552, "node_id": "MDU6SXNzdWU0MDMzNjE1NTI=", "number": 304, "title": "Migrate to pyarrow 0.12", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2019-01-25T22:42:37Z", "updated_at": "2019-08-30T05:04:36Z", "closed_at": "2019-08-30T05:04:36Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Let's upgrade our test image and make sure all tests pass", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/300", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/300/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/300/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/300/events", "html_url": "https://github.com/uber/petastorm/issues/300", "id": 402765884, "node_id": "MDU6SXNzdWU0MDI3NjU4ODQ=", "number": 300, "title": "make_batch_reader fails with S3", "user": {"login": "GregAru", "id": 46323816, "node_id": "MDQ6VXNlcjQ2MzIzODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/46323816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregAru", "html_url": "https://github.com/GregAru", "followers_url": "https://api.github.com/users/GregAru/followers", "following_url": "https://api.github.com/users/GregAru/following{/other_user}", "gists_url": "https://api.github.com/users/GregAru/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregAru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregAru/subscriptions", "organizations_url": "https://api.github.com/users/GregAru/orgs", "repos_url": "https://api.github.com/users/GregAru/repos", "events_url": "https://api.github.com/users/GregAru/events{/privacy}", "received_events_url": "https://api.github.com/users/GregAru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2019-01-24T15:39:17Z", "updated_at": "2019-01-25T18:30:39Z", "closed_at": "2019-01-25T18:30:38Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Looks like `dataset_path` is evaluated differently in `make_reader` and `make_batch_reader`.\r\nIn  `make_reader`: `dataset_path = resolver.get_dataset_path()` - executes correctly\r\nIn `make_batch_reader`: `dataset_path = resolver.parsed_dataset_url().path` - fails the reader when reading from s3\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/290", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/290/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/290/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/290/events", "html_url": "https://github.com/uber/petastorm/issues/290", "id": 400366471, "node_id": "MDU6SXNzdWU0MDAzNjY0NzE=", "number": 290, "title": "Travis build hangs intermittently ", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1197547444, "node_id": "MDU6TGFiZWwxMTk3NTQ3NDQ0", "url": "https://api.github.com/repos/uber/petastorm/labels/build", "name": "build", "color": "1f91f4", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-01-17T16:46:51Z", "updated_at": "2019-08-30T05:05:18Z", "closed_at": "2019-08-30T05:05:18Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "A build would some times fail when running this test:\r\n```\r\n$RUN $PYTEST examples/mnist/tests/test_pytorch_mnist.py petastorm/tests/test_pytorch_dataloader.py\r\n...\r\nNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\r\nCheck the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received\r\nThe build has been terminated\r\n\r\n```\r\nRestarting the build usually solves the issue.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/288", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/288/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/288/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/288/events", "html_url": "https://github.com/uber/petastorm/issues/288", "id": 400083514, "node_id": "MDU6SXNzdWU0MDAwODM1MTQ=", "number": 288, "title": "Hello world example fails", "user": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-17T02:26:24Z", "updated_at": "2019-01-17T02:52:37Z", "closed_at": "2019-01-17T02:52:37Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "# Environment\r\n* petastorm v0.5.1\r\n* Python 2.7.15\r\n* Spark version 2.4.0 (Using Scala version 2.11.12, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_152)\r\n* Java version \"1.8.0_152\"\r\n* MacOS Mojave v10.14.2\r\n\r\n# Problem\r\nAfter setting up the virtual environment, installing Petastorm via pip, I attempt to execute `petastorm/examples/hello_world/generate_hello_world_dataset.py`, which fails with the following output (sorry if this is verbose; not sure what portions of it are relevant):\r\n```\r\n\u2500 $ \u25b6 python generate_hello_world_dataset.py\r\n2019-01-16 18:24:10 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n2019-01-16 18:24:15 WARN  ParquetOutputFormat:152 - Setting parquet.enable.summary-metadata is deprecated, please use parquet.summary.metadata.level\r\n[Stage 0:>                                                          (0 + 2) / 2]objc[10374]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called.\r\nobjc[10374]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\r\nobjc[10373]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called.\r\nobjc[10373]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\r\n2019-01-16 18:24:18 ERROR Utils:91 - Aborting task\r\norg.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:486)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:475)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:593)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:241)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)\r\n        ... 22 more\r\n2019-01-16 18:24:18 ERROR FileFormatWriter:70 - Job job_20190116182415_0000 aborted.\r\n2019-01-16 18:24:18 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)\r\norg.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:254)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:486)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:475)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:593)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:241)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n        ... 10 more\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)\r\n        ... 22 more\r\n2019-01-16 18:24:18 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:254)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:486)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:475)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:593)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:241)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n        ... 10 more\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)\r\n        ... 22 more\r\n\r\n2019-01-16 18:24:18 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job\r\n[Stage 0:>                                                          (0 + 1) / 2]2019-01-16 18:24:18 ERROR FileFormatWriter:91 - Aborting job ba651cdc-389a-45eb-8033-3deef03dca7e.\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:254)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:486)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:475)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:593)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:241)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n        ... 10 more\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)\r\n        ... 22 more\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n        at scala.Option.foreach(Option.scala:257)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\r\n        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n        at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\r\n        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\r\n        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\r\n        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\r\n        at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n        at py4j.Gateway.invoke(Gateway.java:282)\r\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n        at py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:254)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        ... 1 more\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:486)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:475)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:593)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:241)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n        ... 10 more\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)\r\n        ... 22 more\r\nTraceback (most recent call last):\r\n  File \"generate_hello_world_dataset.py\", line 70, in <module>\r\n    generate_hello_world_dataset()\r\n  File \"generate_hello_world_dataset.py\", line 66, in generate_hello_world_dataset\r\n    .parquet(output_url)\r\n  File \"/Users/joshua.goller/Uber/repos/petastorm/env/lib/python2.7/site-packages/pyspark/sql/readwriter.py\", line 841, in parquet\r\n    self._jwrite.parquet(path)\r\n  File \"/Users/joshua.goller/Uber/repos/petastorm/env/lib/python2.7/site-packages/py4j/java_gateway.py\", line 1257, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/Users/joshua.goller/Uber/repos/petastorm/env/lib/python2.7/site-packages/pyspark/sql/utils.py\", line 63, in deco\r\n    return f(*a, **kw)\r\n  File \"/Users/joshua.goller/Uber/repos/petastorm/env/lib/python2.7/site-packages/py4j/protocol.py\", line 328, in get_return_value\r\n    format(target_id, \".\", name), value)\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o50.parquet.\r\n: org.apache.spark.SparkException: Job aborted.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\r\n        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n        at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n        at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\r\n        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\r\n        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\r\n        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\r\n        at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n        at py4j.Gateway.invoke(Gateway.java:282)\r\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n        at py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:254)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:486)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:475)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:593)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:241)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n        ... 10 more\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)\r\n        ... 22 more\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n        at scala.Option.foreach(Option.scala:257)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\r\n        ... 33 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:254)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:168)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        ... 1 more\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:486)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:475)\r\n        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:593)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:241)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:245)\r\n        ... 10 more\r\nCaused by: java.io.EOFException\r\n        at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:578)\r\n        ... 22 more\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/287", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/287/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/287/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/287/events", "html_url": "https://github.com/uber/petastorm/issues/287", "id": 399868310, "node_id": "MDU6SXNzdWUzOTk4NjgzMTA=", "number": 287, "title": "pyarrow.hdfs.HadoopFileSystem not serializable by Spark", "user": {"login": "Limmen", "id": 8254791, "node_id": "MDQ6VXNlcjgyNTQ3OTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8254791?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Limmen", "html_url": "https://github.com/Limmen", "followers_url": "https://api.github.com/users/Limmen/followers", "following_url": "https://api.github.com/users/Limmen/following{/other_user}", "gists_url": "https://api.github.com/users/Limmen/gists{/gist_id}", "starred_url": "https://api.github.com/users/Limmen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Limmen/subscriptions", "organizations_url": "https://api.github.com/users/Limmen/orgs", "repos_url": "https://api.github.com/users/Limmen/repos", "events_url": "https://api.github.com/users/Limmen/events{/privacy}", "received_events_url": "https://api.github.com/users/Limmen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 966212142, "node_id": "MDU6TGFiZWw5NjYyMTIxNDI=", "url": "https://api.github.com/repos/uber/petastorm/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/uber/petastorm/milestones/3", "html_url": "https://github.com/uber/petastorm/milestone/3", "labels_url": "https://api.github.com/repos/uber/petastorm/milestones/3/labels", "id": 3981938, "node_id": "MDk6TWlsZXN0b25lMzk4MTkzOA==", "number": 3, "title": "0.6.0", "description": null, "creator": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "open_issues": 1, "closed_issues": 2, "state": "open", "created_at": "2019-01-17T17:01:21Z", "updated_at": "2019-02-20T19:09:43Z", "due_on": null, "closed_at": null}, "comments": 8, "created_at": "2019-01-16T15:42:20Z", "updated_at": "2019-02-20T19:09:43Z", "closed_at": "2019-02-20T19:09:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/uber/petastorm/blob/e65e327f3bf10ffe95954ceed1a5da89ea0b0ba2/petastorm/etl/dataset_metadata.py#L212\r\n\r\nHi again,\r\n\r\nI am using pyspark 2.4.0, and pyarrow 0.11.1 and spark is not able to serialize pyarrow.hdfs.HadoopFileSystem. Have you encountered this issue before? \r\n\r\nI get \"HDFS Connection Failed\" during the serialization, which is a bit strange. I can open a new filesystem connection inside the spark-mapper with no problem, so there should not be a connection issue but rather a serialization issue. \r\n\r\nSnippet from stacktrace:\r\n`\r\n  File \"/srv/hops/hopsdata/tmp/nm-local-dir/usercache/N8YmHUGK9tr5Q9_iz7ZdAb0oU66QXgWDdYzH4tE4wgI/appcache/application_1547648243443_0001/container_e01_1547648243443_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 566, in loads\r\n    return pickle.loads(obj, encoding=encoding)\r\n  File \"/srv/hops/anaconda/anaconda/envs/petastorm/lib/python3.6/site-packages/pyarrow/hdfs.py\", line 37, in __init__\r\n    self._connect(host, port, user, kerb_ticket, driver, extra_conf)\r\n  File \"pyarrow/io-hdfs.pxi\", line 105, in pyarrow.lib.HadoopFileSystem._connect\r\n  File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIOError: HDFS connection failed\r\n`\r\n\r\nThanks\r\n/Kim", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/286", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/286/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/286/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/286/events", "html_url": "https://github.com/uber/petastorm/issues/286", "id": 399323266, "node_id": "MDU6SXNzdWUzOTkzMjMyNjY=", "number": 286, "title": "Create an 'hello-world' example for non-Petastorm dataset usage", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1051939030, "node_id": "MDU6TGFiZWwxMDUxOTM5MDMw", "url": "https://api.github.com/repos/uber/petastorm/labels/documentation", "name": "documentation", "color": "5319e7", "default": true, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2019-01-15T12:28:19Z", "updated_at": "2019-01-25T22:44:21Z", "closed_at": "2019-01-25T22:44:21Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "We introduced `make_batch_reader` that allows user to access Parquet stores that were not created with Petastorm (i.e. without Petastorm metadata). There is no concise example showing that. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/285", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/285/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/285/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/285/events", "html_url": "https://github.com/uber/petastorm/issues/285", "id": 399294073, "node_id": "MDU6SXNzdWUzOTkyOTQwNzM=", "number": 285, "title": "generate_hello_world_dataset.py fails with \"Passed non-file path\"", "user": {"login": "Limmen", "id": 8254791, "node_id": "MDQ6VXNlcjgyNTQ3OTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8254791?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Limmen", "html_url": "https://github.com/Limmen", "followers_url": "https://api.github.com/users/Limmen/followers", "following_url": "https://api.github.com/users/Limmen/following{/other_user}", "gists_url": "https://api.github.com/users/Limmen/gists{/gist_id}", "starred_url": "https://api.github.com/users/Limmen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Limmen/subscriptions", "organizations_url": "https://api.github.com/users/Limmen/orgs", "repos_url": "https://api.github.com/users/Limmen/repos", "events_url": "https://api.github.com/users/Limmen/events{/privacy}", "received_events_url": "https://api.github.com/users/Limmen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-15T11:04:12Z", "updated_at": "2019-01-15T15:08:31Z", "closed_at": "2019-01-15T15:08:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi! Thanks for sharing this library. \r\n\r\nWhen I am running the example script `generate_hello_world_dataset.py` on our hadoop cluster, the execution fails when it is trying to write out the metadata. E.g:\r\n\r\n```bash\r\nPassed non-file path: /Projects/fs_demo/hello_world/dataset2\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 20, in generate_hello_world_dataset\r\n  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/petastorm/etl/dataset_metadata.py\", line 97, in materialize_dataset\r\n    validate_schema=False)\r\n  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 860, in __init__\r\n    path_or_paths, self.fs, metadata_nthreads=metadata_nthreads)\r\n  File \"/srv/hops/anaconda/anaconda/envs/python36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1035, in _make_manifest\r\n    .format(path))\r\nOSError: Passed non-file path: /Projects/fs_demo/hello_world/dataset2\r\n```\r\n\r\nThe reason for the error is the following:\r\n\r\n- When spark writes out the parquet files to the given path, it creates a directory for the path and then writes the partition-files beneath that directory, rather than saving the data to a single file. \r\n\r\n- After the data have been written to the parquet files, the  `petastorm.etl.dataset_metadata.materialize_dataset()` function will save the unischema metadata. When saving the metadata it first creates a pyarrow dataset for the given path, now since the path was written with spark and refers to a **directory** it fails inside pyarrow.parquet.ParquetDataset() at the line `filesystem.isfile(path)` since path is not a file, it is a directory.\r\n\r\nIt seems that I must be missing some obvious configuration? However I can't find it in the documentation, I took the example code as-is directly from this repository and expected it to work.\r\n\r\nThanks!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/281", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/281/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/281/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/281/events", "html_url": "https://github.com/uber/petastorm/issues/281", "id": 396235517, "node_id": "MDU6SXNzdWUzOTYyMzU1MTc=", "number": 281, "title": "Segmentation fault when use  hdfs as its output_url in  generate_hello_world_dataset.py", "user": {"login": "panfengfeng", "id": 3929979, "node_id": "MDQ6VXNlcjM5Mjk5Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3929979?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panfengfeng", "html_url": "https://github.com/panfengfeng", "followers_url": "https://api.github.com/users/panfengfeng/followers", "following_url": "https://api.github.com/users/panfengfeng/following{/other_user}", "gists_url": "https://api.github.com/users/panfengfeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/panfengfeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panfengfeng/subscriptions", "organizations_url": "https://api.github.com/users/panfengfeng/orgs", "repos_url": "https://api.github.com/users/panfengfeng/repos", "events_url": "https://api.github.com/users/panfengfeng/events{/privacy}", "received_events_url": "https://api.github.com/users/panfengfeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-01-06T07:24:06Z", "updated_at": "2019-05-07T23:12:35Z", "closed_at": "2019-05-07T23:12:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I try generate_hello_world_dataset.py and use hdfs (local file is OK) as its output_url, but shows \"Segmentation fault\", any ideas about this problems?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/280", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/280/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/280/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/280/events", "html_url": "https://github.com/uber/petastorm/issues/280", "id": 395290209, "node_id": "MDU6SXNzdWUzOTUyOTAyMDk=", "number": 280, "title": "A string type is always assumed when filtering by a field used as a partition", "user": {"login": "GregAru", "id": 46323816, "node_id": "MDQ6VXNlcjQ2MzIzODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/46323816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregAru", "html_url": "https://github.com/GregAru", "followers_url": "https://api.github.com/users/GregAru/followers", "following_url": "https://api.github.com/users/GregAru/following{/other_user}", "gists_url": "https://api.github.com/users/GregAru/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregAru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregAru/subscriptions", "organizations_url": "https://api.github.com/users/GregAru/orgs", "repos_url": "https://api.github.com/users/GregAru/repos", "events_url": "https://api.github.com/users/GregAru/events{/privacy}", "received_events_url": "https://api.github.com/users/GregAru/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}, {"id": 966212142, "node_id": "MDU6TGFiZWw5NjYyMTIxNDI=", "url": "https://api.github.com/repos/uber/petastorm/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2019-01-02T15:49:31Z", "updated_at": "2019-02-09T07:39:36Z", "closed_at": "2019-02-09T07:39:36Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I am trying to read data from a partitioned dataset in S3 where the partition column is defined as an integer:\r\n`UnischemaField('partcol', np.int32, (), ScalarCodec(IntegerType()), False),`\r\n\r\nThe code below executes without exceptions but returns nothing:\r\n```   \r\nwith make_reader(dataset_path, \r\n                     workers_count=1,\r\n                     predicate=in_lambda(['partcol'], lambda partcol: (partcol == 20190102))\r\n                     ) as reader:\r\n```\r\n\r\nThe following code returns the expected data from the partition:\r\n```\r\n with make_reader(dataset_path,\r\n                     workers_count=1,\r\n                     predicate=in_lambda(['partcol'], lambda partcol: (partcol == '20190102'))\r\n                     ) as reader:\r\n```\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/250", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/250/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/250/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/250/events", "html_url": "https://github.com/uber/petastorm/issues/250", "id": 386537862, "node_id": "MDU6SXNzdWUzODY1Mzc4NjI=", "number": 250, "title": "petatorm.pytorch related tests are never run by travis", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-12-02T08:12:01Z", "updated_at": "2019-01-17T17:19:33Z", "closed_at": "2019-01-17T17:19:33Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Due to this `--ignore` statement, we never execute the following tests on Travis:\r\n```\r\n    --ignore=examples/mnist/tests/test_pytorch_mnist.py\r\n    --ignore=petastorm/tests/test_pytorch_utils.py\r\n    --ignore=petastorm/tests/test_pytorch_dataloader.py\r\n```\r\nNeed to resurrect. Probably need to include them together with other forked tests.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/240", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/240/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/240/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/240/events", "html_url": "https://github.com/uber/petastorm/issues/240", "id": 381574151, "node_id": "MDU6SXNzdWUzODE1NzQxNTE=", "number": 240, "title": "Petastorm Pytorch DataLoader does not support multiple worker functionality", "user": {"login": "ktertikas", "id": 17889342, "node_id": "MDQ6VXNlcjE3ODg5MzQy", "avatar_url": "https://avatars0.githubusercontent.com/u/17889342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ktertikas", "html_url": "https://github.com/ktertikas", "followers_url": "https://api.github.com/users/ktertikas/followers", "following_url": "https://api.github.com/users/ktertikas/following{/other_user}", "gists_url": "https://api.github.com/users/ktertikas/gists{/gist_id}", "starred_url": "https://api.github.com/users/ktertikas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ktertikas/subscriptions", "organizations_url": "https://api.github.com/users/ktertikas/orgs", "repos_url": "https://api.github.com/users/ktertikas/repos", "events_url": "https://api.github.com/users/ktertikas/events{/privacy}", "received_events_url": "https://api.github.com/users/ktertikas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-16T12:11:12Z", "updated_at": "2019-11-28T04:58:10Z", "closed_at": "2019-11-28T04:58:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am trying to use Petastorm with Pytorch to train a very simple neural network on MNIST. While reading through the source code, I noticed that the Petastorm DataLoader does not support multiple workers for data loading and, especially for the data augmentation part, the transforms are applied sequentially per row, until a batch is \"filled\". This can be seen in the following code snippet: \r\n\r\nhttps://github.com/uber/petastorm/blob/8fe78d3095afef1a7aa98359f32672bad23f7e17/petastorm/pytorch.py#L119-L137\r\n\r\nOn the other hand, the Petastorm Reader supports functionality for parallelization of reading data from the parquet dataset. \r\n\r\nAre there any plans to improve this so that the data loading after data augmentation is more efficient?  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/238", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/238/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/238/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/238/events", "html_url": "https://github.com/uber/petastorm/issues/238", "id": 379652714, "node_id": "MDU6SXNzdWUzNzk2NTI3MTQ=", "number": 238, "title": "Multiple Image Reading", "user": {"login": "xyy-Iv", "id": 44690327, "node_id": "MDQ6VXNlcjQ0NjkwMzI3", "avatar_url": "https://avatars3.githubusercontent.com/u/44690327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xyy-Iv", "html_url": "https://github.com/xyy-Iv", "followers_url": "https://api.github.com/users/xyy-Iv/followers", "following_url": "https://api.github.com/users/xyy-Iv/following{/other_user}", "gists_url": "https://api.github.com/users/xyy-Iv/gists{/gist_id}", "starred_url": "https://api.github.com/users/xyy-Iv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xyy-Iv/subscriptions", "organizations_url": "https://api.github.com/users/xyy-Iv/orgs", "repos_url": "https://api.github.com/users/xyy-Iv/repos", "events_url": "https://api.github.com/users/xyy-Iv/events{/privacy}", "received_events_url": "https://api.github.com/users/xyy-Iv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-12T07:28:58Z", "updated_at": "2018-12-10T18:00:13Z", "closed_at": "2018-12-10T18:00:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Can anyone show me an example of reading two image at the same time in Pytorch?\r\nThe pytorch tutorial did use only make_reader, which contains only one 'data_url'. So my problem is, when using petastorm in semantic segmentation/Image enhancement tasks, how to read multiple data in just one reader?\r\n\r\nAny help? Thank you very much!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/235", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/235/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/235/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/235/events", "html_url": "https://github.com/uber/petastorm/issues/235", "id": 376573692, "node_id": "MDU6SXNzdWUzNzY1NzM2OTI=", "number": 235, "title": "Properly handle all parquet types", "user": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-01T21:15:01Z", "updated_at": "2018-12-27T01:06:15Z", "closed_at": "2018-12-27T01:06:15Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Petastorm cannot handle certain native parquet types (e.g. dates, lists, etc). We should be able to read those properly", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/218", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/218/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/218/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/218/events", "html_url": "https://github.com/uber/petastorm/issues/218", "id": 373690346, "node_id": "MDU6SXNzdWUzNzM2OTAzNDY=", "number": 218, "title": "Support easier usage of s3 parquet stores.", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "VivekPanyam", "id": 1181904, "node_id": "MDQ6VXNlcjExODE5MDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1181904?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VivekPanyam", "html_url": "https://github.com/VivekPanyam", "followers_url": "https://api.github.com/users/VivekPanyam/followers", "following_url": "https://api.github.com/users/VivekPanyam/following{/other_user}", "gists_url": "https://api.github.com/users/VivekPanyam/gists{/gist_id}", "starred_url": "https://api.github.com/users/VivekPanyam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VivekPanyam/subscriptions", "organizations_url": "https://api.github.com/users/VivekPanyam/orgs", "repos_url": "https://api.github.com/users/VivekPanyam/repos", "events_url": "https://api.github.com/users/VivekPanyam/events{/privacy}", "received_events_url": "https://api.github.com/users/VivekPanyam/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "VivekPanyam", "id": 1181904, "node_id": "MDQ6VXNlcjExODE5MDQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1181904?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VivekPanyam", "html_url": "https://github.com/VivekPanyam", "followers_url": "https://api.github.com/users/VivekPanyam/followers", "following_url": "https://api.github.com/users/VivekPanyam/following{/other_user}", "gists_url": "https://api.github.com/users/VivekPanyam/gists{/gist_id}", "starred_url": "https://api.github.com/users/VivekPanyam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VivekPanyam/subscriptions", "organizations_url": "https://api.github.com/users/VivekPanyam/orgs", "repos_url": "https://api.github.com/users/VivekPanyam/repos", "events_url": "https://api.github.com/users/VivekPanyam/events{/privacy}", "received_events_url": "https://api.github.com/users/VivekPanyam/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-10-24T21:46:47Z", "updated_at": "2018-10-26T07:35:56Z", "closed_at": "2018-10-26T07:35:56Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "#210 shows an example of how S3 storage can be read by petastorm (thanks,  @sytham). \r\n\r\nWe should modify the code to support `s3://..` urls in a more natural way. Today, the 's3://' url would trigger an error, unless `pyarrow_filesystem` is specified explicitly. Petastorm should be smart enough to use s3 filesystem just by looking at the url. The following code should work out-of-the-box:\r\n\r\n```python\r\nwith make_reader('s3://...') as reader:\r\n     for sample in reader:\r\n        do something\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/210", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/210/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/210/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/210/events", "html_url": "https://github.com/uber/petastorm/issues/210", "id": 372296028, "node_id": "MDU6SXNzdWUzNzIyOTYwMjg=", "number": 210, "title": "S3 support", "user": {"login": "sytham", "id": 5081109, "node_id": "MDQ6VXNlcjUwODExMDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5081109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytham", "html_url": "https://github.com/sytham", "followers_url": "https://api.github.com/users/sytham/followers", "following_url": "https://api.github.com/users/sytham/following{/other_user}", "gists_url": "https://api.github.com/users/sytham/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytham/subscriptions", "organizations_url": "https://api.github.com/users/sytham/orgs", "repos_url": "https://api.github.com/users/sytham/repos", "events_url": "https://api.github.com/users/sytham/events{/privacy}", "received_events_url": "https://api.github.com/users/sytham/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-10-21T09:31:34Z", "updated_at": "2018-10-29T21:57:15Z", "closed_at": "2018-10-29T21:55:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, are there plans to add S3 support to the reader? I store my datasets in parquet format on S3, it seems that now I first need to download it locally before I can read it in using petastorm? (Since only file:// and hdfs:// are supported)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/195", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/195/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/195/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/195/events", "html_url": "https://github.com/uber/petastorm/issues/195", "id": 365893718, "node_id": "MDU6SXNzdWUzNjU4OTM3MTg=", "number": 195, "title": "Random segmentation faults when using DataLoader", "user": {"login": "miguelvr", "id": 7456627, "node_id": "MDQ6VXNlcjc0NTY2Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7456627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miguelvr", "html_url": "https://github.com/miguelvr", "followers_url": "https://api.github.com/users/miguelvr/followers", "following_url": "https://api.github.com/users/miguelvr/following{/other_user}", "gists_url": "https://api.github.com/users/miguelvr/gists{/gist_id}", "starred_url": "https://api.github.com/users/miguelvr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miguelvr/subscriptions", "organizations_url": "https://api.github.com/users/miguelvr/orgs", "repos_url": "https://api.github.com/users/miguelvr/repos", "events_url": "https://api.github.com/users/miguelvr/events{/privacy}", "received_events_url": "https://api.github.com/users/miguelvr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-10-02T13:19:21Z", "updated_at": "2018-10-05T07:44:29Z", "closed_at": "2018-10-05T07:44:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hey guys,\r\n\r\nI was trying to debug my transform function for petastorm's `DataLoader` when I got a bunch of `Segmentation fault (core dumped)`\r\n\r\nThe code I was running was simply:\r\n\r\n```python\r\nwith Reader(dataset_url) as reader:\r\n    for i, row in enumerate(reader):\r\n        print(row.feat_name.shape)\r\n        break\r\n```\r\nHowever these were not consistent, they just happened sporadically.\r\n\r\nAny idea of what it could be?\r\n\r\nBest,\r\nMiguel ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/193", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/193/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/193/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/193/events", "html_url": "https://github.com/uber/petastorm/issues/193", "id": 365198041, "node_id": "MDU6SXNzdWUzNjUxOTgwNDE=", "number": 193, "title": "How to change driver for pyarrow.hdfs ?", "user": {"login": "manuzhang", "id": 1191767, "node_id": "MDQ6VXNlcjExOTE3Njc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1191767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manuzhang", "html_url": "https://github.com/manuzhang", "followers_url": "https://api.github.com/users/manuzhang/followers", "following_url": "https://api.github.com/users/manuzhang/following{/other_user}", "gists_url": "https://api.github.com/users/manuzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/manuzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manuzhang/subscriptions", "organizations_url": "https://api.github.com/users/manuzhang/orgs", "repos_url": "https://api.github.com/users/manuzhang/repos", "events_url": "https://api.github.com/users/manuzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/manuzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-30T06:44:14Z", "updated_at": "2018-09-30T15:07:13Z", "closed_at": "2018-09-30T15:07:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there an easy way to change the default `libhdfs3` driver for`pyarrow.hdfs` ? We already have Hadoop and `libhdfs` on our classpath.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/192", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/192/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/192/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/192/events", "html_url": "https://github.com/uber/petastorm/issues/192", "id": 364808925, "node_id": "MDU6SXNzdWUzNjQ4MDg5MjU=", "number": 192, "title": "Keras supporting", "user": {"login": "tjliupeng", "id": 2176922, "node_id": "MDQ6VXNlcjIxNzY5MjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/2176922?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjliupeng", "html_url": "https://github.com/tjliupeng", "followers_url": "https://api.github.com/users/tjliupeng/followers", "following_url": "https://api.github.com/users/tjliupeng/following{/other_user}", "gists_url": "https://api.github.com/users/tjliupeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjliupeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjliupeng/subscriptions", "organizations_url": "https://api.github.com/users/tjliupeng/orgs", "repos_url": "https://api.github.com/users/tjliupeng/repos", "events_url": "https://api.github.com/users/tjliupeng/events{/privacy}", "received_events_url": "https://api.github.com/users/tjliupeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-09-28T09:23:02Z", "updated_at": "2018-09-28T23:59:58Z", "closed_at": "2018-09-28T23:52:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Does petastorm support Keras?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/189", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/189/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/189/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/189/events", "html_url": "https://github.com/uber/petastorm/issues/189", "id": 364666793, "node_id": "MDU6SXNzdWUzNjQ2NjY3OTM=", "number": 189, "title": "Support running custom python code on in a decoding thread/process", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-27T21:51:09Z", "updated_at": "2019-01-25T22:39:17Z", "closed_at": "2019-01-25T22:39:17Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "It can be useful to give a user an option to run some custom python implemented transform on decoding thread, right after our decoder. This can be useful in the following scenarios:\r\n- Tensorflow does not provide a good way to scale Python preprocessing (since py_func will run entirely under GIL). This could be a good place for Tensorflow developers to place such preprocessing code.\r\n- PyTorch's `DataLoader` supports running custom code in subprocesses. We don't have this functionality in our implementation of the DataLoader.\r\n\r\nAn idea of an API:\r\n```\r\nReader(..., postprocess_fn=lambda row: f(row))", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/187", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/187/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/187/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/187/events", "html_url": "https://github.com/uber/petastorm/issues/187", "id": 364506865, "node_id": "MDU6SXNzdWUzNjQ1MDY4NjU=", "number": 187, "title": "Reader takes a long time to exit", "user": {"login": "miguelvr", "id": 7456627, "node_id": "MDQ6VXNlcjc0NTY2Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7456627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miguelvr", "html_url": "https://github.com/miguelvr", "followers_url": "https://api.github.com/users/miguelvr/followers", "following_url": "https://api.github.com/users/miguelvr/following{/other_user}", "gists_url": "https://api.github.com/users/miguelvr/gists{/gist_id}", "starred_url": "https://api.github.com/users/miguelvr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miguelvr/subscriptions", "organizations_url": "https://api.github.com/users/miguelvr/orgs", "repos_url": "https://api.github.com/users/miguelvr/repos", "events_url": "https://api.github.com/users/miguelvr/events{/privacy}", "received_events_url": "https://api.github.com/users/miguelvr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-09-27T14:33:18Z", "updated_at": "2019-11-20T19:07:50Z", "closed_at": "2019-01-25T22:39:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I just converted a part of my spark dataset with the petastorm utilities, and now I'm trying to print the results of a single batch.\r\n\r\nEach row of the dataset has 2 columns, each one with a (1, 24) shaped np.ndarray\r\n\r\nI'm using this code to print a single row:\r\n```\r\n    with Reader(output_url) as reader:\r\n        for i, row in enumerate(reader):\r\n            print(row)\r\n            break\r\n```\r\n\r\nI takes a decently big amount of time to print the row, and then it takes even more time to exit the program, which is super weird.\r\n\r\nAm I doing anything wrong?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/186", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/186/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/186/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/186/events", "html_url": "https://github.com/uber/petastorm/issues/186", "id": 364372725, "node_id": "MDU6SXNzdWUzNjQzNzI3MjU=", "number": 186, "title": "Unnecessary dependency on OpenCV", "user": {"login": "miguelvr", "id": 7456627, "node_id": "MDQ6VXNlcjc0NTY2Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7456627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miguelvr", "html_url": "https://github.com/miguelvr", "followers_url": "https://api.github.com/users/miguelvr/followers", "following_url": "https://api.github.com/users/miguelvr/following{/other_user}", "gists_url": "https://api.github.com/users/miguelvr/gists{/gist_id}", "starred_url": "https://api.github.com/users/miguelvr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miguelvr/subscriptions", "organizations_url": "https://api.github.com/users/miguelvr/orgs", "repos_url": "https://api.github.com/users/miguelvr/repos", "events_url": "https://api.github.com/users/miguelvr/events{/privacy}", "received_events_url": "https://api.github.com/users/miguelvr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212142, "node_id": "MDU6TGFiZWw5NjYyMTIxNDI=", "url": "https://api.github.com/repos/uber/petastorm/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-27T09:01:57Z", "updated_at": "2018-09-28T16:07:06Z", "closed_at": "2018-09-28T16:07:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm trying to export a non-image spark dataset to the petastorm format, and when I import the petastorm codecs, I get this:\r\n\r\n```\r\n~/.conda/envs/stb2/lib/python3.5/site-packages/petastorm/codecs.py in <module>()\r\n     25 from io import BytesIO\r\n     26 \r\n---> 27 import cv2\r\n     28 import numpy as np\r\n     29 from pyspark.sql.types import BinaryType, LongType, IntegerType, ShortType, ByteType, StringType\r\n\r\nImportError: No module named 'cv2'\r\n```\r\n\r\nIt doesn't make sense to keep opencv as a dependency, it seems unnecessary.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/178", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/178/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/178/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/178/events", "html_url": "https://github.com/uber/petastorm/issues/178", "id": 362918437, "node_id": "MDU6SXNzdWUzNjI5MTg0Mzc=", "number": 178, "title": "Reduce code complexity of worker_loop function", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1033380378, "node_id": "MDU6TGFiZWwxMDMzMzgwMzc4", "url": "https://api.github.com/repos/uber/petastorm/labels/code-health", "name": "code-health", "color": "fbca04", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-23T06:40:17Z", "updated_at": "2019-08-30T04:56:55Z", "closed_at": "2019-08-30T04:56:55Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Currently, flake8 fails on worker_loop being too complex function. Should evaluate if the function can be refactored into smaller pieces.\r\nThe following check is currently failing:\r\n```\r\nflake8 . --count --max-complexity=10 --statistics\r\n```\r\nEnable the check as a blocking once the code is refactor and the test passes.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/175", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/175/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/175/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/175/events", "html_url": "https://github.com/uber/petastorm/issues/175", "id": 362888782, "node_id": "MDU6SXNzdWUzNjI4ODg3ODI=", "number": 175, "title": "Improve static code analysis", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1033380378, "node_id": "MDU6TGFiZWwxMDMzMzgwMzc4", "url": "https://api.github.com/repos/uber/petastorm/labels/code-health", "name": "code-health", "color": "fbca04", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-22T20:36:22Z", "updated_at": "2019-03-01T13:59:16Z", "closed_at": "2018-09-23T06:16:27Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "We should consider adding more static tests to Travis-CI build. This PR https://github.com/uber/petastorm/pull/174 shows that we are currently missing some checks that could benefit us.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/152", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/152/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/152/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/152/events", "html_url": "https://github.com/uber/petastorm/issues/152", "id": 360026547, "node_id": "MDU6SXNzdWUzNjAwMjY1NDc=", "number": 152, "title": "Implement columns bundling", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-09-13T19:06:48Z", "updated_at": "2019-08-30T04:56:10Z", "closed_at": "2019-08-30T04:56:10Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Helps reduce the amount of columns in our stores as we encounter some Parquet implementation limitations.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/151", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/151/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/151/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/151/events", "html_url": "https://github.com/uber/petastorm/issues/151", "id": 360026191, "node_id": "MDU6SXNzdWUzNjAwMjYxOTE=", "number": 151, "title": "Cleanup project package structure", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1033380378, "node_id": "MDU6TGFiZWwxMDMzMzgwMzc4", "url": "https://api.github.com/repos/uber/petastorm/labels/code-health", "name": "code-health", "color": "fbca04", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/uber/petastorm/milestones/2", "html_url": "https://github.com/uber/petastorm/milestone/2", "labels_url": "https://api.github.com/repos/uber/petastorm/milestones/2/labels", "id": 3652695, "node_id": "MDk6TWlsZXN0b25lMzY1MjY5NQ==", "number": 2, "title": "0.5.0", "description": null, "creator": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 5, "state": "open", "created_at": "2018-09-13T19:01:10Z", "updated_at": "2019-10-19T02:41:20Z", "due_on": null, "closed_at": null}, "comments": 1, "created_at": "2018-09-13T19:05:48Z", "updated_at": "2019-01-17T22:23:29Z", "closed_at": "2019-01-17T22:23:29Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/150", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/150/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/150/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/150/events", "html_url": "https://github.com/uber/petastorm/issues/150", "id": 360026011, "node_id": "MDU6SXNzdWUzNjAwMjYwMTE=", "number": 150, "title": "New Reader API: primitive-parameters as public and dependency-injection like private", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/uber/petastorm/milestones/2", "html_url": "https://github.com/uber/petastorm/milestone/2", "labels_url": "https://api.github.com/repos/uber/petastorm/milestones/2/labels", "id": 3652695, "node_id": "MDk6TWlsZXN0b25lMzY1MjY5NQ==", "number": 2, "title": "0.5.0", "description": null, "creator": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 5, "state": "open", "created_at": "2018-09-13T19:01:10Z", "updated_at": "2019-10-19T02:41:20Z", "due_on": null, "closed_at": null}, "comments": 1, "created_at": "2018-09-13T19:05:20Z", "updated_at": "2018-10-29T18:07:52Z", "closed_at": "2018-10-29T18:07:51Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/149", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/149/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/149/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/149/events", "html_url": "https://github.com/uber/petastorm/issues/149", "id": 360025653, "node_id": "MDU6SXNzdWUzNjAwMjU2NTM=", "number": 149, "title": "Support pyarrow.serialize and grouped decoding in Reader V2", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-09-13T19:04:19Z", "updated_at": "2019-08-30T04:55:58Z", "closed_at": "2019-08-30T04:55:58Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/148", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/148/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/148/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/148/events", "html_url": "https://github.com/uber/petastorm/issues/148", "id": 360025061, "node_id": "MDU6SXNzdWUzNjAwMjUwNjE=", "number": 148, "title": "Benchmarking tools", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/uber/petastorm/milestones/2", "html_url": "https://github.com/uber/petastorm/milestone/2", "labels_url": "https://api.github.com/repos/uber/petastorm/milestones/2/labels", "id": 3652695, "node_id": "MDk6TWlsZXN0b25lMzY1MjY5NQ==", "number": 2, "title": "0.5.0", "description": null, "creator": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 5, "state": "open", "created_at": "2018-09-13T19:01:10Z", "updated_at": "2019-10-19T02:41:20Z", "due_on": null, "closed_at": null}, "comments": 0, "created_at": "2018-09-13T19:02:28Z", "updated_at": "2018-10-08T15:47:43Z", "closed_at": "2018-10-08T15:47:43Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Create a set of benchmarking tools:\r\n- Command line utility to measure samples/sec read rate\r\n- Support creating notebooks with performance measurements.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/147", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/147/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/147/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/147/events", "html_url": "https://github.com/uber/petastorm/issues/147", "id": 360024680, "node_id": "MDU6SXNzdWUzNjAwMjQ2ODA=", "number": 147, "title": "Reader v2 implementation", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-09-13T19:01:16Z", "updated_at": "2019-08-30T04:55:41Z", "closed_at": "2019-08-30T04:55:41Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "This implementation gets rid of the custom `workers_pool` and uses standard python concurrency.futures. It also supports shuffling compressed data.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/139", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/139/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/139/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/139/events", "html_url": "https://github.com/uber/petastorm/issues/139", "id": 359657234, "node_id": "MDU6SXNzdWUzNTk2NTcyMzQ=", "number": 139, "title": "Can we remove Reader.__len__() implementation", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1033380378, "node_id": "MDU6TGFiZWwxMDMzMzgwMzc4", "url": "https://api.github.com/repos/uber/petastorm/labels/code-health", "name": "code-health", "color": "fbca04", "default": false, "description": ""}, {"id": 966212142, "node_id": "MDU6TGFiZWw5NjYyMTIxNDI=", "url": "https://api.github.com/repos/uber/petastorm/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-12T21:20:35Z", "updated_at": "2018-09-14T15:19:26Z", "closed_at": "2018-09-14T15:19:26Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The implementation of len was added to support pytorch integration.  The return value can not be calculated correctly due to predicates. Can we work around the need for `len`?  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/132", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/132/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/132/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/132/events", "html_url": "https://github.com/uber/petastorm/issues/132", "id": 358677210, "node_id": "MDU6SXNzdWUzNTg2NzcyMTA=", "number": 132, "title": "Add pyspark/sql query example to README", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1051939030, "node_id": "MDU6TGFiZWwxMDUxOTM5MDMw", "url": "https://api.github.com/repos/uber/petastorm/labels/documentation", "name": "documentation", "color": "5319e7", "default": true, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-09-10T15:31:56Z", "updated_at": "2018-09-13T18:30:29Z", "closed_at": "2018-09-13T18:30:29Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/104", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/104/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/104/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/104/events", "html_url": "https://github.com/uber/petastorm/issues/104", "id": 354790813, "node_id": "MDU6SXNzdWUzNTQ3OTA4MTM=", "number": 104, "title": "Flakey Travis CI execution of Reader tests using ThreadPool", "user": {"login": "forbearer", "id": 8092876, "node_id": "MDQ6VXNlcjgwOTI4NzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8092876?v=4", "gravatar_id": "", "url": "https://api.github.com/users/forbearer", "html_url": "https://github.com/forbearer", "followers_url": "https://api.github.com/users/forbearer/followers", "following_url": "https://api.github.com/users/forbearer/following{/other_user}", "gists_url": "https://api.github.com/users/forbearer/gists{/gist_id}", "starred_url": "https://api.github.com/users/forbearer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/forbearer/subscriptions", "organizations_url": "https://api.github.com/users/forbearer/orgs", "repos_url": "https://api.github.com/users/forbearer/repos", "events_url": "https://api.github.com/users/forbearer/events{/privacy}", "received_events_url": "https://api.github.com/users/forbearer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-08-28T15:43:10Z", "updated_at": "2018-10-22T16:05:49Z", "closed_at": "2018-10-22T16:05:48Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "A couple examples of such run are:\r\n* Python 2.x: https://travis-ci.com/uber/petastorm/jobs/142290679\r\n    * I've seen the issue happen more regularly with 2.x\r\n* Python 3.x: https://travis-ci.com/uber/petastorm/jobs/142290686\r\n\r\n```\r\npetastorm/tests/test_end_to_end.py::test_simple_read[reader_factory1] /home/travis/.travis/job_stages: line 78:  2987 Segmentation fault      (core dumped) pytest -v --ignore=petastorm/tests/test_pytorch_utils.py --ignore=examples/mnist/tests/test_generate_mnist_dataset.py --cov=./\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/91", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/91/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/91/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/91/events", "html_url": "https://github.com/uber/petastorm/issues/91", "id": 353453293, "node_id": "MDU6SXNzdWUzNTM0NTMyOTM=", "number": 91, "title": "Unify namedtuple/dict way samples are treated in Reader implementation", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1033380378, "node_id": "MDU6TGFiZWwxMDMzMzgwMzc4", "url": "https://api.github.com/repos/uber/petastorm/labels/code-health", "name": "code-health", "color": "fbca04", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-08-23T16:13:21Z", "updated_at": "2018-08-28T15:48:39Z", "closed_at": "2018-08-28T15:48:39Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Currently we use use both namedtuple and dictionary interchangeably. This results in a lot of back and forth conversions and makes the code more cumbersome. \r\n\r\nSince nametuple is kinda syntactic suger for Reader users, we should make all internal usage of rows to be a dictionary (since it is easier for code parametrized by field names to handle).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/84", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/84/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/84/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/84/events", "html_url": "https://github.com/uber/petastorm/issues/84", "id": 352987542, "node_id": "MDU6SXNzdWUzNTI5ODc1NDI=", "number": 84, "title": "write a brief migration guide for petastorm 0.3.0", "user": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/uber/petastorm/milestones/1", "html_url": "https://github.com/uber/petastorm/milestone/1", "labels_url": "https://api.github.com/repos/uber/petastorm/milestones/1/labels", "id": 3591077, "node_id": "MDk6TWlsZXN0b25lMzU5MTA3Nw==", "number": 1, "title": "release 0.3.0", "description": "", "creator": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 1, "state": "open", "created_at": "2018-08-22T14:44:59Z", "updated_at": "2018-08-24T16:00:43Z", "due_on": null, "closed_at": null}, "comments": 0, "created_at": "2018-08-22T14:45:57Z", "updated_at": "2018-08-24T16:00:43Z", "closed_at": "2018-08-24T16:00:43Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Write quick code samples on the wiki for migrating from add_dataset_metadata -> materialize_dataset and using sequences to NGram", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/80", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/80/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/80/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/80/events", "html_url": "https://github.com/uber/petastorm/issues/80", "id": 352798867, "node_id": "MDU6SXNzdWUzNTI3OTg4Njc=", "number": 80, "title": "Create a development.md page", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "kashmawy", "id": 4052609, "node_id": "MDQ6VXNlcjQwNTI2MDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4052609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kashmawy", "html_url": "https://github.com/kashmawy", "followers_url": "https://api.github.com/users/kashmawy/followers", "following_url": "https://api.github.com/users/kashmawy/following{/other_user}", "gists_url": "https://api.github.com/users/kashmawy/gists{/gist_id}", "starred_url": "https://api.github.com/users/kashmawy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kashmawy/subscriptions", "organizations_url": "https://api.github.com/users/kashmawy/orgs", "repos_url": "https://api.github.com/users/kashmawy/repos", "events_url": "https://api.github.com/users/kashmawy/events{/privacy}", "received_events_url": "https://api.github.com/users/kashmawy/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kashmawy", "id": 4052609, "node_id": "MDQ6VXNlcjQwNTI2MDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4052609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kashmawy", "html_url": "https://github.com/kashmawy", "followers_url": "https://api.github.com/users/kashmawy/followers", "following_url": "https://api.github.com/users/kashmawy/following{/other_user}", "gists_url": "https://api.github.com/users/kashmawy/gists{/gist_id}", "starred_url": "https://api.github.com/users/kashmawy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kashmawy/subscriptions", "organizations_url": "https://api.github.com/users/kashmawy/orgs", "repos_url": "https://api.github.com/users/kashmawy/repos", "events_url": "https://api.github.com/users/kashmawy/events{/privacy}", "received_events_url": "https://api.github.com/users/kashmawy/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-08-22T04:25:27Z", "updated_at": "2018-10-22T16:04:30Z", "closed_at": "2018-10-22T16:04:30Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "- Document development environment setup producedures\r\n- Using `--cache-synthetic-dataset`\r\n- Different pytest goodies: `pytest-xdist`, `pytest-repeat`, `pytest-charm`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/71", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/71/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/71/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/71/events", "html_url": "https://github.com/uber/petastorm/issues/71", "id": 351793231, "node_id": "MDU6SXNzdWUzNTE3OTMyMzE=", "number": 71, "title": "A race condition results in get_results never returning", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-18T05:39:25Z", "updated_at": "2018-08-20T05:01:03Z", "closed_at": "2018-08-20T05:01:03Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "To reproduce: \r\nadd a `sleep(2)` in `ventilator.py`\r\n```\r\n            item_to_ventilate = self._items_to_ventilate[self._current_item_to_ventilate]\r\n            sleep(2)\r\n            self._ventilate_fn(**item_to_ventilate)\r\n            self._current_item_to_ventilate += 1\r\n            self._ventilated_items_count += 1\r\n```\r\nand run `petastorm/workers_pool/tests/test_ventilator.py`. The test will fail.\r\n\r\nA *pool mechanism can not differentiate between \"we processed all items\" and \"no items were ever ventilated\".\r\n\r\nNot sure we need to fix due to transition to Reader v2 that uses standard Python pool executors.\r\ntagging: @rgruener ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/69", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/69/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/69/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/69/events", "html_url": "https://github.com/uber/petastorm/issues/69", "id": 351767138, "node_id": "MDU6SXNzdWUzNTE3NjcxMzg=", "number": 69, "title": "Using `six` before it is installed from `setup.py`", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "kashmawy", "id": 4052609, "node_id": "MDQ6VXNlcjQwNTI2MDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4052609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kashmawy", "html_url": "https://github.com/kashmawy", "followers_url": "https://api.github.com/users/kashmawy/followers", "following_url": "https://api.github.com/users/kashmawy/following{/other_user}", "gists_url": "https://api.github.com/users/kashmawy/gists{/gist_id}", "starred_url": "https://api.github.com/users/kashmawy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kashmawy/subscriptions", "organizations_url": "https://api.github.com/users/kashmawy/orgs", "repos_url": "https://api.github.com/users/kashmawy/repos", "events_url": "https://api.github.com/users/kashmawy/events{/privacy}", "received_events_url": "https://api.github.com/users/kashmawy/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "kashmawy", "id": 4052609, "node_id": "MDQ6VXNlcjQwNTI2MDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4052609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kashmawy", "html_url": "https://github.com/kashmawy", "followers_url": "https://api.github.com/users/kashmawy/followers", "following_url": "https://api.github.com/users/kashmawy/following{/other_user}", "gists_url": "https://api.github.com/users/kashmawy/gists{/gist_id}", "starred_url": "https://api.github.com/users/kashmawy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kashmawy/subscriptions", "organizations_url": "https://api.github.com/users/kashmawy/orgs", "repos_url": "https://api.github.com/users/kashmawy/repos", "events_url": "https://api.github.com/users/kashmawy/events{/privacy}", "received_events_url": "https://api.github.com/users/kashmawy/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-17T23:30:10Z", "updated_at": "2018-09-05T17:42:47Z", "closed_at": "2018-09-05T17:42:47Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "`setup.py` would `import petastorm` to get the `__version__`\r\nHowever, `petastorm/__init__.py` has `import six`, which would fail.\r\n\r\nTo reproduce:\r\n```\r\n$ virtualenv .env --no-site-packages && source .env/bin/activate && pip install -e .\r\nNew python executable in .env/bin/python\r\nInstalling setuptools, pip...done.\r\nObtaining file:///home/yevgeni/petastorm\r\n  Running setup.py (path:/home/yevgeni/petastorm/setup.py) egg_info for package from file:///home/yevgeni/petastorm\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 17, in <module>\r\n      File \"/home/yevgeni/petastorm/setup.py\", line 18, in <module>\r\n        from petastorm import __version__\r\n      File \"petastorm/__init__.py\", line 16, in <module>\r\n        import six\r\n    ImportError: No module named six\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n\r\n  File \"<string>\", line 17, in <module>\r\n\r\n  File \"/home/yevgeni/petastorm/setup.py\", line 18, in <module>\r\n\r\n    from petastorm import __version__\r\n\r\n  File \"petastorm/__init__.py\", line 16, in <module>\r\n\r\n    import six\r\n\r\nImportError: No module named six\r\n\r\n----------------------------------------\r\nCleaning up...\r\nCommand python setup.py egg_info failed with error code 1 in /home/yevgeni/petastorm\r\nStoring debug log for failure in /home/yevgeni/.pip/pip.log\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/59", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/59/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/59/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/59/events", "html_url": "https://github.com/uber/petastorm/issues/59", "id": 351324885, "node_id": "MDU6SXNzdWUzNTEzMjQ4ODU=", "number": 59, "title": "Reuse synthetic dataset fixtures cross all tests", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-16T18:48:42Z", "updated_at": "2019-08-30T04:54:57Z", "closed_at": "2019-08-30T04:54:57Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Currently multiple tests generate test datasets. We should try to consolidate the dataset generation cross modules. pytest fixtures should do a good job.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/52", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/52/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/52/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/52/events", "html_url": "https://github.com/uber/petastorm/issues/52", "id": 350956038, "node_id": "MDU6SXNzdWUzNTA5NTYwMzg=", "number": 52, "title": "Chase down haphazard core dump when running mnist example main", "user": {"login": "forbearer", "id": 8092876, "node_id": "MDQ6VXNlcjgwOTI4NzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8092876?v=4", "gravatar_id": "", "url": "https://api.github.com/users/forbearer", "html_url": "https://github.com/forbearer", "followers_url": "https://api.github.com/users/forbearer/followers", "following_url": "https://api.github.com/users/forbearer/following{/other_user}", "gists_url": "https://api.github.com/users/forbearer/gists{/gist_id}", "starred_url": "https://api.github.com/users/forbearer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/forbearer/subscriptions", "organizations_url": "https://api.github.com/users/forbearer/orgs", "repos_url": "https://api.github.com/users/forbearer/repos", "events_url": "https://api.github.com/users/forbearer/events{/privacy}", "received_events_url": "https://api.github.com/users/forbearer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-15T20:31:52Z", "updated_at": "2018-09-07T16:15:33Z", "closed_at": "2018-09-07T16:15:33Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When running the mnist main, about 2 out of every 3 run fails with a core dump, typically during the Reader open phase (before training begins).  Once in my latest run, the seg fault occurs at the end of the first `train` epoch, but before the first `test` batch, so it's very likely still during Reader construction.\r\n\r\nThe core dump occurs in data page release within pyarrow libparquet:\r\n\r\n```\r\n#0  0x00007f1b48f9db48 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() () from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#1  0x00007f1b48febe73 in std::_Sp_counted_ptr_inplace<parquet::DataPage, std::allocator<parquet::DataPage>, (__gnu_cxx::_Lock_policy)2>::_M_dispose() ()\r\n   from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#2  0x00007f1b48f9db29 in std::_Sp_counted_base<(__gnu_cxx::_Lock_policy)2>::_M_release() () from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#3  0x00007f1b48fc520f in parquet::internal::TypedRecordReader<parquet::DataType<(parquet::Type::type)1> >::ReadNewPage() ()\r\n   from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#4  0x00007f1b48fc5ba0 in parquet::internal::TypedRecordReader<parquet::DataType<(parquet::Type::type)1> >::ReadRecords(long) ()\r\n   from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#5  0x00007f1b48f9b676 in parquet::arrow::PrimitiveImpl::NextBatch(long, std::shared_ptr<arrow::Array>*) () from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#6  0x00007f1b48f96fae in parquet::arrow::ColumnReader::NextBatch(long, std::shared_ptr<arrow::Array>*) () from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#7  0x00007f1b48f97bcb in parquet::arrow::FileReader::Impl::ReadColumnChunk(int, int, std::shared_ptr<arrow::Array>*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#8  0x00007f1b48f9810f in parquet::arrow::FileReader::Impl::ReadRowGroup(int, std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}::operator()(int) const () from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#9  0x00007f1b48f98f20 in parquet::arrow::FileReader::Impl::ReadRowGroup(int, std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#10 0x00007f1b48f994c2 in parquet::arrow::FileReader::ReadRowGroup(int, std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/pyarrow/libparquet.so.1\r\n#11 0x00007f1b47de8cc7 in __pyx_pw_7pyarrow_8_parquet_13ParquetReader_7read_row_group(_object*, _object*, _object*) ()\r\n   from /usr/local/lib/python2.7/dist-packages/pyarrow/_parquet.so\r\n#12 0x00000000004cdea9 in do_call (nk=<optimized out>, na=<optimized out>, pp_stack=0x7f1b1493ea70, \r\n    func=<built-in method read_row_group of pyarrow._parquet.ParquetReader object at remote 0x7f1b484fcb50>) at ../Python/ceval.c:4235\r\n#13 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493ea70) at ../Python/ceval.c:4043\r\n#14 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b45f92250, for file /usr/local/lib/python2.7/dist-packages/pyarrow/parquet.py, line 125, in read_row_group (self=<ParquetFile(common_metadata=None, _nested_paths_by_prefix={'digit': [0], 'image': [2], 'idx': [1]}, reader=<pyarrow._parquet.ParquetReader at remote 0x7f1b484fcb50>) at remote 0x7f1b45f7c990>, i=0, columns=set(['digit', 'image', 'idx']), nthreads=1, use_pandas_metadata=False, column_indices=[0, 2, 1]), throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#15 0x00000000004704ea in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x7f1b4858fd88, kwcount=<optimized out>, kws=<optimized out>, \r\n    argcount=<optimized out>, args=<optimized out>, locals=0x0, globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\r\n#16 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526\r\n#17 0x00000000004c9aa5 in PyObject_Call (kw={'use_pandas_metadata': False, 'nthreads': 1, 'columns': set(['digit', 'image', 'idx'])}, \r\n    arg=(<ParquetFile(common_metadata=None, _nested_paths_by_prefix={'digit': [0], 'image': [2], 'idx': [1]}, reader=<pyarrow._parquet.ParquetReader at remote 0x7f1b484fcb50>) at remote 0x7f1b45f7c990>, 0), func=<function at remote 0x7f1b485948c0>) at ../Objects/abstract.c:2529\r\n#18 ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, pp_stack=0x7f1b1493ecb0, func=<function at remote 0x7f1b485948c0>) at ../Python/ceval.c:4333\r\n#19 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b10007080, for file /usr/local/lib/python2.7/dist-packages/pyarrow/parquet.py, line 459, in read (self=<ParquetDatasetPiece(path='/home/ocheng/dev/datasets/mnist/test/part-00000-c04ef970-dd95-44b3-8ca6-0f6d4cbf321e-c000.parquet', partition_keys=[], row_group=0) at remote 0x7f1b0f71e190>, columns=set(['digit', 'image', 'idx']), nthreads=1, partitions=<ParquetPartitions(levels=[], partition_names=set([])) at remote 0x7f1b0f71e090>, open_file_func=<function at remote 0x7f1b45f77ed8>, file=None, use_pandas_metadata=False, reader=<ParquetFile(common_metadata=None, _nested_paths_by_prefix={'digit': [0], 'image': [2], 'idx': [1]}, reader=<pyarrow._parquet.ParquetReader at remote 0x7f1b484fcb50>) at remote 0x7f1b45f7c990>, options={'use_pandas_metadata': False, 'nthreads': 1, 'columns': set(...)}), throwflag=throwflag@entry=0) at ../Python/ceval.c:2705\r\n#20 0x00000000004cfedc in PyEval_EvalCodeEx (co=0x7f1b48575030, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=<optimized out>, \r\n    kws=<optimized out>, kwcount=3, defs=0x7f1b4857ec98, defcount=6, closure=0x0) at ../Python/ceval.c:3252\r\n#21 0x00000000004c8314 in fast_function (nk=<optimized out>, na=1, n=<optimized out>, pp_stack=0x7f1b1493eed0, func=<function at remote 0x7f1b48599320>) at ../Python/ceval.c:4116\r\n#22 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493eed0) at ../Python/ceval.c:4041\r\n#23 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b10006e30, for file /home/ocheng/dev/petastorm/petastorm/reader_worker.py, line 191, in _read_with_shuffle_row_drop (self=<ReaderWorker(_split_pieces=[<ParquetDatasetPiece(path='/home/ocheng/dev/datasets/mnist/test/part-00000-c04ef970-dd95-44b3-8ca6-0f6d4cbf321e-c000.parquet', partition_keys=[], row_group=0) at remote 0x7f1b0f71e190>], publish_func=<function at remote 0x7f1b45f77c08>, _sequence=None, args=('file:///home/ocheng/dev/datasets/mnist/test', <Unischema(digit=<UnischemaField at remote 0x7f1b45fb0e88>, _namedtuple=None, idx=<UnischemaField at remote 0x7f1b45fb0ef0>, image=<UnischemaField at remote 0x7f1b45fb0f58>, _name='MnistSchema', _fields=<OrderedDict(_OrderedDict__root=[[[[[...], [...], 'digit'], [...], 'idx'], [...], 'image'], [...], None], _OrderedDict__map={'digit': [...], 'image': [...], 'idx': [...]}) at remote 0x7f1b46353ab8>) at remote 0x7f1b45f7cd10>, None, [...], <NullCache at remote 0x7f1b45f7c950>, None), _schema=<...>, _dataset=<ParquetDataset(paths='/home/ocheng/dev/datas...(truncated), \r\n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#24 0x00000000004cfedc in PyEval_EvalCodeEx (co=0x7f1b48543830, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=<optimized out>, \r\n    kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, closure=0x0) at ../Python/ceval.c:3252\r\n---Type <return> to continue, or q <return> to quit---\r\n#25 0x00000000004c9419 in fast_function (nk=<optimized out>, na=5, n=5, pp_stack=0x7f1b1493f0f0, func=<function at remote 0x7f1b4855c410>) at ../Python/ceval.c:4116\r\n#26 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493f0f0) at ../Python/ceval.c:4041\r\n#27 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b10006c20, for file /home/ocheng/dev/petastorm/petastorm/reader_worker.py, line 122, in _load_rows (self=<ReaderWorker(_split_pieces=[<ParquetDatasetPiece(path='/home/ocheng/dev/datasets/mnist/test/part-00000-c04ef970-dd95-44b3-8ca6-0f6d4cbf321e-c000.parquet', partition_keys=[], row_group=0) at remote 0x7f1b0f71e190>], publish_func=<function at remote 0x7f1b45f77c08>, _sequence=None, args=('file:///home/ocheng/dev/datasets/mnist/test', <Unischema(digit=<UnischemaField at remote 0x7f1b45fb0e88>, _namedtuple=None, idx=<UnischemaField at remote 0x7f1b45fb0ef0>, image=<UnischemaField at remote 0x7f1b45fb0f58>, _name='MnistSchema', _fields=<OrderedDict(_OrderedDict__root=[[[[[...], [...], 'digit'], [...], 'idx'], [...], 'image'], [...], None], _OrderedDict__map={'digit': [...], 'image': [...], 'idx': [...]}) at remote 0x7f1b46353ab8>) at remote 0x7f1b45f7cd10>, None, [...], <NullCache at remote 0x7f1b45f7c950>, None), _schema=<...>, _dataset=<ParquetDataset(paths='/home/ocheng/dev/datasets/mnist/test', ...(truncated), \r\n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#28 0x00000000004c8762 in fast_function (nk=<optimized out>, na=<optimized out>, n=4, pp_stack=0x7f1b1493f270, func=<function at remote 0x7f1b4855c320>) at ../Python/ceval.c:4106\r\n#29 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493f270) at ../Python/ceval.c:4041\r\n#30 PyEval_EvalFrameEx (f=f@entry=Frame 0x7f1b45fb89b0, for file /home/ocheng/dev/petastorm/petastorm/reader_worker.py, line 104, in <lambda> (), throwflag=throwflag@entry=0)\r\n    at ../Python/ceval.c:2666\r\n#31 0x00000000004cfedc in PyEval_EvalCodeEx (co=0x7f1b4853e530, globals=<optimized out>, locals=<optimized out>, args=<optimized out>, argcount=<optimized out>, \r\n    kws=<optimized out>, kwcount=0, defs=0x0, defcount=0, \r\n    closure=(<cell at remote 0x7f1b45f74b08>, <cell at remote 0x7f1b45f74948>, <cell at remote 0x7f1b45f74bb0>, <cell at remote 0x7f1b45f74a28>)) at ../Python/ceval.c:3252\r\n#32 0x00000000004c9419 in fast_function (nk=<optimized out>, na=0, n=0, pp_stack=0x7f1b1493f490, func=<function at remote 0x7f1b45f77a28>) at ../Python/ceval.c:4116\r\n#33 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493f490) at ../Python/ceval.c:4041\r\n#34 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b45f9ce50, for file /home/ocheng/dev/petastorm/petastorm/cache.py, line 36, in get (self=<NullCache at remote 0x7f1b45f7c950>, key='d6aff736faf6f8954553f8bed1c01cf1:/home/ocheng/dev/datasets/mnist/test/part-00000-c04ef970-dd95-44b3-8ca6-0f6d4cbf321e-c000.parquet:0', fill_cache_func=<function at remote 0x7f1b45f77a28>), \r\n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#35 0x00000000004c8762 in fast_function (nk=<optimized out>, na=<optimized out>, n=3, pp_stack=0x7f1b1493f610, func=<function at remote 0x7f1b4859f5f0>) at ../Python/ceval.c:4106\r\n#36 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493f610) at ../Python/ceval.c:4041\r\n#37 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b10000dc0, for file /home/ocheng/dev/petastorm/petastorm/reader_worker.py, line 104, in process (self=<ReaderWorker(_split_pieces=[<ParquetDatasetPiece(path='/home/ocheng/dev/datasets/mnist/test/part-00000-c04ef970-dd95-44b3-8ca6-0f6d4cbf321e-c000.parquet', partition_keys=[], row_group=0) at remote 0x7f1b0f71e190>], publish_func=<function at remote 0x7f1b45f77c08>, _sequence=None, args=('file:///home/ocheng/dev/datasets/mnist/test', <Unischema(digit=<UnischemaField at remote 0x7f1b45fb0e88>, _namedtuple=None, idx=<UnischemaField at remote 0x7f1b45fb0ef0>, image=<UnischemaField at remote 0x7f1b45fb0f58>, _name='MnistSchema', _fields=<OrderedDict(_OrderedDict__root=[[[[[...], [...], 'digit'], [...], 'idx'], [...], 'image'], [...], None], _OrderedDict__map={'digit': [...], 'image': [...], 'idx': [...]}) at remote 0x7f1b46353ab8>) at remote 0x7f1b45f7cd10>, None, [...], <NullCache at remote 0x7f1b45f7c950>, None), _schema=<...>, _dataset=<ParquetDataset(paths='/home/ocheng/dev/datasets/mnist/test', com...(truncated), \r\n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#38 0x00000000004704ea in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x7f1b10000f38, kwcount=<optimized out>, kws=<optimized out>, \r\n    argcount=<optimized out>, args=<optimized out>, locals=0x0, globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\r\n#39 function_call.15337 (func=<optimized out>, arg=<optimized out>, kw=<optimized out>) at ../Objects/funcobject.c:526\r\n#40 0x00000000004c9aa5 in PyObject_Call (kw={'worker_predicate': None, 'shuffle_row_drop_partition': (0, 1), 'piece_index': 0}, \r\n    arg=(<ReaderWorker(_split_pieces=[<ParquetDatasetPiece(path='/home/ocheng/dev/datasets/mnist/test/part-00000-c04ef970-dd95-44b3-8ca6-0f6d4cbf321e-c000.parquet', partition_keys=[], row_group=0) at remote 0x7f1b0f71e190>], publish_func=<function at remote 0x7f1b45f77c08>, _sequence=None, args=('file:///home/ocheng/dev/datasets/mnist/test', <Unischema(digit=<UnischemaField at remote 0x7f1b45fb0e88>, _namedtuple=None, idx=<UnischemaField at remote 0x7f1b45fb0ef0>, image=<UnischemaField at remote 0x7f1b45fb0f58>, _name='MnistSchema', _fields=<OrderedDict(_OrderedDict__root=[[[[[...], [...], 'digit'], [...], 'idx'], [...], 'image'], [...], None], _OrderedDict__map={'digit': [...], 'image': [...], 'idx': [...]}) at remote 0x7f1b46353ab8>) at remote 0x7f1b45f7cd10>, None, [...], <NullCache at remote 0x7f1b45f7c950>, None), _schema=<...>, _dataset=<ParquetDataset(paths='/home/ocheng/dev/datasets/mnist/test', common_metadata=<pyarrow._parquet.FileMetaData at remote 0x7f1b484fcaf8>, fs=<LocalFileSystem at remote 0x7f1b4a066...(truncated), \r\n    func=<function at remote 0x7f1b4855c2a8>) at ../Objects/abstract.c:2529\r\n#41 ext_do_call (nk=<optimized out>, na=<optimized out>, flags=<optimized out>, pp_stack=0x7f1b1493f850, func=<function at remote 0x7f1b4855c2a8>) at ../Python/ceval.c:4333\r\n#42 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b4613a810, for file /home/ocheng/dev/petastorm/petastorm/workers_pool/thread_pool.py, line 60, in run (self=<WorkerThread(_ventilator_queue=<Queue(unfinished_tasks=1, queue=<collections.deque at remote 0x7f1b45faa980>, maxsize=0, all_tasks_done=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e290>, mutex=<thread.lock at remote 0x7f1b45f75510>, not_full=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e250>, not_empty=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45...(truncated), \r\n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2705\r\n#43 0x00000000004c8762 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7f1b1493f9d0, func=<function at remote 0x7f1b484f2c80>) at ../Python/ceval.c:4106\r\n---Type <return> to continue, or q <return> to quit---\r\n#44 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493f9d0) at ../Python/ceval.c:4041\r\n#45 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b08000910, for file /usr/lib/python2.7/threading.py, line 810, in __bootstrap_inner (self=<WorkerThread(_ventilator_queue=<Queue(unfinished_tasks=1, queue=<collections.deque at remote 0x7f1b45faa980>, maxsize=0, all_tasks_done=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e290>, mutex=<thread.lock at remote 0x7f1b45f75510>, not_full=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e250>, not_empty=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<...(truncated), \r\n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#46 0x00000000004c8762 in fast_function (nk=<optimized out>, na=<optimized out>, n=1, pp_stack=0x7f1b1493fb50, func=<function at remote 0x7f1ba6334398>) at ../Python/ceval.c:4106\r\n#47 call_function (oparg=<optimized out>, pp_stack=0x7f1b1493fb50) at ../Python/ceval.c:4041\r\n#48 PyEval_EvalFrameEx (\r\n    f=f@entry=Frame 0x7f1b45f9cad0, for file /usr/lib/python2.7/threading.py, line 783, in __bootstrap (self=<WorkerThread(_ventilator_queue=<Queue(unfinished_tasks=1, queue=<collections.deque at remote 0x7f1b45faa980>, maxsize=0, all_tasks_done=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e290>, mutex=<thread.lock at remote 0x7f1b45f75510>, not_full=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e250>, not_empty=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-...(truncated), \r\n    throwflag=throwflag@entry=0) at ../Python/ceval.c:2666\r\n#49 0x00000000004704ea in PyEval_EvalCodeEx (closure=<optimized out>, defcount=<optimized out>, defs=0x0, kwcount=<optimized out>, kws=<optimized out>, argcount=<optimized out>, \r\n    args=<optimized out>, locals=0x0, globals=<optimized out>, co=<optimized out>) at ../Python/ceval.c:3252\r\n#50 function_call.15337 (func=func@entry=<function at remote 0x7f1ba63342a8>, \r\n    arg=arg@entry=(<WorkerThread(_ventilator_queue=<Queue(unfinished_tasks=1, queue=<collections.deque at remote 0x7f1b45faa980>, maxsize=0, all_tasks_done=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e290>, mutex=<thread.lock at remote 0x7f1b45f75510>, not_full=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e250>, not_empty=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], rel...(truncated), kw=kw@entry=0x0)\r\n    at ../Objects/funcobject.c:526\r\n#51 0x00000000004d8194 in PyObject_Call (kw=0x0, \r\n    arg=(<WorkerThread(_ventilator_queue=<Queue(unfinished_tasks=1, queue=<collections.deque at remote 0x7f1b45faa980>, maxsize=0, all_tasks_done=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e290>, mutex=<thread.lock at remote 0x7f1b45f75510>, not_full=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e250>, not_empty=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], rel...(truncated), \r\n    func=<function at remote 0x7f1ba63342a8>) at ../Objects/abstract.c:2529\r\n#52 instancemethod_call.8802 (func=<function at remote 0x7f1ba63342a8>, func@entry=<instancemethod at remote 0x7f1b45f7b190>, \r\n    arg=(<WorkerThread(_ventilator_queue=<Queue(unfinished_tasks=1, queue=<collections.deque at remote 0x7f1b45faa980>, maxsize=0, all_tasks_done=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e290>, mutex=<thread.lock at remote 0x7f1b45f75510>, not_full=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], release=<built-in method release of thread.lock object at remote 0x7f1b45f75510>) at remote 0x7f1b0f71e250>, not_empty=<_Condition(_Verbose__verbose=False, _Condition__lock=<thread.lock at remote 0x7f1b45f75510>, acquire=<built-in method acquire of thread.lock object at remote 0x7f1b45f75510>, _Condition__waiters=[], rel...(truncated), arg@entry=(), kw=0x0)\r\n    at ../Objects/classobject.c:2602\r\n#53 0x00000000004d40fb in PyObject_Call (kw=<optimized out>, arg=(), func=<instancemethod at remote 0x7f1b45f7b190>) at ../Objects/abstract.c:2529\r\n#54 PyEval_CallObjectWithKeywords (func=<instancemethod at remote 0x7f1b45f7b190>, arg=(), kw=<optimized out>) at ../Python/ceval.c:3889\r\n#55 0x000000000057f3a2 in t_bootstrap.71638 (boot_raw=0x462e8b0) at ../Modules/threadmodule.c:614\r\n#56 0x00007f1bbd621184 in start_thread (arg=0x7f1b14940700) at pthread_create.c:312\r\n#57 0x00007f1bbd34e37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/44", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/44/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/44/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/44/events", "html_url": "https://github.com/uber/petastorm/issues/44", "id": 349790184, "node_id": "MDU6SXNzdWUzNDk3OTAxODQ=", "number": 44, "title": "Support `tf.data.Dataset` interface", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-08-12T07:01:16Z", "updated_at": "2018-09-13T18:32:28Z", "closed_at": "2018-09-13T18:32:28Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Modern TF code uses `tf.data.Dataset` object to read and preprocess data. Today we output raw tensors `tf_tensor`. Returning `tf.data.Dataset` makes the reader more user friendly.\r\n\r\n(Some code already exists internally. TODO: bring it in and polish)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/43", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/43/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/43/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/43/events", "html_url": "https://github.com/uber/petastorm/issues/43", "id": 349790103, "node_id": "MDU6SXNzdWUzNDk3OTAxMDM=", "number": 43, "title": "Querying a single column with a predicate on the same column will not return any results", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-12T07:00:00Z", "updated_at": "2018-09-07T23:04:53Z", "closed_at": "2018-09-07T23:04:52Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The following pseudo code would fail:\r\n```python\r\nreader = Reader([SomeSchema.a], predicate=in_lambda([SomeSchema.a], lambda a: True),...)\r\nreader.fetch()\r\n```\r\nWill never find results.\r\n\r\nI think the reason is due to this piece of code in the Reader.py:\r\n```python\r\n# Read remaining columns\r\nother_rows = piece.read(\r\n    open_file_func=lambda filepath: file,\r\n    columns=other_column_names,\r\n    partitions=self._dataset.partitions).to_pandas().to_dict('records')\r\n\r\n# Remove rows that were filtered out by the predicate\r\nfiltered_other_rows = [row for i, row in enumerate(other_rows) if match_predicate_mask[i]]\r\n\r\n# Decode remaining columns\r\ndecoded_other_rows = [_decode_row(row, self._schema) for row in filtered_other_rows]\r\n\r\n# Merge predicate needed columns with the remaining\r\nall_cols = [_merge_two_dicts(a, b) for a, b in zip(decoded_other_rows, filtered_decoded_predicate_rows)]\r\n```\r\n`other_rows` will be `[]` since no columns were loaded. This will result in `zip(decoded_other_rows, filtered_decoded_predicate_rows)` returning an empty list `[]` even `filtered_decoded_predicate_rows` could be an array of `[True, True,....]`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/41", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/41/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/41/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/41/events", "html_url": "https://github.com/uber/petastorm/issues/41", "id": 349790032, "node_id": "MDU6SXNzdWUzNDk3OTAwMzI=", "number": 41, "title": "A reader predicate that references both partitioned and non-partitioned keys will fail", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212138, "node_id": "MDU6TGFiZWw5NjYyMTIxMzg=", "url": "https://api.github.com/repos/uber/petastorm/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jsgoller1", "id": 1567977, "node_id": "MDQ6VXNlcjE1Njc5Nzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1567977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsgoller1", "html_url": "https://github.com/jsgoller1", "followers_url": "https://api.github.com/users/jsgoller1/followers", "following_url": "https://api.github.com/users/jsgoller1/following{/other_user}", "gists_url": "https://api.github.com/users/jsgoller1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsgoller1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsgoller1/subscriptions", "organizations_url": "https://api.github.com/users/jsgoller1/orgs", "repos_url": "https://api.github.com/users/jsgoller1/repos", "events_url": "https://api.github.com/users/jsgoller1/events{/privacy}", "received_events_url": "https://api.github.com/users/jsgoller1/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-12T06:58:24Z", "updated_at": "2019-01-31T23:06:54Z", "closed_at": "2019-01-31T23:06:54Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "For example the following test will fail (test_end_to_end)\r\n```python\r\ndef test_predicate_on_multiple_fields(self):\r\n    expected_values = {'id': 0, 'id2': 0, 'partition': 'p_0'}\r\n    reader = Reader(dataset_url=EndToEndDatasetToolkitTest._dataset_dir, shuffle=False,\r\n                    reader_pool=DummyPool(),\r\n                    predicate=EqualPredicate(expected_values))\r\n\r\n    actual = reader.fetch(timeout_s=2)\r\n    self.assertEqual(actual.id, expected_values['id'])\r\n    self.assertEqual(actual.id2, expected_values['id2'])\r\n\r\n    reader.stop()\r\n    reader.join()\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/38", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/38/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/38/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/38/events", "html_url": "https://github.com/uber/petastorm/issues/38", "id": 349785499, "node_id": "MDU6SXNzdWUzNDk3ODU0OTk=", "number": 38, "title": "Add command line tools to setup.py", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-08-12T05:12:14Z", "updated_at": "2018-08-17T20:33:45Z", "closed_at": "2018-08-17T20:33:45Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Should define entry points for these command line tools.\r\nhttp://python-packaging.readthedocs.io/en/latest/command-line-scripts.html\r\n\r\nProbably better to use entry point mechanism as it allows easier testing.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/20", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/20/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/20/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/20/events", "html_url": "https://github.com/uber/petastorm/issues/20", "id": 346776019, "node_id": "MDU6SXNzdWUzNDY3NzYwMTk=", "number": 20, "title": "Leverage pyarrow predicate filtering", "user": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2018-08-01T21:28:26Z", "updated_at": "2020-08-11T03:16:23Z", "closed_at": "2019-08-30T04:53:21Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Pyarrow ParquetDataset supports predicate filtering. We should replace our own implementation to utilize theirs https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L789", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/19", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/19/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/19/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/19/events", "html_url": "https://github.com/uber/petastorm/issues/19", "id": 346775653, "node_id": "MDU6SXNzdWUzNDY3NzU2NTM=", "number": 19, "title": "Switch to using parquet summary metadata file to split row groups", "user": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rgruener", "id": 1903915, "node_id": "MDQ6VXNlcjE5MDM5MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1903915?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rgruener", "html_url": "https://github.com/rgruener", "followers_url": "https://api.github.com/users/rgruener/followers", "following_url": "https://api.github.com/users/rgruener/following{/other_user}", "gists_url": "https://api.github.com/users/rgruener/gists{/gist_id}", "starred_url": "https://api.github.com/users/rgruener/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rgruener/subscriptions", "organizations_url": "https://api.github.com/users/rgruener/orgs", "repos_url": "https://api.github.com/users/rgruener/repos", "events_url": "https://api.github.com/users/rgruener/events{/privacy}", "received_events_url": "https://api.github.com/users/rgruener/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-08-01T21:27:12Z", "updated_at": "2018-08-14T14:24:41Z", "closed_at": "2018-08-14T14:24:41Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "We should use the parquet summary metadata file to split row groups instead of our own custom index.\r\n\r\nThis will be moved to within pyarrow with https://github.com/apache/arrow/pull/2223", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/16", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/16/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/16/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/16/events", "html_url": "https://github.com/uber/petastorm/issues/16", "id": 346300640, "node_id": "MDU6SXNzdWUzNDYzMDA2NDA=", "number": 16, "title": "Split CompressedImageCodec into Png/Jpeg (and may be other) distinct codecs", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}, {"id": 966212142, "node_id": "MDU6TGFiZWw5NjYyMTIxNDI=", "url": "https://api.github.com/repos/uber/petastorm/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-31T18:10:38Z", "updated_at": "2019-08-30T04:53:04Z", "closed_at": "2019-08-30T04:53:04Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "At the moment the same `CompressedImageCodec` is used for jpeg and png. These two codecs can be configured with different parameters (e.g. jpeg: quality, png: compression ratio). To make these parameter more explicit, it makes sense to split these codecs into two classes. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/13", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/13/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/13/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/13/events", "html_url": "https://github.com/uber/petastorm/issues/13", "id": 345996356, "node_id": "MDU6SXNzdWUzNDU5OTYzNTY=", "number": 13, "title": "Setup a lint tool", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-31T01:06:36Z", "updated_at": "2018-08-17T20:51:30Z", "closed_at": "2018-08-17T20:51:30Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Can we have lint tool check that the code is python3 compatible, even if not covered with unit tests?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/uber/petastorm/issues/3", "repository_url": "https://api.github.com/repos/uber/petastorm", "labels_url": "https://api.github.com/repos/uber/petastorm/issues/3/labels{/name}", "comments_url": "https://api.github.com/repos/uber/petastorm/issues/3/comments", "events_url": "https://api.github.com/repos/uber/petastorm/issues/3/events", "html_url": "https://github.com/uber/petastorm/issues/3", "id": 342913366, "node_id": "MDU6SXNzdWUzNDI5MTMzNjY=", "number": 3, "title": "Speed up unit tests", "user": {"login": "selitvin", "id": 3310092, "node_id": "MDQ6VXNlcjMzMTAwOTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3310092?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selitvin", "html_url": "https://github.com/selitvin", "followers_url": "https://api.github.com/users/selitvin/followers", "following_url": "https://api.github.com/users/selitvin/following{/other_user}", "gists_url": "https://api.github.com/users/selitvin/gists{/gist_id}", "starred_url": "https://api.github.com/users/selitvin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selitvin/subscriptions", "organizations_url": "https://api.github.com/users/selitvin/orgs", "repos_url": "https://api.github.com/users/selitvin/repos", "events_url": "https://api.github.com/users/selitvin/events{/privacy}", "received_events_url": "https://api.github.com/users/selitvin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 966212140, "node_id": "MDU6TGFiZWw5NjYyMTIxNDA=", "url": "https://api.github.com/repos/uber/petastorm/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-19T22:38:49Z", "updated_at": "2019-08-30T04:52:39Z", "closed_at": "2019-08-30T04:52:39Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Our build takes now more than 20 minutes. ", "performed_via_github_app": null, "score": 1.0}]}