{"total_count": 10, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/malexer/pytest-spark/issues/17", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/17/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/17/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/17/events", "html_url": "https://github.com/malexer/pytest-spark/issues/17", "id": 563363485, "node_id": "MDU6SXNzdWU1NjMzNjM0ODU=", "number": 17, "title": "Avoid to have enableHiveSupport() in fixture by default creating possible java deps problems", "user": {"login": "mehd-io", "id": 19834862, "node_id": "MDQ6VXNlcjE5ODM0ODYy", "avatar_url": "https://avatars3.githubusercontent.com/u/19834862?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mehd-io", "html_url": "https://github.com/mehd-io", "followers_url": "https://api.github.com/users/mehd-io/followers", "following_url": "https://api.github.com/users/mehd-io/following{/other_user}", "gists_url": "https://api.github.com/users/mehd-io/gists{/gist_id}", "starred_url": "https://api.github.com/users/mehd-io/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mehd-io/subscriptions", "organizations_url": "https://api.github.com/users/mehd-io/orgs", "repos_url": "https://api.github.com/users/mehd-io/repos", "events_url": "https://api.github.com/users/mehd-io/events{/privacy}", "received_events_url": "https://api.github.com/users/mehd-io/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-02-11T17:29:29Z", "updated_at": "2020-05-12T11:07:56Z", "closed_at": "2020-05-12T11:07:56Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi there,\r\nFirst thank you for that plugin! Great initiative!\r\n**Problem:**\r\nIm running into the same issue than here : https://github.com/malexer/pytest-spark/issues/14 and even with the correct some jar, it's creating other java deps problem with my based docker img used for spark testing/development.\r\nAs I'm heavily using spark on AWS glue, this option is not needed at all and I think it makes sense to have a plain SparkSession in the fixture and let the user add the spark options he wants in the `pytest.ini`\r\n\r\n**Suggested solution:**\r\nRemove `enableHiveSupport()` in https://github.com/malexer/pytest-spark/blob/master/pytest_spark/fixtures.py#L29 \r\nand for those who need that :\r\n`enableHiveSupport()` can be set with spark.sql.catalogImplementation=hive if Im not mistaken in `pytest.ini`\r\n\r\nI can do the PR if you want :)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/15", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/15/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/15/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/15/events", "html_url": "https://github.com/malexer/pytest-spark/issues/15", "id": 493922444, "node_id": "MDU6SXNzdWU0OTM5MjI0NDQ=", "number": 15, "title": "Is there any way to redirect test result(output) into a specific folder?", "user": {"login": "Heermosi", "id": 21236293, "node_id": "MDQ6VXNlcjIxMjM2Mjkz", "avatar_url": "https://avatars1.githubusercontent.com/u/21236293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Heermosi", "html_url": "https://github.com/Heermosi", "followers_url": "https://api.github.com/users/Heermosi/followers", "following_url": "https://api.github.com/users/Heermosi/following{/other_user}", "gists_url": "https://api.github.com/users/Heermosi/gists{/gist_id}", "starred_url": "https://api.github.com/users/Heermosi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Heermosi/subscriptions", "organizations_url": "https://api.github.com/users/Heermosi/orgs", "repos_url": "https://api.github.com/users/Heermosi/repos", "events_url": "https://api.github.com/users/Heermosi/events{/privacy}", "received_events_url": "https://api.github.com/users/Heermosi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-16T08:53:46Z", "updated_at": "2019-10-16T11:17:57Z", "closed_at": "2019-10-16T11:17:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm got stuck on this.\r\nIf the test result can only be output to stdout, then it shall be a waste of time in searching for corresponding test results...", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/14", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/14/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/14/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/14/events", "html_url": "https://github.com/malexer/pytest-spark/issues/14", "id": 465904339, "node_id": "MDU6SXNzdWU0NjU5MDQzMzk=", "number": 14, "title": "using spark_session fixture causes pyspark.sql.utils.IllegalArgumentException: \"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder'", "user": {"login": "dockerhub-publics", "id": 52637426, "node_id": "MDQ6VXNlcjUyNjM3NDI2", "avatar_url": "https://avatars2.githubusercontent.com/u/52637426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dockerhub-publics", "html_url": "https://github.com/dockerhub-publics", "followers_url": "https://api.github.com/users/dockerhub-publics/followers", "following_url": "https://api.github.com/users/dockerhub-publics/following{/other_user}", "gists_url": "https://api.github.com/users/dockerhub-publics/gists{/gist_id}", "starred_url": "https://api.github.com/users/dockerhub-publics/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dockerhub-publics/subscriptions", "organizations_url": "https://api.github.com/users/dockerhub-publics/orgs", "repos_url": "https://api.github.com/users/dockerhub-publics/repos", "events_url": "https://api.github.com/users/dockerhub-publics/events{/privacy}", "received_events_url": "https://api.github.com/users/dockerhub-publics/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-09T17:37:44Z", "updated_at": "2019-07-15T08:13:46Z", "closed_at": "2019-07-15T08:13:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have put content of my test_sql_query_automation.py exactly as in the malexer's test/test_spark_session_fixture.py here in master branch. \r\n\r\nTo easily reproduce this you may want to use the same Docker Hub image that I use: danimages/spark-pytests\r\n\r\nAnd here what I get:\r\n\r\n$ pytest --spark_home=$SPARK_HOME -s -vv test_sql_query_automation.py\r\n============================= test session starts ==============================\r\nplatform linux -- Python 3.5.3, pytest-5.0.1, py-1.8.0, pluggy-0.12.0 -- /usr/bin/python3\r\ncachedir: .pytest_cache\r\nspark version -- Spark 2.4.1 built for Hadoop 2.6.5 | Build flags: -B -Pmesos -Pyarn -Pkubernetes -Psparkr -Pkafka-0-8 -Pflume -Phadoop-provided -DzincPort=3038\r\nrootdir: /builds/ber/Aufbau_BI_Platform, inifile: pytest.ini\r\nplugins: spark-0.5.2\r\ncollecting ... collected 2 items\r\n\r\ntest_sql_query_automation.py::test_spark_session_dataframe 2019-07-09 17:26:16,073 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\nERROR\r\ntest_sql_query_automation.py::test_spark_session_sql ERROR\r\n\r\n==================================== ERRORS ====================================\r\n________________ ERROR at setup of test_spark_session_dataframe ________________\r\n\r\na = ('xro49', <py4j.java_gateway.GatewayClient object at 0x7f4e88700ba8>, 'o47', 'sessionState')\r\nkw = {}\r\ns = \"java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"\r\nstackTrace = 'org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1107...nd.java:79)\\n\\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\t at java.lang.Thread.run(Thread.java:748)'\r\n\r\n    def deco(*a, **kw):\r\n        try:\r\n>           return f(*a, **kw)\r\n\r\n/usr/spark-2.4.1/python/pyspark/sql/utils.py:63: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nanswer = 'xro49'\r\ngateway_client = <py4j.java_gateway.GatewayClient object at 0x7f4e88700ba8>\r\ntarget_id = 'o47', name = 'sessionState'\r\n\r\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\r\n        \"\"\"Converts an answer received from the Java gateway into a Python object.\r\n    \r\n        For example, string representation of integers are converted to Python\r\n        integer, string representation of objects are converted to JavaObject\r\n        instances, etc.\r\n    \r\n        :param answer: the string returned by the Java gateway\r\n        :param gateway_client: the gateway client used to communicate with the Java\r\n            Gateway. Only necessary if the answer is a reference (e.g., object,\r\n            list, map)\r\n        :param target_id: the name of the object from which the answer comes from\r\n            (e.g., *object1* in `object1.hello()`). Optional.\r\n        :param name: the name of the member from which the answer comes from\r\n            (e.g., *hello* in `object1.hello()`). Optional.\r\n        \"\"\"\r\n        if is_error(answer)[0]:\r\n            if len(answer) > 1:\r\n                type = answer[1]\r\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n                if answer[1] == REFERENCE_TYPE:\r\n                    raise Py4JJavaError(\r\n                        \"An error occurred while calling {0}{1}{2}.\\n\".\r\n>                       format(target_id, \".\", name), value)\r\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o47.sessionState.\r\nE                   : java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\r\nE                   \tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1107)\r\nE                   \tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:145)\r\nE                   \tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:144)\r\nE                   \tat scala.Option.getOrElse(Option.scala:121)\r\nE                   \tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:144)\r\nE                   \tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:141)\r\nE                   \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nE                   \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nE                   \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nE                   \tat java.lang.reflect.Method.invoke(Method.java:498)\r\nE                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nE                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nE                   \tat py4j.Gateway.invoke(Gateway.java:282)\r\nE                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nE                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nE                   \tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\nE                   \tat java.lang.Thread.run(Thread.java:748)\r\nE                   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.hive.HiveSessionStateBuilder\r\nE                   \tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\nE                   \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\nE                   \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\nE                   \tat java.lang.Class.forName0(Native Method)\r\nE                   \tat java.lang.Class.forName(Class.java:348)\r\nE                   \tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\r\nE                   \tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1102)\r\nE                   \t... 16 more\r\n\r\n/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py:328: Py4JJavaError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    @pytest.fixture(scope='session')\r\n    def _spark_session():\r\n        \"\"\"Internal fixture for SparkSession instance.\r\n    \r\n        Yields SparkSession instance if it is supported by the pyspark\r\n        version, otherwise yields None.\r\n    \r\n        Required to correctly initialize `spark_context` fixture after\r\n        `spark_session` fixture.\r\n    \r\n        ..note::\r\n            It is not possible to create SparkSession from the existing\r\n            SparkContext.\r\n        \"\"\"\r\n    \r\n        try:\r\n            from pyspark.sql import SparkSession\r\n        except ImportError:\r\n            yield\r\n        else:\r\n            session = SparkSession.builder \\\r\n>               .config(conf=SparkConfigBuilder().get()) \\\r\n                .enableHiveSupport() \\\r\n                .getOrCreate()\r\n\r\n/usr/local/lib/python3.5/dist-packages/pytest_spark/fixtures.py:28: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/usr/spark-2.4.1/python/pyspark/sql/session.py:183: in getOrCreate\r\n    session._jsparkSession.sessionState().conf().setConfString(key, value)\r\n/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py:1257: in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\na = ('xro49', <py4j.java_gateway.GatewayClient object at 0x7f4e88700ba8>, 'o47', 'sessionState')\r\nkw = {}\r\ns = \"java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"\r\nstackTrace = 'org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1107...nd.java:79)\\n\\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\t at java.lang.Thread.run(Thread.java:748)'\r\n\r\n    def deco(*a, **kw):\r\n        try:\r\n            return f(*a, **kw)\r\n        except py4j.protocol.Py4JJavaError as e:\r\n            s = e.java_exception.toString()\r\n            stackTrace = '\\n\\t at '.join(map(lambda x: x.toString(),\r\n                                             e.java_exception.getStackTrace()))\r\n            if s.startswith('org.apache.spark.sql.AnalysisException: '):\r\n                raise AnalysisException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.catalyst.analysis'):\r\n                raise AnalysisException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.catalyst.parser.ParseException: '):\r\n                raise ParseException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.streaming.StreamingQueryException: '):\r\n                raise StreamingQueryException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.execution.QueryExecutionException: '):\r\n                raise QueryExecutionException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('java.lang.IllegalArgumentException: '):\r\n>               raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\r\nE               pyspark.sql.utils.IllegalArgumentException: \"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"\r\n\r\n/usr/spark-2.4.1/python/pyspark/sql/utils.py:79: IllegalArgumentException\r\n___________________ ERROR at setup of test_spark_session_sql ___________________\r\n\r\na = ('xro49', <py4j.java_gateway.GatewayClient object at 0x7f4e88700ba8>, 'o47', 'sessionState')\r\nkw = {}\r\ns = \"java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"\r\nstackTrace = 'org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1107...nd.java:79)\\n\\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\t at java.lang.Thread.run(Thread.java:748)'\r\n\r\n    def deco(*a, **kw):\r\n        try:\r\n>           return f(*a, **kw)\r\n\r\n/usr/spark-2.4.1/python/pyspark/sql/utils.py:63: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nanswer = 'xro49'\r\ngateway_client = <py4j.java_gateway.GatewayClient object at 0x7f4e88700ba8>\r\ntarget_id = 'o47', name = 'sessionState'\r\n\r\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\r\n        \"\"\"Converts an answer received from the Java gateway into a Python object.\r\n    \r\n        For example, string representation of integers are converted to Python\r\n        integer, string representation of objects are converted to JavaObject\r\n        instances, etc.\r\n    \r\n        :param answer: the string returned by the Java gateway\r\n        :param gateway_client: the gateway client used to communicate with the Java\r\n            Gateway. Only necessary if the answer is a reference (e.g., object,\r\n            list, map)\r\n        :param target_id: the name of the object from which the answer comes from\r\n            (e.g., *object1* in `object1.hello()`). Optional.\r\n        :param name: the name of the member from which the answer comes from\r\n            (e.g., *hello* in `object1.hello()`). Optional.\r\n        \"\"\"\r\n        if is_error(answer)[0]:\r\n            if len(answer) > 1:\r\n                type = answer[1]\r\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n                if answer[1] == REFERENCE_TYPE:\r\n                    raise Py4JJavaError(\r\n                        \"An error occurred while calling {0}{1}{2}.\\n\".\r\n>                       format(target_id, \".\", name), value)\r\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o47.sessionState.\r\nE                   : java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\r\nE                   \tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1107)\r\nE                   \tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:145)\r\nE                   \tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:144)\r\nE                   \tat scala.Option.getOrElse(Option.scala:121)\r\nE                   \tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:144)\r\nE                   \tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:141)\r\nE                   \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nE                   \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nE                   \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nE                   \tat java.lang.reflect.Method.invoke(Method.java:498)\r\nE                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nE                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nE                   \tat py4j.Gateway.invoke(Gateway.java:282)\r\nE                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nE                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nE                   \tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\nE                   \tat java.lang.Thread.run(Thread.java:748)\r\nE                   Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.hive.HiveSessionStateBuilder\r\nE                   \tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\nE                   \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\nE                   \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\nE                   \tat java.lang.Class.forName0(Native Method)\r\nE                   \tat java.lang.Class.forName(Class.java:348)\r\nE                   \tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\r\nE                   \tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1102)\r\nE                   \t... 16 more\r\n\r\n/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py:328: Py4JJavaError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n    @pytest.fixture(scope='session')\r\n    def _spark_session():\r\n        \"\"\"Internal fixture for SparkSession instance.\r\n    \r\n        Yields SparkSession instance if it is supported by the pyspark\r\n        version, otherwise yields None.\r\n    \r\n        Required to correctly initialize `spark_context` fixture after\r\n        `spark_session` fixture.\r\n    \r\n        ..note::\r\n            It is not possible to create SparkSession from the existing\r\n            SparkContext.\r\n        \"\"\"\r\n    \r\n        try:\r\n            from pyspark.sql import SparkSession\r\n        except ImportError:\r\n            yield\r\n        else:\r\n            session = SparkSession.builder \\\r\n>               .config(conf=SparkConfigBuilder().get()) \\\r\n                .enableHiveSupport() \\\r\n                .getOrCreate()\r\n\r\n/usr/local/lib/python3.5/dist-packages/pytest_spark/fixtures.py:28: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/usr/spark-2.4.1/python/pyspark/sql/session.py:183: in getOrCreate\r\n    session._jsparkSession.sessionState().conf().setConfString(key, value)\r\n/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py:1257: in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\na = ('xro49', <py4j.java_gateway.GatewayClient object at 0x7f4e88700ba8>, 'o47', 'sessionState')\r\nkw = {}\r\ns = \"java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"\r\nstackTrace = 'org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1107...nd.java:79)\\n\\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\t at java.lang.Thread.run(Thread.java:748)'\r\n\r\n    def deco(*a, **kw):\r\n        try:\r\n            return f(*a, **kw)\r\n        except py4j.protocol.Py4JJavaError as e:\r\n            s = e.java_exception.toString()\r\n            stackTrace = '\\n\\t at '.join(map(lambda x: x.toString(),\r\n                                             e.java_exception.getStackTrace()))\r\n            if s.startswith('org.apache.spark.sql.AnalysisException: '):\r\n                raise AnalysisException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.catalyst.analysis'):\r\n                raise AnalysisException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.catalyst.parser.ParseException: '):\r\n                raise ParseException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.streaming.StreamingQueryException: '):\r\n                raise StreamingQueryException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('org.apache.spark.sql.execution.QueryExecutionException: '):\r\n                raise QueryExecutionException(s.split(': ', 1)[1], stackTrace)\r\n            if s.startswith('java.lang.IllegalArgumentException: '):\r\n>               raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\r\nE               pyspark.sql.utils.IllegalArgumentException: \"Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\"\r\n\r\n/usr/spark-2.4.1/python/pyspark/sql/utils.py:79: IllegalArgumentException\r\n=============================== warnings summary ===============================\r\n/usr/spark-2.4.1/python/pyspark/cloudpickle.py:47\r\n  /usr/spark-2.4.1/python/pyspark/cloudpickle.py:47: PendingDeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\r\n    import imp\r\n\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n===================== 1 warnings, 2 error in 4.82 seconds ======================\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/13", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/13/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/13/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/13/events", "html_url": "https://github.com/malexer/pytest-spark/issues/13", "id": 450133319, "node_id": "MDU6SXNzdWU0NTAxMzMzMTk=", "number": 13, "title": "Pipenv pytest not finding spark_home", "user": {"login": "vfrank66", "id": 7524999, "node_id": "MDQ6VXNlcjc1MjQ5OTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/7524999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vfrank66", "html_url": "https://github.com/vfrank66", "followers_url": "https://api.github.com/users/vfrank66/followers", "following_url": "https://api.github.com/users/vfrank66/following{/other_user}", "gists_url": "https://api.github.com/users/vfrank66/gists{/gist_id}", "starred_url": "https://api.github.com/users/vfrank66/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vfrank66/subscriptions", "organizations_url": "https://api.github.com/users/vfrank66/orgs", "repos_url": "https://api.github.com/users/vfrank66/repos", "events_url": "https://api.github.com/users/vfrank66/events{/privacy}", "received_events_url": "https://api.github.com/users/vfrank66/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-05-30T05:12:50Z", "updated_at": "2019-07-12T12:39:17Z", "closed_at": "2019-07-12T12:39:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using pipenv installed through pip the hooks fire findspark.py before the configuration is read. When using `brew install pipenv` and the hooks fire in the right order. This might not be the right place for this issue, except that if I remove this package and set the spark_home through pytest.ini under `env` it does work.\r\n\r\n```\r\npip install --user pipenv\r\npipenv install --dev --ignore-pipfile or pipenv install --dev  pyspark pytest pytest-spark\r\npipenv run py.test --rootdir . test --cov src/ --cov-fail-under 20 -vv test/ --junitxml=pytest-report.xml\r\nor \r\npipenv run py.test \r\n```\r\npytest.ini\r\n```\r\n[pytest]\r\nspark_home=spark/\r\ntestpaths=test\r\nlog_format = %(asctime)s %(levelname)s %(message)s\r\nlog_date_format = %Y-%m-%d %H:%M:%S\r\n```\r\nError\r\n<details>\r\n```\r\npipenv run py.test\r\nINTERNALERROR> Traceback (most recent call last):\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/_pytest/main.py\", line 201, in wrap_session\r\nINTERNALERROR>     config._do_configure()\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/_pytest/config/__init__.py\", line 668, in _do_configure\r\nINTERNALERROR>     self.hook.pytest_configure.call_historic(kwargs=dict(config=self))\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/pluggy/hooks.py\", line 311, in call_historic\r\nINTERNALERROR>     res = self._hookexec(self, self.get_hookimpls(), kwargs)\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/pluggy/manager.py\", line 87, in _hookexec\r\nINTERNALERROR>     return self._inner_hookexec(hook, methods, kwargs)\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/pluggy/manager.py\", line 81, in <lambda>\r\nINTERNALERROR>     firstresult=hook.spec.opts.get(\"firstresult\") if hook.spec else False,\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/pluggy/callers.py\", line 208, in _multicall\r\nINTERNALERROR>     return outcome.get_result()\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/pluggy/callers.py\", line 81, in get_result\r\nINTERNALERROR>     _reraise(*ex)  # noqa\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/pluggy/callers.py\", line 187, in _multicall\r\nINTERNALERROR>     res = hook_impl.function(*args)\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/pytest_spark/__init__.py\", line 30, in pytest_configure\r\nINTERNALERROR>     findspark.init(spark_home)\r\nINTERNALERROR>   File \"/Users/vfrank/.local/share/virtualenvs/lcs-glue-python-extractor-KOyFA_4z/lib/python2.7/site-packages/findspark.py\", line 135, in init\r\nINTERNALERROR>     py4j = glob(os.path.join(spark_python, 'lib', 'py4j-*.zip'))[0]\r\nINTERNALERROR> IndexError: list index out of range\r\n```\r\n</details>\r\n\r\nWorks:\r\n1. Remove pytest-spark `pipenv uninstall pytest.spark`\r\n2. Update pytest.ini\r\npytest.ini\r\n```\r\n[pytest]\r\nenv =\r\n    SPARK_HOME=spark/\r\ntestpaths=test\r\nlog_format = %(asctime)s %(levelname)s %(message)s\r\nlog_date_format = %Y-%m-%d %H:%M:%S\r\n```\r\n\r\nAlso works, through brew and with pytest-spark:\r\n\r\n```\r\nbrew install pipenv\r\npipenv install --dev --ignore-pipfile or pipenv install --dev  pyspark pytest pytest-spark\r\npipenv run py.test --rootdir . test --cov src/ --cov-fail-under 20 -vv test/ --junitxml=pytest-report.xml \r\nor \r\npipenv run py.test \r\n```\r\nProduces\r\n```\r\n============================================= test session starts =============================================\r\nplatform darwin -- Python 2.7.10, pytest-4.5.0, py-1.8.0, pluggy-0.12.0\r\nrootdir: /Users/vfrank/dev-working/blah/lcs-glue-python-extractor, inifile: pytest.ini, testpaths: test\r\nplugins: cov-2.7.1, mock-1.10.4\r\ncollected 3 items                                                                                             \r\n\r\ntest/test_dynamic_etl_script.py .                                                                       [ 33%]\r\ntest/test_hydrox.py ..   \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/10", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/10/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/10/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/10/events", "html_url": "https://github.com/malexer/pytest-spark/issues/10", "id": 447060916, "node_id": "MDU6SXNzdWU0NDcwNjA5MTY=", "number": 10, "title": "spark_session fixture raises ValueError during setup", "user": {"login": "dutchgecko", "id": 2513484, "node_id": "MDQ6VXNlcjI1MTM0ODQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2513484?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dutchgecko", "html_url": "https://github.com/dutchgecko", "followers_url": "https://api.github.com/users/dutchgecko/followers", "following_url": "https://api.github.com/users/dutchgecko/following{/other_user}", "gists_url": "https://api.github.com/users/dutchgecko/gists{/gist_id}", "starred_url": "https://api.github.com/users/dutchgecko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dutchgecko/subscriptions", "organizations_url": "https://api.github.com/users/dutchgecko/orgs", "repos_url": "https://api.github.com/users/dutchgecko/repos", "events_url": "https://api.github.com/users/dutchgecko/events{/privacy}", "received_events_url": "https://api.github.com/users/dutchgecko/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-05-22T10:38:43Z", "updated_at": "2019-06-15T19:41:57Z", "closed_at": "2019-06-15T10:33:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I get the following stacktrace from `pytest` when trying to call the `spark_session` fixture:\r\n\r\n```\r\nERROR at setup of <testname> \r\n\r\n    @pytest.fixture(scope='session')\r\n    def _spark_session():\r\n        \"\"\"Internal fixture for SparkSession instance.\r\n\r\n        Yields SparkSession instance if it is supported by the pyspark\r\n        version, otherwise yields None.\r\n\r\n        Required to correctly initialize `spark_context` fixture after\r\n        `spark_session` fixture.\r\n\r\n        ..note::\r\n            It is not possible to create SparkSession from the existing\r\n            SparkContext.\r\n        \"\"\"\r\n\r\n        try:\r\n            from pyspark.sql import SparkSession\r\n        except ImportError:\r\n            yield\r\n        else:\r\n            session = SparkSession.builder \\\r\n>               .config(conf=SparkConfigBuilder().get()) \\\r\n                .enableHiveSupport() \\\r\n                .getOrCreate()\r\n\r\n../../../.local/share/virtualenvs/Transform-3Dsmlban/lib/python3.4/site-packages/pytest_spark/fixtures.py:28:\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\ncls = <class 'pytest_spark.config.SparkConfigBuilder'>\r\n\r\n    @classmethod\r\n    def get(cls):\r\n        if not cls._instance:\r\n>           raise ValueError\r\nE           ValueError\r\n\r\n../../../.local/share/virtualenvs/Transform-3Dsmlban/lib/python3.4/site-packages/pytest_spark/config.py:49: ValueError\r\n```\r\n\r\nTaking a quick look at the code, it seems to me that the `raise ValueError` line is completely unnecessary, since the configobject can just be initialized then?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/9", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/9/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/9/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/9/events", "html_url": "https://github.com/malexer/pytest-spark/issues/9", "id": 434176947, "node_id": "MDU6SXNzdWU0MzQxNzY5NDc=", "number": 9, "title": "Suggestions to speed up pytest-spark tests", "user": {"login": "juhoautio", "id": 4446608, "node_id": "MDQ6VXNlcjQ0NDY2MDg=", "avatar_url": "https://avatars3.githubusercontent.com/u/4446608?v=4", "gravatar_id": "", "url": "https://api.github.com/users/juhoautio", "html_url": "https://github.com/juhoautio", "followers_url": "https://api.github.com/users/juhoautio/followers", "following_url": "https://api.github.com/users/juhoautio/following{/other_user}", "gists_url": "https://api.github.com/users/juhoautio/gists{/gist_id}", "starred_url": "https://api.github.com/users/juhoautio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/juhoautio/subscriptions", "organizations_url": "https://api.github.com/users/juhoautio/orgs", "repos_url": "https://api.github.com/users/juhoautio/repos", "events_url": "https://api.github.com/users/juhoautio/events{/privacy}", "received_events_url": "https://api.github.com/users/juhoautio/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-17T09:17:05Z", "updated_at": "2019-05-09T22:23:07Z", "closed_at": "2019-05-09T22:23:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "The spark session created by pytest-spark is not so optimized for small unit tests that only work with small dataframes.\r\n\r\npytest-spark seems to rely on whatever are Spark's default settings:\r\n\r\nhttps://github.com/malexer/pytest-spark/blob/0152b555eb532710fd5bd212bd95134f9342e22f/pytest_spark/__init__.py#L101\r\n\r\nThose defaults are aimed at working with some bigger data sets. IMHO it would make more sense to optimize the speed for smaller datasets.\r\n\r\nWe initially used pytest-spark (thanks for that!), but recently changed to create the spark session fixture with our own code (kudos to @artem-garmash!).\r\n\r\n```python\r\n    \"\"\"\r\n    Parameters to reduce parallelism to make it run faster with test data\r\n    \"\"\"\r\n    spark = SparkSession.builder \\\r\n        .config('spark.sql.shuffle.partitions', 1) \\\r\n        .config('spark.default.parallelism', 1) \\\r\n        .config('spark.rdd.compress', False) \\\r\n        .config('spark.shuffle.compress', False) \\\r\n        .enableHiveSupport() \\\r\n        .getOrCreate()\r\n```\r\n\r\nIn our case the total test duration went from 7m:38s down to 3m:03s thanks to this change.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/8", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/8/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/8/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/8/events", "html_url": "https://github.com/malexer/pytest-spark/issues/8", "id": 398039266, "node_id": "MDU6SXNzdWUzOTgwMzkyNjY=", "number": 8, "title": "ability to configure SparkSession", "user": {"login": "blbradley", "id": 1435085, "node_id": "MDQ6VXNlcjE0MzUwODU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1435085?v=4", "gravatar_id": "", "url": "https://api.github.com/users/blbradley", "html_url": "https://github.com/blbradley", "followers_url": "https://api.github.com/users/blbradley/followers", "following_url": "https://api.github.com/users/blbradley/following{/other_user}", "gists_url": "https://api.github.com/users/blbradley/gists{/gist_id}", "starred_url": "https://api.github.com/users/blbradley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/blbradley/subscriptions", "organizations_url": "https://api.github.com/users/blbradley/orgs", "repos_url": "https://api.github.com/users/blbradley/repos", "events_url": "https://api.github.com/users/blbradley/events{/privacy}", "received_events_url": "https://api.github.com/users/blbradley/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-10T22:16:01Z", "updated_at": "2019-05-09T22:22:40Z", "closed_at": "2019-05-09T22:22:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently, this package is unable to let users configure the SparkSession. This would be useful if you wanted to configure Spark to use a package such as `spark-xml`.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/7", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/7/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/7/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/7/events", "html_url": "https://github.com/malexer/pytest-spark/issues/7", "id": 326738014, "node_id": "MDU6SXNzdWUzMjY3MzgwMTQ=", "number": 7, "title": "alphabetical order of test files", "user": {"login": "hfwittmann", "id": 6077743, "node_id": "MDQ6VXNlcjYwNzc3NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6077743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hfwittmann", "html_url": "https://github.com/hfwittmann", "followers_url": "https://api.github.com/users/hfwittmann/followers", "following_url": "https://api.github.com/users/hfwittmann/following{/other_user}", "gists_url": "https://api.github.com/users/hfwittmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/hfwittmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hfwittmann/subscriptions", "organizations_url": "https://api.github.com/users/hfwittmann/orgs", "repos_url": "https://api.github.com/users/hfwittmann/repos", "events_url": "https://api.github.com/users/hfwittmann/events{/privacy}", "received_events_url": "https://api.github.com/users/hfwittmann/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-26T13:22:48Z", "updated_at": "2018-09-05T12:17:57Z", "closed_at": "2018-09-05T12:17:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Good package, thank you!\r\n\r\nIn my tests changing the alphabetical order of test files has an impact.\r\n\r\nFor example changing the names of the test files in this package to \r\n\r\ntest_1_spark_context_fixture.py\r\ntest_2_spark_session_fixture.py\r\n\r\nstill works, but\r\n\r\ntest_2_spark_context_fixture.py\r\ntest_1_spark_session_fixture.py\r\n\r\nfails.\r\n\r\n\r\n\r\n\r\n\r\nDetails: \r\n\r\nSystem is MAC OS\r\n\r\nuname - a yields\r\nDarwin 127.0.0.1 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64\r\n\r\npython verision is 3.6\r\n\r\nSpark verision is 2.3\r\nspark_home=/usr/local/Cellar/apache-spark/2.3.0/libexec\r\n\r\npytest --version\r\n\r\nThis is pytest version 3.5.1, \r\nsetuptools registered plugins:\r\n  pytest-spark-0.4.4 \r\n\r\n\r\n\r\n\r\nThe error is:\r\n\r\n==================================== ERRORS ====================================\r\n_________________ ERROR at setup of test_spark_context_fixture _________________\r\n\r\n    @pytest.fixture(scope='session')\r\n    def spark_context():\r\n        \"\"\"Return a SparkContext instance with reduced logging\r\n        (session scope).\r\n        \"\"\"\r\n    \r\n        from pyspark import SparkContext\r\n    \r\n>       sc = SparkContext()\r\n\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/usr/local/Cellar/apache-spark/2.3.0/libexec/python/pyspark/context.py:115: in __init__\r\n    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ncls = <class 'pyspark.context.SparkContext'>\r\ninstance = <[AttributeError(\"'SparkContext' object has no attribute 'master'\") raised in repr()] SparkContext object at 0x113cad160>\r\ngateway = None, conf = None\r\n\r\n    @classmethod\r\n    def _ensure_initialized(cls, instance=None, gateway=None, conf=None):\r\n        \"\"\"\r\n            Checks whether a SparkContext is initialized or not.\r\n            Throws error if a SparkContext is already running.\r\n            \"\"\"\r\n        with SparkContext._lock:\r\n            if not SparkContext._gateway:\r\n                SparkContext._gateway = gateway or launch_gateway(conf)\r\n                SparkContext._jvm = SparkContext._gateway.jvm\r\n    \r\n            if instance:\r\n                if (SparkContext._active_spark_context and\r\n                        SparkContext._active_spark_context != instance):\r\n                    currentMaster = SparkContext._active_spark_context.master\r\n                    currentAppName = SparkContext._active_spark_context.appName\r\n                    callsite = SparkContext._active_spark_context._callsite\r\n    \r\n                    # Raise error if there is already a running Spark context\r\n                    raise ValueError(\r\n                        \"Cannot run multiple SparkContexts at once; \"\r\n                        \"existing SparkContext(app=%s, master=%s)\"\r\n                        \" created by %s at %s:%s \"\r\n                        % (currentAppName, currentMaster,\r\n>                           callsite.function, callsite.file, callsite.linenum))\r\nE                   ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/6", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/6/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/6/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/6/events", "html_url": "https://github.com/malexer/pytest-spark/issues/6", "id": 319204105, "node_id": "MDU6SXNzdWUzMTkyMDQxMDU=", "number": 6, "title": "Unable to use it in a unittest.TestCase", "user": {"login": "AbdealiJK", "id": 2200743, "node_id": "MDQ6VXNlcjIyMDA3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2200743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AbdealiJK", "html_url": "https://github.com/AbdealiJK", "followers_url": "https://api.github.com/users/AbdealiJK/followers", "following_url": "https://api.github.com/users/AbdealiJK/following{/other_user}", "gists_url": "https://api.github.com/users/AbdealiJK/gists{/gist_id}", "starred_url": "https://api.github.com/users/AbdealiJK/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AbdealiJK/subscriptions", "organizations_url": "https://api.github.com/users/AbdealiJK/orgs", "repos_url": "https://api.github.com/users/AbdealiJK/repos", "events_url": "https://api.github.com/users/AbdealiJK/events{/privacy}", "received_events_url": "https://api.github.com/users/AbdealiJK/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-01T13:54:53Z", "updated_at": "2018-08-28T21:43:51Z", "closed_at": "2018-08-28T21:43:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Was trying to use this in my test cases. I normally use the Class based approach with unittest.TestCase and was not able to get it to work.\r\nExample:\r\n\r\n```\r\nclass ExampleTest(unittest.TestCase):\r\n\r\n    @pytest.mark.usefixtures(\"spark_session\")\r\n    def test_create_df(self, spark_session):\r\n        df = spark_session.createDataFrame([\r\n            [1, 1, 1],\r\n            [1, 2, 1],\r\n            [1, 2, 4]\r\n        ], columns=['x', 'y', 'z'])\r\n```\r\n\r\nI can see that pyspark recognized the plugin, the spark session has been created but it fails with:\r\n`TypeError: test_create_df() missing 1 required positional argument: 'spark_session'`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/malexer/pytest-spark/issues/4", "repository_url": "https://api.github.com/repos/malexer/pytest-spark", "labels_url": "https://api.github.com/repos/malexer/pytest-spark/issues/4/labels{/name}", "comments_url": "https://api.github.com/repos/malexer/pytest-spark/issues/4/comments", "events_url": "https://api.github.com/repos/malexer/pytest-spark/issues/4/events", "html_url": "https://github.com/malexer/pytest-spark/issues/4", "id": 278559380, "node_id": "MDU6SXNzdWUyNzg1NTkzODA=", "number": 4, "title": "findspark.init(spark_home) is never called", "user": {"login": "swhitelaw", "id": 15821789, "node_id": "MDQ6VXNlcjE1ODIxNzg5", "avatar_url": "https://avatars3.githubusercontent.com/u/15821789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/swhitelaw", "html_url": "https://github.com/swhitelaw", "followers_url": "https://api.github.com/users/swhitelaw/followers", "following_url": "https://api.github.com/users/swhitelaw/following{/other_user}", "gists_url": "https://api.github.com/users/swhitelaw/gists{/gist_id}", "starred_url": "https://api.github.com/users/swhitelaw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/swhitelaw/subscriptions", "organizations_url": "https://api.github.com/users/swhitelaw/orgs", "repos_url": "https://api.github.com/users/swhitelaw/repos", "events_url": "https://api.github.com/users/swhitelaw/events{/privacy}", "received_events_url": "https://api.github.com/users/swhitelaw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-01T18:48:27Z", "updated_at": "2017-12-04T15:33:35Z", "closed_at": "2017-12-04T15:33:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "in pytest_configure, find_spark_home_var is called, which will raise an error if it can't find spark_home.  But, find_spark_home_var never calls findspark.init(spark_home), which previously could automatically find the spark_home without being set anywhere by looking in common locations. This means we cannot run any pytests that were previously running due to not being able to fine spark_home", "performed_via_github_app": null, "score": 1.0}]}