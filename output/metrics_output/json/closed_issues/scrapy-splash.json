{"total_count": 170, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/266", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/266/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/266/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/266/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/266", "id": 677050359, "node_id": "MDU6SXNzdWU2NzcwNTAzNTk=", "number": 266, "title": "Scrapy shell works with splash settings on macOS, the spider fails", "user": {"login": "emadboctorx", "id": 58062537, "node_id": "MDQ6VXNlcjU4MDYyNTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/58062537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emadboctorx", "html_url": "https://github.com/emadboctorx", "followers_url": "https://api.github.com/users/emadboctorx/followers", "following_url": "https://api.github.com/users/emadboctorx/following{/other_user}", "gists_url": "https://api.github.com/users/emadboctorx/gists{/gist_id}", "starred_url": "https://api.github.com/users/emadboctorx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emadboctorx/subscriptions", "organizations_url": "https://api.github.com/users/emadboctorx/orgs", "repos_url": "https://api.github.com/users/emadboctorx/repos", "events_url": "https://api.github.com/users/emadboctorx/events{/privacy}", "received_events_url": "https://api.github.com/users/emadboctorx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-11T17:15:20Z", "updated_at": "2020-08-12T11:03:37Z", "closed_at": "2020-08-12T11:03:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to scrape the content from this [link][1] on my macOS, using `scrapy` with `scrapy_splash` settings and `BeautifulSoup` I followed the instructions in the [documentation][2] \r\n\r\n* I tested every single command in scrapy shell and each works perfectly fine, tested on several pages. when I run the spider with the same commands, it fails to detect any of the items.\r\n\r\n`settings.py` \r\n\r\n    BOT_NAME = 'stepstone'\r\n    SPIDER_MODULES = ['stepstone.spiders']\r\n    NEWSPIDER_MODULE = 'stepstone.spiders'\r\n    SPLASH_URL = 'http://0.0.0.0:8050'  # changed from the documentation's http://192.168.59.103:8050 which does not work \r\n    DOWNLOADER_MIDDLEWARES = {\r\n        'scrapy_splash.SplashCookiesMiddleware': 723,\r\n        'scrapy_splash.SplashMiddleware': 725,\r\n        'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\r\n    }\r\n    SPIDER_MIDDLEWARES = {\r\n        'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\r\n    }\r\n    DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\r\n    HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\r\n\r\nThe spider module:\r\n\r\n    from scrapy.spiders import Spider\r\n    from scrapy_splash import SplashRequest\r\n    from scrapy import Request\r\n    from bs4 import BeautifulSoup\r\n    \r\n    \r\n    class StepSpider(Spider):\r\n        name = 'step'\r\n        allowed_domains = ['www.stepstone.de']\r\n        start_urls = [\r\n            'https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs='\r\n            '%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entw'\r\n            'ickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%223000001'\r\n            '15%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geoc'\r\n            'ity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=home'\r\n            'pagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwic'\r\n            'kler%2Fin&ws=Deutschland&ra=30/'\r\n        ]\r\n    \r\n        @staticmethod\r\n        def extract_item(soup, extraction_path):\r\n            result = soup.find(*extraction_path)\r\n            if result:\r\n                return result.getText()\r\n    \r\n        def parse(self, response):\r\n            soup = BeautifulSoup(response.body, features='lxml')\r\n            listings = [\r\n                response.urljoin(item)\r\n                for item in response.xpath('//div/div/a/@href').extract()\r\n                if 'stellenangebote' in item\r\n            ]\r\n            yield from [\r\n                Request(\r\n                    url,\r\n                    callback=self.parse_item,\r\n                    cb_kwargs={'soup': soup},\r\n                    meta={'splash': {'args': {'html': 1, 'png': 1,}}},\r\n                )\r\n                for url in listings\r\n            ]\r\n            next_page = soup.find('a', {'data-at': 'pagination-next'})\r\n            if next_page:\r\n                yield SplashRequest(next_page.get('href'), self.parse)\r\n    \r\n        def parse_header(self, response, soup):\r\n            title = response.xpath('//h1/text()').get()\r\n            location = self.extract_item(\r\n                soup, ('li', {'class': 'at-listing__li: st-icons_location'})\r\n            )\r\n            contract_type = self.extract_item(\r\n                soup, ('li', {'class': 'at-listing__list-icons_contract-type'})\r\n            )\r\n            work_type = self.extract_item(\r\n                soup, ('li', {'class': 'at-listing__list-icons_work-type'})\r\n            )\r\n            return {\r\n                'title': title,\r\n                'location': location,\r\n                'contract_type': contract_type,\r\n                'work_type': work_type,\r\n            }\r\n    \r\n        def parse_body(self, response, soup):\r\n            titles = response.xpath('//h4/text()').extract()\r\n            intro = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-introduction-content'})\r\n            )\r\n            description = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-description-content'})\r\n            )\r\n            profile = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-profile-content'})\r\n            )\r\n            we_offer = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-weoffer-content'})\r\n            )\r\n            contact = self.extract_item(\r\n                soup, ('div', {'class': 'at-section-text-contact-content'})\r\n            )\r\n            return {\r\n                title: text\r\n                for title, text in zip(\r\n                    titles, [intro, description, profile, we_offer, contact]\r\n                )\r\n            }\r\n    \r\n        def parse_item(self, response, soup):\r\n            items = self.parse_header(response, soup)\r\n            items.update(self.parse_body(response, soup))\r\n            yield items\r\n\r\nfull log:\r\n\r\n    2020-08-11 17:57:44 [scrapy.utils.log] INFO: Scrapy 2.2.1 started (bot: stepstone)\r\n    2020-08-11 17:57:44 [scrapy.utils.log] INFO: Versions: lxml 4.5.0.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.3 (default, May 27 2020, 20:54:22) - [Clang 11.0.3 (clang-1103.0.32.59)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.0, Platform macOS-10.15.6-x86_64-i386-64bit\r\n    2020-08-11 17:57:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\r\n    2020-08-11 17:57:44 [scrapy.crawler] INFO: Overridden settings:\r\n    {'BOT_NAME': 'stepstone',\r\n     'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter',\r\n     'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage',\r\n     'NEWSPIDER_MODULE': 'stepstone.spiders',\r\n     'SPIDER_MODULES': ['stepstone.spiders']}\r\n    2020-08-11 17:57:44 [scrapy.extensions.telnet] INFO: Telnet Password: 71c7bd3bdaf32c63\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled extensions:\r\n    ['scrapy.extensions.corestats.CoreStats',\r\n     'scrapy.extensions.telnet.TelnetConsole',\r\n     'scrapy.extensions.memusage.MemoryUsage',\r\n     'scrapy.extensions.logstats.LogStats']\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n    ['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n     'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n     'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n     'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n     'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n     'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n     'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n     'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n     'scrapy_splash.SplashCookiesMiddleware',\r\n     'scrapy_splash.SplashMiddleware',\r\n     'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n     'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n     'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n    ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n     'scrapy_splash.SplashDeduplicateArgsMiddleware',\r\n     'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n     'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n     'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n     'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n    2020-08-11 17:57:44 [scrapy.middleware] INFO: Enabled item pipelines:\r\n    []\r\n    2020-08-11 17:57:44 [scrapy.core.engine] INFO: Spider opened\r\n    2020-08-11 17:57:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n    2020-08-11 17:57:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\r\n    2020-08-11 17:57:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30/> (referer: None)\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: utf-8  confidence = 0.99\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-JP Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: GB2312 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-KR Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: CP949 Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: Big5 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-TW Taiwan confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1251 Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: KOI8-R Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Russian confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: MacCyrillic Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: IBM866 Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: IBM855 Russian confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-7 Greek confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1253 Greek confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-5 Bulgairan confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1251 Bulgarian confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: TIS-620 Thai confidence = 0.041278205445058724\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: ISO-8859-9 Turkish confidence = 0.5186494104315963\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: windows-1255 Hebrew confidence = 0.0\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: utf-8  confidence = 0.99\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: SHIFT_JIS Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-JP Japanese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: GB2312 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-KR Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: CP949 Korean confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: Big5 Chinese confidence = 0.01\r\n    2020-08-11 17:57:46 [chardet.charsetprober] DEBUG: EUC-TW Taiwan confidence = 0.01\r\n    2020-08-11 17:57:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.stepstone.de/stellenangebote--JAVA-Software-Entwickler-m-w-d-Sueddeutschland-TECCON-Consulting-Engineering-GmbH--6582908-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=1_1_25_dynrl_m_0_0_0_0> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\r\n    2020-08-11 17:57:47 [py.warnings] WARNING: /usr/local/lib/python3.8/site-packages/scrapy_splash/request.py:41: ScrapyDeprecationWarning: Call to deprecated function to_native_str. Use to_unicode instead.\r\n      url = to_native_str(url)\r\n    \r\n    2020-08-11 17:57:50 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-fuer-Windowsapplikationen-m-w-d-Stockach-oder-Boeblingen-Baumer-MDS-GmbH--6568164-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=19_19_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler f\u00fcr Windowsapplikationen (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Herausforderung:': None, 'Sie verf\u00fcgen \u00fcber:': None, 'Wir bieten:': None, 'Kontakt:': None}\r\n    2020-08-11 17:57:51 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:51 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--JAVA-Software-Entwickler-m-w-d-Sueddeutschland-TECCON-Consulting-Engineering-GmbH--6582908-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=1_1_25_dynrl_m_0_0_0_0>\r\n    {'title': 'JAVA Software-Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Einleitung': None, 'Ihre Aufgaben': None, 'Ihr Profil': None, 'Wir bieten': None, 'Weitere Informationen': None}\r\n    2020-08-11 17:57:52 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-Business-Engineer-fuer-Blockchain-Team-in-Gruendung-w-m-d-Frankfurt-Main-Deutsche-Bahn-AG--6249570-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=16_16_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler / Business Engineer f\u00fcr Blockchain-Team in Gr\u00fcndung (w/m/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Was dich erwartet': None, 'Was wir erwarten': None, 'Wir bieten': None, 'Standort': None}\r\n    2020-08-11 17:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-w-m-d-Diagnose-und-Visualisierungssysteme-Mannheim-Halle-Stadler-Mannheim-GmbH--6615613-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=13_13_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler (w/m/d) Diagnose und Visualisierungssysteme', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben:': None, 'Ihr Profil:': None, 'Unser Angebot:': None, 'Begeistert?': None}\r\n    2020-08-11 17:57:55 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:57:55 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-m-w-d-Rosenheim-Agenda-Informationssysteme-GmbH-Co-KG--6590641-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=17_17_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben:': None, 'Ihr Profil:': None, 'Das spricht f\u00fcr uns:': None, 'Kontakt:': None, 'Standort': None}\r\n    2020-08-11 17:58:08 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:08 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-w-m-d-fuer-Fahrzeugsteuerung-Mannheim-Halle-Stadler-Mannheim-GmbH--6615612-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=11_11_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software-Entwickler (w/m/d) f\u00fcr Fahrzeugsteuerung', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben:': None, 'Ihr Profil:': None, 'Unser Angebot:': None, 'Begeistert?': None}\r\n    ^C2020-08-11 17:58:09 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \r\n    2020-08-11 17:58:09 [scrapy.core.engine] INFO: Closing spider (shutdown)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-m-w-d-Meissen-Staatliche-Porzellan-Manufaktur-Meissen-GmbH--6462761-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=14_14_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Wir gehen neue Wege': None, 'Ihre Aufgaben': None, 'unsere Anforderungen': None, 'unser Angebot': None, 'Kontakt': None}\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Agiler-Software-Entwickler-m-w-div-Dresden-Otto-Group-Solution-Provider-OSP-GmbH--4573007-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=8_8_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Agiler Software Entwickler (m/w/div)', 'location': None, 'contract_type': None, 'work_type': None, '\u00dcber uns': None, 'Was dich erwartet': None, 'Was du mitbringen solltest': None, 'Diese und weitere Benefits erwarten dich': None, 'Kontakt': None}\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-m-w-d-Essen-Lowell-Group--6615697-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=9_9_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Ihre Aufgaben': None, 'Ihr Profil': None, 'Wir bieten': None, 'Kontakt': None, 'Standort': None}\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Softwareentwickler-m-w-d-Fullstack-Web-Boeblingen-Braunschweig-Deutschlandweit-Ingolstadt-Muenchen-Norddeutschland-Stuttgart-umlaut--6122455-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=15_15_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Softwareentwickler (m/w/d) - Fullstack Web', 'location': None, 'contract_type': None, 'work_type': None, 'our \u00f6ffer': None, 'y\u00f6u': None, 'top 5 reas\u00f6ns': None, 'c\u00f6ntact': None, 'Mitarbeiterbewertungen': None}\r\n    2020-08-11 17:58:13 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://0.0.0.0:8050/render.json> (referer: None)\r\n    2020-08-11 17:58:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.stepstone.de/stellenangebote--Software-Entwickler-E-Commerce-m-w-d-Dresden-Otto-Group-Solution-Provider-OSP-GmbH--6550022-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=10_10_25_dynrl_m_0_0_0_0>\r\n    {'title': 'Software Entwickler E-Commerce (m/w/d)', 'location': None, 'contract_type': None, 'work_type': None, 'Was dich erwartet': None, 'Was du mitbringen solltest': None, 'Kontakt': None, 'Standort': None}\r\n    ^C2020-08-11 17:58:16 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&companyid=0&sourceofthesearchfield=homepagemex%3Ageneral&qs=[{\"id\"%3A216805%2C\"description\"%3A\"Software-Entwickler\\%2Fin\"%2C\"type\"%3A\"jd\"}%2C{\"id\"%3A300000115%2C\"description\"%3A\"Deutschland\"%2C\"type\"%3A\"geocity\"}]&cityid=300000115&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30&suid=90b7defb-2854-4c23-98bd-b39bc15a6922&of=25&action=paging_next via http://0.0.0.0:8050/render.html> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n    2020-08-11 17:58:16 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <POST http://0.0.0.0:8050/render.json> (failed 1 times): [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion: Connection lost.>]\r\n\r\ndocker log:\r\n\r\n    2020-08-11 15:57:27+0000 [-] Log opened.\r\n    2020-08-11 15:57:27.990815 [-] Xvfb is started: ['Xvfb', ':2061643423', '-screen', '0', '1024x768x24', '-nolisten', 'tcp']\r\n    QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-splash'\r\n    2020-08-11 15:57:28.135258 [-] Splash version: 3.4.1\r\n    2020-08-11 15:57:28.203198 [-] Qt 5.13.1, PyQt 5.13.1, WebKit 602.1, Chromium 73.0.3683.105, sip 4.19.19, Twisted 19.7.0, Lua 5.2\r\n    2020-08-11 15:57:28.203826 [-] Python 3.6.9 (default, Nov  7 2019, 10:44:02) [GCC 8.3.0]\r\n    2020-08-11 15:57:28.204679 [-] Open files limit: 1048576\r\n    2020-08-11 15:57:28.205242 [-] Can't bump open files limit\r\n    2020-08-11 15:57:28.229336 [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles\r\n    2020-08-11 15:57:28.229855 [-] memory cache: enabled, private mode: enabled, js cross-domain access: disabled\r\n    2020-08-11 15:57:28.410540 [-] verbosity=1, slots=20, argument_cache_max_entries=500, max-timeout=90.0\r\n    2020-08-11 15:57:28.411484 [-] Web UI: enabled, Lua: enabled (sandbox: enabled), Webkit: enabled, Chromium: enabled\r\n    2020-08-11 15:57:28.412634 [-] Site starting on 8050\r\n    2020-08-11 15:57:28.412924 [-] Starting factory <twisted.web.server.Site object at 0x7fbfa77591d0>\r\n    2020-08-11 15:57:28.414172 [-] Server listening on http://0.0.0.0:8050\r\n    2020-08-11 15:57:49.583386 [events] {\"path\": \"/render.json\", \"rendertime\": 2.339588165283203, \"maxrss\": 236848, \"load\": [0.1, 0.05, 0.06], \"fds\": 102, \"active\": 7, \"qsize\": 0, \"_id\": 140461124347104, \"method\": \"POST\", \"timestamp\": 1597161469, \"user-agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\", \"args\": {\"headers\": {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en\", \"Cookie\": \"cfid=bef36179-b81a-44e3-9bd1-059f5406a911; cftoken=0; USER_HASH_ID=f896d04a-5348-455b-a0ab-ffd0d5f6e674; V5=1; UXUSER=BLACKLIST%3BA%3B%20%3B; STEPSTONEV5LANG=de; ONLINE_CF=14-190; dtCookie=35$77973CDF4397BDD4A3EF1CAFDB05C9FD\", \"Referer\": \"https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30/\", \"User-Agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\"}, \"html\": 1, \"png\": 1, \"url\": \"https://www.stepstone.de/stellenangebote--Software-Entwickler-fuer-Windowsapplikationen-m-w-d-Stockach-oder-Boeblingen-Baumer-MDS-GmbH--6568164-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=19_19_25_dynrl_m_0_0_0_0\", \"uid\": 140461124347104}, \"status_code\": 200, \"client_ip\": \"172.17.0.1\"}\r\n    2020-08-11 15:57:49.584498 [-] \"172.17.0.1\" - - [11/Aug/2020:15:57:48 +0000] \"POST /render.json HTTP/1.1\" 200 371319 \"-\" \"Scrapy/2.2.1 (+https://scrapy.org)\"\r\n    2020-08-11 15:57:49.777352 [events] {\"path\": \"/render.json\", \"rendertime\": 2.6071407794952393, \"maxrss\": 243100, \"load\": [0.1, 0.05, 0.06], \"fds\": 106, \"active\": 6, \"qsize\": 0, \"_id\": 140461124981984, \"method\": \"POST\", \"timestamp\": 1597161469, \"user-agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\", \"args\": {\"headers\": {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en\", \"Cookie\": \"cfid=bef36179-b81a-44e3-9bd1-059f5406a911; cftoken=0; USER_HASH_ID=f896d04a-5348-455b-a0ab-ffd0d5f6e674; V5=1; UXUSER=BLACKLIST%3BA%3B%20%3B; STEPSTONEV5LANG=de; ONLINE_CF=14-190; dtCookie=35$77973CDF4397BDD4A3EF1CAFDB05C9FD\", \"Referer\": \"https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30/\", \"User-Agent\": \"Scrapy/2.2.1 (+https://scrapy.org)\"}, \"html\": 1, \"png\": 1, \"url\": \"https://www.stepstone.de/stellenangebote--JAVA-Software-Entwickler-m-w-d-Sueddeutschland-TECCON-Consulting-Engineering-GmbH--6582908-inline.html?suid=90b7defb-2854-4c23-98bd-b39bc15a6922&rltr=1_1_25_dynrl_m_0_0_0_0\", \"uid\": 140461124981984}, \"status_code\": 200, \"client_ip\": \"172.17.0.1\"}\r\n\r\n  [1]: https://www.stepstone.de/5/ergebnisliste.html?stf=freeText&ns=1&qs=%5B%7B%22id%22%3A%22216805%22%2C%22description%22%3A%22Software-Entwickler%2Fin%22%2C%22type%22%3A%22jd%22%7D%2C%7B%22id%22%3A%22300000115%22%2C%22description%22%3A%22Deutschland%22%2C%22type%22%3A%22geocity%22%7D%5D&companyID=0&cityID=300000115&sourceOfTheSearchField=homepagemex%3Ageneral&searchOrigin=Homepage_top-search&ke=Software-Entwickler%2Fin&ws=Deutschland&ra=30\r\n  [2]: https://github.com/scrapy-plugins/scrapy-splash\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/257", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/257/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/257/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/257/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/257", "id": 611506246, "node_id": "MDU6SXNzdWU2MTE1MDYyNDY=", "number": 257, "title": "No docker installation?", "user": {"login": "alexxsanchezm", "id": 3522488, "node_id": "MDQ6VXNlcjM1MjI0ODg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3522488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexxsanchezm", "html_url": "https://github.com/alexxsanchezm", "followers_url": "https://api.github.com/users/alexxsanchezm/followers", "following_url": "https://api.github.com/users/alexxsanchezm/following{/other_user}", "gists_url": "https://api.github.com/users/alexxsanchezm/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexxsanchezm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexxsanchezm/subscriptions", "organizations_url": "https://api.github.com/users/alexxsanchezm/orgs", "repos_url": "https://api.github.com/users/alexxsanchezm/repos", "events_url": "https://api.github.com/users/alexxsanchezm/events{/privacy}", "received_events_url": "https://api.github.com/users/alexxsanchezm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-03T20:58:54Z", "updated_at": "2020-05-05T07:08:24Z", "closed_at": "2020-05-05T07:08:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I found docker is very useful tool but now everybody wants to use it. I my case I hate use docker if I don't needed. I think the idea behind docker is to simplify configs around something. I would love use docker if the setup is too heavy. But for this??? come on!. We already have Virtual Envs that should be enough.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/256", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/256/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/256/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/256/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/256", "id": 607789372, "node_id": "MDU6SXNzdWU2MDc3ODkzNzI=", "number": 256, "title": "Weird args and proxy behavior in lua script when resending a splash request", "user": {"login": "jeremfrs", "id": 8864964, "node_id": "MDQ6VXNlcjg4NjQ5NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/8864964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeremfrs", "html_url": "https://github.com/jeremfrs", "followers_url": "https://api.github.com/users/jeremfrs/followers", "following_url": "https://api.github.com/users/jeremfrs/following{/other_user}", "gists_url": "https://api.github.com/users/jeremfrs/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeremfrs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeremfrs/subscriptions", "organizations_url": "https://api.github.com/users/jeremfrs/orgs", "repos_url": "https://api.github.com/users/jeremfrs/repos", "events_url": "https://api.github.com/users/jeremfrs/events{/privacy}", "received_events_url": "https://api.github.com/users/jeremfrs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-27T19:20:24Z", "updated_at": "2020-04-27T20:59:53Z", "closed_at": "2020-04-27T20:59:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "I encoutered some weird args behaviors in lua.\r\n\r\nI create my SplashRequest like so:\r\n`yield SplashRequest(url=url,\r\n                callback=self.parse,\r\n                endpoint='execute',\r\n                args={\r\n                    'lua_source': self.LUA_SOURCE,\r\n                    'pproxy': \"NOT_WORKING_PROXY\",\r\n                    'wait': 3,\r\n                },\r\n                cache_args=['lua_source']\r\n            )`\r\n\r\nAnd when my proxy isn't working i catch it in my middleware and i send back the same request but with another proxy:\r\n\r\n```def process_response(self, request, response, spider):\r\ndef process_response(self, request, response, spider):\r\n        print(\"\\tEntering process_response-------------------------------\")\r\n\r\n        print(response.status)\r\n        if (response.status == 504 or response.status == 400):\r\n            meta = deepcopy(response.request.meta)\r\n            meta[\"splash\"][\"args\"][\"pproxy\"] = \"WORKING_PROXY\"\r\n            newrq = response.request.replace(meta=meta, dont_filter=True)\r\n            return newrq\r\n\r\n        return response\r\n```\r\n\r\nI double checked, newrq has the \"WORKING_PROXY\" and not the \"NOT_WORKING_PROXY\"\r\n\r\nSo now here is the lua code in self.LUA_SOURCE\r\n```\r\nfunction useproxy(splash)\r\n    local host = splash.args.pproxy\r\n    local port = 80\r\n\r\n    print(host)\r\n    splash:on_request(function (request)\r\n        print(host)\r\n        request:set_proxy{host, port, username=\"myuser\", password=\"mypwd\"}\r\n    end)\r\nend\r\n\r\nfunction main(splash)\r\n    local user_agent = \"Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36\"\r\n    splash:set_user_agent(user_agent)\r\n    assert(splash:go(splash.args.url))\r\n    useproxy(splash)\r\n    return splash:html()\r\nend\r\n```\r\n\r\nWhen the order of function in the main is EXACTLY like that, the proxy is set correctly and my request works BUT both print in useproxy print the old proxy (NOT_WORKING_PROXY). \r\nAnother weird behavior, if i remove the line that set the user agent, the proxy is set to the old one (NOT_WORKING_PROXY) and if i put useproxy before splash:go it's the same.\r\n\r\nJust so you know, my code is working perfectly fine if i remove my code in process_response and i don't return a new request.\r\n\r\nI think it is a bug related to lua but i'm not sure or maybe i'm stuck with a mistake, i don't know.\r\nIf you know a fix or if it's a mistake, let me know please.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/248", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/248/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/248/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/248/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/248", "id": 545409304, "node_id": "MDU6SXNzdWU1NDU0MDkzMDQ=", "number": 248, "title": "No results at \"Server listening on http://0.0.0.0:8050\"", "user": {"login": "mgrazianoc", "id": 22849928, "node_id": "MDQ6VXNlcjIyODQ5OTI4", "avatar_url": "https://avatars0.githubusercontent.com/u/22849928?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mgrazianoc", "html_url": "https://github.com/mgrazianoc", "followers_url": "https://api.github.com/users/mgrazianoc/followers", "following_url": "https://api.github.com/users/mgrazianoc/following{/other_user}", "gists_url": "https://api.github.com/users/mgrazianoc/gists{/gist_id}", "starred_url": "https://api.github.com/users/mgrazianoc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mgrazianoc/subscriptions", "organizations_url": "https://api.github.com/users/mgrazianoc/orgs", "repos_url": "https://api.github.com/users/mgrazianoc/repos", "events_url": "https://api.github.com/users/mgrazianoc/events{/privacy}", "received_events_url": "https://api.github.com/users/mgrazianoc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-01-05T12:12:49Z", "updated_at": "2020-01-07T12:17:50Z", "closed_at": "2020-01-07T12:17:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm on Windows using Docker tool box.\r\n\r\nWhen I run `docker run -it -p 8050:8050 --rm scrapinghub/splash`, I receive the message `Server listening on http://0.0.0.0:8050`,as it should be. But when I go to the address, the only thing that returns is `ERR_ADDRESS_INVALID`, the page isn't loading. \r\n\r\nI try to search for this problem, changing the `SPLASH_URL` on my `settings.py` according to my Docker IP adress, but still with no sucess. The reason I'm intereseting in fixing this issue on my machine is because when I crawl with my spider, it just does nothing, i.e., the logging info returns:\r\n\r\n` [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)`\r\n\r\n\r\nAny ideias on how I messed this up? Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/240", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/240/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/240/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/240/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/240", "id": 491953178, "node_id": "MDU6SXNzdWU0OTE5NTMxNzg=", "number": 240, "title": "Scraping with splash and luminatii", "user": {"login": "anthonysanchezmk", "id": 45187720, "node_id": "MDQ6VXNlcjQ1MTg3NzIw", "avatar_url": "https://avatars3.githubusercontent.com/u/45187720?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anthonysanchezmk", "html_url": "https://github.com/anthonysanchezmk", "followers_url": "https://api.github.com/users/anthonysanchezmk/followers", "following_url": "https://api.github.com/users/anthonysanchezmk/following{/other_user}", "gists_url": "https://api.github.com/users/anthonysanchezmk/gists{/gist_id}", "starred_url": "https://api.github.com/users/anthonysanchezmk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anthonysanchezmk/subscriptions", "organizations_url": "https://api.github.com/users/anthonysanchezmk/orgs", "repos_url": "https://api.github.com/users/anthonysanchezmk/repos", "events_url": "https://api.github.com/users/anthonysanchezmk/events{/privacy}", "received_events_url": "https://api.github.com/users/anthonysanchezmk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-11T00:07:16Z", "updated_at": "2019-11-21T16:41:33Z", "closed_at": "2019-11-21T16:41:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to use luminatii for my proxy and splash to render the javascript, but my luminatii can't access the splash to render the javascript. I'm getting a 403. I guess a clarification that could help as well is, should my proxy be called first then splash is called through proxy, or should it be splash url being called, which then uses proxy to make request to website. Thank you for taking the time.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/239", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/239/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/239/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/239/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/239", "id": 489570065, "node_id": "MDU6SXNzdWU0ODk1NzAwNjU=", "number": 239, "title": "Scraping a site with recaptcha challange", "user": {"login": "sjaanus", "id": 964450, "node_id": "MDQ6VXNlcjk2NDQ1MA==", "avatar_url": "https://avatars1.githubusercontent.com/u/964450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjaanus", "html_url": "https://github.com/sjaanus", "followers_url": "https://api.github.com/users/sjaanus/followers", "following_url": "https://api.github.com/users/sjaanus/following{/other_user}", "gists_url": "https://api.github.com/users/sjaanus/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjaanus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjaanus/subscriptions", "organizations_url": "https://api.github.com/users/sjaanus/orgs", "repos_url": "https://api.github.com/users/sjaanus/repos", "events_url": "https://api.github.com/users/sjaanus/events{/privacy}", "received_events_url": "https://api.github.com/users/sjaanus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-05T07:11:37Z", "updated_at": "2019-11-21T16:41:14Z", "closed_at": "2019-11-21T16:41:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "Getting this message and not able to parse site.\r\n\r\n> Please upgrade to a supported browser to get a reCAPTCHA challenge.\r\n\r\nIs there a way to set some parameter, so recaptcha accepts splash as supported browser?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/238", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/238/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/238/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/238/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/238", "id": 484693375, "node_id": "MDU6SXNzdWU0ODQ2OTMzNzU=", "number": 238, "title": "js in lua script doesn't run at all", "user": {"login": "zorroli93", "id": 28912198, "node_id": "MDQ6VXNlcjI4OTEyMTk4", "avatar_url": "https://avatars3.githubusercontent.com/u/28912198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zorroli93", "html_url": "https://github.com/zorroli93", "followers_url": "https://api.github.com/users/zorroli93/followers", "following_url": "https://api.github.com/users/zorroli93/following{/other_user}", "gists_url": "https://api.github.com/users/zorroli93/gists{/gist_id}", "starred_url": "https://api.github.com/users/zorroli93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zorroli93/subscriptions", "organizations_url": "https://api.github.com/users/zorroli93/orgs", "repos_url": "https://api.github.com/users/zorroli93/repos", "events_url": "https://api.github.com/users/zorroli93/events{/privacy}", "received_events_url": "https://api.github.com/users/zorroli93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-23T20:13:02Z", "updated_at": "2019-11-21T16:41:46Z", "closed_at": "2019-11-21T16:41:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there, this is my lua script blow:\r\nscript = '''\r\nfunction main(splash, args)\r\n\r\n  assert(splash:go(args.url))\r\n  assert(splash:wait(args.wait))\r\n  \r\n  js = string.format(\"document.querySelector('#pg_paper_Performance #next_paper_Performance span').click();\", args.page)\r\n  splash:runjs(js)\r\n  splash:wait(5)\r\n  \r\n  return splash:html()\r\nend\r\n'''\r\nthe problem is when I run it in \"SplashRequest\", the js part didn't run at all, so i always got the first page table content, could you help me out?thank a lot!!!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/237", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/237/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/237/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/237/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/237", "id": 481871615, "node_id": "MDU6SXNzdWU0ODE4NzE2MTU=", "number": 237, "title": "Fetch number of results from Google News for a given time range", "user": {"login": "mayankgautam", "id": 1979588, "node_id": "MDQ6VXNlcjE5Nzk1ODg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1979588?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mayankgautam", "html_url": "https://github.com/mayankgautam", "followers_url": "https://api.github.com/users/mayankgautam/followers", "following_url": "https://api.github.com/users/mayankgautam/following{/other_user}", "gists_url": "https://api.github.com/users/mayankgautam/gists{/gist_id}", "starred_url": "https://api.github.com/users/mayankgautam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mayankgautam/subscriptions", "organizations_url": "https://api.github.com/users/mayankgautam/orgs", "repos_url": "https://api.github.com/users/mayankgautam/repos", "events_url": "https://api.github.com/users/mayankgautam/events{/privacy}", "received_events_url": "https://api.github.com/users/mayankgautam/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-08-17T10:18:43Z", "updated_at": "2019-11-21T16:40:56Z", "closed_at": "2019-11-21T16:40:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want to get the total number of news articles published for a list of keywords between a specified time period (Like last six months).\r\n\r\nI've tried to use scrapy library to scrap google news but I'm unable to get results for the specified time period using the below code:\r\n\r\n```\r\nimport scrapy\r\n\r\nclass QuotesSpider(scrapy.Spider):\r\n    name = 'quotes'\r\n    allowed_domains = ['google.com']\r\n    start_urls = ['https://www.google.com/search?tbs=cdr%3A1%2Ccd_min%3A1%2F1%2F2019%2Ccd_max%3A8%2F1%2F2019&tbm=nws&ei=1tJXXfHLM4-S9QO07onwCg&q=%22Apple+Inc%22&oq=%22Apple+Inc%22&gs_l=psy-ab.3..0l3.48094.50359.0.51273.9.9.0.0.0.0.318.893.0j2j1j1.4.0....0...1c.1.64.psy-ab..6.3.752....0.2cwnwIbhfS4']\r\n\r\ndef parse(self, response):\r\n    item = {\r\n        'search_title': response.css('input#sbhost::attr(value)').get(),\r\n        'results': response.css('#resultStats::text').get(),\r\n    }\r\n    yield item\r\n```\r\nI want to create a data frame as final output with the entity and number of results found on google news.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/236", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/236/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/236/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/236/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/236", "id": 481540787, "node_id": "MDU6SXNzdWU0ODE1NDA3ODc=", "number": 236, "title": "Scrapy-splash not rendering", "user": {"login": "TheGr8Destructo", "id": 42117602, "node_id": "MDQ6VXNlcjQyMTE3NjAy", "avatar_url": "https://avatars0.githubusercontent.com/u/42117602?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheGr8Destructo", "html_url": "https://github.com/TheGr8Destructo", "followers_url": "https://api.github.com/users/TheGr8Destructo/followers", "following_url": "https://api.github.com/users/TheGr8Destructo/following{/other_user}", "gists_url": "https://api.github.com/users/TheGr8Destructo/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheGr8Destructo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheGr8Destructo/subscriptions", "organizations_url": "https://api.github.com/users/TheGr8Destructo/orgs", "repos_url": "https://api.github.com/users/TheGr8Destructo/repos", "events_url": "https://api.github.com/users/TheGr8Destructo/events{/privacy}", "received_events_url": "https://api.github.com/users/TheGr8Destructo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-08-16T10:11:54Z", "updated_at": "2019-11-21T16:40:46Z", "closed_at": "2019-11-21T16:40:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Mike - I have posted this question on stack overflow. if someone can help me understand where I am going wrong it would be a huge help.  I am sure the issue lies with me but , if you  or others could help me fix the issue it would be much appreciated.\r\n\r\n[https://stackoverflow.com/questions/57498590/why-is-splash-request-not-rendering?noredirect=1#comment101507553_57498590](url)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/234", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/234/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/234/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/234/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/234", "id": 475362898, "node_id": "MDU6SXNzdWU0NzUzNjI4OTg=", "number": 234, "title": "Splash with Tor", "user": {"login": "turangojayev", "id": 4106961, "node_id": "MDQ6VXNlcjQxMDY5NjE=", "avatar_url": "https://avatars2.githubusercontent.com/u/4106961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/turangojayev", "html_url": "https://github.com/turangojayev", "followers_url": "https://api.github.com/users/turangojayev/followers", "following_url": "https://api.github.com/users/turangojayev/following{/other_user}", "gists_url": "https://api.github.com/users/turangojayev/gists{/gist_id}", "starred_url": "https://api.github.com/users/turangojayev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/turangojayev/subscriptions", "organizations_url": "https://api.github.com/users/turangojayev/orgs", "repos_url": "https://api.github.com/users/turangojayev/repos", "events_url": "https://api.github.com/users/turangojayev/events{/privacy}", "received_events_url": "https://api.github.com/users/turangojayev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1690347378, "node_id": "MDU6TGFiZWwxNjkwMzQ3Mzc4", "url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/labels/needs%20more%20info", "name": "needs more info", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-31T21:20:42Z", "updated_at": "2019-11-24T20:59:55Z", "closed_at": "2019-11-24T20:59:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am creating a new issue, since the ones I have seen that were related, did not solve my problem. I have the following scenario: I have a docker image that is built on top of [https://github.com/dperson/torproxy](https://github.com/dperson/torproxy) and inside I have spiders for different domains (Tor and crawler project share the same docker image). I use the image above for the reason that I change IP address every N requests. Now the first time I need to scrape some data from a domain, where main things that I am interested in, require rendering javascript and after some googling I ended up being here. I am able to use splash with scrapy without Tor, however when I want to combine those two, I keep getting  \r\n\r\n`2019-07-31 20:37:55 [scrapy.core.scraper] ERROR: Error downloading <GET SOME_URL via http://splash:8050/execute>`\r\n`Traceback (most recent call last):`\r\n`File \"/usr/local/lib/python3.5/dist-packages/scrapy/core/downloader/middleware.py\", line 44, in process_request`\r\n`    defer.returnValue((yield download_func(request=request, spider=spider)))`\r\n`twisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 111: Connection refused.\r\n`\r\n\r\nMy docker-compose.yml looks like this:\r\n<pre><code>\r\nversion: '2'\r\nservices:\r\n  scraper:\r\n    restart: always\r\n    build: .\r\n    container_name: scraper\r\n    ports:\r\n      - 8118:8118\r\n      - 9050:9050\r\n      - 9051:9051\r\n    volumes:\r\n      - /home/user/data:/data\r\n    environment:\r\n      - DATA_DIR=/data\r\n  splash:\r\n    image: scrapinghub/splash\r\n    container_name: splash\r\n    links:\r\n      - scraper\r\n    ports:\r\n      - 8050:8050\r\n</code></pre>\r\n\r\nand my Lua script looks like this:\r\n<pre><code>function main(splash)\r\n    splash:on_request(function(request)\r\n         request:set_proxy{ host = \"scraper\", port = 9050, type='socks' }\r\n    end)\r\n    splash:go('SOME_URL')\r\n    splash:wait(0.5)\r\n    local elements = splash:select_all('#Decisions > tbody > tr[style=\"cursor: pointer;\"]')\r\n    for _, el in ipairs(elements) do\r\n       el:click()\r\n       splash:wait(0.3)\r\n    end\r\n    return {splash:html()}\r\nend\r\n</code></pre>\r\n\r\nWhen I log into scraper container with docker exec and try to use splash with curl, it works. Also when I log into splash container and try to use Tor proxy running on scraper container with `curl -s --socks5-hostname scraper:9050 SOME_OTHER_URL` things seem to be working. Any ideas what could be the problem?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/233", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/233/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/233/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/233/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/233", "id": 473818112, "node_id": "MDU6SXNzdWU0NzM4MTgxMTI=", "number": 233, "title": "Cant load next page when clicking button", "user": {"login": "h4ckbarth", "id": 6439109, "node_id": "MDQ6VXNlcjY0MzkxMDk=", "avatar_url": "https://avatars1.githubusercontent.com/u/6439109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/h4ckbarth", "html_url": "https://github.com/h4ckbarth", "followers_url": "https://api.github.com/users/h4ckbarth/followers", "following_url": "https://api.github.com/users/h4ckbarth/following{/other_user}", "gists_url": "https://api.github.com/users/h4ckbarth/gists{/gist_id}", "starred_url": "https://api.github.com/users/h4ckbarth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/h4ckbarth/subscriptions", "organizations_url": "https://api.github.com/users/h4ckbarth/orgs", "repos_url": "https://api.github.com/users/h4ckbarth/repos", "events_url": "https://api.github.com/users/h4ckbarth/events{/privacy}", "received_events_url": "https://api.github.com/users/h4ckbarth/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-07-29T02:14:33Z", "updated_at": "2019-07-29T03:52:36Z", "closed_at": "2019-07-29T03:52:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm having a problem with a JS dinamically loaded website. There's no URL change when change pages in this website, there's only javascript loading content dinamically. So what i'm trying to do is call the \"Next\" button via splash:runjs and update the content before scrape. \r\nThe bit of code works directly on browser but not on splash. Here follows the code:\r\n\r\n```\r\nclass ZapSpider(scrapy.Spider):\r\n    name = 'zap'\r\n    allowed_domains = ['www.zapimoveis.com.br']\r\n\r\n    start_urls = ['https://www.zapimoveis.com.br/aluguel/apartamentos/pr+curitiba++batel/?__zt=nsldp:a']\r\n\r\n    def start_requests(self):\r\n        script = \"\"\"\r\n                function main(splash)\r\n                local url = splash.args.url\r\n                assert(splash:go(url))\r\n                assert(splash:wait(0.5))\r\n                assert(splash:runjs(\"document.getElementById('proximaPagina').click();\"))\r\n                splash:wait(5)\r\n                return {\r\n                    html = splash:html()\r\n                }\r\n                end\r\n                \"\"\"\r\n        for url in self.start_urls:\r\n            yield scrapy.Request(url, self.parse, meta={\r\n                'splash': {\r\n                    'args': {'lua_source': script},\r\n                    'endpoint': 'execute',\r\n                }\r\n            })\r\n\r\n\r\n    def parse(self, response):\r\n\r\n        # fetch base URL because response url is the Splash endpoint\r\n        baseurl = response.meta[\"splash\"][\"args\"][\"url\"]\r\n\r\n        # decode JSON response\r\n        splash_json = json.loads(response.body_as_unicode())\r\n\r\n        print(splash_json['html'])\r\n\r\n        # and build a new selector from the response \"html\" key from that object\r\n        selector = scrapy.Selector(text=splash_json[\"html\"], type=\"html\")\r\n\r\n        prices = []\r\n        places = []\r\n\r\n        for price in selector.xpath('//div[@class=\"preco\"]//strong/text()'):\r\n            price = price.get()\r\n            prices.append(price)\r\n\r\n        for lote in selector.xpath('//section[@class=\"endereco pull-right\"]//p/text()'):\r\n            lote = lote.get()\r\n            places.append(lote)\r\n\r\n        i = 0\r\n        for price in prices:\r\n            yield PlacesCrawlerItem(description=places[i], price=price)\r\n            i += 1\r\n```\r\n\r\nIt doesn't returns error on docker's terminal, i don't know if it's a bug, me doing something wrong or there's some additional behavior on the website.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/232", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/232/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/232/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/232/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/232", "id": 473654056, "node_id": "MDU6SXNzdWU0NzM2NTQwNTY=", "number": 232, "title": "SplashRequest does not execute LUA script", "user": {"login": "sgalich", "id": 49820348, "node_id": "MDQ6VXNlcjQ5ODIwMzQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/49820348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sgalich", "html_url": "https://github.com/sgalich", "followers_url": "https://api.github.com/users/sgalich/followers", "following_url": "https://api.github.com/users/sgalich/following{/other_user}", "gists_url": "https://api.github.com/users/sgalich/gists{/gist_id}", "starred_url": "https://api.github.com/users/sgalich/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sgalich/subscriptions", "organizations_url": "https://api.github.com/users/sgalich/orgs", "repos_url": "https://api.github.com/users/sgalich/repos", "events_url": "https://api.github.com/users/sgalich/events{/privacy}", "received_events_url": "https://api.github.com/users/sgalich/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-27T16:23:14Z", "updated_at": "2019-08-10T12:47:31Z", "closed_at": "2019-08-10T12:42:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "In Splash browser my lua script works fine. It must be executed in several seconds due to the some \"splash:wait(1)\" lines. But SplachRequest yields response with url's html.text as soon as starts.\r\n\r\nHere is my code:\r\n\r\n```\r\ndef parse(self, response):\r\n\t\t<...>\r\n    script ='''\r\nfunction main(splash)\r\nsplash.resource_timeout = 10.0\r\nassert(splash:go(splash.args.url))\r\nsplash:wait(2)\r\nlocal element = splash:select('a.phone-trigger')\r\nlocal bounds = element:bounds()\r\nelement:mouse_click{x=bounds.width/2, y=bounds.height/2}\r\nsplash:wait(2)\r\nreturn splash:html()\r\nend\r\n'''\r\n    args = {'lua_source': script,\r\n\t\t'timeout': 10,\r\n\t\t'resource_timeout': 10,\r\n\t\t'wait': 5,\r\n\t\t'har': 1,\r\n\t\t'html': 1,\r\n\t\t}\r\n    yield SplashRequest(url=buyers_link,\r\n\t\t\tcallback=self.parse_buyer,\r\n\t\t\tcb_kwargs=data,\r\n\t\t\tendpoint='execute',\r\n\t\t\targs=args,\r\n\t\t\tdont_filter = True\r\n\t\t\t)\r\ndef parse_buyer(self, response,\r\n\t                Title,\r\n\t\t\tName,\r\n\t                Contact,\r\n\t                Region\r\n\t                ):\r\n<...>\r\n```\r\n\r\nAnd no matters what is it in lua script, SplashRequest always does the same. Even if I write:\r\n`script = \"\"\"bla-bla-bla\"\"\"`\r\nI did everything exectly with documentation: run Docker, install all required packages, make these project settings:\r\n```SPLASH_URL = 'http://localhost:8050'\r\nDOWNLOADER_MIDDLEWARES = {\r\n                        'scrapy_splash.SplashCookiesMiddleware': 723,\r\n                        'scrapy_splash.SplashMiddleware': 725,\r\n                        'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810\r\n                       }\r\nSPIDER_MIDDLEWARES = {\r\n                        'scrapy_splash.SplashDeduplicateArgsMiddleware': 100\r\n                    }\r\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\r\nHTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\r\n```\r\n\r\nBut it works like there is no splash at all! Please, help me in solving this problem!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/229", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/229/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/229/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/229/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/229", "id": 461031290, "node_id": "MDU6SXNzdWU0NjEwMzEyOTA=", "number": 229, "title": "Cant get any response with Scrapy+Splash but with Pure Scrapy I get response", "user": {"login": "kadenn", "id": 20323479, "node_id": "MDQ6VXNlcjIwMzIzNDc5", "avatar_url": "https://avatars3.githubusercontent.com/u/20323479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kadenn", "html_url": "https://github.com/kadenn", "followers_url": "https://api.github.com/users/kadenn/followers", "following_url": "https://api.github.com/users/kadenn/following{/other_user}", "gists_url": "https://api.github.com/users/kadenn/gists{/gist_id}", "starred_url": "https://api.github.com/users/kadenn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kadenn/subscriptions", "organizations_url": "https://api.github.com/users/kadenn/orgs", "repos_url": "https://api.github.com/users/kadenn/repos", "events_url": "https://api.github.com/users/kadenn/events{/privacy}", "received_events_url": "https://api.github.com/users/kadenn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-26T15:13:05Z", "updated_at": "2019-07-10T06:52:55Z", "closed_at": "2019-07-10T06:52:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried \"ROBOTSTXT_OBEY = False\", increasing timeout and \"http://localhost:8050\" but didn't work with Scrapy+Splash. Then I tried Pure Scrapy Request with same code and same settings and It worked. Pure Scarpy isn't enough itself for me. I need your help to understand why splash doesn't work with that website and how to fix it. The website Im talking about: https://www.asos.com/ ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/228", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/228/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/228/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/228/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/228", "id": 459677710, "node_id": "MDU6SXNzdWU0NTk2Nzc3MTA=", "number": 228, "title": "endpoint 'execute' returns 'TextResponse' not 'SplashJsonResponse'", "user": {"login": "kimjuny", "id": 9837872, "node_id": "MDQ6VXNlcjk4Mzc4NzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/9837872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kimjuny", "html_url": "https://github.com/kimjuny", "followers_url": "https://api.github.com/users/kimjuny/followers", "following_url": "https://api.github.com/users/kimjuny/following{/other_user}", "gists_url": "https://api.github.com/users/kimjuny/gists{/gist_id}", "starred_url": "https://api.github.com/users/kimjuny/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kimjuny/subscriptions", "organizations_url": "https://api.github.com/users/kimjuny/orgs", "repos_url": "https://api.github.com/users/kimjuny/repos", "events_url": "https://api.github.com/users/kimjuny/events{/privacy}", "received_events_url": "https://api.github.com/users/kimjuny/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-24T04:09:46Z", "updated_at": "2019-06-24T06:09:48Z", "closed_at": "2019-06-24T06:09:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "#### settings\r\n\r\n```python\r\n# -*- coding: utf-8 -*-\r\n\r\n# Scrapy settings for crawler project\r\n#\r\n# For simplicity, this file contains only settings considered important or\r\n# commonly used. You can find more settings consulting the documentation:\r\n#\r\n#     https://doc.scrapy.org/en/latest/topics/settings.html\r\n#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\r\n#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\r\n\r\nBOT_NAME = 'crawler'\r\n\r\nSPIDER_MODULES = ['crawler.spiders']\r\nNEWSPIDER_MODULE = 'crawler.spiders'\r\n\r\nDUPEFILTER_CLASS = 'scrapyjs.SplashAwareDupeFilter'\r\n\r\nSPLASH_URL = 'http://localhost:8050'\r\n\r\n# Obey robots.txt rules\r\nROBOTSTXT_OBEY = True\r\n\r\n# Enable or disable downloader middlewares\r\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\r\nDOWNLOADER_MIDDLEWARES = {\r\n   'scrapyjs.SplashMiddleware': 725,\r\n}\r\n\r\n# Configure item pipelines\r\n# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\r\nITEM_PIPELINES = {\r\n   'crawler.pipelines.CrawlerPipeline': 300,\r\n}\r\n\r\n```\r\n\r\n#### spider\r\n\r\n```python\r\nscript = \"\"\"\r\nfunction main(splash)\r\n  assert(splash:wait(1))\r\n  return {\r\n      url = splash:url(),\r\n      html = splash:html(),\r\n  }\r\nend\r\n\"\"\"\r\n\r\nclass TestSpider(scrapy.Spider):\r\n  name = 'test'\r\n  # ...\r\n  def start_request(self):\r\n    yield SplashRequest(\r\n      url = url,\r\n      callback = self.parse,\r\n      endpoint = 'execute',\r\n      dont_process_response = False,\r\n      args = { 'lua_source': script }\r\n    )\r\n\r\n  def parse(self, response):\r\n    print(response.data) # AttributeError: 'TextResponse' object has no attribute 'data'\r\n```\r\n\r\nI don't understand here, isn't `response` supposed to be `SplashJsonResponse`?\r\n\r\nIs that my env problem?\r\n\r\n#### pip3 list\r\n```\r\nScrapy               1.6.0   \r\nscrapy-splash        0.7.2   \r\nscrapyjs             0.2 \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/227", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/227/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/227/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/227/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/227", "id": 450344694, "node_id": "MDU6SXNzdWU0NTAzNDQ2OTQ=", "number": 227, "title": "splash.args.cookies did not work anymore", "user": {"login": "chems-eddin", "id": 19524261, "node_id": "MDQ6VXNlcjE5NTI0MjYx", "avatar_url": "https://avatars2.githubusercontent.com/u/19524261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chems-eddin", "html_url": "https://github.com/chems-eddin", "followers_url": "https://api.github.com/users/chems-eddin/followers", "following_url": "https://api.github.com/users/chems-eddin/following{/other_user}", "gists_url": "https://api.github.com/users/chems-eddin/gists{/gist_id}", "starred_url": "https://api.github.com/users/chems-eddin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chems-eddin/subscriptions", "organizations_url": "https://api.github.com/users/chems-eddin/orgs", "repos_url": "https://api.github.com/users/chems-eddin/repos", "events_url": "https://api.github.com/users/chems-eddin/events{/privacy}", "received_events_url": "https://api.github.com/users/chems-eddin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1690347378, "node_id": "MDU6TGFiZWwxNjkwMzQ3Mzc4", "url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/labels/needs%20more%20info", "name": "needs more info", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-30T14:46:21Z", "updated_at": "2020-01-07T16:13:12Z", "closed_at": "2020-01-07T16:13:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi everyone,\r\n\r\nwhen i try to use splash:init_cookies(splash.args.cookies), i get a error, and when i return splash.args.cookies i get nothing, can anyone explain this issue please.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/226", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/226/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/226/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/226/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/226", "id": 447470790, "node_id": "MDU6SXNzdWU0NDc0NzA3OTA=", "number": 226, "title": "Why does SplashMiddleware expose my spider?", "user": {"login": "internalG", "id": 834419, "node_id": "MDQ6VXNlcjgzNDQxOQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/834419?v=4", "gravatar_id": "", "url": "https://api.github.com/users/internalG", "html_url": "https://github.com/internalG", "followers_url": "https://api.github.com/users/internalG/followers", "following_url": "https://api.github.com/users/internalG/following{/other_user}", "gists_url": "https://api.github.com/users/internalG/gists{/gist_id}", "starred_url": "https://api.github.com/users/internalG/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/internalG/subscriptions", "organizations_url": "https://api.github.com/users/internalG/orgs", "repos_url": "https://api.github.com/users/internalG/repos", "events_url": "https://api.github.com/users/internalG/events{/privacy}", "received_events_url": "https://api.github.com/users/internalG/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-23T06:24:57Z", "updated_at": "2019-11-21T16:30:37Z", "closed_at": "2019-11-21T16:30:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a same project running on 2 machines. One has no firewall and all ports are available, the splash runs well. Another machine has the firewall, only 80 and 443 are available. The SplashRequest always returns 403 to abandon the crawl. I find I can comment out the scrapy_splash.SplashMiddleware option to get the crawl working. I want to know it's caused by SplashMiddleware or the docker behind the firewall?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/225", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/225/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/225/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/225/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/225", "id": 446158415, "node_id": "MDU6SXNzdWU0NDYxNTg0MTU=", "number": 225, "title": "LUA script doesn't return evaljs content on Ubuntu 16.04.", "user": {"login": "laranicolas", "id": 1824473, "node_id": "MDQ6VXNlcjE4MjQ0NzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1824473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laranicolas", "html_url": "https://github.com/laranicolas", "followers_url": "https://api.github.com/users/laranicolas/followers", "following_url": "https://api.github.com/users/laranicolas/following{/other_user}", "gists_url": "https://api.github.com/users/laranicolas/gists{/gist_id}", "starred_url": "https://api.github.com/users/laranicolas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laranicolas/subscriptions", "organizations_url": "https://api.github.com/users/laranicolas/orgs", "repos_url": "https://api.github.com/users/laranicolas/repos", "events_url": "https://api.github.com/users/laranicolas/events{/privacy}", "received_events_url": "https://api.github.com/users/laranicolas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-20T14:46:48Z", "updated_at": "2019-05-21T15:25:55Z", "closed_at": "2019-05-21T15:25:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to scrap the HTML and the Window object from site using Splash V.3.3.1. Locally on OSX environment it works perfectly but when I deploy to Ubuntu 16.04 production server LUA scripts return the object only with 'html' key.\r\n\r\n```\r\ndef parse(self, response):\r\n    print(response.data)\r\n```\r\n\r\nResult:\r\n\r\n```\r\n{'html': 'ALL HTML'}\r\n```\r\n\r\nScript:\r\n\r\n```\r\n    def start_requests(self):\r\n        \"\"\"Starting place for request.\"\"\"\r\n        script = \"\"\"\r\n        function main(splash, args)\r\n          assert(splash:go(args.url))\r\n          assert(splash:wait(0.5))\r\n          local data = splash:evaljs(\"window.typeOfObject\")\r\n          return {\r\n            js = data,\r\n            html = splash:html()\r\n          }\r\n        end\r\n        \"\"\"\r\n        yield SplashRequest(self.url, self.parse, endpoint='execute', args={\r\n            'wait': 1,\r\n            'proxy': 'http://localhost:24000',\r\n            'lua_source': script\r\n        })\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/224", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/224/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/224/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/224/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/224", "id": 443091915, "node_id": "MDU6SXNzdWU0NDMwOTE5MTU=", "number": 224, "title": "How to fix Lua error in scrapy_splash when running on docker?", "user": {"login": "Mageshwaran2314", "id": 16083113, "node_id": "MDQ6VXNlcjE2MDgzMTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/16083113?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mageshwaran2314", "html_url": "https://github.com/Mageshwaran2314", "followers_url": "https://api.github.com/users/Mageshwaran2314/followers", "following_url": "https://api.github.com/users/Mageshwaran2314/following{/other_user}", "gists_url": "https://api.github.com/users/Mageshwaran2314/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mageshwaran2314/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mageshwaran2314/subscriptions", "organizations_url": "https://api.github.com/users/Mageshwaran2314/orgs", "repos_url": "https://api.github.com/users/Mageshwaran2314/repos", "events_url": "https://api.github.com/users/Mageshwaran2314/events{/privacy}", "received_events_url": "https://api.github.com/users/Mageshwaran2314/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-05-12T11:04:46Z", "updated_at": "2019-11-21T16:39:50Z", "closed_at": "2019-11-21T16:39:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "I set up docker image for scrapy splash and I test it with many urls, after some times I got an error when try the url (https://www.udemy.com/)\r\n\r\nHTTP Error 400 (Bad Request)\r\nType: ScriptError -> LUA_ERROR\r\n```\r\n\r\nError happened while executing Lua script\r\n\r\nLua error: [string \"function main(splash, args) ...\"]:2: http403\r\n\r\n\r\n{\r\n    \"type\": \"ScriptError\",\r\n    \"info\": {\r\n        \"message\": \"Lua error: [string \\\"function main(splash, args)\\r...\\\"]:2: http403\",\r\n        \"type\": \"LUA_ERROR\",\r\n        \"source\": \"[string \\\"function main(splash, args)\\r...\\\"]\",\r\n        \"line_number\": 2,\r\n        \"error\": \"http403\"\r\n    },\r\n    \"error\": 400,\r\n    \"description\": \"Error happened while executing Lua script\"\r\n}\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/221", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/221/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/221/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/221/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/221", "id": 437314436, "node_id": "MDU6SXNzdWU0MzczMTQ0MzY=", "number": 221, "title": "Run splash using docker compose with args like --max-timeout  --disable-lua-sandbox", "user": {"login": "largomath2010", "id": 35832276, "node_id": "MDQ6VXNlcjM1ODMyMjc2", "avatar_url": "https://avatars3.githubusercontent.com/u/35832276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/largomath2010", "html_url": "https://github.com/largomath2010", "followers_url": "https://api.github.com/users/largomath2010/followers", "following_url": "https://api.github.com/users/largomath2010/following{/other_user}", "gists_url": "https://api.github.com/users/largomath2010/gists{/gist_id}", "starred_url": "https://api.github.com/users/largomath2010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/largomath2010/subscriptions", "organizations_url": "https://api.github.com/users/largomath2010/orgs", "repos_url": "https://api.github.com/users/largomath2010/repos", "events_url": "https://api.github.com/users/largomath2010/events{/privacy}", "received_events_url": "https://api.github.com/users/largomath2010/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-25T17:29:11Z", "updated_at": "2019-11-21T16:18:49Z", "closed_at": "2019-11-21T16:18:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, it's normal to run splash like:\r\n\r\ndocker run -p 8050:8050 scrapinghub/splash --max-timeout 3600 --disable-lua-sandbox\r\n\r\nNow I move to docker-composer v3. How can i add those args like max-timeout, disable-lua-sandbox.\r\n\r\nMy current code that not working:\r\n\r\nsplash:\r\n    image: scrapinghub/splash\r\n    command: bash \"splash --max-timeout 3600\"\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/220", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/220/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/220/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/220/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/220", "id": 437051392, "node_id": "MDU6SXNzdWU0MzcwNTEzOTI=", "number": 220, "title": "Only get popup message content as response", "user": {"login": "kadenn", "id": 20323479, "node_id": "MDQ6VXNlcjIwMzIzNDc5", "avatar_url": "https://avatars3.githubusercontent.com/u/20323479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kadenn", "html_url": "https://github.com/kadenn", "followers_url": "https://api.github.com/users/kadenn/followers", "following_url": "https://api.github.com/users/kadenn/following{/other_user}", "gists_url": "https://api.github.com/users/kadenn/gists{/gist_id}", "starred_url": "https://api.github.com/users/kadenn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kadenn/subscriptions", "organizations_url": "https://api.github.com/users/kadenn/orgs", "repos_url": "https://api.github.com/users/kadenn/repos", "events_url": "https://api.github.com/users/kadenn/events{/privacy}", "received_events_url": "https://api.github.com/users/kadenn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-25T07:38:03Z", "updated_at": "2019-04-25T10:57:58Z", "closed_at": "2019-04-25T10:57:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Website Im trying to scrape: https://uk.tommy.com/flat-logo-espadrilles-fw0fw04007731\r\n\r\nWhat should I to reach real content of website.\r\n\r\nHTML response from splash: \r\n[uk.tommy.com.txt](https://github.com/scrapy-plugins/scrapy-splash/files/3115811/uk.tommy.com.txt)\r\n\r\n\r\n![uk tommy com](https://user-images.githubusercontent.com/20323479/56717658-d9715b80-6745-11e9-9587-dbe95dd9c607.png)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/218", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/218/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/218/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/218/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/218", "id": 435290027, "node_id": "MDU6SXNzdWU0MzUyOTAwMjc=", "number": 218, "title": "Problems rendering embedded videos", "user": {"login": "alleycat08", "id": 49795579, "node_id": "MDQ6VXNlcjQ5Nzk1NTc5", "avatar_url": "https://avatars2.githubusercontent.com/u/49795579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alleycat08", "html_url": "https://github.com/alleycat08", "followers_url": "https://api.github.com/users/alleycat08/followers", "following_url": "https://api.github.com/users/alleycat08/following{/other_user}", "gists_url": "https://api.github.com/users/alleycat08/gists{/gist_id}", "starred_url": "https://api.github.com/users/alleycat08/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alleycat08/subscriptions", "organizations_url": "https://api.github.com/users/alleycat08/orgs", "repos_url": "https://api.github.com/users/alleycat08/repos", "events_url": "https://api.github.com/users/alleycat08/events{/privacy}", "received_events_url": "https://api.github.com/users/alleycat08/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-19T20:06:58Z", "updated_at": "2019-11-21T16:18:44Z", "closed_at": "2019-11-21T16:18:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI've been having difficulties capturing screenshots of sites like Twitter. When I try, I get screenshots that have boxes that say \"We cannot play the video in this browser. Please try a different web browser.\" - see attached image. I've tried playing with enabling plugins, html5, and changing the user agent, to no success.\r\n\r\nAdditionally, when encountering embedded videos elsewhere, such as NYT, a similar problem arises where there's an empty box where there should be an embedded video.\r\n\r\nAny help is appreciated\r\n\r\n![error](https://user-images.githubusercontent.com/49795579/56442032-26c07980-62bd-11e9-88df-07fe0d4c7c10.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/216", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/216/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/216/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/216/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/216", "id": 430026477, "node_id": "MDU6SXNzdWU0MzAwMjY0Nzc=", "number": 216, "title": "splash can't render full page", "user": {"login": "duaneya", "id": 12255185, "node_id": "MDQ6VXNlcjEyMjU1MTg1", "avatar_url": "https://avatars1.githubusercontent.com/u/12255185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/duaneya", "html_url": "https://github.com/duaneya", "followers_url": "https://api.github.com/users/duaneya/followers", "following_url": "https://api.github.com/users/duaneya/following{/other_user}", "gists_url": "https://api.github.com/users/duaneya/gists{/gist_id}", "starred_url": "https://api.github.com/users/duaneya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/duaneya/subscriptions", "organizations_url": "https://api.github.com/users/duaneya/orgs", "repos_url": "https://api.github.com/users/duaneya/repos", "events_url": "https://api.github.com/users/duaneya/events{/privacy}", "received_events_url": "https://api.github.com/users/duaneya/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-04-06T11:53:58Z", "updated_at": "2019-04-06T13:52:29Z", "closed_at": "2019-04-06T13:52:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have tried to use splash(v3.3.1) to render \"https://flights.ctrip.com/itinerary/oneway/sha-ctu?date=2019-04-06\" ,my scripy is:\r\n```\r\nfunction main(splash, args)\r\n  splash.js_enabled = true\r\n  splash.images_enabled = true\r\n  splash.resource_timeout = 10\r\n  splash:set_user_agent('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/73.0.3683.75 Chrome/73.0.3683.75 Safari/537.36')\r\n  assert(splash:go(args.url))\r\n  assert(splash:wait(3))\r\n  --splash:evaljs('var q=document.documentElement.scrollTop=100000')\r\n  --splash:wait(3)\r\n  return {\r\n    html = splash:html(),\r\n    png = splash:png(),\r\n    har = splash:har(),\r\n  }\r\nend\r\n```\r\nthe web screenshot should like this:\r\n\r\n![Screenshot from 2019-04-06 19-50-23](https://user-images.githubusercontent.com/12255185/55669043-3eebce00-58a5-11e9-9713-5164051321f5.png)\r\n\r\nbut what I got is:\r\n![flights ctrip com](https://user-images.githubusercontent.com/12255185/55669049-4e6b1700-58a5-11e9-92e1-525a6838015c.png)\r\nIt seems that splash didn't run javascript,because when I disable javascript in my chrome,I got the same result.Do I have something wrong with it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/213", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/213/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/213/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/213/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/213", "id": 427041165, "node_id": "MDU6SXNzdWU0MjcwNDExNjU=", "number": 213, "title": "Concurrency is not handled properly", "user": {"login": "Erhanjinn", "id": 38042267, "node_id": "MDQ6VXNlcjM4MDQyMjY3", "avatar_url": "https://avatars0.githubusercontent.com/u/38042267?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erhanjinn", "html_url": "https://github.com/Erhanjinn", "followers_url": "https://api.github.com/users/Erhanjinn/followers", "following_url": "https://api.github.com/users/Erhanjinn/following{/other_user}", "gists_url": "https://api.github.com/users/Erhanjinn/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erhanjinn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erhanjinn/subscriptions", "organizations_url": "https://api.github.com/users/Erhanjinn/orgs", "repos_url": "https://api.github.com/users/Erhanjinn/repos", "events_url": "https://api.github.com/users/Erhanjinn/events{/privacy}", "received_events_url": "https://api.github.com/users/Erhanjinn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-03-29T15:23:23Z", "updated_at": "2019-11-21T16:29:38Z", "closed_at": "2019-11-21T16:29:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI would like to use scrapy-splash to scrape multiple sites from one domain in parallel fashion. The site uses javascript to render some things I am interested in.\r\n\r\nI do get mixed responses however. When setting\r\n\r\n`CONCURRENT_REQUESTS = 2`\r\n\r\nor even\r\n\r\n`CONCURRENT_REQUESTS = 1`\r\n\r\nthe responses get mixed and are not 100% correct. \r\n\r\nI am creating the requests as follows:\r\n\r\n```\r\nyield scrapy.Request(url,\r\n                     self.parse,\r\n                     headers={'User-Agent': self.custom_user_agent},\r\n                     meta={'splash': {'args': {'wait': 15},\r\n                           'endpoint': 'render.html',\r\n                           'slot_policy': SlotPolicy.SINGLE_SLOT})\r\n```\r\n\r\nI tried to set `slot_policy` to both `SINGLE_SLOT` or `PER_DOMAIN` and it did not help.\r\n\r\nWhat more should I set when scraping only one domain?\r\n\r\nThanks, \r\n\r\nJan", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/212", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/212/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/212/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/212/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/212", "id": 425808108, "node_id": "MDU6SXNzdWU0MjU4MDgxMDg=", "number": 212, "title": "cookie", "user": {"login": "foreverbeyoung", "id": 37017145, "node_id": "MDQ6VXNlcjM3MDE3MTQ1", "avatar_url": "https://avatars3.githubusercontent.com/u/37017145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/foreverbeyoung", "html_url": "https://github.com/foreverbeyoung", "followers_url": "https://api.github.com/users/foreverbeyoung/followers", "following_url": "https://api.github.com/users/foreverbeyoung/following{/other_user}", "gists_url": "https://api.github.com/users/foreverbeyoung/gists{/gist_id}", "starred_url": "https://api.github.com/users/foreverbeyoung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/foreverbeyoung/subscriptions", "organizations_url": "https://api.github.com/users/foreverbeyoung/orgs", "repos_url": "https://api.github.com/users/foreverbeyoung/repos", "events_url": "https://api.github.com/users/foreverbeyoung/events{/privacy}", "received_events_url": "https://api.github.com/users/foreverbeyoung/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-27T07:43:28Z", "updated_at": "2019-05-10T11:47:16Z", "closed_at": "2019-05-10T11:47:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/211", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/211/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/211/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/211/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/211", "id": 425011009, "node_id": "MDU6SXNzdWU0MjUwMTEwMDk=", "number": 211, "title": "Proxies not set per request?", "user": {"login": "stever123", "id": 24877508, "node_id": "MDQ6VXNlcjI0ODc3NTA4", "avatar_url": "https://avatars3.githubusercontent.com/u/24877508?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stever123", "html_url": "https://github.com/stever123", "followers_url": "https://api.github.com/users/stever123/followers", "following_url": "https://api.github.com/users/stever123/following{/other_user}", "gists_url": "https://api.github.com/users/stever123/gists{/gist_id}", "starred_url": "https://api.github.com/users/stever123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stever123/subscriptions", "organizations_url": "https://api.github.com/users/stever123/orgs", "repos_url": "https://api.github.com/users/stever123/repos", "events_url": "https://api.github.com/users/stever123/events{/privacy}", "received_events_url": "https://api.github.com/users/stever123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-03-25T16:55:10Z", "updated_at": "2019-11-21T16:28:29Z", "closed_at": "2019-11-21T16:28:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have been using proxies for a while (with scrapy-spash). That was a static one and the problem is now regards rotating proxies. I have the following code:\r\n\r\n```\r\nproxies = ['82.209.49.196:8080', '217.9.91.88:8080', '85.142.158.45:8080', '134.209.115.223:3128']\r\nfor i in range(0,3):\r\n   yield SplashRequest(callback = self.parse, endpoint ='execute', meta={'dont_retry' : False,}, args={'lua_source': \r\n   self.luaScripts['checkIP'], 'proxy' : 'http://' + proxies[i], \r\n   'timeout': 90}, dont_filter=True)\r\n```\r\n\r\nThis is my what is in my Lua script:\r\n```\r\nfunction main(splash, args)\r\n  assert(splash:go('https://httpbin.org/ip'))\r\n  local _linksToBeFixed = 0\r\n  return {mypng = splash:png(),}\r\nend\r\n```\r\n\r\nThe proxy used is always the first proxy specified (in this case, 82.209.49.196:8080). This seems quite strange to me. Note that `self.luaScripts['checkIP']` is a lua script that goes to https://httpbin.org/ip. \r\n\r\nWhy is it only the first proxy specified that is used in ALL your SplashRequests? How can you specify different proxies per request as with Scrapy requests (i.e. meta['proxy'])?\r\n\r\nEven `request.body` has different `proxies` (as set per request) - so this only makes it even stranger that only the one set in the first request is being used for all future SplashRequests...\r\n\r\nIt also happens if I use `splash:set_proxy` in the script.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/210", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/210/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/210/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/210/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/210", "id": 421791157, "node_id": "MDU6SXNzdWU0MjE3OTExNTc=", "number": 210, "title": "Restrict acess by username and password", "user": {"login": "ChristophKind", "id": 6160370, "node_id": "MDQ6VXNlcjYxNjAzNzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6160370?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChristophKind", "html_url": "https://github.com/ChristophKind", "followers_url": "https://api.github.com/users/ChristophKind/followers", "following_url": "https://api.github.com/users/ChristophKind/following{/other_user}", "gists_url": "https://api.github.com/users/ChristophKind/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChristophKind/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChristophKind/subscriptions", "organizations_url": "https://api.github.com/users/ChristophKind/orgs", "repos_url": "https://api.github.com/users/ChristophKind/repos", "events_url": "https://api.github.com/users/ChristophKind/events{/privacy}", "received_events_url": "https://api.github.com/users/ChristophKind/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1690316928, "node_id": "MDU6TGFiZWwxNjkwMzE2OTI4", "url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/labels/docs", "name": "docs", "color": "bfdadc", "default": false, "description": ""}, {"id": 39541876, "node_id": "MDU6TGFiZWwzOTU0MTg3Ng==", "url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-16T10:03:38Z", "updated_at": "2019-11-26T11:51:27Z", "closed_at": "2019-11-26T11:51:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to set a username and a password for the http api?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/209", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/209/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/209/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/209/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/209", "id": 416840228, "node_id": "MDU6SXNzdWU0MTY4NDAyMjg=", "number": 209, "title": "Live Webkit window OSX", "user": {"login": "ddofborg", "id": 150388, "node_id": "MDQ6VXNlcjE1MDM4OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/150388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ddofborg", "html_url": "https://github.com/ddofborg", "followers_url": "https://api.github.com/users/ddofborg/followers", "following_url": "https://api.github.com/users/ddofborg/following{/other_user}", "gists_url": "https://api.github.com/users/ddofborg/gists{/gist_id}", "starred_url": "https://api.github.com/users/ddofborg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ddofborg/subscriptions", "organizations_url": "https://api.github.com/users/ddofborg/orgs", "repos_url": "https://api.github.com/users/ddofborg/repos", "events_url": "https://api.github.com/users/ddofborg/events{/privacy}", "received_events_url": "https://api.github.com/users/ddofborg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-04T14:56:18Z", "updated_at": "2019-11-21T16:28:18Z", "closed_at": "2019-11-21T16:28:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was not able to get this running on OSX using the command on https://splash.readthedocs.io/en/stable/kernel.html#live-webkit-window Maybe they are different on OSX (and I have tried some other from the Internet), but it never worked.\r\n\r\nIs there anyone who has this working? I would be great to be able to interact with Splash while it's running.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/208", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/208/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/208/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/208/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/208", "id": 415653444, "node_id": "MDU6SXNzdWU0MTU2NTM0NDQ=", "number": 208, "title": "How to maintain session in scrapy-splash request ", "user": {"login": "Durai4024", "id": 33537349, "node_id": "MDQ6VXNlcjMzNTM3MzQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/33537349?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Durai4024", "html_url": "https://github.com/Durai4024", "followers_url": "https://api.github.com/users/Durai4024/followers", "following_url": "https://api.github.com/users/Durai4024/following{/other_user}", "gists_url": "https://api.github.com/users/Durai4024/gists{/gist_id}", "starred_url": "https://api.github.com/users/Durai4024/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Durai4024/subscriptions", "organizations_url": "https://api.github.com/users/Durai4024/orgs", "repos_url": "https://api.github.com/users/Durai4024/repos", "events_url": "https://api.github.com/users/Durai4024/events{/privacy}", "received_events_url": "https://api.github.com/users/Durai4024/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-02-28T14:35:14Z", "updated_at": "2019-11-21T16:28:14Z", "closed_at": "2019-11-21T16:28:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using scrapy-splash for website where  JavaScript is rendering properly.I want to click next page button to navigate and button click is working fine it is going 2nd page.But the problem here is when I do callback It is loading first page of the website.So I am not able to navigate 3rd,4th...further page. Please suggest the solution to handle this", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/207", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/207/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/207/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/207/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/207", "id": 413144322, "node_id": "MDU6SXNzdWU0MTMxNDQzMjI=", "number": 207, "title": "Lua script failed clicking on a button", "user": {"login": "elados93", "id": 32566733, "node_id": "MDQ6VXNlcjMyNTY2NzMz", "avatar_url": "https://avatars0.githubusercontent.com/u/32566733?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elados93", "html_url": "https://github.com/elados93", "followers_url": "https://api.github.com/users/elados93/followers", "following_url": "https://api.github.com/users/elados93/following{/other_user}", "gists_url": "https://api.github.com/users/elados93/gists{/gist_id}", "starred_url": "https://api.github.com/users/elados93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elados93/subscriptions", "organizations_url": "https://api.github.com/users/elados93/orgs", "repos_url": "https://api.github.com/users/elados93/repos", "events_url": "https://api.github.com/users/elados93/events{/privacy}", "received_events_url": "https://api.github.com/users/elados93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-02-21T22:02:37Z", "updated_at": "2019-05-11T16:22:05Z", "closed_at": "2019-05-11T16:22:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to scrape flights from [link](http://www.iaa.gov.il/en-us/airports/BenGurion/Pages/OnlineFlights.aspx) with scrapy-splash using this lua script:\r\n\r\n```\r\nfunction main(splash)\r\n                local waiting_time = 2 \r\n\r\n                -- Go to the URL\r\n                assert(splash:go(splash.args.url))\r\n                splash:wait(waiting_time)\r\n\r\n                -- Click on \"Outgoing tab\"\r\n                local outgoing_tab = splash:select('#linkRealTimeOutgoing')\r\n                outgoing_tab:mouse_click()\r\n                splash:wait(waiting_time)\r\n\r\n                -- Click on \"More Flights\" button\r\n                local more_flights_btn = splash:select('#ctl00_rptOutgoingFlights_ctl26_divPaging > div.advanced.noTop > a')\r\n                more_flights_btn:mouse_click()\r\n                splash:wait(waiting_time)\r\n\r\n                return splash:html()\r\nend\r\n```\r\nand from some reason I'm getting this error:\r\n\r\n`'LUA_ERROR', 'message': 'Lua error: [string \"...\"]:16: attempt to index local \\'more_flights_btn\\' (a nil value)', 'error': \"attempt to index local 'more_flights_btn' (a nil value)\"}, 'type': 'ScriptError', 'description': 'Error happened while executing Lua script'}`\r\nDoes anyone know why this happens? Also does anyone know where I can get a toturial for lua script integration with splash? besides the offical site?\r\n\r\nThanks in advance!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/206", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/206/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/206/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/206/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/206", "id": 408008635, "node_id": "MDU6SXNzdWU0MDgwMDg2MzU=", "number": 206, "title": "Unable to run the container in Azure Web App Service", "user": {"login": "sadiq-mc", "id": 14868332, "node_id": "MDQ6VXNlcjE0ODY4MzMy", "avatar_url": "https://avatars1.githubusercontent.com/u/14868332?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sadiq-mc", "html_url": "https://github.com/sadiq-mc", "followers_url": "https://api.github.com/users/sadiq-mc/followers", "following_url": "https://api.github.com/users/sadiq-mc/following{/other_user}", "gists_url": "https://api.github.com/users/sadiq-mc/gists{/gist_id}", "starred_url": "https://api.github.com/users/sadiq-mc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sadiq-mc/subscriptions", "organizations_url": "https://api.github.com/users/sadiq-mc/orgs", "repos_url": "https://api.github.com/users/sadiq-mc/repos", "events_url": "https://api.github.com/users/sadiq-mc/events{/privacy}", "received_events_url": "https://api.github.com/users/sadiq-mc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-02-08T05:12:10Z", "updated_at": "2019-11-21T16:39:30Z", "closed_at": "2019-11-21T16:39:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to run the Splash container in Azure Web App service by pulling from Docker Hub. Unfortunately, the app was not starting. Below is the log from the Azure service,\r\n\r\n> 2019-02-07 04:58:35.812 INFO  - Issuing docker pull: imagename =scrapinghub/splash:latest\r\n2019-02-07 04:58:35.949 INFO  - Issuing docker pull: imagename =scrapinghub/splash:latest\r\n2019-02-07 04:58:36.063 INFO  - Issuing docker pull scrapinghub/splash:latest\r\n2019-02-07 04:59:25.695 INFO  - docker pull returned STDOUT>> latest: Pulling from scrapinghub/splash\r\n7b722c1070cd: Pulling fs layer\r\n5fbf74db61f1: Pulling fs layer\r\ned41cb72e5c9: Pulling fs layer\r\n7ea47a67709e: Pulling fs layer\r\ne352f2e1f6cd: Pulling fs layer\r\nb995bfaabf1d: Pulling fs layer\r\n1ed340027368: Pulling fs layer\r\n386db981613e: Pulling fs layer\r\ne2e4f0e521c0: Pulling fs layer\r\nb995bfaabf1d: Verifying Checksum\r\nb995bfaabf1d: Download complete\r\n7ea47a67709e: Verifying Checksum\r\n7ea47a67709e: Download complete\r\n5fbf74db61f1: Verifying Checksum\r\n5fbf74db61f1: Download complete\r\ned41cb72e5c9: Verifying Checksum\r\ned41cb72e5c9: Download complete\r\ne352f2e1f6cd: Verifying Checksum\r\ne352f2e1f6cd: Download complete\r\ne2e4f0e521c0: Verifying Checksum\r\ne2e4f0e521c0: Download complete\r\n7b722c1070cd: Verifying Checksum\r\n7b722c1070cd: Download complete\r\n7b722c1070cd: Pull complete\r\n5fbf74db61f1: Pull complete\r\ned41cb72e5c9: Pull complete\r\n386db981613e: Verifying Checksum\r\n386db981613e: Download complete\r\n7ea47a67709e: Pull complete\r\n1ed340027368: Verifying Checksum\r\n1ed340027368: Download complete\r\ne352f2e1f6cd: Pull complete\r\nb995bfaabf1d: Pull complete\r\n  \r\n> 2019-02-07 04:59:25.696 ERROR - docker pull returned STDERR>> failed to register layer: Error processing tar file(exit status 1): Container ID 834600147 cannot be mapped to a host ID\r\n\r\n> 2019-02-07 04:59:25.698 INFO  - Starting container for site\r\n2019-02-07 04:59:25.699 INFO  - docker run -d -p 47498:8050 --name mc-splash_0 -e PORT=8050 -e WEBSITES_ENABLE_APP_SERVICE_STORAGE=false -e WEBSITE_SITE_NAME=mc-splash -e WEBSITE_AUTH_ENABLED=False -e WEBSITE_ROLE_INSTANCE_ID=0 -e WEBSITE_INSTANCE_ID=d0fd332e2e657cc5a5784e0cd0ebc3d6a210cb33ba151113cbc811b1e894878 -e HTTP_LOGGING_ENABLED=1 scrapinghub/splash:latest  \r\n\r\n> 2019-02-07 04:59:25.833 ERROR - Container create failed for mc-splash_0 with System.AggregateException, One or more errors occurred.\r\nInnerException: Docker.DotNet.DockerContainerNotFoundException, Docker API responded with status code=NotFound, response={\"message\":\"No such image: scrapinghub/splash:latest\"}\r\n\r\nAny help is appreciated.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/205", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/205/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/205/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/205/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/205", "id": 401369628, "node_id": "MDU6SXNzdWU0MDEzNjk2Mjg=", "number": 205, "title": "Splash Requested URL vs Real URL", "user": {"login": "civanescu", "id": 16117463, "node_id": "MDQ6VXNlcjE2MTE3NDYz", "avatar_url": "https://avatars2.githubusercontent.com/u/16117463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/civanescu", "html_url": "https://github.com/civanescu", "followers_url": "https://api.github.com/users/civanescu/followers", "following_url": "https://api.github.com/users/civanescu/following{/other_user}", "gists_url": "https://api.github.com/users/civanescu/gists{/gist_id}", "starred_url": "https://api.github.com/users/civanescu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/civanescu/subscriptions", "organizations_url": "https://api.github.com/users/civanescu/orgs", "repos_url": "https://api.github.com/users/civanescu/repos", "events_url": "https://api.github.com/users/civanescu/events{/privacy}", "received_events_url": "https://api.github.com/users/civanescu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-21T14:08:40Z", "updated_at": "2019-11-21T16:28:10Z", "closed_at": "2019-11-21T16:28:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI'm trying to obtain in Python the \"last\" url form a requested url.\r\nOn splash site, here is render.json for google.com: {\"requestedUrl\": \"http://www.google.com/\", \"url\": \"https://www.google.com/?gws_rd=ssl\", \"title\": \"Google\", \"geometry\": [0, 0, 1024, 768]}\r\n\r\nI couldn't obtain the same result using the scrapy_splash module.\r\nIf I ask for:\r\n            'requested_url': response.data['requestedUrl'],\r\n            'real_url': response.data['url'],\r\n            'splash_url': response.real_url\r\n\r\nI only obtain\r\n{\"requested_url\": \"http://www.google.com/\", \"real_url\": \"http://www.google.com/\", \"splash_url\": \"http://127.0.0.1:8050/render.json\"}\r\n\r\nIs there a method, parameter to see the last url?\r\n\r\nThank you", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/204", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/204/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/204/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/204/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/204", "id": 400979351, "node_id": "MDU6SXNzdWU0MDA5NzkzNTE=", "number": 204, "title": "How to send new request on rendered page after splashrequest?", "user": {"login": "justinleeone", "id": 46787446, "node_id": "MDQ6VXNlcjQ2Nzg3NDQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/46787446?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justinleeone", "html_url": "https://github.com/justinleeone", "followers_url": "https://api.github.com/users/justinleeone/followers", "following_url": "https://api.github.com/users/justinleeone/following{/other_user}", "gists_url": "https://api.github.com/users/justinleeone/gists{/gist_id}", "starred_url": "https://api.github.com/users/justinleeone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justinleeone/subscriptions", "organizations_url": "https://api.github.com/users/justinleeone/orgs", "repos_url": "https://api.github.com/users/justinleeone/repos", "events_url": "https://api.github.com/users/justinleeone/events{/privacy}", "received_events_url": "https://api.github.com/users/justinleeone/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-19T09:39:13Z", "updated_at": "2019-11-21T16:27:45Z", "closed_at": "2019-11-21T16:27:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "HI,\r\nI have spider project, need input username and password. after that I must wait verification code from e-mail to finish login.\r\nIn this  case,  I input login's information through splashrequest first time.When I get verification code, send code through splashrequest second. However, response is not login page.\r\nSo, have any way to go back login page that first time splashrequest? I mean that I want to find fist time splashrequest's rendered page, and then I can send new js code.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/203", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/203/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/203/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/203/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/203", "id": 397633318, "node_id": "MDU6SXNzdWUzOTc2MzMzMTg=", "number": 203, "title": "Third party plugin used in website are not run", "user": {"login": "bikashburma615", "id": 10557387, "node_id": "MDQ6VXNlcjEwNTU3Mzg3", "avatar_url": "https://avatars1.githubusercontent.com/u/10557387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bikashburma615", "html_url": "https://github.com/bikashburma615", "followers_url": "https://api.github.com/users/bikashburma615/followers", "following_url": "https://api.github.com/users/bikashburma615/following{/other_user}", "gists_url": "https://api.github.com/users/bikashburma615/gists{/gist_id}", "starred_url": "https://api.github.com/users/bikashburma615/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bikashburma615/subscriptions", "organizations_url": "https://api.github.com/users/bikashburma615/orgs", "repos_url": "https://api.github.com/users/bikashburma615/repos", "events_url": "https://api.github.com/users/bikashburma615/events{/privacy}", "received_events_url": "https://api.github.com/users/bikashburma615/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-01-10T01:19:26Z", "updated_at": "2019-11-21T16:27:40Z", "closed_at": "2019-11-21T16:27:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Example: \r\n\r\nlet take a site:\r\n\r\nhttps://www.chubbiesshorts.com/products/the-arnolds\r\n\r\nif inspect the website, and search for \"yotpo-main-widget\" class, the dynamic html content generated by this yotpo is not not rendered. Can any one help me?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/202", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/202/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/202/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/202/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/202", "id": 394479469, "node_id": "MDU6SXNzdWUzOTQ0Nzk0Njk=", "number": 202, "title": "Get compressed bodysize of file", "user": {"login": "Tobias-Keller", "id": 39527519, "node_id": "MDQ6VXNlcjM5NTI3NTE5", "avatar_url": "https://avatars1.githubusercontent.com/u/39527519?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tobias-Keller", "html_url": "https://github.com/Tobias-Keller", "followers_url": "https://api.github.com/users/Tobias-Keller/followers", "following_url": "https://api.github.com/users/Tobias-Keller/following{/other_user}", "gists_url": "https://api.github.com/users/Tobias-Keller/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tobias-Keller/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tobias-Keller/subscriptions", "organizations_url": "https://api.github.com/users/Tobias-Keller/orgs", "repos_url": "https://api.github.com/users/Tobias-Keller/repos", "events_url": "https://api.github.com/users/Tobias-Keller/events{/privacy}", "received_events_url": "https://api.github.com/users/Tobias-Keller/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-12-27T20:13:33Z", "updated_at": "2019-11-21T16:27:24Z", "closed_at": "2019-11-21T16:27:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there a way to get the compressed bodysize of the requestet files in .har?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/200", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/200/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/200/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/200/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/200", "id": 384091667, "node_id": "MDU6SXNzdWUzODQwOTE2Njc=", "number": 200, "title": "crawlera integration in scrapy splash", "user": {"login": "spokharel41", "id": 12790944, "node_id": "MDQ6VXNlcjEyNzkwOTQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/12790944?v=4", "gravatar_id": "", "url": "https://api.github.com/users/spokharel41", "html_url": "https://github.com/spokharel41", "followers_url": "https://api.github.com/users/spokharel41/followers", "following_url": "https://api.github.com/users/spokharel41/following{/other_user}", "gists_url": "https://api.github.com/users/spokharel41/gists{/gist_id}", "starred_url": "https://api.github.com/users/spokharel41/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/spokharel41/subscriptions", "organizations_url": "https://api.github.com/users/spokharel41/orgs", "repos_url": "https://api.github.com/users/spokharel41/repos", "events_url": "https://api.github.com/users/spokharel41/events{/privacy}", "received_events_url": "https://api.github.com/users/spokharel41/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-11-25T15:56:01Z", "updated_at": "2019-11-21T16:28:39Z", "closed_at": "2019-11-21T16:28:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "![error](https://user-images.githubusercontent.com/12790944/48981233-0d931280-f0fb-11e8-8c92-f6e72634b767.png)\r\nI'm getting this error while integrating scrapy splash with crawlera.\r\nMy spider code is :\r\nyield SplashRequest(url = final_url, callback = self.parse, endpoint = 'render.html', splash_headers = {'Authorization':basic_auth_header(self.settings['my splash api key'], ''),}, args = {'lua_source':self.LUA_SOURCE, 'crawlera_user':self.settings['my crawlera api key'],}, cache_args = {'lua_source'}, meta = {'some_value': some_value})\r\n\r\nscripts/crawlera.lua , settings and setup.py are similar as the official docs.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/199", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/199/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/199/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/199/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/199", "id": 383207045, "node_id": "MDU6SXNzdWUzODMyMDcwNDU=", "number": 199, "title": "JavaScript consoloe output?", "user": {"login": "Simoron", "id": 1523790, "node_id": "MDQ6VXNlcjE1MjM3OTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1523790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Simoron", "html_url": "https://github.com/Simoron", "followers_url": "https://api.github.com/users/Simoron/followers", "following_url": "https://api.github.com/users/Simoron/following{/other_user}", "gists_url": "https://api.github.com/users/Simoron/gists{/gist_id}", "starred_url": "https://api.github.com/users/Simoron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Simoron/subscriptions", "organizations_url": "https://api.github.com/users/Simoron/orgs", "repos_url": "https://api.github.com/users/Simoron/repos", "events_url": "https://api.github.com/users/Simoron/events{/privacy}", "received_events_url": "https://api.github.com/users/Simoron/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-21T16:47:08Z", "updated_at": "2019-11-21T16:27:21Z", "closed_at": "2019-11-21T16:27:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, set \"console\":1 \r\n\r\nWhere is result place?  responce.data[\"?\"]", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/198", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/198/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/198/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/198/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/198", "id": 382984526, "node_id": "MDU6SXNzdWUzODI5ODQ1MjY=", "number": 198, "title": "Why two difference URL randomly return one of the same pages on Splash?", "user": {"login": "p0we7", "id": 8407573, "node_id": "MDQ6VXNlcjg0MDc1NzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/8407573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/p0we7", "html_url": "https://github.com/p0we7", "followers_url": "https://api.github.com/users/p0we7/followers", "following_url": "https://api.github.com/users/p0we7/following{/other_user}", "gists_url": "https://api.github.com/users/p0we7/gists{/gist_id}", "starred_url": "https://api.github.com/users/p0we7/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/p0we7/subscriptions", "organizations_url": "https://api.github.com/users/p0we7/orgs", "repos_url": "https://api.github.com/users/p0we7/repos", "events_url": "https://api.github.com/users/p0we7/events{/privacy}", "received_events_url": "https://api.github.com/users/p0we7/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-21T07:10:10Z", "updated_at": "2020-07-26T04:51:43Z", "closed_at": "2018-11-24T06:05:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "I save the page to ASN_detail.html & SEARCH_detail.html, \r\nIf the same page is returned then the two html file are the same.\r\nI have no idea why this happens, have any one can fix this problem?\r\n\r\n\r\n## MySpider\r\n```\r\n# -*- coding: utf-8 -*-\r\nimport scrapy\r\nfrom scrapy_splash import SplashRequest\r\n\r\nlua_script = \"\"\"\r\nfunction main(splash)\r\n  --splash:set_user_agent(splash.args.ua)\r\n  assert(splash:go(splash.args.url))\r\n\r\n  -- requires Splash 2.3  \r\n  while not splash:select('#search') and not splash:select('#whois') do\r\n    splash:wait(1)\r\n  end\r\n   return {\r\n    html = splash:html(),\r\n    cookies = splash:get_cookies(),\r\n  }\r\nend\r\n\"\"\"\r\nclass BgpspiderSpider(scrapy.Spider):\r\n    name = 'BGPSpider'\r\n    allowed_domains = ['bgp.he.net']\r\n    urls = [\r\n        'https://bgp.he.net/AS395354',\r\n        'https://bgp.he.net/search?search%5Bsearch%5D=starry&commit=Search'\r\n        ]\r\n\r\n    cookies = []\r\n    def start_requests(self):\r\n            yield SplashRequest(\r\n                url=self.urls[1], \r\n                callback=self.parse_search,\r\n                endpoint='execute',\r\n                args={\r\n                    'lua_source': lua_script\r\n                }\r\n            )\r\n            yield SplashRequest(\r\n                url=self.urls[0], \r\n                callback=self.parse,\r\n                endpoint='execute',\r\n                args={\r\n                    'lua_source': lua_script\r\n                }\r\n            )\r\n\r\n \r\n    def parse(self, response):\r\n        # 9 item\r\n        next_request = []\r\n\r\n        self.logger.debug('on parse ASN, URL %s', response.url)\r\n        \r\n        f = open('ASN_detail.html', 'w')\r\n        f.write(response.body.decode('utf-8'))\r\n        f.close()\r\n\r\n        if not self.cookies:\r\n            self.cookies = response.data['cookies']\r\n        \r\n        as_number = response.xpath('//h1/a/text()').re(r'AS\\d+')[0]\r\n        table = response.xpath('//*[@id=\"table_prefixes4\"]/tbody/tr')\r\n                \r\n        for row in table:\r\n            next_request.append(response.urljoin(row.xpath('./td[1]/a/@href').extract_first()))\r\n        \r\n        self.logger.debug('Have %d domain item need crawl on AS', len(next_request))\r\n        \r\n\r\n    def parse_search(self, response):\r\n        # 13 item\r\n        \r\n        next_request = []\r\n        self.logger.debug('on parse SEARCH, URL %s', response.url)\r\n        \r\n        companys = ['Starry, Inc.']\r\n        table = response.xpath('//*[@id=\"search\"]/table/tbody/tr')\r\n        self.logger.debug(table)\r\n\r\n        f = open('SEARCH_detail.html', 'w')\r\n        f.write(response.body.decode('utf-8'))\r\n        f.close()\r\n        for row in table:\r\n            \r\n            ip = row.xpath('./td[1]/a/text()').extract_first()\r\n            company_name = row.xpath('./td[2]/text()').extract_first()\r\n\r\n            try:\r\n                if company_name in companys:\r\n                    self.logger.debug('Company name is %s', company_name)\r\n                    next_request.append(response.urljoin(row.xpath('./td[1]/a/@href').extract_first()))\r\n                else:\r\n                    continue\r\n            except Exception as e:\r\n                self.logger.debug(e)\r\n                continue\r\n\r\n        self.logger.debug('Have %d domain item need crawl on SEARCH', len(next_request))\r\n```\r\n\r\n# Spider Setting\r\n```\r\n# -*- coding: utf-8 -*-\r\n\r\n# Scrapy settings for bgp_bots project\r\n\r\nBOT_NAME = 'bgp_bots'\r\n\r\nSPIDER_MODULES = ['bgp_bots.spiders']\r\nNEWSPIDER_MODULE = 'bgp_bots.spiders'\r\n\r\nSPLASH_URL = 'http://192.168.99.100:32788'\r\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\r\nUSER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'\r\nROBOTSTXT_OBEY = True\r\n\r\nSPIDER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\r\n}\r\n\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashCookiesMiddleware': 723,\r\n    'scrapy_splash.SplashMiddleware': 725,\r\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\r\n}\r\n\r\n```\r\n\r\n## Normal Log\r\n```\r\n(bgp_check) \u279c  bgp_bots scrapy crawl BGPSpider -o BGP.json\r\n2018-11-21 14:50:39 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: bgp_bots)\r\n2018-11-21 14:50:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (default, Jun 24 2018, 19:04:05) - [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\r\n2018-11-21 14:50:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'bgp_bots', 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'FEED_FORMAT': 'json', 'FEED_URI': 'BGP.json', 'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage', 'NEWSPIDER_MODULE': 'bgp_bots.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['bgp_bots.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\r\n2018-11-21 14:50:39 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.feedexport.FeedExporter',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2018-11-21 14:50:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy_splash.SplashCookiesMiddleware',\r\n 'scrapy_splash.SplashMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2018-11-21 14:50:39 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy_splash.SplashDeduplicateArgsMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2018-11-21 14:50:39 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2018-11-21 14:50:39 [scrapy.core.engine] INFO: Spider opened\r\n2018-11-21 14:50:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2018-11-21 14:50:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6035\r\n2018-11-21 14:50:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://bgp.he.net/robots.txt> (referer: None)\r\n2018-11-21 14:50:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://192.168.99.100:32788/robots.txt> (referer: None)\r\n2018-11-21 14:50:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://bgp.he.net/AS395354 via http://192.168.99.100:32788/execute> (referer: None)\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: on parse ASN, URL https://bgp.he.net/AS395354\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Have 9 domain item need crawl on AS\r\n2018-11-21 14:50:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://bgp.he.net/search?search%5Bsearch%5D=starry&commit=Search via http://192.168.99.100:32788/execute> (referer: None)\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: on parse SEARCH, URL https://bgp.he.net/search?search%5Bsearch%5D=starry&commit=Search\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: [<Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr><td><a href=\"/dns/starry\" title=\"sta'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/AS395'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/AS276'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/AS134'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/8.3.88'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/8.3.87'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/8.3.86'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/8.3.84'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/8.3.80'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/64.18.'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr'data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/64.18.'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/64.18.'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/45.125'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/45.125'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/45.125'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/45.120'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/45.120'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/45.120'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/45.120'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/43.241'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2a0a:6'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2607:7'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2607:7'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/2403:a'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/23.236'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/23.236'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/23.236'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/205.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/202.16'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/202.16'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/202.16'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/202.16'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/185.17'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/185.17'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/185.17'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/122.12'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/122.12'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/122.12'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.56'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.56'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.56'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.27'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.27'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.27'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.20'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.19'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.19'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.19'>, <Selector xpath='//*[@id=\"search\"]/table/tbody/tr' data='<tr>\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t<td><a href=\"/net/103.19'>]\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Company name is Starry, Inc.\r\n2018-11-21 14:50:45 [BGPSpider] DEBUG: Have 13 domain item need crawl on SEARCH\r\n2018-11-21 14:50:45 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2018-11-21 14:50:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 2772,\r\n 'downloader/request_count': 4,\r\n 'downloader/request_method_count/GET': 2,\r\n 'downloader/request_method_count/POST': 2,\r\n 'downloader/response_bytes': 129504,\r\n 'downloader/response_count': 4,\r\n 'downloader/response_status_count/200': 3,\r\n 'downloader/response_status_count/404': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2018, 11, 21, 6, 50, 45, 932761),\r\n 'log_count/DEBUG': 23,\r\n 'log_count/INFO': 7,\r\n 'memusage/max': 50995200,\r\n 'memusage/startup': 50987008,\r\n 'response_received_count': 4,\r\n 'scheduler/dequeued': 4,\r\n 'scheduler/dequeued/memory': 4,\r\n 'scheduler/enqueued': 4,\r\n 'scheduler/enqueued/memory': 4,\r\n 'splash/execute/request_count': 2,\r\n 'splash/execute/response_count/200': 2,\r\n 'start_time': datetime.datetime(2018, 11, 21, 6, 50, 39, 469401)}\r\n2018-11-21 14:50:45 [scrapy.core.engine] INFO: Spider closed (finished)\r\n(bgp_check) \u279c  bgp_bots\r\n```\r\n\r\n## Randomly return same page\r\n```\r\n(bgp_check) \u279c  bgp_bots scrapy crawl BGPSpider -o BGP.json\r\n2018-11-21 14:51:39 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: bgp_bots)\r\n2018-11-21 14:51:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.19.0, Twisted 18.9.0, Python 3.6.5 (default, Jun 24 2018, 19:04:05) - [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Darwin-17.7.0-x86_64-i386-64bit\r\n2018-11-21 14:51:39 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'bgp_bots', 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'FEED_FORMAT': 'json', 'FEED_URI': 'BGP.json', 'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage', 'NEWSPIDER_MODULE': 'bgp_bots.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['bgp_bots.spiders'], 'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\r\n2018-11-21 14:51:39 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.memusage.MemoryUsage',\r\n 'scrapy.extensions.feedexport.FeedExporter',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2018-11-21 14:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy_splash.SplashCookiesMiddleware',\r\n 'scrapy_splash.SplashMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2018-11-21 14:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy_splash.SplashDeduplicateArgsMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2018-11-21 14:51:39 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2018-11-21 14:51:39 [scrapy.core.engine] INFO: Spider opened\r\n2018-11-21 14:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2018-11-21 14:51:39 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6035\r\n2018-11-21 14:51:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://bgp.he.net/robots.txt> (referer: None)\r\n2018-11-21 14:51:41 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://192.168.99.100:32788/robots.txt> (referer: None)\r\n2018-11-21 14:51:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://bgp.he.net/AS395354 via http://192.168.99.100:32788/execute> (referer: None)\r\n2018-11-21 14:51:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://bgp.he.net/search?search%5Bsearch%5D=starry&commit=Search via http://192.168.99.100:32788/execute> (referer: None)\r\n2018-11-21 14:51:45 [BGPSpider] DEBUG: on parse ASN, URL https://bgp.he.net/AS395354\r\n2018-11-21 14:51:45 [BGPSpider] DEBUG: Have 9 domain item need crawl on AS\r\n2018-11-21 14:51:45 [BGPSpider] DEBUG: on parse SEARCH, URL https://bgp.he.net/search?search%5Bsearch%5D=starry&commit=Search\r\n2018-11-21 14:51:45 [BGPSpider] DEBUG: []\r\n2018-11-21 14:51:45 [BGPSpider] DEBUG: Have 0 domain item need crawl on SEARCH\r\n2018-11-21 14:51:45 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2018-11-21 14:51:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 2772,\r\n 'downloader/request_count': 4,\r\n 'downloader/request_method_count/GET': 2,\r\n 'downloader/request_method_count/POST': 2,\r\n 'downloader/response_bytes': 144798,\r\n 'downloader/response_count': 4,\r\n 'downloader/response_status_count/200': 3,\r\n 'downloader/response_status_count/404': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2018, 11, 21, 6, 51, 45, 907315),\r\n 'log_count/DEBUG': 10,\r\n 'log_count/INFO': 7,\r\n 'memusage/max': 50925568,\r\n 'memusage/startup': 50921472,\r\n 'response_received_count': 4,\r\n 'scheduler/dequeued': 4,\r\n 'scheduler/dequeued/memory': 4,\r\n 'scheduler/enqueued': 4,\r\n 'scheduler/enqueued/memory': 4,\r\n 'splash/execute/request_count': 2,\r\n 'splash/execute/response_count/200': 2,\r\n 'start_time': datetime.datetime(2018, 11, 21, 6, 51, 39, 836163)}\r\n2018-11-21 14:51:45 [scrapy.core.engine] INFO: Spider closed (finished)\r\n(bgp_check) \u279c  bgp_bots\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/197", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/197/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/197/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/197/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/197", "id": 381908257, "node_id": "MDU6SXNzdWUzODE5MDgyNTc=", "number": 197, "title": "Filters - not working? ", "user": {"login": "harrisonhparker1994", "id": 43893703, "node_id": "MDQ6VXNlcjQzODkzNzAz", "avatar_url": "https://avatars0.githubusercontent.com/u/43893703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harrisonhparker1994", "html_url": "https://github.com/harrisonhparker1994", "followers_url": "https://api.github.com/users/harrisonhparker1994/followers", "following_url": "https://api.github.com/users/harrisonhparker1994/following{/other_user}", "gists_url": "https://api.github.com/users/harrisonhparker1994/gists{/gist_id}", "starred_url": "https://api.github.com/users/harrisonhparker1994/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harrisonhparker1994/subscriptions", "organizations_url": "https://api.github.com/users/harrisonhparker1994/orgs", "repos_url": "https://api.github.com/users/harrisonhparker1994/repos", "events_url": "https://api.github.com/users/harrisonhparker1994/events{/privacy}", "received_events_url": "https://api.github.com/users/harrisonhparker1994/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-17T22:41:17Z", "updated_at": "2019-11-21T16:27:16Z", "closed_at": "2019-11-21T16:27:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Mikhail, \r\n\r\nI sent you an email, but I thought I'd detail out the issue here.  Basically it doesn't seem like filters work for dockerized splash.  \r\n\r\nI basically followed the example on the main splash page about blocking [fonts ](https://splash.readthedocs.io/en/stable/api.html#request-filters).  I made the same file in the correct location.  Indeed, when splash starts up, it prints out that it is reading all the filter txt files.  \r\n\r\nHowever, I paste an html render link (will paste below) into firefox after clearing all my cache and the fonts are not blocked at all!  Is this a bug? \r\n\r\nThis is what my nofont filter looks like (I've tried many other adblock syntaxes, none work) \r\n\r\n```\r\n.ttf|\r\n.woff|\r\n```\r\nHere is the link I put into my firefox: \r\n\r\nhttp://<ip_of_splash>:8050/render.html?url=https://www.cnn.com&timeout=60&wait=5&image=0&filters=nofont\r\n\r\nAnd here's what I see when I load it in firefox: \r\n\r\n![image](https://user-images.githubusercontent.com/43893703/48666428-b2b44800-ea76-11e8-8eb9-4d684146f478.png)\r\n\r\nIt seems like the fonts still make it through.  I'd like to use the adblock filters to remove all images, CSS, fonts, etc.  Bascially all the fat except the text of the pages. \r\n\r\nLet me know what you think!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/196", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/196/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/196/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/196/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/196", "id": 380131035, "node_id": "MDU6SXNzdWUzODAxMzEwMzU=", "number": 196, "title": "scrapy-splash do not get real_url", "user": {"login": "xhochipe", "id": 9391575, "node_id": "MDQ6VXNlcjkzOTE1NzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9391575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xhochipe", "html_url": "https://github.com/xhochipe", "followers_url": "https://api.github.com/users/xhochipe/followers", "following_url": "https://api.github.com/users/xhochipe/following{/other_user}", "gists_url": "https://api.github.com/users/xhochipe/gists{/gist_id}", "starred_url": "https://api.github.com/users/xhochipe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xhochipe/subscriptions", "organizations_url": "https://api.github.com/users/xhochipe/orgs", "repos_url": "https://api.github.com/users/xhochipe/repos", "events_url": "https://api.github.com/users/xhochipe/events{/privacy}", "received_events_url": "https://api.github.com/users/xhochipe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-13T09:26:09Z", "updated_at": "2018-11-13T09:48:22Z", "closed_at": "2018-11-13T09:48:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "code:\r\n![image](https://user-images.githubusercontent.com/9391575/48403556-171a9300-e769-11e8-8e9c-30a353e56527.png)\r\n\r\n2018-11-13 17:24:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.baidu.com/link?url=bDheoIYn-YjbY3nud_oXqHX81L-UXsTBArYvUNGYTsAlEuROfpoWlQhsq11aCH9V via http://192.168.189.129:8050/render.html> (referer: None)\r\nparseBase:  http://www.baidu.com/link?url=bDheoIYn-YjbY3nud_oXqHX81L-UXsTBArYvUNGYTsAlEuROfpoWlQhsq11aCH9V\uff08real_url\uff1ahttps://weibo.com/guangxianliuyan\uff09\r\nhttp://192.168.189.129:8050/render.html", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/195", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/195/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/195/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/195/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/195", "id": 378252260, "node_id": "MDU6SXNzdWUzNzgyNTIyNjA=", "number": 195, "title": "Scraping is blocked", "user": {"login": "eliavm", "id": 994728, "node_id": "MDQ6VXNlcjk5NDcyOA==", "avatar_url": "https://avatars0.githubusercontent.com/u/994728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliavm", "html_url": "https://github.com/eliavm", "followers_url": "https://api.github.com/users/eliavm/followers", "following_url": "https://api.github.com/users/eliavm/following{/other_user}", "gists_url": "https://api.github.com/users/eliavm/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliavm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliavm/subscriptions", "organizations_url": "https://api.github.com/users/eliavm/orgs", "repos_url": "https://api.github.com/users/eliavm/repos", "events_url": "https://api.github.com/users/eliavm/events{/privacy}", "received_events_url": "https://api.github.com/users/eliavm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-07T11:39:44Z", "updated_at": "2019-05-12T15:35:30Z", "closed_at": "2019-05-12T15:35:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "When trying to scrape a page I'm getting empty page with one div `<div id=\"distilIdentificationBlock\"> </div>`\r\n\r\nFound something on stackoverflow that might be relevant: https://stackoverflow.com/questions/45060011/crawling-web-using-selenium-chrome-driver-but-still-blocked \r\n\r\nAny idea how to bypass this?\r\n\r\nUsing Scrapy 1.5.1 + scrapy-splash 0.7.2", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/194", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/194/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/194/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/194/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/194", "id": 377062490, "node_id": "MDU6SXNzdWUzNzcwNjI0OTA=", "number": 194, "title": "AttributeError: 'HtmlResponse' object has no attribute 'data'", "user": {"login": "JavierRuano", "id": 34353851, "node_id": "MDQ6VXNlcjM0MzUzODUx", "avatar_url": "https://avatars1.githubusercontent.com/u/34353851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JavierRuano", "html_url": "https://github.com/JavierRuano", "followers_url": "https://api.github.com/users/JavierRuano/followers", "following_url": "https://api.github.com/users/JavierRuano/following{/other_user}", "gists_url": "https://api.github.com/users/JavierRuano/gists{/gist_id}", "starred_url": "https://api.github.com/users/JavierRuano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JavierRuano/subscriptions", "organizations_url": "https://api.github.com/users/JavierRuano/orgs", "repos_url": "https://api.github.com/users/JavierRuano/repos", "events_url": "https://api.github.com/users/JavierRuano/events{/privacy}", "received_events_url": "https://api.github.com/users/JavierRuano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-03T15:05:39Z", "updated_at": "2019-11-21T16:30:26Z", "closed_at": "2019-11-21T16:30:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "yield scrapy_splash.SplashRequest(\"https://example.com\", self.parse,\r\n                                endpoint='execute',\r\n                                args={'lua_source': script, 'wait':60})\r\n\r\nLua script returns:\r\nreturn {\r\n   \r\n    html = splash:html(),\r\n    cookie = cookies\r\n\r\n}\r\n\r\nBut parse, only read response.body. I tried with render.json (not execute), iframes, response.data['cookie'] (AttributeError: 'HtmlResponse' object has no attribute 'data')\r\n\r\nI have tried to save from Lua script the data to windows filesystem, but the file doesn't appear.\r\nInside of Lua script there is redirecting, perhaps it produces differences between reponse.body and the tool from http://localhost:8050 (left html output).\r\n\r\nAnother question is the format png, it is a white page, but i have website has changed by har, could it be a not full load of javascript or redirecting again?\r\n\r\nAny solution?\r\ni see something as \r\n    \r\n    https://github.com/scrapinghub/splash/blob/master/splash/tests/test_response_tracking.py\r\n     -> resp = self.request_lua(\"\"\"\r\n\r\nI would like something similar from response, with yield scrapy_splash.SplashRequest and the callback parse\r\nSorry, if that is not a technical question.\r\n\r\nRegards.\r\nJavier Ruano.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/193", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/193/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/193/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/193/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/193", "id": 376323660, "node_id": "MDU6SXNzdWUzNzYzMjM2NjA=", "number": 193, "title": "click error", "user": {"login": "bswbatman", "id": 16164964, "node_id": "MDQ6VXNlcjE2MTY0OTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/16164964?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bswbatman", "html_url": "https://github.com/bswbatman", "followers_url": "https://api.github.com/users/bswbatman/followers", "following_url": "https://api.github.com/users/bswbatman/following{/other_user}", "gists_url": "https://api.github.com/users/bswbatman/gists{/gist_id}", "starred_url": "https://api.github.com/users/bswbatman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bswbatman/subscriptions", "organizations_url": "https://api.github.com/users/bswbatman/orgs", "repos_url": "https://api.github.com/users/bswbatman/repos", "events_url": "https://api.github.com/users/bswbatman/events{/privacy}", "received_events_url": "https://api.github.com/users/bswbatman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-01T09:52:03Z", "updated_at": "2018-11-03T01:19:27Z", "closed_at": "2018-11-03T01:19:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nIn Chrome, the buttun CSS Selector like:\r\n`#gl-pagenav > section > a.c-btn.c-btn-awr.c-p-next` \r\n(copy from Chrome)\r\n\r\n\r\nIn the SplashRequest  use Lua:\r\n`        yield SplashRequest(self.url ,callback=self.parse ,endpoint='execute' ,args={\r\n            'lua_source': open(join(self.LUA_DIR,\r\n                                    'h2.lua')).read(),\r\n            'js_source': open(join(self.JS_DIR,\r\n                                   'headless_horseman_nomal.js')).read(),\r\n            'wait': 5, 'proxy': self.proxy1['http']})\r\n`\r\n(use h2.lua)\r\n\r\nIn the h2.lua:\r\n`\r\nfunction main(splash)\r\n  local input = splash:select_all(\"#hf section a\")\r\n  input[1]:mouse_click()\r\n`\r\n(I think **select_all()** return a list, so I use **input[1]** to click)\r\n\r\nthan some error happen:\r\n**[scrapy_splash.middleware] WARNING: Bad request to Splash:\r\n {'type': 'ScriptError', \r\n'description': 'Error happened while executing Lua script',\r\n 'error': 400, \r\n'info': {'type': 'LUA_ERROR',\r\n 'message': \"Lua error: /app/splash/lua_modules/wraputils.lua:89: attempt to index local 'name' (a number value)\"}}**\r\n\r\nps : h2.lua was rename from headless.lua\r\n\r\n-------------------------------------------------\r\nAnother question is:\r\nI want use **splash:select()** but the CSS was looklike 'class=\"c-btn c-btn-awr c-p-next\"'.  Space in there, I don't know how to use `splash:select()`\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/191", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/191/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/191/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/191/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/191", "id": 366503144, "node_id": "MDU6SXNzdWUzNjY1MDMxNDQ=", "number": 191, "title": "Cannot parse splash:html() returned item", "user": {"login": "Swordyjohn", "id": 32469251, "node_id": "MDQ6VXNlcjMyNDY5MjUx", "avatar_url": "https://avatars0.githubusercontent.com/u/32469251?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Swordyjohn", "html_url": "https://github.com/Swordyjohn", "followers_url": "https://api.github.com/users/Swordyjohn/followers", "following_url": "https://api.github.com/users/Swordyjohn/following{/other_user}", "gists_url": "https://api.github.com/users/Swordyjohn/gists{/gist_id}", "starred_url": "https://api.github.com/users/Swordyjohn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Swordyjohn/subscriptions", "organizations_url": "https://api.github.com/users/Swordyjohn/orgs", "repos_url": "https://api.github.com/users/Swordyjohn/repos", "events_url": "https://api.github.com/users/Swordyjohn/events{/privacy}", "received_events_url": "https://api.github.com/users/Swordyjohn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2018-10-03T19:58:44Z", "updated_at": "2019-11-21T16:42:05Z", "closed_at": "2019-11-21T16:42:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to parse a response from a SplashRequest execute endpoint but seem to be unable to, would you be able to help me figure out what i am doing wrong? run log is attached and code below:\r\n   \r\n    import scrapy\r\n    from scrapy_splash import SplashRequest\r\n    class Kapow_crawler(scrapy.Spider):\r\n        name = \"Kapow_crawler\"\r\n\r\n        def start_requests(self):\r\n     \r\n            urls = [\r\n                \"https://vmatrix1.brevardclerk.us/beca/name_search.cfm\"\r\n            ]\r\n            for url in urls:\r\n                yield SplashRequest(url=url, callback=self.parse_first_page, endpoint='render.html')\r\n\r\n        def parse_first_page(self, response):\r\n            id_list=[]\r\n            inputs = response.css('input').extract()\r\n            input_count = 0\r\n            for inp in inputs:\r\n                input_count = input_count+1 \r\n                if 'Yes'.upper() in inp.upper():\r\n                    disc_yes = input_count -1 \r\n                    try:\r\n                        id_list.append(['id',response.css('input::attr(id)')[disc_yes].extract()])\r\n                    except Exception:\r\n                        id_list.append(['name',response.css('input::attr(name)')[disc_yes].extract()])\r\n                    if 'radio'.upper() in inp.upper():\r\n                        inputs = response.css('input').extract()\r\n                        input_count = 0\r\n                        for inp in inputs:\r\n                            input_count = input_count+1 \r\n                            if 'Submit'.upper() in inp.upper():\r\n                                disc_submit = input_count -1\r\n                                try:\r\n                                    id_list.append(['id',response.css('input::attr(id)')[disc_submit].extract()])\r\n                                except Exception:\r\n                                    id_list.append(['name',response.css('input::attr(name)')[disc_submit].extract()])\r\n                elif 'Accept'.upper() in inp.upper():\r\n                    disc_accept = input_count -1 \r\n                    try:\r\n                        id_list.append(['id',response.css('input::attr(id)')[disc_accept].extract()])\r\n                    except Exception:\r\n                        id_list.append(['name',response.css('input::attr(name)')[disc_accept].extract()])\r\n            buttons = response.css('button').extract()\r\n            button_count = 0\r\n            for button in buttons:\r\n                button_count = button_count +1\r\n                if 'Yes'.upper() in button.upper():\r\n                    disc_yes = button_count -1 \r\n                    try:\r\n                        id_list.append(['id',response.css('input::attr(id)')[disc_yes].extract()])\r\n                    except Exception:\r\n                        id_list.append(['name',response.css('input::attr(name)')[disc_yes].extract()])\r\n                elif 'Accept'.upper() in button.upper():\r\n                    disc_accept = button_count -1 \r\n                    try:\r\n                        id_list.append(['id',response.css('input::attr(id)')[disc_accept].extract()])\r\n                    except Exception:\r\n                        id_list.append(['name',response.css('input::attr(name)')[disc_accept].extract()])\r\n            if len(id_list) == 1:\r\n                disclaimer_script=\"\"\"\r\n                function main(splash)\r\n                  assert(splash:go(\"{0}\"))\r\n                  assert(splash:wait(0.5))\r\n                  local disc_accept = splash:select('[{1}=\"{2}\"]')\r\n                  assert(disc_accept:mouse_click())\r\n                  assert(splash:wait(0.5))\r\n                  return splash:html()\r\n                end\r\n               \"\"\".format(response.url,id_list[0][0],id_list[0][1])\r\n            if len(id_list) == 2:\r\n               disclaimer_script=\"\"\"\r\n               function main(splash)\r\n                 assert(splash:go(\"{0}\"))\r\n                 assert(splash:wait(0.5))\r\n                 local disc_check = splash:select('[{1}=\"{2}\"]')\r\n                 local disc_sub = splash:select('[{3}=\"{4}\"]')\r\n                 assert(disc_check:mouse_click())\r\n                 assert(disc_sub:mouse_click())\r\n                 assert(splash:wait(0.5))\r\n                 return splash:html()\r\n               end\r\n               \"\"\".format(response.url,id_list[0][0],id_list[0][1],id_list[1][0],id_list[1][1])\r\n            if len(id_list) >= 1:\r\n                yield SplashRequest(url=response.url, callback=self.parse_second_page, \r\n    endpoint='execute',args={'har':1,'html':1,'lua_source':disclaimer_script,'wait':0.5})\r\n            else:\r\n                yield SplashRequest(url=response.url, callback=self.parse_second_page, \r\n    endpoint='render.html')\r\n            \r\n    \r\n        def parse_second_page(self, response):\r\n            inputs = response.css('input').extract()\r\n            for inp in inputs:\r\n                yield print(inp) \r\n  \r\n           \r\n[log.txt](https://github.com/scrapy-plugins/scrapy-splash/files/2443602/log.txt)\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/190", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/190/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/190/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/190/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/190", "id": 364320693, "node_id": "MDU6SXNzdWUzNjQzMjA2OTM=", "number": 190, "title": "scrapy-splash with proxy", "user": {"login": "voanhq", "id": 43615742, "node_id": "MDQ6VXNlcjQzNjE1NzQy", "avatar_url": "https://avatars2.githubusercontent.com/u/43615742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/voanhq", "html_url": "https://github.com/voanhq", "followers_url": "https://api.github.com/users/voanhq/followers", "following_url": "https://api.github.com/users/voanhq/following{/other_user}", "gists_url": "https://api.github.com/users/voanhq/gists{/gist_id}", "starred_url": "https://api.github.com/users/voanhq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/voanhq/subscriptions", "organizations_url": "https://api.github.com/users/voanhq/orgs", "repos_url": "https://api.github.com/users/voanhq/repos", "events_url": "https://api.github.com/users/voanhq/events{/privacy}", "received_events_url": "https://api.github.com/users/voanhq/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-27T06:20:12Z", "updated_at": "2018-09-28T15:43:11Z", "closed_at": "2018-09-28T15:43:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello fellow scrapers. I am writing a spider to check if the proxy lua script works by printing the text from \"http://httpbin.org/ip\".\r\n\r\nThe spider is ran through splash with docker.\r\n\r\nMy list.txt file (where the proxies are located ) are written in the following format:\r\n208.38.227.219:36354\r\n51.68.162.103:5836\r\n41.170.14.26:43370\r\n78.26.154.73:58207\r\n...\r\n...\r\n\r\nMy spider is as follows: \r\n    import scrapy\r\n    from scrapy_splash import SplashRequest\r\n    import random\r\n\r\n\r\n    class ProxySpider(scrapy.Spider):\r\n        name = 'check'\r\n        f = open(\"D:/project/random-proxy/list.txt\",\"r\")\r\n        l = []\r\n        for i in f:\r\n        l.append(i)\r\n        line = [i.replace('\\n','') for i in l]\r\n        proxy = random.choice(line)\r\n        proxy_addr = \"http://\" + str(proxy)\r\n\r\n        lua_script = \"\"\"\r\n            function main(splash, args)\r\n            assert(splash:go{splash.args.url,http_method=splash.args.http_method,body=splash.args.body})\r\n            assert(splash:wait(0.5))\r\n            splash:on_request(function(request)\r\n                request:set_proxy{host = \"http://%s\", port = %s}\r\n            end)\r\n            return splash:html()\r\n            end\r\n            \"\"\" % (proxy.split(':')[0],proxy.split(':')[1])\r\n\r\n\r\n        def start_requests(self):\r\n            yield SplashRequest(url='http://httpbin.org/ip',callback=self.parse,endpoint='execute',\r\n                args={'lua_source': self.lua_script,'proxy':self.proxy_addr})\r\n     \r\n        def parse(self, response):\r\n            yield{'Text':response.text}\r\n\r\nAnd the output:\r\n\r\n    2018-09-27 13:14:32 [scrapy_splash.middleware] WARNING: Bad request to Splash: {'type': \r\n    'ScriptError', \r\n    'description': 'Error happened while executing Lua script', 'info': {'line_number': 3, 'type': 'LUA_ERROR', \r\n    'error': 'network1', 'message': 'Lua error: [string \"...\"]:3: network1', 'source': '[string \"...\"]'}, 'error': 400}\r\n    2018-09-27 13:14:32 [scrapy.core.engine] DEBUG: Crawled (400) <GET http://httpbin.org/ip via \r\n    http://10.0.75.1:8050/execute> (referer: None)\r\n    2018-09-27 13:14:32 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <400 \r\n    http://httpbin.org/ip>: HTTP status code is not handled or not allowed\r\n    2018-09-27 13:14:32 [scrapy.core.engine] INFO: Closing spider (finished)\r\n\r\n\r\nPlease advise me on what the problems is. As I am relatively new to scraping, I hope you can provide me with answers that are easy to understand. Many thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/189", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/189/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/189/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/189/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/189", "id": 362469809, "node_id": "MDU6SXNzdWUzNjI0Njk4MDk=", "number": 189, "title": "How to scrape angularjs website by using splash ?", "user": {"login": "Baddsha", "id": 43463354, "node_id": "MDQ6VXNlcjQzNDYzMzU0", "avatar_url": "https://avatars3.githubusercontent.com/u/43463354?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Baddsha", "html_url": "https://github.com/Baddsha", "followers_url": "https://api.github.com/users/Baddsha/followers", "following_url": "https://api.github.com/users/Baddsha/following{/other_user}", "gists_url": "https://api.github.com/users/Baddsha/gists{/gist_id}", "starred_url": "https://api.github.com/users/Baddsha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Baddsha/subscriptions", "organizations_url": "https://api.github.com/users/Baddsha/orgs", "repos_url": "https://api.github.com/users/Baddsha/repos", "events_url": "https://api.github.com/users/Baddsha/events{/privacy}", "received_events_url": "https://api.github.com/users/Baddsha/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-21T06:20:14Z", "updated_at": "2019-11-21T16:26:28Z", "closed_at": "2019-11-21T16:26:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "i am facing an issue to scrape the data in an angularjs website. In that when ever login to page unable to post the data to next URL ,Because in that website ng-route for making that website as single page application.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/187", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/187/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/187/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/187/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/187", "id": 354260233, "node_id": "MDU6SXNzdWUzNTQyNjAyMzM=", "number": 187, "title": "splash set proxy failed. ", "user": {"login": "Rockyzsu", "id": 7868260, "node_id": "MDQ6VXNlcjc4NjgyNjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7868260?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rockyzsu", "html_url": "https://github.com/Rockyzsu", "followers_url": "https://api.github.com/users/Rockyzsu/followers", "following_url": "https://api.github.com/users/Rockyzsu/following{/other_user}", "gists_url": "https://api.github.com/users/Rockyzsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rockyzsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rockyzsu/subscriptions", "organizations_url": "https://api.github.com/users/Rockyzsu/orgs", "repos_url": "https://api.github.com/users/Rockyzsu/repos", "events_url": "https://api.github.com/users/Rockyzsu/events{/privacy}", "received_events_url": "https://api.github.com/users/Rockyzsu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-08-27T10:12:00Z", "updated_at": "2019-01-08T11:35:38Z", "closed_at": "2018-09-08T02:02:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "stuck on this issue for a day.  Thanks anyone who can help in advanced.\r\n\r\ni use scrapy-splash to check whether proxy is working.\r\n\r\non my spider file:\r\n\r\n#### demo.py #######\r\n\r\nimport scrapy\r\nfrom scrapy_splash import SplashRequest\r\n\r\n```python\r\nclass ExampleSpider(scrapy.Spider):\r\n    name = 'proxy'\r\n\r\n\r\n    def start_requests(self):\r\n        url = 'http://httpbin.org/get'\r\n        yield scrapy.Request(url)\r\n\r\n    def parse(self, response):\r\n        url='http://httpbin.org/ip'\r\n        proxy=self.get_proxy()\r\n        yield scrapy.Request(\r\n            url=url,\r\n            meta={'splash':\r\n                      {\r\n                          'args':{\r\n                              'url':url,\r\n                              'wait':5,'timeout':30,'proxy':proxy\r\n                          },\r\n                          'endpoint':'render.html'\r\n                      }},\r\n            callback=self.parse_item\r\n\r\n        )\r\n\r\n    def parse_item(self,response):\r\n        print(response.text)\r\n```\r\n\r\n##### \r\nreturn proxy was like : http://101.7.XX.XX:8888\r\n\r\non start_request only just check website my currrent ip. On parse i set proxy ip on splash, but it failed to work.\r\n\r\nerror message on scrapy output:\r\n\r\n```python\r\n2018-08-27 18:07:25 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 120.79.150.101:8081\r\n2018-08-27 18:07:25 [urllib3.connectionpool] DEBUG: http://120.79.150.101:8081 \"GET /dynamicIp/common/getDynamicIp.do HTTP/1.1\" 200 None\r\n2018-08-27 18:07:31 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://10.18.6.102:8050/render.html> (referer: None)\r\n<html><head>\r\n<meta type=\"copyright\" content=\"Copyright (C) 1996-2016 The Squid Software Foundation and contributors\">\r\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\r\n<title>ERROR: The requested URL could not be retrieved</title>\r\n<style type=\"text/css\"><!--\r\n /*\r\n * Copyright (C) 1996-2016 The Squid Software Foundation and contributors\r\n *\r\n * Squid software is distributed under GPLv2+ license and includes\r\n * contributions from numerous individuals and organizations.\r\n * Please see the COPYING and CONTRIBUTORS files for details.\r\n */\r\n\r\n/*\r\n Stylesheet for Squid Error pages\r\n Adapted from design by Free CSS Templates\r\n http://www.freecsstemplates.org\r\n Released for free under a Creative Commons Attribution 2.5 License\r\n*/\r\n\r\n/* Page basics */\r\n* {\r\n\tfont-family: verdana, sans-serif;\r\n}\r\n\r\nhtml body {\r\n\tmargin: 0;\r\n\tpadding: 0;\r\n\tbackground: #efefef;\r\n\tfont-size: 12px;\r\n\tcolor: #1e1e1e;\r\n}\r\n\r\n/* Page displayed title area */\r\n#titles {\r\n\tmargin-left: 15px;\r\n\tpadding: 10px;\r\n\tpadding-left: 100px;\r\n\tbackground: url('/squid-internal-static/icons/SN.png') no-repeat left;\r\n}\r\n\r\n/* initial title */\r\n#titles h1 {\r\n\tcolor: #000000;\r\n}\r\n#titles h2 {\r\n\tcolor: #000000;\r\n}\r\n\r\n/* special event: FTP success page titles */\r\n#titles ftpsuccess {\r\n\tbackground-color:#00ff00;\r\n\twidth:100%;\r\n}\r\n\r\n/* Page displayed body content area */\r\n#content {\r\n\tpadding: 10px;\r\n\tbackground: #ffffff;\r\n}\r\n\r\n/* General text */\r\np {\r\n}\r\n\r\n/* error brief description */\r\n#error p {\r\n}\r\n\r\n/* some data which may have caused the problem */\r\n#data {\r\n}\r\n\r\n/* the error message received from the system or other software */\r\n#sysmsg {\r\n}\r\n\r\npre {\r\n    font-family:sans-serif;\r\n}\r\n\r\n/* special event: FTP / Gopher directory listing */\r\n#dirmsg {\r\n    font-family: courier;\r\n    color: black;\r\n    font-size: 10pt;\r\n}\r\n#dirlisting {\r\n    margin-left: 2%;\r\n    margin-right: 2%;\r\n}\r\n#dirlisting tr.entry td.icon,td.filename,td.size,td.date {\r\n    border-bottom: groove;\r\n}\r\n#dirlisting td.size {\r\n    width: 50px;\r\n    text-align: right;\r\n    padding-right: 5px;\r\n}\r\n\r\n/* horizontal lines */\r\nhr {\r\n\tmargin: 0;\r\n}\r\n\r\n/* page displayed footer area */\r\n#footer {\r\n\tfont-size: 9px;\r\n\tpadding-left: 10px;\r\n}\r\n\r\n\r\nbody\r\n:lang(fa) { direction: rtl; font-size: 100%; font-family: Tahoma, Roya, sans-serif; float: right; }\r\n:lang(he) { direction: rtl; }\r\n --></style>\r\n</head><body id=\"ERR_ACCESS_DENIED\">\r\n<div id=\"titles\">\r\n<h1>ERROR</h1>\r\n<h2>The requested URL could not be retrieved</h2>\r\n</div>\r\n<hr>\r\n\r\n<div id=\"content\">\r\n<p>The following error was encountered while trying to retrieve the URL: <a href=\"http://httpbin.org/ip\">http://httpbin.org/ip</a></p>\r\n\r\n<blockquote id=\"error\">\r\n<p><b>Access Denied.</b></p>\r\n</blockquote>\r\n\r\n<p>Access control configuration prevents your request from being allowed at this time. Please contact your service provider if you feel this is incorrect.</p>\r\n\r\n<p>Your cache administrator is <a href=\"mailto:root?subject=CacheErrorInfo%20-%20ERR_ACCESS_DENIED&amp;body=CacheHost%3A%20cloud%0D%0AErrPage%3A%20ERR_ACCESS_DENIED%0D%0AErr%3A%20%5Bnone%5D%0D%0ATimeStamp%3A%20Mon,%2027%20Aug%202018%2010%3A08%3A20%20GMT%0D%0A%0D%0AClientIP%3A%20123.58.32.79%0D%0A%0D%0AHTTP%20Request%3A%0D%0AGET%20%2Fip%20HTTP%2F1.1%0AReferer%3A%20http%3A%2F%2Fhttpbin.org%2Fget%0D%0AUser-Agent%3A%20Scrapy%2F1.5.1%20(+https%3A%2F%2Fscrapy.org)%0D%0AAccept%3A%20text%2Fhtml,application%2Fxhtml+xml,application%2Fxml%3Bq%3D0.9,*%2F*%3Bq%3D0.8%0D%0AAccept-Language%3A%20en%0D%0AConnection%3A%20Keep-Alive%0D%0AAccept-Encoding%3A%20gzip,%20deflate%0D%0AHost%3A%20httpbin.org%0D%0A%0D%0A%0D%0A\">root</a>.</p>\r\n<br>\r\n</div>\r\n\r\n<hr>\r\n<div id=\"footer\">\r\n<p>Generated Mon, 27 Aug 2018 10:08:20 GMT by cloud (squid/3.5.20)</p>\r\n<!-- ERR_ACCESS_DENIED -->\r\n</div>\r\n\r\n</body></html>\r\n2018-08-27 18:07:31 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2018-08-27 18:07:31 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 829,\r\n 'downloader/request_count': 2,\r\n 'downloader/request_method_count/GET': 1,\r\n 'downloader/request_method_count/POST': 1,\r\n 'downloader/response_bytes': 4332,\r\n 'downloader/response_count': 2,\r\n 'downloader/response_status_count/200': 2,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2018, 8, 27, 10, 7, 31, 637620),\r\n 'log_count/DEBUG': 5,\r\n 'log_count/INFO': 7,\r\n 'request_depth_max': 1,\r\n 'response_received_count': 2,\r\n 'scheduler/dequeued': 3,\r\n 'scheduler/dequeued/memory': 3,\r\n 'scheduler/enqueued': 3,\r\n 'scheduler/enqueued/memory': 3,\r\n 'splash/render.html/request_count': 1,\r\n 'splash/render.html/response_count/200': 1,\r\n 'start_time': datetime.datetime(2018, 8, 27, 10, 7, 24, 409620)}\r\n2018-08-27 18:07:31 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\n\r\n############# log output from docker splash #################\r\n```\r\n2018-08-27 18:07:47.687084 [events] {\"load\": [1.0, 1.01, 1.05], \"user-agent\": \"Scrapy/1.5.1 (+https://scrapy.org)\", \"rendertime\": 5.380727052688599, \"_id\": 140403157087904, \"qsize\": 0, \"fds\": 20, \"maxrss\": 88076, \"path\": \"/render.html\", \"method\": \"POST\", \"active\": 0, \"client_ip\": \"10.150.133.14\", \"status_code\": 200, \"timestamp\": 1535364467, \"args\": {\"proxy\": \"http://222.85.50.102:28793\", \"url\": \"http://httpbin.org/ip\", \"uid\": 140403157087904, \"headers\": {\"User-Agent\": \"Scrapy/1.5.1 (+https://scrapy.org)\", \"Accept-Language\": \"en\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Referer\": \"http://httpbin.org/get\"}, \"wait\": 5, \"timeout\": 30}}\r\n2018-08-27 18:07:47.687641 [-] \"10.150.133.14\" - - [27/Aug/2018:10:07:46 +0000] \"POST /render.html HTTP/1.1\" 200 3640 \"-\" \"Scrapy/1.5.1 (+https://scrapy.org)\"\r\n2018-08-27 18:08:15.744028 [events] {\"load\": [1.0, 1.01, 1.05], \"user-agent\": \"Scrapy/1.5.1 (+https://scrapy.org)\", \"rendertime\": 6.455333948135376, \"_id\": 140403162958424, \"qsize\": 0, \"fds\": 20, \"maxrss\": 88076, \"path\": \"/render.html\", \"method\": \"POST\", \"active\": 0, \"client_ip\": \"10.150.133.14\", \"status_code\": 200, \"timestamp\": 1535364495, \"args\": {\"proxy\": \"http://114.216.184.69:35733\", \"url\": \"http://httpbin.org/ip\", \"uid\": 140403162958424, \"headers\": {\"User-Agent\": \"Scrapy/1.5.1 (+https://scrapy.org)\", \"Accept-Language\": \"en\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Referer\": \"http://httpbin.org/get\"}, \"wait\": 5, \"timeout\": 30}}\r\n2018-08-27 18:08:15.744295 [-] \"10.150.133.14\" - - [27/Aug/2018:10:08:14 +0000] \"POST /render.html HTTP/1.1\" 200 3640 \"-\" \"Scrapy/1.5.1 (+https://scrapy.org)\"\r\n```\r\n\r\nhow to fix it ?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/186", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/186/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/186/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/186/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/186", "id": 352683609, "node_id": "MDU6SXNzdWUzNTI2ODM2MDk=", "number": 186, "title": "\"Thai\" Language error", "user": {"login": "tarekbadrshalaan", "id": 31471186, "node_id": "MDQ6VXNlcjMxNDcxMTg2", "avatar_url": "https://avatars0.githubusercontent.com/u/31471186?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tarekbadrshalaan", "html_url": "https://github.com/tarekbadrshalaan", "followers_url": "https://api.github.com/users/tarekbadrshalaan/followers", "following_url": "https://api.github.com/users/tarekbadrshalaan/following{/other_user}", "gists_url": "https://api.github.com/users/tarekbadrshalaan/gists{/gist_id}", "starred_url": "https://api.github.com/users/tarekbadrshalaan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tarekbadrshalaan/subscriptions", "organizations_url": "https://api.github.com/users/tarekbadrshalaan/orgs", "repos_url": "https://api.github.com/users/tarekbadrshalaan/repos", "events_url": "https://api.github.com/users/tarekbadrshalaan/events{/privacy}", "received_events_url": "https://api.github.com/users/tarekbadrshalaan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-21T19:47:38Z", "updated_at": "2019-11-21T16:26:24Z", "closed_at": "2019-11-21T16:26:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to take screen shot of this tweet\r\n\"https://twitter.com/nanamaboy/status/1030380062300823552\"\r\nand got error in rendering \"Thai\" text\r\n![1030380062300823552](https://user-images.githubusercontent.com/31471186/44425224-88dca980-a58b-11e8-80e7-73c2913bb1a3.png)\r\n\r\nnote: screen shot works with me very well with all language , except \"Thai\".\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/185", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/185/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/185/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/185/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/185", "id": 349800174, "node_id": "MDU6SXNzdWUzNDk4MDAxNzQ=", "number": 185, "title": "set multi SPLASH_URLS", "user": {"login": "wisew", "id": 37316360, "node_id": "MDQ6VXNlcjM3MzE2MzYw", "avatar_url": "https://avatars0.githubusercontent.com/u/37316360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wisew", "html_url": "https://github.com/wisew", "followers_url": "https://api.github.com/users/wisew/followers", "following_url": "https://api.github.com/users/wisew/following{/other_user}", "gists_url": "https://api.github.com/users/wisew/gists{/gist_id}", "starred_url": "https://api.github.com/users/wisew/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wisew/subscriptions", "organizations_url": "https://api.github.com/users/wisew/orgs", "repos_url": "https://api.github.com/users/wisew/repos", "events_url": "https://api.github.com/users/wisew/events{/privacy}", "received_events_url": "https://api.github.com/users/wisew/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-12T10:04:53Z", "updated_at": "2019-11-21T16:26:19Z", "closed_at": "2019-11-21T16:26:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have multiple servers,so I want to implement dynamic urls.What should I do?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/184", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/184/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/184/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/184/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/184", "id": 346182975, "node_id": "MDU6SXNzdWUzNDYxODI5NzU=", "number": 184, "title": "How to handle Python Scrapy Splash error 502 due to auto-launch download", "user": {"login": "VishalSharmavj", "id": 41957966, "node_id": "MDQ6VXNlcjQxOTU3OTY2", "avatar_url": "https://avatars1.githubusercontent.com/u/41957966?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VishalSharmavj", "html_url": "https://github.com/VishalSharmavj", "followers_url": "https://api.github.com/users/VishalSharmavj/followers", "following_url": "https://api.github.com/users/VishalSharmavj/following{/other_user}", "gists_url": "https://api.github.com/users/VishalSharmavj/gists{/gist_id}", "starred_url": "https://api.github.com/users/VishalSharmavj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VishalSharmavj/subscriptions", "organizations_url": "https://api.github.com/users/VishalSharmavj/orgs", "repos_url": "https://api.github.com/users/VishalSharmavj/repos", "events_url": "https://api.github.com/users/VishalSharmavj/events{/privacy}", "received_events_url": "https://api.github.com/users/VishalSharmavj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-31T13:16:41Z", "updated_at": "2019-11-21T16:26:15Z", "closed_at": "2019-11-21T16:26:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using the code:\r\n\r\n```\r\ndef start_requests(self):\r\n\r\n\r\n        yield SplashRequest(\"https://www.example.com/myfilelinkextract/\", callback=self.parse)\r\n    def parse(self, response):\r\n        # y = response.xpath('//*[@id=\"download_sub_text\"]').extract()\r\n        # print(y)\r\n        print(response.body)\r\n```\r\n\r\nso that I can extract the download link of a file present in a onclick js like -\r\n```\r\n<div id=\"download_div\" class=\"row\" style=\"margin-left: 2%; margin-right: 2%\">\r\n<p id=\"download_sub_text\" class=\"hide-on-small-only\" style=\"text-align: center;\">\r\n    You could also download directly by\r\n    <a onclick=\"ga('send', 'event', 'link', 'click_here', 'wholesale.item');\"\r\n        href=\"http://example.com/f2c9bd13afd7a17af35ad30a2c593c7f4bea2dd347b4149\">\r\n        clicking here!\r\n    </a>\r\n```\r\nbut i'm getting the following Debug.\r\n\r\n```\r\n\r\n2018-07-31 04:26:14 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying\r\n <GET https://www.example.com/myfilelinkextract/ via http://localhost:8050/render.html> (failed 3 times): 502 Ba\r\nd Gateway\r\n2018-07-31 04:26:14 [scrapy.core.engine] DEBUG: Crawled (502) <GET https://www.example.com/myfilelinkextract/ vi\r\na http://localhost:8050/render.html> (referer: None)\r\nb'{\"info\": {\"code\": 102, \"url\": \"https://www.example.com/myfilelinkextracted_link_which_I_want\", \"type\": \"WebKit\", \"text\": \"Frame load interrupted by policy change\"}, \"error\": 502, \"type\": \"RenderError\", \"description\": \"Error rendering page\"}\r\n```\r\n\r\nIt must be noted that except the pages which triggers the download, the code is working fine and I'm able to extract response.body without an issue.\r\n\r\nAlso, is there a way that I can extract/yield the dictionary I got in the Debug bcz it will do the work for me.\r\n\r\n``b'{\"info\": {\"code\": 102, \"url\": \"https://www.example.com/myfilelinkextracted_link_which_I_want\", \"type\": \"WebKit\", \"text\": \"Frame load interrupted by policy change\"}, \"error\": 502, \"type\": \"RenderError\", \"description\": \"Error rendering page\"}'`\r\n`\r\nbecause what I want is just the url \"https://www.example.com/myfilelinkextracted_link_which_I_want\" which is same as present in onclick and also which gets auto-triggered for download.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/183", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/183/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/183/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/183/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/183", "id": 345434925, "node_id": "MDU6SXNzdWUzNDU0MzQ5MjU=", "number": 183, "title": "JavaScript? Nope! Doesn't execute document.location.reload();", "user": {"login": "asio1337", "id": 26015020, "node_id": "MDQ6VXNlcjI2MDE1MDIw", "avatar_url": "https://avatars0.githubusercontent.com/u/26015020?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asio1337", "html_url": "https://github.com/asio1337", "followers_url": "https://api.github.com/users/asio1337/followers", "following_url": "https://api.github.com/users/asio1337/following{/other_user}", "gists_url": "https://api.github.com/users/asio1337/gists{/gist_id}", "starred_url": "https://api.github.com/users/asio1337/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asio1337/subscriptions", "organizations_url": "https://api.github.com/users/asio1337/orgs", "repos_url": "https://api.github.com/users/asio1337/repos", "events_url": "https://api.github.com/users/asio1337/events{/privacy}", "received_events_url": "https://api.github.com/users/asio1337/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-28T09:28:07Z", "updated_at": "2019-11-21T16:26:08Z", "closed_at": "2019-11-21T16:26:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "After 20 or more crawls, the website return following response:\r\n\r\nRaw: https://pastebin.com/W8JVAkEe\r\nFormated: https://pastebin.com/kBhbjUr3\r\n\r\nA normal Browser execute the javascript and reload the pages sometimes. Splash doesnt execute the response, so i cant select anything. A browser returns after some reloads the correct content.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/182", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/182/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/182/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/182/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/182", "id": 341175738, "node_id": "MDU6SXNzdWUzNDExNzU3Mzg=", "number": 182, "title": "SplashMiddleware breaks \"script\": 1 invocations", "user": {"login": "Datamance", "id": 8699411, "node_id": "MDQ6VXNlcjg2OTk0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/8699411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Datamance", "html_url": "https://github.com/Datamance", "followers_url": "https://api.github.com/users/Datamance/followers", "following_url": "https://api.github.com/users/Datamance/following{/other_user}", "gists_url": "https://api.github.com/users/Datamance/gists{/gist_id}", "starred_url": "https://api.github.com/users/Datamance/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Datamance/subscriptions", "organizations_url": "https://api.github.com/users/Datamance/orgs", "repos_url": "https://api.github.com/users/Datamance/repos", "events_url": "https://api.github.com/users/Datamance/events{/privacy}", "received_events_url": "https://api.github.com/users/Datamance/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2018-07-13T22:20:59Z", "updated_at": "2019-11-21T16:30:02Z", "closed_at": "2019-11-21T16:30:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "See:\r\nhttps://github.com/scrapy-plugins/scrapy-splash/blob/master/scrapy_splash/middleware.py#L324\r\n\r\nThis line:\r\n`body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)`\r\n\r\nBreaks any SplashRequest that is trying to emulate the following:\r\n\r\n```bash\r\n# Render page and execute simple Javascript function, display the js output\r\ncurl -X POST -H 'content-type: application/javascript' \\\r\n    -d 'function getAd(x){ return x; } getAd(\"abc\");' \\\r\n    'http://localhost:8050/render.json?url=http://domain.com&script=1'\r\n```\r\n\r\nIf you just steamroll the intended POST body with a json dump of the args, then it's basically impossible to structure a `render.json` request (that doesn't use LUA, from what I can see).\r\n\r\nIn fact,  a Splash Request structured as such:\r\n\r\n```python\r\nsplash_request = scrapy.Request(\r\n            my_interesting_request_url,\r\n            callback=self.parse,\r\n            errback=self.err_back.errback_httpbin,\r\n            meta={\r\n                \"request_item\": request_item,\r\n                \"splash\": {\r\n                    \"args\": {\r\n                        \"method\": \"POST\",\r\n                        \"body\": JS_SOURCE,\r\n                        \"url\": request_item[\"request_url\"],\r\n                        \"html\": 1,\r\n                        \"script\": 1,\r\n                        \"max-timeout\": Config.SPLASH_MAX_TIMEOUT,\r\n                        \"slots\": Config.SPLASH_NUM_OF_SLOTS,\r\n                    },\r\n                    \"splash_headers\": {\"Content-Type\": \"application/javascript\"},\r\n                    \"endpoint\": Config.SPLASH_RENDER_JSON_ENDPOINT + \"?html=1&script=1&url=\" + request_item[\"request_url\"]\r\n                }\r\n            },\r\n        )\r\n```\r\n\r\nWill give me a body like this:\r\n```\r\nb'{\\n    \"headers\": {\\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\\n        \"Accept-Encoding\": \"gzip,deflate\",\\n\r\n      \"Accept-Language\": \"en\",\\n        \"User-Agent\": \"Scrapy/1.4.0 (+http://scrapy.org)\"\\n    },\\n    \"html\": 1,\\n    \"js_source\": \"JSON.stringify(WSI.assortmentJson)\",\\n\r\n    \"max-timeout\": 3600,\\n    \"method\": \"GET\",\\n    \"proxy\": \"http://ec2-54-234-146-220.compute-1.amazonaws.com:8080\",\\n    \"script\": 1,\\n    \"slots\": 5,\\n    \"url\": \"https://www.potterybarn.com/products/cambria-stoneware-mug-stone/\"\\n}'\r\n```\r\n\r\nWhen I log the request body in MySpider.parse (retrieved the response object). You can even see headers get set into the args object here: https://github.com/scrapy-plugins/scrapy-splash/blob/master/scrapy_splash/middleware.py#L322\r\n\r\nThis is obviously wrong.  I haven't the faintest idea of why headers would be put into the POST body. If there is a \"right way\" to do it, it's not clear from the documentation - in fact the documentation seems to be _misleading_. I've also tried with SplashRequest, but this is just another layer of abstraction that doesn't treat the actual problem of the request body being screwed up.\r\n\r\nI'll try and submit a pull request for this in the next few weeks, but honestly I'm surprised this hasn't been complained about elsewhere.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/181", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/181/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/181/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/181/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/181", "id": 339919838, "node_id": "MDU6SXNzdWUzMzk5MTk4Mzg=", "number": 181, "title": "some questions with proxy when i use scrapy-splash", "user": {"login": "AceXia", "id": 23203072, "node_id": "MDQ6VXNlcjIzMjAzMDcy", "avatar_url": "https://avatars0.githubusercontent.com/u/23203072?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AceXia", "html_url": "https://github.com/AceXia", "followers_url": "https://api.github.com/users/AceXia/followers", "following_url": "https://api.github.com/users/AceXia/following{/other_user}", "gists_url": "https://api.github.com/users/AceXia/gists{/gist_id}", "starred_url": "https://api.github.com/users/AceXia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AceXia/subscriptions", "organizations_url": "https://api.github.com/users/AceXia/orgs", "repos_url": "https://api.github.com/users/AceXia/repos", "events_url": "https://api.github.com/users/AceXia/events{/privacy}", "received_events_url": "https://api.github.com/users/AceXia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-10T16:33:45Z", "updated_at": "2019-11-21T16:27:04Z", "closed_at": "2019-11-21T16:27:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "i want to use scrapy-splash with proxy,my code like this:\r\n  **def start_requests(self):\r\n        for url in self.start_urls:\r\n            yield SplashRequest(url, self.parse,args={'wait': 10,'proxy': 'http://127.0.0.1:1080'})**\r\nbut i get some messsage in console like this:   **502 Bad Gateway**\r\nand splash show some message :\r\n    **2018-07-10 16:00:16.670873 [events] {\"qsize\": 0, \"load\": [0.02, 0.04, 0.0], \"rendertime\": 0.02126336097717285, \"error\": {\"type\": \"RenderError\", \"info\": {\"text\": \"Proxy connection refused\", \"code\": 99, \"url\": \"https://www.baidu.com\", \"type\": \"Network\"}, \"description\": \"Error rendering page\", \"error\": 502}, \"path\": \"/render.html\", \"_id\": 139738842022520, \"fds\": 20, \"args\": {\"proxy\": \"http://127.0.0.1:1081\", \"wait\": 10, \"headers\": {\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"User-Agent\": \"Scrapy/1.5.0 (+https://scrapy.org)\", \"Accept-Language\": \"en\"}, \"uid\": 139738842022520, \"url\": \"https://www.baidu.com\"}, \"maxrss\": 149472, \"user-agent\": \"Scrapy/1.5.0 (+https://scrapy.org)\", \"method\": \"POST\", \"client_ip\": \"172.17.0.1\", \"timestamp\": 1531238416, \"status_code\": 502, \"active\": 0}\r\n2018-07-10 16:00:16.672596 [-] \"172.17.0.1\" - - [10/Jul/2018:16:00:15 +0000] \"POST /render.html HTTP/1.1\" 502 269 \"-\" \"Scrapy/1.5.0 (+https://scrapy.org)\"**\r\n   who can help me ,please\r\n   ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/179", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/179/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/179/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/179/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/179", "id": 336264417, "node_id": "MDU6SXNzdWUzMzYyNjQ0MTc=", "number": 179, "title": "Crash after running the docker container for a while and using scrapy", "user": {"login": "lfdversluis", "id": 3618917, "node_id": "MDQ6VXNlcjM2MTg5MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3618917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lfdversluis", "html_url": "https://github.com/lfdversluis", "followers_url": "https://api.github.com/users/lfdversluis/followers", "following_url": "https://api.github.com/users/lfdversluis/following{/other_user}", "gists_url": "https://api.github.com/users/lfdversluis/gists{/gist_id}", "starred_url": "https://api.github.com/users/lfdversluis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lfdversluis/subscriptions", "organizations_url": "https://api.github.com/users/lfdversluis/orgs", "repos_url": "https://api.github.com/users/lfdversluis/repos", "events_url": "https://api.github.com/users/lfdversluis/events{/privacy}", "received_events_url": "https://api.github.com/users/lfdversluis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-27T14:52:28Z", "updated_at": "2019-11-21T16:25:23Z", "closed_at": "2019-11-21T16:25:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Using scrapy 1.4.0 and 0.7.2. of this plugin. Got a weird crash after running it for a while (don't know exactly how long). Header is set via string. Looks lik qt5 broke?\r\n\r\n```\r\n2018-06-26 09:12:45.015610 [-] \"172.17.0.1\" - - [26/Jun/2018:09:12:44 +0000] \"POST /execute HTTP/1.1\" 200 234258 \"-\" \"Mozilla/5.0 (X11; Linux x86_64; rv:7.0.1) Gecko/20170205 Firefox/7.7\"\r\n1   0x7f7e38d85147 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(WTFCrash+0x17) [0x7f7e38d85147]\r\n2   0x7f7e38dc6aa3 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(+0xed4aa3) [0x7f7e38dc6aa3]\r\n3   0x7f7e38d90e55 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(+0xe9ee55) [0x7f7e38d90e55]\r\n4   0x7f7e38d90f3b /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(_ZN3WTF19MetaAllocatorHandleD1Ev+0xab) [0x7f7e38d90f3b]\r\n5   0x7f7e38793237 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(+0x8a1237) [0x7f7e38793237]\r\n6   0x7f7e386d7c74 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(_ZN3JSC9CodeBlockD1Ev+0xa84) [0x7f7e386d7c74]\r\n7   0x7f7e3875c077 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(+0x86a077) [0x7f7e3875c077]\r\n8   0x7f7e384fa526 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(+0x608526) [0x7f7e384fa526]\r\n9   0x7f7e384fa82d /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(_ZN3JSC4Heap7collectENS_13HeapOperationE+0x5d) [0x7f7e384fa82d]\r\n10  0x7f7e3875f919 /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(_ZN3JSC18GCActivityCallback6doWorkEv+0x79) [0x7f7e3875f919]\r\n11  0x7f7e387608be /opt/qt59/5.9.1/gcc_64/lib/libQt5WebKit.so.5(+0x86e8be) [0x7f7e387608be]\r\n12  0x7f7e42e6a37b /opt/qt59/5.9.1/gcc_64/lib/libQt5Core.so.5(_ZN7QObject5eventEP6QEvent+0x7b) [0x7f7e42e6a37b]\r\n13  0x7f7e3b8a44bc /opt/qt59/5.9.1/gcc_64/lib/libQt5Widgets.so.5(_ZN19QApplicationPrivate13notify_helperEP7QObjectP6QEvent+0x9c) [0x7f7e3b8a44bc]\r\n14  0x7f7e3b8ab8e7 /opt/qt59/5.9.1/gcc_64/lib/libQt5Widgets.so.5(_ZN12QApplication6notifyEP7QObjectP6QEvent+0x227) [0x7f7e3b8ab8e7]\r\n15  0x7f7e3c33b4de /usr/lib/python3/dist-packages/PyQt5/QtWidgets.so(+0x3b94de) [0x7f7e3c33b4de]\r\n16  0x7f7e42e3eeb8 /opt/qt59/5.9.1/gcc_64/lib/libQt5Core.so.5(_ZN16QCoreApplication15notifyInternal2EP7QObjectP6QEvent+0x108) [0x7f7e42e3eeb8]\r\n17  0x7f7e42e905fe /opt/qt59/5.9.1/gcc_64/lib/libQt5Core.so.5(_ZN14QTimerInfoList14activateTimersEv+0x46e) [0x7f7e42e905fe]\r\n18  0x7f7e42e90de1 /opt/qt59/5.9.1/gcc_64/lib/libQt5Core.so.5(+0x2cade1) [0x7f7e42e90de1]\r\n19  0x7f7e40147197 /lib/x86_64-linux-gnu/libglib-2.0.so.0(g_main_context_dispatch+0x2a7) [0x7f7e40147197]\r\n20  0x7f7e401473f0 /lib/x86_64-linux-gnu/libglib-2.0.so.0(+0x4a3f0) [0x7f7e401473f0]\r\n21  0x7f7e4014749c /lib/x86_64-linux-gnu/libglib-2.0.so.0(g_main_context_iteration+0x2c) [0x7f7e4014749c]\r\n22  0x7f7e42e9111f /opt/qt59/5.9.1/gcc_64/lib/libQt5Core.so.5(_ZN20QEventDispatcherGlib13processEventsE6QFlagsIN10QEventLoop17ProcessEventsFlagEE+0x5f) [0x7f7e42e9111f]\r\n23  0x7f7e42e3d4aa /opt/qt59/5.9.1/gcc_64/lib/libQt5Core.so.5(_ZN10QEventLoop4execE6QFlagsINS_17ProcessEventsFlagEE+0xea) [0x7f7e42e3d4aa]\r\n24  0x7f7e43426dab /usr/lib/python3/dist-packages/PyQt5/QtCore.so(+0x126dab) [0x7f7e43426dab]\r\n25  0x4e9bc7 python3(PyCFunction_Call+0x77) [0x4e9bc7]\r\n26  0x524414 python3(PyEval_EvalFrameEx+0x614) [0x524414]\r\n27  0x52d2e3 python3() [0x52d2e3]\r\n28  0x528eee python3(PyEval_EvalFrameEx+0x50ee) [0x528eee]\r\n29  0x52d2e3 python3() [0x52d2e3]\r\n30  0x528eee python3(PyEval_EvalFrameEx+0x50ee) [0x528eee]\r\n31  0x52d2e3 python3() [0x52d2e3]\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/178", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/178/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/178/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/178/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/178", "id": 334735842, "node_id": "MDU6SXNzdWUzMzQ3MzU4NDI=", "number": 178, "title": "Is it possible to inherit SplashTextResponse from HtmlResponse but not TextResponse", "user": {"login": "FajunChen", "id": 6104197, "node_id": "MDQ6VXNlcjYxMDQxOTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6104197?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FajunChen", "html_url": "https://github.com/FajunChen", "followers_url": "https://api.github.com/users/FajunChen/followers", "following_url": "https://api.github.com/users/FajunChen/following{/other_user}", "gists_url": "https://api.github.com/users/FajunChen/gists{/gist_id}", "starred_url": "https://api.github.com/users/FajunChen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FajunChen/subscriptions", "organizations_url": "https://api.github.com/users/FajunChen/orgs", "repos_url": "https://api.github.com/users/FajunChen/repos", "events_url": "https://api.github.com/users/FajunChen/events{/privacy}", "received_events_url": "https://api.github.com/users/FajunChen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-22T03:54:33Z", "updated_at": "2019-11-21T16:25:17Z", "closed_at": "2019-11-21T16:25:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "In order to naturally use of CrawlSpider, is it possible to change the super class of SplashTextResponse to HtmlResponse from TextResponse? Though HtmlResponse is just a naive wrapper of TextResponse, However, CrawlSpider use the type of response  as a condition for future processing in method ` _requests_to_follow`, it requires an instance of HtmlResponse but not TextResponse.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/177", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/177/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/177/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/177/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/177", "id": 329371181, "node_id": "MDU6SXNzdWUzMjkzNzExODE=", "number": 177, "title": "Redirect not working in scrapy splash", "user": {"login": "Loberauer", "id": 39946930, "node_id": "MDQ6VXNlcjM5OTQ2OTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/39946930?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Loberauer", "html_url": "https://github.com/Loberauer", "followers_url": "https://api.github.com/users/Loberauer/followers", "following_url": "https://api.github.com/users/Loberauer/following{/other_user}", "gists_url": "https://api.github.com/users/Loberauer/gists{/gist_id}", "starred_url": "https://api.github.com/users/Loberauer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Loberauer/subscriptions", "organizations_url": "https://api.github.com/users/Loberauer/orgs", "repos_url": "https://api.github.com/users/Loberauer/repos", "events_url": "https://api.github.com/users/Loberauer/events{/privacy}", "received_events_url": "https://api.github.com/users/Loberauer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-05T09:22:39Z", "updated_at": "2018-06-07T06:56:02Z", "closed_at": "2018-06-07T06:56:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "**[UPDATE2]**: When i extract something from the response for another request the url looks like this\r\n        https://www.hornbach.at{{article.localizedExternalArticleLink}}\r\n\r\n**[UPDATE]**: when i do **_open_in_browser(response)_** it opens txt file with the right url inside. seems that only the **_response.url_** prints the base url without the redirected url \r\n    redirectedURL: https://www.hornbach.at/shop/Badarmaturen/Waschtischarmaturen/S3596/artikelliste.html\r\nbaseURL: https://www.hornbach.at/shop/Badarmaturen/Waschtischarmaturen/S3596/artikelliste.html\r\nBut it should be like this(from open_in_browser(response)) :\r\n    https://www.hornbach.at/shop/Badarmaturen/Waschtischarmaturen/S3596/artikelliste.html#/eyJuIjoxLCJ2aWV3IjoiZ2FsbGVyeSIsImxpc3RDcml0ZXJpYSI6eyJwYWdlTnVtYmVyIjoyLCJwYWdlU2l6ZSI6NzIsInNvcnRPcmRlciI6InNvcnRNb2RlRHYifSwiYWN0aXZlRmlsdGVycyI6W10sInVybFZlciI6Mn0=\r\n\r\n\r\n\r\nhi i\u00b4m facing the same problem like https://github.com/scrapy-plugins/scrapy-splash/issues/152. did anyone find `any` solution?\r\n\r\nthe redirect works in jupyter notebook with splash and in the splash UI but not in scrapy. I want scrapy to fetch the next page of the shop. it\u00b4s made with angularjs.\r\n`\r\n    \r\n        nextpage_lua:\"\"\"\r\n       function main(splash, args)\r\n            splash.images_enabled = false\r\n            splash.private_mode_enabled = false\r\n            assert(splash:go(args.url)) \r\n            assert(splash:wait(1.0)) \r\n            splash:set_viewport_full() \r\n            assert(splash:wait(1.0))\r\n            for i=1,10 do\r\n                scroll_to_bottom(splash)    \r\n                assert(splash:wait(0.2))\r\n            end\r\n            btn = splash:select('a.right:nth-child(4)')\r\n            btn:mouse_click()\r\n            assert(splash:wait(25.0))\r\n            return splash:url()\r\n        end\r\n\r\n      function scroll_to(splash, x, y)\r\n          local js = string.format(\r\n            \"window.scrollTo(%s, %s);\", \r\n            tonumber(x), \r\n            tonumber(y)\r\n          )\r\n          return {\r\n            url = splash:url(),\r\n            html = splash:html(),\r\n            png = splash:png(),\r\n            har = splash:har(),\r\n          }\r\n        end\r\n\r\n        function get_doc_height(splash)\r\n          return splash:runjs([[\r\n            Math.max(\r\n                Math.max(document.body.scrollHeight, document.documentElement.scrollHeight),\r\n                Math.max(document.body.offsetHeight, document.documentElement.offsetHeight),\r\n                Math.max(document.body.clientHeight, document.documentElement.clientHeight)\r\n            )\r\n          ]])\r\n        end\r\n\r\n\r\n        function scroll_to_bottom(splash)\r\n          local y = get_doc_height(splash)\r\n          return scroll_to(splash, 0, y)\r\n        end\r\n\"\"\"\r\n`\r\n`\r\n    python\r\n    \r\n    class GitHubSpider(Spider):\r\n    name = 'githubspider'\r\n    start_urls = ['https://www.hornbach.at/shop/Badarmaturen/Waschtischarmaturen/S3596/artikelliste.html']\r\n    render_wait_time = 10.5\r\n    time_out = 500\r\n    def start_requests(self):\r\n        for url in self.start_urls:\r\n            print('Start crawling with: ' + url)\r\n            yield SplashRequest(url, self.parse, endpoint='execute', magic_response=True,\r\n                                meta={'handle_httpstatus_all': True},\r\n                                args={'lua_source': nextpage_lua, 'wait': self.render_wait_time,\r\n                                      'timeout': self.time_out}, dont_filter=False)\r\n\r\n    def parse(self, response):\r\n        print(\"parse: \" + response.url)\r\n        baseurl = response.meta[\"splash\"][\"args\"][\"url\"]\r\n        print(\"parseBase: \" + baseurl)\r\n        open_in_browser(response)\r\n       \r\n`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/172", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/172/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/172/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/172/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/172", "id": 310278919, "node_id": "MDU6SXNzdWUzMTAyNzg5MTk=", "number": 172, "title": "can't work with session and cookie", "user": {"login": "NewUserHa", "id": 32261870, "node_id": "MDQ6VXNlcjMyMjYxODcw", "avatar_url": "https://avatars3.githubusercontent.com/u/32261870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NewUserHa", "html_url": "https://github.com/NewUserHa", "followers_url": "https://api.github.com/users/NewUserHa/followers", "following_url": "https://api.github.com/users/NewUserHa/following{/other_user}", "gists_url": "https://api.github.com/users/NewUserHa/gists{/gist_id}", "starred_url": "https://api.github.com/users/NewUserHa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NewUserHa/subscriptions", "organizations_url": "https://api.github.com/users/NewUserHa/orgs", "repos_url": "https://api.github.com/users/NewUserHa/repos", "events_url": "https://api.github.com/users/NewUserHa/events{/privacy}", "received_events_url": "https://api.github.com/users/NewUserHa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-31T23:08:57Z", "updated_at": "2019-11-21T16:08:02Z", "closed_at": "2019-11-21T16:08:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "win10 10586\r\npython3.6.5rc1 64\r\nscrapy 1.5.0\r\nscrapy_splash 0.7.2\r\nsplash last docker image from cn mirror in centos.\r\n\r\nstart from `scrapy shell \"http://<docker host>/render.html?url=...\"`\r\n\r\nI tried the lua script from `Session Handling` of the readme, but it nerver work.\r\n`splash.args.cookies` says `'NoneType' object is not iterable`.\r\nI tried response.headers in both `execute` endpoint and `render.html` endpoin and always is\r\n```\r\n{b'Content-Type': b'application/json',\r\n b'Date': b'Sat, 31 Mar 2018 22:52:51 GMT',\r\n b'Server': b'TwistedWeb/16.1.1'}\r\n```\r\nonly.\r\nI tried response.cookiejar in `render.json` endpoint but it return `None`.\r\n\r\n---\r\nbtw, I'm using like\r\n```\r\na = scrapy_splash.SplashRequest(\"...\", session_id='1', magic_response=True)  # (already requested the site ...)\r\nfetch(a)\r\n```\r\n,then DUPEFILTER will keep stucking cmd.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/171", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/171/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/171/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/171/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/171", "id": 310205126, "node_id": "MDU6SXNzdWUzMTAyMDUxMjY=", "number": 171, "title": "ARM version", "user": {"login": "Imparium", "id": 26612284, "node_id": "MDQ6VXNlcjI2NjEyMjg0", "avatar_url": "https://avatars1.githubusercontent.com/u/26612284?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Imparium", "html_url": "https://github.com/Imparium", "followers_url": "https://api.github.com/users/Imparium/followers", "following_url": "https://api.github.com/users/Imparium/following{/other_user}", "gists_url": "https://api.github.com/users/Imparium/gists{/gist_id}", "starred_url": "https://api.github.com/users/Imparium/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Imparium/subscriptions", "organizations_url": "https://api.github.com/users/Imparium/orgs", "repos_url": "https://api.github.com/users/Imparium/repos", "events_url": "https://api.github.com/users/Imparium/events{/privacy}", "received_events_url": "https://api.github.com/users/Imparium/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-31T02:23:26Z", "updated_at": "2019-09-05T15:29:44Z", "closed_at": "2018-04-06T09:34:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello. I have an ARM computer (Raspberry Pi) and I can't seen to run it.\r\nWhen I try: `sudo docker run -p 8050:8050 scrapinghub/splash`\r\nI get: `standard_init_linux.go:190: exec user process caused \"exec format error\"`\r\n\r\nThank you so much", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/170", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/170/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/170/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/170/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/170", "id": 309256346, "node_id": "MDU6SXNzdWUzMDkyNTYzNDY=", "number": 170, "title": "[QUESTION]wisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060:", "user": {"login": "JayVae", "id": 22939222, "node_id": "MDQ6VXNlcjIyOTM5MjIy", "avatar_url": "https://avatars0.githubusercontent.com/u/22939222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JayVae", "html_url": "https://github.com/JayVae", "followers_url": "https://api.github.com/users/JayVae/followers", "following_url": "https://api.github.com/users/JayVae/following{/other_user}", "gists_url": "https://api.github.com/users/JayVae/gists{/gist_id}", "starred_url": "https://api.github.com/users/JayVae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JayVae/subscriptions", "organizations_url": "https://api.github.com/users/JayVae/orgs", "repos_url": "https://api.github.com/users/JayVae/repos", "events_url": "https://api.github.com/users/JayVae/events{/privacy}", "received_events_url": "https://api.github.com/users/JayVae/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-28T07:49:59Z", "updated_at": "2018-04-06T10:18:25Z", "closed_at": "2018-04-06T10:18:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "HI!\r\nI am a new bird of scrapy-splash,I have some problems running the project which tells:wisted.internet.error.TCPTimedOutError: TCP connection timed out: 10060!\r\nI don't know how to tackle this problem, but it didn't occur if I do not use scrapy-splash~\r\n\r\nSO, can you give me some advice to handle this problem? Thanks a lot.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/169", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/169/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/169/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/169/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/169", "id": 303247014, "node_id": "MDU6SXNzdWUzMDMyNDcwMTQ=", "number": 169, "title": "how to run splash on some url ? splash_id default is 0.0.0.0 . i want to run it on some url?", "user": {"login": "Waseemrajashaik", "id": 12861819, "node_id": "MDQ6VXNlcjEyODYxODE5", "avatar_url": "https://avatars1.githubusercontent.com/u/12861819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Waseemrajashaik", "html_url": "https://github.com/Waseemrajashaik", "followers_url": "https://api.github.com/users/Waseemrajashaik/followers", "following_url": "https://api.github.com/users/Waseemrajashaik/following{/other_user}", "gists_url": "https://api.github.com/users/Waseemrajashaik/gists{/gist_id}", "starred_url": "https://api.github.com/users/Waseemrajashaik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Waseemrajashaik/subscriptions", "organizations_url": "https://api.github.com/users/Waseemrajashaik/orgs", "repos_url": "https://api.github.com/users/Waseemrajashaik/repos", "events_url": "https://api.github.com/users/Waseemrajashaik/events{/privacy}", "received_events_url": "https://api.github.com/users/Waseemrajashaik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-07T20:25:35Z", "updated_at": "2019-11-21T16:23:58Z", "closed_at": "2019-11-21T16:23:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/167", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/167/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/167/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/167/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/167", "id": 296565060, "node_id": "MDU6SXNzdWUyOTY1NjUwNjA=", "number": 167, "title": "Injecting anti headless browser detection script", "user": {"login": "vionemc", "id": 6565672, "node_id": "MDQ6VXNlcjY1NjU2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6565672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vionemc", "html_url": "https://github.com/vionemc", "followers_url": "https://api.github.com/users/vionemc/followers", "following_url": "https://api.github.com/users/vionemc/following{/other_user}", "gists_url": "https://api.github.com/users/vionemc/gists{/gist_id}", "starred_url": "https://api.github.com/users/vionemc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vionemc/subscriptions", "organizations_url": "https://api.github.com/users/vionemc/orgs", "repos_url": "https://api.github.com/users/vionemc/repos", "events_url": "https://api.github.com/users/vionemc/events{/privacy}", "received_events_url": "https://api.github.com/users/vionemc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-12T23:44:56Z", "updated_at": "2018-02-20T13:37:36Z", "closed_at": "2018-02-20T13:37:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://intoli.com/blog/making-chrome-headless-undetectable/\r\n\r\nIf you see there, they provide a script to avoid getting detected that we are using a headless browser. Some sites are okay to be scraped using a standard request, but not okay to be scraped using a Splash Request.  It will show a captcha request. So I want to use the script injection. But, is it possible?\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/166", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/166/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/166/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/166/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/166", "id": 295856033, "node_id": "MDU6SXNzdWUyOTU4NTYwMzM=", "number": 166, "title": "response select nothing", "user": {"login": "falltodis", "id": 7006864, "node_id": "MDQ6VXNlcjcwMDY4NjQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/7006864?v=4", "gravatar_id": "", "url": "https://api.github.com/users/falltodis", "html_url": "https://github.com/falltodis", "followers_url": "https://api.github.com/users/falltodis/followers", "following_url": "https://api.github.com/users/falltodis/following{/other_user}", "gists_url": "https://api.github.com/users/falltodis/gists{/gist_id}", "starred_url": "https://api.github.com/users/falltodis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/falltodis/subscriptions", "organizations_url": "https://api.github.com/users/falltodis/orgs", "repos_url": "https://api.github.com/users/falltodis/repos", "events_url": "https://api.github.com/users/falltodis/events{/privacy}", "received_events_url": "https://api.github.com/users/falltodis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-09T12:32:32Z", "updated_at": "2019-11-21T16:23:53Z", "closed_at": "2019-11-21T16:23:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "I try demo code like below\r\n```\r\n    start_urls = ['http://jandan.net/ooxx']\r\n\r\n    def parse(self, response):\r\n        le = LinkExtractor()\r\n        for link in le.extract_links(response):\r\n            print link.url\r\n            yield SplashRequest(\r\n                link.url,\r\n                self.parse_link,\r\n                endpoint='render.html',\r\n                args={\r\n                    'har': 1,\r\n                    'html': 1,\r\n                    'wait': 4.5,\r\n                    'http_method': 'GET'\r\n                }\r\n            )\r\n\r\n    def parse_link(self, response):\r\n        print(\"PARSED\", response.real_url, response.url)\r\n        print(response.css(\".row\").extract())\r\n        print(response.headers.get('Content-Type'))\r\n```\r\nbut I got nothing with selector `response.css(\".row\")`  \r\nof course the page contain element like `<div class='row'>`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/165", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/165/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/165/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/165/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/165", "id": 295734236, "node_id": "MDU6SXNzdWUyOTU3MzQyMzY=", "number": 165, "title": "Invalid ELF header", "user": {"login": "zbury", "id": 8207123, "node_id": "MDQ6VXNlcjgyMDcxMjM=", "avatar_url": "https://avatars1.githubusercontent.com/u/8207123?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zbury", "html_url": "https://github.com/zbury", "followers_url": "https://api.github.com/users/zbury/followers", "following_url": "https://api.github.com/users/zbury/following{/other_user}", "gists_url": "https://api.github.com/users/zbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/zbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zbury/subscriptions", "organizations_url": "https://api.github.com/users/zbury/orgs", "repos_url": "https://api.github.com/users/zbury/repos", "events_url": "https://api.github.com/users/zbury/events{/privacy}", "received_events_url": "https://api.github.com/users/zbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-09T02:26:13Z", "updated_at": "2018-02-09T16:16:45Z", "closed_at": "2018-02-09T16:16:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Using the command `docker run -p 8050:8050 scrapinghub/splash` results in the following error for me:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/app/bin/splash\", line 3, in <module>\r\n        from splash.server import main\r\n      File \"/app/splash/server.py\", line 11, in <module>\r\n        from splash.qtutils import init_qt_app\r\n      File \"/app/splash/qtutils.py\", line 15, in <module>\r\n        from PyQt5.QtWebKit import QWebSettings\r\n    ImportError: /opt/qt59/5.9.1/gcc_64/lib/libQt5Quick.so.5: invalid ELF header\r\n\r\nThe first time I tried the command I didn't encounter this error..but subsequent attempts all result in that error.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/161", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/161/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/161/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/161/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/161", "id": 289860442, "node_id": "MDU6SXNzdWUyODk4NjA0NDI=", "number": 161, "title": "section in readme.rst with solutions for common issues", "user": {"login": "iAnanich", "id": 28541423, "node_id": "MDQ6VXNlcjI4NTQxNDIz", "avatar_url": "https://avatars0.githubusercontent.com/u/28541423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iAnanich", "html_url": "https://github.com/iAnanich", "followers_url": "https://api.github.com/users/iAnanich/followers", "following_url": "https://api.github.com/users/iAnanich/following{/other_user}", "gists_url": "https://api.github.com/users/iAnanich/gists{/gist_id}", "starred_url": "https://api.github.com/users/iAnanich/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iAnanich/subscriptions", "organizations_url": "https://api.github.com/users/iAnanich/orgs", "repos_url": "https://api.github.com/users/iAnanich/repos", "events_url": "https://api.github.com/users/iAnanich/events{/privacy}", "received_events_url": "https://api.github.com/users/iAnanich/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-19T04:36:45Z", "updated_at": "2018-02-02T13:21:58Z", "closed_at": "2018-02-02T09:42:51Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I wasted my day in frustration why does splash/scrapy-splash/ randomly decides to render or not, and then tried to set `wait: 5`...\r\n\r\nAnd as I discovered it is a pretty common issue. So I'm proposing to add something like `FAQ` or `Troubleshooting` section in the `README.rst` file to help newcomers use this great plugin. Also, mention such things as `Splash`&`Scrapy` documentation and their help-pages.\r\n\r\n- [x] StackOverflow\r\n- [x] Scrapy bugs reporting\r\n- [x] Splash FAQ\r\n- [ ] `wait` argument\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/159", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/159/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/159/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/159/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/159", "id": 289425359, "node_id": "MDU6SXNzdWUyODk0MjUzNTk=", "number": 159, "title": "[question] Why does SplashTextResponse inherits from TextResponse instead HtmlResponse?", "user": {"login": "iAnanich", "id": 28541423, "node_id": "MDQ6VXNlcjI4NTQxNDIz", "avatar_url": "https://avatars0.githubusercontent.com/u/28541423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iAnanich", "html_url": "https://github.com/iAnanich", "followers_url": "https://api.github.com/users/iAnanich/followers", "following_url": "https://api.github.com/users/iAnanich/following{/other_user}", "gists_url": "https://api.github.com/users/iAnanich/gists{/gist_id}", "starred_url": "https://api.github.com/users/iAnanich/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iAnanich/subscriptions", "organizations_url": "https://api.github.com/users/iAnanich/orgs", "repos_url": "https://api.github.com/users/iAnanich/repos", "events_url": "https://api.github.com/users/iAnanich/events{/privacy}", "received_events_url": "https://api.github.com/users/iAnanich/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-17T21:47:55Z", "updated_at": "2018-01-17T22:03:02Z", "closed_at": "2018-01-17T22:03:02Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Why does `scrapy_splash.reponse.SplashTextResponse` inherits from `scrapy.http.reponse.text.TextResponse` and not from `scrapy.http.reponse.html.HtmlResponse`?\r\n\r\nI understand that they are identical, and `HtmlResponse` uses `TextResponse` without any change, but they mean different things and Scrapy uses `isinstance` check to distinguish them.\r\n\r\nIn my case, it causes problem because I have type check for `HtmlResponse` where I expect to get HTML page and use `css` method to extract data from it, but `SplashTextResponse` doesn't pass it.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/155", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/155/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/155/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/155/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/155", "id": 287632456, "node_id": "MDU6SXNzdWUyODc2MzI0NTY=", "number": 155, "title": "scrapy always Starting new HTTP connection after crawl finished when I use scrapy-splash with scrapy", "user": {"login": "3xp10it", "id": 15134333, "node_id": "MDQ6VXNlcjE1MTM0MzMz", "avatar_url": "https://avatars3.githubusercontent.com/u/15134333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/3xp10it", "html_url": "https://github.com/3xp10it", "followers_url": "https://api.github.com/users/3xp10it/followers", "following_url": "https://api.github.com/users/3xp10it/following{/other_user}", "gists_url": "https://api.github.com/users/3xp10it/gists{/gist_id}", "starred_url": "https://api.github.com/users/3xp10it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/3xp10it/subscriptions", "organizations_url": "https://api.github.com/users/3xp10it/orgs", "repos_url": "https://api.github.com/users/3xp10it/repos", "events_url": "https://api.github.com/users/3xp10it/events{/privacy}", "received_events_url": "https://api.github.com/users/3xp10it/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-11T01:17:30Z", "updated_at": "2019-08-07T06:40:20Z", "closed_at": "2019-08-07T06:40:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "More details is [here][1],may be it's scrapy-splash's behavior.\r\n\r\n[1]: https://github.com/scrapy/scrapy/issues/3068", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/154", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/154/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/154/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/154/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/154", "id": 287102521, "node_id": "MDU6SXNzdWUyODcxMDI1MjE=", "number": 154, "title": "Problem with lua script in Spider", "user": {"login": "mas205", "id": 26790510, "node_id": "MDQ6VXNlcjI2NzkwNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/26790510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mas205", "html_url": "https://github.com/mas205", "followers_url": "https://api.github.com/users/mas205/followers", "following_url": "https://api.github.com/users/mas205/following{/other_user}", "gists_url": "https://api.github.com/users/mas205/gists{/gist_id}", "starred_url": "https://api.github.com/users/mas205/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mas205/subscriptions", "organizations_url": "https://api.github.com/users/mas205/orgs", "repos_url": "https://api.github.com/users/mas205/repos", "events_url": "https://api.github.com/users/mas205/events{/privacy}", "received_events_url": "https://api.github.com/users/mas205/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-01-09T14:13:41Z", "updated_at": "2018-01-11T12:39:39Z", "closed_at": "2018-01-11T12:39:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "When i run this both the url and the body are from the original url, when a run that in shell it gives me the body from the url I just clicked.\r\n\r\ncode:\r\n\r\nfrom scrapy.spiders import Spider\r\nfrom scrapy_splash import SplashRequest\r\n\r\nscript2=\"\"\"\r\nfunction main(splash, args)\r\n  local hacer_click = splash:jsfunc([[\r\n    function(boton){\r\n    var start_id = 'ctl00_ctl00_FormMasterContentPlaceHolder_ContentPlaceHolder1_'\r\n    var boton = document.getElementById(start_id+boton);\r\n    boton.click();\r\n  }\r\n    ]])\r\n  splash:go(args.url)\r\n  hacer_click('ucSearch')\r\n  splash:wait(1.0)\r\n  hacer_click('GvOfertaGestao_ctl02_btnDetalhes')\r\n  splash:wait(1.0)\r\n  return splash:html()\r\nend\r\n    \"\"\"\r\n    \r\n    \r\nclass Spider(Spider):\r\n    name = \"bep_gov2__pt\"\r\n    allowed_domain=[]\r\n    \r\n    def start_requests(self):\r\n        yield SplashRequest(url='https://www.bep.gov.pt/pages/oferta/Oferta_Pesquisa_basica.aspx', endpoint='execute', args={'lua_source':script2}, callback=self.parse)\r\n\r\n\r\n    def parse(self, response):\r\n        foo=response.url\r\n        foo2=response.body\r\n        print foo, foo2\r\n\r\nI want the response of parse to be the return of the script, I don't know if I'm doing anything wrong or something, but it gives me the body of the page after the first click (that loads a table).\r\nPlease help!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/153", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/153/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/153/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/153/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/153", "id": 286799791, "node_id": "MDU6SXNzdWUyODY3OTk3OTE=", "number": 153, "title": "OffsiteMiddleware TypeError: expected string or buffer", "user": {"login": "aidiss", "id": 5175311, "node_id": "MDQ6VXNlcjUxNzUzMTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/5175311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aidiss", "html_url": "https://github.com/aidiss", "followers_url": "https://api.github.com/users/aidiss/followers", "following_url": "https://api.github.com/users/aidiss/following{/other_user}", "gists_url": "https://api.github.com/users/aidiss/gists{/gist_id}", "starred_url": "https://api.github.com/users/aidiss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aidiss/subscriptions", "organizations_url": "https://api.github.com/users/aidiss/orgs", "repos_url": "https://api.github.com/users/aidiss/repos", "events_url": "https://api.github.com/users/aidiss/events{/privacy}", "received_events_url": "https://api.github.com/users/aidiss/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-08T16:19:39Z", "updated_at": "2019-11-21T16:23:15Z", "closed_at": "2019-11-21T16:23:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "after I modified settings.py according to scrapy_splash documentation.\r\n```\r\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy_crawlera.CrawleraMiddleware': 610,\r\n    'scrapy_splash.SplashCookiesMiddleware': 723,\r\n    'scrapy_splash.SplashMiddleware': 725,\r\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\r\n}\r\nSPIDER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\r\n}\r\n```\r\n\r\nThis error started to appear\r\n```\r\n2018-01-08 18:15:13 [scrapy.utils.signal] ERROR: Error caught on signal handler: <bound method ?.spider_opened of <scrapy.spidermiddlewares.offsite.OffsiteMiddleware object at 0x7f475a7630d0>>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/site-packages/twisted/internet/defer.py\", line 150, in maybeDeferred\r\n    result = f(*args, **kw)\r\n  File \"/usr/lib/python2.7/site-packages/pydispatch/robustapply.py\", line 55, in robustApply\r\n    return receiver(*arguments, **named)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 65, in spider_opened\r\n    self.host_regex = self.get_host_regex(spider)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy/spidermiddlewares/offsite.py\", line 58, in get_host_regex\r\n    if url_pattern.match(domain):\r\nTypeError: expected string or buffer\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/151", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/151/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/151/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/151/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/151", "id": 276224340, "node_id": "MDU6SXNzdWUyNzYyMjQzNDA=", "number": 151, "title": "Can't use Scrapy Splash with Firewall and Crawlera on", "user": {"login": "vionemc", "id": 6565672, "node_id": "MDQ6VXNlcjY1NjU2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6565672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vionemc", "html_url": "https://github.com/vionemc", "followers_url": "https://api.github.com/users/vionemc/followers", "following_url": "https://api.github.com/users/vionemc/following{/other_user}", "gists_url": "https://api.github.com/users/vionemc/gists{/gist_id}", "starred_url": "https://api.github.com/users/vionemc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vionemc/subscriptions", "organizations_url": "https://api.github.com/users/vionemc/orgs", "repos_url": "https://api.github.com/users/vionemc/repos", "events_url": "https://api.github.com/users/vionemc/events{/privacy}", "received_events_url": "https://api.github.com/users/vionemc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-11-22T22:43:58Z", "updated_at": "2019-11-21T16:23:36Z", "closed_at": "2019-11-21T16:23:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "So previously my server got hacked because I left Splash port open to public. I now use Firewall to my server address, and Scrapy Splash stop working with this error:\r\n\r\n```\r\n\r\n2017-11-22 22:12:45 [scrapy.core.scraper] ERROR: Error downloading <GET https://example.com via http://52.230.25.109:8050/execute>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python2.7/site-packages/twisted/internet/defer.py\", line 1299, in _inlineCallbacks\r\n    result = g.send(result)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 53, in process_response\r\n    spider=spider)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy_splash/middleware.py\", line 387, in process_response\r\n    response = self._change_response_class(request, response)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy_splash/middleware.py\", line 402, in _change_response_class\r\n    response = response.replace(cls=respcls, request=request)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy/http/response/text.py\", line 50, in replace\r\n    return Response.replace(self, *args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy/http/response/__init__.py\", line 79, in replace\r\n    return cls(*args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/scrapy_splash/response.py\", line 33, in __init__\r\n    super(_SplashResponseMixin, self).__init__(url, *args, **kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'encoding'\r\n```\r\n\r\nI also use Crawlera, so I think that's one factor. But I really need Crawlera?\r\nHow can I still make my crawler working but still have enough security? Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/150", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/150/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/150/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/150/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/150", "id": 275297407, "node_id": "MDU6SXNzdWUyNzUyOTc0MDc=", "number": 150, "title": "`execute` lua script set proxy fail", "user": {"login": "3xp10it", "id": 15134333, "node_id": "MDQ6VXNlcjE1MTM0MzMz", "avatar_url": "https://avatars3.githubusercontent.com/u/15134333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/3xp10it", "html_url": "https://github.com/3xp10it", "followers_url": "https://api.github.com/users/3xp10it/followers", "following_url": "https://api.github.com/users/3xp10it/following{/other_user}", "gists_url": "https://api.github.com/users/3xp10it/gists{/gist_id}", "starred_url": "https://api.github.com/users/3xp10it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/3xp10it/subscriptions", "organizations_url": "https://api.github.com/users/3xp10it/orgs", "repos_url": "https://api.github.com/users/3xp10it/repos", "events_url": "https://api.github.com/users/3xp10it/events{/privacy}", "received_events_url": "https://api.github.com/users/3xp10it/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-11-20T10:02:39Z", "updated_at": "2018-08-25T08:32:17Z", "closed_at": "2018-01-26T01:45:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "I try to set proxy in lua script with `execute`,below is my code:\r\n\r\n```\r\ndef get_random_proxy():\r\n    IPPOOL = eval(requests.get(\r\n        \"http://192.168.89.190:8000/?types=0&count=50&country=\u56fd\u5185\").text)\r\n    random_choose = random.choice(IPPOOL)\r\n    proxy_addr = \"http://\" + \\\r\n        str(random_choose[0]) + \":\" + str(random_choose[1])\r\n\r\n    return [str(random_choose[0]),random_choose[1]]\r\n\r\n\r\nclass Exp10itSpider(scrapy.Spider):\r\n    name = \"exp10it\"\r\n    collected_urls = []\r\n    domain = \"\"\r\n    start_url = \"\"\r\n    a=get_random_proxy()\r\n    # here print the proxy ip and port as a list\r\n    print(a)\r\n    lua_script = \"\"\"\r\n    function main(splash, args)\r\n      assert(splash:go{splash.args.url,http_method=splash.args.http_method,body=splash.args.body})\r\n      assert(splash:wait(0.5))\r\n\r\n      splash:on_request(function(request)\r\n          request:set_proxy{\r\n              host = \"%s\",\r\n              port = %d\r\n          }\r\n      end)\r\n\r\n      return splash:html()\r\n    end\r\n    \"\"\" % (a[0],a[1])\r\n\r\n    def start_requests(self):\r\n        urls = [\r\n            'http://httpbin.org/ip'\r\n        ]\r\n        self.domain = urlparse(urls[0]).hostname\r\n        self.start_url = urls[0]\r\n        for url in urls:\r\n              yield SplashRequest(url, self.parse_get, endpoint='execute',\r\n                                    magic_response=True, meta={'handle_httpstatus_all': True},\r\n                                    args={'lua_source': self.lua_script})\r\n\r\n```\r\n\r\nbelow is the output,but the output shows me I didn't set the proxy successfully,can you help me?\r\n\r\n```\r\n['101.53.101.172', 9999]\r\n...\r\n\r\n2017-11-20 17:49:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://httpbin.org/ip via http://192.168.89.190:8050/execute> (referer: None)\r\n<html><head></head><body><pre style=\"word-wrap: break-word; white-space: pre-wrap;\">{\r\n  \"origin\": \"115.174.68.89\"\r\n}\r\n</pre></body></html>\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/149", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/149/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/149/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/149/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/149", "id": 274046226, "node_id": "MDU6SXNzdWUyNzQwNDYyMjY=", "number": 149, "title": "it wasn't installed on azure vm ubuntu with manual way", "user": {"login": "JayStevency", "id": 13361980, "node_id": "MDQ6VXNlcjEzMzYxOTgw", "avatar_url": "https://avatars3.githubusercontent.com/u/13361980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JayStevency", "html_url": "https://github.com/JayStevency", "followers_url": "https://api.github.com/users/JayStevency/followers", "following_url": "https://api.github.com/users/JayStevency/following{/other_user}", "gists_url": "https://api.github.com/users/JayStevency/gists{/gist_id}", "starred_url": "https://api.github.com/users/JayStevency/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JayStevency/subscriptions", "organizations_url": "https://api.github.com/users/JayStevency/orgs", "repos_url": "https://api.github.com/users/JayStevency/repos", "events_url": "https://api.github.com/users/JayStevency/events{/privacy}", "received_events_url": "https://api.github.com/users/JayStevency/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-11-15T06:14:51Z", "updated_at": "2017-11-15T06:26:46Z", "closed_at": "2017-11-15T06:26:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi , I tried to install splash on azure vm ubuntu using manual way on docs. \r\nbut this shell script was not succeed because opt-qt591 couldn't install on server.\r\n\r\n```\r\nIgn:15 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main Translation-en_US\r\nIgn:16 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main Translation-en\r\nIgn:13 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main amd64 Packages\r\nIgn:14 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main all Packages\r\nIgn:15 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main Translation-en_US\r\nIgn:16 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main Translation-en\r\nErr:13 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main amd64 Packages\r\n  404  Not Found\r\nIgn:14 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main all Packages\r\nIgn:15 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main Translation-en_US\r\nIgn:16 http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial/main Translation-en\r\nFetched 102 kB in 11s (9,033 B/s)\r\nReading package lists...\r\nW: The repository 'http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu xenial Release' does not have a Release file.\r\nE: Failed to fetch http://ppa.launchpad.net/beineri/opt-qt591-trusty/ubuntu/dists/xenial/main/binary-amd64/Packages  404  Not Found\r\nE: Some index files failed to download. They have been ignored, or old ones used instead.\r\nCommand failed (exitcode: 100), exiting...\r\n```\r\n\r\nplz . let me know how can i do? thanks\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/148", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/148/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/148/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/148/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/148", "id": 272863426, "node_id": "MDU6SXNzdWUyNzI4NjM0MjY=", "number": 148, "title": "Caching issue?", "user": {"login": "deathemperor", "id": 4255482, "node_id": "MDQ6VXNlcjQyNTU0ODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/4255482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deathemperor", "html_url": "https://github.com/deathemperor", "followers_url": "https://api.github.com/users/deathemperor/followers", "following_url": "https://api.github.com/users/deathemperor/following{/other_user}", "gists_url": "https://api.github.com/users/deathemperor/gists{/gist_id}", "starred_url": "https://api.github.com/users/deathemperor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deathemperor/subscriptions", "organizations_url": "https://api.github.com/users/deathemperor/orgs", "repos_url": "https://api.github.com/users/deathemperor/repos", "events_url": "https://api.github.com/users/deathemperor/events{/privacy}", "received_events_url": "https://api.github.com/users/deathemperor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-10T09:01:01Z", "updated_at": "2019-05-16T04:14:11Z", "closed_at": "2019-05-16T04:14:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "I installed splash and got it running at http://localhost:8050 with ease.\r\n\r\nThe problem is:\r\n- When I try rendering this page https://shopee.com.my/Women's-Clothing-cat.16 the FIRST time it shows fine below\r\n![OK](https://i.snag.gy/ke5WKY.jpg) \r\n- But when I do it again it couldn't render the page correctly. Notice the differences in UI and a lot of red requests that seem to be errors but no information.\r\n![NOT OK](https://i.snag.gy/Lc2tfa.jpg) \r\n\r\nI've also attached the HAR data here https://pastebin.com/K63CMWQ5\r\n\r\nBut if I restart docker container of splash and run it again it works fine. In fact, If I restart docker before every run it works 100% of the time.\r\n\r\nI also notice that if I run it with error, then do nothing for a few min (about 5m+), it works again.\r\n\r\nThis happens only with this website. The site uses React. I tried other React sites it works fine.\r\n\r\nIs it about cache control?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/147", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/147/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/147/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/147/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/147", "id": 271020707, "node_id": "MDU6SXNzdWUyNzEwMjA3MDc=", "number": 147, "title": "SplashRequest missing errback parameter", "user": {"login": "verzola", "id": 2985522, "node_id": "MDQ6VXNlcjI5ODU1MjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2985522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/verzola", "html_url": "https://github.com/verzola", "followers_url": "https://api.github.com/users/verzola/followers", "following_url": "https://api.github.com/users/verzola/following{/other_user}", "gists_url": "https://api.github.com/users/verzola/gists{/gist_id}", "starred_url": "https://api.github.com/users/verzola/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/verzola/subscriptions", "organizations_url": "https://api.github.com/users/verzola/orgs", "repos_url": "https://api.github.com/users/verzola/repos", "events_url": "https://api.github.com/users/verzola/events{/privacy}", "received_events_url": "https://api.github.com/users/verzola/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-03T15:35:36Z", "updated_at": "2018-01-16T14:34:28Z", "closed_at": "2018-01-16T14:34:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to pass a callback to handle errors like the errback parameter of scrapy.http.Request?\r\n\r\nhttps://doc.scrapy.org/en/0.10.3/topics/request-response.html#scrapy.http.Request", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/146", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/146/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/146/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/146/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/146", "id": 266823634, "node_id": "MDU6SXNzdWUyNjY4MjM2MzQ=", "number": 146, "title": "how to get status code other than 200 from scrapy-splash", "user": {"login": "3xp10it", "id": 15134333, "node_id": "MDQ6VXNlcjE1MTM0MzMz", "avatar_url": "https://avatars3.githubusercontent.com/u/15134333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/3xp10it", "html_url": "https://github.com/3xp10it", "followers_url": "https://api.github.com/users/3xp10it/followers", "following_url": "https://api.github.com/users/3xp10it/following{/other_user}", "gists_url": "https://api.github.com/users/3xp10it/gists{/gist_id}", "starred_url": "https://api.github.com/users/3xp10it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/3xp10it/subscriptions", "organizations_url": "https://api.github.com/users/3xp10it/orgs", "repos_url": "https://api.github.com/users/3xp10it/repos", "events_url": "https://api.github.com/users/3xp10it/events{/privacy}", "received_events_url": "https://api.github.com/users/3xp10it/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-19T12:24:52Z", "updated_at": "2019-08-07T06:40:45Z", "closed_at": "2019-08-07T06:40:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to get request status code with scrapy and scrapy-splash,below is spider code.\r\n```\r\nclass Exp10itSpider(scrapy.Spider):\r\n    name = \"exp10it\"\r\n\r\n    def start_requests(self):\r\n        urls = [\r\n                'http://192.168.8.240:8000/xxxx' \r\n        ]\r\n        for url in urls:\r\n            #yield SplashRequest(url, self.parse, args={'wait': 0.5, 'dont_redirect': True},meta={'handle_httpstatus_all': True})\r\n            #yield scrapy.Request(url, self.parse, meta={'handle_httpstatus_all': True})\r\n            yield scrapy.Request(url, self.parse, meta={'handle_httpstatus_all': True,'splash': {\r\n                'args': {\r\n                    'html': 1,\r\n                    'png': 1,\r\n                    }\r\n            }\r\n            }\r\n            )\r\n\r\n\r\n    def parse(self, response):\r\n        input(\"start .........\")\r\n        print(\"status code is:\\n\")\r\n        input(response.status)\r\n```\r\n\r\nMy start url `http://192.168.8.240:8000/xxxx` is a 404 status code url,there are threee kinds of request way upon:\r\n\r\nthe first is:\r\n\r\n`yield SplashRequest(url, self.parse, args={'wait': 0.5, 'dont_redirect': True},meta={'handle_httpstatus_all': True})`\r\n\r\nthe second is:\r\n\r\n`yield scrapy.Request(url, self.parse, meta={'handle_httpstatus_all': True})`\r\n\r\nthe third is:\r\n```\r\n            yield scrapy.Request(url, self.parse, meta={'handle_httpstatus_all': True,'splash': {\r\n                'args': {\r\n                    'html': 1,\r\n                    'png': 1,\r\n                    }\r\n            }\r\n            }\r\n            )\r\n```\r\n\r\nOnly the second request way `yield scrapy.Request(url, self.parse, meta={'handle_httpstatus_all': True})` can get the right status code `404`,the first and the third both get status code `200`,that's to say,after I try to use scrapy-splash,I can not get the right status code `404`,can you help me?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/145", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/145/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/145/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/145/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/145", "id": 266626201, "node_id": "MDU6SXNzdWUyNjY2MjYyMDE=", "number": 145, "title": "Ecnoding Error", "user": {"login": "ijharulislam", "id": 10129259, "node_id": "MDQ6VXNlcjEwMTI5MjU5", "avatar_url": "https://avatars3.githubusercontent.com/u/10129259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ijharulislam", "html_url": "https://github.com/ijharulislam", "followers_url": "https://api.github.com/users/ijharulislam/followers", "following_url": "https://api.github.com/users/ijharulislam/following{/other_user}", "gists_url": "https://api.github.com/users/ijharulislam/gists{/gist_id}", "starred_url": "https://api.github.com/users/ijharulislam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ijharulislam/subscriptions", "organizations_url": "https://api.github.com/users/ijharulislam/orgs", "repos_url": "https://api.github.com/users/ijharulislam/repos", "events_url": "https://api.github.com/users/ijharulislam/events{/privacy}", "received_events_url": "https://api.github.com/users/ijharulislam/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-18T20:11:38Z", "updated_at": "2019-05-08T13:29:42Z", "closed_at": "2019-05-08T13:29:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI am using Scrapy-splash render.json endpoint. Some times I am getting this error.\r\nI saw similar issue on another thread but did not find any solution.\r\nhttps://github.com/scrapy-plugins/scrapy-splash/issues/107\r\n\r\n`TypeError: __init__() got an unexpected keyword argument 'encoding' `\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/144", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/144/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/144/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/144/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/144", "id": 262083371, "node_id": "MDU6SXNzdWUyNjIwODMzNzE=", "number": 144, "title": "url problem", "user": {"login": "mylha", "id": 32456521, "node_id": "MDQ6VXNlcjMyNDU2NTIx", "avatar_url": "https://avatars3.githubusercontent.com/u/32456521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mylha", "html_url": "https://github.com/mylha", "followers_url": "https://api.github.com/users/mylha/followers", "following_url": "https://api.github.com/users/mylha/following{/other_user}", "gists_url": "https://api.github.com/users/mylha/gists{/gist_id}", "starred_url": "https://api.github.com/users/mylha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mylha/subscriptions", "organizations_url": "https://api.github.com/users/mylha/orgs", "repos_url": "https://api.github.com/users/mylha/repos", "events_url": "https://api.github.com/users/mylha/events{/privacy}", "received_events_url": "https://api.github.com/users/mylha/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-10-02T13:24:04Z", "updated_at": "2019-11-21T16:29:23Z", "closed_at": "2019-11-21T16:29:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "My script is very simple and works for most urls. Unfortunately, in my spider above I get a redirection during the crawling and the screenshot I get is not the one I want (the order on the webpage is the default one and not the one I want).\r\n\r\nI tried to add \"splash_url=url\" in my splash request in order to force url but then I get a 404 error (\"HTTP status code is not handled or not allowed\"). I also tried meta options like dont_redirect=True but still it doesn't work.\r\n\r\nWhen I try the url on my browser ('http://192.168.99.100:8050/), I get the expected screenshot.\r\n\r\nCould you please help me with this ? I really don't understand what's going on here...\r\n\r\nHere is my spider : \r\n\r\n```\r\n    import scrapy\r\n    from scrapy_splash import SplashRequest\r\n\r\n    import base64\r\n    import os\r\n\r\n    class SplashtestSpider(scrapy.Spider):\r\n        name = 'splashtest'\r\n        allowed_domains = ['booking.com']\r\n        start_urls = ['https://www.booking.com/searchresults.fr.html?city=1456928;dest_type=city;ss=paris;order=score']\r\n\r\n        def parse(self, response):\r\n            for url in self.start_urls: \r\n                yield SplashRequest(url,self.parse_info,endpoint='render.json',#splash_url=url,\r\n                                args={'html':1,'png':1,'render_all':1,'wait':5})\r\n\r\n        def parse_info(self, response):\r\n\r\n            path = os.getcwd()+\"\\\\screenshot.png\"\r\n            png_bytes = base64.b64decode(response.data['png'])\r\n            fichierpng = open(path,\"wb\")\r\n            fichierpng.write(png_bytes)\r\n            fichierpng.close()\r\n```\r\n\r\nThe settings file :\r\n\r\n\r\n```\r\nBOT_NAME = 'SplashTest'\r\n\r\nSPIDER_MODULES = ['SplashTest.spiders']\r\nNEWSPIDER_MODULE = 'SplashTest.spiders'\r\n\r\nSPLASH_URL = 'http://192.168.99.100:8050/'\r\n\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashCookiesMiddleware': 723,\r\n    'scrapy_splash.SplashMiddleware': 725,\r\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\r\n}\r\n\r\n\r\n\r\nSPIDER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,\r\n}\r\n\r\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\r\nHTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\r\n\r\nROBOTSTXT_OBEY = True\r\n\r\nCONCURRENT_REQUESTS = 1\r\nCONCURRENT_REQUESTS_PER_DOMAIN = 1\r\n\r\nAUTOTHROTTLE_ENABLED = True\r\nAUTOTHROTTLE_START_DELAY = 5\r\nAUTOTHROTTLE_MAX_DELAY = 60\r\nAUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\r\n```\r\n\r\nAnd the log file:\r\n\r\n`2017-10-02 13:29:43 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: SplashTest)\r\n2017-10-02 13:29:43 [scrapy.utils.log] INFO: Overridden settings: {'AUTOTHROTTLE_ENABLED': True, 'BOT_NAME': 'SplashTest', 'CONCURRENT_REQUESTS': 1, 'CONCURRENT_REQUESTS_PER_DOMAIN': 1, 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage', 'LOG_FILE': 'log.txt', 'NEWSPIDER_MODULE': 'SplashTest.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['SplashTest.spiders']}\r\n2017-10-02 13:29:43 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.logstats.LogStats',\r\n 'scrapy.extensions.throttle.AutoThrottle']\r\n2017-10-02 13:29:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy_splash.SplashCookiesMiddleware',\r\n 'scrapy_splash.SplashMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2017-10-02 13:29:44 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy_splash.SplashDeduplicateArgsMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2017-10-02 13:29:44 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2017-10-02 13:29:44 [scrapy.core.engine] INFO: Spider opened\r\n2017-10-02 13:29:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-10-02 13:29:44 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-10-02 13:29:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.booking.com/robots.txt> (referer: None)\r\n2017-10-02 13:29:51 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://www.booking.com/searchresults.fr.html?city=1456928;dest_type=city;ss=paris> from <GET https://www.booking.com/searchresults.fr.html?city=1456928;dest_type=city;ss=paris;order=score>\r\n2017-10-02 13:29:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.booking.com/searchresults.fr.html?city=1456928;dest_type=city;ss=paris> (referer: None)\r\n2017-10-02 13:29:57 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://192.168.99.100:8050/robots.txt> (referer: None)\r\n2017-10-02 13:30:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.booking.com/searchresults.fr.html?city=1456928;dest_type=city;ss=paris;order=score via http://192.168.99.100:8050/render.json> (referer: None)\r\n2017-10-02 13:30:06 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2017-10-02 13:30:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 2202,\r\n 'downloader/request_count': 5,\r\n 'downloader/request_method_count/GET': 4,\r\n 'downloader/request_method_count/POST': 1,\r\n 'downloader/response_bytes': 3833046,\r\n 'downloader/response_count': 5,\r\n 'downloader/response_status_count/200': 3,\r\n 'downloader/response_status_count/301': 1,\r\n 'downloader/response_status_count/404': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2017, 10, 2, 11, 30, 6, 773931),\r\n 'log_count/DEBUG': 6,\r\n 'log_count/INFO': 7,\r\n 'request_depth_max': 1,\r\n 'response_received_count': 4,\r\n 'scheduler/dequeued': 4,\r\n 'scheduler/dequeued/memory': 4,\r\n 'scheduler/enqueued': 4,\r\n 'scheduler/enqueued/memory': 4,\r\n 'splash/render.json/request_count': 1,\r\n 'splash/render.json/response_count/200': 1,\r\n 'start_time': datetime.datetime(2017, 10, 2, 11, 29, 44, 45545)}\r\n2017-10-02 13:30:06 [scrapy.core.engine] INFO: Spider closed (finished)`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/143", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/143/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/143/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/143/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/143", "id": 261704711, "node_id": "MDU6SXNzdWUyNjE3MDQ3MTE=", "number": 143, "title": "HTTPS proxy does not support", "user": {"login": "Miando", "id": 15835632, "node_id": "MDQ6VXNlcjE1ODM1NjMy", "avatar_url": "https://avatars0.githubusercontent.com/u/15835632?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Miando", "html_url": "https://github.com/Miando", "followers_url": "https://api.github.com/users/Miando/followers", "following_url": "https://api.github.com/users/Miando/following{/other_user}", "gists_url": "https://api.github.com/users/Miando/gists{/gist_id}", "starred_url": "https://api.github.com/users/Miando/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Miando/subscriptions", "organizations_url": "https://api.github.com/users/Miando/orgs", "repos_url": "https://api.github.com/users/Miando/repos", "events_url": "https://api.github.com/users/Miando/events{/privacy}", "received_events_url": "https://api.github.com/users/Miando/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-09-29T17:03:56Z", "updated_at": "2017-09-29T18:10:51Z", "closed_at": "2017-09-29T17:39:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I try add httpS proxy, it not working\r\n`args={\r\n                    'proxy': 'https://35.185.80.76:3128',\r\n}`\r\n\r\n`Bad request to Splash: {'info': {'type': 'bad_argument', 'argument': 'proxy', 'description': 'Invalid proxy URL format.'}, 'description': 'Incorrect HTTP API arguments', 'error': 400, 'type': 'BadOption'}\r\n`\r\nMay be need use other way? Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/142", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/142/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/142/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/142/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/142", "id": 261043685, "node_id": "MDU6SXNzdWUyNjEwNDM2ODU=", "number": 142, "title": "splash.wait doesn't work inside Lua script when splash request endpint endpoint='render.json'", "user": {"login": "shafayeatsumit", "id": 9054649, "node_id": "MDQ6VXNlcjkwNTQ2NDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/9054649?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shafayeatsumit", "html_url": "https://github.com/shafayeatsumit", "followers_url": "https://api.github.com/users/shafayeatsumit/followers", "following_url": "https://api.github.com/users/shafayeatsumit/following{/other_user}", "gists_url": "https://api.github.com/users/shafayeatsumit/gists{/gist_id}", "starred_url": "https://api.github.com/users/shafayeatsumit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shafayeatsumit/subscriptions", "organizations_url": "https://api.github.com/users/shafayeatsumit/orgs", "repos_url": "https://api.github.com/users/shafayeatsumit/repos", "events_url": "https://api.github.com/users/shafayeatsumit/events{/privacy}", "received_events_url": "https://api.github.com/users/shafayeatsumit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-09-27T16:43:56Z", "updated_at": "2017-09-27T16:59:39Z", "closed_at": "2017-09-27T16:59:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "I posted detail in stackoverflow.\r\nhttps://stackoverflow.com/questions/46451549/splash-doesnt-wait-when-scrapy-endpoint-render-json", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/141", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/141/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/141/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/141/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/141", "id": 259395535, "node_id": "MDU6SXNzdWUyNTkzOTU1MzU=", "number": 141, "title": "mac works, but Ubuntu not", "user": {"login": "shaunrong", "id": 2459831, "node_id": "MDQ6VXNlcjI0NTk4MzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2459831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shaunrong", "html_url": "https://github.com/shaunrong", "followers_url": "https://api.github.com/users/shaunrong/followers", "following_url": "https://api.github.com/users/shaunrong/following{/other_user}", "gists_url": "https://api.github.com/users/shaunrong/gists{/gist_id}", "starred_url": "https://api.github.com/users/shaunrong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shaunrong/subscriptions", "organizations_url": "https://api.github.com/users/shaunrong/orgs", "repos_url": "https://api.github.com/users/shaunrong/repos", "events_url": "https://api.github.com/users/shaunrong/events{/privacy}", "received_events_url": "https://api.github.com/users/shaunrong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-21T07:04:47Z", "updated_at": "2017-09-21T10:15:05Z", "closed_at": "2017-09-21T08:20:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey, \r\n\r\nSorry to bother you again. I was able to run my codes with scrapy-splash on Mac, everything was fine.\r\n\r\nThen I thought about deploying the same codes on a Ubuntu server. I have docker, docker-machine all installed, splash was running. Then it always popped the following error messages:\r\n\r\n`ERROR: Error downloading <GET http://pubs.rsc.org/en/journals/journalissues/an#!issueid=an140001&type=current&issnprint=0003-2654 via http://192.168.99.100:8050/render.html>: Connection was refused by other side: 111: Connection refused.`\r\n\r\nI checked online. I thought it might be the firewall problem. So I specifically added port 8050 to the iptables:\r\n\r\n```\r\nChain INPUT (policy DROP)\r\ntarget     prot opt source               destination\r\nACCEPT     all  --  anywhere             anywhere\r\nACCEPT     all  --  anywhere             anywhere             state RELATED,ESTABLISHED\r\nACCEPT     tcp  --  anywhere             anywhere             tcp dpt:http\r\nACCEPT     tcp  --  wappa.mit.edu        anywhere             tcp dpt:ssh\r\nACCEPT     tcp  --  128.3.0.0/16         anywhere             tcp dpt:ssh\r\nACCEPT     tcp  --  128.3.0.0/16         anywhere             tcp dpt:27017\r\nACCEPT     tcp  --  131.243.220.0/22     anywhere             tcp dpt:ssh\r\nACCEPT     tcp  --  131.243.220.0/22     anywhere             tcp dpt:27017\r\nACCEPT     tcp  --  18.0.0.0/8           anywhere             tcp dpt:27017\r\nACCEPT     tcp  --  anywhere             anywhere             tcp dpt:8050\r\n\r\nChain FORWARD (policy DROP)\r\ntarget     prot opt source               destination\r\nDOCKER-USER  all  --  anywhere             anywhere\r\nDOCKER-ISOLATION  all  --  anywhere             anywhere\r\nACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED\r\nDOCKER     all  --  anywhere             anywhere\r\nACCEPT     all  --  anywhere             anywhere\r\nACCEPT     all  --  anywhere             anywhere\r\n\r\nChain OUTPUT (policy ACCEPT)\r\ntarget     prot opt source               destination\r\n\r\nChain DOCKER (1 references)\r\ntarget     prot opt source               destination\r\nACCEPT     tcp  --  anywhere             172.17.0.2           tcp dpt:8050\r\n\r\nChain DOCKER-ISOLATION (1 references)\r\ntarget     prot opt source               destination\r\nRETURN     all  --  anywhere             anywhere\r\n\r\nChain DOCKER-USER (1 references)\r\ntarget     prot opt source               destination\r\nRETURN     all  --  anywhere             anywhere\r\n\r\n```\r\n\r\nDo you have some clues what might went wrong? Any help is appreciated. Thank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/140", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/140/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/140/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/140/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/140", "id": 258679364, "node_id": "MDU6SXNzdWUyNTg2NzkzNjQ=", "number": 140, "title": "Non-html content type returns 502", "user": {"login": "p8a", "id": 2385530, "node_id": "MDQ6VXNlcjIzODU1MzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2385530?v=4", "gravatar_id": "", "url": "https://api.github.com/users/p8a", "html_url": "https://github.com/p8a", "followers_url": "https://api.github.com/users/p8a/followers", "following_url": "https://api.github.com/users/p8a/following{/other_user}", "gists_url": "https://api.github.com/users/p8a/gists{/gist_id}", "starred_url": "https://api.github.com/users/p8a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/p8a/subscriptions", "organizations_url": "https://api.github.com/users/p8a/orgs", "repos_url": "https://api.github.com/users/p8a/repos", "events_url": "https://api.github.com/users/p8a/events{/privacy}", "received_events_url": "https://api.github.com/users/p8a/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-19T02:49:42Z", "updated_at": "2019-11-21T16:31:03Z", "closed_at": "2019-11-21T16:31:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to build a generic crawler to capture a HAR containing the URL with all its referenced resources - some of which may be binary. The problem I'm running into is if the original URL returns content type other than HTML I get a 502 error back and I cannot figure out how to get pass that, I would like to get a HAR back regardless.\r\n\r\nCrawler code using plash, scrapy and scrapy-splash:\r\n`\r\n\r\n     for url in self._urls:\r\n            splash_args = {\r\n                'png': 1,\r\n                'har': 1,\r\n                'width': 600,\r\n                'wait': 0.5,\r\n                'headers': {}\r\n            }\r\n            splash_args['headers']['User-Agent'] = USER_AGENT\r\n            splash_args['headers']['Accept-Language'] = LOCALE_EN\r\n            yield SplashRequest(url, self.parse_result, endpoint='render.json',\r\n                                args=splash_args,\r\n                                slot_policy=SlotPolicy.SCRAPY_DEFAULT)\r\n`\r\n\r\nCrawling for https://download.sysinternals.com/files/ProcessExplorer.zip results in \r\n\r\n`\r\n[scrapy.spidermiddlewares.httperror] INFO: Ignoring response <502 https://download.sysinternals.com/files/ProcessExplorer.zip>: HTTP status code is not handled or not allowed\r\n`\r\n\r\nAny suggestions how to fix it ? Thanks\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/139", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/139/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/139/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/139/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/139", "id": 258381214, "node_id": "MDU6SXNzdWUyNTgzODEyMTQ=", "number": 139, "title": "JS EOF Error when sending long JS string to scrapy splash", "user": {"login": "fbuchinger", "id": 287434, "node_id": "MDQ6VXNlcjI4NzQzNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/287434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fbuchinger", "html_url": "https://github.com/fbuchinger", "followers_url": "https://api.github.com/users/fbuchinger/followers", "following_url": "https://api.github.com/users/fbuchinger/following{/other_user}", "gists_url": "https://api.github.com/users/fbuchinger/gists{/gist_id}", "starred_url": "https://api.github.com/users/fbuchinger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fbuchinger/subscriptions", "organizations_url": "https://api.github.com/users/fbuchinger/orgs", "repos_url": "https://api.github.com/users/fbuchinger/repos", "events_url": "https://api.github.com/users/fbuchinger/events{/privacy}", "received_events_url": "https://api.github.com/users/fbuchinger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-18T06:36:32Z", "updated_at": "2017-09-18T14:58:07Z", "closed_at": "2017-09-18T14:58:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following splash/lua script concatenates all style sheets of the loaded webpage to a string and returns in. It runs fine the splash demopage (localhost:8050):\r\n```lua\r\nfunction main(splash, args)\r\n  splash.images_enabled = false\r\n  local get_stylesheet_rules = splash:jsfunc([[\r\n  function () {\r\n       var cssout = [];\r\n       for (var i = 0; i < document.styleSheets.length; i++){\r\n        var curStyleSheet = document.styleSheets[i];\r\n            for (var j = 0; j < curStyleSheet.cssRules.length; j++){\r\n                cssout.push(curStyleSheet.cssRules[j].cssText);\r\n            }\r\n       }\r\n       return cssout.join('\\n');\r\n    }\r\n  ]])\r\n  splash:go(args.url)\r\n  splash:wait(0.5)\r\n  return get_stylesheet_rules ()\r\nend\r\n```\r\nWhen using the same code in scrapy-splash, my log gets filled up with the following errors:\r\n\r\n`\r\n2017-09-17 23:10:41 [scrapy_splash.middleware] WARNING: Bad request to Splash: {u'info': {u'js_error': u'SyntaxError: Unexpected EOF', u'js_error_type': u'SyntaxError', u'js_error_message': u'Unexpected EOF', u'error': u\"error during JS function call: 'SyntaxError: Unexpected EOF'\", u'message': u\"error during JS function call: 'SyntaxError: Unexpected EOF'\", u'type': u'JS_ERROR'}, u'type': u'ScriptError', u'description': u'Error happened while executing Lua script', u'error': 400}\r\n`\r\n\r\nand later\r\n\r\n`\r\n2017-09-17 23:10:41 [scrapy] DEBUG: Ignoring response <400 <webpageurl/>: HTTP status code is not handled or not allowed\r\n`\r\n\r\nAny hints?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/138", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/138/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/138/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/138/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/138", "id": 258287760, "node_id": "MDU6SXNzdWUyNTgyODc3NjA=", "number": 138, "title": "render.png bug", "user": {"login": "IaroslavR", "id": 9788811, "node_id": "MDQ6VXNlcjk3ODg4MTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/9788811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IaroslavR", "html_url": "https://github.com/IaroslavR", "followers_url": "https://api.github.com/users/IaroslavR/followers", "following_url": "https://api.github.com/users/IaroslavR/following{/other_user}", "gists_url": "https://api.github.com/users/IaroslavR/gists{/gist_id}", "starred_url": "https://api.github.com/users/IaroslavR/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IaroslavR/subscriptions", "organizations_url": "https://api.github.com/users/IaroslavR/orgs", "repos_url": "https://api.github.com/users/IaroslavR/repos", "events_url": "https://api.github.com/users/IaroslavR/events{/privacy}", "received_events_url": "https://api.github.com/users/IaroslavR/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-17T08:16:11Z", "updated_at": "2019-11-21T16:22:53Z", "closed_at": "2019-11-21T16:22:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Python 3.6.2, Scrapy 1.4.0, scrapy_splash 0.7.2,  splash from `docker pull scrapinghub/splash`  \r\nSpider:\r\n```python\r\nimport scrapy\r\nfrom scrapy_splash import SplashRequest\r\n\r\n\r\nclass TestSpider(scrapy.Spider):\r\n    name = 'test'\r\n\r\n    def start_requests(self):\r\n        splash_args = {\r\n            'png': 1,\r\n            'render_all': 1,\r\n            'wait': 2,\r\n        }\r\n        url = 'https://google.com'\r\n        yield SplashRequest(\r\n            url,\r\n            callback=self.parse_splash,\r\n            endpoint='render.png',\r\n            args=splash_args\r\n        )\r\n        yield scrapy.Request(\r\n            f\"http://localhost:8050/render.png?url={url}&wait=2&render_all=1\",\r\n            self.parse_request,\r\n        )\r\n\r\n    def parse_request(self, response):\r\n        with open('request.png', 'wb') as f:\r\n            f.write(response.body)\r\n\r\n    def parse_splash(self, response):\r\n        with open('splash.png', 'wb') as f:\r\n            f.write(response.body)\r\n```\r\nwith scrapy.Request all ok, but in splash.png I see [garbage](https://i.stack.imgur.com/dPTY4.png) instead of the Google page screenshot.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/137", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/137/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/137/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/137/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/137", "id": 256212810, "node_id": "MDU6SXNzdWUyNTYyMTI4MTA=", "number": 137, "title": "Scrapy + Splash not rendering page correctly", "user": {"login": "chairam", "id": 6491188, "node_id": "MDQ6VXNlcjY0OTExODg=", "avatar_url": "https://avatars3.githubusercontent.com/u/6491188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chairam", "html_url": "https://github.com/chairam", "followers_url": "https://api.github.com/users/chairam/followers", "following_url": "https://api.github.com/users/chairam/following{/other_user}", "gists_url": "https://api.github.com/users/chairam/gists{/gist_id}", "starred_url": "https://api.github.com/users/chairam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chairam/subscriptions", "organizations_url": "https://api.github.com/users/chairam/orgs", "repos_url": "https://api.github.com/users/chairam/repos", "events_url": "https://api.github.com/users/chairam/events{/privacy}", "received_events_url": "https://api.github.com/users/chairam/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-08T10:39:33Z", "updated_at": "2019-11-21T16:22:31Z", "closed_at": "2019-11-21T16:22:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Im' using Scrapy + Splash, I have problems downloading this page:\r\n[http://new.abb.com/jobs/it/center#JobCountry=IT&JobCity=any&JobFunction=any&JobRole=any&JobText=](http://new.abb.com/jobs/it/center#JobCountry=IT&JobCity=any&JobFunction=any&JobRole=any&JobText=)\r\n\r\nIt seems that Splash cannot execute the javascript correctly: in the list the spinner never goes away and the page numbers are not loaded. Here is a stripped down, working, self contanied, version of my program (sorry if not stripped down at best)\r\n\r\n\r\n```\r\n# -*- coding: utf-8 -*- \r\nimport scrapy from scrapy_splash \r\nimport SplashRequest from scrapy.selector \r\nimport Selector from scrapy.http \r\nimport HtmlResponse \r\nimport sys \r\nimport io \r\nimport os \r\nimport base64\r\n\r\ndef saveFile(ss, fileNameExt, folderName):\r\n    f = open(folderName + '/' + fileNameExt, 'w')\r\n    f.write(ss)\r\n    f.close()\r\n    return fileNameExt\r\n\r\ndef savePng(png_bytes, fileNameExt, folderName):\r\n    f = open( folderName +'/' + fileNameExt, 'wb')\r\n    f.write(png_bytes)\r\n    f.close()\r\n    return fileNameExt\r\n\r\ndef savePageOriginalInFolder(response, folderName, chiave='pag1'):\r\n    fileName = \"site.html\"\r\n    testo = response.data[chiave].decode('utf8')       \r\n    return saveFile(testo, fileName, folderName)       def savePagePng(response, folderName, pngDataName):\r\n    fileName = 'site.png'\r\n    if hasattr(response, 'data'):\r\n        png_bytes = base64.b64decode(response.data[pngDataName])\r\n        return savePng(png_bytes, fileName, folderName)\r\n\r\nclass GenericoSpider(scrapy.Spider):\r\n    name = 'provaAbb'\r\n\r\n    def asSplashRequest(self, url, callback, id_elenco=\"no_id\", id_sessione=\"no_id_sessione\"):\r\n        return SplashRequest(\r\n                    url = url,\r\n                    endpoint='execute',\r\n                    args={'lua_source': self.script, 'id_elenco': id_elenco, 'id_sessione': id_sessione},\r\n                    callback=callback,\r\n                )\r\n\r\n    outDir = name # prendo in nome della cartella dal nome dello spider\r\n    db_name = \"\"\r\n\r\n    def start_requests(self):   \r\n        sito = 'http://new.abb.com/jobs/it/center#JobCountry=IT&JobCity=any&JobFunction=any&JobRole=any&JobText='\r\n        yield self.asSplashRequest(sito, self.parse_list, 'id_mio_elenco')\r\n\r\n    script = \"\"\"\r\n    function main(splash)\r\n      local url = splash.args.url\r\n      splash:set_viewport_size(1280, 2500)      \r\n      splash:init_cookies(splash.args.cookies)\r\n      assert(splash:go(url))\r\n      assert(splash:wait(10))\r\n      return {\r\n        url  = splash:url(),\r\n        pag1 = splash:html(),\r\n        png1  = splash:png(),\r\n        id_elenco = splash.args.id_elenco,\r\n        id_sessione = splash.args.id_sessione,\r\n\r\n        cookies = splash:get_cookies(),\r\n        tt = splash.args\r\n      }\r\n    end\r\n    \"\"\"\r\n    def parse_list(self, response):\r\n            for ss in response.data:\r\n                if len(ss) >= 4:\r\n                    if ss[0:3] == 'pag':\r\n                        fileName = savePageOriginalInFolder(response, self.outDir, ss)\r\n                    elif ss[0:3] == 'png':\r\n                        fileName = savePagePng(response, self.outDir,ss)\r\n```\r\n\r\nA part of the settings.py (nothing exotic)\r\n```\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashCookiesMiddleware': 723,\r\n    'scrapy_splash.SplashMiddleware': 725,\r\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810, }\r\n\r\nSPIDER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100, }\r\n\r\nDUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'\r\n\r\nHTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/136", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/136/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/136/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/136/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/136", "id": 256091905, "node_id": "MDU6SXNzdWUyNTYwOTE5MDU=", "number": 136, "title": "scrapy-splash only renders part of the page", "user": {"login": "shaunrong", "id": 2459831, "node_id": "MDQ6VXNlcjI0NTk4MzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2459831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shaunrong", "html_url": "https://github.com/shaunrong", "followers_url": "https://api.github.com/users/shaunrong/followers", "following_url": "https://api.github.com/users/shaunrong/following{/other_user}", "gists_url": "https://api.github.com/users/shaunrong/gists{/gist_id}", "starred_url": "https://api.github.com/users/shaunrong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shaunrong/subscriptions", "organizations_url": "https://api.github.com/users/shaunrong/orgs", "repos_url": "https://api.github.com/users/shaunrong/repos", "events_url": "https://api.github.com/users/shaunrong/events{/privacy}", "received_events_url": "https://api.github.com/users/shaunrong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-07T22:44:30Z", "updated_at": "2017-09-07T23:36:17Z", "closed_at": "2017-09-07T23:36:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey,\r\n\r\nI was trying to web crawl some published articles, just the title for each article to do some NLP tasks later. From this website http://pubs.rsc.org/en/journals/journalissues/an#!issueid=an142017&type=current&issnprint=0003-2654\r\n\r\nThe code is fairly simple following your example:\r\n\r\n```\r\nclass RSCAnalystSpider(scrapy.Spider):\r\n    name = \"RSC_Analyst\"\r\n\r\n    with open(os.path.join(settings.REPO_ROOT_DIR, 'SynthesisPaper/spiders/start_urls.yaml'), 'r') as yf:\r\n        url_yaml = yaml.load(yf)\r\n\r\n    start_urls = [url_yaml[name.split(\"_\")[0]][name.split(\"_\")[1]]]\r\n\r\n    def start_requests(self):\r\n        for url in self.start_urls:\r\n            url = str(url)\r\n            yield SplashRequest(url, self.parse, args={'wait': 2})\r\n\r\n    def parse(self, response):\r\n        filename = 'Analyst.html'\r\n        with open(filename, 'wb') as f:\r\n            f.write(response.body)\r\n```\r\nstart_urls is `[http://pubs.rsc.org/en/journals/journalissues/an#!issueid=an142017&type=current&issnprint=0003-2654]`\r\nHowever, it was only able to save part of the page:\r\n![screen shot 2017-09-07 at 3 42 52 pm](https://user-images.githubusercontent.com/2459831/30188456-4a2b7ec4-93e3-11e7-92c4-c9d22eb8dca7.png)\r\n\r\nNo articles information shown. \r\n\r\nI am a new user of scrapy-splash. So please forgive me if this is not an issue but a stupid question.\r\n\r\nThank you!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/135", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/135/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/135/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/135/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/135", "id": 255558983, "node_id": "MDU6SXNzdWUyNTU1NTg5ODM=", "number": 135, "title": "Use rules and CrawlSpider together with splash", "user": {"login": "nysthee", "id": 11569296, "node_id": "MDQ6VXNlcjExNTY5Mjk2", "avatar_url": "https://avatars3.githubusercontent.com/u/11569296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nysthee", "html_url": "https://github.com/nysthee", "followers_url": "https://api.github.com/users/nysthee/followers", "following_url": "https://api.github.com/users/nysthee/following{/other_user}", "gists_url": "https://api.github.com/users/nysthee/gists{/gist_id}", "starred_url": "https://api.github.com/users/nysthee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nysthee/subscriptions", "organizations_url": "https://api.github.com/users/nysthee/orgs", "repos_url": "https://api.github.com/users/nysthee/repos", "events_url": "https://api.github.com/users/nysthee/events{/privacy}", "received_events_url": "https://api.github.com/users/nysthee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-09-06T10:41:20Z", "updated_at": "2017-09-20T11:34:33Z", "closed_at": "2017-09-20T11:34:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there a way I can use CrawlSpider rules (https://doc.scrapy.org/en/latest/topics/spiders.html#crawling-rules) together with splash? It doesn't seem to work out of the box.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/134", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/134/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/134/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/134/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/134", "id": 252835729, "node_id": "MDU6SXNzdWUyNTI4MzU3Mjk=", "number": 134, "title": "How to handle 302, 301 Page redirecttion", "user": {"login": "ijharulislam", "id": 10129259, "node_id": "MDQ6VXNlcjEwMTI5MjU5", "avatar_url": "https://avatars3.githubusercontent.com/u/10129259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ijharulislam", "html_url": "https://github.com/ijharulislam", "followers_url": "https://api.github.com/users/ijharulislam/followers", "following_url": "https://api.github.com/users/ijharulislam/following{/other_user}", "gists_url": "https://api.github.com/users/ijharulislam/gists{/gist_id}", "starred_url": "https://api.github.com/users/ijharulislam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ijharulislam/subscriptions", "organizations_url": "https://api.github.com/users/ijharulislam/orgs", "repos_url": "https://api.github.com/users/ijharulislam/repos", "events_url": "https://api.github.com/users/ijharulislam/events{/privacy}", "received_events_url": "https://api.github.com/users/ijharulislam/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-08-25T08:47:03Z", "updated_at": "2017-09-03T17:28:29Z", "closed_at": "2017-09-03T17:28:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "There is a website redirects url and It seems that splash can't handle that. How to handle 302, 301 status in splash?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/131", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/131/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/131/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/131/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/131", "id": 248753979, "node_id": "MDU6SXNzdWUyNDg3NTM5Nzk=", "number": 131, "title": "Only the first url is being rendered", "user": {"login": "milos-simic", "id": 13156982, "node_id": "MDQ6VXNlcjEzMTU2OTgy", "avatar_url": "https://avatars2.githubusercontent.com/u/13156982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/milos-simic", "html_url": "https://github.com/milos-simic", "followers_url": "https://api.github.com/users/milos-simic/followers", "following_url": "https://api.github.com/users/milos-simic/following{/other_user}", "gists_url": "https://api.github.com/users/milos-simic/gists{/gist_id}", "starred_url": "https://api.github.com/users/milos-simic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/milos-simic/subscriptions", "organizations_url": "https://api.github.com/users/milos-simic/orgs", "repos_url": "https://api.github.com/users/milos-simic/repos", "events_url": "https://api.github.com/users/milos-simic/events{/privacy}", "received_events_url": "https://api.github.com/users/milos-simic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-08T15:26:24Z", "updated_at": "2019-11-21T16:21:51Z", "closed_at": "2019-11-21T16:21:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "For each Disqus url I want to scrape name and usernames of the followers. However, the first url is being rendered regardless of the actual url in the request.\r\n\r\nHere is my spider:\r\n\r\n```\r\nimport scrapy\r\nfrom disqus.items import DisqusItem\r\n\r\nclass DisqusSpider(scrapy.Spider):\r\n    name = \"disqusSpider\"\r\n    start_urls = [\"https://disqus.com/by/disqus_sAggacVY39/\", \"https://disqus.com/by/VladimirUlayanov/\", \"https://disqus.com/by/Beasleyhillman/\", \"https://disqus.com/by/Slick312/\"]\r\n    splash_def = {\"endpoint\" : \"render.html\", \"args\" : {\"wait\" : 10}}\r\n\r\n    def start_requests(self):\r\n        for url in self.start_urls:\r\n            yield scrapy.Request(url = url, callback = self.parse_basic, dont_filter = True, meta = {\r\n                \"splash\" : self.splash_def,\r\n                \"base_profile_url\" : url\r\n            })\r\n\r\n    def parse_basic(self, response):\r\n        name = response.css(\"h1.cover-profile-name.text-largest.truncate-line::text\").extract_first()\r\n        disqusItem = DisqusItem(name = name)\r\n        request = scrapy.Request(url = response.meta[\"base_profile_url\"] + \"followers/\", callback = self.parse_followers, dont_filter = True, meta = {\r\n            \"item\" : disqusItem,\r\n            \"base_profile_url\" : response.meta[\"base_profile_url\"],\r\n            \"splash\": self.splash_def\r\n        })\r\n        print \"parse_basic\", response.url, request.url\r\n        yield request\r\n\r\n    def parse_followers(self, response):\r\n        print \"parse_followers\", response.meta[\"base_profile_url\"], response.meta[\"item\"]\r\n        followers = response.css(\"div.user-info a::attr(href)\").extract()\r\n```\r\nThis is the definition of `DisqusItem`:\r\n\r\n```\r\nimport scrapy\r\nclass DisqusItem(scrapy.Item):\r\n    name = scrapy.Field()\r\n    followers = scrapy.Field()\r\n```\r\n\r\nHere are the settings:\r\n\r\n```\r\n# -*- coding: utf-8 -*-\r\n\r\n# Scrapy settings for disqus project\r\n#\r\n\r\nBOT_NAME = 'disqus'\r\n\r\nSPIDER_MODULES = ['disqus.spiders']\r\nNEWSPIDER_MODULE = 'disqus.spiders'\r\n\r\nROBOTSTXT_OBEY = False\r\n\r\nSPLASH_URL = 'http://localhost:8050' \r\n\r\nDOWNLOADER_MIDDLEWARES = {\r\n    'scrapy_splash.SplashCookiesMiddleware': 723,\r\n    'scrapy_splash.SplashMiddleware': 725,\r\n    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,\r\n}\r\n\r\nDUPEFILTER_CLASS = 'scrapyjs.SplashAwareDupeFilter'\r\nDUPEFILTER_DEBUG = True\r\n\r\nDOWNLOAD_DELAY = 10\r\n```\r\n\r\nWhen `parse_followers` gets executed, `response.meta[\"item\"]` is always the same. Here is the log:\r\n\r\n```\r\n2017-08-08 17:09:34 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_basic https://disqus.com/by/disqus_sAggacVY39/ https://disqus.com/by/disqus_sAggacVY39/followers/\r\n2017-08-08 17:09:42 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_basic https://disqus.com/by/disqus_sAggacVY39/ https://disqus.com/by/VladimirUlayanov/followers/\r\n2017-08-08 17:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_basic https://disqus.com/by/disqus_sAggacVY39/ https://disqus.com/by/Beasleyhillman/followers/\r\n2017-08-08 17:10:09 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_basic https://disqus.com/by/disqus_sAggacVY39/ https://disqus.com/by/Slick312/followers/\r\n2017-08-08 17:10:21 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_followers https://disqus.com/by/disqus_sAggacVY39/ {'name': u'Trailer Trash'}\r\n2017-08-08 17:10:21 [scrapy.extensions.logstats] INFO: Crawled 5 pages (at 5 pages/min), scraped 0 items (at 0 items/min)\r\n2017-08-08 17:10:36 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_followers https://disqus.com/by/VladimirUlayanov/ {'name': u'Trailer Trash'}\r\n2017-08-08 17:10:50 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_followers https://disqus.com/by/Beasleyhillman/ {'name': u'Trailer Trash'}\r\n2017-08-08 17:11:03 [scrapy.core.engine] DEBUG: Crawled (200) <POST http://localhost:8050/render.html> (referer: None)\r\nparse_followers https://disqus.com/by/Slick312/ {'name': u'Trailer Trash'}\r\n2017-08-08 17:11:03 [scrapy.core.engine] INFO: Closing spider (finished)\r\n\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/130", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/130/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/130/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/130/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/130", "id": 248560233, "node_id": "MDU6SXNzdWUyNDg1NjAyMzM=", "number": 130, "title": "Spiders fail when run with scrapyd", "user": {"login": "ntindicator", "id": 30362690, "node_id": "MDQ6VXNlcjMwMzYyNjkw", "avatar_url": "https://avatars2.githubusercontent.com/u/30362690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ntindicator", "html_url": "https://github.com/ntindicator", "followers_url": "https://api.github.com/users/ntindicator/followers", "following_url": "https://api.github.com/users/ntindicator/following{/other_user}", "gists_url": "https://api.github.com/users/ntindicator/gists{/gist_id}", "starred_url": "https://api.github.com/users/ntindicator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ntindicator/subscriptions", "organizations_url": "https://api.github.com/users/ntindicator/orgs", "repos_url": "https://api.github.com/users/ntindicator/repos", "events_url": "https://api.github.com/users/ntindicator/events{/privacy}", "received_events_url": "https://api.github.com/users/ntindicator/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-08-07T23:35:58Z", "updated_at": "2018-07-18T07:53:41Z", "closed_at": "2018-07-18T07:53:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using scrapyd to schedule multiple spiders, and it works well until it's run with the scrapy-splash spiders. After some time everything stops working, and it ceases to render the pages. It seems to be not responding at all, and the only way to stop it is by interrupting with the keyboard. Cancelling the job has also worked after some time. I've tried making alterations to the scrapyd configuration, ensuring that only 1 spider is running at a time, and limiting the concurrent requests, but the issue is persisting, and I can only run the splash spiders individually without using scrapyd. Scrapyd is also running on localhost, so it could be that there's not enough resources available to run scrapyd and splash at the same time. Is there anything that can be done to ensure that multiple spiders will run with scrapy-splash and scrapyd?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/129", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/129/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/129/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/129/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/129", "id": 244929745, "node_id": "MDU6SXNzdWUyNDQ5Mjk3NDU=", "number": 129, "title": "Set max-timeout to remote Splash server", "user": {"login": "ijharulislam", "id": 10129259, "node_id": "MDQ6VXNlcjEwMTI5MjU5", "avatar_url": "https://avatars3.githubusercontent.com/u/10129259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ijharulislam", "html_url": "https://github.com/ijharulislam", "followers_url": "https://api.github.com/users/ijharulislam/followers", "following_url": "https://api.github.com/users/ijharulislam/following{/other_user}", "gists_url": "https://api.github.com/users/ijharulislam/gists{/gist_id}", "starred_url": "https://api.github.com/users/ijharulislam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ijharulislam/subscriptions", "organizations_url": "https://api.github.com/users/ijharulislam/orgs", "repos_url": "https://api.github.com/users/ijharulislam/repos", "events_url": "https://api.github.com/users/ijharulislam/events{/privacy}", "received_events_url": "https://api.github.com/users/ijharulislam/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-23T19:10:55Z", "updated_at": "2017-07-23T22:08:25Z", "closed_at": "2017-07-23T22:08:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "I bought a small instance of Splash. I am not using docker. How to set max-timeout for remote splash server?\r\nI am trying to pass as args. But it is not working.\r\n\r\nargs={'lua_source': script, 'max-timeout':3600}", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/128", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/128/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/128/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/128/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/128", "id": 244847626, "node_id": "MDU6SXNzdWUyNDQ4NDc2MjY=", "number": 128, "title": "Splash Time out Issue", "user": {"login": "ijharulislam", "id": 10129259, "node_id": "MDQ6VXNlcjEwMTI5MjU5", "avatar_url": "https://avatars3.githubusercontent.com/u/10129259?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ijharulislam", "html_url": "https://github.com/ijharulislam", "followers_url": "https://api.github.com/users/ijharulislam/followers", "following_url": "https://api.github.com/users/ijharulislam/following{/other_user}", "gists_url": "https://api.github.com/users/ijharulislam/gists{/gist_id}", "starred_url": "https://api.github.com/users/ijharulislam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ijharulislam/subscriptions", "organizations_url": "https://api.github.com/users/ijharulislam/orgs", "repos_url": "https://api.github.com/users/ijharulislam/repos", "events_url": "https://api.github.com/users/ijharulislam/events{/privacy}", "received_events_url": "https://api.github.com/users/ijharulislam/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-22T13:34:24Z", "updated_at": "2019-07-04T12:14:07Z", "closed_at": "2017-07-22T20:46:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Here is my Lua script. I am getting time out error. \r\n\r\n```\r\ntreat = require(\"treat\")\r\nfunction main(splash)\r\n  local url = splash.args.url\r\n  assert(splash:go(url))\r\n  local links = splash:select_all('a[onclick]')\r\n  count = 1\r\n  local results = {}\r\n  for i, v in ipairs( links ) do\r\n    splash:go(url)\r\n    local links = splash:select_all('a[onclick]')\r\n    obj = {}\r\n    count = count + 1\r\n    links[i]:click()\r\n    obj[\"html\"] = splash:html()\r\n    obj[\"url\"] = splash:evaljs(\"window.location.href\")\r\n    results[#results+1] = obj\r\n \r\n  end\r\n  return {\r\n    html = splash:html(),\r\n    results = treat.as_array(results),\r\n    count = count\r\n  }\r\nend\r\n```\r\n\r\nHere is the error message. \r\n```\r\n{\r\n    \"error\": 504,\r\n    \"type\": \"GlobalTimeoutError\",\r\n    \"description\": \"Timeout exceeded rendering page\",\r\n    \"info\": {\r\n        \"timeout\": 60\r\n    }\r\n}\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/127", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/127/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/127/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/127/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/127", "id": 244675360, "node_id": "MDU6SXNzdWUyNDQ2NzUzNjA=", "number": 127, "title": "Js cookies ddos protection with scrapy", "user": {"login": "akudelka", "id": 7499565, "node_id": "MDQ6VXNlcjc0OTk1NjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/7499565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akudelka", "html_url": "https://github.com/akudelka", "followers_url": "https://api.github.com/users/akudelka/followers", "following_url": "https://api.github.com/users/akudelka/following{/other_user}", "gists_url": "https://api.github.com/users/akudelka/gists{/gist_id}", "starred_url": "https://api.github.com/users/akudelka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akudelka/subscriptions", "organizations_url": "https://api.github.com/users/akudelka/orgs", "repos_url": "https://api.github.com/users/akudelka/repos", "events_url": "https://api.github.com/users/akudelka/events{/privacy}", "received_events_url": "https://api.github.com/users/akudelka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-21T13:52:09Z", "updated_at": "2019-11-21T16:20:53Z", "closed_at": "2019-11-21T16:20:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! I have some trouble.\r\nOn this site [http://rubrikator.org](http://rubrikator.org) if disable js retrun:\r\n\r\n```\r\n<html>\r\n--\r\n\u00a0 | <head>\r\n\u00a0 | <meta http-equiv=\"Content-Type\" content=\"text/html; \" />\r\n\u00a0 | <script>\r\n\u00a0 | document.cookie='esteq_ddos_intercepter=b4d16037b21d8507041dd96ccb00f16b; max-age=604800; path=/';\r\n\u00a0 | var nc = function() {return document.cookie.indexOf('esteq_ddos_intercepter=b4d16037b21d8507041dd96ccb00f16b')==-1;};\r\n\u00a0 | var w = function() {document.body.innerHTML = document.getElementsByTagName('noscript')[0].textContent;};\r\n\u00a0 | if (!window.opera) {\r\n\u00a0 | if (!nc()) {window.location.reload(true);}\r\n\u00a0 | var r = function() {if (nc()) w();};\r\n\u00a0 | } else {\r\n\u00a0 | var r = function () {\r\n\u00a0 | if (!nc()) {window.location.reload(true);}\r\n\u00a0 | else {w();}\r\n\u00a0 | }\r\n\u00a0 | }\r\n\u00a0 | </script>\r\n\u00a0 | </head>\r\n\u00a0 | <body onload=\"r()\">\r\n\u00a0 | <noscript>You have to turn on javascript and cookies support in browser to visit this site.<br/>\r\n\u00a0 | <a href=\"http://esteq.net\">EsteQ</a>\r\n\u00a0 | </noscript>\r\n\u00a0 | </body>\r\n\u00a0 | </html>\r\n```\r\nhow parse pages with protection like this using scrapy-splash??\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/125", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/125/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/125/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/125/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/125", "id": 238864884, "node_id": "MDU6SXNzdWUyMzg4NjQ4ODQ=", "number": 125, "title": "Required argument is missing: url", "user": {"login": "sarbazx", "id": 5629946, "node_id": "MDQ6VXNlcjU2Mjk5NDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/5629946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sarbazx", "html_url": "https://github.com/sarbazx", "followers_url": "https://api.github.com/users/sarbazx/followers", "following_url": "https://api.github.com/users/sarbazx/following{/other_user}", "gists_url": "https://api.github.com/users/sarbazx/gists{/gist_id}", "starred_url": "https://api.github.com/users/sarbazx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sarbazx/subscriptions", "organizations_url": "https://api.github.com/users/sarbazx/orgs", "repos_url": "https://api.github.com/users/sarbazx/repos", "events_url": "https://api.github.com/users/sarbazx/events{/privacy}", "received_events_url": "https://api.github.com/users/sarbazx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2017-06-27T14:11:28Z", "updated_at": "2020-01-07T12:58:21Z", "closed_at": "2017-09-22T09:29:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "`{\"info\": {\"argument\": \"url\", \"type\": \"argument_required\", \"description\": \"Required argument is missing: url\"}, \"type\": \"BadOption\", \"error\": 400, \"description\": \"Incorrect HTTP API arguments\"}`\r\n\r\ngetting this error with this splash request like this:\r\n\r\n```\r\nstart_urls = [\"http://www.gittigidiyor.com/\"]\r\n\r\n    def start_requests(self):\r\n        for url in self.start_urls:\r\n            yield SplashRequest(url, self.parse, args={'wait': 1.5, 'http_method': 'POST'}, endpoint='render.json')\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/124", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/124/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/124/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/124/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/124", "id": 238623831, "node_id": "MDU6SXNzdWUyMzg2MjM4MzE=", "number": 124, "title": "Splash only renders the last url in the list", "user": {"login": "nwihardjo", "id": 19996695, "node_id": "MDQ6VXNlcjE5OTk2Njk1", "avatar_url": "https://avatars2.githubusercontent.com/u/19996695?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nwihardjo", "html_url": "https://github.com/nwihardjo", "followers_url": "https://api.github.com/users/nwihardjo/followers", "following_url": "https://api.github.com/users/nwihardjo/following{/other_user}", "gists_url": "https://api.github.com/users/nwihardjo/gists{/gist_id}", "starred_url": "https://api.github.com/users/nwihardjo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nwihardjo/subscriptions", "organizations_url": "https://api.github.com/users/nwihardjo/orgs", "repos_url": "https://api.github.com/users/nwihardjo/repos", "events_url": "https://api.github.com/users/nwihardjo/events{/privacy}", "received_events_url": "https://api.github.com/users/nwihardjo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-06-26T18:05:43Z", "updated_at": "2019-11-21T16:20:04Z", "closed_at": "2019-11-21T16:20:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Previously on [stackoverflow](https://stackoverflow.com/questions/44764954/scrapy-only-render-the-last-url-in-a-list-of-url). It seems that my splash only able to get the content from the last rendered url in the url lists. below is my code:\r\n\r\n```\r\nimport scrapy\r\nfrom scrapy_splash import ScrapyRequest\r\nclass e27_spider(scrapy.Spider):\r\n\tname = 'e27'\r\n\tstart_urls = ['http://www.e27.co/startup/urban-ladder','http://www.e27.co/startup/flipkart']\r\n\tdef start_requests(self):\r\n\t\tfor url in self.start_urls:\r\n\t\t\tyield SplashRequest(url = url, callback = self.parse, args={'http_method': 'GET','follow_redirects':False})\r\n\t\t\r\n\tdef parse(self, response):\r\n\t\tfilename = 'asd-%s.html' % response.xpath('//*[@id=\"page-container\"]/div[3]/div/div/div/div/div/div[2]/div[1]/div/h1/text()').extract_first()\r\n\t\twith open(filename, 'wb') as f:\r\n\t\t\tf.write(response.body)\r\n\r\n\t\tfinal_data = {}\r\n\t\tstartup_name = response.xpath('//*[@id=\"page-container\"]/div[3]/div/div/div/div/div/div[2]/div[1]/div/h1/text()').extract_first()\r\n\t\tstartup_founding_date = response.xpath('//*[@id=\"page-container\"]/div[3]/div/div/div/div/div/div[2]/div[1]/div/p[3]/span/text()').extract_first()\r\n\t\tstartup_description = response.xpath('//*[@id=\"page-container\"]/div[4]/div/div/div/div/div[1]/div[1]/div[1]/div/p/text()').extract_first()\r\n\t\tfinal_data['Name'] = startup_name\r\n\t\tfinal_data['Founding Date'] = startup_founding_date\r\n\t\tfinal_data['Description'] = startup_description\r\n\r\n\t\tprint (final_data)\r\n```\r\n\r\nand here's the output:\r\n```\r\n2017-06-27 02:03:40 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: webcrawler)\r\n2017-06-27 02:03:40 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'webcrawler', 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage', 'NEWSPIDER_MODULE': 'webcrawler.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['webcrawler.spiders']}\r\n2017-06-27 02:03:40 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2017-06-27 02:03:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy_splash.SplashCookiesMiddleware',\r\n 'scrapy_splash.SplashMiddleware',\r\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2017-06-27 02:03:40 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy_splash.SplashDeduplicateArgsMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2017-06-27 02:03:40 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2017-06-27 02:03:40 [scrapy.core.engine] INFO: Spider opened\r\n2017-06-27 02:03:40 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-06-27 02:03:40 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-06-27 02:03:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.e27.co/robots.txt> (referer: None)\r\n2017-06-27 02:03:40 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://192.168.99.100:8050/robots.txt> (referer: None)\r\n2017-06-27 02:03:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.e27.co/startup/urban-ladder via http://192.168.99.100:8050/render.html> (referer: None)\r\n{'Name': None, 'Founding Date': None, 'Description': None}\r\n2017-06-27 02:03:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.e27.co/startup/flipkart via http://192.168.99.100:8050/render.html> (referer: None)\r\n{'Name': 'Flipkart', 'Founding Date': 'October 2007', 'Description': '\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFlipkart is India\u2019s leading ecommerce marketplace offering more than 30 million products cross 70+ categories including Books, Media, Consumer Electronics and Lifestyle. '}\r\n2017-06-27 02:03:43 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2017-06-27 02:03:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/request_bytes': 1878,\r\n 'downloader/request_count': 4,\r\n 'downloader/request_method_count/GET': 2,\r\n 'downloader/request_method_count/POST': 2,\r\n 'downloader/response_bytes': 229858,\r\n 'downloader/response_count': 4,\r\n 'downloader/response_status_count/200': 3,\r\n 'downloader/response_status_count/404': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2017, 6, 26, 18, 3, 43, 667545),\r\n 'log_count/DEBUG': 5,\r\n 'log_count/INFO': 7,\r\n 'response_received_count': 4,\r\n 'scheduler/dequeued': 4,\r\n 'scheduler/dequeued/memory': 4,\r\n 'scheduler/enqueued': 4,\r\n 'scheduler/enqueued/memory': 4,\r\n 'splash/render.html/request_count': 2,\r\n 'splash/render.html/response_count/200': 2,\r\n 'start_time': datetime.datetime(2017, 6, 26, 18, 3, 40, 209629)}\r\n2017-06-27 02:03:43 [scrapy.core.engine] INFO: Spider closed (finished)\r\n```\r\n\r\nany ideas why only one url content received correctly?? thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/123", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/123/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/123/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/123/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/123", "id": 236839246, "node_id": "MDU6SXNzdWUyMzY4MzkyNDY=", "number": 123, "title": "how to set splash timeout", "user": {"login": "JhonSmith0x7b", "id": 15826416, "node_id": "MDQ6VXNlcjE1ODI2NDE2", "avatar_url": "https://avatars2.githubusercontent.com/u/15826416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JhonSmith0x7b", "html_url": "https://github.com/JhonSmith0x7b", "followers_url": "https://api.github.com/users/JhonSmith0x7b/followers", "following_url": "https://api.github.com/users/JhonSmith0x7b/following{/other_user}", "gists_url": "https://api.github.com/users/JhonSmith0x7b/gists{/gist_id}", "starred_url": "https://api.github.com/users/JhonSmith0x7b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JhonSmith0x7b/subscriptions", "organizations_url": "https://api.github.com/users/JhonSmith0x7b/orgs", "repos_url": "https://api.github.com/users/JhonSmith0x7b/repos", "events_url": "https://api.github.com/users/JhonSmith0x7b/events{/privacy}", "received_events_url": "https://api.github.com/users/JhonSmith0x7b/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-06-19T10:01:19Z", "updated_at": "2019-07-04T11:30:11Z", "closed_at": "2017-06-19T11:09:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "I run a splash server in the docker.\r\ncommond: `docker run  -p 8050:8050 scrapinghub/splash --max-timeout 3600`\r\nBut I got a 504 error.\r\n`\"error\": {\"info\": {\"timeout\": 30}, \"description\": \"Timeout exceeded rendering page\", \"error\": 504, \"type\": \"GlobalTimeoutError\"}`\r\n\r\nAlthough I try to add `splash.resource_timeout`, `request:set_timeout` and `SPLASH_URL = 'http://localhost:8050?timeout=1800.0'`, nothing changed.\r\nThanks for help.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/122", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/122/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/122/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/122/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/122", "id": 236412194, "node_id": "MDU6SXNzdWUyMzY0MTIxOTQ=", "number": 122, "title": "D-Bus library appears to be incorrectly set up", "user": {"login": "ryan715", "id": 1661606, "node_id": "MDQ6VXNlcjE2NjE2MDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1661606?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ryan715", "html_url": "https://github.com/ryan715", "followers_url": "https://api.github.com/users/ryan715/followers", "following_url": "https://api.github.com/users/ryan715/following{/other_user}", "gists_url": "https://api.github.com/users/ryan715/gists{/gist_id}", "starred_url": "https://api.github.com/users/ryan715/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ryan715/subscriptions", "organizations_url": "https://api.github.com/users/ryan715/orgs", "repos_url": "https://api.github.com/users/ryan715/repos", "events_url": "https://api.github.com/users/ryan715/events{/privacy}", "received_events_url": "https://api.github.com/users/ryan715/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-06-16T08:16:55Z", "updated_at": "2019-11-21T16:23:25Z", "closed_at": "2019-11-21T16:23:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Guys.\r\nI have scrapied some websites with splash, I got a error message these days, but I cannot solve it. any help? \r\n`\r\n2017-06-16 08:12:17+0000 [-] Log opened.\r\n2017-06-16 08:12:17.591094 [-] Splash version: 2.3.3\r\n2017-06-16 08:12:17.591519 [-] Qt 5.5.1, PyQt 5.5.1, WebKit 538.1, sip 4.17, Twisted 16.1.1, Lua 5.2\r\n2017-06-16 08:12:17.591622 [-] Python 3.4.3 (default, Nov 17 2016, 01:08:31) [GCC 4.8.4]\r\n2017-06-16 08:12:17.591694 [-] Open files limit: 1048576\r\n2017-06-16 08:12:17.591801 [-] Can't bump open files limit\r\n2017-06-16 08:12:17.696192 [-] Xvfb is started: ['Xvfb', ':1', '-screen', '0', '1024x768x24', '-nolisten', 'tcp']\r\n2017-06-16 08:12:17.755154 [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles\r\n2017-06-16 08:12:17.845871 [-] verbosity=1\r\n2017-06-16 08:12:17.846008 [-] slots=50\r\n2017-06-16 08:12:17.846133 [-] argument_cache_max_entries=500\r\n2017-06-16 08:12:17.846635 [-] Web UI: enabled, Lua: enabled (sandbox: enabled)\r\n2017-06-16 08:12:17.849314 [-] Site starting on 8050\r\n2017-06-16 08:12:17.849485 [-] Starting factory <twisted.web.server.Site object at 0x7fea4330e240>\r\nprocess 1: D-Bus library appears to be incorrectly set up; failed to read machine uuid: Failed to open \"/etc/machine-id\": No such file or directory\r\nSee the manual page for dbus-uuidgen to correct this issue.\r\nload glyph failed err=6 face=0x2652380, glyph=2797\r\n1   0x7fea8fd2cee7 /opt/qt55/lib/libQt5WebKit.so.5(WTFCrash+0x17) [0x7fea8fd2cee7]\r\n2   0x7fea8fd0bf95 /opt/qt55/lib/libQt5WebKit.so.5(_ZN3JSC9Structure22materializePropertyMapERNS_2VME+0x715) [0x7fea8fd0bf95]\r\n3   0x7fea8fc97f30 /opt/qt55/lib/libQt5WebKit.so.5(_ZN3JSC14JSGlobalObject18getOwnPropertySlotEPNS_6JSCellEPNS_9ExecStateENS_12PropertyNameERNS_12PropertySlotE+0x240) [0x7fea8fc97f30]\r\n4   0x7fea8f8feedb /opt/qt55/lib/libQt5WebKit.so.5(+0x151aedb) [0x7fea8f8feedb]\r\n5   0x7fea8fcc240a /opt/qt55/lib/libQt5WebKit.so.5(+0x18de40a) [0x7fea8fcc240a]\r\n6   0x7fea8fcbffdc /opt/qt55/lib/libQt5WebKit.so.5(+0x18dbfdc) [0x7fea8fcbffdc]\r\n7   0x7fea8fbaaa1a /opt/qt55/lib/libQt5WebKit.so.5(+0x17c6a1a) [0x7fea8fbaaa1a]\r\n8   0x7fea8fbb64b2 /opt/qt55/lib/libQt5WebKit.so.5(+0x17d24b2) [0x7fea8fbb64b2]\r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/119", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/119/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/119/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/119/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/119", "id": 225286973, "node_id": "MDU6SXNzdWUyMjUyODY5NzM=", "number": 119, "title": "tell people to read the code and learn", "user": {"login": "lexonight", "id": 1079503, "node_id": "MDQ6VXNlcjEwNzk1MDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/1079503?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lexonight", "html_url": "https://github.com/lexonight", "followers_url": "https://api.github.com/users/lexonight/followers", "following_url": "https://api.github.com/users/lexonight/following{/other_user}", "gists_url": "https://api.github.com/users/lexonight/gists{/gist_id}", "starred_url": "https://api.github.com/users/lexonight/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lexonight/subscriptions", "organizations_url": "https://api.github.com/users/lexonight/orgs", "repos_url": "https://api.github.com/users/lexonight/repos", "events_url": "https://api.github.com/users/lexonight/events{/privacy}", "received_events_url": "https://api.github.com/users/lexonight/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-04-29T21:37:25Z", "updated_at": "2017-04-29T21:37:51Z", "closed_at": "2017-04-29T21:37:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've gone through this issues list and it sounds like the copy + paste gurus have a problem with understanding code. Like kids wanting the problem to be written out for them. RTFM then read the code to see what it does. My bad not an issue. but compared to the other issues this is trivial lol", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/117", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/117/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/117/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/117/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/117", "id": 218053022, "node_id": "MDU6SXNzdWUyMTgwNTMwMjI=", "number": 117, "title": "Proxy servers do not work with Scrapy-Splash", "user": {"login": "eusid", "id": 552127, "node_id": "MDQ6VXNlcjU1MjEyNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/552127?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eusid", "html_url": "https://github.com/eusid", "followers_url": "https://api.github.com/users/eusid/followers", "following_url": "https://api.github.com/users/eusid/following{/other_user}", "gists_url": "https://api.github.com/users/eusid/gists{/gist_id}", "starred_url": "https://api.github.com/users/eusid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eusid/subscriptions", "organizations_url": "https://api.github.com/users/eusid/orgs", "repos_url": "https://api.github.com/users/eusid/repos", "events_url": "https://api.github.com/users/eusid/events{/privacy}", "received_events_url": "https://api.github.com/users/eusid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-03-30T01:08:23Z", "updated_at": "2017-07-30T22:35:32Z", "closed_at": "2017-03-30T10:30:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "http://stackoverflow.com/questions/43090352/proxy-servers-with-scrapy-splash\r\n\r\nCan someone please tell me why this is not working, or if it works at all? I have consulted all available documentation on the issue.  I'd really like to use Scrapy-Splash with proxies.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/113", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/113/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/113/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/113/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/113", "id": 217038132, "node_id": "MDU6SXNzdWUyMTcwMzgxMzI=", "number": 113, "title": "Error raised while running the example \"quotes\"", "user": {"login": "sckieer", "id": 23121577, "node_id": "MDQ6VXNlcjIzMTIxNTc3", "avatar_url": "https://avatars3.githubusercontent.com/u/23121577?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sckieer", "html_url": "https://github.com/sckieer", "followers_url": "https://api.github.com/users/sckieer/followers", "following_url": "https://api.github.com/users/sckieer/following{/other_user}", "gists_url": "https://api.github.com/users/sckieer/gists{/gist_id}", "starred_url": "https://api.github.com/users/sckieer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sckieer/subscriptions", "organizations_url": "https://api.github.com/users/sckieer/orgs", "repos_url": "https://api.github.com/users/sckieer/repos", "events_url": "https://api.github.com/users/sckieer/events{/privacy}", "received_events_url": "https://api.github.com/users/sckieer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-26T09:00:52Z", "updated_at": "2017-03-31T15:20:40Z", "closed_at": "2017-03-31T15:20:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "I\u2018d tried disabling the ROBOTSTXT_OBEY, but the error remained the same except for 'DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)' .\r\nI was running this on Python3.6,Windows 7.\r\n\r\nHere is the log\uff1a\r\n\r\n2017-03-26 16:44:24 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: scrashtest)\r\n2017-03-26 16:44:24 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'scrashtest', 'DUPEFILTER_CLASS': 'scrapy_splash.SplashAwareDupeFilter', 'HTTPCACHE_STORAGE': 'scrapy_splash.SplashAwareFSCacheStorage', 'NEWSPIDER_MODULE': 'scrashtest.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_LOADER_WARN_ONLY': True, 'SPIDER_MODULES': ['scrashtest.spiders']}\r\n2017-03-26 16:44:24 [scrapy.middleware] INFO: Enabled extensions:\r\n['scrapy.extensions.corestats.CoreStats',\r\n 'scrapy.extensions.telnet.TelnetConsole',\r\n 'scrapy.extensions.logstats.LogStats']\r\n2017-03-26 16:44:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\r\n['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\r\n 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\r\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\r\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\r\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\r\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\r\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\r\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\r\n 'scrapy_splash.SplashCookiesMiddleware',\r\n 'scrapy_splash.SplashMiddleware',\r\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\r\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\r\n2017-03-26 16:44:24 [scrapy.middleware] INFO: Enabled spider middlewares:\r\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\r\n 'scrapy_splash.SplashDeduplicateArgsMiddleware',\r\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\r\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\r\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\r\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\r\n2017-03-26 16:44:24 [scrapy.middleware] INFO: Enabled item pipelines:\r\n[]\r\n2017-03-26 16:44:24 [scrapy.core.engine] INFO: Spider opened\r\n2017-03-26 16:44:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\r\n2017-03-26 16:44:24 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\r\n2017-03-26 16:44:26 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\r\n2017-03-26 16:44:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/> (referer: None)\r\n2017-03-26 16:44:26 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite request to 'www.goodreads.com': <GET https://www.goodreads.com/quotes>\r\n2017-03-26 16:44:26 [scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite request to 'scrapinghub.com': <GET https://scrapinghub.com/>\r\n2017-03-26 16:44:27 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://127.0.0.1:8050/robots.txt> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:28 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://127.0.0.1:8050/robots.txt> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:29 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://127.0.0.1:8050/robots.txt> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:29 [scrapy.downloadermiddlewares.robotstxt] ERROR: Error downloading <GET http://127.0.0.1:8050/robots.txt>: Connection was refused by other side: 10061.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/J-K-Rowling via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/abilities/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/deep-thoughts/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/world/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/change/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/thinking/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:30 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/choices/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Albert-Einstein via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/login via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/simile/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/success/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Andre-Gide via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/value/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Thomas-A-Edison via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:31 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/love/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/J-K-Rowling via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/choices/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/world/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/thinking/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/deep-thoughts/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/change/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:32 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/abilities/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Albert-Einstein via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/login via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Thomas-A-Edison via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Andre-Gide via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/success/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/value/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/love/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:33 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/simile/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/J-K-Rowling via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/choices/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/world/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/deep-thoughts/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/thinking/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/abilities/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/change/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/J-K-Rowling via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/choices/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/world/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/deep-thoughts/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/thinking/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/abilities/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:34 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/change/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/Albert-Einstein via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/login via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/Thomas-A-Edison via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/Andre-Gide via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/value/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/simile/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/success/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/love/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/Albert-Einstein via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/login via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/Thomas-A-Edison via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/Andre-Gide via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/value/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/simile/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/success/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:35 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/love/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/truth/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/friends/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/friendship/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/reading/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/books/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/humor/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/life/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:36 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/inspirational/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/love/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/page/2/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/simile/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Steve-Martin via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/obvious/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/misattributed-eleanor-roosevelt/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/paraphrased/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:37 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Eleanor-Roosevelt via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/truth/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/friends/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/friendship/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/books/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/reading/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/humor/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/life/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:38 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/inspirational/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/love/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/page/2/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/simile/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Steve-Martin via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/obvious/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/paraphrased/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/misattributed-eleanor-roosevelt/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:39 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Eleanor-Roosevelt via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/truth/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/friends/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/friendship/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/books/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/humor/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/reading/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/life/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/inspirational/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/truth/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/friends/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/friendship/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/books/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/humor/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/reading/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/life/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:40 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/inspirational/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/love/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/page/2/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/simile/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/Steve-Martin via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/obvious/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/paraphrased/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/misattributed-eleanor-roosevelt/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/Eleanor-Roosevelt via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/love/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/page/2/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/simile/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/Steve-Martin via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/obvious/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/paraphrased/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/misattributed-eleanor-roosevelt/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:41 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/Eleanor-Roosevelt via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/failure/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/edison/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/adulthood/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/be-yourself/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Marilyn-Monroe via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/humor/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/books/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:42 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/classic/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/aliteracy/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Jane-Austen via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/miracles/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/miracle/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/live/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/life/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/inspirational/page/1/ via http://127.0.0.1:8050/render.json> (failed 1 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:43 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/failure/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/edison/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/adulthood/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Marilyn-Monroe via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/be-yourself/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/humor/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/books/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/classic/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:44 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/aliteracy/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/author/Jane-Austen via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/miracles/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/live/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/miracle/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/life/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET http://quotes.toscrape.com/tag/inspirational/page/1/ via http://127.0.0.1:8050/render.json> (failed 2 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/failure/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/edison/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:45 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/failure/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/edison/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/adulthood/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/Marilyn-Monroe via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/be-yourself/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/humor/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/books/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/classic/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/aliteracy/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/adulthood/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/author/Jane-Austen via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/Marilyn-Monroe via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/be-yourself/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/humor/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/books/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:46 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/classic/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/aliteracy/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/author/Jane-Austen via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/miracles/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/live/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/miracle/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/life/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.downloadermiddlewares.retry] DEBUG: Gave up retrying <GET http://quotes.toscrape.com/tag/inspirational/page/1/ via http://127.0.0.1:8050/render.json> (failed 3 times): Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/miracles/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/live/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/miracle/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/life/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:47 [scrapy.core.scraper] ERROR: Error downloading <GET http://quotes.toscrape.com/tag/inspirational/page/1/ via http://127.0.0.1:8050/render.json>\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\internet\\defer.py\", line 1299, in _inlineCallbacks\r\n    result = result.throwExceptionIntoGenerator(g)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\twisted\\python\\failure.py\", line 393, in throwExceptionIntoGenerator\r\n    return g.throw(self.type, self.value, self.tb)\r\n  File \"c:\\users\\administrator\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\core\\downloader\\middleware.py\", line 43, in process_request\r\n    defer.returnValue((yield download_func(request=request,spider=spider)))\r\ntwisted.internet.error.ConnectionRefusedError: Connection was refused by other side: 10061.\r\n2017-03-26 16:44:48 [scrapy.core.engine] INFO: Closing spider (finished)\r\n2017-03-26 16:44:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\r\n{'downloader/exception_count': 144,\r\n 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 144,\r\n 'downloader/request_bytes': 84474,\r\n 'downloader/request_count': 146,\r\n 'downloader/request_method_count/GET': 5,\r\n 'downloader/request_method_count/POST': 141,\r\n 'downloader/response_bytes': 2701,\r\n 'downloader/response_count': 2,\r\n 'downloader/response_status_count/200': 1,\r\n 'downloader/response_status_count/404': 1,\r\n 'finish_reason': 'finished',\r\n 'finish_time': datetime.datetime(2017, 3, 26, 8, 44, 48, 90000),\r\n 'log_count/DEBUG': 149,\r\n 'log_count/ERROR': 48,\r\n 'log_count/INFO': 7,\r\n 'offsite/domains': 2,\r\n 'offsite/filtered': 2,\r\n 'request_depth_max': 1,\r\n 'response_received_count': 2,\r\n 'scheduler/dequeued': 189,\r\n 'scheduler/dequeued/memory': 189,\r\n 'scheduler/enqueued': 189,\r\n 'scheduler/enqueued/memory': 189,\r\n 'splash/render.json/request_count': 47,\r\n 'start_time': datetime.datetime(2017, 3, 26, 8, 44, 24, 823000)}\r\n2017-03-26 16:44:48 [scrapy.core.engine] INFO: Spider closed (finished)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/112", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/112/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/112/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/112/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/112", "id": 216646587, "node_id": "MDU6SXNzdWUyMTY2NDY1ODc=", "number": 112, "title": "How to scrap next page's url link", "user": {"login": "ranafge", "id": 10490010, "node_id": "MDQ6VXNlcjEwNDkwMDEw", "avatar_url": "https://avatars3.githubusercontent.com/u/10490010?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ranafge", "html_url": "https://github.com/ranafge", "followers_url": "https://api.github.com/users/ranafge/followers", "following_url": "https://api.github.com/users/ranafge/following{/other_user}", "gists_url": "https://api.github.com/users/ranafge/gists{/gist_id}", "starred_url": "https://api.github.com/users/ranafge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ranafge/subscriptions", "organizations_url": "https://api.github.com/users/ranafge/orgs", "repos_url": "https://api.github.com/users/ranafge/repos", "events_url": "https://api.github.com/users/ranafge/events{/privacy}", "received_events_url": "https://api.github.com/users/ranafge/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-24T04:39:11Z", "updated_at": "2019-11-21T16:15:06Z", "closed_at": "2019-11-21T16:15:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello i am new in programming and scrapy. Trying to learn scrapy i try scrap some items. but unable to do the scrap next page url to scrap items, please help how to grap next page url link for this web site. \r\n\r\nHere is my code:\r\n\r\n    import scrapy\r\n    from scrapy.linkextractors import LinkExtractor \r\n\r\n    \r\n\r\n    class BdJobs(scrapy.Spider):\r\n\t\tname = 'bdjobs'\r\n\t\tallowed_domains = ['BdJobs.com']\r\n\t\tstart_urls = [\r\n\t\t\t\t\t  'http://jobs.bdjobs.com/',\r\n\t\t\t\t\t  'http://jobs.bdjobs.com/jobsearch.asp?fcatId=8&icatId='\r\n\t\t\t\t\t  ]\r\n\t\t#rules=( Rule(LinkExtractor(allow()), callback='parse', follow=True))\r\n\r\n\t\tdef parse(self, response):\r\n\t\t\tfor title in response.xpath('//div[@class=\"job-title-text\"]/a'):\r\n\t\t\t\tyield {\r\n\t\t\t\t'titles': title.xpath('./text()').extract()[0].strip()\r\n\t\t\t\t}\r\n\r\n        nextPageLink:\t\t\r\n\r\n        for grab the next page url here is the inspect Element url:\r\n        https://08733078838609164420.googlegroups.com/attach/58c611bdb536b/bdjobs.png?part=0.1&view=1&vt=ANaJVrEDQr4PODzoOkFRO_fLhL2ZF3x-Mts4XJ8m8qb2RSX1b4n6kv0E-62A2yvw0HkBjrmUOwCrFpMBk_h8UYSWDO6hZXyt-N2brbcYwtltG-A6NiHeaGc\r\n\r\n    Here is output:\r\n\r\n    \r\n    {\"titles\": \"Senior Software Engineer (.Net)\"},\r\n    {\"titles\": \"Java programmer\"},\r\n    {\"titles\": \"VLSI Design Engineer (Japan)\"},\r\n    {\"titles\": \"Assistant Executive (Computer Lab-Evening programs)\"},\r\n    {\"titles\": \"IT Officer, Business System Management\"},\r\n    {\"titles\": \"Executive, IT\"},\r\n    {\"titles\": \"Officer, IT\"},\r\n    {\"titles\": \"Laravel PHP Developer\"},\r\n    {\"titles\": \"Executive - IT (EDISON Footwear)\"},\r\n    {\"titles\": \"Software Engineer (PHP/ MySQL)\"},\r\n    {\"titles\": \"Software Engineer [Back End]\"},\r\n    {\"titles\": \"Full Stack Developer\"},\r\n    {\"titles\": \"Mobile Application Developer (iOS/ Android)\"},\r\n    {\"titles\": \"Head of IT Security Operations\"},\r\n    {\"titles\": \"Database Administrator, Senior Analyst\"},\r\n    {\"titles\": \"Infrastructure Delivery Senior Analyst, Network Security\"},\r\n    {\"titles\": \"Head of IT Support Operations\"},\r\n    {\"titles\": \"Hardware Engineer\"},\r\n    {\"titles\": \"JavaScript/ Coffee Script Programmer\"},\r\n    {\"titles\": \"Trainer - Auto CAD\"},\r\n    {\"titles\": \"ASSISTENT PRODUCTION OFFICER\"},\r\n    {\"titles\": \"Customer Relationship Executive\"},\r\n    {\"titles\": \"Head of Sales\"},\r\n    {\"titles\": \"Sample Master\"},\r\n    {\"titles\": \"Manager/ AGM (Finance & Accounts)\"},\r\n    {\"titles\": \"Night Aiditor\"},\r\n    {\"titles\": \"Officer- Poultry\"},\r\n    {\"titles\": \"Business Analyst\"},\r\n    {\"titles\": \"Sr. Executive - Sales & Marketing (Sewing Thread)\"},\r\n    {\"titles\": \"Civil Engineer\"},\r\n    {\"titles\": \"Executive Director-HR\"},\r\n    {\"titles\": \"Sr. Executive (MIS & Internal Audit)\"},\r\n    {\"titles\": \"Manager, Health & Safety\"},\r\n    {\"titles\": \"Computer Engineer (Diploma)\"},\r\n    {\"titles\": \"Sr. Manager/ Manager, Procurement\"},\r\n    {\"titles\": \"Specialist, Content\"},\r\n    {\"titles\": \"Manager, Warranty and Maintenance\"},\r\n    {\"titles\": \"Asst. Manager - Compliance\"},\r\n    {\"titles\": \"Officer/Sr. Officer/Asst. Manager (Store)\"},\r\n    {\"titles\": \"Manager, Maintenance (Sewing)\"}\r\n  \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/111", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/111/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/111/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/111/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/111", "id": 216615937, "node_id": "MDU6SXNzdWUyMTY2MTU5Mzc=", "number": 111, "title": "TypeError: SplashJsonResponse url must be str, got unicode ", "user": {"login": "wenxzhen", "id": 2071967, "node_id": "MDQ6VXNlcjIwNzE5Njc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2071967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenxzhen", "html_url": "https://github.com/wenxzhen", "followers_url": "https://api.github.com/users/wenxzhen/followers", "following_url": "https://api.github.com/users/wenxzhen/following{/other_user}", "gists_url": "https://api.github.com/users/wenxzhen/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenxzhen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenxzhen/subscriptions", "organizations_url": "https://api.github.com/users/wenxzhen/orgs", "repos_url": "https://api.github.com/users/wenxzhen/repos", "events_url": "https://api.github.com/users/wenxzhen/events{/privacy}", "received_events_url": "https://api.github.com/users/wenxzhen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-03-24T01:00:26Z", "updated_at": "2019-11-21T16:19:48Z", "closed_at": "2019-11-21T16:19:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "My Splash version is in 2.3.2:\r\n\r\n> 2017-03-23 10:24:49.591255 [-] Splash version: 2.3.2\r\n> 2017-03-23 10:24:49.592435 [-] Qt 5.5.1, PyQt 5.5.1, WebKit 538.1, sip 4.17, Twisted 16.1.1, Lua 5.2\r\n> 2017-03-23 10:24:49.592688 [-] Python 3.4.3 (default, Nov 17 2016, 01:08:31) [GCC 4.8.4]\r\n> 2017-03-23 10:24:49.592960 [-] Open files limit: 99999\r\n> 2017-03-23 10:24:49.593182 [-] Can't bump open files limit\r\n> 2017-03-23 10:24:49.703665 [-] Xvfb is started: ['Xvfb', ':1', '-screen', '0', '1024x768x24', '-nolisten', 'tcp']\r\n> 2017-03-23 10:24:49.911082 [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles\r\n> 2017-03-23 10:24:50.651053 [-] verbosity=1\r\n> 2017-03-23 10:24:50.651382 [-] slots=50\r\n> 2017-03-23 10:24:50.651615 [-] argument_cache_max_entries=500\r\n> 2017-03-23 10:24:50.652616 [-] Web UI: enabled, Lua: enabled (sandbox: enabled)\r\n> 2017-03-23 10:24:50.659033 [-] Site starting on 8050\r\n> 2017-03-23 10:24:50.659403 [-] Starting factory <twisted.web.server.Site object at 0x7f332408a240>\r\n\r\nWhile my Scrapy splash is in 0.7.1, and run with Python 2.7 as described below. Is it an issue in the python version?\r\n\r\n> 2017-03-23 19:10:05 [scrapy.core.scraper] ERROR: Error downloading <GET http://192.168.6.115:8050/render.json>\r\n> Traceback (most recent call last):\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/Twisted-17.1.0-py2.7-linux-x86_64.egg/twisted/internet/defer.py\", line 1301, in _inlineCallbacks\r\n>     result = g.send(result)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy/core/downloader/middleware.py\", line 53, in process_response\r\n>     spider=spider)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy_splash/middleware.py\", line 387, in process_response\r\n>     response = self._change_response_class(request, response)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy_splash/middleware.py\", line 402, in _change_response_class\r\n>     response = response.replace(cls=respcls, request=request)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy/http/response/text.py\", line 50, in replace\r\n>     return Response.replace(self, *args, **kwargs)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy/http/response/__init__.py\", line 79, in replace\r\n>     return cls(*args, **kwargs)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy_splash/response.py\", line 95, in __init__\r\n>     super(SplashJsonResponse, self).__init__(*args, **kwargs)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy_splash/response.py\", line 33, in __init__\r\n>     super(_SplashResponseMixin, self).__init__(url, *args, **kwargs)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy/http/response/__init__.py\", line 21, in __init__\r\n>     self._set_url(url)\r\n>   File \"/root/wenxzhen/python27_env/lib/python2.7/site-packages/scrapy/http/response/__init__.py\", line 43, in _set_url\r\n>     type(url).__name__))\r\n> TypeError: SplashJsonResponse url must be str, got unicode:\r\n\r\nThanks, ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/110", "repository_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash", "labels_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/110/labels{/name}", "comments_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/110/comments", "events_url": "https://api.github.com/repos/scrapy-plugins/scrapy-splash/issues/110/events", "html_url": "https://github.com/scrapy-plugins/scrapy-splash/issues/110", "id": 215261206, "node_id": "MDU6SXNzdWUyMTUyNjEyMDY=", "number": 110, "title": "response.real_url does not exist in my environment", "user": {"login": "zperfet", "id": 19884129, "node_id": "MDQ6VXNlcjE5ODg0MTI5", "avatar_url": "https://avatars2.githubusercontent.com/u/19884129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zperfet", "html_url": "https://github.com/zperfet", "followers_url": "https://api.github.com/users/zperfet/followers", "following_url": "https://api.github.com/users/zperfet/following{/other_user}", "gists_url": "https://api.github.com/users/zperfet/gists{/gist_id}", "starred_url": "https://api.github.com/users/zperfet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zperfet/subscriptions", "organizations_url": "https://api.github.com/users/zperfet/orgs", "repos_url": "https://api.github.com/users/zperfet/repos", "events_url": "https://api.github.com/users/zperfet/events{/privacy}", "received_events_url": "https://api.github.com/users/zperfet/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-19T12:38:04Z", "updated_at": "2017-03-20T13:04:34Z", "closed_at": "2017-03-20T13:04:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "When i run the example(quotes.py) code, an error \" AttributeError: 'TextResponse' object has no attribute 'real_url'\" appears. i do not know why.", "performed_via_github_app": null, "score": 1.0}]}