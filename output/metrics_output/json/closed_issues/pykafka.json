{"total_count": 487, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/Parsely/pykafka/issues/960", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/960/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/960/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/960/events", "html_url": "https://github.com/Parsely/pykafka/issues/960", "id": 483088782, "node_id": "MDU6SXNzdWU0ODMwODg3ODI=", "number": 960, "title": "RdKafkaException: receive.message.max.bytes must be >= fetch.max.bytes + 512", "user": {"login": "aguyyala-disco", "id": 51929884, "node_id": "MDQ6VXNlcjUxOTI5ODg0", "avatar_url": "https://avatars1.githubusercontent.com/u/51929884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aguyyala-disco", "html_url": "https://github.com/aguyyala-disco", "followers_url": "https://api.github.com/users/aguyyala-disco/followers", "following_url": "https://api.github.com/users/aguyyala-disco/following{/other_user}", "gists_url": "https://api.github.com/users/aguyyala-disco/gists{/gist_id}", "starred_url": "https://api.github.com/users/aguyyala-disco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aguyyala-disco/subscriptions", "organizations_url": "https://api.github.com/users/aguyyala-disco/orgs", "repos_url": "https://api.github.com/users/aguyyala-disco/repos", "events_url": "https://api.github.com/users/aguyyala-disco/events{/privacy}", "received_events_url": "https://api.github.com/users/aguyyala-disco/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-20T21:20:52Z", "updated_at": "2019-09-16T21:22:08Z", "closed_at": "2019-09-16T21:22:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm running into the following exception when using balanced_consumer with `rdkafka=True`.\r\n\r\n> pykafka.exceptions.RdKafkaException: `receive.message.max.bytes` must be >= `fetch.max.bytes` + 512\r\n\r\nThe same piece of code by just making `rdkafka=False` will work fine.\r\n\r\nThe following is the code.\r\n\r\n```from pykafka import KafkaClient\r\nimport io\r\nimport avro.schema\r\nimport time\r\nimport avro.io\r\n\r\nimport logging\r\nkafkalogger = logging.getLogger('pykafka')\r\nkafkalogger.setLevel(logging.DEBUG)\r\nconsole_handler = logging.StreamHandler()\r\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\r\nconsole_handler.setFormatter(formatter)\r\nconsole_handler.setLevel(logging.DEBUG)\r\nkafkalogger.addHandler(console_handler)\r\n\r\nclient = KafkaClient(hosts='localhost:9092')\r\n\r\ntopic = client.topics['ingest']\r\n\r\nkey_schema = avro.schema.Parse(open(\"ifs_ingest_key.avsc\", \"r\").read())\r\nvalue_schema = avro.schema.Parse(open(\"ifs_ingest_value.avsc\", \"r\").read())\r\n\r\nbalanced_consumer = topic.get_balanced_consumer(\r\n    consumer_group='testgroup',\r\n    zookeeper_connect='10.10.146.168:2181,10.10.132.73:2181,10.10.175.137:2181',\r\n    use_rdkafka=True\r\n)\r\n\r\nstart_time = time.time()\r\n\r\ncnt = 0\r\nfor message in balanced_consumer:\r\n    if message is not None:\r\n        bytes_reader = io.BytesIO(message.value)\r\n        decoder = avro.io.BinaryDecoder(bytes_reader)\r\n        reader = avro.io.DatumReader(value_schema)\r\n        record = reader.read(decoder)\r\n        print(record)\r\n        cnt += 1\r\n\r\nbalanced_consumer.stop()\r\nprint(\"%d --- %s seconds ---\" % (cnt, time.time() - start_time))\r\n```\r\n\r\n\r\nThe following is debug output:\r\n\r\n```2019-08-20 16:17:47,039 - pykafka.balancedconsumer - ERROR - Stopping consumer in response to error\r\nTraceback (most recent call last):\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 342, in start\r\n    self._rebalance()\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 615, in _rebalance\r\n    self._update_member_assignment()\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 586, in _update_member_assignment\r\n    if self._setup_internal_consumer(new_partitions):\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 395, in _setup_internal_consumer\r\n    cns = self._get_internal_consumer(partitions=list(partitions), start=start)\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 464, in _get_internal_consumer\r\n    cns.start()\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/simpleconsumer.py\", line 300, in start\r\n    self._fetch_workers = self._setup_fetch_workers()\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/rdkafka/simple_consumer.py\", line 88, in _setup_fetch_workers\r\n    self._rdk_consumer.start(**start_kwargs)\r\npykafka.exceptions.RdKafkaException: receive.message.max.bytes must be >= fetch.max.bytes + 512\r\n2019-08-20 16:17:47,041 - pykafka.balancedconsumer - DEBUG - Stopping <pykafka.balancedconsumer.BalancedConsumer at 0x1093f7cc0 (consumer_group=testgroup)>\r\nTraceback (most recent call last):\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 748, in consume\r\n    message = self._consumer.consume(block=block, unblock_event=self._rebalancing_in_progress)\r\nAttributeError: 'NoneType' object has no attribute 'consume'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/guyyala/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/192.5728.105/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 2060, in <module>\r\n    main()\r\n  File \"/Users/guyyala/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/192.5728.105/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 2054, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/Users/guyyala/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/192.5728.105/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1405, in run\r\n    return self._exec(is_module, entry_point_fn, module_name, file, globals, locals)\r\n  File \"/Users/guyyala/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/192.5728.105/PyCharm.app/Contents/helpers/pydev/pydevd.py\", line 1412, in _exec\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/Users/guyyala/Library/Application Support/JetBrains/Toolbox/apps/PyCharm-P/ch-0/192.5728.105/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/guyyala/git/IFS/pykafka_consumer.py\", line 40, in <module>\r\n    for message in balanced_consumer:\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 767, in __iter__\r\n    message = self.consume(block=True)\r\n  File \"/Users/guyyala/git/IFS/venv/lib/python3.6/site-packages/pykafka/balancedconsumer.py\", line 757, in consume\r\n    raise ConsumerStoppedException\r\npykafka.exceptions.ConsumerStoppedException\r\n2019-08-20 16:17:48,174 - pykafka.simpleconsumer - DEBUG - Finalising <pykafka.rdkafka.simple_consumer.RdKafkaSimpleConsumer at 0x109411940 (consumer_group=b'testgroup')>\r\n2019-08-20 16:17:48,174 - pykafka.rdkafka.simple_consumer - DEBUG - Issued stop to _rdk_consumer.\r\n2019-08-20 16:17:48,175 - pykafka.balancedconsumer - DEBUG - Finalising <pykafka.balancedconsumer.BalancedConsumer at 0x1093f7cc0 (consumer_group=testgroup)>\r\n```\r\n\r\n**PyKafka version**: 2.8.0\r\n**Kafka version**: 2.2.1 (AWS MSK)\r\n**librdkafka**: 1.1.0 (installed on mac using brew)\r\n\r\nI googled on this error and searched all stackoverflow but not able to find any solution. Can someone please help me out here.\r\n\r\nThank you!!!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/935", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/935/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/935/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/935/events", "html_url": "https://github.com/Parsely/pykafka/issues/935", "id": 433152806, "node_id": "MDU6SXNzdWU0MzMxNTI4MDY=", "number": 935, "title": "why only one partition that can receives message?", "user": {"login": "migowei0621", "id": 2839878, "node_id": "MDQ6VXNlcjI4Mzk4Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/2839878?v=4", "gravatar_id": "", "url": "https://api.github.com/users/migowei0621", "html_url": "https://github.com/migowei0621", "followers_url": "https://api.github.com/users/migowei0621/followers", "following_url": "https://api.github.com/users/migowei0621/following{/other_user}", "gists_url": "https://api.github.com/users/migowei0621/gists{/gist_id}", "starred_url": "https://api.github.com/users/migowei0621/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/migowei0621/subscriptions", "organizations_url": "https://api.github.com/users/migowei0621/orgs", "repos_url": "https://api.github.com/users/migowei0621/repos", "events_url": "https://api.github.com/users/migowei0621/events{/privacy}", "received_events_url": "https://api.github.com/users/migowei0621/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-04-15T08:19:56Z", "updated_at": "2019-04-15T10:08:29Z", "closed_at": "2019-04-15T10:08:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I use pykafka 2.8.0, topic has 3 partitions, and my producer is like:\r\n\r\nwith topic.get_producer() as producer:\r\n    producer.produce(msg.encode())\r\n\r\nbut I found that only one partiton can receive messages, could anyone tell me why?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/930", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/930/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/930/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/930/events", "html_url": "https://github.com/Parsely/pykafka/issues/930", "id": 422584250, "node_id": "MDU6SXNzdWU0MjI1ODQyNTA=", "number": 930, "title": "Bi-direct communication handle in kafka", "user": {"login": "teguh87", "id": 6670919, "node_id": "MDQ6VXNlcjY2NzA5MTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/6670919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teguh87", "html_url": "https://github.com/teguh87", "followers_url": "https://api.github.com/users/teguh87/followers", "following_url": "https://api.github.com/users/teguh87/following{/other_user}", "gists_url": "https://api.github.com/users/teguh87/gists{/gist_id}", "starred_url": "https://api.github.com/users/teguh87/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teguh87/subscriptions", "organizations_url": "https://api.github.com/users/teguh87/orgs", "repos_url": "https://api.github.com/users/teguh87/repos", "events_url": "https://api.github.com/users/teguh87/events{/privacy}", "received_events_url": "https://api.github.com/users/teguh87/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-19T07:39:07Z", "updated_at": "2019-03-19T18:44:57Z", "closed_at": "2019-03-19T18:44:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hai I need to know how pykafka handle bi-direct message, ex: if producer sending message and consumer received and send back to producer to the origin topic.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/926", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/926/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/926/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/926/events", "html_url": "https://github.com/Parsely/pykafka/issues/926", "id": 421891146, "node_id": "MDU6SXNzdWU0MjE4OTExNDY=", "number": 926, "title": "pykafka witth librdkafka", "user": {"login": "ananthmeka", "id": 32932822, "node_id": "MDQ6VXNlcjMyOTMyODIy", "avatar_url": "https://avatars3.githubusercontent.com/u/32932822?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ananthmeka", "html_url": "https://github.com/ananthmeka", "followers_url": "https://api.github.com/users/ananthmeka/followers", "following_url": "https://api.github.com/users/ananthmeka/following{/other_user}", "gists_url": "https://api.github.com/users/ananthmeka/gists{/gist_id}", "starred_url": "https://api.github.com/users/ananthmeka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ananthmeka/subscriptions", "organizations_url": "https://api.github.com/users/ananthmeka/orgs", "repos_url": "https://api.github.com/users/ananthmeka/repos", "events_url": "https://api.github.com/users/ananthmeka/events{/privacy}", "received_events_url": "https://api.github.com/users/ananthmeka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-17T07:20:58Z", "updated_at": "2019-03-19T19:11:22Z", "closed_at": "2019-03-17T16:09:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to use the pykafka with rdkafka , but I do not have development environment(gcc), since it was mentioned that we can install the librdkafka and use pykafka in [https://github.com/Parsely/pykafka] , we followed the steps mentioned in [Using the librdkafka extension] in the section \"Using the librdkafka extension\" to have the pykafka with rdkafka as follows :\r\n\r\ninstall the librdkafka\r\nyum install librdkafka-devel\r\n\r\ninstall the pykafka\r\npip install pykafka\r\n\r\nAfter installation of the pykafka, checked the package pykafka.rdkafka is available in python import, but it was not available (LD_LIBRARY_PATH was set with the location of the .so file)\r\n\r\nBut if I use the same steps in my MAC , by replacing the step 1 ( i.e yum install librdkafka-devel ) with brew install librdkafka and followed step 2, I was getting the pykafka.rdkafka package in my Mac book (LD_LIBRARY_PATH was set with /usr/local/lib) .\r\n\r\nDo we need to any additional steps if we want use the librdkafka with pykafka installation ( not re-build) using the yum install ?  \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/923", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/923/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/923/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/923/events", "html_url": "https://github.com/Parsely/pykafka/issues/923", "id": 419289040, "node_id": "MDU6SXNzdWU0MTkyODkwNDA=", "number": 923, "title": "ManagedBalancedConsumer compacted_topic default value", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-03-11T04:25:49Z", "updated_at": "2019-03-18T18:17:07Z", "closed_at": "2019-03-18T18:17:07Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "From Carl Gieringer on the mailing list:\r\n\r\n----------------------------------------------------------------------\r\n\r\nI am learning Kafka and PyKafka. I am wondering why ManagedBalancedConsumer uses a [default of True for compacted_topic](https://github.com/Parsely/pykafka/blob/7781e5523c1ed06df03d070d3571cc8bd620250c/pykafka/managedbalancedconsumer.py#L74) while both [SimpleConsumer](https://github.com/Parsely/pykafka/blob/7781e5523c1ed06df03d070d3571cc8bd620250c/pykafka/simpleconsumer.py#L77) and [BalancedConsumer](https://github.com/Parsely/pykafka/blob/7781e5523c1ed06df03d070d3571cc8bd620250c/pykafka/balancedconsumer.py#L104) use False as the default for this parameter.  I would think that strictly from an API design perspective, consistency between these defaults would have been preferable.  Is there a reason that the default differs for ManagedBalancedConsumer?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/920", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/920/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/920/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/920/events", "html_url": "https://github.com/Parsely/pykafka/issues/920", "id": 410646713, "node_id": "MDU6SXNzdWU0MTA2NDY3MTM=", "number": 920, "title": "librdkafka dependency and setup issues", "user": {"login": "carsonip", "id": 9133397, "node_id": "MDQ6VXNlcjkxMzMzOTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9133397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carsonip", "html_url": "https://github.com/carsonip", "followers_url": "https://api.github.com/users/carsonip/followers", "following_url": "https://api.github.com/users/carsonip/following{/other_user}", "gists_url": "https://api.github.com/users/carsonip/gists{/gist_id}", "starred_url": "https://api.github.com/users/carsonip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carsonip/subscriptions", "organizations_url": "https://api.github.com/users/carsonip/orgs", "repos_url": "https://api.github.com/users/carsonip/repos", "events_url": "https://api.github.com/users/carsonip/events{/privacy}", "received_events_url": "https://api.github.com/users/carsonip/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-02-15T07:54:38Z", "updated_at": "2019-04-07T03:03:21Z", "closed_at": "2019-04-07T03:03:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**PyKafka version**: 2.8.0\r\n\r\nIn setup.py, with CPython, it detects the existence of librdkafka and compile the extension by default. When it fails, it silently disables the librdkafka support.\r\n\r\nWhen I set up the environment for librdkafka, I installed the system `librdkafka-dev` then I ran setup.py. And I actually bump into a silent error when I ran `pip install -e .`. I had to run `setup.py install` to see the error message:\r\n```\r\nrunning build_ext\r\nbuilding 'pykafka.rdkafka._rd_kafka' extension\r\nx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c pykafka/rdkafka/_rd_kafkamodule.c -o build/temp.linux-x86_64-2.7/pykafka/rdkafka/_rd_kafkamodule.o\r\npykafka/rdkafka/_rd_kafkamodule.c: In function 'Producer_delivery_report_callback':\r\npykafka/rdkafka/_rd_kafkamodule.c:568:9: error: unknown type name 'rd_kafka_timestamp_type_t'\r\n        rd_kafka_timestamp_type_t timestamp_type;\r\n        ^\r\npykafka/rdkafka/_rd_kafkamodule.c:569:29: warning: implicit declaration of function 'rd_kafka_message_timestamp' [-Wimplicit-function-declaration]\r\n        int64_t timestamp = rd_kafka_message_timestamp(rkmessage, &timestamp_ty\r\n                            ^\r\npykafka/rdkafka/_rd_kafkamodule.c:570:31: error: 'RD_KAFKA_TIMESTAMP_NOT_AVAILABLE' undeclared (first use in this function)\r\n        if (timestamp_type != RD_KAFKA_TIMESTAMP_NOT_AVAILABLE) {\r\n                              ^\r\npykafka/rdkafka/_rd_kafkamodule.c:570:31: note: each undeclared identifier is reported only once for each function it appears in\r\npykafka/rdkafka/_rd_kafkamodule.c: In function 'Consumer_consume':\r\npykafka/rdkafka/_rd_kafkamodule.c:978:9: error: unknown type name 'rd_kafka_timestamp_type_t'\r\n        rd_kafka_timestamp_type_t timestamp_type;\r\n        ^\r\npykafka/rdkafka/_rd_kafkamodule.c:980:31: error: 'RD_KAFKA_TIMESTAMP_NOT_AVAILABLE' undeclared (first use in this function)\r\n        if (timestamp_type == RD_KAFKA_TIMESTAMP_NOT_AVAILABLE) {\r\n```\r\n\r\n1. Is it possible to let users specify rdkafka support and fail the setup if the compilation fails? If the user expects rdkafa support, failing at setup sounds better than failing with ImportError at runtime.\r\n2. Should we document the version of librdkafka needed for the extension to compile successfully? I got librdkafka 0.8.6 (the latest in apt for Ubuntu 16.04) but it doesn't compile. When I manually compile the v0.11.6 from GitHub, it works. There's a similar issue in confluent-kafka-python https://github.com/confluentinc/confluent-kafka-python/issues/230\r\n3. Should we vendor librdkafka? git clone, git checkout to tag, compile and use that shared library file (or use extension)?\r\n\r\nI could help with the fix and the doc but there are some decisions to be made. Thanks!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/919", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/919/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/919/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/919/events", "html_url": "https://github.com/Parsely/pykafka/issues/919", "id": 410269765, "node_id": "MDU6SXNzdWU0MTAyNjk3NjU=", "number": 919, "title": "Does pykafka support kafka version 1.0.x", "user": {"login": "jinsust", "id": 2959978, "node_id": "MDQ6VXNlcjI5NTk5Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/2959978?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinsust", "html_url": "https://github.com/jinsust", "followers_url": "https://api.github.com/users/jinsust/followers", "following_url": "https://api.github.com/users/jinsust/following{/other_user}", "gists_url": "https://api.github.com/users/jinsust/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinsust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinsust/subscriptions", "organizations_url": "https://api.github.com/users/jinsust/orgs", "repos_url": "https://api.github.com/users/jinsust/repos", "events_url": "https://api.github.com/users/jinsust/events{/privacy}", "received_events_url": "https://api.github.com/users/jinsust/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-02-14T12:09:37Z", "updated_at": "2019-02-14T17:20:32Z", "closed_at": "2019-02-14T17:20:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Same as the title\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/916", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/916/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/916/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/916/events", "html_url": "https://github.com/Parsely/pykafka/issues/916", "id": 408764662, "node_id": "MDU6SXNzdWU0MDg3NjQ2NjI=", "number": 916, "title": "Starting consumer in django", "user": {"login": "kyao-Frndz", "id": 20722023, "node_id": "MDQ6VXNlcjIwNzIyMDIz", "avatar_url": "https://avatars2.githubusercontent.com/u/20722023?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kyao-Frndz", "html_url": "https://github.com/kyao-Frndz", "followers_url": "https://api.github.com/users/kyao-Frndz/followers", "following_url": "https://api.github.com/users/kyao-Frndz/following{/other_user}", "gists_url": "https://api.github.com/users/kyao-Frndz/gists{/gist_id}", "starred_url": "https://api.github.com/users/kyao-Frndz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kyao-Frndz/subscriptions", "organizations_url": "https://api.github.com/users/kyao-Frndz/orgs", "repos_url": "https://api.github.com/users/kyao-Frndz/repos", "events_url": "https://api.github.com/users/kyao-Frndz/events{/privacy}", "received_events_url": "https://api.github.com/users/kyao-Frndz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-02-11T12:43:37Z", "updated_at": "2019-02-12T10:18:09Z", "closed_at": "2019-02-11T17:03:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @emmett9001,\r\n\r\nWe are having problems starting consumers in django. Once consumers have been started (in django's wsgi.py file) all subsequent http request to the application consuming data results in a gateway timeout. \r\n\r\nCan you suggest a better way starting the consumers in django?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/914", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/914/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/914/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/914/events", "html_url": "https://github.com/Parsely/pykafka/issues/914", "id": 408635072, "node_id": "MDU6SXNzdWU0MDg2MzUwNzI=", "number": 914, "title": "Incompatible LZ4 compression causes error in Kafka and fails the delivery", "user": {"login": "carsonip", "id": 9133397, "node_id": "MDQ6VXNlcjkxMzMzOTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9133397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carsonip", "html_url": "https://github.com/carsonip", "followers_url": "https://api.github.com/users/carsonip/followers", "following_url": "https://api.github.com/users/carsonip/following{/other_user}", "gists_url": "https://api.github.com/users/carsonip/gists{/gist_id}", "starred_url": "https://api.github.com/users/carsonip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carsonip/subscriptions", "organizations_url": "https://api.github.com/users/carsonip/orgs", "repos_url": "https://api.github.com/users/carsonip/repos", "events_url": "https://api.github.com/users/carsonip/events{/privacy}", "received_events_url": "https://api.github.com/users/carsonip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-02-11T04:58:22Z", "updated_at": "2019-03-07T03:47:06Z", "closed_at": "2019-03-07T03:47:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**PyKafka version**: 2.8.0\r\n**Kafka version**: 2.0.1\r\n\r\nError in application (in stderr):\r\n```\r\nERROR:pykafka.producer:Message not delivered!! UnknownError('Produce request for my-topic/5 to 127.0.0.1:31092 failed with error code -1.',)\r\n```\r\n\r\nError in kafka:\r\n```\r\n[2019-02-11 04:39:56,320] ERROR [ReplicaManager broker=0] Error processing append operation on partition my-topic-0 (kafka.server.ReplicaManager)\r\norg.apache.kafka.common.KafkaException: java.lang.RuntimeException: Dependent block stream is unsupported\r\n    at org.apache.kafka.common.record.CompressionType$4.wrapForInput(CompressionType.java:113)\r\n    at org.apache.kafka.common.record.AbstractLegacyRecordBatch$DeepRecordsIterator.<init>(AbstractLegacyRecordBatch.java:330)\r\n    at org.apache.kafka.common.record.AbstractLegacyRecordBatch$DeepRecordsIterator.<init>(AbstractLegacyRecordBatch.java:310)\r\n    at org.apache.kafka.common.record.AbstractLegacyRecordBatch.iterator(AbstractLegacyRecordBatch.java:232)\r\n    at org.apache.kafka.common.record.AbstractLegacyRecordBatch.iterator(AbstractLegacyRecordBatch.java:227)\r\n    at scala.collection.convert.Wrappers$JIterableWrapper.iterator(Wrappers.scala:54)\r\n    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\r\n    at kafka.log.LogValidator$$anonfun$validateMessagesAndAssignOffsetsCompressed$1.apply(LogValidator.scala:267)\r\n    at kafka.log.LogValidator$$anonfun$validateMessagesAndAssignOffsetsCompressed$1.apply(LogValidator.scala:259)\r\n    at scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\r\n    at kafka.log.LogValidator$.validateMessagesAndAssignOffsetsCompressed(LogValidator.scala:259)\r\n    at kafka.log.LogValidator$.validateMessagesAndAssignOffsets(LogValidator.scala:70)\r\n    at kafka.log.Log$$anonfun$append$2.liftedTree1$1(Log.scala:771)\r\n    at kafka.log.Log$$anonfun$append$2.apply(Log.scala:770)\r\n    at kafka.log.Log$$anonfun$append$2.apply(Log.scala:752)\r\n    at kafka.log.Log.maybeHandleIOException(Log.scala:1842)\r\n    at kafka.log.Log.append(Log.scala:752)\r\n    at kafka.log.Log.appendAsLeader(Log.scala:722)\r\n    at kafka.cluster.Partition$$anonfun$13.apply(Partition.scala:660)\r\n    at kafka.cluster.Partition$$anonfun$13.apply(Partition.scala:648)\r\n    at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)\r\n    at kafka.utils.CoreUtils$.inReadLock(CoreUtils.scala:257)\r\n    at kafka.cluster.Partition.appendRecordsToLeader(Partition.scala:647)\r\n    at kafka.server.ReplicaManager$$anonfun$appendToLocalLog$2.apply(ReplicaManager.scala:745)\r\n    at kafka.server.ReplicaManager$$anonfun$appendToLocalLog$2.apply(ReplicaManager.scala:733)\r\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n    at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\r\n    at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\r\n    at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\r\n    at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n    at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\r\n    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n    at scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n    at kafka.server.ReplicaManager.appendToLocalLog(ReplicaManager.scala:733)\r\n    at kafka.server.ReplicaManager.appendRecords(ReplicaManager.scala:471)\r\n    at kafka.server.KafkaApis.handleProduceRequest(KafkaApis.scala:489)\r\n    at kafka.server.KafkaApis.handle(KafkaApis.scala:113)\r\n    at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:69)\r\n    at java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nMy best guess is that something is wrong with the LZ4 compression in producer. I am producing a lot of messages in batch with async producer and this is reproducible. I'll have to move away from lz4 in the meantime.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/912", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/912/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/912/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/912/events", "html_url": "https://github.com/Parsely/pykafka/issues/912", "id": 403163252, "node_id": "MDU6SXNzdWU0MDMxNjMyNTI=", "number": 912, "title": "Loosing messages when producing", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-01-25T13:45:04Z", "updated_at": "2020-07-02T04:55:00Z", "closed_at": "2020-07-02T04:55:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @emmett9001 ,\r\n\r\nWe have a problematic thing going on with producing messages. We have an application running with a number of consumers and two producers. Consuming occurs continuously, but producing is just every 30 minutes (due to batch processing of input file). When starting the application and running the first batch, everything works fine. Which means every message that is produced, is available on the topic and can be consumed by the consuming application. But after 30 minutes when the 2nd batch is processed, we are getting \"Error encountered when producing to broker x. Retrying.\" errors in the debuglog of pykafka and we are loosing some messages. With other words we send for example 100 messages, but always a few are gone (1 or 2). So they never arrive on the topic and are thus not consumed by the consuming application. This happens on both producers, each producing to a separate topic. We are running the producer in asynchronous mode (sync=False on get_producer).\r\n\r\nWhen running in synchronous mode (sync=True), same error appears in the debuglog, but none of the messages are gone. So 100% of the messages produced appear on the topic.\r\n\r\nWe are running pykafka 2.8.0, but also tried 2.7.0. We have a Kafka cluster setup of 3 nodes, with replication-factor 3 for all the topics. Kafka version is 1.1.0.\r\n\r\nUnderneath is a part of the debuglog of pykafka where you can see the errors I mentioned.\r\n\r\nLooking at the pykafka code, I think the \"Error encountered when producing to broker x. Retrying.\" errors is because of a SocketDisconnectedError. I guess that in the 30 minutes between producing, the connection is lost somehow due to timeouts. It successfully reconnects in both sync and async mode, but unfortunately messages are lost in async mode.\r\n\r\nAny thoughts on this? \r\n\r\nThank you very much.\r\n\r\n----\r\n2019-01-25 12:11:00,622 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,622 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,632 Error encountered when producing to broker ip:9092. Retrying.\r\n2019-01-25 12:11:00,633 Updating cluster, attempt 1/3\r\n2019-01-25 12:11:00,634 Error encountered when producing to broker ip:9292. Retrying.\r\n2019-01-25 12:11:00,634 Updating cluster, attempt 1/3\r\n2019-01-25 12:11:00,635 Connecting to ip:9092\r\n2019-01-25 12:11:00,635 Connecting to ip:9092\r\n2019-01-25 12:11:00,636 Successfully connected to ip:9092\r\n2019-01-25 12:11:00,636 Successfully connected to ip:9092\r\n2019-01-25 12:11:00,640 RequestHandler.stop: about to flush requests queue\r\n2019-01-25 12:11:00,641 Discovered 3 brokers\r\n2019-01-25 12:11:00,641 Reconnecting to broker id 0: ip:9092\r\n2019-01-25 12:11:00,643 Connecting to ip:9092\r\n2019-01-25 12:11:00,644 Successfully connected to ip:9092\r\n2019-01-25 12:11:00,644 RequestHandler.stop: about to flush requests queue\r\n2019-01-25 12:11:00,645 RequestHandler.stop: about to flush requests queue\r\n2019-01-25 12:11:00,647 Discovered 3 brokers\r\n2019-01-25 12:11:00,647 Broker ip:9092 metadata unchanged. Continuing.\r\n2019-01-25 12:11:00,647 Broker ip:9192 metadata unchanged. Continuing.\r\n2019-01-25 12:11:00,648 Reconnecting to broker id 2: ip:9292\r\n2019-01-25 12:11:00,649 Connecting to ip:9292\r\n2019-01-25 12:11:00,649 Broker ip:9192 metadata unchanged. Continuing.\r\n2019-01-25 12:11:00,649 Broker ip:9292 metadata unchanged. Continuing.\r\n2019-01-25 12:11:00,650 Discovered 16 topics\r\n2019-01-25 12:11:00,650 Adding 1 partitions\r\n2019-01-25 12:11:00,650 Starting new produce worker for broker 0\r\n2019-01-25 12:11:00,650 Successfully connected to ip:9292\r\n2019-01-25 12:11:00,650 RequestHandler.stop: about to flush requests queue\r\n2019-01-25 12:11:00,652 Discovered 16 topics\r\n2019-01-25 12:11:00,652 Adding 1 partitions\r\n2019-01-25 12:11:00,652 Starting new produce worker for broker 2\r\n2019-01-25 12:11:00,656 Successfully sent 0/1 messages to broker 0\r\n2019-01-25 12:11:00,656 Successfully sent 0/1 messages to broker 2\r\n2019-01-25 12:11:00,663 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,663 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,668 Fetched 1 messages for partition 0\r\n2019-01-25 12:11:00,668 Partition 0 queue holds 1 messages\r\n2019-01-25 12:11:00,669 Successfully sent 1/1 messages to broker 2\r\n2019-01-25 12:11:00,672 Successfully sent 1/1 messages to broker 0\r\n2019-01-25 12:11:00,675 Fetched 1 messages for partition 0\r\n2019-01-25 12:11:00,675 Partition 0 queue holds 1 messages\r\n2019-01-25 12:11:00,680 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,681 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,687 Successfully sent 1/1 messages to broker 2\r\n2019-01-25 12:11:00,687 Fetched 1 messages for partition 0\r\n2019-01-25 12:11:00,687 Partition 0 queue holds 1 messages\r\n2019-01-25 12:11:00,688 Successfully sent 1/1 messages to broker 0\r\n2019-01-25 12:11:00,692 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,692 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,693 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,694 Fetched 1 messages for partition 0\r\n2019-01-25 12:11:00,695 Partition 0 queue holds 1 messages\r\n2019-01-25 12:11:00,697 Successfully sent 1/1 messages to broker 2\r\n2019-01-25 12:11:00,697 Successfully sent 1/1 messages to broker 0\r\n2019-01-25 12:11:00,698 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,698 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,701 Successfully sent 1/1 messages to broker 2\r\n2019-01-25 12:11:00,702 Successfully sent 1/1 messages to broker 0\r\n2019-01-25 12:11:00,704 Fetched 2 messages for partition 0\r\n2019-01-25 12:11:00,704 Partition 0 queue holds 2 messages\r\n2019-01-25 12:11:00,705 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,706 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,710 Successfully sent 1/1 messages to broker 2\r\n2019-01-25 12:11:00,710 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,711 Successfully sent 1/1 messages to broker 0\r\n2019-01-25 12:11:00,711 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,713 Successfully sent 1/1 messages to broker 2\r\n2019-01-25 12:11:00,714 Fetched 3 messages for partition 0\r\n2019-01-25 12:11:00,714 Partition 0 queue holds 3 messages\r\n2019-01-25 12:11:00,714 Successfully sent 1/1 messages to broker 0\r\n2019-01-25 12:11:00,717 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,718 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,720 Error encountered when producing to broker ip:9292. Retrying.\r\n2019-01-25 12:11:00,720 Updating cluster, attempt 1/3\r\n2019-01-25 12:11:00,722 Successfully sent 1/1 messages to broker 2\r\n2019-01-25 12:11:00,722 Connecting to ip:9092\r\n2019-01-25 12:11:00,722 Successfully sent 1/1 messages to broker 0\r\n2019-01-25 12:11:00,722 Successfully connected to ip:9092\r\n2019-01-25 12:11:00,726 RequestHandler.stop: about to flush requests queue\r\n2019-01-25 12:11:00,728 Fetched 3 messages for partition 0\r\n2019-01-25 12:11:00,728 Partition 0 queue holds 3 messages\r\n2019-01-25 12:11:00,728 Discovered 3 brokers\r\n2019-01-25 12:11:00,728 Broker ip:9092 metadata unchanged. Continuing.\r\n2019-01-25 12:11:00,728 Broker ip:9192 metadata unchanged. Continuing.\r\n2019-01-25 12:11:00,728 Reconnecting to broker id 2: ip:9292\r\n2019-01-25 12:11:00,729 Connecting to ip:9292\r\n2019-01-25 12:11:00,730 Successfully connected to ip:9292\r\n2019-01-25 12:11:00,730 RequestHandler.stop: about to flush requests queue\r\n2019-01-25 12:11:00,731 Discovered 16 topics\r\n2019-01-25 12:11:00,731 Adding 1 partitions\r\n2019-01-25 12:11:00,731 Starting new produce worker for broker 2\r\n2019-01-25 12:11:00,731 Fetched 2 messages for partition 0\r\n2019-01-25 12:11:00,731 Partition 0 queue holds 2 messages\r\n2019-01-25 12:11:00,731 Successfully sent 0/1 messages to broker 2\r\n2019-01-25 12:11:00,740 Sending 1 messages to broker 2\r\n2019-01-25 12:11:00,740 Sending 1 messages to broker 0\r\n2019-01-25 12:11:00,741 Sending 1 messages to broker 2", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/909", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/909/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/909/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/909/events", "html_url": "https://github.com/Parsely/pykafka/issues/909", "id": 401288954, "node_id": "MDU6SXNzdWU0MDEyODg5NTQ=", "number": 909, "title": "pykafka.exceptions.UnknownTopicOrPartition after kafka topic partition reassignment", "user": {"login": "ethicalmohit", "id": 8760382, "node_id": "MDQ6VXNlcjg3NjAzODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/8760382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ethicalmohit", "html_url": "https://github.com/ethicalmohit", "followers_url": "https://api.github.com/users/ethicalmohit/followers", "following_url": "https://api.github.com/users/ethicalmohit/following{/other_user}", "gists_url": "https://api.github.com/users/ethicalmohit/gists{/gist_id}", "starred_url": "https://api.github.com/users/ethicalmohit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ethicalmohit/subscriptions", "organizations_url": "https://api.github.com/users/ethicalmohit/orgs", "repos_url": "https://api.github.com/users/ethicalmohit/repos", "events_url": "https://api.github.com/users/ethicalmohit/events{/privacy}", "received_events_url": "https://api.github.com/users/ethicalmohit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 360183304, "node_id": "MDU6TGFiZWwzNjAxODMzMDQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/help%20wanted", "name": "help wanted", "color": "006b75", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-01-21T10:33:37Z", "updated_at": "2019-02-02T10:00:19Z", "closed_at": "2019-02-02T10:00:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "We are using sync pykafka producer with 3 nodes of the kafka cluster. We were trying to replace a broker in the cluster in order to do some upgrades.\r\n\r\nWe have done partition reassignment manually of that particular topic to other 2 Kafka brokers while Kafka producer was already running. The partition reassignment was successful but we got the below error as it was trying to communicate with the broker which is no longer serving as a producer for that topic/partition.\r\n\r\nThis goes on until the application server is restarted, i.e; the KafkaClient object is re-created.\r\n\r\n```\r\ndef send(self, topic_name: str, data, raise_exception=False) -> None:\r\n        \"\"\"Publishes a message on a topic\r\n        \"\"\"\r\n        message = self.encode(data)\r\n        topic = self.client.topics[topic_name.encode('utf-8')]\r\n        try:\r\n            with topic.get_sync_producer(linger_ms=0) as producer:\r\n                producer.produce(message)\r\n        except Exception as e:\r\n            monitoring.record_exception()\r\n            logger.exception('Pushing entry to kafka failed for topic: {topic} and message: {message}'.format(\r\n                topic=topic.name,\r\n                message=message\r\n            ))\r\n            if raise_exception:\r\n                raise e\r\n```\r\nActual Behaviour:\r\n\r\n```\r\n[WARNING] [2018-12-20 11:13:33,482] [pykafka.producer] [producer.py:520] Produce request for b'test-load-2001'/2 to b'172.16.45.154':9092 failed with error code 3.\r\n[WARNING] [2018-12-20 11:13:33,607] [pykafka.producer] [producer.py:520] Produce request for b'test-load-2001'/2 to b'172.16.45.154':9092 failed with error code 3.\r\n[WARNING] [2018-12-20 11:13:33,778] [pykafka.producer] [producer.py:520] Produce request for b'test-load-2001'/2 to b'172.16.45.154':9092 failed with error code 3.\r\n[WARNING] [2018-12-20 11:13:33,907] [pykafka.producer] [producer.py:520] Produce request for b'test-load-2001'/2 to b'172.16.45.154':9092 failed with error code 3.\r\n[ERROR] [2018-12-20 11:13:34,092] [pykafka.producer] [producer.py:551] Message not delivered!! UnknownTopicOrPartition(\"Produce request for b'test-load-2001'/2 to b'172.16.45.154':9092 failed with error code 3.\",)\r\n[ERROR] [2018-12-20 11:13:34,095] [root] [<console>:8] Failure.\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 5, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/pykafka/producer.py\", line 413, in produce\r\n    raise exc\r\npykafka.exceptions.UnknownTopicOrPartition: Produce request for b'test-load-2001'/2 to b'172.16.45.154':9092 failed with error code 3.\r\n```\r\n\r\nExpected behavior: Kafka client should recover from these errors automatically, by updating cluster metadata.\r\n\r\n**PyKafka version**:  3.6\r\n**Kafka version**: 1.1.0\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/902", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/902/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/902/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/902/events", "html_url": "https://github.com/Parsely/pykafka/issues/902", "id": 396387659, "node_id": "MDU6SXNzdWUzOTYzODc2NTk=", "number": 902, "title": "Missing LZ4 documentation", "user": {"login": "carsonip", "id": 9133397, "node_id": "MDQ6VXNlcjkxMzMzOTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9133397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carsonip", "html_url": "https://github.com/carsonip", "followers_url": "https://api.github.com/users/carsonip/followers", "following_url": "https://api.github.com/users/carsonip/following{/other_user}", "gists_url": "https://api.github.com/users/carsonip/gists{/gist_id}", "starred_url": "https://api.github.com/users/carsonip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carsonip/subscriptions", "organizations_url": "https://api.github.com/users/carsonip/orgs", "repos_url": "https://api.github.com/users/carsonip/repos", "events_url": "https://api.github.com/users/carsonip/events{/privacy}", "received_events_url": "https://api.github.com/users/carsonip/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-01-07T08:13:31Z", "updated_at": "2019-01-07T17:45:07Z", "closed_at": "2019-01-07T17:45:07Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In https://pykafka.readthedocs.io/en/latest/api/common.html#pykafka.common.CompressionType there is no LZ4, despite LZ4's existence in code (https://github.com/Parsely/pykafka/blob/master/pykafka/common.py#L51).\r\n\r\nThe doc seems quite outdated. Also note the 2015 copyright in footer. Please fix. Thanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/901", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/901/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/901/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/901/events", "html_url": "https://github.com/Parsely/pykafka/issues/901", "id": 395041619, "node_id": "MDU6SXNzdWUzOTUwNDE2MTk=", "number": 901, "title": "High CPU Utilization with Synchronous Producer", "user": {"login": "rjemanuele", "id": 377119, "node_id": "MDQ6VXNlcjM3NzExOQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/377119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rjemanuele", "html_url": "https://github.com/rjemanuele", "followers_url": "https://api.github.com/users/rjemanuele/followers", "following_url": "https://api.github.com/users/rjemanuele/following{/other_user}", "gists_url": "https://api.github.com/users/rjemanuele/gists{/gist_id}", "starred_url": "https://api.github.com/users/rjemanuele/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rjemanuele/subscriptions", "organizations_url": "https://api.github.com/users/rjemanuele/orgs", "repos_url": "https://api.github.com/users/rjemanuele/repos", "events_url": "https://api.github.com/users/rjemanuele/events{/privacy}", "received_events_url": "https://api.github.com/users/rjemanuele/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-12-31T23:30:17Z", "updated_at": "2019-01-07T19:37:37Z", "closed_at": "2019-01-04T00:17:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Greetings!\r\n\r\nI'm using pykafka 2.8.0 on Python 3.7.1.  When producing messages into my Kafka cluster I am experiencing very high CPU load on some secondary thread within my Python script.  I'm guessing this secondary thread is one created by the pykafka producer.\r\n\r\n![screen shot 2018-12-31 at 3 35 29 pm 1](https://user-images.githubusercontent.com/377119/50569054-cc051100-0d11-11e9-94aa-409d5b8fadc6.png)\r\n\r\nI am creating my producer as so:\r\n`producer = kafka.topics['test_topic'].get_producer(delivery_reports=False, linger_ms=0, sync=True)`\r\n\r\nA full example is at: https://github.com/rjemanuele/pykafka_debug\r\n\r\nThank you", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/895", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/895/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/895/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/895/events", "html_url": "https://github.com/Parsely/pykafka/issues/895", "id": 388032277, "node_id": "MDU6SXNzdWUzODgwMzIyNzc=", "number": 895, "title": "Consumer Fetcher Thread exits due to connection issue", "user": {"login": "rvsharath", "id": 45646255, "node_id": "MDQ6VXNlcjQ1NjQ2MjU1", "avatar_url": "https://avatars3.githubusercontent.com/u/45646255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rvsharath", "html_url": "https://github.com/rvsharath", "followers_url": "https://api.github.com/users/rvsharath/followers", "following_url": "https://api.github.com/users/rvsharath/following{/other_user}", "gists_url": "https://api.github.com/users/rvsharath/gists{/gist_id}", "starred_url": "https://api.github.com/users/rvsharath/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rvsharath/subscriptions", "organizations_url": "https://api.github.com/users/rvsharath/orgs", "repos_url": "https://api.github.com/users/rvsharath/repos", "events_url": "https://api.github.com/users/rvsharath/events{/privacy}", "received_events_url": "https://api.github.com/users/rvsharath/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-12-06T02:41:09Z", "updated_at": "2018-12-12T23:40:25Z", "closed_at": "2018-12-12T23:40:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "I Have a client which is making a call to create a consumer and i see that from the logs, initial connection to the broker is established successfully. \r\n\r\nlog output:\r\n    2018-12-05 11:46:43,549:pykafka.connection:connection:connect:164:DEBUG:Connecting to broker1:9092\r\n  2018-12-05 11:46:43,751:pykafka.connection:connection:connect:174:DEBUG:Successfully connected to broker1:9092\r\n                   .....................................\r\n                   .....................................\r\n\r\n 2018-12-05 11:46:44,200:pykafka.simpleconsumer:simpleconsumer:_setup_fetch_workers:454:INFO:Starting 1 fetcher threads\r\n                         .......................................\r\n                         .......................................\r\n\r\n--> after this point the control comes back to the client which made a call to get_simple_consumer, since no exception was thrown at this point, i went ahead and tried to consume(). \r\n\r\n2018-12-05 12:02:22,832:pykafka.simpleconsumer:simpleconsumer:fetch:810:INFO:Updating cluster in response to error in fetch() for broker id 1 \r\n2018-12-05 12:02:22,834:pykafka.connection:connection:connect:164:DEBUG:Connecting to broker1:9092\r\n                         ..................After multiple attempts..........\r\n                         .................................................................\r\n2018-12-05 12:06:55,810:pykafka.cluster:cluster:_request_random_broker:284:ERROR:Socket disconnected during request for broker broker1:9092. Continuing.\r\n    167 2018-12-05 12:06:55,811:pykafka.simpleconsumer:simpleconsumer:fetcher:453:DEBUG:Fetcher thread exiting\r\n\r\n\r\nWhen pykafka spawned the fetcher thread, it was unable to connect to the broker to update the cluster with offset information. After multiple retrials, the fetcher thread exited. In client i have an implementation to repeatedly try if there is SocketDisconnectedError, if so then it continuously retries. \r\n\r\nis it by design that the main thread does not yeild to see if the fetcher thread successfully connected to the broker?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/893", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/893/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/893/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/893/events", "html_url": "https://github.com/Parsely/pykafka/issues/893", "id": 383680501, "node_id": "MDU6SXNzdWUzODM2ODA1MDE=", "number": 893, "title": "Kafka consumer runtime throws an exception", "user": {"login": "zengzhiying", "id": 13583823, "node_id": "MDQ6VXNlcjEzNTgzODIz", "avatar_url": "https://avatars1.githubusercontent.com/u/13583823?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zengzhiying", "html_url": "https://github.com/zengzhiying", "followers_url": "https://api.github.com/users/zengzhiying/followers", "following_url": "https://api.github.com/users/zengzhiying/following{/other_user}", "gists_url": "https://api.github.com/users/zengzhiying/gists{/gist_id}", "starred_url": "https://api.github.com/users/zengzhiying/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zengzhiying/subscriptions", "organizations_url": "https://api.github.com/users/zengzhiying/orgs", "repos_url": "https://api.github.com/users/zengzhiying/repos", "events_url": "https://api.github.com/users/zengzhiying/events{/privacy}", "received_events_url": "https://api.github.com/users/zengzhiying/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-23T01:04:41Z", "updated_at": "2018-11-27T01:59:12Z", "closed_at": "2018-11-27T01:59:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "One of my online programs, the runtime throws the following error. \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bulk_srvdb_scylla.py\", line 334, in <module>\r\n    message = balanced_consumer.consume()\r\n  File \"/yisa_bigdata/python2.7/lib/python2.7/site-packages/pykafka-2.8.0-py2.7-linux-x86_64.egg/pykafka/balancedconsumer.py\", line 748, in consume\r\n    message = self._consumer.consume(block=block, unblock_event=self._rebalancing_in_progress)\r\n  File \"/yisa_bigdata/python2.7/lib/python2.7/site-packages/pykafka-2.8.0-py2.7-linux-x86_64.egg/pykafka/simpleconsumer.py\", line 483, in consume\r\n    self._raise_worker_exceptions()\r\n  File \"/yisa_bigdata/python2.7/lib/python2.7/site-packages/pykafka-2.8.0-py2.7-linux-x86_64.egg/pykafka/simpleconsumer.py\", line 276, in _raise_worker_exceptions\r\n    reraise(*self._worker_exception)\r\n  File \"/yisa_bigdata/python2.7/lib/python2.7/site-packages/pykafka-2.8.0-py2.7-linux-x86_64.egg/pykafka/simpleconsumer.py\", line 440, in fetcher\r\n    self.fetch()\r\n  File \"/yisa_bigdata/python2.7/lib/python2.7/site-packages/pykafka-2.8.0-py2.7-linux-x86_64.egg/pykafka/simpleconsumer.py\", line 823, in fetch\r\n    success_handler=_handle_success)\r\n  File \"/yisa_bigdata/python2.7/lib/python2.7/site-packages/pykafka-2.8.0-py2.7-linux-x86_64.egg/pykafka/utils/error_handlers.py\", line 50, in handle_partition_responses\r\n    error_handlers[errcode](parts)\r\nKeyError: 2\r\n```\r\nThe consumer structure in the code is as follows: \r\n```python\r\n kafka_client = KafkaClient(zookeeper_hosts=configs['kafka']['zookeeper_hosts'])\r\n topic = kafka_client.topics[configs['kafka']['topic']]\r\n balanced_consumer = topic.get_balanced_consumer(\r\n        consumer_group=configs['kafka']['group_id'],\r\n        auto_commit_enable=True,\r\n        reset_offset_on_start=False,\r\n        auto_offset_reset=OffsetType.LATEST,\r\n        zookeeper_connect=configs['kafka']['zookeeper_hosts']\r\n )\r\n```\r\nThe program has been running stably before, and the parameters have not been modified during the operation.\r\nWhat is the reason? thank you very much\r\n\r\nKafka Version: kafka_2.11-1.0.1\r\npykafka Version: 2.8.0\r\npython Version: 2.7.5\r\n\r\nExcuse me, my English is poor.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/892", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/892/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/892/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/892/events", "html_url": "https://github.com/Parsely/pykafka/issues/892", "id": 382686078, "node_id": "MDU6SXNzdWUzODI2ODYwNzg=", "number": 892, "title": "balanced_consumer using #partitions threads ", "user": {"login": "Gaturron", "id": 6035670, "node_id": "MDQ6VXNlcjYwMzU2NzA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6035670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gaturron", "html_url": "https://github.com/Gaturron", "followers_url": "https://api.github.com/users/Gaturron/followers", "following_url": "https://api.github.com/users/Gaturron/following{/other_user}", "gists_url": "https://api.github.com/users/Gaturron/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gaturron/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gaturron/subscriptions", "organizations_url": "https://api.github.com/users/Gaturron/orgs", "repos_url": "https://api.github.com/users/Gaturron/repos", "events_url": "https://api.github.com/users/Gaturron/events{/privacy}", "received_events_url": "https://api.github.com/users/Gaturron/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-20T14:23:05Z", "updated_at": "2018-11-21T19:13:27Z", "closed_at": "2018-11-21T19:13:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "First of all, Thanks for all you guys for this great client! It's notable the efford that each one have coding it.\r\n\r\nNow the question: Is there any way to config a balanced_consumer that lanched N threads where N is the number of partition of the topic? I mean this is the best configuration for consuming a topic correctly. And perhaps it is a good enhancement to have it already coded. Also, I think that Java client have this setting.\r\n\r\nOtherwise you have to make N python producers by your own. That's what I'm doing right now.\r\n\r\n**PyKafka version**: 2.8.0\r\n**Kafka version**: 0.8.2.2\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/887", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/887/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/887/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/887/events", "html_url": "https://github.com/Parsely/pykafka/issues/887", "id": 378225008, "node_id": "MDU6SXNzdWUzNzgyMjUwMDg=", "number": 887, "title": "Dependabot couldn't fetch all your path-based dependencies", "user": {"login": "dependabot-preview[bot]", "id": 27856297, "node_id": "MDM6Qm90Mjc4NTYyOTc=", "avatar_url": "https://avatars3.githubusercontent.com/in/2141?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dependabot-preview%5Bbot%5D", "html_url": "https://github.com/apps/dependabot-preview", "followers_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/followers", "following_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/repos", "events_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/dependabot-preview%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-07T10:20:10Z", "updated_at": "2018-11-08T23:02:42Z", "closed_at": "2018-11-08T23:02:42Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Dependabot couldn't fetch one or more of your project's path-based Python dependencies. The affected dependencies were `git+https:/github.com/Parsely/testinstances.git@0.3.0/setup.py`.\n\nTo use path-based dependancies with Dependabot the paths must be relative and resolve to a directory in this project's source code.\n\nYou can mention @dependabot in the comments below to contact the Dependabot team.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/881", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/881/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/881/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/881/events", "html_url": "https://github.com/Parsely/pykafka/issues/881", "id": 373904134, "node_id": "MDU6SXNzdWUzNzM5MDQxMzQ=", "number": 881, "title": "Offset 0 is skipped when consuming", "user": {"login": "jacopofar", "id": 1280346, "node_id": "MDQ6VXNlcjEyODAzNDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1280346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacopofar", "html_url": "https://github.com/jacopofar", "followers_url": "https://api.github.com/users/jacopofar/followers", "following_url": "https://api.github.com/users/jacopofar/following{/other_user}", "gists_url": "https://api.github.com/users/jacopofar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacopofar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacopofar/subscriptions", "organizations_url": "https://api.github.com/users/jacopofar/orgs", "repos_url": "https://api.github.com/users/jacopofar/repos", "events_url": "https://api.github.com/users/jacopofar/events{/privacy}", "received_events_url": "https://api.github.com/users/jacopofar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-25T11:38:01Z", "updated_at": "2018-10-29T17:43:50Z", "closed_at": "2018-10-29T17:43:50Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm trying to read all the rows from a partitioned topic, and noticed that the message with offset 0 at each partition is apparently skipped. After that, every other message comes as expected.\r\n\r\nI'm using `pykafka 2.8.0` with `Python 3.7` on macOS.\r\n\r\nI use `earliest_available_offset` to get the first offset of every partition (which in my case is always 0)  and pass them to `reset_offsets` to initialize a consumer for each partition.\r\n\r\nReading with `consumer.consume(block=False)` I can get the messages and their offset and cannot see the message at offset 0. Using a tool like [Lenses](https://www.landoop.com/lenses-overview/) I see indeed the message at offset 0 and in the end I get every message except for the three what happen to be at offset 0 in each of the three partition.\r\n\r\nI've looked at previous issues and can't find anything about this problem. Also I tried to pass `-1` instead of `0` to `reset offsets` but in that case the consumer simply hang.\r\n\r\n<details><summary>minimal code to reproduce the issue</summary>\r\n<p>\r\n\r\n\r\n```python\r\nfrom functools import lru_cache\r\nimport io\r\nimport json\r\nfrom pathlib import Path\r\nimport struct\r\n\r\nfrom fastavro import schemaless_reader\r\nfrom pykafka import KafkaClient, common\r\n\r\n\r\nbootstrap_servers = ','.join([\r\n    'broker-01.local:9091',\r\n    'broker-02.local:9091',\r\n    'broker-03.local:9091',\r\n    'broker-04.local:9091',\r\n    'broker-05.local:9091',\r\n])\r\n\r\n\r\n@lru_cache()\r\ndef get_schema(schema_id):\r\n    '''Get the schema object from the FS'''\r\n    cached_file = Path(\"schema/\" + str(schema_id) + \".json\")\r\n    if cached_file.is_file():\r\n        f = open(cached_file, 'r')\r\n        ret = json.loads(f.read())\r\n        f.close()\r\n        return ret\r\n    raise IOError('no schema', schema_id)\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    client = KafkaClient(hosts=bootstrap_servers)\r\n    topic_obj = list(client.topics.keys())\r\n\r\n    topic_consumers = []\r\n    topic = 'data-master-cities'\r\n    consumer = client.topics[\r\n        topic.encode('utf8')\r\n    ].get_simple_consumer(\r\n        auto_offset_reset=common.OffsetType.EARLIEST,\r\n        # We go read-only, avoid side effects\r\n        reset_offset_on_fetch=False,\r\n        auto_commit_enable=False\r\n    )\r\n\r\n    partition_offset_pairs = []\r\n    for p_index, p in consumer.partitions.items():\r\n        # just start from the beginning\r\n        first_offset = p.earliest_available_offset()\r\n        print('For the partition', p_index, ' initial offset: ', first_offset)\r\n        partition_offset_pairs.append((p, first_offset))\r\n    print('partition offset pairs for topic', topic, partition_offset_pairs)\r\n    consumer.reset_offsets(partition_offsets=partition_offset_pairs)\r\n    topic_consumers.append((topic, consumer))\r\n\r\n    print('All the consumers are ready, start consuming from Kafka')\r\n    while True:\r\n        for topic, consumer in topic_consumers:\r\n            message = consumer.consume(block=False)\r\n            if message is not None:\r\n                magic, schema_id = struct.unpack('>bI', message.value[:5])\r\n                schema = get_schema(schema_id)\r\n                payload = schemaless_reader(\r\n                    io.BytesIO(message.value[5:]),\r\n                    schema)\r\n\r\n                with open(f'cities_{message.partition_id}.jsonl', 'a') as f:\r\n                    f.write(f'{message.offset} \\n')\r\n                    f.write(json.dumps(payload))\r\n                    f.write('\\n')\r\n\r\n```\r\n\r\nThis produces\r\n\r\n```\r\nFor the partition 2  initial offset:  0\r\nFor the partition 1  initial offset:  0\r\nFor the partition 0  initial offset:  0\r\npartition offset pairs for topic data-master-cities [(<pykafka.partition.Partition at 0x10ec43588 (id=2)>, 0), (<pykafka.partition.Partition at 0x10e49d9e8 (id=1)>, 0), (<pykafka.partition.Partition at 0x10e49d978 (id=0)>, 0)]\r\n```\r\n\r\nand then I interrupt the program and in the files I see they contain everything I can see on Lenses, with the same order, except for the three messages at offset 0.\r\n</p>\r\n</details>\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/875", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/875/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/875/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/875/events", "html_url": "https://github.com/Parsely/pykafka/issues/875", "id": 367098346, "node_id": "MDU6SXNzdWUzNjcwOTgzNDY=", "number": 875, "title": "Support for exactly once processing guarantee", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-05T07:51:47Z", "updated_at": "2020-07-02T04:54:47Z", "closed_at": "2020-07-02T04:54:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @emmett9001 \r\n\r\nI was wondering if pykafka 2.8.0 supports the exactly once processing guarantee which is part of Kafka since 0.11 release (just read https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/).\r\n\r\nThe reason for asking is that I'm having a Kafka streaming application running with \"processing.guarantee=exactly_once\" enabled, which reads from one topic and branches into multiple topics based on some predicates. But when consuming from one of this topics (the ones that are branched, so the ones that are filled by the streaming application) the SimpleConsumer fails with this error:\r\n\r\n`\r\nTraceback (most recent call last):\r\n  File \"consume.py\", line 47, in <module>\r\n    p.start()\r\n  File \"consume.py\", line 38, in start\r\n    msg = self._consumer.consume(block=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/simpleconsumer.py\", line 483, in consume\r\n    self._raise_worker_exceptions()\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/simpleconsumer.py\", line 276, in _raise_worker_exceptions\r\n    reraise(*self._worker_exception)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/simpleconsumer.py\", line 440, in fetcher\r\n    self.fetch()\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/simpleconsumer.py\", line 804, in fetch\r\n    min_bytes=self._fetch_min_bytes\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/broker.py\", line 45, in wrapped\r\n    return fn(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/broker.py\", line 327, in fetch_messages\r\n    return future.get(response_class, broker_version=self._broker_version)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/handlers.py\", line 76, in get\r\n    return response_cls(self.response, **response_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/protocol/fetch.py\", line 216, in __init__\r\n    broker_version=broker_version)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/protocol/fetch.py\", line 165, in __init__\r\n    broker_version=broker_version),\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/protocol/fetch.py\", line 172, in _unpack_message_set\r\n    message_set = MessageSet.decode(buff, partition_id=partition_id)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/protocol/message.py\", line 272, in decode\r\n    partition_id=partition_id)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/protocol/message.py\", line 109, in decode\r\n    (key, val) = struct_helpers.unpack_from('YY', buff, offset)\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/utils/struct_helpers.py\", line 49, in unpack_from\r\n    output = _unpack(fmt, buff, offset, 1)[0]\r\n  File \"/usr/local/lib/python2.7/dist-packages/pykafka/utils/struct_helpers.py\", line 96, in _unpack\r\n    items.extend(struct.unpack_from('!' + ch, buff, offset))\r\nstruct.error: unpack_from requires a buffer of at least 1088403925 bytes\r\n`\r\n\r\nI'm running Kafka 1.1.0 cluster with 3 nodes and with replication factor 3. I'm also providing the right broker_version to the KafkaClient.\r\n\r\nI've been debugging this for a while and I found out that this \"processing.guarantee=exactly_once\" in the streaming app is causing the struct.error. When removing this setting, SimpleConsumer is consuming the branched topics like a charm. I also added some debug lines in the pykafka code, and it seems that there are some extra bytes in the buffer which could not be parsed to a message. So that brings me to my initial question, if pykafka is supporting this exactly once feature.\r\n\r\nFurthermore another related issue is that I've seen gaps in the offsets of the branched topics, which pykafka cannot handle well: the enqueue function in SimpleConsumer handles it for non compacted topics like this:\r\n\r\n`(not self._is_compacted_topic and message.offset != self.next_offset):`\r\n\r\nWhich skips items when there's a gap. message.offset goes for example from 3 to 5 but next_offset expects 4 and this never happens and skips all messages because message.offset will never turn to 4. This only occurs on those branched topics by the streaming app. Just mentioning because it may ring a bell.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/865", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/865/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/865/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/865/events", "html_url": "https://github.com/Parsely/pykafka/issues/865", "id": 360904215, "node_id": "MDU6SXNzdWUzNjA5MDQyMTU=", "number": 865, "title": "Pykafka failing with AttributeError: 'module' object has no attribute 'Timeout'", "user": {"login": "mukkanti1241", "id": 40744137, "node_id": "MDQ6VXNlcjQwNzQ0MTM3", "avatar_url": "https://avatars2.githubusercontent.com/u/40744137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mukkanti1241", "html_url": "https://github.com/mukkanti1241", "followers_url": "https://api.github.com/users/mukkanti1241/followers", "following_url": "https://api.github.com/users/mukkanti1241/following{/other_user}", "gists_url": "https://api.github.com/users/mukkanti1241/gists{/gist_id}", "starred_url": "https://api.github.com/users/mukkanti1241/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mukkanti1241/subscriptions", "organizations_url": "https://api.github.com/users/mukkanti1241/orgs", "repos_url": "https://api.github.com/users/mukkanti1241/repos", "events_url": "https://api.github.com/users/mukkanti1241/events{/privacy}", "received_events_url": "https://api.github.com/users/mukkanti1241/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-17T14:46:21Z", "updated_at": "2018-09-17T19:41:42Z", "closed_at": "2018-09-17T16:07:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI started building Kafka application using Pykafka and ended up with issues. Please help me out to fix it.\r\nI am getting the following error when I executed.\r\n\r\nFile \"C:\\Users\\167212\\Anaconda2\\lib\\site-packages\\kazoo\\handlers\\gevent.py\", line 63, in SequentialGeventHandler\r\nclass timeout_exception(gevent.event.Timeout):\r\n\r\nAttributeError: 'module' object has no attribute 'Timeout'\r\n![pykafka error](https://user-images.githubusercontent.com/40744137/45630403-a0159680-ba66-11e8-8863-5dd3bbf7459f.png)\r\n\r\nI am using the following versions:\r\npykafka-2.7.0\r\nkazoo-2.4.0\r\ntabulate-0.8.2\r\nconda-4.5.11\r\n\r\nPlease find the attached screen shot and help me out ASAP.\r\n\r\nThanks,\r\nMukkanti", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/860", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/860/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/860/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/860/events", "html_url": "https://github.com/Parsely/pykafka/issues/860", "id": 354816356, "node_id": "MDU6SXNzdWUzNTQ4MTYzNTY=", "number": 860, "title": "feature request: allow log level configuration", "user": {"login": "cdaringe", "id": 1003261, "node_id": "MDQ6VXNlcjEwMDMyNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/1003261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cdaringe", "html_url": "https://github.com/cdaringe", "followers_url": "https://api.github.com/users/cdaringe/followers", "following_url": "https://api.github.com/users/cdaringe/following{/other_user}", "gists_url": "https://api.github.com/users/cdaringe/gists{/gist_id}", "starred_url": "https://api.github.com/users/cdaringe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cdaringe/subscriptions", "organizations_url": "https://api.github.com/users/cdaringe/orgs", "repos_url": "https://api.github.com/users/cdaringe/repos", "events_url": "https://api.github.com/users/cdaringe/events{/privacy}", "received_events_url": "https://api.github.com/users/cdaringe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-08-28T16:49:12Z", "updated_at": "2019-03-19T19:10:26Z", "closed_at": "2019-03-19T19:10:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Kafka version**: 2.x\r\n\r\n# problem\r\n\r\n`pykafka` can be _very, very noisy_, and doesn't expose any config for its logging mechanisms\r\n\r\n# discussion\r\n\r\nit would be great if we could config the loggers via any supported means", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/855", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/855/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/855/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/855/events", "html_url": "https://github.com/Parsely/pykafka/issues/855", "id": 351225673, "node_id": "MDU6SXNzdWUzNTEyMjU2NzM=", "number": 855, "title": "A \"pykafka.exceptions.UnknownMemberId\" error occurred while consuming the message in the program", "user": {"login": "ltwin", "id": 33997314, "node_id": "MDQ6VXNlcjMzOTk3MzE0", "avatar_url": "https://avatars1.githubusercontent.com/u/33997314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ltwin", "html_url": "https://github.com/ltwin", "followers_url": "https://api.github.com/users/ltwin/followers", "following_url": "https://api.github.com/users/ltwin/following{/other_user}", "gists_url": "https://api.github.com/users/ltwin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ltwin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ltwin/subscriptions", "organizations_url": "https://api.github.com/users/ltwin/orgs", "repos_url": "https://api.github.com/users/ltwin/repos", "events_url": "https://api.github.com/users/ltwin/events{/privacy}", "received_events_url": "https://api.github.com/users/ltwin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-16T14:17:30Z", "updated_at": "2018-08-16T21:44:14Z", "closed_at": "2018-08-16T21:44:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "**PyKafka version**: 2.7.0\r\n**Kafka version**: 1.1.0\r\n**Log info**: \r\n\r\n> 2018-08-16 21:42:56\tINFO\tsimpleconsumer.py: 301\tContinuing in response to UnknownMemberId\r\n2018-08-16 21:42:56\tERROR\tsimpleconsumer.py: 519\tError committing offsets for topic 'secondary' from consumer id 'pykafka-77974ba8-b5fc-4b8c-b1bf-538c079c38ae'(errors: {<class 'pykafka.exceptions.UnknownMemberId'>: [4, 5]})\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/854", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/854/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/854/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/854/events", "html_url": "https://github.com/Parsely/pykafka/issues/854", "id": 350977499, "node_id": "MDU6SXNzdWUzNTA5Nzc0OTk=", "number": 854, "title": "Handling connection loss in Topic", "user": {"login": "rueberger", "id": 8816362, "node_id": "MDQ6VXNlcjg4MTYzNjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/8816362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rueberger", "html_url": "https://github.com/rueberger", "followers_url": "https://api.github.com/users/rueberger/followers", "following_url": "https://api.github.com/users/rueberger/following{/other_user}", "gists_url": "https://api.github.com/users/rueberger/gists{/gist_id}", "starred_url": "https://api.github.com/users/rueberger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rueberger/subscriptions", "organizations_url": "https://api.github.com/users/rueberger/orgs", "repos_url": "https://api.github.com/users/rueberger/repos", "events_url": "https://api.github.com/users/rueberger/events{/privacy}", "received_events_url": "https://api.github.com/users/rueberger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 439232563, "node_id": "MDU6TGFiZWw0MzkyMzI1NjM=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/info-archive", "name": "info-archive", "color": "0e8a16", "default": false, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-15T21:41:25Z", "updated_at": "2018-08-15T22:15:20Z", "closed_at": "2018-08-15T22:15:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Sometimes I run into a scenario like this:\r\n\r\n```\r\nclient = pykafka.KafkaClient(...)\r\ntopic = client.topics['test_topic']\r\nlatest = topic.latest_available_offset() # raises SocketDisconnectedError\r\n```\r\n\r\nIn this scenario my general approach would be to reinstantiate the topic `topic = client.topics['test_topic']`. \r\n\r\nHowever, I'm reasonably confident (have not tested explicitly, but reading between the lines) that this is not always sufficient, and there are cases when the reinstantiated topic still has connection problems. \r\n\r\nThus I'm now planning to reinstantiate the client as well in this scenario, which hopefully will work.\r\n\r\nBut I just wanted to check to see if there is a recommended best practice for this scenario. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/852", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/852/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/852/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/852/events", "html_url": "https://github.com/Parsely/pykafka/issues/852", "id": 348025434, "node_id": "MDU6SXNzdWUzNDgwMjU0MzQ=", "number": 852, "title": "Bug in gevent 1.3.x triggers problem publishing with delivery_reports enabled", "user": {"login": "bpowers39", "id": 40673070, "node_id": "MDQ6VXNlcjQwNjczMDcw", "avatar_url": "https://avatars2.githubusercontent.com/u/40673070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bpowers39", "html_url": "https://github.com/bpowers39", "followers_url": "https://api.github.com/users/bpowers39/followers", "following_url": "https://api.github.com/users/bpowers39/following{/other_user}", "gists_url": "https://api.github.com/users/bpowers39/gists{/gist_id}", "starred_url": "https://api.github.com/users/bpowers39/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bpowers39/subscriptions", "organizations_url": "https://api.github.com/users/bpowers39/orgs", "repos_url": "https://api.github.com/users/bpowers39/repos", "events_url": "https://api.github.com/users/bpowers39/events{/privacy}", "received_events_url": "https://api.github.com/users/bpowers39/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 981765487, "node_id": "MDU6TGFiZWw5ODE3NjU0ODc=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/dependencies", "name": "dependencies", "color": "0025ff", "default": false, "description": null}, {"id": 603289911, "node_id": "MDU6TGFiZWw2MDMyODk5MTE=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/gevent", "name": "gevent", "color": "e99695", "default": false, "description": null}, {"id": 8945348, "node_id": "MDU6TGFiZWw4OTQ1MzQ4", "url": "https://api.github.com/repos/Parsely/pykafka/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-06T18:01:53Z", "updated_at": "2018-10-15T18:04:12Z", "closed_at": "2018-10-15T18:04:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Due to a Gevent bug here https://github.com/gevent/gevent/issues/1266, publishing synchronously, or with delivery_reports error results in a exception being thrown.\r\n\r\nPython Version: 3.6.5\r\nPykafka Version: 2.6.0\r\nGevent Version: 1.3.4 (works with 1.1.0)\r\n\r\n### Example Stack Trace\r\n```\r\n  File \"/bb/bin/ibwebapi-dependencies.zip/pykafka/producer.py\", line 333, in produce\r\n    self._raise_worker_exceptions()\r\n  File \"/bb/bin/ibwebapi-dependencies.zip/pykafka/producer.py\", line 197, in _raise_worker_exceptions\r\n    reraise(*self._worker_exception)\r\n  File \"/opt/bb/lib/python3.6/site-packages/six.py\", line 692, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/bb/bin/ibwebapi-dependencies.zip/pykafka/producer.py\", line 537, in queue_reader\r\n    self.producer._send_request(batch, self)\r\n  File \"/bb/bin/ibwebapi-dependencies.zip/pykafka/producer.py\", line 428, in _send_request\r\n    mark_as_delivered(messages)\r\n  File \"/bb/bin/ibwebapi-dependencies.zip/pykafka/producer.py\", line 410, in mark_as_delivered\r\n    self._delivery_reports.put(msg)\r\n  File \"/bb/bin/ibwebapi-dependencies.zip/pykafka/producer.py\", line 685, in put\r\n    msg.delivery_report_q.put((msg, exc))\r\n  File \"src/gevent/local.py\", line 455, in gevent._local.local.__getattribute__\r\nAttributeError: '_DeliveryReportQueue' object has no attribute 'delivery_report_q'\r\n```\r\n\r\nTo work around this issue temporarily, I patched the producer to drop the ```@staticmethod``` decorator on the _DeliveryReportQueue class. There doesn't seem to be a good reason these functions are static. I've attached the patch below. Not sure this is worth a PR, since it will be fixed upstream soon, but I figured I'd mention it here in case anyone else runs into this issue.\r\n\r\n```diff\r\n--- a/pykafka/producer.py\r\n+++ b/pykafka/producer.py\r\n@@ -680,8 +680,7 @@ class _DeliveryReportQueue(threading.local):\r\n     def __init__(self, handler):\r\n         self.queue = handler.Queue()\r\n\r\n-    @staticmethod\r\n-    def put(msg, exc=None):\r\n+    def put(self, msg, exc=None):\r\n         msg.delivery_report_q.put((msg, exc))\r\n\r\n\r\n@@ -690,6 +689,5 @@ class _DeliveryReportNone(object):\r\n     def __init__(self):\r\n         self.queue = None\r\n\r\n-    @staticmethod\r\n-    def put(msg, exc=None):\r\n+    def put(self, msg, exc=None):\r\n         return\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/851", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/851/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/851/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/851/events", "html_url": "https://github.com/Parsely/pykafka/issues/851", "id": 347646737, "node_id": "MDU6SXNzdWUzNDc2NDY3Mzc=", "number": 851, "title": "Consumer can't get any messages after reset_offsets", "user": {"login": "andrey-puzyr", "id": 28754334, "node_id": "MDQ6VXNlcjI4NzU0MzM0", "avatar_url": "https://avatars1.githubusercontent.com/u/28754334?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrey-puzyr", "html_url": "https://github.com/andrey-puzyr", "followers_url": "https://api.github.com/users/andrey-puzyr/followers", "following_url": "https://api.github.com/users/andrey-puzyr/following{/other_user}", "gists_url": "https://api.github.com/users/andrey-puzyr/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrey-puzyr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrey-puzyr/subscriptions", "organizations_url": "https://api.github.com/users/andrey-puzyr/orgs", "repos_url": "https://api.github.com/users/andrey-puzyr/repos", "events_url": "https://api.github.com/users/andrey-puzyr/events{/privacy}", "received_events_url": "https://api.github.com/users/andrey-puzyr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-04T20:38:23Z", "updated_at": "2018-08-13T18:59:32Z", "closed_at": "2018-08-07T19:36:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following code hangs (on the last consume() call). \r\n\r\n_Case definition: Kafka contains topic `test` with a lot of messages, I'm trying to move offset to some position using the `reset_offsets`. pykafka 2.7.0, Python 3.7.0_\r\n```python\r\nfrom pykafka import KafkaClient\r\n\r\npyclient = KafkaClient(hosts=\"localhost:9092\")\r\ntopic = pyclient.topics[b'test']\r\nc = topic.get_simple_consumer(\r\n    consumer_group=b'mygroup', \r\n    auto_offset_reset=common.OffsetType.EARLIEST)\r\nc.reset_offsets(partition_offsets=[(c.partitions[0], 10)])\r\nc.commit_offsets()\r\nc.consume()\r\n```\r\nNew instances of consumer work as expected (starting from 11 offset).\r\nIt looks like a bug. After some manipulations I have found few workarounds:\r\n```python\r\n# ...\r\nc.reset_offsets(partition_offsets=[(c.partitions[0], 10)])\r\nc.commit_offsets()\r\nc.consume(block=False) # return None\r\nc.consume() # now works as expected\r\n```\r\n```python\r\n# ...\r\nc.reset_offsets(partition_offsets=[(c.partitions[0], 10)])\r\nc.commit_offsets()\r\nc.fetch()\r\nc.consume() # now works as expected\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/850", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/850/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/850/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/850/events", "html_url": "https://github.com/Parsely/pykafka/issues/850", "id": 347399518, "node_id": "MDU6SXNzdWUzNDczOTk1MTg=", "number": 850, "title": "ImportError: cannot import name '_rd_kafka'", "user": {"login": "sunhailin-Leo", "id": 17564655, "node_id": "MDQ6VXNlcjE3NTY0NjU1", "avatar_url": "https://avatars1.githubusercontent.com/u/17564655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sunhailin-Leo", "html_url": "https://github.com/sunhailin-Leo", "followers_url": "https://api.github.com/users/sunhailin-Leo/followers", "following_url": "https://api.github.com/users/sunhailin-Leo/following{/other_user}", "gists_url": "https://api.github.com/users/sunhailin-Leo/gists{/gist_id}", "starred_url": "https://api.github.com/users/sunhailin-Leo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sunhailin-Leo/subscriptions", "organizations_url": "https://api.github.com/users/sunhailin-Leo/orgs", "repos_url": "https://api.github.com/users/sunhailin-Leo/repos", "events_url": "https://api.github.com/users/sunhailin-Leo/events{/privacy}", "received_events_url": "https://api.github.com/users/sunhailin-Leo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 981765487, "node_id": "MDU6TGFiZWw5ODE3NjU0ODc=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/dependencies", "name": "dependencies", "color": "0025ff", "default": false, "description": null}, {"id": 170576876, "node_id": "MDU6TGFiZWwxNzA1NzY4NzY=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/librdkafka", "name": "librdkafka", "color": "006b75", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-08-03T13:35:51Z", "updated_at": "2019-03-21T17:33:16Z", "closed_at": "2019-03-21T17:33:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "**PyKafka version**: 2.7.0\r\n**Kafka version**: 1.0.0\r\n**Python version**: 3.5.4\r\n\r\n```Python\r\nTraceback (most recent call last):\r\n  File \"C:\\Python\\Python35\\lib\\site-packages\\pykafka\\topic.py\", line 42, in <module>\r\n    from . import rdkafka\r\n  File \"C:\\Python\\Python35\\lib\\site-packages\\pykafka\\rdkafka\\__init__.py\", line 1, in <module>\r\n    from .producer import RdKafkaProducer\r\n  File \"C:\\Python\\Python35\\lib\\site-packages\\pykafka\\rdkafka\\producer.py\", line 7, in <module>\r\n    from . import _rd_kafka\r\nImportError: cannot import name '_rd_kafka'\r\n```\r\n\r\nThis error happened when I run in a multiprocess mode.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/846", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/846/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/846/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/846/events", "html_url": "https://github.com/Parsely/pykafka/issues/846", "id": 345963482, "node_id": "MDU6SXNzdWUzNDU5NjM0ODI=", "number": 846, "title": "KeyError: 17 when auto-creating an illegal topic name", "user": {"login": "RichardFoo", "id": 11896412, "node_id": "MDQ6VXNlcjExODk2NDEy", "avatar_url": "https://avatars1.githubusercontent.com/u/11896412?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RichardFoo", "html_url": "https://github.com/RichardFoo", "followers_url": "https://api.github.com/users/RichardFoo/followers", "following_url": "https://api.github.com/users/RichardFoo/following{/other_user}", "gists_url": "https://api.github.com/users/RichardFoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/RichardFoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RichardFoo/subscriptions", "organizations_url": "https://api.github.com/users/RichardFoo/orgs", "repos_url": "https://api.github.com/users/RichardFoo/repos", "events_url": "https://api.github.com/users/RichardFoo/events{/privacy}", "received_events_url": "https://api.github.com/users/RichardFoo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-30T22:38:48Z", "updated_at": "2018-08-15T18:25:04Z", "closed_at": "2018-08-15T18:25:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Connecting to (auto-creating) a non-existent topic with a name that violates rules (e.g., '/sys/foo') causes a fatal error in PyKafka's error reporting.\r\n```\r\n$ ./test.py \r\nTopic b'/sys/foo' not found. Attempting to auto-create.\r\nTraceback (most recent call last):\r\n  File \"./test.py\", line 15, in <module>\r\n    **topic = client.topics[b'/sys/foo']**\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pykafka/cluster.py\", line 63, in __getitem__\r\n    topic_ref = super(TopicDict, self).__getitem__(key)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pykafka/cluster.py\", line 82, in __missing__\r\n    self._create_topic(key)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pykafka/cluster.py\", line 111, in _create_topic\r\n    **raise ERROR_CODES[err](\r\nKeyError: 17**\r\n```\r\n$ pip3 show pykafka\r\nName: pykafka\r\nVersion: 2.7.0\r\n\r\n$ python3 --version\r\nPython 3.6.4", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/843", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/843/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/843/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/843/events", "html_url": "https://github.com/Parsely/pykafka/issues/843", "id": 344733250, "node_id": "MDU6SXNzdWUzNDQ3MzMyNTA=", "number": 843, "title": "\"Consuming the last N messages\" example is broken", "user": {"login": "matthew-d-jones", "id": 13455433, "node_id": "MDQ6VXNlcjEzNDU1NDMz", "avatar_url": "https://avatars2.githubusercontent.com/u/13455433?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthew-d-jones", "html_url": "https://github.com/matthew-d-jones", "followers_url": "https://api.github.com/users/matthew-d-jones/followers", "following_url": "https://api.github.com/users/matthew-d-jones/following{/other_user}", "gists_url": "https://api.github.com/users/matthew-d-jones/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthew-d-jones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthew-d-jones/subscriptions", "organizations_url": "https://api.github.com/users/matthew-d-jones/orgs", "repos_url": "https://api.github.com/users/matthew-d-jones/repos", "events_url": "https://api.github.com/users/matthew-d-jones/events{/privacy}", "received_events_url": "https://api.github.com/users/matthew-d-jones/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 15352879, "node_id": "MDU6TGFiZWwxNTM1Mjg3OQ==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/documentation", "name": "documentation", "color": "0b02e1", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-26T07:52:18Z", "updated_at": "2018-08-15T22:08:43Z", "closed_at": "2018-08-15T22:08:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "The example code for \"Consuming the last N messages from a topic\" (http://pykafka.readthedocs.io/en/latest/usage.html#consuming-the-last-n-messages-from-a-topic) appears to get the last N - 2 messages rather than the last N. \r\n\r\n(note: using python 3.6)\r\n```python\r\nLAST_N_MESSAGES = 10  # adjust this depending on how many messages you want\r\n\r\nconsumer = topic.get_simple_consumer(\r\n    auto_offset_reset=OffsetType.LATEST,\r\n    reset_offset_on_start=True)\r\noffsets = [(p, op.next_offset - LAST_N_MESSAGES) for p, op in consumer._partitions.items()]\r\nconsumer.reset_offsets(offsets)\r\nconsumer.consume()\r\n\r\nfor message in consumer:\r\n    print(\"offset: \" + str(message.offset))\r\n```\r\n\r\nResults in only the last 8 messages rather than the expected 10:\r\n```\r\noffset: 1071434\r\noffset: 1071435\r\noffset: 1071436\r\noffset: 1071437\r\noffset: 1071438\r\noffset: 1071439\r\noffset: 1071440\r\noffset: 1071441\r\n```\r\nWhen 1071442 is the high watermark offset.\r\n\r\nI observe this behaviour on pykafka 2.7.0 and current `master` (commit: 1ba51f6).\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/840", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/840/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/840/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/840/events", "html_url": "https://github.com/Parsely/pykafka/issues/840", "id": 342903472, "node_id": "MDU6SXNzdWUzNDI5MDM0NzI=", "number": 840, "title": "Split protocol.py into multiple files", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-19T21:56:55Z", "updated_at": "2018-07-25T18:04:51Z", "closed_at": "2018-07-25T18:04:51Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The `protocol.py` file has gotten to the point that it's too big to easily manage. It should be split up into one file per Kafka API and importable in the same way it is now.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/836", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/836/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/836/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/836/events", "html_url": "https://github.com/Parsely/pykafka/issues/836", "id": 341744548, "node_id": "MDU6SXNzdWUzNDE3NDQ1NDg=", "number": 836, "title": "use pykafka in pyspark error: AssertionError: is not a subpath of *", "user": {"login": "whwby", "id": 15358928, "node_id": "MDQ6VXNlcjE1MzU4OTI4", "avatar_url": "https://avatars1.githubusercontent.com/u/15358928?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whwby", "html_url": "https://github.com/whwby", "followers_url": "https://api.github.com/users/whwby/followers", "following_url": "https://api.github.com/users/whwby/following{/other_user}", "gists_url": "https://api.github.com/users/whwby/gists{/gist_id}", "starred_url": "https://api.github.com/users/whwby/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whwby/subscriptions", "organizations_url": "https://api.github.com/users/whwby/orgs", "repos_url": "https://api.github.com/users/whwby/repos", "events_url": "https://api.github.com/users/whwby/events{/privacy}", "received_events_url": "https://api.github.com/users/whwby/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 981765487, "node_id": "MDU6TGFiZWw5ODE3NjU0ODc=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/dependencies", "name": "dependencies", "color": "0025ff", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-17T02:38:49Z", "updated_at": "2018-07-18T18:39:30Z", "closed_at": "2018-07-18T18:39:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "version:\r\n```\r\n[root@node-61 ~]# pip list|grep py\r\npykafka                      2.7.0  \r\npyspark                      2.3.1\r\n\r\n[root@node-61 ~]# pyspark\r\nPython 2.7.5 (default, Nov 20 2015, 02:00:19) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n2018-07-17 10:24:42 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\nWelcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.1\r\n      /_/\r\n\r\nUsing Python version 2.7.5 (default, Nov 20 2015 02:00:19)\r\nSparkSession available as 'spark'.\r\n>>> from pykafka import KafkaClient\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/__init__.py\", line 1, in <module>\r\n    from .broker import Broker\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/broker.py\", line 23, in <module>\r\n    from .connection import BrokerConnection\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/connection.py\", line 27, in <module>\r\n    from .utils.socket import recvall_into\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/utils/__init__.py\", line 16, in <module>\r\n    from pkg_resources import parse_version\r\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 2998, in <module>\r\n    _declare_state('object', working_set = WorkingSet())\r\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 476, in __init__\r\n    self.add_entry(entry)\r\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 491, in add_entry\r\n    for dist in find_distributions(entry, True):\r\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 1964, in find_in_zip\r\n    if metadata.has_metadata('PKG-INFO'):\r\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 1415, in has_metadata\r\n    return self.egg_info and self._has(self._fn(self.egg_info,name))\r\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 1737, in _has\r\n    zip_path = self._zipinfo_name(fspath)\r\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 1611, in _zipinfo_name\r\n    \"%s is not a subpath of %s\" % (fspath,self.zip_pre)\r\nAssertionError: /usr/lib/python2.7/site-packages/pyspark-2.3.1-py2.7.egg/EGG-INFO/PKG-INFO is not a subpath of /usr/lib/python2.7/site-packages/pyspark-2.3.1-py2.7.egg/pyspark/python/lib/py4j-0.10.7-src.zip/\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/833", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/833/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/833/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/833/events", "html_url": "https://github.com/Parsely/pykafka/issues/833", "id": 340809136, "node_id": "MDU6SXNzdWUzNDA4MDkxMzY=", "number": 833, "title": "Standardize timestamp interfaces", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-07-12T21:43:38Z", "updated_at": "2018-07-23T21:40:32Z", "closed_at": "2018-07-19T21:09:30Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "There are a few functions in the public API that accept timestamps, for example [`Topic.fetch_offset_limits`](https://github.com/Parsely/pykafka/blob/83e61962d240448f8c611971157376d4866101a0/pykafka/topic.py#L110). All such functions should accept a `datetime.datetime` instance instead of `int` or any other type.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/830", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/830/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/830/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/830/events", "html_url": "https://github.com/Parsely/pykafka/issues/830", "id": 340328748, "node_id": "MDU6SXNzdWUzNDAzMjg3NDg=", "number": 830, "title": "Revisit SimpleConsumer.reset_offsets", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-07-11T17:03:46Z", "updated_at": "2018-08-10T18:57:00Z", "closed_at": "2018-07-18T22:00:57Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "With the knowledge gained [here](https://github.com/Parsely/pykafka/blob/83e61962d240448f8c611971157376d4866101a0/pykafka/topic.py#L113), the logic [here](https://github.com/Parsely/pykafka/blob/83e61962d240448f8c611971157376d4866101a0/pykafka/simpleconsumer.py#L696) in `SimpleConsumer.reset_offsets` can probably be improved to address issues like https://github.com/Parsely/pykafka/issues/733 and https://github.com/Parsely/pykafka/issues/187#issuecomment-403971167. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/829", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/829/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/829/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/829/events", "html_url": "https://github.com/Parsely/pykafka/issues/829", "id": 340072814, "node_id": "MDU6SXNzdWUzNDAwNzI4MTQ=", "number": 829, "title": "Efficiently pulling latest message from a topic", "user": {"login": "rueberger", "id": 8816362, "node_id": "MDQ6VXNlcjg4MTYzNjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/8816362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rueberger", "html_url": "https://github.com/rueberger", "followers_url": "https://api.github.com/users/rueberger/followers", "following_url": "https://api.github.com/users/rueberger/following{/other_user}", "gists_url": "https://api.github.com/users/rueberger/gists{/gist_id}", "starred_url": "https://api.github.com/users/rueberger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rueberger/subscriptions", "organizations_url": "https://api.github.com/users/rueberger/orgs", "repos_url": "https://api.github.com/users/rueberger/repos", "events_url": "https://api.github.com/users/rueberger/events{/privacy}", "received_events_url": "https://api.github.com/users/rueberger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 15352879, "node_id": "MDU6TGFiZWwxNTM1Mjg3OQ==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/documentation", "name": "documentation", "color": "0b02e1", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-11T02:51:24Z", "updated_at": "2018-08-15T22:08:43Z", "closed_at": "2018-08-15T22:08:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am interested in a method to efficiently pull the last message from an existing consumer\r\n\r\nAfter a little bit of digging I've come up with the following incantation\r\n\r\n```python\r\nconsumer = client = KafkaClient(hosts=\"xxxxxxx\")\r\ntopic = client.topics['mytopic']\r\nconsumer = topic.get_simple_consumer()\r\n\r\ndef consume_latest():\r\n     latest_offsets = [\r\n           (partition, pykafka.common.OffsetType.LATEST) for partition in consumer._partitions.keys()\r\n        ]\r\n     consumer.reset_offsets(latest_offsets)\r\n     # now step the offset back by one to fetch the latest...\r\n     almost_latest_offsets = []\r\n     for partition, owned_partition in consumer._partition.items():\r\n         almost_latest_offsets.append(\r\n              (partition, owned_partition.next_offset - 1)\r\n          )\r\n\r\n     consumer.reset_offsets(almost_latest_offsets)\r\n     return consumer.consume()\r\n```\r\n\r\nwhich looks a little funky but to me but I _think_ it should work. However because of the funkiness I wanted to check if there might be better way to do it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/826", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/826/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/826/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/826/events", "html_url": "https://github.com/Parsely/pykafka/issues/826", "id": 338532549, "node_id": "MDU6SXNzdWUzMzg1MzI1NDk=", "number": 826, "title": "Balanced_consumer keeps timing out and exiting", "user": {"login": "MokhFn", "id": 20328292, "node_id": "MDQ6VXNlcjIwMzI4Mjky", "avatar_url": "https://avatars2.githubusercontent.com/u/20328292?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MokhFn", "html_url": "https://github.com/MokhFn", "followers_url": "https://api.github.com/users/MokhFn/followers", "following_url": "https://api.github.com/users/MokhFn/following{/other_user}", "gists_url": "https://api.github.com/users/MokhFn/gists{/gist_id}", "starred_url": "https://api.github.com/users/MokhFn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MokhFn/subscriptions", "organizations_url": "https://api.github.com/users/MokhFn/orgs", "repos_url": "https://api.github.com/users/MokhFn/repos", "events_url": "https://api.github.com/users/MokhFn/events{/privacy}", "received_events_url": "https://api.github.com/users/MokhFn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-07-05T11:18:50Z", "updated_at": "2018-07-10T23:16:23Z", "closed_at": "2018-07-09T17:29:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using a pykafka 2.5.0 (and 2.8.0-dev.2 just to make sure this is not version-related, at least starting from 2.5) to consume messages from a kafka cluster.\r\n\r\nOn a development machine, it all works fine, but on the production environment, where i am trying to migrate, my consumer doesn't seem to connect to the zookeeper. I tried to start a simple consumer, and it works fine. The zookeeper servers are pingable and reachable.\r\n\r\nThis is my log output (starting from the balanced consumer instanciation) : \r\n\r\n`2018-07-05 13:03:50,891 - pykafka.handlers - INFO - RequestHandler worker: exiting cleanly\r\nINFO:pykafka.handlers:RequestHandler worker: exiting cleanly\r\n2018-07-05 13:03:51,212 - pykafka.handlers - INFO - RequestHandler worker: exiting cleanly\r\nINFO:pykafka.handlers:RequestHandler worker: exiting cleanly\r\nWARNING:kazoo.client:Connection dropped: socket connection error: None\r\n2018-07-05 13:04:50,295 - pykafka.balancedconsumer - ERROR - Stopping consumer in response to error\r\nTraceback (most recent call last):\r\n  File \"/opt/consumer/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 332, in start\r\n    self._zookeeper_connection_timeout_ms)\r\n  File \"/opt/consumer/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 382, in _setup_zookeeper\r\n    self._zookeeper.start()\r\n  File \"/opt/consumer/lib/python2.7/site-packages/kazoo/client.py\", line 560, in start\r\n    raise self.handler.timeout_exception(\"Connection time-out\")\r\nKazooTimeoutError: Connection time-out\r\nERROR:pykafka.balancedconsumer:Stopping consumer in response to error\r\nTraceback (most recent call last):\r\n  File \"/opt/consumer/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 332, in start\r\n    self._zookeeper_connection_timeout_ms)\r\n  File \"/opt/consumer/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 382, in _setup_zookeeper\r\n    self._zookeeper.start()\r\n  File \"/opt/consumer/lib/python2.7/site-packages/kazoo/client.py\", line 560, in start\r\n    raise self.handler.timeout_exception(\"Connection time-out\")\r\nKazooTimeoutError: Connection time-out\r\n2018-07-05 13:04:50,296 - pykafka.balancedconsumer - DEBUG - Stopping <pykafka.balancedconsumer.BalancedConsumer at 0x7f14c6b8bd90 (consumer_group=consumer_radio_data)>\r\nDEBUG:pykafka.balancedconsumer:Stopping <pykafka.balancedconsumer.BalancedConsumer at 0x7f14c6b8bd90 (consumer_group=consumer_radio_data)>\r\nTraceback (most recent call last):\r\n  File \"scripts/pykafka_consumer.py\", line 78, in <module>\r\n    # Rework following if, pre-prepare it\r\n  File \"/opt/consumer/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 766, in __iter__\r\n    message = self.consume(block=True)\r\n  File \"/opt/consumer/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 756, in consume\r\n    raise ConsumerStoppedException\r\npykafka.exceptions.ConsumerStoppedException\r\n2018-07-05 13:04:50,324 - pykafka.balancedconsumer - DEBUG - Finalising <pykafka.balancedconsumer.BalancedConsumer at 0x7f14c6b8bd90 (consumer_group=consumer_radio_data)>\r\nDEBUG:pykafka.balancedconsumer:Finalising <pykafka.balancedconsumer.BalancedConsumer at 0x7f14c6b8bd90 (consumer_group=consumer_radio_data)>\r\n2018-07-05 13:04:50,325 - pykafka.handlers - INFO - RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\n2018-07-05 13:04:50,325 - pykafka.handlers - INFO - RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\n2018-07-05 13:04:50,325 - pykafka.handlers - INFO - RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\n2018-07-05 13:04:50,325 - pykafka.handlers - INFO - RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/823", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/823/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/823/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/823/events", "html_url": "https://github.com/Parsely/pykafka/issues/823", "id": 336234617, "node_id": "MDU6SXNzdWUzMzYyMzQ2MTc=", "number": 823, "title": "ReferenceError: weakly-referenced object no longer exists", "user": {"login": "pri22296", "id": 11555869, "node_id": "MDQ6VXNlcjExNTU1ODY5", "avatar_url": "https://avatars3.githubusercontent.com/u/11555869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pri22296", "html_url": "https://github.com/pri22296", "followers_url": "https://api.github.com/users/pri22296/followers", "following_url": "https://api.github.com/users/pri22296/following{/other_user}", "gists_url": "https://api.github.com/users/pri22296/gists{/gist_id}", "starred_url": "https://api.github.com/users/pri22296/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pri22296/subscriptions", "organizations_url": "https://api.github.com/users/pri22296/orgs", "repos_url": "https://api.github.com/users/pri22296/repos", "events_url": "https://api.github.com/users/pri22296/events{/privacy}", "received_events_url": "https://api.github.com/users/pri22296/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-06-27T13:39:06Z", "updated_at": "2018-11-26T23:01:16Z", "closed_at": "2018-11-26T23:01:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I'm using kafka as a queue for web crawling. I noticed that some times some spiders just hang and stop crawling. I noticed the following exception from pykafka each time the spider hangs. Any help would be appreciated.\r\n\r\n```\r\nException in thread 35: pykafka.SimpleConsumer.autocommiter:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 763, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/home/priyam.singh/venv/local/lib/python2.7/site-packages/pykafka/simpleconsumer.py\", line 387, in autocommitter\r\n    self.cleanup()\r\nReferenceError: weakly-referenced object no longer exists\r\n```\r\n\r\n\r\n\r\n**PyKafka version**: 2.7.0\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/822", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/822/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/822/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/822/events", "html_url": "https://github.com/Parsely/pykafka/issues/822", "id": 335065428, "node_id": "MDU6SXNzdWUzMzUwNjU0Mjg=", "number": 822, "title": "samsa bug", "user": {"login": "cysy-lil", "id": 39638892, "node_id": "MDQ6VXNlcjM5NjM4ODky", "avatar_url": "https://avatars1.githubusercontent.com/u/39638892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cysy-lil", "html_url": "https://github.com/cysy-lil", "followers_url": "https://api.github.com/users/cysy-lil/followers", "following_url": "https://api.github.com/users/cysy-lil/following{/other_user}", "gists_url": "https://api.github.com/users/cysy-lil/gists{/gist_id}", "starred_url": "https://api.github.com/users/cysy-lil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cysy-lil/subscriptions", "organizations_url": "https://api.github.com/users/cysy-lil/orgs", "repos_url": "https://api.github.com/users/cysy-lil/repos", "events_url": "https://api.github.com/users/cysy-lil/events{/privacy}", "received_events_url": "https://api.github.com/users/cysy-lil/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-23T03:53:31Z", "updated_at": "2018-06-25T15:59:18Z", "closed_at": "2018-06-25T15:59:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "when I  write a demo which from https://pypi.org/project/samsa/, then I run it, but throw a error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\samsa\\cluster.py\", line 44,\r\nin __init__\r\n    self.brokers = BrokerMap(self)\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\samsa\\brokers.py\", line 45,\r\nin __init__\r\n    self._node_path, self._configure\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", l\r\nine 290, in __init__\r\n    self._get_children()\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", l\r\nine 37, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", l\r\nine 340, in _get_children\r\n    result = self._func(children)\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\samsa\\brokers.py\", line 66,\r\nin _configure\r\n    broker = Broker(self.cluster, id_=broker_id)\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\samsa\\brokers.py\", line 146,\r\n in __init__\r\n    self._node_path, self._configure\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", l\r\nine 131, in __init__\r\n    self._get_data()\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", l\r\nine 37, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", l\r\nine 206, in _get_data\r\n    self._log_func_exception(data, stat, event)\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", l\r\nine 164, in _log_func_exception\r\n    result = self._func(data, stat)\r\n  File \"D:\\Program Files\\Python27\\lib\\site-packages\\samsa\\brokers.py\", line 160,\r\n in _configure\r\n    creator, self.__host, port = data.split(':')\r\nValueError: too many values to unpack\r\n```\r\nthen I edit the source of samsa brokers.py:\r\n```python\r\n    def _configure(self, data, stat):\r\n        \"\"\"\r\n        Configures a broker based on it's state in ZooKeeper.\r\n        \"\"\"\r\n        logger.info('Retrieved broker data for %s...', self)\r\n        if data is None:\r\n            logger.info('Broker data field was empty. Assuming it was dead.')\r\n            self.__host = self.__port = None\r\n        else:\r\n            jsonObj = json.loads(data)\r\n            print jsonObj\r\n            print jsonObj['host']\r\n            # creator, self.__host, port = data.split(':')\r\n            self.__host=jsonObj['host']\r\n            self.__port=jsonObj['port']\r\n            # self.__port = int(port)\r\n```\r\nthen I run it again,but throw a new error:\r\n```\r\nNo handlers could be found for logger \"kazoo.recipe.watchers\"\r\nTraceback (most recent call last):\r\n  File \"D:/workspace/pyCharm/nlp/extractKey/kafka_test1.py\", line 7, in <module>\r\n    topic = cluster.topics['zaful']\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\samsa\\topics.py\", line 50, in __getitem__\r\n    return self.get(key)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\samsa\\topics.py\", line 59, in get\r\n    topic = self.__topics[name] = Topic(self.cluster, name)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\samsa\\topics.py\", line 79, in __init__\r\n    self.partitions = PartitionMap(self.cluster, self)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\samsa\\partitions.py\", line 53, in __init__\r\n    self._topic_changed, allow_missing_node=True\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", line 131, in __init__\r\n    self._get_data()\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", line 37, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", line 206, in _get_data\r\n    self._log_func_exception(data, stat, event)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", line 164, in _log_func_exception\r\n    result = self._func(data, stat)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\samsa\\partitions.py\", line 62, in _topic_changed\r\n    self._configure\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", line 290, in __init__\r\n    self._get_children()\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", line 37, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\kazoo\\recipe\\watchers.py\", line 340, in _get_children\r\n    result = self._func(children)\r\n  File \"D:\\workspace\\pyCharm\\nlp\\extractKey\\venv\\lib\\site-packages\\samsa\\partitions.py\", line 69, in _configure\r\n    brokers = map(self.cluster.brokers.get, map(int, broker_ids))\r\nValueError: invalid literal for int() with base 10: 'partitions'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/820", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/820/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/820/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/820/events", "html_url": "https://github.com/Parsely/pykafka/issues/820", "id": 333518415, "node_id": "MDU6SXNzdWUzMzM1MTg0MTU=", "number": 820, "title": "Standardize zookeeper host string interface", "user": {"login": "zpdev", "id": 15672287, "node_id": "MDQ6VXNlcjE1NjcyMjg3", "avatar_url": "https://avatars2.githubusercontent.com/u/15672287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zpdev", "html_url": "https://github.com/zpdev", "followers_url": "https://api.github.com/users/zpdev/followers", "following_url": "https://api.github.com/users/zpdev/following{/other_user}", "gists_url": "https://api.github.com/users/zpdev/gists{/gist_id}", "starred_url": "https://api.github.com/users/zpdev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zpdev/subscriptions", "organizations_url": "https://api.github.com/users/zpdev/orgs", "repos_url": "https://api.github.com/users/zpdev/repos", "events_url": "https://api.github.com/users/zpdev/events{/privacy}", "received_events_url": "https://api.github.com/users/zpdev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 15352879, "node_id": "MDU6TGFiZWwxNTM1Mjg3OQ==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/documentation", "name": "documentation", "color": "0b02e1", "default": true, "description": null}, {"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-19T04:52:04Z", "updated_at": "2018-07-19T20:41:44Z", "closed_at": "2018-07-19T20:41:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nI cloud see them in document:\r\n### pykafka.balancedconsumer.BalancedConsumer\r\n> zookeeper_connect  (str) \u2013 Comma-separated (ip1:port1,ip2:port2) strings indicating the zookeeper nodes to which to connect.\r\n\r\n### pykafka.client.KafkaClient\r\n> zookeeper_hosts  (str) \u2013 KazooClient-formatted string of ZooKeeper hosts to which to connect. If not None, this argument takes precedence over hosts\r\n\r\nbut i don't know which should be used.\r\n\r\nthanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/816", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/816/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/816/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/816/events", "html_url": "https://github.com/Parsely/pykafka/issues/816", "id": 331778170, "node_id": "MDU6SXNzdWUzMzE3NzgxNzA=", "number": 816, "title": "client CPU 100% after brokers restart", "user": {"login": "lokesh-b", "id": 5994573, "node_id": "MDQ6VXNlcjU5OTQ1NzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/5994573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lokesh-b", "html_url": "https://github.com/lokesh-b", "followers_url": "https://api.github.com/users/lokesh-b/followers", "following_url": "https://api.github.com/users/lokesh-b/following{/other_user}", "gists_url": "https://api.github.com/users/lokesh-b/gists{/gist_id}", "starred_url": "https://api.github.com/users/lokesh-b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lokesh-b/subscriptions", "organizations_url": "https://api.github.com/users/lokesh-b/orgs", "repos_url": "https://api.github.com/users/lokesh-b/repos", "events_url": "https://api.github.com/users/lokesh-b/events{/privacy}", "received_events_url": "https://api.github.com/users/lokesh-b/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-12T22:34:52Z", "updated_at": "2018-06-20T20:50:20Z", "closed_at": "2018-06-20T20:50:20Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**PyKafka version**: 2.5.0 (Also happens in 2.7.0)\r\n**Kafka version**: 0.10.2.1\r\n\r\nSteps to Reproduce:\r\n\r\n1. Start Kafka server with multiple brokers say 5 and 15 partitions \r\n2. Set replication factor of a test topic to maybe 3\r\n3. Start python client with gunicorn\r\n4. Send messages to test topic (Maybe greater than 10 CPM)\r\n4. Do rolling restart of brokers\r\n5. Watch out for the CPU percentage of gunicorn workers\r\n6. Repeat step 4. At some point you will see python CPU usage of one or more workers reach 100%\r\n\r\nBelow screenshot shows you that _produce method is in loop forever because it doesn't have the right owned brokers list.\r\n![image](https://user-images.githubusercontent.com/5994573/41320640-6f1c216e-6e55-11e8-9e65-2e101b7dcacb.png)\r\n\r\nScreenshot which shows the cpu usage of the worker:\r\n![pykafka2](https://user-images.githubusercontent.com/5994573/41320699-b0198094-6e55-11e8-9bf0-10d2819e8e6d.png)\r\n\r\nScreenshot which shows the leader ID is right but the owned_brokers dict is not updated.\r\n![image](https://user-images.githubusercontent.com/5994573/41320778-fb89ec4e-6e55-11e8-8a91-ec3c9bdbda51.png)\r\n\r\nPlease advice on what can be done here. Thanks\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/813", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/813/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/813/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/813/events", "html_url": "https://github.com/Parsely/pykafka/issues/813", "id": 330375302, "node_id": "MDU6SXNzdWUzMzAzNzUzMDI=", "number": 813, "title": "support arbitrary partition/offset pairs in commit_offset", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-07T17:55:57Z", "updated_at": "2018-06-14T18:38:08Z", "closed_at": "2018-06-14T18:38:08Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "[SimpleConsumer.commit_offsets](https://github.com/Parsely/pykafka/blob/56efe3975ee707f4ec48aa8da81fde9fbbf14dda/pykafka/simpleconsumer.py#L521) only commits the most recently consumed offset for each `OwnedPartition`. It should also support the committing of arbitrary `(partition, offset)` pairs. This would separate the notions of \"consumed\" and \"processed\" messages, addressing real-world use cases in which the two concepts are distinct.\r\n\r\nTake as an example an Apache Storm topology with a spout that uses a pykafka consumer and sends consumed messages downstream as tuples for further processing. Here, a message is \"consumed\" at the spout, but it is not considered fully \"processed\" until it has made it through the entire Storm topology as a tuple. If its offset is committed as soon as it's \"consumed\", any stoppage of processing before it's \"processed\" would result in that message being lost. On the other hand, waiting until it's \"processed\" to commit its offset means that a processing stoppage would result in the tuple being retried next time the consumer was restarted.\r\n\r\nThis change might expose `PartitionOffsetCommitRequest` as a public API. Whether it does depends mostly on how other similar interfaces in pykafka handle things like this, which I haven't looked deeply into at the moment.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/812", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/812/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/812/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/812/events", "html_url": "https://github.com/Parsely/pykafka/issues/812", "id": 329286688, "node_id": "MDU6SXNzdWUzMjkyODY2ODg=", "number": 812, "title": "Message not delivering", "user": {"login": "kadnan", "id": 273196, "node_id": "MDQ6VXNlcjI3MzE5Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/273196?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kadnan", "html_url": "https://github.com/kadnan", "followers_url": "https://api.github.com/users/kadnan/followers", "following_url": "https://api.github.com/users/kadnan/following{/other_user}", "gists_url": "https://api.github.com/users/kadnan/gists{/gist_id}", "starred_url": "https://api.github.com/users/kadnan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kadnan/subscriptions", "organizations_url": "https://api.github.com/users/kadnan/orgs", "repos_url": "https://api.github.com/users/kadnan/repos", "events_url": "https://api.github.com/users/kadnan/events{/privacy}", "received_events_url": "https://api.github.com/users/kadnan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-05T03:11:34Z", "updated_at": "2018-06-06T08:17:09Z", "closed_at": "2018-06-05T18:27:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am writing this simple program by following the example and it gives an error:\r\n\r\n```\r\nFailed to connect newly created broker for b'adnans-mbp':9092\r\nError encountered when producing to broker b'adnans-mbp':9092. Retrying.\r\nError encountered when producing to broker b'adnans-mbp':9092. Retrying.\r\nError encountered when producing to broker b'adnans-mbp':9092. Retrying.\r\nError encountered when producing to broker b'adnans-mbp':9092. Retrying.\r\nMessage not delivered!! SocketDisconnectedError()\r\n```\r\n\r\nThe program is given below:\r\n```\r\nfrom pykafka import KafkaClient\r\n\r\nif __name__ == '__main__':\r\n   client = KafkaClient(\"0.0.0.0:9092\", broker_version='0.10.0')\r\n    # client = KafkaClient(\"0.0.0.0:9092\")\r\n    topic = client.topics[b'poem']\r\n    producer = topic.get_producer()\r\n    producer.produce(b\"your message\")\r\n\r\n    consumer = topic.get_simple_consumer()\r\n    msg = consumer.consume()\r\n    print(' % s[key = % s, id = % s, offset = % s]' %\r\n          (msg.value, msg.partition_key, msg.partition_id, msg.offset))\r\n```\r\n\r\nI enabled logging and here is the dump:\r\n```\r\nINFO:pykafka.cluster:Requesting API version information\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Got api version info: {0: ApiVersionsSpec(key=0, min=0, max=5), 1: ApiVersionsSpec(key=1, min=0, max=7), 2: ApiVersionsSpec(key=2, min=0, max=2), 3: ApiVersionsSpec(key=3, min=0, max=5), 4: ApiVersionsSpec(key=4, min=0, max=1), 5: ApiVersionsSpec(key=5, min=0, max=0), 6: ApiVersionsSpec(key=6, min=0, max=4), 7: ApiVersionsSpec(key=7, min=0, max=1), 8: ApiVersionsSpec(key=8, min=0, max=3), 9: ApiVersionsSpec(key=9, min=0, max=3), 10: ApiVersionsSpec(key=10, min=0, max=1), 11: ApiVersionsSpec(key=11, min=0, max=2), 12: ApiVersionsSpec(key=12, min=0, max=1), 13: ApiVersionsSpec(key=13, min=0, max=1), 14: ApiVersionsSpec(key=14, min=0, max=1), 15: ApiVersionsSpec(key=15, min=0, max=1), 16: ApiVersionsSpec(key=16, min=0, max=1), 17: ApiVersionsSpec(key=17, min=0, max=1), 18: ApiVersionsSpec(key=18, min=0, max=1), 19: ApiVersionsSpec(key=19, min=0, max=2), 20: ApiVersionsSpec(key=20, min=0, max=1), 21: ApiVersionsSpec(key=21, min=0, max=0), 22: ApiVersionsSpec(key=22, min=0, max=0), 23: ApiVersionsSpec(key=23, min=0, max=0), 24: ApiVersionsSpec(key=24, min=0, max=0), 25: ApiVersionsSpec(key=25, min=0, max=0), 26: ApiVersionsSpec(key=26, min=0, max=0), 27: ApiVersionsSpec(key=27, min=0, max=0), 28: ApiVersionsSpec(key=28, min=0, max=0), 29: ApiVersionsSpec(key=29, min=0, max=0), 30: ApiVersionsSpec(key=30, min=0, max=0), 31: ApiVersionsSpec(key=31, min=0, max=0), 32: ApiVersionsSpec(key=32, min=0, max=1), 33: ApiVersionsSpec(key=33, min=0, max=0), 34: ApiVersionsSpec(key=34, min=0, max=0), 35: ApiVersionsSpec(key=35, min=0, max=0), 36: ApiVersionsSpec(key=36, min=0, max=0), 37: ApiVersionsSpec(key=37, min=0, max=0), 38: ApiVersionsSpec(key=38, min=0, max=0), 39: ApiVersionsSpec(key=39, min=0, max=0), 40: ApiVersionsSpec(key=40, min=0, max=0), 41: ApiVersionsSpec(key=41, min=0, max=0), 42: ApiVersionsSpec(key=42, min=0, max=0)}\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 1 brokers\r\nINFO:pykafka.connection:Failed to connect to b'adnans-mbp':9092\r\nINFO:pykafka.connection:[Errno 8] nodename nor servname provided, or not known\r\nWARNING:pykafka.broker:Failed to connect newly created broker for b'adnans-mbp':9092\r\nINFO:pykafka.cluster:Discovered 2 topics\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.topic:Adding 1 partitions\r\nINFO:pykafka.simpleconsumer:Starting 1 fetcher threads\r\nINFO:pykafka.simpleconsumer:Updating cluster in response to error in fetch() for broker id 0\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 1 brokers\r\nINFO:pykafka.cluster:Reconnecting to broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.connection:Failed to connect to b'adnans-mbp':9092\r\nINFO:pykafka.connection:[Errno 8] nodename nor servname provided, or not known\r\nINFO:pykafka.cluster:Failed to re-establish connection with broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.cluster:Discovered 2 topics\r\nINFO:pykafka.topic:Adding 1 partitions\r\nINFO:pykafka.simpleconsumer:Updating cluster in response to error in fetch() for broker id 0\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 1 brokers\r\nINFO:pykafka.cluster:Reconnecting to broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.connection:Failed to connect to b'adnans-mbp':9092\r\nINFO:pykafka.connection:[Errno 8] nodename nor servname provided, or not known\r\nINFO:pykafka.cluster:Failed to re-establish connection with broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.cluster:Discovered 2 topics\r\nINFO:pykafka.topic:Adding 1 partitions\r\nINFO:pykafka.simpleconsumer:Updating cluster in response to error in fetch() for broker id 0\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 1 brokers\r\nINFO:pykafka.cluster:Reconnecting to broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.connection:Failed to connect to b'adnans-mbp':9092\r\nINFO:pykafka.connection:[Errno 8] nodename nor servname provided, or not known\r\nINFO:pykafka.cluster:Failed to re-establish connection with broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.cluster:Discovered 2 topics\r\nINFO:pykafka.topic:Adding 1 partitions\r\nINFO:pykafka.simpleconsumer:Updating cluster in response to error in fetch() for broker id 0\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 1 brokers\r\nINFO:pykafka.cluster:Reconnecting to broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.connection:Failed to connect to b'adnans-mbp':9092\r\nINFO:pykafka.connection:[Errno 8] nodename nor servname provided, or not known\r\nINFO:pykafka.cluster:Failed to re-establish connection with broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.cluster:Discovered 2 topics\r\nINFO:pykafka.topic:Adding 1 partitions\r\nINFO:pykafka.simpleconsumer:Updating cluster in response to error in fetch() for broker id 0\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 1 brokers\r\nINFO:pykafka.cluster:Reconnecting to broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.connection:Failed to connect to b'adnans-mbp':9092\r\nINFO:pykafka.connection:[Errno 8] nodename nor servname provided, or not known\r\nINFO:pykafka.cluster:Failed to re-establish connection with broker id 0: b'adnans-mbp':9092\r\nINFO:pykafka.cluster:Discovered 2 topics\r\nINFO:pykafka.topic:Adding 1 partitions\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/808", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/808/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/808/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/808/events", "html_url": "https://github.com/Parsely/pykafka/issues/808", "id": 325318910, "node_id": "MDU6SXNzdWUzMjUzMTg5MTA=", "number": 808, "title": "Question on multi threading", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-22T14:25:54Z", "updated_at": "2018-05-22T16:39:26Z", "closed_at": "2018-05-22T16:37:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @emmett9001,\r\n\r\nI've got a multi threaded application, where each thread consumes from or produces to a specific topic. It's a Python application with Threading module (so not using multiprocessing). Am I right when I implement a KafkaClient connection per thread? Or is it better to use one connection in my entire application and \"re-use\" it in the different threads? I can image that consuming multiple topics and producing to multiple topics through one KafkaClient creates a kind of funnel which could lead to silting up? What is your advice on this?\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/806", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/806/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/806/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/806/events", "html_url": "https://github.com/Parsely/pykafka/issues/806", "id": 324516571, "node_id": "MDU6SXNzdWUzMjQ1MTY1NzE=", "number": 806, "title": "Help with message serialization ", "user": {"login": "georgeha", "id": 1215400, "node_id": "MDQ6VXNlcjEyMTU0MDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1215400?v=4", "gravatar_id": "", "url": "https://api.github.com/users/georgeha", "html_url": "https://github.com/georgeha", "followers_url": "https://api.github.com/users/georgeha/followers", "following_url": "https://api.github.com/users/georgeha/following{/other_user}", "gists_url": "https://api.github.com/users/georgeha/gists{/gist_id}", "starred_url": "https://api.github.com/users/georgeha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/georgeha/subscriptions", "organizations_url": "https://api.github.com/users/georgeha/orgs", "repos_url": "https://api.github.com/users/georgeha/repos", "events_url": "https://api.github.com/users/georgeha/events{/privacy}", "received_events_url": "https://api.github.com/users/georgeha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-18T18:53:21Z", "updated_at": "2018-05-21T17:40:27Z", "closed_at": "2018-05-21T17:40:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "**PyKafka version**: '2.7.0'\r\n**Kafka version**: Latest\r\n\r\nHello,\r\nI am trying to use pickle to serialize the messages that I am producing.\r\n\r\n```\r\nwith topic.get_sync_producer( max_request_size=21000012) as producer:\r\n    #producer.produce(unp)\r\n    producer.produce(pfile)\r\n```\r\n\r\nwith pickle this is how I serialize my messages:\r\n\r\n```\r\npfile = pickle.dumps(data, pickle.HIGHEST_PROTOCOL)\r\n```\r\n\r\nHow can I use pickle for that and not utf-8?\r\n\r\nThanks\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/802", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/802/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/802/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/802/events", "html_url": "https://github.com/Parsely/pykafka/issues/802", "id": 322675345, "node_id": "MDU6SXNzdWUzMjI2NzUzNDU=", "number": 802, "title": "AttributeError: 'bytes' object has no attribute 'get_simple_consumer'", "user": {"login": "kadnan", "id": 273196, "node_id": "MDQ6VXNlcjI3MzE5Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/273196?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kadnan", "html_url": "https://github.com/kadnan", "followers_url": "https://api.github.com/users/kadnan/followers", "following_url": "https://api.github.com/users/kadnan/following{/other_user}", "gists_url": "https://api.github.com/users/kadnan/gists{/gist_id}", "starred_url": "https://api.github.com/users/kadnan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kadnan/subscriptions", "organizations_url": "https://api.github.com/users/kadnan/orgs", "repos_url": "https://api.github.com/users/kadnan/repos", "events_url": "https://api.github.com/users/kadnan/events{/privacy}", "received_events_url": "https://api.github.com/users/kadnan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-14T05:17:29Z", "updated_at": "2018-05-14T05:25:07Z", "closed_at": "2018-05-14T05:25:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using the latest Kafka and PyKafka version. I am trying the code and getting the message:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Development/DataScience/Kafka/intro.py\", line 13, in <module>\r\n    consumer = topic.get_simple_consumer()\r\nAttributeError: 'bytes' object has no attribute 'get_simple_consumer'\r\n```\r\n\r\nMy code is given below:\r\n```\r\nfrom pykafka import KafkaClient\r\n\r\nif __name__ == '__main__':\r\n    client = KafkaClient(hosts=\"0.0.0.0:9092\")\r\n    topics = client.topics\r\n    # topic_daily_sales = client.topics['DAILY_SALES']\r\n\r\n    for topic in topics:\r\n        name = str(topic, 'utf-8')\r\n        if name == 'DAILY_SALES':\r\n\r\n            # Get consumer\r\n            consumer = topic.get_simple_consumer()\r\n            for message in consumer:\r\n                if message is not None:\r\n                    print(message.offset, message.value)\r\n            break\r\n```\r\n\r\nEven the following simple code is not working:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    client = KafkaClient(hosts=\"0.0.0.0:9092\")\r\n    print(client.topics)\r\n    topic = client.topics[\"DAILY_SALES\"]\r\n    producer = topic.get_producer()\r\n    consumer = topic.get_simple_consumer()\r\n```\r\n\r\nand gives error:\r\n```\r\n  topic = client.topics[\"DAILY_SALES\"]\r\n  File \"/anaconda3/anaconda/lib/python3.6/site-packages/pykafka/cluster.py\", line 59, in __getitem__\r\n    \"got '%s'\", type(key))\r\nTypeError: (\"TopicDict.__getitem__ accepts a bytes object, but it got '%s'\", <class 'str'>)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/801", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/801/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/801/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/801/events", "html_url": "https://github.com/Parsely/pykafka/issues/801", "id": 322406789, "node_id": "MDU6SXNzdWUzMjI0MDY3ODk=", "number": 801, "title": "gevent upgrade breaks pykafka 2.4.0 import", "user": {"login": "eedwards-sk", "id": 34585329, "node_id": "MDQ6VXNlcjM0NTg1MzI5", "avatar_url": "https://avatars1.githubusercontent.com/u/34585329?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eedwards-sk", "html_url": "https://github.com/eedwards-sk", "followers_url": "https://api.github.com/users/eedwards-sk/followers", "following_url": "https://api.github.com/users/eedwards-sk/following{/other_user}", "gists_url": "https://api.github.com/users/eedwards-sk/gists{/gist_id}", "starred_url": "https://api.github.com/users/eedwards-sk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eedwards-sk/subscriptions", "organizations_url": "https://api.github.com/users/eedwards-sk/orgs", "repos_url": "https://api.github.com/users/eedwards-sk/repos", "events_url": "https://api.github.com/users/eedwards-sk/events{/privacy}", "received_events_url": "https://api.github.com/users/eedwards-sk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-05-11T19:21:11Z", "updated_at": "2018-06-14T19:17:04Z", "closed_at": "2018-06-14T19:17:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "**PyKafka version**: `pykafka==2.4.0`\r\n**Kafka version**: N\\A\r\n\r\npykafka `2.4.0` is failing to import due to revision on `gevent` library\r\n\r\nprevious working version of gevent which was getting installed was `1.2.2`, but now it's pulling in version `1.3.0` which breaks as below:\r\n\r\n```\r\n  File \"/home/ubuntu/our-product/libs/pos/task/common.py\", line 11, in <module>\r\n\r\n    from core.kafka.kafka_utils import send_kafka\r\n\r\n  File \"/home/ubuntu/our-product/core/kafka/kafka_utils.py\", line 6, in <module>\r\n\r\n    from pykafka import KafkaClient\r\n\r\n  File \"/home/ubuntu/our-product/.tox/py27/local/lib/python2.7/site-packages/pykafka/__init__.py\", line 3, in <module>\r\n\r\n    from .cluster import Cluster\r\n\r\n  File \"/home/ubuntu/our-product/.tox/py27/local/lib/python2.7/site-packages/pykafka/cluster.py\", line 36, in <module>\r\n\r\n    from .topic import Topic\r\n\r\n  File \"/home/ubuntu/our-product/.tox/py27/local/lib/python2.7/site-packages/pykafka/topic.py\", line 23, in <module>\r\n\r\n    from .balancedconsumer import BalancedConsumer\r\n\r\n  File \"/home/ubuntu/our-product/.tox/py27/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 31, in <module>\r\n\r\n    from kazoo.handlers.gevent import SequentialGeventHandler\r\n\r\n  File \"/home/ubuntu/our-product/.tox/py27/local/lib/python2.7/site-packages/kazoo/handlers/gevent.py\", line 32, in <module>\r\n\r\n    class SequentialGeventHandler(object):\r\n\r\n  File \"/home/ubuntu/our-product/.tox/py27/local/lib/python2.7/site-packages/kazoo/handlers/gevent.py\", line 63, in SequentialGeventHandler\r\n\r\n    class timeout_exception(gevent.event.Timeout):\r\n\r\nAttributeError: 'module' object has no attribute 'Timeout'\r\n```\r\n\r\nWe're on a fairly older version of pykafka, so I'm hoping there's a slightly newer version that might have changed the requirement that we can easily upgrade to, otherwise we're just pinning `gevent==1.2.2` for now.\r\n\r\nA patch revision on 2.4.0 that pessimistically pins to `gevent ~>1.2` would also work.\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/800", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/800/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/800/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/800/events", "html_url": "https://github.com/Parsely/pykafka/issues/800", "id": 321691266, "node_id": "MDU6SXNzdWUzMjE2OTEyNjY=", "number": 800, "title": "logging issue in cluster.py", "user": {"login": "BonuV", "id": 29787715, "node_id": "MDQ6VXNlcjI5Nzg3NzE1", "avatar_url": "https://avatars1.githubusercontent.com/u/29787715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BonuV", "html_url": "https://github.com/BonuV", "followers_url": "https://api.github.com/users/BonuV/followers", "following_url": "https://api.github.com/users/BonuV/following{/other_user}", "gists_url": "https://api.github.com/users/BonuV/gists{/gist_id}", "starred_url": "https://api.github.com/users/BonuV/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BonuV/subscriptions", "organizations_url": "https://api.github.com/users/BonuV/orgs", "repos_url": "https://api.github.com/users/BonuV/repos", "events_url": "https://api.github.com/users/BonuV/events{/privacy}", "received_events_url": "https://api.github.com/users/BonuV/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-09T19:11:32Z", "updated_at": "2018-05-09T21:54:52Z", "closed_at": "2018-05-09T21:54:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nFacing json searlizing issue while logging the logs .\r\nIn cluster.py , consumer_group value  is used  for logging which is basically a bytes type  provided while connecting.\r\n\r\nIis there any way to supress the library logs\r\n\r\n**PyKafka version**: d710898\r\n**Kafka version**: NA\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/798", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/798/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/798/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/798/events", "html_url": "https://github.com/Parsely/pykafka/issues/798", "id": 319943147, "node_id": "MDU6SXNzdWUzMTk5NDMxNDc=", "number": 798, "title": "Can't delete topic using `pykafka/cli/kafka_tools.py` script", "user": {"login": "vikt0rs", "id": 5230490, "node_id": "MDQ6VXNlcjUyMzA0OTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/5230490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vikt0rs", "html_url": "https://github.com/vikt0rs", "followers_url": "https://api.github.com/users/vikt0rs/followers", "following_url": "https://api.github.com/users/vikt0rs/following{/other_user}", "gists_url": "https://api.github.com/users/vikt0rs/gists{/gist_id}", "starred_url": "https://api.github.com/users/vikt0rs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vikt0rs/subscriptions", "organizations_url": "https://api.github.com/users/vikt0rs/orgs", "repos_url": "https://api.github.com/users/vikt0rs/repos", "events_url": "https://api.github.com/users/vikt0rs/events{/privacy}", "received_events_url": "https://api.github.com/users/vikt0rs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-03T14:24:40Z", "updated_at": "2018-05-07T21:04:46Z", "closed_at": "2018-05-07T21:04:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, folks,\r\n\r\nI'm trying to remove a topic from Kafka using the `pykafka/cli/kafka_tools.py` script, but getting an error message `pykafka.exceptions.SocketDisconnectedError: <broker b'10.12.227.146':9092>`. Please, suggest, if I do something wrong.\r\nYou may find shell command and it's traceback below.\r\n\r\nThanks!\r\n\r\n\r\n```\r\n(py36) macmini987:notifications_api vserhei$ python .tox/py36/lib/python3.6/site-packages/pykafka/cli/kafka_tools.py --broker=10.12.227.146:9092 --broker_version=0.10.0 delete_topic test_topic_1\r\nTraceback (most recent call last):\r\n  File \".tox/py36/lib/python3.6/site-packages/pykafka/cli/kafka_tools.py\", line 466, in <module>\r\n    main()\r\n  File \".tox/py36/lib/python3.6/site-packages/pykafka/cli/kafka_tools.py\", line 460, in main\r\n    args.func(client, args)\r\n  File \".tox/py36/lib/python3.6/site-packages/pykafka/cli/kafka_tools.py\", line 280, in delete_topic\r\n    client.cluster.controller_broker.delete_topics([args.topic], args.timeout)\r\n  File \"/Users/vserhei/src/notifications_api/.tox/py36/lib/python3.6/site-packages/pykafka/broker.py\", line 45, in wrapped\r\n    return fn(self, *args, **kwargs)\r\n  File \"/Users/vserhei/src/notifications_api/.tox/py36/lib/python3.6/site-packages/pykafka/broker.py\", line 584, in delete_topics\r\n    return future.get(DeleteTopicsResponse)\r\n  File \"/Users/vserhei/src/notifications_api/.tox/py36/lib/python3.6/site-packages/pykafka/handlers.py\", line 74, in get\r\n    raise self.error\r\n  File \"/Users/vserhei/src/notifications_api/.tox/py36/lib/python3.6/site-packages/pykafka/handlers.py\", line 212, in worker\r\n    res = shared.connection.response()\r\n  File \"/Users/vserhei/src/notifications_api/.tox/py36/lib/python3.6/site-packages/pykafka/connection.py\", line 217, in response\r\n    raise SocketDisconnectedError(\"<broker {}:{}>\".format(self.host, self.port))\r\npykafka.exceptions.SocketDisconnectedError: <broker b'10.12.227.146':9092>\r\n(py36) macmini987:notifications_api vserhei$\r\n(py36) macmini987:notifications_api vserhei$\r\n(py36) macmini987:notifications_api vserhei$ pip freeze | grep pykafka\r\npykafka==2.7.0\r\n(py36) macmini987:notifications_api vserhei$\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/797", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/797/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/797/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/797/events", "html_url": "https://github.com/Parsely/pykafka/issues/797", "id": 319165712, "node_id": "MDU6SXNzdWUzMTkxNjU3MTI=", "number": 797, "title": "KeyError when connecting to partially started cluster", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 240229229, "node_id": "MDU6TGFiZWwyNDAyMjkyMjk=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/in%20progress", "name": "in progress", "color": "ededed", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-05-01T10:26:11Z", "updated_at": "2018-05-16T16:19:28Z", "closed_at": "2018-05-16T16:19:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @emmett9001 \r\n\r\nI have a Kafka cluster setup running 3 servers. I have a topic which has a replication-factor of 2. When I describe my topic, I see that Kafka configured my topic this way:\r\n\r\nTopic: mytopic Partition: 0 Leader: 1 Replicas: 2,1 Isr: 1\r\n\r\nSo the topic is residing in Kafka servers 1 and 2. Then I start server 0 and 1 and I do not start server 2. In theory, mytopic is available on server 0, so I should be able to access this topic.\r\n\r\nBut when I start my application using pykafka 2.7.0, I'm getting a KeyError when calling:\r\n\r\n`kafka_client_object.topics[\"mytopic\"]`\r\n\r\nDebugging into the topics property I see that my topic name is actually in the dictionary. Digging deeper I see that it's a custom dictionary of type TopicDict. Getting an item from this dict is calling __getitem__ function on TopicDict class. Within this function, a Topic class is instantiated:\r\n\r\n`topic = Topic(self._cluster(), meta.topics[key])`\r\n\r\nIn the constructor of Topic the update function is called and that's were the error originates:\r\n```python\r\nif meta.id not in self._partitions:\r\n  log.debug('Adding partition %s/%s', self.name, meta.id)\r\n  self._partitions[meta.id] = Partition(\r\n    self, meta.id,\r\n    brokers[meta.leader],\r\n    [brokers[b] for b in meta.replicas],   # <---- this line\r\n    [brokers[b] for b in meta.isr],\r\n  )\r\n```\r\nFor mytopic meta.replicas holds 2,1 but there are only 2 brokers connected: 0,1. So when accessing broker[2] it is throwing an KeyError.\r\n\r\nI think the solution is to just only append brokers to this list when the broker exists, but I can't oversee if this covers the whole issue.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/796", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/796/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/796/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/796/events", "html_url": "https://github.com/Parsely/pykafka/issues/796", "id": 318989415, "node_id": "MDU6SXNzdWUzMTg5ODk0MTU=", "number": 796, "title": "How to maintain SimpleConsumer and Producer connection", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 15352879, "node_id": "MDU6TGFiZWwxNTM1Mjg3OQ==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/documentation", "name": "documentation", "color": "0b02e1", "default": true, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-30T18:31:43Z", "updated_at": "2018-05-07T20:47:20Z", "closed_at": "2018-05-07T20:47:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @emmett9001,\r\n\r\nI'm looking for a proper way to handle connection error situations. I'm setting up different threads with each a KafkaClient connection and either a SimpleConsumer object or Producer object (trying with and without rdkafka). A consumer thread notes a connection problem very quickly and then generates an error like: \"Unable to connect to a broker to fetch metadata.\". But what do I need to do then? Simply clean the consumer and kafkaclient objects and setup a new connection?\r\n\r\nAnd in case of the Producer: the producer doesn't notice a connection problem directly, so I want to monitor the connection every 10 seconds for example. I thought calling the latest_available_offsets every 10 seconds is an idea, to check the connection and if it's broken set it up again (by cleaning the kafkaclient and producer and setup a new connection).\r\n\r\nI couldn't find a good reference how to implement this in a proper and stable way and if my thoughts are making any sense. So a reference to a good document / post or some help would be appreciated.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/795", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/795/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/795/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/795/events", "html_url": "https://github.com/Parsely/pykafka/issues/795", "id": 318783475, "node_id": "MDU6SXNzdWUzMTg3ODM0NzU=", "number": 795, "title": "client.topics returning list instead of dict of the topics", "user": {"login": "oditouchiha", "id": 12369866, "node_id": "MDQ6VXNlcjEyMzY5ODY2", "avatar_url": "https://avatars2.githubusercontent.com/u/12369866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oditouchiha", "html_url": "https://github.com/oditouchiha", "followers_url": "https://api.github.com/users/oditouchiha/followers", "following_url": "https://api.github.com/users/oditouchiha/following{/other_user}", "gists_url": "https://api.github.com/users/oditouchiha/gists{/gist_id}", "starred_url": "https://api.github.com/users/oditouchiha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oditouchiha/subscriptions", "organizations_url": "https://api.github.com/users/oditouchiha/orgs", "repos_url": "https://api.github.com/users/oditouchiha/repos", "events_url": "https://api.github.com/users/oditouchiha/events{/privacy}", "received_events_url": "https://api.github.com/users/oditouchiha/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-04-30T04:28:32Z", "updated_at": "2018-04-30T04:44:24Z", "closed_at": "2018-04-30T04:44:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "client = KafkaClient(hosts=\"localhost\")\r\nprint(client.topics)\r\n\r\noutput : \r\n['__consumer_offsets', 'test']", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/793", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/793/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/793/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/793/events", "html_url": "https://github.com/Parsely/pykafka/issues/793", "id": 317482848, "node_id": "MDU6SXNzdWUzMTc0ODI4NDg=", "number": 793, "title": "'NoneType' object has no attribute 'api_versions'", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-25T05:25:32Z", "updated_at": "2018-04-25T18:57:12Z", "closed_at": "2018-04-25T17:33:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @emmett9001 ,\r\n\r\nI've got pykafka 2.7.0 installed and what I see is that when I try to connect to a Kafka cluster, which is disconnected at that time, the following error occurs:\r\n\r\n'NoneType' object has no attribute 'api_versions'\r\n\r\nNormally I should get a SocketDisconnectedError, but since a change in cluster.py which is incorporated in 2.7.0 the above error appears. The fetch_api_versions function is new and is being called once from the Cluster class constructor. The fetch_api_versions function is getting the api version info from the response object, but when a connection isn't possible, the following line in fetch_api_versions results in above error:\r\n\r\nif response.api_versions:\r\n\r\nAnd that's because reponse is None.\r\n\r\nWould be nice if this could be fixed.\r\n\r\nI'm referring to commit 80a74ca on URL:\r\nhttps://github.com/Parsely/pykafka/commit/80a74cacc8f6d48d749a7a08432208f4cdc9c00d#diff-27e6827c14ac03e8b996238c6e8813be\r\n\r\nKind regards,\r\nRuben \r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/792", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/792/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/792/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/792/events", "html_url": "https://github.com/Parsely/pykafka/issues/792", "id": 317473230, "node_id": "MDU6SXNzdWUzMTc0NzMyMzA=", "number": 792, "title": "pykafka.exceptions.SocketDisconnectedError: when call function broker.list_groups().groups.keys()", "user": {"login": "foreversunyao", "id": 19499755, "node_id": "MDQ6VXNlcjE5NDk5NzU1", "avatar_url": "https://avatars1.githubusercontent.com/u/19499755?v=4", "gravatar_id": "", "url": "https://api.github.com/users/foreversunyao", "html_url": "https://github.com/foreversunyao", "followers_url": "https://api.github.com/users/foreversunyao/followers", "following_url": "https://api.github.com/users/foreversunyao/following{/other_user}", "gists_url": "https://api.github.com/users/foreversunyao/gists{/gist_id}", "starred_url": "https://api.github.com/users/foreversunyao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/foreversunyao/subscriptions", "organizations_url": "https://api.github.com/users/foreversunyao/orgs", "repos_url": "https://api.github.com/users/foreversunyao/repos", "events_url": "https://api.github.com/users/foreversunyao/events{/privacy}", "received_events_url": "https://api.github.com/users/foreversunyao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-25T04:14:36Z", "updated_at": "2018-04-25T16:43:14Z", "closed_at": "2018-04-25T16:43:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nThe environment is as below:\r\n**PyKafka version**: \r\npykafka-tools (0.1.8)  - The pykafka-tools extend the functionalities of python pykafka module.\r\npykafka (2.7.0)        - Full-Featured Pure-Python Kafka Client\r\n**Kafka version**: 0.8.2.1\r\n\r\nAnd I used python 2.7 and when I call function as below:\r\n```\r\nclient = KafkaClient(hosts=kafka_brokers)\r\ntopics = client.topics\r\nbrokers = client.brokers\r\nconsumer_groups = []\r\nfor broker_id, broker in brokers.iteritems():\r\n        consumer_groups = consumer_groups + broker.list_groups().groups.keys()\r\n```\r\nseems all work well except  **broker.list_groups().groups.keys()**\r\n\r\nthe stack trace message are below:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/kafka_consumer_offset_check.py\", line 49, in <module>\r\n    consumer_groups = consumer_groups + broker.list_groups().groups.keys()\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/broker.py\", line 45, in wrapped\r\n    return fn(self, *args, **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/broker.py\", line 544, in list_groups\r\n    return future.get(ListGroupsResponse)\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/handlers.py\", line 74, in get\r\n    raise self.error\r\npykafka.exceptions.SocketDisconnectedError: <broker ip:9092>\r\n```\r\n\r\nAnd I can guarantee  to connect broker ip and port, and the same code works well when request kafka 1.0 version.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/791", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/791/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/791/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/791/events", "html_url": "https://github.com/Parsely/pykafka/issues/791", "id": 317448337, "node_id": "MDU6SXNzdWUzMTc0NDgzMzc=", "number": 791, "title": "consumer rate decreasing", "user": {"login": "arita37", "id": 18707623, "node_id": "MDQ6VXNlcjE4NzA3NjIz", "avatar_url": "https://avatars3.githubusercontent.com/u/18707623?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arita37", "html_url": "https://github.com/arita37", "followers_url": "https://api.github.com/users/arita37/followers", "following_url": "https://api.github.com/users/arita37/following{/other_user}", "gists_url": "https://api.github.com/users/arita37/gists{/gist_id}", "starred_url": "https://api.github.com/users/arita37/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arita37/subscriptions", "organizations_url": "https://api.github.com/users/arita37/orgs", "repos_url": "https://api.github.com/users/arita37/repos", "events_url": "https://api.github.com/users/arita37/events{/privacy}", "received_events_url": "https://api.github.com/users/arita37/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-25T01:23:18Z", "updated_at": "2018-04-25T16:27:45Z", "closed_at": "2018-04-25T16:27:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nWe are using normal code as follow :\r\nhttps://github.com/Parsely/pykafka/issues/781\r\n\r\n\r\n```\r\nconsumer = topic.get_balanced_consumer()  # all default parameters...  3 balanced consumer\r\nwhile True:\r\n    try:\r\n        for message in consumer:\r\n              ii+=1\r\n              msgraw = consumer.consume()\r\n              msg    = msgraw.value.decode('utf8')\r\n              msg    = json.loads(msg)   \r\n              .... #very fast process\r\n              producer.produce(json_data)\r\n   except Exception as e : \r\n               print(e)\r\n```\r\n\r\n\r\nWhat happens :   \r\n   0 min - 5 mins :   Input flow is  same Kafka server (test server with fixed flow).\r\n   After  5 mins:      Almost half of the expected.\r\n\r\n1) Just wondering if you have done some tests to see the consuming flow is matching kafka output ?\r\n\r\n2)  Is 1/2 of the flow matching anything in consumer config ?\r\n\r\nCan we re-open this issue ?\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/790", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/790/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/790/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/790/events", "html_url": "https://github.com/Parsely/pykafka/issues/790", "id": 317150339, "node_id": "MDU6SXNzdWUzMTcxNTAzMzk=", "number": 790, "title": "consuming rate decreasing", "user": {"login": "arita37", "id": 18707623, "node_id": "MDQ6VXNlcjE4NzA3NjIz", "avatar_url": "https://avatars3.githubusercontent.com/u/18707623?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arita37", "html_url": "https://github.com/arita37", "followers_url": "https://api.github.com/users/arita37/followers", "following_url": "https://api.github.com/users/arita37/following{/other_user}", "gists_url": "https://api.github.com/users/arita37/gists{/gist_id}", "starred_url": "https://api.github.com/users/arita37/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arita37/subscriptions", "organizations_url": "https://api.github.com/users/arita37/orgs", "repos_url": "https://api.github.com/users/arita37/repos", "events_url": "https://api.github.com/users/arita37/events{/privacy}", "received_events_url": "https://api.github.com/users/arita37/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-04-24T09:47:13Z", "updated_at": "2018-05-07T22:29:34Z", "closed_at": "2018-05-07T22:29:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "We have the following scheme with several python jobs:\r\n\r\nWhile True : \r\n    balancedConsumer\r\n    processMsg\r\n    produceMsg\r\n\r\n\r\nAll in auto-commit\r\n\r\nKafka server has fixed output rate  1000 msg/sec.\r\nFor some reasons,  Input rate is very high at beginning 1200 msg/sec and after\r\ndrops to  500 msg/sec, around half of the normal output rate.\r\n\r\nWhy the reason of the drop ?\r\nHow to check that consumer input rate is same than kafka server ?\r\n\r\nprocessMSG is very low latency (<1 ms processing).\r\n\r\n\r\n\r\n**PyKafka version**:    v2.7.0\r\n**Kafka version**:        0.8.1\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/788", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/788/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/788/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/788/events", "html_url": "https://github.com/Parsely/pykafka/issues/788", "id": 316661414, "node_id": "MDU6SXNzdWUzMTY2NjE0MTQ=", "number": 788, "title": "Producer linger_ms=0 blocks gevent", "user": {"login": "carsonip", "id": 9133397, "node_id": "MDQ6VXNlcjkxMzMzOTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9133397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carsonip", "html_url": "https://github.com/carsonip", "followers_url": "https://api.github.com/users/carsonip/followers", "following_url": "https://api.github.com/users/carsonip/following{/other_user}", "gists_url": "https://api.github.com/users/carsonip/gists{/gist_id}", "starred_url": "https://api.github.com/users/carsonip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carsonip/subscriptions", "organizations_url": "https://api.github.com/users/carsonip/orgs", "repos_url": "https://api.github.com/users/carsonip/repos", "events_url": "https://api.github.com/users/carsonip/events{/privacy}", "received_events_url": "https://api.github.com/users/carsonip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-23T04:55:57Z", "updated_at": "2018-05-08T21:12:44Z", "closed_at": "2018-05-08T21:12:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**PyKafka version**: 2.8.0dev2 (master) (but any would do)\r\n**Kafka version**: 0.10+, 1.0+ (but any would do)\r\n\r\nI am using Gevent handler.\r\nTo flush messages right away, I set `linger_ms=0` in producer. When there is no message, it keeps flushing in queue_reader and effectively blocks the whole program. I can imagine that the threaded version will not really \"block\" but a high CPU usage may be observed.\r\n\r\nI realize that to flush when there is at least 1 message, `min_queued_messages=1` should be used. \r\n\r\nHowever, the inline comment says:\r\n\r\n> linger_ms=0 indicates no lingering.\r\n\r\nwithout indicating the consequences, which may be misleading. If there is no plan to change the flush loop, I suggest adding a note of the consequences of `linger_ms=0`.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/786", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/786/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/786/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/786/events", "html_url": "https://github.com/Parsely/pykafka/issues/786", "id": 316214950, "node_id": "MDU6SXNzdWUzMTYyMTQ5NTA=", "number": 786, "title": "Subscribe to a regex topic pattern in pykafka?", "user": {"login": "vikt0rs", "id": 5230490, "node_id": "MDQ6VXNlcjUyMzA0OTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/5230490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vikt0rs", "html_url": "https://github.com/vikt0rs", "followers_url": "https://api.github.com/users/vikt0rs/followers", "following_url": "https://api.github.com/users/vikt0rs/following{/other_user}", "gists_url": "https://api.github.com/users/vikt0rs/gists{/gist_id}", "starred_url": "https://api.github.com/users/vikt0rs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vikt0rs/subscriptions", "organizations_url": "https://api.github.com/users/vikt0rs/orgs", "repos_url": "https://api.github.com/users/vikt0rs/repos", "events_url": "https://api.github.com/users/vikt0rs/events{/privacy}", "received_events_url": "https://api.github.com/users/vikt0rs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-20T10:30:19Z", "updated_at": "2018-04-28T09:09:36Z", "closed_at": "2018-04-23T16:40:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, folks,\r\n\r\nplease, suggest, if there is a possibility in pykafka to get a consumer, subscribed to the topic pattern (not a single topic) as kafka-python [has](http://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html#kafka.KafkaConsumer.subscribe) ?\r\nIf not - could you please suggest a workaround for this feature?\r\n\r\nThanks,\r\nViktor", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/782", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/782/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/782/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/782/events", "html_url": "https://github.com/Parsely/pykafka/issues/782", "id": 313004888, "node_id": "MDU6SXNzdWUzMTMwMDQ4ODg=", "number": 782, "title": "simplify test skipping logic", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 38059277, "node_id": "MDU6TGFiZWwzODA1OTI3Nw==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/testing", "name": "testing", "color": "fef2c0", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-04-10T16:45:24Z", "updated_at": "2018-07-24T16:58:08Z", "closed_at": "2018-07-24T16:58:08Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Now that https://github.com/pytest-dev/pytest/issues/568 has been fixed, we should investigate upgrading the pytest dependency and removing the extra code we use to handle test skipping.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/780", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/780/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/780/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/780/events", "html_url": "https://github.com/Parsely/pykafka/issues/780", "id": 311379764, "node_id": "MDU6SXNzdWUzMTEzNzk3NjQ=", "number": 780, "title": "[silly question] is there any way to force the broker to use the same port used when accessing kafka?", "user": {"login": "joshsleeper", "id": 849941, "node_id": "MDQ6VXNlcjg0OTk0MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/849941?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joshsleeper", "html_url": "https://github.com/joshsleeper", "followers_url": "https://api.github.com/users/joshsleeper/followers", "following_url": "https://api.github.com/users/joshsleeper/following{/other_user}", "gists_url": "https://api.github.com/users/joshsleeper/gists{/gist_id}", "starred_url": "https://api.github.com/users/joshsleeper/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joshsleeper/subscriptions", "organizations_url": "https://api.github.com/users/joshsleeper/orgs", "repos_url": "https://api.github.com/users/joshsleeper/repos", "events_url": "https://api.github.com/users/joshsleeper/events{/privacy}", "received_events_url": "https://api.github.com/users/joshsleeper/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 439232563, "node_id": "MDU6TGFiZWw0MzkyMzI1NjM=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/info-archive", "name": "info-archive", "color": "0e8a16", "default": false, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-04T20:29:57Z", "updated_at": "2018-04-11T18:58:46Z", "closed_at": "2018-04-11T18:14:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "# versions\r\n* `kafka 0.11.0.1`\r\n* `pykafka 2.7.0`\r\n\r\n# context\r\n* running kafka (among other services) in a `docker-compose` cluster locally\r\n* running kafka tests on the host machine, which means I'm using host ports mapped to internal container ports to get access to the container services\r\n* ports for services are exposed at _random_ host ports\r\n    * this is done by telling `docker-compose` which ports to expose for which container, but not telling it _which_ host ports to map to. in that case, `docker-compose` finds open ports on the system and maps the exposed internal ports to those.\r\n    * e.g. kafka may use port `9092` inside the container, but be randomly mapped to `31953` on the host\r\n* interested in querying the offset information for various topics with commands such as `pykafka.topic.Topic.latest_available_offsets()\r\n\r\n# problem/experience\r\n`pykafka.KafkaClient()` connects to kafka at the random port **just fine**, but always uses port `9092` when attempting to connect to the broker, which doesn't work because `9092` is only valid inside the container.\r\n\r\n## example connection pseudocode\r\n```py\r\nfrom pykafka import KafkaClient\r\n\r\nkafka_port = determine_random_kafka_port()\r\nkclient = KafkaClient(hosts='127.0.0.1:{}'.format(kafka_port))\r\nlatest_offsets = kclient.topics['topic-name'].latest_available_offsets()\r\n```\r\n\r\n# question\r\nas the title says, is there any way to get the broker connection to use the same port that's provided to `pykafka.KafkaClient()` in the `hosts` parameter?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/779", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/779/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/779/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/779/events", "html_url": "https://github.com/Parsely/pykafka/issues/779", "id": 309082174, "node_id": "MDU6SXNzdWUzMDkwODIxNzQ=", "number": 779, "title": "Comprehensive list of circumstances under which producer will fail to produce a message ", "user": {"login": "rueberger", "id": 8816362, "node_id": "MDQ6VXNlcjg4MTYzNjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/8816362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rueberger", "html_url": "https://github.com/rueberger", "followers_url": "https://api.github.com/users/rueberger/followers", "following_url": "https://api.github.com/users/rueberger/following{/other_user}", "gists_url": "https://api.github.com/users/rueberger/gists{/gist_id}", "starred_url": "https://api.github.com/users/rueberger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rueberger/subscriptions", "organizations_url": "https://api.github.com/users/rueberger/orgs", "repos_url": "https://api.github.com/users/rueberger/repos", "events_url": "https://api.github.com/users/rueberger/events{/privacy}", "received_events_url": "https://api.github.com/users/rueberger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-27T18:26:42Z", "updated_at": "2018-05-19T22:39:40Z", "closed_at": "2018-05-15T22:22:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Apologies if this was covered somewhere in the docs that I missed - but could we discuss the circumstances under which it is possible for a producer to fail delivery? \r\n\r\nI've been butting heads with a flaky failure for the last couple weeks that I am starting to suspect is caused by silent message delivery failure.  \r\n\r\nIn async mode, as pointed out in issue #603 messages can be lost when an unhandled exception is encountered in the producer's thread. Is this also true for synchronous producers?  \r\n\r\nUnder what other circumstances might an async producer silently fail to deliver a message? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/777", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/777/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/777/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/777/events", "html_url": "https://github.com/Parsely/pykafka/issues/777", "id": 306938611, "node_id": "MDU6SXNzdWUzMDY5Mzg2MTE=", "number": 777, "title": "print_managed_consumer_groups fails with SocketDisconnectedError when listing consumer groups", "user": {"login": "aldraco", "id": 9719214, "node_id": "MDQ6VXNlcjk3MTkyMTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9719214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aldraco", "html_url": "https://github.com/aldraco", "followers_url": "https://api.github.com/users/aldraco/followers", "following_url": "https://api.github.com/users/aldraco/following{/other_user}", "gists_url": "https://api.github.com/users/aldraco/gists{/gist_id}", "starred_url": "https://api.github.com/users/aldraco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aldraco/subscriptions", "organizations_url": "https://api.github.com/users/aldraco/orgs", "repos_url": "https://api.github.com/users/aldraco/repos", "events_url": "https://api.github.com/users/aldraco/events{/privacy}", "received_events_url": "https://api.github.com/users/aldraco/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-20T16:20:46Z", "updated_at": "2018-03-20T18:32:54Z", "closed_at": "2018-03-20T18:29:01Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "CLI command:\r\n`kafka-tools -b <broker>:9092 print_managed_consumer_groups <topic>`\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \".../bin/kafka-tools\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \".../pykafka/cli/kafka_tools.py\", line 460, in main\r\n    args.func(client, args)\r\n  File \".../pykafka/cli/kafka_tools.py\", line 133, in print_managed_consumer_groups\r\n    groups = broker.list_groups().groups.keys()\r\n  File \".../pykafka/broker.py\", line 45, in wrapped\r\n    return fn(self, *args, **kwargs)\r\n  File \".../pykafka/broker.py\", line 544, in list_groups\r\n    return future.get(ListGroupsResponse)\r\n  File \".../pykafka/handlers.py\", line 74, in get\r\n    raise self.error\r\npykafka.exceptions.SocketDisconnectedError: <broker ..redacted:9092>\r\n```\r\nThis is a known topic on this broker and can be seen when running `print_topics`. \r\n\r\nKafka 2.11-0.8.21", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/776", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/776/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/776/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/776/events", "html_url": "https://github.com/Parsely/pykafka/issues/776", "id": 305411850, "node_id": "MDU6SXNzdWUzMDU0MTE4NTA=", "number": 776, "title": "Monitor pykafka stream", "user": {"login": "arita37", "id": 18707623, "node_id": "MDQ6VXNlcjE4NzA3NjIz", "avatar_url": "https://avatars3.githubusercontent.com/u/18707623?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arita37", "html_url": "https://github.com/arita37", "followers_url": "https://api.github.com/users/arita37/followers", "following_url": "https://api.github.com/users/arita37/following{/other_user}", "gists_url": "https://api.github.com/users/arita37/gists{/gist_id}", "starred_url": "https://api.github.com/users/arita37/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arita37/subscriptions", "organizations_url": "https://api.github.com/users/arita37/orgs", "repos_url": "https://api.github.com/users/arita37/repos", "events_url": "https://api.github.com/users/arita37/events{/privacy}", "received_events_url": "https://api.github.com/users/arita37/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-15T04:28:23Z", "updated_at": "2018-03-22T17:12:57Z", "closed_at": "2018-03-15T16:34:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nWhen running in production, just wondering how people are monitoring\r\npykafka streaming flows (producer sending message) ?\r\n\r\nIs there any tool similar to Spark Streaming  monitor ?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/775", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/775/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/775/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/775/events", "html_url": "https://github.com/Parsely/pykafka/issues/775", "id": 303770747, "node_id": "MDU6SXNzdWUzMDM3NzA3NDc=", "number": 775, "title": "testing pykafka in spark nodes", "user": {"login": "arita37", "id": 18707623, "node_id": "MDQ6VXNlcjE4NzA3NjIz", "avatar_url": "https://avatars3.githubusercontent.com/u/18707623?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arita37", "html_url": "https://github.com/arita37", "followers_url": "https://api.github.com/users/arita37/followers", "following_url": "https://api.github.com/users/arita37/following{/other_user}", "gists_url": "https://api.github.com/users/arita37/gists{/gist_id}", "starred_url": "https://api.github.com/users/arita37/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arita37/subscriptions", "organizations_url": "https://api.github.com/users/arita37/orgs", "repos_url": "https://api.github.com/users/arita37/repos", "events_url": "https://api.github.com/users/arita37/events{/privacy}", "received_events_url": "https://api.github.com/users/arita37/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-09T09:07:32Z", "updated_at": "2018-03-12T16:17:35Z", "closed_at": "2018-03-12T16:17:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "We aiming to use pykafka under this configuration :   \r\n   python 3.5.4  (anaconda)\r\n   Docker image Centos\r\n   Spark nodes managed with Mesos.\r\n   Run the docker image with spark-submit\r\n  \r\nIs there any restriction using this config ?\r\n\r\nWe do not want to use spark streaming but pykafka for memory reasons...\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/772", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/772/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/772/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/772/events", "html_url": "https://github.com/Parsely/pykafka/issues/772", "id": 300834063, "node_id": "MDU6SXNzdWUzMDA4MzQwNjM=", "number": 772, "title": "No handlers could be found for logger", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-02-27T23:31:35Z", "updated_at": "2018-02-28T18:53:28Z", "closed_at": "2018-02-28T18:53:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "In many cases (such as [this one](https://github.com/Parsely/pykafka/issues/722#issuecomment-330232891)), PyKafka components spit out `No handlers could be found for logger` errors. This is a nuisance for a [lot](https://stackoverflow.com/q/48930612/735204) of users. Apparently it can be fixed with the [`NullHandler`](https://docs.python.org/3.1/library/logging.html#nullhandler). ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/767", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/767/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/767/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/767/events", "html_url": "https://github.com/Parsely/pykafka/issues/767", "id": 294032724, "node_id": "MDU6SXNzdWUyOTQwMzI3MjQ=", "number": 767, "title": "Storing Offsets Outside Kafka", "user": {"login": "dev-walker", "id": 25332338, "node_id": "MDQ6VXNlcjI1MzMyMzM4", "avatar_url": "https://avatars0.githubusercontent.com/u/25332338?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dev-walker", "html_url": "https://github.com/dev-walker", "followers_url": "https://api.github.com/users/dev-walker/followers", "following_url": "https://api.github.com/users/dev-walker/following{/other_user}", "gists_url": "https://api.github.com/users/dev-walker/gists{/gist_id}", "starred_url": "https://api.github.com/users/dev-walker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dev-walker/subscriptions", "organizations_url": "https://api.github.com/users/dev-walker/orgs", "repos_url": "https://api.github.com/users/dev-walker/repos", "events_url": "https://api.github.com/users/dev-walker/events{/privacy}", "received_events_url": "https://api.github.com/users/dev-walker/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-02T21:44:28Z", "updated_at": "2018-02-05T17:52:43Z", "closed_at": "2018-02-05T17:52:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi!\r\n\r\nHow I can implement [\"Storing Offsets Outside Kafka\"](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#rebalancecallback)? In Java Consumer they have method [`seek`](https://kafka.apache.org/10/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#seek-org.apache.kafka.common.TopicPartition-long-). \r\nI'm interested to do this with `ManagedBalancedConsumer`.\r\nI can see that there is the method `reset_offsets` in `SimpleConsumer`, but it seems that it commits offset to Kafka anyway.\r\n\r\npykafka **v2.7.0**\r\nkafka **v1.0.0**\r\n\r\nThanks in advance!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/766", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/766/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/766/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/766/events", "html_url": "https://github.com/Parsely/pykafka/issues/766", "id": 289847310, "node_id": "MDU6SXNzdWUyODk4NDczMTA=", "number": 766, "title": "offset = -1", "user": {"login": "Strong-Man", "id": 9315425, "node_id": "MDQ6VXNlcjkzMTU0MjU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9315425?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Strong-Man", "html_url": "https://github.com/Strong-Man", "followers_url": "https://api.github.com/users/Strong-Man/followers", "following_url": "https://api.github.com/users/Strong-Man/following{/other_user}", "gists_url": "https://api.github.com/users/Strong-Man/gists{/gist_id}", "starred_url": "https://api.github.com/users/Strong-Man/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Strong-Man/subscriptions", "organizations_url": "https://api.github.com/users/Strong-Man/orgs", "repos_url": "https://api.github.com/users/Strong-Man/repos", "events_url": "https://api.github.com/users/Strong-Man/events{/privacy}", "received_events_url": "https://api.github.com/users/Strong-Man/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-19T02:55:17Z", "updated_at": "2018-02-27T23:03:35Z", "closed_at": "2018-02-27T23:03:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "i used pykafka2.6,  when i used fetch_offsets of the SimpleConsumer, i get the offset = -1, that cause i will consume news than i haved consume", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/764", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/764/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/764/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/764/events", "html_url": "https://github.com/Parsely/pykafka/issues/764", "id": 287770064, "node_id": "MDU6SXNzdWUyODc3NzAwNjQ=", "number": 764, "title": "kafka timestamp function can't be used ", "user": {"login": "biansutao", "id": 371557, "node_id": "MDQ6VXNlcjM3MTU1Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/371557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/biansutao", "html_url": "https://github.com/biansutao", "followers_url": "https://api.github.com/users/biansutao/followers", "following_url": "https://api.github.com/users/biansutao/following{/other_user}", "gists_url": "https://api.github.com/users/biansutao/gists{/gist_id}", "starred_url": "https://api.github.com/users/biansutao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/biansutao/subscriptions", "organizations_url": "https://api.github.com/users/biansutao/orgs", "repos_url": "https://api.github.com/users/biansutao/repos", "events_url": "https://api.github.com/users/biansutao/events{/privacy}", "received_events_url": "https://api.github.com/users/biansutao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 22, "created_at": "2018-01-11T12:31:28Z", "updated_at": "2018-07-31T15:51:13Z", "closed_at": "2018-07-31T15:51:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "# Summary : \r\n\r\n> if you use kakfa 1.0.0 version, you want get the message's timestamp .\r\n> \r\n> KafkaClient  Object must be added broker_version parameter.\r\n> \r\n> Example:  client = KafkaClient(hosts=hosts,broker_version='1.0.0')\r\n\r\n\r\n\r\n\r\n\r\n\r\nwhen is send a mesage with timestamp,but when i consume this message i can't get the correct timestamp, only 0 . \r\n\r\nplease help me  out. \r\n\r\nthanks \r\n\r\n\r\n\r\npykafka version:  pykafka (2.7.0.dev1)\r\n\r\nit's my produce program:\r\n\r\n```python\r\nclient = KafkaClient(hosts=\"(localhost:9092,localhost:9093,localhost:9094\")\r\n\r\n    topic = client.topics['test_timestamp_1']\r\n    with topic.get_sync_producer() as producer:\r\n         for i in range(numbers):\r\n             producer._protocol_version = 1\r\n             producer.produce('test',timestamp=datetime.now());\r\n             # producer.produce('test message ' + str(i ** 2),timestamp=time.time())\r\n             producer.produce('test timestamp 1');\r\n```\r\nit's consumer progarm:\r\n\r\n```python\r\ndef consume(topic_name,hosts):\r\n    client = KafkaClient(hosts=hosts)\r\n    topic = client.topics[topic_name]\r\n    offset_dict = {}\r\n    total = 0\r\n    count = 0\r\n    for key, val in topic.earliest_available_offsets().items():\r\n        try:\r\n            offset_dict[str(key)] = val.offset[0]\r\n        except Exception,ex:\r\n            traceback.print_exc()\r\n            print ex.message\r\n            continue\r\n\r\n    for key, val in topic.latest_available_offsets().items():\r\n        total += (val.offset[0] - offset_dict[str(key)])\r\n\r\n    print('total records %d' % total)\r\n\r\n    if total == 0:\r\n        return\r\n\r\n    consumer = topic.get_simple_consumer(\r\n        consumer_group='consumer_' + id_generator())\r\n\r\n    with open(topic_name + '.txt', 'a+') as f:\r\n        for message in consumer:\r\n            if message is not None:\r\n\r\n                count += 1\r\n                print message.value +  '\\t' + str(message.protocol_version) + '\\t' + str(message.timestamp)\r\n\r\n                f.write(message.value.replace('\\\\n', '') + '\\t' + str(message.timestamp) + '\\n')\r\n                if count == total:\r\n                    break\r\n        consumer.stop()\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/762", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/762/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/762/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/762/events", "html_url": "https://github.com/Parsely/pykafka/issues/762", "id": 285874116, "node_id": "MDU6SXNzdWUyODU4NzQxMTY=", "number": 762, "title": "it's better to identify which partitioner in producer doc", "user": {"login": "j-style", "id": 6703387, "node_id": "MDQ6VXNlcjY3MDMzODc=", "avatar_url": "https://avatars2.githubusercontent.com/u/6703387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/j-style", "html_url": "https://github.com/j-style", "followers_url": "https://api.github.com/users/j-style/followers", "following_url": "https://api.github.com/users/j-style/following{/other_user}", "gists_url": "https://api.github.com/users/j-style/gists{/gist_id}", "starred_url": "https://api.github.com/users/j-style/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/j-style/subscriptions", "organizations_url": "https://api.github.com/users/j-style/orgs", "repos_url": "https://api.github.com/users/j-style/repos", "events_url": "https://api.github.com/users/j-style/events{/privacy}", "received_events_url": "https://api.github.com/users/j-style/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-04T03:44:42Z", "updated_at": "2018-01-04T18:58:17Z", "closed_at": "2018-01-04T18:58:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "**PyKafka version**: latest\r\n**Kafka version**:  any\r\n\r\nif initing a produce without specifying partitioner, pykafka will use RandomPartitioner rather than HashingPartitioner to identify partion_id, no matter setting an unnull partition_key or not.\r\n\r\nthis leads msg with same partition_key sended to random partition,  which is unexpected.\r\n\r\n=============\r\ni think it's necessary to notice this feature in document.\r\n\r\n\r\n\r\n\r\n  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/761", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/761/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/761/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/761/events", "html_url": "https://github.com/Parsely/pykafka/issues/761", "id": 284057520, "node_id": "MDU6SXNzdWUyODQwNTc1MjA=", "number": 761, "title": "NoNodeError on commit offset using 2.7.0-dev2", "user": {"login": "rodrabe", "id": 22826611, "node_id": "MDQ6VXNlcjIyODI2NjEx", "avatar_url": "https://avatars0.githubusercontent.com/u/22826611?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rodrabe", "html_url": "https://github.com/rodrabe", "followers_url": "https://api.github.com/users/rodrabe/followers", "following_url": "https://api.github.com/users/rodrabe/following{/other_user}", "gists_url": "https://api.github.com/users/rodrabe/gists{/gist_id}", "starred_url": "https://api.github.com/users/rodrabe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rodrabe/subscriptions", "organizations_url": "https://api.github.com/users/rodrabe/orgs", "repos_url": "https://api.github.com/users/rodrabe/repos", "events_url": "https://api.github.com/users/rodrabe/events{/privacy}", "received_events_url": "https://api.github.com/users/rodrabe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-22T02:35:21Z", "updated_at": "2018-02-27T23:45:04Z", "closed_at": "2018-02-27T23:45:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have 3 consumers reading the same topic with 120 partitions.\r\nOnly one of the three consumers failed;  however the lag for the topic increased.\r\nRestarting the consumer worked.\r\n\r\nLooks like we are getting a NoNodeError when trying to commit offsets:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Metrics-Transformation-Service/code_dir/consumer.py\", line 476, in run\r\n    self._process_messages(partitions)\r\n  File \"/Metrics-Transformation-Service/code_dir/consumer.py\", line 522, in _process_messages\r\n    self._consumer.commit_offsets()\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 750, in commit_offsets\r\n    self._raise_worker_exceptions()\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 284, in _raise_worker_exceptions\r\n    reraise(*self._worker_exception)\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 60, in wrapped\r\n    ret = fn(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 648, in _brokers_changed\r\n    self._rebalance()\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 579, in _rebalance\r\n    self.commit_offsets()\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 750, in commit_offsets\r\n    self._raise_worker_exceptions()\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 284, in _raise_worker_exceptions\r\n    reraise(*self._worker_exception)\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 60, in wrapped\r\n    ret = fn(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 648, in _brokers_changed\r\n    self._rebalance()\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 587, in _rebalance\r\n    self._update_member_assignment()\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 556, in _update_member_assignment\r\n    self._remove_partitions(current_zk_parts - new_partitions)\r\n  File \"/usr/local/lib/python2.7/site-packages/pykafka/balancedconsumer.py\", line 607, in _remove_partitions\r\n    self._zookeeper.delete(self._path_from_partition(p))\r\n  File \"/usr/local/lib/python2.7/site-packages/kazoo/client.py\", line 1316, in delete\r\n    return self.delete_async(path, version).get()\r\n  File \"/usr/local/lib/python2.7/site-packages/kazoo/handlers/utils.py\", line 79, in get\r\n    raise self._exception\r\nNoNodeError\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/759", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/759/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/759/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/759/events", "html_url": "https://github.com/Parsely/pykafka/issues/759", "id": 282409217, "node_id": "MDU6SXNzdWUyODI0MDkyMTc=", "number": 759, "title": "Type Error in client.topics", "user": {"login": "Juntaran", "id": 13732099, "node_id": "MDQ6VXNlcjEzNzMyMDk5", "avatar_url": "https://avatars2.githubusercontent.com/u/13732099?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Juntaran", "html_url": "https://github.com/Juntaran", "followers_url": "https://api.github.com/users/Juntaran/followers", "following_url": "https://api.github.com/users/Juntaran/following{/other_user}", "gists_url": "https://api.github.com/users/Juntaran/gists{/gist_id}", "starred_url": "https://api.github.com/users/Juntaran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Juntaran/subscriptions", "organizations_url": "https://api.github.com/users/Juntaran/orgs", "repos_url": "https://api.github.com/users/Juntaran/repos", "events_url": "https://api.github.com/users/Juntaran/events{/privacy}", "received_events_url": "https://api.github.com/users/Juntaran/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-15T12:11:50Z", "updated_at": "2017-12-16T03:45:18Z", "closed_at": "2017-12-16T03:45:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Error:\r\n\r\n```\r\nINFO:pykafka.cluster:Discovered 3 brokers\r\nINFO:pykafka.cluster:Discovered 46 topics\r\nTraceback (most recent call last):\r\n  File \"/Users/juntaran/workspace/pyWorkspace/kafkaTest/kafkaTest.py\", line 16, in <module>\r\n    topic = client.topics[\"test\"]\r\n  File \"/Users/juntaran/test/lib/python3.6/site-packages/pykafka/cluster.py\", line 56, in __getitem__\r\n    \"got '%s'\", type(key))\r\nTypeError: (\"TopicDict.__getitem__ accepts a bytes object, but it got '%s'\", <class 'str'>)\r\n```\r\n\r\nHere is my code:\r\n\r\n```python\r\nclient = KafkaClient(hosts=\"docker155:9092,docker156:9092,docker157:9092\")\r\ntopic = client.topics[\"test\"]\r\n\r\ncosumer = topic.get_simple_consumer(consumer_timeout_ms=10000)\r\ncnt = 0\r\nfor message in cosumer:\r\n\tif message is not None:\r\n\t\tprint(message.offset, message.value)\r\n\tcnt += 1\r\n\tprint(cnt)\r\n```\r\n\r\nPlease help me, thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/758", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/758/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/758/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/758/events", "html_url": "https://github.com/Parsely/pykafka/issues/758", "id": 282272014, "node_id": "MDU6SXNzdWUyODIyNzIwMTQ=", "number": 758, "title": "Creating a SimpleConsumer with consumer_group causes RequestError. ", "user": {"login": "jhcarter1985", "id": 7432650, "node_id": "MDQ6VXNlcjc0MzI2NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7432650?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhcarter1985", "html_url": "https://github.com/jhcarter1985", "followers_url": "https://api.github.com/users/jhcarter1985/followers", "following_url": "https://api.github.com/users/jhcarter1985/following{/other_user}", "gists_url": "https://api.github.com/users/jhcarter1985/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhcarter1985/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhcarter1985/subscriptions", "organizations_url": "https://api.github.com/users/jhcarter1985/orgs", "repos_url": "https://api.github.com/users/jhcarter1985/repos", "events_url": "https://api.github.com/users/jhcarter1985/events{/privacy}", "received_events_url": "https://api.github.com/users/jhcarter1985/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-14T23:24:16Z", "updated_at": "2018-05-29T22:56:34Z", "closed_at": "2018-05-29T22:56:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI'm trying to use PyKafka for a project to allow communication between microservices. I'm fairly green to Kafka, So sorry if these questions come across as n00b. I have looked through many stack overflow messages. And to the best of my knowledge no-one has posted a similar issue.\r\n\r\nIn my case I'm using a SimpleConsumer to connect to a 3 node cluster. My version numbers are as follows:\r\nPython2.7\r\nPyKafka 2.6.0\r\nKafka 0.9\r\n\r\nHere is the code where I'm utilizing my consumer. In this case i only have a single consumer running. We have not yet to begun to experiment with partitions or parallel tasks yet. I have been able to get things to work locally with a single confluent instance, and a landoop docker version as well. \r\n\r\n```\r\nif ENVIRONMENT == 'staging':\r\n    kafka_url = '<staging_kafka_ip_node1>:9092'\r\nelse:\r\n    kafka_url = '<dev_kafka>:9092'\r\n\r\nkafka_client = KafkaClient(kafka_url)\r\n\r\ntopics = ['topicA', 'topicB']\r\n\r\nkafka_consumers = {}\r\n\r\nfor topic in topics:\r\n    kafka_consumers[topic] = kafka_client.topics[topic].get_simple_consumer(consumer_group='task-runner', auto_commit_enable=True)\r\n```\r\nSo when trying to instantiate the consumer I'm getting the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"mapper/pm_mapper.py\", line 34, in <module>\r\n    kafka_consumers[topic] = kafka_client.topics[topic].get_simple_consumer(consumer_group='task-runner', auto_commit_enable=True)\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/topic.py\", line 196, in get_simple_consumer\r\n    **kwargs)\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/simpleconsumer.py\", line 190, in __init__\r\n    self._discover_group_coordinator()\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/simpleconsumer.py\", line 321, in _discover_group_coordinator\r\n    self._group_coordinator = self._cluster.get_group_coordinator(self._consumer_group)\r\n  File \"/usr/lib/python2.7/site-packages/pykafka/cluster.py\", line 412, in get_group_coordinator\r\n    future = broker.handler.request(req)\r\nAttributeError: 'NoneType' object has no attribute 'request'\r\n```\r\nRemoving the group name is not included the error does not occur. Is there something that I'm doing when setting this up that is wrong. \r\n\r\n   \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/755", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/755/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/755/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/755/events", "html_url": "https://github.com/Parsely/pykafka/issues/755", "id": 281572071, "node_id": "MDU6SXNzdWUyODE1NzIwNzE=", "number": 755, "title": "Support updated versions of MetadataRequest", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-12T23:20:26Z", "updated_at": "2017-12-14T23:28:24Z", "closed_at": "2017-12-14T23:28:24Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "pykafka currently only supports `MetadataRequest` and `MetadataResponse` v0. It needs to handle requests from later versions of the protocol. #750 is blocked by this.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/754", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/754/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/754/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/754/events", "html_url": "https://github.com/Parsely/pykafka/issues/754", "id": 281520147, "node_id": "MDU6SXNzdWUyODE1MjAxNDc=", "number": 754, "title": "Support automatic api version discovery", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 360183304, "node_id": "MDU6TGFiZWwzNjAxODMzMDQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/help%20wanted", "name": "help wanted", "color": "006b75", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-12T20:06:34Z", "updated_at": "2017-12-14T18:51:41Z", "closed_at": "2017-12-14T18:51:41Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Right now, we have a bunch of information about the supported versions of various APIs hardcoded into pykafka. This information should, when possible, be dynamically generated via the process outlined [here](https://kafka.apache.org/protocol#api_versions) (the `ApiVersionsRequest`). ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/747", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/747/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/747/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/747/events", "html_url": "https://github.com/Parsely/pykafka/issues/747", "id": 277717356, "node_id": "MDU6SXNzdWUyNzc3MTczNTY=", "number": 747, "title": "can not use bootstrap-server in new kafka version?", "user": {"login": "asynchronoust", "id": 6292860, "node_id": "MDQ6VXNlcjYyOTI4NjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6292860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asynchronoust", "html_url": "https://github.com/asynchronoust", "followers_url": "https://api.github.com/users/asynchronoust/followers", "following_url": "https://api.github.com/users/asynchronoust/following{/other_user}", "gists_url": "https://api.github.com/users/asynchronoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/asynchronoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asynchronoust/subscriptions", "organizations_url": "https://api.github.com/users/asynchronoust/orgs", "repos_url": "https://api.github.com/users/asynchronoust/repos", "events_url": "https://api.github.com/users/asynchronoust/events{/privacy}", "received_events_url": "https://api.github.com/users/asynchronoust/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-29T11:20:01Z", "updated_at": "2017-11-30T02:01:12Z", "closed_at": "2017-11-29T16:58:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nIn new version of kafka, consumer does not need to connect zookeeper, instead of bootstrap-server.\r\n\r\nDoes the pykafka not support this way for consumer?  I do not find any options in simpleConsumer or balancedConsumer\r\n\r\nDoes this mean that the balanced consumer in pykafka has to connect zookeeper ?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/745", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/745/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/745/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/745/events", "html_url": "https://github.com/Parsely/pykafka/issues/745", "id": 275783188, "node_id": "MDU6SXNzdWUyNzU3ODMxODg=", "number": 745, "title": "fetch_offsets unconditionally resets the offsets", "user": {"login": "bsideup", "id": 1050762, "node_id": "MDQ6VXNlcjEwNTA3NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/1050762?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bsideup", "html_url": "https://github.com/bsideup", "followers_url": "https://api.github.com/users/bsideup/followers", "following_url": "https://api.github.com/users/bsideup/following{/other_user}", "gists_url": "https://api.github.com/users/bsideup/gists{/gist_id}", "starred_url": "https://api.github.com/users/bsideup/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bsideup/subscriptions", "organizations_url": "https://api.github.com/users/bsideup/orgs", "repos_url": "https://api.github.com/users/bsideup/repos", "events_url": "https://api.github.com/users/bsideup/events{/privacy}", "received_events_url": "https://api.github.com/users/bsideup/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-21T16:46:01Z", "updated_at": "2018-03-05T19:38:13Z", "closed_at": "2018-03-05T19:38:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nIn pykafka v2.6.0, call to `fetch_offsets` will populate `partition_offsets_to_reset` and consumer will attempt to reset the offsets.\r\nThis is very unpleasant side-effect because we use pykafka to calculate consumer lag and except it to operate in read-only mode.\r\n\r\nIs it possible to add some flag to make sure that `fetch_offsets` will not reset the offsets?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/744", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/744/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/744/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/744/events", "html_url": "https://github.com/Parsely/pykafka/issues/744", "id": 275551126, "node_id": "MDU6SXNzdWUyNzU1NTExMjY=", "number": 744, "title": "ManagedBalancedConsumer unique consumer_group", "user": {"login": "djamalouch", "id": 17527960, "node_id": "MDQ6VXNlcjE3NTI3OTYw", "avatar_url": "https://avatars1.githubusercontent.com/u/17527960?v=4", "gravatar_id": "", "url": "https://api.github.com/users/djamalouch", "html_url": "https://github.com/djamalouch", "followers_url": "https://api.github.com/users/djamalouch/followers", "following_url": "https://api.github.com/users/djamalouch/following{/other_user}", "gists_url": "https://api.github.com/users/djamalouch/gists{/gist_id}", "starred_url": "https://api.github.com/users/djamalouch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/djamalouch/subscriptions", "organizations_url": "https://api.github.com/users/djamalouch/orgs", "repos_url": "https://api.github.com/users/djamalouch/repos", "events_url": "https://api.github.com/users/djamalouch/events{/privacy}", "received_events_url": "https://api.github.com/users/djamalouch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 15352879, "node_id": "MDU6TGFiZWwxNTM1Mjg3OQ==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/documentation", "name": "documentation", "color": "0b02e1", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-11-21T01:04:51Z", "updated_at": "2017-11-21T22:36:04Z", "closed_at": "2017-11-21T22:36:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI think it would be great, if the description of the ManagedBalancedConsumer is clarified with respect to the parameter consumer_group. I used the same consumer_group string for different consumer instances on different topics and wondered why the consumers are dropping some messages. It would have saved me a lot of hours, if I had seen the keyword \"unique\" somewhere in the description.\r\n\r\nApart from that: keep up the great work with this library!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/741", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/741/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/741/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/741/events", "html_url": "https://github.com/Parsely/pykafka/issues/741", "id": 273983255, "node_id": "MDU6SXNzdWUyNzM5ODMyNTU=", "number": 741, "title": "Upgrade tests to use latest Kafka broker version", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 38059277, "node_id": "MDU6TGFiZWwzODA1OTI3Nw==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/testing", "name": "testing", "color": "fef2c0", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-11-14T23:40:17Z", "updated_at": "2017-11-16T21:39:55Z", "closed_at": "2017-11-16T21:39:55Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "At the time of this writing, the most recent version of the Kafka broker is 1.0.0. Our Travis tests should be updated to test both 0.8.2 and 1.0.0 (or whatever the current latest version is), since these are the oldest and newest versions we support.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/740", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/740/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/740/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/740/events", "html_url": "https://github.com/Parsely/pykafka/issues/740", "id": 271895017, "node_id": "MDU6SXNzdWUyNzE4OTUwMTc=", "number": 740, "title": "producer thread safe", "user": {"login": "asynchronoust", "id": 6292860, "node_id": "MDQ6VXNlcjYyOTI4NjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6292860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asynchronoust", "html_url": "https://github.com/asynchronoust", "followers_url": "https://api.github.com/users/asynchronoust/followers", "following_url": "https://api.github.com/users/asynchronoust/following{/other_user}", "gists_url": "https://api.github.com/users/asynchronoust/gists{/gist_id}", "starred_url": "https://api.github.com/users/asynchronoust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asynchronoust/subscriptions", "organizations_url": "https://api.github.com/users/asynchronoust/orgs", "repos_url": "https://api.github.com/users/asynchronoust/repos", "events_url": "https://api.github.com/users/asynchronoust/events{/privacy}", "received_events_url": "https://api.github.com/users/asynchronoust/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-11-07T16:24:27Z", "updated_at": "2019-03-06T17:17:57Z", "closed_at": "2017-11-07T19:51:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is the producer in pykafka is thread safe?  \r\n\r\nI want to run multi threads to fetch data from other place, and in every thread, i will produce the message to kafka. Using multi threads to increase the throughtput.\r\n\r\nbut i did not find  instruction about producer  thread safety. \r\n\r\nthe Kakfa-python library has illustrated the producer thread safety, the link is: [https://kafka-python.readthedocs.io/en/master/index.html](https://kafka-python.readthedocs.io/en/master/index.html)\r\n`Thread safety\r\nThe KafkaProducer can be used across threads without issue, unlike the KafkaConsumer which cannot.`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/739", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/739/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/739/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/739/events", "html_url": "https://github.com/Parsely/pykafka/issues/739", "id": 271383775, "node_id": "MDU6SXNzdWUyNzEzODM3NzU=", "number": 739, "title": "Need help with producer blocking forever", "user": {"login": "carsonip", "id": 9133397, "node_id": "MDQ6VXNlcjkxMzMzOTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9133397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carsonip", "html_url": "https://github.com/carsonip", "followers_url": "https://api.github.com/users/carsonip/followers", "following_url": "https://api.github.com/users/carsonip/following{/other_user}", "gists_url": "https://api.github.com/users/carsonip/gists{/gist_id}", "starred_url": "https://api.github.com/users/carsonip/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carsonip/subscriptions", "organizations_url": "https://api.github.com/users/carsonip/orgs", "repos_url": "https://api.github.com/users/carsonip/repos", "events_url": "https://api.github.com/users/carsonip/events{/privacy}", "received_events_url": "https://api.github.com/users/carsonip/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 603289911, "node_id": "MDU6TGFiZWw2MDMyODk5MTE=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/gevent", "name": "gevent", "color": "e99695", "default": false, "description": null}, {"id": 360183304, "node_id": "MDU6TGFiZWwzNjAxODMzMDQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/help%20wanted", "name": "help wanted", "color": "006b75", "default": true, "description": null}, {"id": 439232563, "node_id": "MDU6TGFiZWw0MzkyMzI1NjM=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/info-archive", "name": "info-archive", "color": "0e8a16", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-11-06T07:40:52Z", "updated_at": "2018-02-27T22:43:53Z", "closed_at": "2018-02-27T22:43:53Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\n**PyKafka version**: commit d5206629683c376e243af7f3aa6843b551e7ace5\r\n**Kafka version**: 0.10.1.0-0\r\n**Python version**: 2.7.6\r\n\r\nMy producer is blocked forever and would like to seek help here. I use greenlet in my application. This happens very rarely, but when it happens, it causes any code after the produce() function to not run.\r\n\r\npykafka Config (shared among greenlets):\r\n```\r\n_push_client = KafkaClient(hosts=KAFKA_HOSTS, use_greenlets=True)\r\n_push_topic = _push_client.topics[KAFKA_TOPIC]\r\n\r\ndef get_push_topic():\r\n    return _push_topic\r\n```\r\n\r\nHow I use pykafka (in each greenlet):\r\n```\r\nwith get_push_topic().get_producer() as producer:\r\n    payload = {...}\r\n    producer.produce(json.dumps(payload))\r\n```\r\n\r\nLogs (IP address of broker is masked):\r\n```\r\nINFO:pykafka.producer:Starting new produce worker for broker 0\r\nINFO:pykafka.producer:Blocking until all messages are sent\r\nDEBUG:pykafka.producer:Sending 1 messages to broker 0\r\nWARNING:pykafka.producer:Error encountered when producing to broker -.-.-.-:9092. Retrying.\r\nDEBUG:pykafka.cluster:Updating cluster, attempt 1/3\r\nDEBUG:pykafka.connection:Connecting to -.-.-.-:9092\r\nDEBUG:pykafka.connection:Successfully connected to -.-.-.-:9092\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 1 brokers\r\nINFO:pykafka.cluster:Reconnecting to broker id 0: -.-.-.-:9092\r\nDEBUG:pykafka.connection:Connecting to -.-.-.-:9092\r\nDEBUG:pykafka.connection:Successfully connected to -.-.-.-:9092\r\nINFO:pykafka.handlers:RequestHandler.stop: about to flush requests queue\r\nINFO:pykafka.cluster:Discovered 5 topics\r\nINFO:pykafka.topic:Adding 1 partitions\r\nINFO:pykafka.producer:Starting new produce worker for broker 0\r\nDEBUG:pykafka.producer:Successfully sent 0/1 messages to broker 0\r\nINFO:pykafka.producer:Worker exited for broker -.-.-.-:9092\r\nINFO:pykafka.producer:Blocking until all messages are sent\r\nINFO:pykafka.handlers:RequestHandler worker: exiting cleanly\r\nINFO:pykafka.handlers:RequestHandler worker: exiting cleanly\r\nINFO:pykafka.producer:Worker exited for broker -.-.-.-:9092\r\n```\r\n\r\nObservations:\r\n\r\n1. Subsequent lines don't run\r\n2. That specific greenlet is still alive\r\n\r\nBackground info:\r\n1. Applied monkey patch from gevent and gevent_openssl\r\n2. Set socket timeout to 2 minutes `socket.setdefaulttimeout(2*60)`\r\n3. There is also a balancedconsumer running in another greenlet\r\n\r\nQuestions:\r\n\r\n1. Is it ok to share a KafkaClient among greenlets and spawn producers (and produce messages) in each greenlet? Any problem with my setup / usage?\r\n2. Any idea why pykafka exits before all messages are sent?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/738", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/738/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/738/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/738/events", "html_url": "https://github.com/Parsely/pykafka/issues/738", "id": 268681313, "node_id": "MDU6SXNzdWUyNjg2ODEzMTM=", "number": 738, "title": "I would like to know how kafka.OffsetOutOfRangeException is resolved", "user": {"login": "15663671003", "id": 26687843, "node_id": "MDQ6VXNlcjI2Njg3ODQz", "avatar_url": "https://avatars3.githubusercontent.com/u/26687843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/15663671003", "html_url": "https://github.com/15663671003", "followers_url": "https://api.github.com/users/15663671003/followers", "following_url": "https://api.github.com/users/15663671003/following{/other_user}", "gists_url": "https://api.github.com/users/15663671003/gists{/gist_id}", "starred_url": "https://api.github.com/users/15663671003/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/15663671003/subscriptions", "organizations_url": "https://api.github.com/users/15663671003/orgs", "repos_url": "https://api.github.com/users/15663671003/repos", "events_url": "https://api.github.com/users/15663671003/events{/privacy}", "received_events_url": "https://api.github.com/users/15663671003/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-26T09:04:46Z", "updated_at": "2017-10-26T16:55:11Z", "closed_at": "2017-10-26T16:55:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/737", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/737/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/737/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/737/events", "html_url": "https://github.com/Parsely/pykafka/issues/737", "id": 268680738, "node_id": "MDU6SXNzdWUyNjg2ODA3Mzg=", "number": 737, "title": "I would like to know whether the offset range of this partition has changed since kafka periodically cleaned up the data", "user": {"login": "15663671003", "id": 26687843, "node_id": "MDQ6VXNlcjI2Njg3ODQz", "avatar_url": "https://avatars3.githubusercontent.com/u/26687843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/15663671003", "html_url": "https://github.com/15663671003", "followers_url": "https://api.github.com/users/15663671003/followers", "following_url": "https://api.github.com/users/15663671003/following{/other_user}", "gists_url": "https://api.github.com/users/15663671003/gists{/gist_id}", "starred_url": "https://api.github.com/users/15663671003/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/15663671003/subscriptions", "organizations_url": "https://api.github.com/users/15663671003/orgs", "repos_url": "https://api.github.com/users/15663671003/repos", "events_url": "https://api.github.com/users/15663671003/events{/privacy}", "received_events_url": "https://api.github.com/users/15663671003/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-26T09:02:53Z", "updated_at": "2017-10-26T16:55:17Z", "closed_at": "2017-10-26T16:55:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/734", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/734/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/734/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/734/events", "html_url": "https://github.com/Parsely/pykafka/issues/734", "id": 265666990, "node_id": "MDU6SXNzdWUyNjU2NjY5OTA=", "number": 734, "title": "How to get timestamp of message?", "user": {"login": "joukosusi", "id": 20793539, "node_id": "MDQ6VXNlcjIwNzkzNTM5", "avatar_url": "https://avatars2.githubusercontent.com/u/20793539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joukosusi", "html_url": "https://github.com/joukosusi", "followers_url": "https://api.github.com/users/joukosusi/followers", "following_url": "https://api.github.com/users/joukosusi/following{/other_user}", "gists_url": "https://api.github.com/users/joukosusi/gists{/gist_id}", "starred_url": "https://api.github.com/users/joukosusi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joukosusi/subscriptions", "organizations_url": "https://api.github.com/users/joukosusi/orgs", "repos_url": "https://api.github.com/users/joukosusi/repos", "events_url": "https://api.github.com/users/joukosusi/events{/privacy}", "received_events_url": "https://api.github.com/users/joukosusi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-16T07:10:30Z", "updated_at": "2017-10-17T18:26:15Z", "closed_at": "2017-10-17T18:26:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want to get timestamp of message with pykafka, but i checked the code and didn't find the way.  I checked the `Message`  structor and only find following attributes:\r\n\r\n `\r\n   \r\n    __slots__ = [\r\n\r\n        \"compression_type\",\r\n\r\n        \"partition_key\",\r\n\r\n        \"value\",\r\n\r\n        \"offset\",\r\n\r\n        \"partition_id\",\r\n\r\n        \"partition\",\r\n\r\n        \"produce_attempt\",\r\n\r\n        \"delivery_report_q\"\r\n\r\n    ]\r\n`\r\nwhat should i do for that ?\r\n\r\n**PyKafka version**:  2.6.0\r\n**Kafka version**: 0.10.2\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/733", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/733/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/733/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/733/events", "html_url": "https://github.com/Parsely/pykafka/issues/733", "id": 264708089, "node_id": "MDU6SXNzdWUyNjQ3MDgwODk=", "number": 733, "title": "simple consumer reset_offsets fails", "user": {"login": "yhuangbl", "id": 14951226, "node_id": "MDQ6VXNlcjE0OTUxMjI2", "avatar_url": "https://avatars0.githubusercontent.com/u/14951226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yhuangbl", "html_url": "https://github.com/yhuangbl", "followers_url": "https://api.github.com/users/yhuangbl/followers", "following_url": "https://api.github.com/users/yhuangbl/following{/other_user}", "gists_url": "https://api.github.com/users/yhuangbl/gists{/gist_id}", "starred_url": "https://api.github.com/users/yhuangbl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yhuangbl/subscriptions", "organizations_url": "https://api.github.com/users/yhuangbl/orgs", "repos_url": "https://api.github.com/users/yhuangbl/repos", "events_url": "https://api.github.com/users/yhuangbl/events{/privacy}", "received_events_url": "https://api.github.com/users/yhuangbl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-11T19:31:26Z", "updated_at": "2018-07-19T21:20:53Z", "closed_at": "2018-07-19T21:20:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a simple consumer and I want it to consume from an offset. However, even though there are new messages between the starting offset and current time (I manually checked from Kafka console and there are new messages), the consumer does not consume any message and is blocked indefinitely. \r\nLogger print out:\r\n```\r\npykafka.simpleconsumer - WARNING - Offset reset for partition 0 to timestamp 34668160 failed. Setting partition 0's internal counter to 34668160\r\nWARNING:pykafka.simpleconsumer:Offset reset for partition 1 to timestamp 34659355 failed. Setting partition 1's internal counter to 34659355\r\nstop offset: {0: OffsetPartitionResponse(offset=[34668258], err=0), 1: OffsetPartitionResponse(offset=[34659479], err=0)}\r\n```\r\nabstract of the code:\r\n```\r\nconsumer.reset_offsets(start_offsets)\r\nfor msg in consumer:\r\n    if msg is not None:\r\n            print msg.value\r\n```\r\n\r\n(with the same code, same version of python and pykafka, sometimes the consumer can print out messages, sometimes it doesn't)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/732", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/732/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/732/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/732/events", "html_url": "https://github.com/Parsely/pykafka/issues/732", "id": 264268701, "node_id": "MDU6SXNzdWUyNjQyNjg3MDE=", "number": 732, "title": "Pure-Python component speed optimization", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-10T15:17:01Z", "updated_at": "2017-12-07T21:12:03Z", "closed_at": "2017-12-07T21:12:03Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "As mentioned in his blogpost [here](http://matthewrocklin.com/blog/work/2017/10/10/kafka-python), @mrocklin noticed that there are some potential easy wins for throughput optimization in the pure python consumer and producer. We should investigate potential changes that can lead to significant increases in message processing speed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/730", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/730/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/730/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/730/events", "html_url": "https://github.com/Parsely/pykafka/issues/730", "id": 261795621, "node_id": "MDU6SXNzdWUyNjE3OTU2MjE=", "number": 730, "title": "Add pykafka recipe to conda-forge", "user": {"login": "mrocklin", "id": 306380, "node_id": "MDQ6VXNlcjMwNjM4MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/306380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrocklin", "html_url": "https://github.com/mrocklin", "followers_url": "https://api.github.com/users/mrocklin/followers", "following_url": "https://api.github.com/users/mrocklin/following{/other_user}", "gists_url": "https://api.github.com/users/mrocklin/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrocklin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrocklin/subscriptions", "organizations_url": "https://api.github.com/users/mrocklin/orgs", "repos_url": "https://api.github.com/users/mrocklin/repos", "events_url": "https://api.github.com/users/mrocklin/events{/privacy}", "received_events_url": "https://api.github.com/users/mrocklin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 360183304, "node_id": "MDU6TGFiZWwzNjAxODMzMDQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/help%20wanted", "name": "help wanted", "color": "006b75", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-29T23:22:06Z", "updated_at": "2017-11-30T18:00:38Z", "closed_at": "2017-11-30T18:00:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "It might be useful to have a `pykafka` recipe on conda-forge.  This would automatically build conda packages for various platforms and would probably help users looking to use pykafka with librdkafka, for which there is already a [binary package](https://github.com/conda-forge/librdkafka-feedstock/).  \r\n\r\n[Adding a recipe](https://conda-forge.org/#add_recipe) is relatively easy.  I'm happy to help, though the conda-forge maintainers are also some of the most helpful devs that I've met online.  \r\n\r\nThere might also be some opportunity to help pykafka automatically find librdkafka when installed in this manner.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/728", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/728/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/728/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/728/events", "html_url": "https://github.com/Parsely/pykafka/issues/728", "id": 260219942, "node_id": "MDU6SXNzdWUyNjAyMTk5NDI=", "number": 728, "title": "Fetch offsets using timestamp", "user": {"login": "nikhilbiyani", "id": 4973563, "node_id": "MDQ6VXNlcjQ5NzM1NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4973563?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikhilbiyani", "html_url": "https://github.com/nikhilbiyani", "followers_url": "https://api.github.com/users/nikhilbiyani/followers", "following_url": "https://api.github.com/users/nikhilbiyani/following{/other_user}", "gists_url": "https://api.github.com/users/nikhilbiyani/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikhilbiyani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikhilbiyani/subscriptions", "organizations_url": "https://api.github.com/users/nikhilbiyani/orgs", "repos_url": "https://api.github.com/users/nikhilbiyani/repos", "events_url": "https://api.github.com/users/nikhilbiyani/events{/privacy}", "received_events_url": "https://api.github.com/users/nikhilbiyani/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-09-25T10:06:04Z", "updated_at": "2020-07-28T02:36:23Z", "closed_at": "2018-04-12T23:25:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "The functionality to fetch offsets using time stamps is coded in the `Topic` class method: `fetch_offset_limits`. But I am having problems with fetching the offsets. In principle, this method should return a list of offsets that were written at a timestamp.\r\n\r\nSee this test code snippet below:\r\n\r\n```\r\nclient = KafkaClient(broker_version=\"0.11.0\")\r\ntopic = client.topics['mytopic']\r\np = topic.get_producer()\r\ntm = long(time.time() * 1000)\r\nfor m_id in range(0,10):\r\n        p.produce('-%d-' % m_id, timestamp = tm, partition_key='k/e/y')\r\np.stop()\r\n\r\n# The following should give me a list of 10 offsets\r\nprint topic.fetch_offset_limits(tm, 10)\r\n```\r\nThe last line prints:\r\n```\r\n{0: OffsetPartitionResponse(offset=[], err=0)}\r\n```\r\nThis `offset` list should have been of length 10, but is empty.\r\n\r\nFYI: I tried `kafka-python` consumer method `offset_for_time` and it works fine in this test.\r\n\r\nCan you please help me with this.\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/727", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/727/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/727/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/727/events", "html_url": "https://github.com/Parsely/pykafka/issues/727", "id": 259698815, "node_id": "MDU6SXNzdWUyNTk2OTg4MTU=", "number": 727, "title": "Offset rolled back when kafka server restart", "user": {"login": "joukosusi", "id": 20793539, "node_id": "MDQ6VXNlcjIwNzkzNTM5", "avatar_url": "https://avatars2.githubusercontent.com/u/20793539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joukosusi", "html_url": "https://github.com/joukosusi", "followers_url": "https://api.github.com/users/joukosusi/followers", "following_url": "https://api.github.com/users/joukosusi/following{/other_user}", "gists_url": "https://api.github.com/users/joukosusi/gists{/gist_id}", "starred_url": "https://api.github.com/users/joukosusi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joukosusi/subscriptions", "organizations_url": "https://api.github.com/users/joukosusi/orgs", "repos_url": "https://api.github.com/users/joukosusi/repos", "events_url": "https://api.github.com/users/joukosusi/events{/privacy}", "received_events_url": "https://api.github.com/users/joukosusi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-22T04:19:28Z", "updated_at": "2017-09-25T03:10:34Z", "closed_at": "2017-09-25T03:10:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I restarted kafka server at about 2017-09-14 18:30, and then found the client got the old offset after few minutes according to following logs:\r\n\r\n\r\n\r\n> [2017-09-14 18:31:04,372][56565][140131847137088-MainThread][DEBUG]<kafka_consumer_client.py:139> get message for topic: 4a47d7071de0489bd83094bb770d0770c953f3c83c03362d113465fede862ae0c81e495a70c5223ebf2453497d213f32070e314abbdc3af054d8708e2e1b48ca, group: 4a47d7071de0489bd83094bb770d0770c953f3c83c03362d113465fede862ae0c81e495a70c5223ebf2453497d213f32070e314abbdc3af054d8708e2e1b48ca, offset: 2186774, length: 192\r\n...\r\n[2017-09-14 18:36:50,108][56565][140131847137088-MainThread][DEBUG]<kafka_consumer_client.py:139> get message for topic: 4a47d7071de0489bd83094bb770d0770c953f3c83c03362d113465fede862ae0c81e495a70c5223ebf2453497d213f32070e314abbdc3af054d8708e2e1b48ca, group: 4a47d7071de0489bd83094bb770d0770c953f3c83c03362d113465fede862ae0c81e495a70c5223ebf2453497d213f32070e314abbdc3af054d8708e2e1b48ca, offset: 2151272, length: 757056\r\n`\r\n\r\ni checked the log of pykafka and found the following error:\r\n\r\n\r\n> [2017-09-14 18:36:35,282[140131847137088-MainThread][INFO]<cluster.py:415> Found coordinator broker with id 0\r\n[2017-09-14 18:36:35,282][140131847137088-MainThread][ERROR]<simpleconsumer.py:582> Error fetching offsets for topic '4a47d7071de0489bd83094bb770d0770c953f3c83c03362d113465fede862ae0c81e495a70c5223ebf2453497d213f32070e314abbdc3af054d8708e2e1b48ca' (errors: {<class 'pykafka.exceptions.NotCoordinatorForGroup'>: [0]})\r\n[2017-09-14 18:36:36,238][140129677866752-941: pykafka.RequestHandler.worker for x.x.x.x:9093][INFO]<handlers.py:214> RequestHandler worker: exiting cleanly\r\n[2017-09-14 18:36:39,286][140131847137088-MainThread][INFO]<simpleconsumer.py:403> Starting 1 fetcher threads\r\n[2017-09-14 18:36:39,287][140131847137088-MainThread][INFO]<balancedconsumer.py:579> Rebalancing Complete.\r\n[2017-09-14 18:36:39,297][140129677866752-942: pykafka.SimpleConsumer.fetcher][INFO]<simpleconsumer.py:274> Resetting offsets in response to OffsetOutOfRangeError\r\n[2017-09-14 18:36:39,297][140129677866752-942: pykafka.SimpleConsumer.fetcher][INFO]<simpleconsumer.py:658> Resetting offsets for 1 partitions`\r\n\r\n\r\nit seems that pykafka commited an invalid offset and used the default value(auto_offset_reset=EARLIEST) to set. I have checked the code and still confused about this.  #209 seems to be a similar issue, should this also be a problem about auto-commit?? we didn't use auto-commit in the code as following:\r\n\r\n` \r\n\r\n        result = self.consumer.get(self.topic, self.group, block=False, auto_commit_enable=False)\r\n\r\n         if result:\r\n\r\n                message = get_plain_text(result.value, self.key_iv)\r\n\r\n                self.consumer.commit_offsets(self.topic, self.group)\r\n\r\n                return message\r\n`\r\n\r\n[logs.zip](https://github.com/Parsely/pykafka/files/1323406/logs.zip)\r\n\r\n**PyKafka version**:  2.5.0\r\n**Kafka version**: 9.0.0\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/724", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/724/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/724/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/724/events", "html_url": "https://github.com/Parsely/pykafka/issues/724", "id": 258483458, "node_id": "MDU6SXNzdWUyNTg0ODM0NTg=", "number": 724, "title": "Read all messages on startup in log compacted topic and exit", "user": {"login": "nikhilbiyani", "id": 4973563, "node_id": "MDQ6VXNlcjQ5NzM1NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4973563?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikhilbiyani", "html_url": "https://github.com/nikhilbiyani", "followers_url": "https://api.github.com/users/nikhilbiyani/followers", "following_url": "https://api.github.com/users/nikhilbiyani/following{/other_user}", "gists_url": "https://api.github.com/users/nikhilbiyani/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikhilbiyani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikhilbiyani/subscriptions", "organizations_url": "https://api.github.com/users/nikhilbiyani/orgs", "repos_url": "https://api.github.com/users/nikhilbiyani/repos", "events_url": "https://api.github.com/users/nikhilbiyani/events{/privacy}", "received_events_url": "https://api.github.com/users/nikhilbiyani/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-18T13:40:23Z", "updated_at": "2018-04-17T18:12:10Z", "closed_at": "2017-09-18T18:18:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a Kafka topic with log compaction (partition_key -> values) and in an application I want to read in all the messages present in the topic at startup and exit the loop. \r\n\r\nI did the following now:\r\n```\r\nclient = KafkaClient(broker_version=\"0.11.0\")\r\nc = client.topics['mytopic'].get_simple_consumer(\r\n                auto_offset_reset=OffsetType.EARLIEST,\r\n                reset_offset_on_start=True,\r\n                compacted_topic=True)\r\nwhile True:\r\n        message = c.consume(False)\r\n        if message is not None:\r\n                print message.offset, message.partition_key, message.value\r\n        else:\r\n                break\r\n```\r\n\r\nBut it is quite possible that there is a `None` message before consuming all messages, and then the logic breaks.\r\n\r\nThanks for the help in advance.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/722", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/722/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/722/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/722/events", "html_url": "https://github.com/Parsely/pykafka/issues/722", "id": 257381260, "node_id": "MDU6SXNzdWUyNTczODEyNjA=", "number": 722, "title": "Support CreateTopics / DeleteTopics APIs", "user": {"login": "messense", "id": 1556054, "node_id": "MDQ6VXNlcjE1NTYwNTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1556054?v=4", "gravatar_id": "", "url": "https://api.github.com/users/messense", "html_url": "https://github.com/messense", "followers_url": "https://api.github.com/users/messense/followers", "following_url": "https://api.github.com/users/messense/following{/other_user}", "gists_url": "https://api.github.com/users/messense/gists{/gist_id}", "starred_url": "https://api.github.com/users/messense/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/messense/subscriptions", "organizations_url": "https://api.github.com/users/messense/orgs", "repos_url": "https://api.github.com/users/messense/repos", "events_url": "https://api.github.com/users/messense/events{/privacy}", "received_events_url": "https://api.github.com/users/messense/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-09-13T13:31:06Z", "updated_at": "2017-12-18T22:24:55Z", "closed_at": "2017-12-18T22:24:55Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://kafka.apache.org/protocol#The_Messages_CreateTopics\r\n\r\nIt would be awesome to be able to create/delete topics in pykafka.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/721", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/721/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/721/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/721/events", "html_url": "https://github.com/Parsely/pykafka/issues/721", "id": 257282649, "node_id": "MDU6SXNzdWUyNTcyODI2NDk=", "number": 721, "title": "ImportError: cannot import name _rd_kafka", "user": {"login": "dzmhust", "id": 13388122, "node_id": "MDQ6VXNlcjEzMzg4MTIy", "avatar_url": "https://avatars3.githubusercontent.com/u/13388122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzmhust", "html_url": "https://github.com/dzmhust", "followers_url": "https://api.github.com/users/dzmhust/followers", "following_url": "https://api.github.com/users/dzmhust/following{/other_user}", "gists_url": "https://api.github.com/users/dzmhust/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzmhust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzmhust/subscriptions", "organizations_url": "https://api.github.com/users/dzmhust/orgs", "repos_url": "https://api.github.com/users/dzmhust/repos", "events_url": "https://api.github.com/users/dzmhust/events{/privacy}", "received_events_url": "https://api.github.com/users/dzmhust/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-13T07:26:34Z", "updated_at": "2017-09-22T05:05:44Z", "closed_at": "2017-09-21T22:19:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I run, prompt for the following error\uff0c how can i resolve this problem\u3002\r\n```\r\n2017-09-13 15:08:30 [pykafka.topic] INFO: Could not load pykafka.rdkafka extension.\r\n2017-09-13 15:08:30 [pykafka.topic] DEBUG: Traceback:\r\nTraceback (most recent call last):\r\n  File \"D:\\Python27\\lib\\site-packages\\pykafka\\topic.py\", line 39, in <module>\r\n    from . import rdkafka\r\n  File \"D:\\Python27\\lib\\site-packages\\pykafka\\rdkafka\\__init__.py\", line 1, in <module>\r\n    from .producer import RdKafkaProducer\r\n  File \"D:\\Python27\\lib\\site-packages\\pykafka\\rdkafka\\producer.py\", line 7, in <module>\r\n    from . import _rd_kafka\r\nImportError: cannot import name _rd_kafka\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/720", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/720/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/720/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/720/events", "html_url": "https://github.com/Parsely/pykafka/issues/720", "id": 256479588, "node_id": "MDU6SXNzdWUyNTY0Nzk1ODg=", "number": 720, "title": "simpleconsumer does not raise errors in fetch_offsets.", "user": {"login": "iciclespider", "id": 81206, "node_id": "MDQ6VXNlcjgxMjA2", "avatar_url": "https://avatars1.githubusercontent.com/u/81206?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iciclespider", "html_url": "https://github.com/iciclespider", "followers_url": "https://api.github.com/users/iciclespider/followers", "following_url": "https://api.github.com/users/iciclespider/following{/other_user}", "gists_url": "https://api.github.com/users/iciclespider/gists{/gist_id}", "starred_url": "https://api.github.com/users/iciclespider/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iciclespider/subscriptions", "organizations_url": "https://api.github.com/users/iciclespider/orgs", "repos_url": "https://api.github.com/users/iciclespider/repos", "events_url": "https://api.github.com/users/iciclespider/events{/privacy}", "received_events_url": "https://api.github.com/users/iciclespider/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-10T02:18:46Z", "updated_at": "2017-09-14T19:40:09Z", "closed_at": "2017-09-13T21:26:50Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "simpleconsumer is not correctly raising KafkaException in the fetch_offsets method. This is due to the following test: \r\n https://github.com/Parsely/pykafka/blob/master/pykafka/simpleconsumer.py#L597-L598\r\n```\r\n        if len(parts_by_error) > 1:\r\n            raise KafkaException(parts_by_error)\r\n```\r\nWhen the same error code is returned for all partitions, such as GroupLoadInProgress.ERROR, then the length of parts_by_error will be 1, preventing the raising of KafkaException. This results in the subsequent reseting of the partition's offsets when messages are consumed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/717", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/717/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/717/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/717/events", "html_url": "https://github.com/Parsely/pykafka/issues/717", "id": 253067645, "node_id": "MDU6SXNzdWUyNTMwNjc2NDU=", "number": 717, "title": "why balancedconsumer  do not write offset into zookeeper?", "user": {"login": "lixl1019", "id": 17242755, "node_id": "MDQ6VXNlcjE3MjQyNzU1", "avatar_url": "https://avatars1.githubusercontent.com/u/17242755?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lixl1019", "html_url": "https://github.com/lixl1019", "followers_url": "https://api.github.com/users/lixl1019/followers", "following_url": "https://api.github.com/users/lixl1019/following{/other_user}", "gists_url": "https://api.github.com/users/lixl1019/gists{/gist_id}", "starred_url": "https://api.github.com/users/lixl1019/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lixl1019/subscriptions", "organizations_url": "https://api.github.com/users/lixl1019/orgs", "repos_url": "https://api.github.com/users/lixl1019/repos", "events_url": "https://api.github.com/users/lixl1019/events{/privacy}", "received_events_url": "https://api.github.com/users/lixl1019/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}, {"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-26T06:31:24Z", "updated_at": "2018-02-27T22:24:28Z", "closed_at": "2018-02-27T22:24:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "**this is my consumer code:**\r\n```\r\nconsumer = topic.get_balanced_consumer(consumer_group=\"test1\".encode(),\r\n                                               reset_offset_on_start=False,\r\n                                               auto_commit_enable=True,\r\n                                               auto_offset_reset=OffsetType.LATEST,\r\n                                               use_rdkafka=True,\r\n                                               auto_commit_interval_ms=1000,\r\n                                               queued_max_messages=10000,\r\n                                               consumer_timeout_ms=1000,\r\n```\r\n                                               zookeeper_connect=\"10.92.244.165:2181,10.92.244.167:2181,10.92.244.187:2181\")\r\n**this is the consumer info in zookeeper**\r\nls /consumers/test1\r\n[ids, owners]\r\n\r\nWhen I use logstash to consume, the consumer info in zookeeper like this:\r\n[ids, owners, offsets]\r\n\r\nwhy balancedconsumer  do not write offset into zookeeper?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/711", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/711/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/711/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/711/events", "html_url": "https://github.com/Parsely/pykafka/issues/711", "id": 252144451, "node_id": "MDU6SXNzdWUyNTIxNDQ0NTE=", "number": 711, "title": "Does pykafka support 0.9.x, 0.10.x", "user": {"login": "zuiwanting", "id": 2741280, "node_id": "MDQ6VXNlcjI3NDEyODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2741280?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuiwanting", "html_url": "https://github.com/zuiwanting", "followers_url": "https://api.github.com/users/zuiwanting/followers", "following_url": "https://api.github.com/users/zuiwanting/following{/other_user}", "gists_url": "https://api.github.com/users/zuiwanting/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuiwanting/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuiwanting/subscriptions", "organizations_url": "https://api.github.com/users/zuiwanting/orgs", "repos_url": "https://api.github.com/users/zuiwanting/repos", "events_url": "https://api.github.com/users/zuiwanting/events{/privacy}", "received_events_url": "https://api.github.com/users/zuiwanting/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 15352879, "node_id": "MDU6TGFiZWwxNTM1Mjg3OQ==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/documentation", "name": "documentation", "color": "0b02e1", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-08-23T02:49:59Z", "updated_at": "2017-08-24T18:23:14Z", "closed_at": "2017-08-24T18:23:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "i found in this page [kafka python clients](https://cwiki.apache.org//confluence/display/KAFKA/Clients#Clients-Python) , pykafka only support   Kafka Version: 0.8.x,  can i  use pykafka if the kafka server is 0.9.x or 0.10.x", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/710", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/710/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/710/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/710/events", "html_url": "https://github.com/Parsely/pykafka/issues/710", "id": 252045139, "node_id": "MDU6SXNzdWUyNTIwNDUxMzk=", "number": 710, "title": "Issue while importing pykafka", "user": {"login": "riomario1", "id": 12400952, "node_id": "MDQ6VXNlcjEyNDAwOTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/12400952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riomario1", "html_url": "https://github.com/riomario1", "followers_url": "https://api.github.com/users/riomario1/followers", "following_url": "https://api.github.com/users/riomario1/following{/other_user}", "gists_url": "https://api.github.com/users/riomario1/gists{/gist_id}", "starred_url": "https://api.github.com/users/riomario1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riomario1/subscriptions", "organizations_url": "https://api.github.com/users/riomario1/orgs", "repos_url": "https://api.github.com/users/riomario1/repos", "events_url": "https://api.github.com/users/riomario1/events{/privacy}", "received_events_url": "https://api.github.com/users/riomario1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-22T18:16:43Z", "updated_at": "2017-08-23T19:07:56Z", "closed_at": "2017-08-23T19:07:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI have installed pykafka but when I import it, I get an error. Any help?\r\n\r\n    from pykafka import KafkaClient \r\n    ImportError: cannot import name KafkaClient\r\n\r\n    (test) [cloudera@quickstart ~]$ pip list | grep pykafka\r\n    DEPRECATION: The default format will switch to columns in the future. You can use --format=\r\n    (legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to \r\n    disable this warning.\r\n    pykafka (2.6.0)\r\n\r\nThis is how I installed pykafka:\r\npip install pykafka\r\n\r\nThanks,\r\nRio", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/709", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/709/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/709/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/709/events", "html_url": "https://github.com/Parsely/pykafka/issues/709", "id": 251728875, "node_id": "MDU6SXNzdWUyNTE3Mjg4NzU=", "number": 709, "title": "lz4 Support", "user": {"login": "sghaskell", "id": 8450537, "node_id": "MDQ6VXNlcjg0NTA1Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/8450537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sghaskell", "html_url": "https://github.com/sghaskell", "followers_url": "https://api.github.com/users/sghaskell/followers", "following_url": "https://api.github.com/users/sghaskell/following{/other_user}", "gists_url": "https://api.github.com/users/sghaskell/gists{/gist_id}", "starred_url": "https://api.github.com/users/sghaskell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sghaskell/subscriptions", "organizations_url": "https://api.github.com/users/sghaskell/orgs", "repos_url": "https://api.github.com/users/sghaskell/repos", "events_url": "https://api.github.com/users/sghaskell/events{/privacy}", "received_events_url": "https://api.github.com/users/sghaskell/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945345, "node_id": "MDU6TGFiZWw4OTQ1MzQ1", "url": "https://api.github.com/repos/Parsely/pykafka/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 360183304, "node_id": "MDU6TGFiZWwzNjAxODMzMDQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/help%20wanted", "name": "help wanted", "color": "006b75", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "sghaskell", "id": 8450537, "node_id": "MDQ6VXNlcjg0NTA1Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/8450537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sghaskell", "html_url": "https://github.com/sghaskell", "followers_url": "https://api.github.com/users/sghaskell/followers", "following_url": "https://api.github.com/users/sghaskell/following{/other_user}", "gists_url": "https://api.github.com/users/sghaskell/gists{/gist_id}", "starred_url": "https://api.github.com/users/sghaskell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sghaskell/subscriptions", "organizations_url": "https://api.github.com/users/sghaskell/orgs", "repos_url": "https://api.github.com/users/sghaskell/repos", "events_url": "https://api.github.com/users/sghaskell/events{/privacy}", "received_events_url": "https://api.github.com/users/sghaskell/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sghaskell", "id": 8450537, "node_id": "MDQ6VXNlcjg0NTA1Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/8450537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sghaskell", "html_url": "https://github.com/sghaskell", "followers_url": "https://api.github.com/users/sghaskell/followers", "following_url": "https://api.github.com/users/sghaskell/following{/other_user}", "gists_url": "https://api.github.com/users/sghaskell/gists{/gist_id}", "starred_url": "https://api.github.com/users/sghaskell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sghaskell/subscriptions", "organizations_url": "https://api.github.com/users/sghaskell/orgs", "repos_url": "https://api.github.com/users/sghaskell/repos", "events_url": "https://api.github.com/users/sghaskell/events{/privacy}", "received_events_url": "https://api.github.com/users/sghaskell/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2017-08-21T17:55:02Z", "updated_at": "2017-11-16T22:00:55Z", "closed_at": "2017-11-16T22:00:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is support for lz4 compression on the roadmap and if so, when do you anticipate a release?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/708", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/708/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/708/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/708/events", "html_url": "https://github.com/Parsely/pykafka/issues/708", "id": 251306789, "node_id": "MDU6SXNzdWUyNTEzMDY3ODk=", "number": 708, "title": "Connecting pykafka to Spark and loading to HDFS", "user": {"login": "riomario1", "id": 12400952, "node_id": "MDQ6VXNlcjEyNDAwOTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/12400952?v=4", "gravatar_id": "", "url": "https://api.github.com/users/riomario1", "html_url": "https://github.com/riomario1", "followers_url": "https://api.github.com/users/riomario1/followers", "following_url": "https://api.github.com/users/riomario1/following{/other_user}", "gists_url": "https://api.github.com/users/riomario1/gists{/gist_id}", "starred_url": "https://api.github.com/users/riomario1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/riomario1/subscriptions", "organizations_url": "https://api.github.com/users/riomario1/orgs", "repos_url": "https://api.github.com/users/riomario1/repos", "events_url": "https://api.github.com/users/riomario1/events{/privacy}", "received_events_url": "https://api.github.com/users/riomario1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945347, "node_id": "MDU6TGFiZWw4OTQ1MzQ3", "url": "https://api.github.com/repos/Parsely/pykafka/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-18T17:14:26Z", "updated_at": "2017-08-23T19:15:43Z", "closed_at": "2017-08-23T19:15:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI have 2 questions:\r\n\r\n1. How do you connect pykafka with Spark 2.x? Both Spark streaming and structured streaming.\r\n\r\n2. How to load consumer data into HDFS using pykafka?\r\n\r\nThanks,\r\nRio", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/704", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/704/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/704/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/704/events", "html_url": "https://github.com/Parsely/pykafka/issues/704", "id": 243744754, "node_id": "MDU6SXNzdWUyNDM3NDQ3NTQ=", "number": 704, "title": "RdKafkaSimpleConsumer ignores worker exceptions", "user": {"login": "bobgrigoryan", "id": 464179, "node_id": "MDQ6VXNlcjQ2NDE3OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/464179?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bobgrigoryan", "html_url": "https://github.com/bobgrigoryan", "followers_url": "https://api.github.com/users/bobgrigoryan/followers", "following_url": "https://api.github.com/users/bobgrigoryan/following{/other_user}", "gists_url": "https://api.github.com/users/bobgrigoryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/bobgrigoryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bobgrigoryan/subscriptions", "organizations_url": "https://api.github.com/users/bobgrigoryan/orgs", "repos_url": "https://api.github.com/users/bobgrigoryan/repos", "events_url": "https://api.github.com/users/bobgrigoryan/events{/privacy}", "received_events_url": "https://api.github.com/users/bobgrigoryan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}, {"id": 170576876, "node_id": "MDU6TGFiZWwxNzA1NzY4NzY=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/librdkafka", "name": "librdkafka", "color": "006b75", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-07-18T14:53:00Z", "updated_at": "2017-09-08T17:19:11Z", "closed_at": "2017-08-24T18:12:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using RdKafkaSimpleConsumer with auto_commit enabled.\r\nAt some point autocommit worker thread receives some exception and dies.\r\nBefore exiting it sets:\r\n`self._worker_exception = sys.exc_info()` \r\n\r\nThis worker exception never reaches main thread and it continues to work like nothing happened, since nowhere self._raise_worker_exceptions() is called.\r\n\r\nIn difference with SimpleConsumer which calls self._raise_worker_exceptions() from consume() function, the overridden consume() function in RdKafkaSimpleConsumer does not raise worker exceptions.\r\n\r\nCan be reproduced by manually raising some exception in autocommit worker thread.\r\nAlso simulated exception on SimpleConsumer, and it works as expected - consume() exits.\r\n\r\npykafka==2.6.0\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/703", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/703/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/703/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/703/events", "html_url": "https://github.com/Parsely/pykafka/issues/703", "id": 243666723, "node_id": "MDU6SXNzdWUyNDM2NjY3MjM=", "number": 703, "title": " BrokenPipeError: [Errno 32] Broken pipe", "user": {"login": "ShichaoMa", "id": 20733718, "node_id": "MDQ6VXNlcjIwNzMzNzE4", "avatar_url": "https://avatars2.githubusercontent.com/u/20733718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ShichaoMa", "html_url": "https://github.com/ShichaoMa", "followers_url": "https://api.github.com/users/ShichaoMa/followers", "following_url": "https://api.github.com/users/ShichaoMa/following{/other_user}", "gists_url": "https://api.github.com/users/ShichaoMa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ShichaoMa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ShichaoMa/subscriptions", "organizations_url": "https://api.github.com/users/ShichaoMa/orgs", "repos_url": "https://api.github.com/users/ShichaoMa/repos", "events_url": "https://api.github.com/users/ShichaoMa/events{/privacy}", "received_events_url": "https://api.github.com/users/ShichaoMa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 227537714, "node_id": "MDU6TGFiZWwyMjc1Mzc3MTQ=", "url": "https://api.github.com/repos/Parsely/pykafka/labels/hazy", "name": "hazy", "color": "fbca04", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-18T10:14:53Z", "updated_at": "2017-09-21T22:20:48Z", "closed_at": "2017-09-21T22:20:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'd like to know why this error heppen when I use it in scrapy pipiline.\r\nhere is the code\r\n\r\n```\r\ndef async_produce_wrapper(producer, logger):\r\n    count = 0\r\n\r\n    def wrapper(func):\r\n\r\n        def inner(*args, **kwargs):\r\n            result = func(*args, **kwargs)\r\n            nonlocal count\r\n            count += 1\r\n            if count % 10 == 0:  # adjust this or bring lots of RAM ;)\r\n                while True:\r\n                    try:\r\n                        msg, exc = producer.get_delivery_report(block=False)\r\n                        if exc is not None:\r\n                            logger.error('Failed to deliver msg {}: {}'.format(\r\n                                msg.partition_key, repr(exc)))\r\n                        else:\r\n                            logger.info('Successfully delivered msg {}'.format(\r\n                                msg.partition_key))\r\n                    except Empty:\r\n                        break\r\n            return result\r\n        return inner\r\n    return wrapper\r\n\r\n\r\nkafka = KafkaClient(\"192.168.200.140:9092,192.168.200.141:9092,192.168.200.142:9092\")\r\nproducer = kafka.topics[b\"jay_firehose_ingress\"].get_producer(delivery_reports=True)\r\nproducer.produce = async_produce_wrapper(producer, logger)(producer.produce)\r\n.......\r\n.......\r\n.......\r\nproducer.produce(message.encode(\"utf-8\"))\r\n````\r\nhere is the error\r\n```\r\nb'error heppend in pipline Traceback (most recent call last):'\r\nb' File \"/app/crawling/pipelines.py\", line 76, in process_item'\r\nb' self.producer.produce(message.encode(\"utf-8\"))'\r\nb' File \"/app/crawling/spiders/utils.py\", line 47, in inner'\r\nb' result = func(*args, **kwargs)'\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/producer.py\", line 350, in produce'\r\nb' self._raise_worker_exceptions()'\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/producer.py\", line 204, in _raise_worker_exceptions'\r\nb' raise ex'\r\n[above repeats >100 times]\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/producer.py\", line 541, in queue_reader'\r\nb' self.producer._send_request(batch, self)'\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/producer.py\", line 421, in _send_request'\r\nb' response = owned_broker.broker.produce_messages(req)'\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/broker.py\", line 296, in produce_messages'\r\nb' return future.get(ProduceResponse)'\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/handlers.py\", line 70, in get'\r\nb' raise self.error'\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/handlers.py\", line 205, in worker'\r\nb' shared.connection.request(task.request)'\r\nb' File \"/usr/local/lib/python3.6/site-packages/pykafka/connection.py\", line 198, in request'\r\nb' self._socket.sendall(bytes_)'\r\nb'BrokenPipeError: [Errno 32] Broken pipe'\r\n```\r\n\r\nI wonder why the error message is so long and repeat many times,\r\n\r\nI start 30 docker container to run this code. \r\n\r\nhow to avoid this error?\r\n\r\nmy English is pool, I hope you can know my meaning. thanks. \r\n\r\n**PyKafka version**: 2.5.0\r\n**Kafka version**: 0.10.1.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/701", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/701/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/701/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/701/events", "html_url": "https://github.com/Parsely/pykafka/issues/701", "id": 243163589, "node_id": "MDU6SXNzdWUyNDMxNjM1ODk=", "number": 701, "title": "balancedconsumer will get a PartitionOwnedError", "user": {"login": "nsf-remoter", "id": 8180526, "node_id": "MDQ6VXNlcjgxODA1MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/8180526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nsf-remoter", "html_url": "https://github.com/nsf-remoter", "followers_url": "https://api.github.com/users/nsf-remoter/followers", "following_url": "https://api.github.com/users/nsf-remoter/following{/other_user}", "gists_url": "https://api.github.com/users/nsf-remoter/gists{/gist_id}", "starred_url": "https://api.github.com/users/nsf-remoter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nsf-remoter/subscriptions", "organizations_url": "https://api.github.com/users/nsf-remoter/orgs", "repos_url": "https://api.github.com/users/nsf-remoter/repos", "events_url": "https://api.github.com/users/nsf-remoter/events{/privacy}", "received_events_url": "https://api.github.com/users/nsf-remoter/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-15T09:04:56Z", "updated_at": "2017-09-19T21:59:47Z", "closed_at": "2017-09-19T21:59:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "1. start a kafka server and not include any data, create a topic with 3(any number greater than one)partition.\r\n2. create a balancedconsumer, set `consumer_timeout_ms = -1`.\r\n3. start the balancedconsumer client, and then start it again, here are two client named clientA, clientB.\r\n4. after rebalance_max_retries*rebalance_backoff_ms ms, clientB will get a `PartitionOwnedError`.\r\n5. i have debug the code, find the reason is `consume()` function will always acquire the `_rebalancing_lock` and not to release it, so `_rebalance()` will block at `with self._rebalancing_lock`.\r\n\r\nit occur in both master and dev version.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/699", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/699/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/699/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/699/events", "html_url": "https://github.com/Parsely/pykafka/issues/699", "id": 242643426, "node_id": "MDU6SXNzdWUyNDI2NDM0MjY=", "number": 699, "title": "Wrong error message when certificate verify fails", "user": {"login": "rubinatorz", "id": 11735227, "node_id": "MDQ6VXNlcjExNzM1MjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/11735227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubinatorz", "html_url": "https://github.com/rubinatorz", "followers_url": "https://api.github.com/users/rubinatorz/followers", "following_url": "https://api.github.com/users/rubinatorz/following{/other_user}", "gists_url": "https://api.github.com/users/rubinatorz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubinatorz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubinatorz/subscriptions", "organizations_url": "https://api.github.com/users/rubinatorz/orgs", "repos_url": "https://api.github.com/users/rubinatorz/repos", "events_url": "https://api.github.com/users/rubinatorz/events{/privacy}", "received_events_url": "https://api.github.com/users/rubinatorz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-13T09:41:31Z", "updated_at": "2017-08-07T21:23:08Z", "closed_at": "2017-08-07T21:23:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was debugging an issue with setting up my connection and the error message I got was:\r\n\r\n'Unable to connect to a broker to fetch metadata. See logs.'\r\n\r\nSo I enabled debug logging and indeed, some more context was given there:\r\n\r\n```\r\n2017-07-12 14:25:06,311 Failed to connect to 123.123.123.123:9093\r\n2017-07-12 14:25:06,311 Failed to connect newly created broker for 123.123.123.123:9093\r\n2017-07-12 14:25:06,311 Socket disconnected during metadata request for broker 123.123.123.123:9093. Continuing.\r\n```\r\n\r\nAfter looking for some time, I found out that this is not a metadata issue, but my (SSL) CA file was incorrect, which resulted in a certificate verification error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\r\n\r\nBut this error wasn't logged... So here are the steps that I took to find out:\r\n\r\nI was looking into this issue and trying to find out what was going wrong here. I found all three messages in the source code (respectively connection.py, broker.py, cluster.py). I noticed that in connection.py in the function def connect two errors are caught in one line:\r\n\r\n`except (self._handler.SockErr, self._handler.GaiError):`\r\n\r\nAnd when one of those 2 errors happen, the following will be executed:\r\n\r\n```\r\nlog.info(\"Failed to connect to %s:%s\", self.host, self.port)\r\nraise SocketDisconnectedError\r\n```\r\n\r\nSo that's my first logmessage. Then the SocketDisconnectedError is raised and caught by the Broker constructor:\r\n\r\n`log.warning(\"Failed to connect newly created broker for {host}:{port}\".format(host=self._host, port=self._port))`\r\n\r\nThat's my second logmessage and the constructor just continues. So where does the 3rd logmessage comes from: def _request_metadata in cluster.py:\r\n\r\n```\r\nexcept SocketDisconnectedError:\r\n   log.error(\"Socket disconnected during metadata request for \"\r\n       \"broker %s:%s. Continuing.\", host, port)\r\n```\r\n\r\nSo if the Broker constructor has caught the SocketDisconnectedError itself, some other line must raise a SocketDisconnectedError also. So the call to broker.request_metadata is the one that also raises such error. And indeed the @_check_handler doest this.\r\n\r\n_check_handler raising a SocketDisconnectedError is fine, but logging \"Socket disconnected during metadata request\" is not completely right. Because request_metadata is not reached because of @_check_handler. So I was figuring out what was wrong in metadata context... but I found out something else was going on.\r\n\r\nDrilling down the code I came again at the lines in connection.py:\r\n\r\n```\r\ndef connect(self, timeout):\r\n    \"\"\"Connect to the broker.\"\"\"\r\n    log.debug(\"Connecting to %s:%s\", self.host, self.port)\r\n    try:\r\n        self._socket = self._wrap_socket(\r\n            self._handler.Socket.create_connection(\r\n                (self.host, self.port),\r\n                timeout / 1000,\r\n                (self.source_host, self.source_port)\r\n            ))\r\n    except (self._handler.SockErr, self._handler.GaiError):\r\n        log.info(\"Failed to connect to %s:%s\", self.host, self.port)\r\n        raise SocketDisconnectedError\r\n    log.debug(\"Successfully connected to %s:%s\", self.host, self.port)\r\n```\r\n\r\nNo actual errormessage is logged... so I logged the SockErr itself:\r\n\r\n```\r\n    except (self._handler.SockErr as, self._handler.GaiError) as e:\r\n        log.info(\"Failed to connect to %s:%s %s\", self.host, self.port, str(e))\r\n```\r\n\r\nAnd found out that this is going on:\r\n\r\nFailed to connect to 123.123.123.123:9093 [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)\r\n\r\nSo I would like to suggest to log this str(e) error so you exactly know what's going on.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/697", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/697/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/697/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/697/events", "html_url": "https://github.com/Parsely/pykafka/issues/697", "id": 239940334, "node_id": "MDU6SXNzdWUyMzk5NDAzMzQ=", "number": 697, "title": "Please do not use Broker-side exceptions for Client-side errors", "user": {"login": "jeffwidman", "id": 483314, "node_id": "MDQ6VXNlcjQ4MzMxNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/483314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeffwidman", "html_url": "https://github.com/jeffwidman", "followers_url": "https://api.github.com/users/jeffwidman/followers", "following_url": "https://api.github.com/users/jeffwidman/following{/other_user}", "gists_url": "https://api.github.com/users/jeffwidman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeffwidman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeffwidman/subscriptions", "organizations_url": "https://api.github.com/users/jeffwidman/orgs", "repos_url": "https://api.github.com/users/jeffwidman/repos", "events_url": "https://api.github.com/users/jeffwidman/events{/privacy}", "received_events_url": "https://api.github.com/users/jeffwidman/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 8945343, "node_id": "MDU6TGFiZWw4OTQ1MzQz", "url": "https://api.github.com/repos/Parsely/pykafka/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-07-01T10:04:40Z", "updated_at": "2017-10-19T07:24:07Z", "closed_at": "2017-08-24T19:52:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When debugging https://github.com/Parsely/pykafka/issues/696 where the broker successfully returns a message to the consumer and then the consumer blows up, I was confused because the error that was raised was `MessageSizeTooLarge` which is a broker error code number 10: https://kafka.apache.org/protocol.html#protocol_error_codes\r\n\r\nThis was extremely confusing to me because initial spelunking in the pykafka source showed that this exception subclassed `ProtocolClientError`, indicating that the broker was returning the error code:\r\nhttps://github.com/Parsely/pykafka/blob/c73f060abbf46f81ccc42798e5dc7e55bb3c65a2/pykafka/exceptions.py#L138\r\n\r\nFurthermore, it made no sense contextually because that error code is only emitted by the broker to a producer trying to produce too large of a message, not to a consumer.\r\n\r\nIt had me scratching my head until I realized that `pykafka` was re-using what is semantically a broker-side exception for a client-side error:  https://github.com/Parsely/pykafka/blame/c80f66c0d0b11d830aa333ed486967b5f242bc2f/pykafka/protocol.py#L383\r\n\r\nDigging a little deeper, I see that originally this exception wasn't the same as a broker-side error, but in #500 @emmett9001 changed it to re-use that exception class based on discussion in #497.\r\n\r\nCan I request that you limit the use of broker-side exceptions to only situations where the broker is actually returning an error code?\r\n\r\nIn this particular case, the proper fix is probably removing this particular usage of the exception altogether as suggested in #696. If you choose to keep it a hard-limit and throw an exception, can you use an exception that more clearly indicates a client-side error? Probably with a suggestion to the user to increase their `fetch_message_max_bytes` setting.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Parsely/pykafka/issues/695", "repository_url": "https://api.github.com/repos/Parsely/pykafka", "labels_url": "https://api.github.com/repos/Parsely/pykafka/issues/695/labels{/name}", "comments_url": "https://api.github.com/repos/Parsely/pykafka/issues/695/comments", "events_url": "https://api.github.com/repos/Parsely/pykafka/issues/695/events", "html_url": "https://github.com/Parsely/pykafka/issues/695", "id": 238002200, "node_id": "MDU6SXNzdWUyMzgwMDIyMDA=", "number": 695, "title": "Separate protocol tests for broker versions", "user": {"login": "emmett9001", "id": 723615, "node_id": "MDQ6VXNlcjcyMzYxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/723615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emmett9001", "html_url": "https://github.com/emmett9001", "followers_url": "https://api.github.com/users/emmett9001/followers", "following_url": "https://api.github.com/users/emmett9001/following{/other_user}", "gists_url": "https://api.github.com/users/emmett9001/gists{/gist_id}", "starred_url": "https://api.github.com/users/emmett9001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emmett9001/subscriptions", "organizations_url": "https://api.github.com/users/emmett9001/orgs", "repos_url": "https://api.github.com/users/emmett9001/repos", "events_url": "https://api.github.com/users/emmett9001/events{/privacy}", "received_events_url": "https://api.github.com/users/emmett9001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 38059277, "node_id": "MDU6TGFiZWwzODA1OTI3Nw==", "url": "https://api.github.com/repos/Parsely/pykafka/labels/testing", "name": "testing", "color": "fef2c0", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-06-22T23:35:10Z", "updated_at": "2018-02-28T18:47:59Z", "closed_at": "2018-02-28T18:47:59Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Currently the protocol tests leave out some behaviors that differ between protocol versions. All versions of protocol behavior supported by pykafka should also be exercised by the test suite. At this time it looks like it's just `FetchResponse` that has some uncovered functionality.", "performed_via_github_app": null, "score": 1.0}]}