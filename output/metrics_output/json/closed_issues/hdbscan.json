{"total_count": 129, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/396", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/396/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/396/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/396/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/396", "id": 640053952, "node_id": "MDU6SXNzdWU2NDAwNTM5NTI=", "number": 396, "title": "No cosine distance? ", "user": {"login": "alexlenail", "id": 2761597, "node_id": "MDQ6VXNlcjI3NjE1OTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2761597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexlenail", "html_url": "https://github.com/alexlenail", "followers_url": "https://api.github.com/users/alexlenail/followers", "following_url": "https://api.github.com/users/alexlenail/following{/other_user}", "gists_url": "https://api.github.com/users/alexlenail/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexlenail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexlenail/subscriptions", "organizations_url": "https://api.github.com/users/alexlenail/orgs", "repos_url": "https://api.github.com/users/alexlenail/repos", "events_url": "https://api.github.com/users/alexlenail/events{/privacy}", "received_events_url": "https://api.github.com/users/alexlenail/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-17T00:28:45Z", "updated_at": "2020-06-17T19:44:16Z", "closed_at": "2020-06-17T19:44:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/392", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/392/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/392/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/392/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/392", "id": 636492730, "node_id": "MDU6SXNzdWU2MzY0OTI3MzA=", "number": 392, "title": "Problems with approximate_predict ", "user": {"login": "HemakshiShardha", "id": 48849813, "node_id": "MDQ6VXNlcjQ4ODQ5ODEz", "avatar_url": "https://avatars3.githubusercontent.com/u/48849813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HemakshiShardha", "html_url": "https://github.com/HemakshiShardha", "followers_url": "https://api.github.com/users/HemakshiShardha/followers", "following_url": "https://api.github.com/users/HemakshiShardha/following{/other_user}", "gists_url": "https://api.github.com/users/HemakshiShardha/gists{/gist_id}", "starred_url": "https://api.github.com/users/HemakshiShardha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HemakshiShardha/subscriptions", "organizations_url": "https://api.github.com/users/HemakshiShardha/orgs", "repos_url": "https://api.github.com/users/HemakshiShardha/repos", "events_url": "https://api.github.com/users/HemakshiShardha/events{/privacy}", "received_events_url": "https://api.github.com/users/HemakshiShardha/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-10T19:28:11Z", "updated_at": "2020-06-10T19:34:25Z", "closed_at": "2020-06-10T19:33:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have vectors with above 500 dimensions. I want to cluster those. I first did UMAP and then i did HDBSCAN. I looked at the number of clusters and i see that there are 18 distinct clusters. However, when i use approximate_predict function to classify both unseen vectors as well as vectors which were used to train the model, the results include cluster numbers which were not even there when I trained the model (i.e. vectors assigned to clusters that I don't have).\r\n\r\nNote: When running the HDBSCAN, I had the prediction_data set to True.\r\n\r\nCan you please look into this.\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/390", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/390/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/390/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/390/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/390", "id": 633985404, "node_id": "MDU6SXNzdWU2MzM5ODU0MDQ=", "number": 390, "title": "Incorrect line widths and colours when plotting single linkage tree dendrograms with repeated lambda_val", "user": {"login": "GregDemand", "id": 66441553, "node_id": "MDQ6VXNlcjY2NDQxNTUz", "avatar_url": "https://avatars3.githubusercontent.com/u/66441553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GregDemand", "html_url": "https://github.com/GregDemand", "followers_url": "https://api.github.com/users/GregDemand/followers", "following_url": "https://api.github.com/users/GregDemand/following{/other_user}", "gists_url": "https://api.github.com/users/GregDemand/gists{/gist_id}", "starred_url": "https://api.github.com/users/GregDemand/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GregDemand/subscriptions", "organizations_url": "https://api.github.com/users/GregDemand/orgs", "repos_url": "https://api.github.com/users/GregDemand/repos", "events_url": "https://api.github.com/users/GregDemand/events{/privacy}", "received_events_url": "https://api.github.com/users/GregDemand/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-08T01:58:53Z", "updated_at": "2020-06-08T02:46:00Z", "closed_at": "2020-06-08T02:46:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "\r\nThe line widths and hence colours used when plotting single_linkage_tree_ dendrograms are incorrectly calculated when there are repeated lambda_val.\r\n\r\nThis can be observed using the following code snippet\r\n\r\n```python\r\nimport hdbscan\r\nimport numpy as np\r\nX= np.array([\r\n    [1, 1],\r\n    [1, 2],\r\n    [2, 3],\r\n    [2, 4],\r\n    [2, 5],\r\n    [3, 6],\r\n    [3, 7],\r\n    [3, 8],\r\n    [3, 9]\r\n])\r\nclusterer = hdbscan.HDBSCAN(metric='hamming', min_cluster_size=2)\r\nclusterer.fit(X)\r\n\r\nclusterer.single_linkage_tree_.plot()\r\n```\r\n\r\nThe produced dendrogram has incorrect line width and colour for the left top-level branch (distance between 0.5 and 1.0), and the rightmost point in each of the three clusters (distance between 0 and 0.5).\r\n\r\n![incorrect_dendrogram](https://user-images.githubusercontent.com/66441553/83985621-eaf4d300-a907-11ea-8556-fae82603171c.png)\r\n\r\nI've linked a pull request that fixes the issue by traversing the single linkage tree in order to calculate the correct line widths.  This method results in the following dendrogram.\r\n![correct_dendrogram](https://user-images.githubusercontent.com/66441553/83985618-ea5c3c80-a907-11ea-9a1f-c2ee9b3896c7.png)\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/389", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/389/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/389/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/389/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/389", "id": 627966635, "node_id": "MDU6SXNzdWU2Mjc5NjY2MzU=", "number": 389, "title": "ERROR: pip Installation Error, Could not build dev version of scikit-learn from source", "user": {"login": "trentpark8800", "id": 47793696, "node_id": "MDQ6VXNlcjQ3NzkzNjk2", "avatar_url": "https://avatars3.githubusercontent.com/u/47793696?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trentpark8800", "html_url": "https://github.com/trentpark8800", "followers_url": "https://api.github.com/users/trentpark8800/followers", "following_url": "https://api.github.com/users/trentpark8800/following{/other_user}", "gists_url": "https://api.github.com/users/trentpark8800/gists{/gist_id}", "starred_url": "https://api.github.com/users/trentpark8800/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trentpark8800/subscriptions", "organizations_url": "https://api.github.com/users/trentpark8800/orgs", "repos_url": "https://api.github.com/users/trentpark8800/repos", "events_url": "https://api.github.com/users/trentpark8800/events{/privacy}", "received_events_url": "https://api.github.com/users/trentpark8800/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-31T13:44:27Z", "updated_at": "2020-06-01T13:24:51Z", "closed_at": "2020-06-01T13:24:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "* Python: 3.8.1\r\n* pip: 20.1.1\r\n* OS: Windows 10 Enterprise (2019)\r\n* __Note:__ `sklearn-env` is the virtual environment I created to work on scikit.\r\n\r\nI have just started out with contributing to scikit and am having trouble building the package from source.\r\nI have followed the steps in the [Contributor's Guide](https://scikit-learn.org/stable/developers/advanced_installation.html#compiler-windows), namely I have the C++ compiler installed:\r\n```\r\n$ . \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Auxiliary\\Build\\vcvarsall\" x64\r\n**********************************************************************\r\n** Visual Studio 2019 Developer Command Prompt v16.6.0\r\n** Copyright (c) 2020 Microsoft Corporation\r\n**********************************************************************\r\n[vcvarsall.bat] Environment initialized for: 'x64'\r\n```\r\nHowever when I try to run the command `pip install --verbose --no-build-isolation --editable .` I get the following output:\r\n\r\n```\r\nERROR: Command errored out with exit status 1: 'c:\\python\\.virtualenvs\\sklearn-env\\scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps Check the logs for full command output.\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 188, in _main\r\n    status = self.run(options, args)\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 185, in wrapper\r\n    return func(self, options, args)\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 398, in run\r\n    installed = install_given_reqs(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 67, in install_given_reqs\r\n    requirement.install(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 780, in install\r\n    install_editable_legacy(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\operations\\install\\editable_legacy.py\", line 49, in install_editable\r\n    call_subprocess(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\utils\\subprocess.py\", line 241, in call_subprocess\r\n    raise InstallationError(exc_msg)\r\npip._internal.exceptions.InstallationError: Command errored out with exit status 1: 'c:\\python\\.virtualenvs\\sklearn-env\\scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps Check the logs for full command output.\r\nRemoved build tracker: 'C:\\\\Users\\\\<my-user>\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-cljegruo'\r\n```\r\nThe full output is as follows:\r\n```\r\nNon-user install because user site-packages disabled\r\nCreated temporary directory: C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ycnrlrxz\r\nCreated temporary directory: C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-req-tracker-cljegruo\r\nInitialized build tracking at C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-req-tracker-cljegruo\r\nCreated build tracker: C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-req-tracker-cljegruo\r\nEntered build tracker: C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-req-tracker-cljegruo\r\nCreated temporary directory: C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-install-l7zki36n\r\nObtaining file:///C:/Users/<my-user>/Projects/dev_projects/scikit-learn\r\n  Added file:///C:/Users/<my-user>/Projects/dev_projects/scikit-learn to build tracker 'C:\\\\Users\\\\<my-user>\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-cljegruo'\r\n\r\n    Created temporary directory: C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\r\n    Running command 'c:\\python\\.virtualenvs\\sklearn-env\\scripts\\python.exe' 'c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' prepare_metadata_for_build_wheel 'C:\\Users\\TRENT~1.PAR\\AppData\\Local\\Temp\\tmpc6n9inca'\r\n    running dist_info\r\n    creating C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\r\n    writing C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\\PKG-INFO\r\n    writing dependency_links to C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\\dependency_links.txt\r\n    writing requirements to C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\\requires.txt\r\n    writing top-level names to C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\\top_level.txt\r\n    writing manifest file 'C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\\SOURCES.txt'\r\n    reading manifest file 'C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\\SOURCES.txt'\r\n    reading manifest template 'MANIFEST.in'\r\n    writing manifest file 'C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.egg-info\\SOURCES.txt'\r\n    creating 'C:\\Users\\<my-user>\\AppData\\Local\\Temp\\pip-modern-metadata-9j651yda\\scikit_learn.dist-info'\r\n    adding license file \"COPYING\" (matched pattern \"COPYING*\")\r\n    Partial import of sklearn during the build process.\r\n    Preparing wheel metadata ... done\r\n  Source in c:\\users\\<my-user>\\projects\\dev_projects\\scikit-learn has version 0.24.dev0, which satisfies requirement scikit-learn==0.24.dev0 from file:///C:/Users/<my-user>/Projects/dev_projects/scikit-learn\r\n  Removed scikit-learn==0.24.dev0 from file:///C:/Users/<my-user>/Projects/dev_projects/scikit-learn from build tracker 'C:\\\\Users\\\\<my-user>\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-cljegruo'\r\nRequirement already satisfied: scipy>=0.19.1 in c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages (from scikit-learn==0.24.dev0) (1.4.1)\r\nRequirement already satisfied: joblib>=0.11 in c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages (from scikit-learn==0.24.dev0) (0.15.1)\r\nRequirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages (from scikit-learn==0.24.dev0) (2.1.0)\r\nRequirement already satisfied: numpy>=1.13.3 in c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages (from scikit-learn==0.24.dev0) (1.18.4)\r\nInstalling collected packages: scikit-learn\r\n  Running setup.py develop for scikit-learn\r\n    Running command 'c:\\python\\.virtualenvs\\sklearn-env\\scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps\r\n    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\r\n    cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT /Tctest_program.c /Foobjects\\test_program.obj\r\n    Partial import of sklearn during the build process.\r\n    Traceback (most recent call last):\r\n      File \"C:\\Python\\Python38\\lib\\distutils\\_msvccompiler.py\", line 438, in compile\r\n        self.spawn(args)\r\n      File \"C:\\Python\\Python38\\lib\\distutils\\_msvccompiler.py\", line 557, in spawn\r\n        return super().spawn(cmd)\r\n      File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\numpy\\distutils\\ccompiler.py\", line 92, in <lambda>\r\n        m = lambda self, *args, **kw: func(self, *args, **kw)\r\n      File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\numpy\\distutils\\ccompiler.py\", line 175, in CCompiler_spawn\r\n        raise DistutilsExecError('Command \"%s\" failed with exit status %d%s' %\r\n    distutils.errors.DistutilsExecError: Command \"cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT /Tctest_program.c /Foobjects\\test_program.obj\" failed with exit status 127\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"C:\\Users\\<my-user>\\Projects\\dev_projects\\scikit-learn\\setup.py\", line 304, in <module>\r\n        setup_package()\r\n      File \"C:\\Users\\<my-user>\\Projects\\dev_projects\\scikit-learn\\setup.py\", line 300, in setup_package\r\n        setup(**metadata)\r\n      File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\numpy\\distutils\\core.py\", line 137, in setup\r\n        config = configuration()\r\n      File \"C:\\Users\\<my-user>\\Projects\\dev_projects\\scikit-learn\\setup.py\", line 183, in configuration\r\n        config.add_subpackage('sklearn')\r\n      File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 1033, in add_subpackage\r\n        config_list = self.get_subpackage(subpackage_name, subpackage_path,\r\n      File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 999, in get_subpackage\r\n        config = self._get_configuration_from_setup_py(\r\n      File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\numpy\\distutils\\misc_util.py\", line 941, in _get_configuration_from_setup_py\r\n        config = setup_module.configuration(*args)\r\n      File \"sklearn\\setup.py\", line 83, in configuration\r\n        cythonize_extensions(top_path, config)\r\n      File \"C:\\Users\\<my-user>\\Projects\\dev_projects\\scikit-learn\\sklearn\\_build_utils\\__init__.py\", line 50, in cythonize_extensions\r\n        basic_check_build()\r\n      File \"C:\\Users\\<my-user>\\Projects\\dev_projects\\scikit-learn\\sklearn\\_build_utils\\pre_build_helpers.py\", line 70, in basic_check_build\r\n        compile_test_program(code)\r\n      File \"C:\\Users\\<my-user>\\Projects\\dev_projects\\scikit-learn\\sklearn\\_build_utils\\pre_build_helpers.py\", line 39, in compile_test_program\r\n        ccompiler.compile(['test_program.c'], output_dir='objects',\r\n      File \"C:\\Python\\Python38\\lib\\distutils\\_msvccompiler.py\", line 440, in compile\r\n        raise CompileError(msg)\r\n    distutils.errors.CompileError: Command \"cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT /Tctest_program.c /Foobjects\\test_program.obj\" failed with exit status 127\r\nERROR: Command errored out with exit status 1: 'c:\\python\\.virtualenvs\\sklearn-env\\scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps Check the logs for full command output.\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 188, in _main\r\n    status = self.run(options, args)\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 185, in wrapper\r\n    return func(self, options, args)\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 398, in run\r\n    installed = install_given_reqs(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\req\\__init__.py\", line 67, in install_given_reqs\r\n    requirement.install(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 780, in install\r\n    install_editable_legacy(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\operations\\install\\editable_legacy.py\", line 49, in install_editable\r\n    call_subprocess(\r\n  File \"c:\\python\\.virtualenvs\\sklearn-env\\lib\\site-packages\\pip\\_internal\\utils\\subprocess.py\", line 241, in call_subprocess\r\n    raise InstallationError(exc_msg)\r\npip._internal.exceptions.InstallationError: Command errored out with exit status 1: 'c:\\python\\.virtualenvs\\sklearn-env\\scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\<my-user>\\\\Projects\\\\dev_projects\\\\scikit-learn\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps Check the logs for full command output.\r\nRemoved build tracker: 'C:\\\\Users\\\\<my-user>\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-cljegruo'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/383", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/383/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/383/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/383/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/383", "id": 626801048, "node_id": "MDU6SXNzdWU2MjY4MDEwNDg=", "number": 383, "title": "min_cluster_selection_eps not always respected", "user": {"login": "neontty", "id": 55955360, "node_id": "MDQ6VXNlcjU1OTU1MzYw", "avatar_url": "https://avatars3.githubusercontent.com/u/55955360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neontty", "html_url": "https://github.com/neontty", "followers_url": "https://api.github.com/users/neontty/followers", "following_url": "https://api.github.com/users/neontty/following{/other_user}", "gists_url": "https://api.github.com/users/neontty/gists{/gist_id}", "starred_url": "https://api.github.com/users/neontty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neontty/subscriptions", "organizations_url": "https://api.github.com/users/neontty/orgs", "repos_url": "https://api.github.com/users/neontty/repos", "events_url": "https://api.github.com/users/neontty/events{/privacy}", "received_events_url": "https://api.github.com/users/neontty/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-28T20:41:49Z", "updated_at": "2020-05-29T04:32:29Z", "closed_at": "2020-05-29T04:32:28Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "this might be related to issue #370  . \r\n\r\nplease consider the following scenario where I try to cluster a simple gaussian:\r\n\r\n```\r\nfrom hdbscan import HDBSCAN\r\nimport numpy as np\r\n\r\nclusterer = HDBSCAN(cluster_selection_epsilon=100, min_cluster_size=5, min_samples=10, metric='euclidean', leaf_size=50, allow_single_cluster=True, cluster_selection_method='eom')\r\nnp.random.seed(0)\r\ndata = np.random.normal(100, 2.5, 1024)\r\nlabels = clusterer.fit_predict(data.reshape(-1,1))\r\nprint(np.unique(labels, return_counts=True))\r\n```\r\n\r\nNote: with np.random.seed(0) you always get the interesting example case where we get three clusters (noise and two labels, which is bad) when I would expect two (noise and one label). Am I correct in understanding that any clusters in my single linkage tree graph below my cluster_selection_epsilon should be merged into one, right? No matter what I select for cluster_selection_epsilon in this scenario, I will always get 3 clusters (-1, 0, 1). \r\n\r\n\r\nI am going to try to work on this since I need to test a fix for #370 also, but I think I might need help on the algorithms side. \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/380", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/380/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/380/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/380/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/380", "id": 621081351, "node_id": "MDU6SXNzdWU2MjEwODEzNTE=", "number": 380, "title": "Build fails with \" hdbscan\\dist_metrics.c(11558): error C2065: 'M_PI': undeclared identifier\"", "user": {"login": "sk1p", "id": 5778, "node_id": "MDQ6VXNlcjU3Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5778?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sk1p", "html_url": "https://github.com/sk1p", "followers_url": "https://api.github.com/users/sk1p/followers", "following_url": "https://api.github.com/users/sk1p/following{/other_user}", "gists_url": "https://api.github.com/users/sk1p/gists{/gist_id}", "starred_url": "https://api.github.com/users/sk1p/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sk1p/subscriptions", "organizations_url": "https://api.github.com/users/sk1p/orgs", "repos_url": "https://api.github.com/users/sk1p/repos", "events_url": "https://api.github.com/users/sk1p/events{/privacy}", "received_events_url": "https://api.github.com/users/sk1p/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-19T15:37:26Z", "updated_at": "2020-05-22T13:48:40Z", "closed_at": "2020-05-22T13:48:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "See an example detailed log here:\r\n\r\nhttps://ci.appveyor.com/project/sk1p/libertem/builds/32981335/job/6yet7c9kntx1381r\r\n\r\nI'm not sure why this is happening - possibly the build environment changed on AppVeyor?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/376", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/376/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/376/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/376/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/376", "id": 616738418, "node_id": "MDU6SXNzdWU2MTY3Mzg0MTg=", "number": 376, "title": "integer overflow", "user": {"login": "MichaelMonashev", "id": 6323434, "node_id": "MDQ6VXNlcjYzMjM0MzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/6323434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MichaelMonashev", "html_url": "https://github.com/MichaelMonashev", "followers_url": "https://api.github.com/users/MichaelMonashev/followers", "following_url": "https://api.github.com/users/MichaelMonashev/following{/other_user}", "gists_url": "https://api.github.com/users/MichaelMonashev/gists{/gist_id}", "starred_url": "https://api.github.com/users/MichaelMonashev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MichaelMonashev/subscriptions", "organizations_url": "https://api.github.com/users/MichaelMonashev/orgs", "repos_url": "https://api.github.com/users/MichaelMonashev/repos", "events_url": "https://api.github.com/users/MichaelMonashev/events{/privacy}", "received_events_url": "https://api.github.com/users/MichaelMonashev/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-12T15:13:39Z", "updated_at": "2020-05-13T06:36:24Z", "closed_at": "2020-05-13T06:36:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Code:\r\n```\r\n    clusterer = hdbscan.HDBSCAN(\r\n        core_dist_n_jobs=os.cpu_count(),\r\n        min_cluster_size = 2000,\r\n    )\r\n    clusterer.fit(embedding)\r\n```\r\n`embedding` has shape (314496, 20).\r\n\r\nError:\r\n```\r\njoblib.externals.loky.process_executor._RemoteTraceback:\r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py\", line 344, in _sendback_result\r\n    exception=exception))\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/joblib/externals/loky/backend/queues.py\", line 240, in put\r\n    self._writer.send_bytes(obj)\r\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\r\n    self._send_bytes(m[offset:offset + size])\r\n  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 393, in _send_bytes\r\n    header = struct.pack(\"!i\", n)\r\nstruct.error: 'i' format requires -2147483648 <= number <= 2147483647\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"cluster_images.py\", line 158, in <module>\r\n    _main(parser.parse_args())\r\n  File \"cluster_images.py\", line 115, in _main\r\n    clusterer.fit(embedding)\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/hdbscan/hdbscan_.py\", line 919, in fit\r\n    self._min_spanning_tree) = hdbscan(X, **kwargs)\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/hdbscan/hdbscan_.py\", line 615, in hdbscan\r\n    core_dist_n_jobs, **kwargs)\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/joblib/memory.py\", line 355, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/hdbscan/hdbscan_.py\", line 278, in _hdbscan_boruvka_kdtree\r\n    n_jobs=core_dist_n_jobs, **kwargs)\r\n  File \"hdbscan/_hdbscan_boruvka.pyx\", line 375, in hdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm.__init__\r\n  File \"hdbscan/_hdbscan_boruvka.pyx\", line 411, in hdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm._compute_bounds\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/joblib/parallel.py\", line 1017, in __call__\r\n    self.retrieve()\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/joblib/parallel.py\", line 909, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"/home/xxx/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 562, in wrap_future_result\r\n    return future.result(timeout=timeout)\r\n  File \"/usr/lib/python3.6/concurrent/futures/_base.py\", line 432, in result\r\n    return self.__get_result()\r\n  File \"/usr/lib/python3.6/concurrent/futures/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\nstruct.error: 'i' format requires -2147483648 <= number <= 2147483647\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/370", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/370/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/370/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/370/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/370", "id": 603611852, "node_id": "MDU6SXNzdWU2MDM2MTE4NTI=", "number": 370, "title": "Crash when allow_single_cluster used with cluster_selection_epsilon", "user": {"login": "danielzgtg", "id": 25646384, "node_id": "MDQ6VXNlcjI1NjQ2Mzg0", "avatar_url": "https://avatars1.githubusercontent.com/u/25646384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielzgtg", "html_url": "https://github.com/danielzgtg", "followers_url": "https://api.github.com/users/danielzgtg/followers", "following_url": "https://api.github.com/users/danielzgtg/following{/other_user}", "gists_url": "https://api.github.com/users/danielzgtg/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielzgtg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielzgtg/subscriptions", "organizations_url": "https://api.github.com/users/danielzgtg/orgs", "repos_url": "https://api.github.com/users/danielzgtg/repos", "events_url": "https://api.github.com/users/danielzgtg/events{/privacy}", "received_events_url": "https://api.github.com/users/danielzgtg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2020-04-21T00:05:05Z", "updated_at": "2020-07-07T04:04:24Z", "closed_at": "2020-05-30T00:07:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I use `cluster_selection_epsilon=x` where x > 0 and `allow_single_cluster=True` together, HDBSCAN crashes.\r\n\r\nI am using those two options together to try and get the `no_structure` [toy dataset](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) (bottom row, square) clustered properly. I want the square to be completely blue like how DBSCAN does it. When I only use `cluster_selection_epsilon`, multiple clusters appear in that square. When I only use `allow_single_cluster=True`, part of that square is grey. I think I can only get the desired result using both of those arguments, but HDBSCAN crashes when I do that.\r\n\r\n# Code\r\n\r\n```python\r\nimport numpy as np\r\nimport hdbscan\r\n\r\nif __name__ == '__main__':\r\n    no_structure = np.random.rand(1500, 2)\r\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=15, cluster_selection_epsilon=3, allow_single_cluster=True)\r\n    clusterer.fit(no_structure)\r\n```\r\n\r\n# Expected behavior\r\n\r\nHDBSCAN to cluster the data without crashing. Preferably, painting all points in the square blue as described.\r\n\r\n# Actual behavior\r\n\r\nHDBSCAN crashes with the following traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/home/PycharmProjects/sandbox/crash_example.py\", line 7, in <module>\r\n    clusterer.fit(no_structure)\r\n  File \"/home/home/PycharmProjects/sandbox/venv/lib/python3.8/site-packages/hdbscan/hdbscan_.py\", line 919, in fit\r\n    self._min_spanning_tree) = hdbscan(X, **kwargs)\r\n  File \"/home/home/PycharmProjects/sandbox/venv/lib/python3.8/site-packages/hdbscan/hdbscan_.py\", line 632, in hdbscan\r\n    return _tree_to_labels(X,\r\n  File \"/home/home/PycharmProjects/sandbox/venv/lib/python3.8/site-packages/hdbscan/hdbscan_.py\", line 59, in _tree_to_labels\r\n    labels, probabilities, stabilities = get_clusters(condensed_tree,\r\n  File \"hdbscan/_hdbscan_tree.pyx\", line 645, in hdbscan._hdbscan_tree.get_clusters\r\n  File \"hdbscan/_hdbscan_tree.pyx\", line 733, in hdbscan._hdbscan_tree.get_clusters\r\n  File \"hdbscan/_hdbscan_tree.pyx\", line 631, in hdbscan._hdbscan_tree.epsilon_search\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/353", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/353/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/353/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/353/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/353", "id": 582692192, "node_id": "MDU6SXNzdWU1ODI2OTIxOTI=", "number": 353, "title": "Unexpected results from weighted_cluster_medoid", "user": {"login": "gclen", "id": 4321675, "node_id": "MDQ6VXNlcjQzMjE2NzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4321675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gclen", "html_url": "https://github.com/gclen", "followers_url": "https://api.github.com/users/gclen/followers", "following_url": "https://api.github.com/users/gclen/following{/other_user}", "gists_url": "https://api.github.com/users/gclen/gists{/gist_id}", "starred_url": "https://api.github.com/users/gclen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gclen/subscriptions", "organizations_url": "https://api.github.com/users/gclen/orgs", "repos_url": "https://api.github.com/users/gclen/repos", "events_url": "https://api.github.com/users/gclen/events{/privacy}", "received_events_url": "https://api.github.com/users/gclen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-17T01:31:00Z", "updated_at": "2020-03-17T19:54:34Z", "closed_at": "2020-03-17T19:54:34Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Plotting some sample data and looking at the centroids I see\r\n![weighted_centroids](https://user-images.githubusercontent.com/4321675/76813006-fd5de580-67cc-11ea-935e-7433ae967b55.png)\r\n\r\nBut when I use weighted_cluster_medoid the \"representative point\" is always near the outside of the cluster\r\n\r\n![weighted_medoids](https://user-images.githubusercontent.com/4321675/76813039-1b2b4a80-67cd-11ea-907b-bb09062df058.png)\r\n\r\nI think this is due to the use of argmax (instead of argmin) in https://github.com/scikit-learn-contrib/hdbscan/blob/master/hdbscan/hdbscan_.py#L1027\r\n\r\nChanging that gives the result you would expect\r\n\r\n![weighted_medoids_fixed](https://user-images.githubusercontent.com/4321675/76813147-604f7c80-67cd-11ea-92e1-175a0680b272.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/347", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/347/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/347/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/347/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/347", "id": 569151130, "node_id": "MDU6SXNzdWU1NjkxNTExMzA=", "number": 347, "title": "Want to run HDBSCAN on multiple machines", "user": {"login": "olivatooo", "id": 15200079, "node_id": "MDQ6VXNlcjE1MjAwMDc5", "avatar_url": "https://avatars1.githubusercontent.com/u/15200079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olivatooo", "html_url": "https://github.com/olivatooo", "followers_url": "https://api.github.com/users/olivatooo/followers", "following_url": "https://api.github.com/users/olivatooo/following{/other_user}", "gists_url": "https://api.github.com/users/olivatooo/gists{/gist_id}", "starred_url": "https://api.github.com/users/olivatooo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olivatooo/subscriptions", "organizations_url": "https://api.github.com/users/olivatooo/orgs", "repos_url": "https://api.github.com/users/olivatooo/repos", "events_url": "https://api.github.com/users/olivatooo/events{/privacy}", "received_events_url": "https://api.github.com/users/olivatooo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-02-21T19:49:27Z", "updated_at": "2020-02-27T16:59:45Z", "closed_at": "2020-02-27T16:59:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Anyone have any idea of how to achieve that?\r\nThere are any example of code/article that does that?\r\nWhat parts can/cannot be parallelized?\r\n\r\nI really need to execute HDBSCAN in a more efficient way, and I can contribute to achieve that\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/343", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/343/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/343/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/343/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/343", "id": 560272310, "node_id": "MDU6SXNzdWU1NjAyNzIzMTA=", "number": 343, "title": "`pip install hdbscan` fails", "user": {"login": "richard-kunert", "id": 52922078, "node_id": "MDQ6VXNlcjUyOTIyMDc4", "avatar_url": "https://avatars1.githubusercontent.com/u/52922078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/richard-kunert", "html_url": "https://github.com/richard-kunert", "followers_url": "https://api.github.com/users/richard-kunert/followers", "following_url": "https://api.github.com/users/richard-kunert/following{/other_user}", "gists_url": "https://api.github.com/users/richard-kunert/gists{/gist_id}", "starred_url": "https://api.github.com/users/richard-kunert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/richard-kunert/subscriptions", "organizations_url": "https://api.github.com/users/richard-kunert/orgs", "repos_url": "https://api.github.com/users/richard-kunert/repos", "events_url": "https://api.github.com/users/richard-kunert/events{/privacy}", "received_events_url": "https://api.github.com/users/richard-kunert/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-05T10:19:35Z", "updated_at": "2020-02-05T10:28:03Z", "closed_at": "2020-02-05T10:28:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "When installing `hdbscan` through pip I encountered the following error:\r\n\r\nI use the following set-up:\r\n* Python 3.6.8\r\n* pip 20.0.2\r\n* macOS 10.15.3\r\n\r\n```root@1234567:/app/data-science/ pip install hdbscan\r\nCollecting hdbscan\r\n  Using cached hdbscan-0.8.24.tar.gz (4.4 MB)\r\n  Installing build dependencies ... error\r\n  ERROR: Command errored out with exit status 1:\r\n   command: /usr/bin/python3 /usr/local/lib/python3.6/dist-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-d0g5e19g/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel cython numpy\r\n       cwd: None\r\n  Complete output (14 lines):\r\n  Traceback (most recent call last):\r\n    File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n      \"__main__\", mod_spec)\r\n    File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n      exec(code, run_globals)\r\n    File \"/usr/local/lib/python3.6/dist-packages/pip/__main__.py\", line 16, in <module>\r\n      from pip._internal.cli.main import main as _main  # isort:skip # noqa\r\n    File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/main.py\", line 5, in <module>\r\n      import locale\r\n    File \"/usr/lib/python3.6/locale.py\", line 16, in <module>\r\n      import re\r\n    File \"/usr/lib/python3.6/re.py\", line 142, in <module>\r\n      class RegexFlag(enum.IntFlag):\r\n  AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ----------------------------------------\r\nERROR: Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.6/dist-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-d0g5e19g/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel cython numpy Check the logs for full command output.```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/340", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/340/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/340/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/340/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/340", "id": 545623383, "node_id": "MDU6SXNzdWU1NDU2MjMzODM=", "number": 340, "title": "Not able to pass \"cluster_selection_epsilon\" argument", "user": {"login": "nish10", "id": 19671929, "node_id": "MDQ6VXNlcjE5NjcxOTI5", "avatar_url": "https://avatars3.githubusercontent.com/u/19671929?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nish10", "html_url": "https://github.com/nish10", "followers_url": "https://api.github.com/users/nish10/followers", "following_url": "https://api.github.com/users/nish10/following{/other_user}", "gists_url": "https://api.github.com/users/nish10/gists{/gist_id}", "starred_url": "https://api.github.com/users/nish10/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nish10/subscriptions", "organizations_url": "https://api.github.com/users/nish10/orgs", "repos_url": "https://api.github.com/users/nish10/repos", "events_url": "https://api.github.com/users/nish10/events{/privacy}", "received_events_url": "https://api.github.com/users/nish10/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-06T09:19:26Z", "updated_at": "2020-08-11T12:00:08Z", "closed_at": "2020-01-10T14:22:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Getting following error \r\n\r\nTypeError: __init__() got an unexpected keyword argument 'cluster_selection_epsilon'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/328", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/328/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/328/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/328/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/328", "id": 517330481, "node_id": "MDU6SXNzdWU1MTczMzA0ODE=", "number": 328, "title": "Merge clusters based on condensed tree representation", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-04T18:38:29Z", "updated_at": "2019-12-23T21:50:04Z", "closed_at": "2019-12-23T21:50:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nCan we use the condensed tree representation that is available from `hdbscan` and the information related to `parent` and `child` cluster labels to merge clusters. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/327", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/327/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/327/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/327/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/327", "id": 505942707, "node_id": "MDU6SXNzdWU1MDU5NDI3MDc=", "number": 327, "title": "Cannot install", "user": {"login": "r0f1", "id": 7324891, "node_id": "MDQ6VXNlcjczMjQ4OTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7324891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/r0f1", "html_url": "https://github.com/r0f1", "followers_url": "https://api.github.com/users/r0f1/followers", "following_url": "https://api.github.com/users/r0f1/following{/other_user}", "gists_url": "https://api.github.com/users/r0f1/gists{/gist_id}", "starred_url": "https://api.github.com/users/r0f1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/r0f1/subscriptions", "organizations_url": "https://api.github.com/users/r0f1/orgs", "repos_url": "https://api.github.com/users/r0f1/repos", "events_url": "https://api.github.com/users/r0f1/events{/privacy}", "received_events_url": "https://api.github.com/users/r0f1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-11T16:18:05Z", "updated_at": "2019-11-05T19:47:40Z", "closed_at": "2019-10-14T11:18:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I get the following error message, when I try installing the package:\r\n\r\n```\r\n  ERROR: Command errored out with exit status 1: \r\ncommand: /projects/anaconda3/bin/python /projects/anaconda3/lib/\r\npython3.7/site-packages/pip install --ignore-installed --no-user --prefix /\r\ntmp/pip-build-env-93se382n/overlay --no-warn-script-location --no-binary \r\n:none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel\r\ncython numpy cwd: None                                               \r\n\r\nComplete output (14 lines):\r\nTraceback (most recent call last):                                         \r\nFile \"/projects/anaconda3/lib/python3.7/runpy.py\", line 193, in\r\n _run_module_as_main \"__main__\", mod_spec) \r\n File \"/projects/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code exec(code, run_globals) \r\n File \"/projects/anaconda3/lib/python3.7/site-packages/pip/__main__.py\", line 16, in <module> from pip._internal import main as _main  # isort:skip #noqa                                                          \r\n File \"/projects/anaconda3/lib/python3.7/site-packages/pip/_internal/__init__.py\", line 4, in <module>                     import locale \r\n File \"/projects/anaconda3/lib/python3.7/locale.py\", line 16, in <module> import re \r\n File \"/projects/anaconda3/lib/python3.7/re.py\", line 143, in <module> class RegexFlag(enum.IntFlag): AttributeError: module 'enum' has no attribute 'IntFlag'\r\n  ---------------------------------------- ERROR: Command errored out with\r\n  exit status 1: /projects/anaconda3/bin/python /projects/anaconda3/lib/\r\n  python3.7/site-packages/pip install --ignore-installed --no-user \r\n  --prefix /tmp/pip-build-env-93se382n/overlay --no-warn-script-location\r\n   --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- \r\n   setuptools wheel cython numpy Check the logs for full command output.\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/324", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/324/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/324/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/324/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/324", "id": 495983347, "node_id": "MDU6SXNzdWU0OTU5ODMzNDc=", "number": 324, "title": "Non existent parameter \"cluster_selection_method\"", "user": {"login": "jorgeCollinet", "id": 7537430, "node_id": "MDQ6VXNlcjc1Mzc0MzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/7537430?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jorgeCollinet", "html_url": "https://github.com/jorgeCollinet", "followers_url": "https://api.github.com/users/jorgeCollinet/followers", "following_url": "https://api.github.com/users/jorgeCollinet/following{/other_user}", "gists_url": "https://api.github.com/users/jorgeCollinet/gists{/gist_id}", "starred_url": "https://api.github.com/users/jorgeCollinet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jorgeCollinet/subscriptions", "organizations_url": "https://api.github.com/users/jorgeCollinet/orgs", "repos_url": "https://api.github.com/users/jorgeCollinet/repos", "events_url": "https://api.github.com/users/jorgeCollinet/events{/privacy}", "received_events_url": "https://api.github.com/users/jorgeCollinet/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-19T19:23:39Z", "updated_at": "2019-09-19T19:46:13Z", "closed_at": "2019-09-19T19:46:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am reading the doc : https://hdbscan.readthedocs.io/en/latest/parameter_selection.html#leaf-clustering\r\n\r\nand I am trying to use \"cluster_selection_method\"\r\nbut when I do:\r\n```\r\nimport hdbscan\r\nclusterer = hdbscan.HDBSCAN(min_cluster_size=5, cluster_selection_method='leaf')\r\nclusterer.fit_predict(vectors)\r\n```\r\n\r\nIt raise an error:\r\nTypeError: __init__() got an unexpected keyword argument 'cluster_selection_method'\r\n\r\nis cluster_selection_method a valid parameter?\r\nmy hdbscan version is 0.8.22", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/318", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/318/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/318/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/318/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/318", "id": 486715837, "node_id": "MDU6SXNzdWU0ODY3MTU4Mzc=", "number": 318, "title": "pip install fail--CANNOT Building wheels for Hdbscan Windows ", "user": {"login": "mengxibai", "id": 48046007, "node_id": "MDQ6VXNlcjQ4MDQ2MDA3", "avatar_url": "https://avatars1.githubusercontent.com/u/48046007?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mengxibai", "html_url": "https://github.com/mengxibai", "followers_url": "https://api.github.com/users/mengxibai/followers", "following_url": "https://api.github.com/users/mengxibai/following{/other_user}", "gists_url": "https://api.github.com/users/mengxibai/gists{/gist_id}", "starred_url": "https://api.github.com/users/mengxibai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mengxibai/subscriptions", "organizations_url": "https://api.github.com/users/mengxibai/orgs", "repos_url": "https://api.github.com/users/mengxibai/repos", "events_url": "https://api.github.com/users/mengxibai/events{/privacy}", "received_events_url": "https://api.github.com/users/mengxibai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-08-29T03:29:48Z", "updated_at": "2019-11-19T13:31:16Z", "closed_at": "2019-08-29T10:31:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I had visual studio community 2019 on windows and setted environment variable for cl.exe. But there is still an exist error 2 for cl.exe. \r\nAlready had cython.\r\nPlease help. \r\n\r\n\r\n  Building wheel for HDBSCAN (PEP 517) ... error\r\n   ERROR: Command errored out with exit status 1:\r\n   command: 'c:\\users\\NAME\\desktop\\python 3.7.2\\python.exe' 'c:\\users\\NAME\\desktop\\python 3.7.2\\lib\\site-packages\\pip\\_vendor\\pep517\\_in_process.py' build_wheel 'C:\\Users\\NAME\\AppData\\Local\\Temp\\tmpa8byq2e5'\r\n       cwd: C:\\Users\\NAME\\AppData\\Local\\Temp\\pip-install-z0y595n7\\HDBSCAN\r\n  Complete output (28 lines):\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build\\lib.win-amd64-3.7\r\n  creating build\\lib.win-amd64-3.7\\hdbscan\r\n  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-3.7\\hdbscan\r\n  copying hdbscan\\plots.py -> build\\lib.win-amd64-3.7\\hdbscan\r\n  copying hdbscan\\prediction.py -> build\\lib.win-amd64-3.7\\hdbscan\r\n  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-3.7\\hdbscan\r\n  copying hdbscan\\validity.py -> build\\lib.win-amd64-3.7\\hdbscan\r\n  copying hdbscan\\__init__.py -> build\\lib.win-amd64-3.7\\hdbscan\r\n  creating build\\lib.win-amd64-3.7\\hdbscan\\tests\r\n  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-3.7\\hdbscan\\tests\r\n  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-3.7\\hdbscan\\tests\r\n  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-3.7\\hdbscan\\tests\r\n  running build_ext\r\n  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\r\n  building 'hdbscan._hdbscan_tree' extension\r\n  creating build\\temp.win-amd64-3.7\r\n  creating build\\temp.win-amd64-3.7\\Release\r\n  creating build\\temp.win-amd64-3.7\\Release\\hdbscan\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\bin\\HostX86\\x64\\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MT \"-Ic:\\users\\NAME\\desktop\\python 3.7.2\\include\" \"-Ic:\\users\\mengxi bai\\desktop\\python 3.7.2\\include\" -IC:\\Users\\NAME\\AppData\\Local\\Temp\\pip-build-env-b1hyxd4u\\overlay\\Lib\\site-packages\\numpy\\core\\include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.22.27905\\include\" /Tchdbscan\\_hdbscan_tree.c /Fobuild\\temp.win-amd64-3.7\\Release\\hdbscan\\_hdbscan_tree.obj\r\n  _hdbscan_tree.c\r\n  c:\\users\\NAME\\desktop\\python 3.7.2\\include\\pyconfig.h(59): fatal error C1083: Cannot open include file: 'io.h': No such file or directory\r\n  C:\\Users\\NAME\\AppData\\Local\\Temp\\pip-build-env-b1hyxd4u\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\NAME\\AppData\\Local\\Temp\\pip-install-z0y595n7\\HDBSCAN\\hdbscan\\_hdbscan_tree.pyx\r\n    tree = Parsing.p_module(s, pxd, full_module_name)\r\n  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\Community\\\\VC\\\\Tools\\\\MSVC\\\\14.22.27905\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2\r\n  ----------------------------------------\r\n  ERROR: Failed building wheel for HDBSCAN\r\n  Running setup.py clean for HDBSCAN\r\nFailed to build HDBSCAN\r\nERROR: Could not build wheels for HDBSCAN which use PEP 517 and cannot be installed directly", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/316", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/316/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/316/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/316/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/316", "id": 480258839, "node_id": "MDU6SXNzdWU0ODAyNTg4Mzk=", "number": 316, "title": "What is a good way for hyperparameters tuning?", "user": {"login": "tedapham", "id": 24615789, "node_id": "MDQ6VXNlcjI0NjE1Nzg5", "avatar_url": "https://avatars3.githubusercontent.com/u/24615789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tedapham", "html_url": "https://github.com/tedapham", "followers_url": "https://api.github.com/users/tedapham/followers", "following_url": "https://api.github.com/users/tedapham/following{/other_user}", "gists_url": "https://api.github.com/users/tedapham/gists{/gist_id}", "starred_url": "https://api.github.com/users/tedapham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tedapham/subscriptions", "organizations_url": "https://api.github.com/users/tedapham/orgs", "repos_url": "https://api.github.com/users/tedapham/repos", "events_url": "https://api.github.com/users/tedapham/events{/privacy}", "received_events_url": "https://api.github.com/users/tedapham/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-13T16:46:51Z", "updated_at": "2019-08-27T17:12:36Z", "closed_at": "2019-08-27T17:12:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I'm trying to cluster about 2000 lat,long locations using driving distance. When used hdbscan in default mode, the number of cluster is usually 37-41 which are too many for my application. I wanted to get around 5-11 clusters so I ran 2 for loops for min_cluster_size, and min_samples from 1 to 60 and if the resulting number of cluster is within 5-11 then I save the results and compute the silhouette_score for the non-noise clusters. Then I selected the best combination of min_cluster_size and min_samples that gave me the largest silhouette_score. However, visually, the clusters don't often make sense\r\nMy question is, is there a better way to tune the parameters?\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/314", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/314/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/314/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/314/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/314", "id": 474976192, "node_id": "MDU6SXNzdWU0NzQ5NzYxOTI=", "number": 314, "title": "How make approximate_predict with precomputed distances ", "user": {"login": "JevgeniyVnk", "id": 37244996, "node_id": "MDQ6VXNlcjM3MjQ0OTk2", "avatar_url": "https://avatars3.githubusercontent.com/u/37244996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JevgeniyVnk", "html_url": "https://github.com/JevgeniyVnk", "followers_url": "https://api.github.com/users/JevgeniyVnk/followers", "following_url": "https://api.github.com/users/JevgeniyVnk/following{/other_user}", "gists_url": "https://api.github.com/users/JevgeniyVnk/gists{/gist_id}", "starred_url": "https://api.github.com/users/JevgeniyVnk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JevgeniyVnk/subscriptions", "organizations_url": "https://api.github.com/users/JevgeniyVnk/orgs", "repos_url": "https://api.github.com/users/JevgeniyVnk/repos", "events_url": "https://api.github.com/users/JevgeniyVnk/events{/privacy}", "received_events_url": "https://api.github.com/users/JevgeniyVnk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-31T07:14:50Z", "updated_at": "2019-07-31T18:05:09Z", "closed_at": "2019-07-31T18:05:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello.\r\nI want to use Gower distance. For this I precompute pairwise distances between points and fit hdbscan model. But in a future I need to classify new points (approximate_predict + fited model).\r\nHow can I do this? Yes, compute distances between \"old\" and new points, but how to make prediction on this new distances? \r\nNow, if Im trying to make predictions like this:\r\n\r\n`clustering_model = hdbscan.HDBSCAN(metric='precomputed',prediction_data=True)\r\nclusters = clustering_model.fit_predict(gower_dist_matrix)\r\nhdbscan.approximate_predict(clustering_model,gower_dist_matrix)`\r\n\r\nget error:\r\n`AttributeError: No prediction data was generated`\r\n\r\n\r\nThanks. Sorry for bad English.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/310", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/310/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/310/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/310/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/310", "id": 466262147, "node_id": "MDU6SXNzdWU0NjYyNjIxNDc=", "number": 310, "title": "Not using all available CPUs?", "user": {"login": "scribu", "id": 225715, "node_id": "MDQ6VXNlcjIyNTcxNQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/225715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scribu", "html_url": "https://github.com/scribu", "followers_url": "https://api.github.com/users/scribu/followers", "following_url": "https://api.github.com/users/scribu/following{/other_user}", "gists_url": "https://api.github.com/users/scribu/gists{/gist_id}", "starred_url": "https://api.github.com/users/scribu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scribu/subscriptions", "organizations_url": "https://api.github.com/users/scribu/orgs", "repos_url": "https://api.github.com/users/scribu/repos", "events_url": "https://api.github.com/users/scribu/events{/privacy}", "received_events_url": "https://api.github.com/users/scribu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-10T11:46:16Z", "updated_at": "2020-05-13T22:02:38Z", "closed_at": "2020-05-13T22:02:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI'm running the following algorithm on 100K samples, using a machine with 16 cores:\r\n\r\n```python\r\nclusterer = HDBSCAN(\r\n    algorithm='boruvka_balltree',\r\n    core_dist_n_jobs=16,\r\n)\r\n```\r\n\r\nBehaviour:\r\n\r\n1. First few seconds: Uses 16 cores.\r\n2. Next few minutes: Uses 4 cores.\r\n3. Remaining time: Uses 1 core.\r\n\r\nI would expect stage 2 to also use 16 cores.\r\n\r\nIs there another parameter I need to pass in order to use all cores?\r\n\r\nNote that `cpu_count()` correctly returns 16 as well.\r\n\r\nUsing hdbscan 0.8.22", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/302", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/302/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/302/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/302/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/302", "id": 448801676, "node_id": "MDU6SXNzdWU0NDg4MDE2NzY=", "number": 302, "title": "Persistence score and labels", "user": {"login": "earikan", "id": 17343221, "node_id": "MDQ6VXNlcjE3MzQzMjIx", "avatar_url": "https://avatars2.githubusercontent.com/u/17343221?v=4", "gravatar_id": "", "url": "https://api.github.com/users/earikan", "html_url": "https://github.com/earikan", "followers_url": "https://api.github.com/users/earikan/followers", "following_url": "https://api.github.com/users/earikan/following{/other_user}", "gists_url": "https://api.github.com/users/earikan/gists{/gist_id}", "starred_url": "https://api.github.com/users/earikan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/earikan/subscriptions", "organizations_url": "https://api.github.com/users/earikan/orgs", "repos_url": "https://api.github.com/users/earikan/repos", "events_url": "https://api.github.com/users/earikan/events{/privacy}", "received_events_url": "https://api.github.com/users/earikan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-27T11:19:55Z", "updated_at": "2019-06-12T13:58:47Z", "closed_at": "2019-06-12T13:58:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI want to know which labels in the cluster that has been given the persistent score. \r\n\r\nFor example, as a result of cluster_persistence_ ,\r\n    Out[34]:\r\n    array([ 0.35, 0.2 , 0.44, 0.23 , 0.16 , 0.44, 0.02])\r\n\r\nHow can I learn the labels of cluster with a persistence score of 0.35? Is it possible?\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/301", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/301/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/301/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/301/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/301", "id": 447188976, "node_id": "MDU6SXNzdWU0NDcxODg5NzY=", "number": 301, "title": "AttributeError: module 'hdbscan' has no attribute 'HDBSCAN'", "user": {"login": "traysmith29", "id": 44208203, "node_id": "MDQ6VXNlcjQ0MjA4MjAz", "avatar_url": "https://avatars0.githubusercontent.com/u/44208203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/traysmith29", "html_url": "https://github.com/traysmith29", "followers_url": "https://api.github.com/users/traysmith29/followers", "following_url": "https://api.github.com/users/traysmith29/following{/other_user}", "gists_url": "https://api.github.com/users/traysmith29/gists{/gist_id}", "starred_url": "https://api.github.com/users/traysmith29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/traysmith29/subscriptions", "organizations_url": "https://api.github.com/users/traysmith29/orgs", "repos_url": "https://api.github.com/users/traysmith29/repos", "events_url": "https://api.github.com/users/traysmith29/events{/privacy}", "received_events_url": "https://api.github.com/users/traysmith29/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-22T15:01:04Z", "updated_at": "2019-05-23T02:18:20Z", "closed_at": "2019-05-23T02:18:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I am having an issue using HDBSCAN on my Mac and PC and cannot resolve it.\r\n\r\nI installed Anaconda 3.7 and installed the hdbscan module using the following line in the terminal \"conda install -c conda-forge hdbscan\". Then, I ran the test \"nosetests -s hdbscan\" and received the following output:\r\n\r\n.../anaconda3/lib/python3.7/site-packages/hdbscan/hdbscan_.py:217: UserWarning: Cannot generate Minimum Spanning Tree; the implemented Prim's does not produce the full minimum spanning tree \r\n  'the full minimum spanning tree ', UserWarning)\r\n./anaconda3/lib/python3.7/site-packages/hdbscan/hdbscan_.py:253: UserWarning: Cannot generate Minimum Spanning Tree; the implemented Prim's does not produce the full minimum spanning tree \r\n  'the full minimum spanning tree ', UserWarning)\r\n..................................\r\n----------------------------------------------------------------------\r\nRan 38 tests in 6.121s\r\nOK\r\n\r\nFinally, when I only import hdbscan in my python file the script runs with no issues. However, when I apply hdbscan to my data as follows \"hdbscanFam = hdbscan.HDBSCAN(min_cluster_size = 9)\" I received the following error \"AttributeError: module 'hdbscan' has no attribute 'HDBSCAN\".\r\n\r\nAlso, my sklearn version is 0.20.3. I would appreciate any assistance because I am completely lost with this issue. I tried running my exact python script on a friends machine who has the same version of Anaconda, sklearn, and hdbscan and it ran successfully on his machine. However, it has failed in the manner described above both on my Mac and PC. Thanks for the help!\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/296", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/296/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/296/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/296/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/296", "id": 442601029, "node_id": "MDU6SXNzdWU0NDI2MDEwMjk=", "number": 296, "title": "Import of hdbscan==0.8.20 fails with scikit-learn==0.21.0", "user": {"login": "uellue", "id": 25689052, "node_id": "MDQ6VXNlcjI1Njg5MDUy", "avatar_url": "https://avatars2.githubusercontent.com/u/25689052?v=4", "gravatar_id": "", "url": "https://api.github.com/users/uellue", "html_url": "https://github.com/uellue", "followers_url": "https://api.github.com/users/uellue/followers", "following_url": "https://api.github.com/users/uellue/following{/other_user}", "gists_url": "https://api.github.com/users/uellue/gists{/gist_id}", "starred_url": "https://api.github.com/users/uellue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/uellue/subscriptions", "organizations_url": "https://api.github.com/users/uellue/orgs", "repos_url": "https://api.github.com/users/uellue/repos", "events_url": "https://api.github.com/users/uellue/events{/privacy}", "received_events_url": "https://api.github.com/users/uellue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-05-10T08:32:10Z", "updated_at": "2020-04-15T18:05:42Z", "closed_at": "2020-04-15T18:05:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "If scikit-learn is installed in version 0.21.0, importing hdbscan in version 0.8.20 gives the following error:\r\n\r\n```\r\n~/libertem-venv/lib/python3.6/site-packages/hdbscan/__init__.py in <module>\r\n----> 1 from .hdbscan_ import HDBSCAN, hdbscan\r\n      2 from .robust_single_linkage_ import RobustSingleLinkage, robust_single_linkage\r\n      3 from .validity import validity_index\r\n      4 from .prediction import approximate_predict, membership_vector, all_points_membership_vectors\r\n      5 \r\n\r\n~/libertem-venv/lib/python3.6/site-packages/hdbscan/hdbscan_.py in <module>\r\n     15 from warnings import warn\r\n     16 from sklearn.utils import check_array\r\n---> 17 from sklearn.externals.joblib.parallel import cpu_count\r\n     18 \r\n     19 from scipy.sparse import csgraph\r\n\r\nModuleNotFoundError: No module named 'sklearn.externals.joblib.parallel'\r\n```\r\n\r\nDowngrading to scikit-learn==0.20.2 works around the issue.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/295", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/295/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/295/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/295/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/295", "id": 439534306, "node_id": "MDU6SXNzdWU0Mzk1MzQzMDY=", "number": 295, "title": "all_points_membership_vectors and membership_vectors return nan probabilities", "user": {"login": "alberto-sibner", "id": 49283178, "node_id": "MDQ6VXNlcjQ5MjgzMTc4", "avatar_url": "https://avatars1.githubusercontent.com/u/49283178?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alberto-sibner", "html_url": "https://github.com/alberto-sibner", "followers_url": "https://api.github.com/users/alberto-sibner/followers", "following_url": "https://api.github.com/users/alberto-sibner/following{/other_user}", "gists_url": "https://api.github.com/users/alberto-sibner/gists{/gist_id}", "starred_url": "https://api.github.com/users/alberto-sibner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alberto-sibner/subscriptions", "organizations_url": "https://api.github.com/users/alberto-sibner/orgs", "repos_url": "https://api.github.com/users/alberto-sibner/repos", "events_url": "https://api.github.com/users/alberto-sibner/events{/privacy}", "received_events_url": "https://api.github.com/users/alberto-sibner/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-05-02T11:02:56Z", "updated_at": "2019-06-13T08:10:08Z", "closed_at": "2019-06-13T08:10:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI'm using hdbscan with haversine metric to find clusters based on latitudes and longitudes. The algorithm works really well for me. However, when I use `all_points_membership_vectors `and `membership_vectors` with some coordinates these methods return nan probabilities.\r\n\r\nIn other words, I have N points in my dataset and they are all classified quite well using soft clustering. Although for a small part of these N points I get NAN probabilities of them belonging to any of the clusters. \r\n\r\nI have checked some of these points individually and they seem totally normal for me, they are surrounded by a lot of clusters in the map and yet they don't have probabilities of belonging to any of them.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/293", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/293/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/293/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/293/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/293", "id": 438875646, "node_id": "MDU6SXNzdWU0Mzg4NzU2NDY=", "number": 293, "title": "ERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly", "user": {"login": "boompig", "id": 2791268, "node_id": "MDQ6VXNlcjI3OTEyNjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/2791268?v=4", "gravatar_id": "", "url": "https://api.github.com/users/boompig", "html_url": "https://github.com/boompig", "followers_url": "https://api.github.com/users/boompig/followers", "following_url": "https://api.github.com/users/boompig/following{/other_user}", "gists_url": "https://api.github.com/users/boompig/gists{/gist_id}", "starred_url": "https://api.github.com/users/boompig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/boompig/subscriptions", "organizations_url": "https://api.github.com/users/boompig/orgs", "repos_url": "https://api.github.com/users/boompig/repos", "events_url": "https://api.github.com/users/boompig/events{/privacy}", "received_events_url": "https://api.github.com/users/boompig/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2019-04-30T16:49:20Z", "updated_at": "2020-06-18T03:54:17Z", "closed_at": "2019-04-30T16:52:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "python: Python 2.7.15rc1\r\n\r\nOS: Linux 6a039c3530c7 4.15.0-48-generic #51-Ubuntu SMP Wed Apr 3 08:28:49 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\npip: pip 19.1\r\n\r\ntried running: `pip install hdbscan`\r\n\r\n```\r\n  Building wheel for hdbscan (PEP 517): finished with status 'error'                                                   \u2502\r\n  ERROR: Complete output from command /opt/conda/bin/python /opt/conda/lib/python3.6/site-packages/pip/_vendor/pep517/_\u2502\r\nin_process.py build_wheel /tmp/tmpvwnr9hhz:                                                                            \u2502\r\n  ERROR: running bdist_wheel                                                                                           \u2502\r\n  running build                                                                                                        \u2502\r\n  running build_py                                                                                                     \u2502\r\n  creating build                                                                                                       \u2502\r\n  creating build/lib.linux-x86_64-3.6                                                                                  \u2502\r\n  creating build/lib.linux-x86_64-3.6/hdbscan                                                                          \u2502\r\n  copying hdbscan/prediction.py -> build/lib.linux-x86_64-3.6/hdbscan                                                  \u2502\r\n  copying hdbscan/robust_single_linkage_.py -> build/lib.linux-x86_64-3.6/hdbscan                                      \u2502\r\n  copying hdbscan/__init__.py -> build/lib.linux-x86_64-3.6/hdbscan                                                    \u2502\r\n  copying hdbscan/validity.py -> build/lib.linux-x86_64-3.6/hdbscan                                                    \u2502\r\n  copying hdbscan/plots.py -> build/lib.linux-x86_64-3.6/hdbscan                                                       \u2502\r\n  copying hdbscan/hdbscan_.py -> build/lib.linux-x86_64-3.6/hdbscan                                                    \u2502\r\n  creating build/lib.linux-x86_64-3.6/hdbscan/tests                                                                    \u2502\r\n  copying hdbscan/tests/test_rsl.py -> build/lib.linux-x86_64-3.6/hdbscan/tests                                        \u2502\r\n  copying hdbscan/tests/__init__.py -> build/lib.linux-x86_64-3.6/hdbscan/tests                                        \u2502\r\n  copying hdbscan/tests/test_hdbscan.py -> build/lib.linux-x86_64-3.6/hdbscan/tests                                    \u2502\r\n  running build_ext                                                                                                    \u2502\r\n  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan/_hdbscan_tree.c                                                       \u2502\r\n  building 'hdbscan._hdbscan_tree' extension                                                                           \u2502\r\n  creating build/temp.linux-x86_64-3.6                                                                                 \u2502\r\n  creating build/temp.linux-x86_64-3.6/hdbscan                                                                         \u2502\r\n  gcc -pthread -B /opt/conda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prot\u2502\r\notypes -fPIC -I/opt/conda/include/python3.6m -I/tmp/pip-build-env-s09rgxp0/overlay/lib/python3.6/site-packages/numpy/co\u2502\r\nre/include -c hdbscan/_hdbscan_tree.c -o build/temp.linux-x86_64-3.6/hdbscan/_hdbscan_tree.o                           \u2502\r\n  /tmp/pip-build-env-s09rgxp0/overlay/lib/python3.6/site-packages/Cython/Compiler/Main.py:367: FutureWarning: Cython di\u2502\r\nrective 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /tmp/pip-install-rf\u2502\r\nyrnh0q/hdbscan/hdbscan/_hdbscan_tree.pyx                                                                               \u2502\r\n    tree = Parsing.p_module(s, pxd, full_module_name)                                                                  \u2502\r\n  error: command 'gcc' failed with exit status 1                                                                       \u2502\r\n  ----------------------------------------                                                                             \u2502\r\n  ERROR: Failed building wheel for hdbscan                                                                             \u2502\r\n  Running setup.py clean for hdbscan                                                                                   \u2502\r\nFailed to build hdbscan                                                                                                \u2502\r\nERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly                           \u2502\r\nThe command '/bin/sh -c pip install hdbscan' returned a non-zero code: 1                                               \u2502```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/292", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/292/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/292/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/292/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/292", "id": 436754549, "node_id": "MDU6SXNzdWU0MzY3NTQ1NDk=", "number": 292, "title": "joblib.Memory Problem", "user": {"login": "rising02", "id": 49952670, "node_id": "MDQ6VXNlcjQ5OTUyNjcw", "avatar_url": "https://avatars0.githubusercontent.com/u/49952670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rising02", "html_url": "https://github.com/rising02", "followers_url": "https://api.github.com/users/rising02/followers", "following_url": "https://api.github.com/users/rising02/following{/other_user}", "gists_url": "https://api.github.com/users/rising02/gists{/gist_id}", "starred_url": "https://api.github.com/users/rising02/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rising02/subscriptions", "organizations_url": "https://api.github.com/users/rising02/orgs", "repos_url": "https://api.github.com/users/rising02/repos", "events_url": "https://api.github.com/users/rising02/events{/privacy}", "received_events_url": "https://api.github.com/users/rising02/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-24T15:11:47Z", "updated_at": "2019-04-24T15:31:32Z", "closed_at": "2019-04-24T15:31:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there,\r\n\r\nI went through the entire documentation for HDBSCAN and looked at the docs for joblib.Memory and had a look at issue #212 but I can't for the life of me figure out how to make use of the joblib.Memory function.\r\n\r\nSince it is supposed to store the hard computation and thus make looking at different cluster sizes faster, I time my code and also check the directories. However, whenever I try to make use of the cached results a new directory is created.\r\n\r\nMy code in jupyter notebook currently looks like this:\r\n\r\nCell 1:\r\n%%time\r\nmem = Memory(cachedir='path/clustering')\r\nclusterer = hdbscan.HDBSCAN(min_cluster_size=50, algorithm='boruvka_kdtree', \r\n                            memory=mem).fit(data)\r\nCell 2:\r\n%%time\r\nclusterer = hdbscan.HDBSCAN(min_cluster_size=100, algorithm='boruvka_kdtree', \r\n                            memory=mem).fit(data)\r\n\r\nThis always produces two caching directories and I can't seem to make use of the previously cached data.\r\n\r\nCould you please help me out and tell me what I am missing?\r\n\r\nBest regards", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/287", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/287/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/287/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/287/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/287", "id": 432939333, "node_id": "MDU6SXNzdWU0MzI5MzkzMzM=", "number": 287, "title": "HDBSCAN AttributeError", "user": {"login": "rick77777", "id": 46786445, "node_id": "MDQ6VXNlcjQ2Nzg2NDQ1", "avatar_url": "https://avatars3.githubusercontent.com/u/46786445?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rick77777", "html_url": "https://github.com/rick77777", "followers_url": "https://api.github.com/users/rick77777/followers", "following_url": "https://api.github.com/users/rick77777/following{/other_user}", "gists_url": "https://api.github.com/users/rick77777/gists{/gist_id}", "starred_url": "https://api.github.com/users/rick77777/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rick77777/subscriptions", "organizations_url": "https://api.github.com/users/rick77777/orgs", "repos_url": "https://api.github.com/users/rick77777/repos", "events_url": "https://api.github.com/users/rick77777/events{/privacy}", "received_events_url": "https://api.github.com/users/rick77777/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-04-14T06:50:39Z", "updated_at": "2019-04-14T14:23:55Z", "closed_at": "2019-04-14T14:23:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to use HDBSCAN for a project. After understanding it I was trying to implement the most basic system generated clustering. But I got an error -\r\n AttributeError: 'HDBSCAN' object has no attribute 'labels_'\r\nHere is the code I wrote -\r\nblobs,labels= make_blobs(n_samples=2000, n_features=17)\r\nclusterer = hd.HDBSCAN()\r\nclusterer.fit(blobs)\r\nclusterer = hd.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\r\n    gen_min_span_tree=False, leaf_size=40,    metric='euclidean', min_cluster_size=5,\r\n    min_samples=None, p=None)\r\n\r\nclusterer.labels_\r\n\r\nSo, I can't get the cluster labels.\r\nPlease suggest where I went wrong.\r\nThanks in advance!!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/286", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/286/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/286/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/286/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/286", "id": 430736949, "node_id": "MDU6SXNzdWU0MzA3MzY5NDk=", "number": 286, "title": "Value error: numpy.ufunc size changed, may indicate binary incompatibility. ", "user": {"login": "antonio-catalano", "id": 38168821, "node_id": "MDQ6VXNlcjM4MTY4ODIx", "avatar_url": "https://avatars0.githubusercontent.com/u/38168821?v=4", "gravatar_id": "", "url": "https://api.github.com/users/antonio-catalano", "html_url": "https://github.com/antonio-catalano", "followers_url": "https://api.github.com/users/antonio-catalano/followers", "following_url": "https://api.github.com/users/antonio-catalano/following{/other_user}", "gists_url": "https://api.github.com/users/antonio-catalano/gists{/gist_id}", "starred_url": "https://api.github.com/users/antonio-catalano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/antonio-catalano/subscriptions", "organizations_url": "https://api.github.com/users/antonio-catalano/orgs", "repos_url": "https://api.github.com/users/antonio-catalano/repos", "events_url": "https://api.github.com/users/antonio-catalano/events{/privacy}", "received_events_url": "https://api.github.com/users/antonio-catalano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-04-09T02:26:31Z", "updated_at": "2019-04-09T02:31:53Z", "closed_at": "2019-04-09T02:31:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/285", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/285/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/285/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/285/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/285", "id": 426186043, "node_id": "MDU6SXNzdWU0MjYxODYwNDM=", "number": 285, "title": "scipy kmeans vs sklearn kmeans performance", "user": {"login": "Willyees", "id": 35226908, "node_id": "MDQ6VXNlcjM1MjI2OTA4", "avatar_url": "https://avatars0.githubusercontent.com/u/35226908?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Willyees", "html_url": "https://github.com/Willyees", "followers_url": "https://api.github.com/users/Willyees/followers", "following_url": "https://api.github.com/users/Willyees/following{/other_user}", "gists_url": "https://api.github.com/users/Willyees/gists{/gist_id}", "starred_url": "https://api.github.com/users/Willyees/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Willyees/subscriptions", "organizations_url": "https://api.github.com/users/Willyees/orgs", "repos_url": "https://api.github.com/users/Willyees/repos", "events_url": "https://api.github.com/users/Willyees/events{/privacy}", "received_events_url": "https://api.github.com/users/Willyees/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-27T21:12:51Z", "updated_at": "2019-04-08T08:26:57Z", "closed_at": "2019-04-08T08:26:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "[https://github.com/scikit-learn-contrib/hdbscan/blob/master/docs/performance_and_scalability.rst](url)\r\nAt the end of the doc is specified how the sklern implementation is superior to the scipy based on the time registered. \r\n\"Finally it demonstrates again how much of a difference implementation can make: the sklearn implementation of K-Means is far better than the scipy implementation\"\r\nI think it should be mentioned that using default settings for the two implementations, the number of iterations is different. sklearn 10, scipy 20\r\n[https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans.html#scipy.cluster.vq.kmeans](url)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/283", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/283/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/283/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/283/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/283", "id": 423207294, "node_id": "MDU6SXNzdWU0MjMyMDcyOTQ=", "number": 283, "title": "The Proper Way of Calculating Performance Metrics", "user": {"login": "psygo", "id": 11823725, "node_id": "MDQ6VXNlcjExODIzNzI1", "avatar_url": "https://avatars3.githubusercontent.com/u/11823725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/psygo", "html_url": "https://github.com/psygo", "followers_url": "https://api.github.com/users/psygo/followers", "following_url": "https://api.github.com/users/psygo/following{/other_user}", "gists_url": "https://api.github.com/users/psygo/gists{/gist_id}", "starred_url": "https://api.github.com/users/psygo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/psygo/subscriptions", "organizations_url": "https://api.github.com/users/psygo/orgs", "repos_url": "https://api.github.com/users/psygo/repos", "events_url": "https://api.github.com/users/psygo/events{/privacy}", "received_events_url": "https://api.github.com/users/psygo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-03-20T11:43:47Z", "updated_at": "2019-03-26T13:09:11Z", "closed_at": "2019-03-26T13:09:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "I didn't find this in the [documentation](https://hdbscan.readthedocs.io/en/latest/index.html) (which is awesome in my opinion) so I figured I could maybe ask it here. \r\n\r\nCan you just simply use common performance metrics for HDBSCAN? For example, one thing I tried with the dataset I'm working on is to take out the data that's being considered as noise (the results with the noise weren't that much different) and apply the Silhouette Index, the Calinsky-Harabasz Index and the Davies-Bouldin Index to it. However, I have no way of knowing this is anything correct. CHI and DBI might be ok, but the SIL is really weird, because when I calculate it for K-Means, I get close to 0.8 and, for HDBSCAN, a horrible 0 (actually, -0.03). So what is the proper way of using these performance metrics (or any other) on HDBSCAN?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/258", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/258/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/258/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/258/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/258", "id": 385691444, "node_id": "MDU6SXNzdWUzODU2OTE0NDQ=", "number": 258, "title": "Import hdbscan ISSUE", "user": {"login": "greg2626", "id": 19469606, "node_id": "MDQ6VXNlcjE5NDY5NjA2", "avatar_url": "https://avatars3.githubusercontent.com/u/19469606?v=4", "gravatar_id": "", "url": "https://api.github.com/users/greg2626", "html_url": "https://github.com/greg2626", "followers_url": "https://api.github.com/users/greg2626/followers", "following_url": "https://api.github.com/users/greg2626/following{/other_user}", "gists_url": "https://api.github.com/users/greg2626/gists{/gist_id}", "starred_url": "https://api.github.com/users/greg2626/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/greg2626/subscriptions", "organizations_url": "https://api.github.com/users/greg2626/orgs", "repos_url": "https://api.github.com/users/greg2626/repos", "events_url": "https://api.github.com/users/greg2626/events{/privacy}", "received_events_url": "https://api.github.com/users/greg2626/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 27, "created_at": "2018-11-29T11:30:30Z", "updated_at": "2020-07-19T18:53:16Z", "closed_at": "2018-12-02T08:20:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI have followed all the steps for installing hdbscan, but still I'm getting the error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-29-3f5a460d7435> in <module>\r\n----> 1 import hdbscan\r\n\r\n/opt/conda/lib/python3.6/site-packages/hdbscan/__init__.py in <module>\r\n----> 1 from .hdbscan_ import HDBSCAN, hdbscan\r\n      2 from .robust_single_linkage_ import RobustSingleLinkage, robust_single_linkage\r\n      3 from .validity import validity_index\r\n      4 from .prediction import approximate_predict, membership_vector, all_points_membership_vectors\r\n      5 \r\n\r\n/opt/conda/lib/python3.6/site-packages/hdbscan/hdbscan_.py in <module>\r\n     19 from scipy.sparse import csgraph\r\n     20 \r\n---> 21 from ._hdbscan_linkage import (single_linkage,\r\n     22                                mst_linkage_core,\r\n     23                                mst_linkage_core_vector,\r\n\r\nhdbscan/_hdbscan_linkage.pyx in init hdbscan._hdbscan_linkage()\r\n\r\nAttributeError: type object 'hdbscan._hdbscan_linkage.UnionFind' has no attribute '__reduce_cython__'\r\n```\r\n\r\nDoes someone knows how to fix it ?\r\nBests,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/255", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/255/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/255/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/255/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/255", "id": 381367965, "node_id": "MDU6SXNzdWUzODEzNjc5NjU=", "number": 255, "title": "Clustering on the outliers", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-15T22:32:20Z", "updated_at": "2019-08-06T04:03:44Z", "closed_at": "2019-08-06T04:03:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nThanks a lot for the library. I have been using it regularly and have few questions about some of the things that I noticed on the results. I noticed this behavior over several datasets that I experimented with.\r\n\r\nI start with clustering a huge dataset on 20-ish features. I get ~40 clusters and ~5% of them are called as outliers/anomalies. But I want to reduce the number of outliers or try to group them.  \r\n\r\nThe way I do grouping for identifying consistent outliers/anomalies is by maintaining all the initial clusterer settings but reducing the `min_cluster_size` (lets say by half), let's say that as second_min_cluster_size,  and then clustering just on the outliers. That way, I am hoping to identify few groups that couldn't form as clusters (because of min_cluster_size) and ended up as anomalies. \r\n\r\nThe second clustering generates lot less outliers and few cluster groups which are spread all over.  Some of the cluster groups have lot of observations, some times 10 times more than the first_min_cluster_size. If it could form such a big cluster, then shouldn't it have been formed in the first place with first_clusterer settings?\r\n\r\nSorry if my question sounds too confusing, my intention is to use the clusters for some information but also reduce the anomalies by grouping them as consistent anomalies so that I will have few anomaly groups and less number of anomalies at the end of the second step\r\n\r\nThanks\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/253", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/253/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/253/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/253/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/253", "id": 380651002, "node_id": "MDU6SXNzdWUzODA2NTEwMDI=", "number": 253, "title": "Misleading errors msg with sparse precomputed matrix", "user": {"login": "thomasopsomer", "id": 11439747, "node_id": "MDQ6VXNlcjExNDM5NzQ3", "avatar_url": "https://avatars3.githubusercontent.com/u/11439747?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasopsomer", "html_url": "https://github.com/thomasopsomer", "followers_url": "https://api.github.com/users/thomasopsomer/followers", "following_url": "https://api.github.com/users/thomasopsomer/following{/other_user}", "gists_url": "https://api.github.com/users/thomasopsomer/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasopsomer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasopsomer/subscriptions", "organizations_url": "https://api.github.com/users/thomasopsomer/orgs", "repos_url": "https://api.github.com/users/thomasopsomer/repos", "events_url": "https://api.github.com/users/thomasopsomer/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasopsomer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-14T11:16:06Z", "updated_at": "2018-12-04T01:29:30Z", "closed_at": "2018-12-04T01:29:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\nWhen using hdscan with a sparse precomputed distance matrix, it may throw an error saying that the distance matrix has more than one component and that one should apply dbscan on each component.\r\n\r\nHowever, this error / message can also happen in case where the distance matrix has only one component, leading to some frustration :).\r\n\r\nLooking at the implementation, the error is thrown looking at the `mutual_reachability`. The way it is computed can indeed lead to a `mutual_reachability` having several component while the distance matrix has only one, because if the reachability is 'infinite' (no k'th neighbor) then the mutual reachability becomes also infinite and isn't stored in the `mutual_reachability` matrix (https://github.com/scikit-learn-contrib/hdbscan/blob/master/hdbscan/_hdbscan_reachability.pyx#L91)\r\n\r\nSo concerning the error maybe we could actually have 2 errors: a first one raised if the distance matrix have more than 1 component, and then a second one if there an issue with the `mutual_reachability` ?\r\n\r\nAnother question, why can't we set a max value in the `mutual reachability` matrix when the reachability in infinite ?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/249", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/249/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/249/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/249/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/249", "id": 377543140, "node_id": "MDU6SXNzdWUzNzc1NDMxNDA=", "number": 249, "title": "AttrivbuteError: No prediction data was generated", "user": {"login": "mrxiaohe", "id": 3064882, "node_id": "MDQ6VXNlcjMwNjQ4ODI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3064882?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrxiaohe", "html_url": "https://github.com/mrxiaohe", "followers_url": "https://api.github.com/users/mrxiaohe/followers", "following_url": "https://api.github.com/users/mrxiaohe/following{/other_user}", "gists_url": "https://api.github.com/users/mrxiaohe/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrxiaohe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrxiaohe/subscriptions", "organizations_url": "https://api.github.com/users/mrxiaohe/orgs", "repos_url": "https://api.github.com/users/mrxiaohe/repos", "events_url": "https://api.github.com/users/mrxiaohe/events{/privacy}", "received_events_url": "https://api.github.com/users/mrxiaohe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-05T19:15:43Z", "updated_at": "2018-11-05T20:37:07Z", "closed_at": "2018-11-05T20:37:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "I trained a HDBSCAN model on a set of data, and would like to use the model to make predictions on new data. I did the following:\r\n\r\n`hdbscan.approximate_predict(hdbscan_, data[0])\r\n`\r\nand got the following error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\hdbscan\\prediction.py\", line 373, in approximate_predict\r\n    if clusterer.prediction_data_ is None:\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\hdbscan\\hdbscan_.py\", line 909, in prediction_data_\r\n    raise AttributeError('No prediction data was generated')\r\nAttributeError: No prediction data was generated\r\n```\r\n\r\nI wonder what might be the cause of this. Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/248", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/248/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/248/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/248/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/248", "id": 376081525, "node_id": "MDU6SXNzdWUzNzYwODE1MjU=", "number": 248, "title": "Running out of memory", "user": {"login": "SantiagoOrdonez", "id": 16404819, "node_id": "MDQ6VXNlcjE2NDA0ODE5", "avatar_url": "https://avatars3.githubusercontent.com/u/16404819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SantiagoOrdonez", "html_url": "https://github.com/SantiagoOrdonez", "followers_url": "https://api.github.com/users/SantiagoOrdonez/followers", "following_url": "https://api.github.com/users/SantiagoOrdonez/following{/other_user}", "gists_url": "https://api.github.com/users/SantiagoOrdonez/gists{/gist_id}", "starred_url": "https://api.github.com/users/SantiagoOrdonez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SantiagoOrdonez/subscriptions", "organizations_url": "https://api.github.com/users/SantiagoOrdonez/orgs", "repos_url": "https://api.github.com/users/SantiagoOrdonez/repos", "events_url": "https://api.github.com/users/SantiagoOrdonez/events{/privacy}", "received_events_url": "https://api.github.com/users/SantiagoOrdonez/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-31T17:37:01Z", "updated_at": "2018-11-09T19:38:29Z", "closed_at": "2018-11-09T19:38:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nI have a data set of ~3000000 lat/lng data points and I was hoping to use HDBSCAN. Unfortunately, when I ran it on this data set I got an out of memory error.  I had ran HDBSCAN on a smaller portion of the data set and It worked fine. I am using the haversine metric and thus cannot use the algorithm \"prims_kdtree\". \r\n\r\nHere is my stack trace: \r\nTraceback (most recent call last):\r\n  File \"create_regions.py\", line 167, in <module>\r\n    create_clusters(df, epsilon, 1000, \"HDBSCAN\")\r\n  File \"create_regions.py\", line 43, in create_clusters\r\n    labels = clusterer.fit_predict(np.radians(df))\r\n  File \"/var/opt/mapworkflows/venv/local/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 876, in fit_predict\r\n    self.fit(X)\r\n  File \"/var/opt/mapworkflows/venv/local/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 854, in fit\r\n    self._min_spanning_tree) = hdbscan(X, **kwargs)\r\n  File \"/var/opt/mapworkflows/venv/local/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 589, in hdbscan\r\n    core_dist_n_jobs, **kwargs)\r\n  File \"/var/opt/mapworkflows/venv/local/lib/python2.7/site-packages/sklearn/externals/joblib/memory.py\", line 329, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/var/opt/mapworkflows/venv/local/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 285, in _hdbscan_boruvka_balltree\r\n    n_jobs=core_dist_n_jobs, **kwargs)\r\n  File \"hdbscan/_hdbscan_boruvka.pyx\", line 976, in hdbscan._hdbscan_boruvka.BallTreeBoruvkaAlgorithm.__init__\r\n  File \"hdbscan/dist_metrics.pyx\", line 398, in hdbscan.dist_metrics.DistanceMetric.pairwise\r\nMemoryError", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/244", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/244/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/244/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/244/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/244", "id": 374015183, "node_id": "MDU6SXNzdWUzNzQwMTUxODM=", "number": 244, "title": "Visual Studio C++ Build Tools 2015", "user": {"login": "chrispikula", "id": 22281606, "node_id": "MDQ6VXNlcjIyMjgxNjA2", "avatar_url": "https://avatars3.githubusercontent.com/u/22281606?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrispikula", "html_url": "https://github.com/chrispikula", "followers_url": "https://api.github.com/users/chrispikula/followers", "following_url": "https://api.github.com/users/chrispikula/following{/other_user}", "gists_url": "https://api.github.com/users/chrispikula/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrispikula/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrispikula/subscriptions", "organizations_url": "https://api.github.com/users/chrispikula/orgs", "repos_url": "https://api.github.com/users/chrispikula/repos", "events_url": "https://api.github.com/users/chrispikula/events{/privacy}", "received_events_url": "https://api.github.com/users/chrispikula/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-25T15:49:31Z", "updated_at": "2018-10-25T16:23:31Z", "closed_at": "2018-10-25T16:23:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "So my installation failed due to the requirement of the Visual Studio C++ 2015 Build Tools .\r\n\r\nIt's a bit of a long-shot, but I was wondering if there is there any way around this requirement?\r\n\r\nI'd like to try out HDBSCAN on my dataset, but as I'm in a corporate environment, that is not feasible due to licensing issues.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/233", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/233/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/233/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/233/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/233", "id": 357889307, "node_id": "MDU6SXNzdWUzNTc4ODkzMDc=", "number": 233, "title": "Reupload to PyPI to take advantage of pyproject.toml", "user": {"login": "alugowski", "id": 2730364, "node_id": "MDQ6VXNlcjI3MzAzNjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2730364?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alugowski", "html_url": "https://github.com/alugowski", "followers_url": "https://api.github.com/users/alugowski/followers", "following_url": "https://api.github.com/users/alugowski/following{/other_user}", "gists_url": "https://api.github.com/users/alugowski/gists{/gist_id}", "starred_url": "https://api.github.com/users/alugowski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alugowski/subscriptions", "organizations_url": "https://api.github.com/users/alugowski/orgs", "repos_url": "https://api.github.com/users/alugowski/repos", "events_url": "https://api.github.com/users/alugowski/events{/privacy}", "received_events_url": "https://api.github.com/users/alugowski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-09-07T01:47:04Z", "updated_at": "2018-09-13T11:15:38Z", "closed_at": "2018-09-13T02:05:51Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "This commit was recently merged: https://github.com/scikit-learn-contrib/hdbscan/commit/7a1f7738e9b095fa2454687f2afc09aa6b7581d6\r\n\r\nIt adds a `pyproject.toml` which means `pip install hdbscan` will work even if you don't have Cython installed yet. This makes it possible to use hdbscan in `requirements.txt` files and have it work as expected. It also means it's possible to use hdbscan in places that only allow one `pip install` statement, such as Google AppEngine.\r\n\r\nThis only requires a minor version bump and a reupload, nothing else.\r\n\r\nThank you for your work!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/230", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/230/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/230/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/230/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/230", "id": 352967223, "node_id": "MDU6SXNzdWUzNTI5NjcyMjM=", "number": 230, "title": "pip install hdbscan (>0.8.13) fails with Python 3.6.5", "user": {"login": "themmes", "id": 12343995, "node_id": "MDQ6VXNlcjEyMzQzOTk1", "avatar_url": "https://avatars1.githubusercontent.com/u/12343995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/themmes", "html_url": "https://github.com/themmes", "followers_url": "https://api.github.com/users/themmes/followers", "following_url": "https://api.github.com/users/themmes/following{/other_user}", "gists_url": "https://api.github.com/users/themmes/gists{/gist_id}", "starred_url": "https://api.github.com/users/themmes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/themmes/subscriptions", "organizations_url": "https://api.github.com/users/themmes/orgs", "repos_url": "https://api.github.com/users/themmes/repos", "events_url": "https://api.github.com/users/themmes/events{/privacy}", "received_events_url": "https://api.github.com/users/themmes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-22T14:00:47Z", "updated_at": "2018-08-24T08:19:00Z", "closed_at": "2018-08-24T08:16:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Description\r\n\r\n`pip install hdbscan` fails with Python3.6, even after installing cython first.\r\n\r\n## Expected behaviour\r\n\r\ncorrect install of latest version\r\n\r\n## Actual result\r\n\r\n```\r\n(env) tom@pastoe:~/portaal$ pip3 install --upgrade hdbscan==0.8.14\r\nCollecting hdbscan==0.8.14\r\n  Downloading https://files.pythonhosted.org/packages/19/11/9471a7fcc23b8a9d409646217b2cdb5ba260d0d691fd2fd691976a290225/hdbscan-0.8.14.tar.gz (4.0MB)\r\n    100% |################################| 4.0MB 4.8MB/s\r\nRequirement already satisfied, skipping upgrade: numpy in ./env/lib/python3.6/site-packages (from hdbscan==0.8.14) (1.15.1)\r\nRequirement already satisfied, skipping upgrade: scikit-learn>=0.16 in ./env/lib/python3.6/site-packages (from hdbscan==0.8.14) (0.19.2)\r\nRequirement already satisfied, skipping upgrade: cython>=0.26 in ./env/lib/python3.6/site-packages (from hdbscan==0.8.14) (0.28.5)\r\nBuilding wheels for collected packages: hdbscan\r\n  Running setup.py bdist_wheel for hdbscan ... error\r\n  Complete output from command /home/tom/portaal/env/bin/python3.6 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-install-edgxjb9z/hdbscan/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/pip-wheel-pbj18dq7 --python-tag cp36:\r\n  running bdist_wheel\r\n  running build\r\n  running build_py\r\n  creating build\r\n  creating build/lib.linux-x86_64-3.6\r\n  creating build/lib.linux-x86_64-3.6/hdbscan\r\n  copying hdbscan/plots.py -> build/lib.linux-x86_64-3.6/hdbscan\r\n  copying hdbscan/robust_single_linkage_.py -> build/lib.linux-x86_64-3.6/hdbscan\r\n  copying hdbscan/hdbscan_.py -> build/lib.linux-x86_64-3.6/hdbscan\r\n  copying hdbscan/__init__.py -> build/lib.linux-x86_64-3.6/hdbscan\r\n  copying hdbscan/validity.py -> build/lib.linux-x86_64-3.6/hdbscan\r\n  copying hdbscan/prediction.py -> build/lib.linux-x86_64-3.6/hdbscan\r\n  creating build/lib.linux-x86_64-3.6/hdbscan/tests\r\n  copying hdbscan/tests/test_hdbscan.py -> build/lib.linux-x86_64-3.6/hdbscan/tests\r\n  copying hdbscan/tests/__init__.py -> build/lib.linux-x86_64-3.6/hdbscan/tests\r\n  copying hdbscan/tests/test_rsl.py -> build/lib.linux-x86_64-3.6/hdbscan/tests\r\n  running build_ext\r\n  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan/_hdbscan_tree.c\r\n  building 'hdbscan._hdbscan_tree' extension\r\n  creating build/temp.linux-x86_64-3.6\r\n  creating build/temp.linux-x86_64-3.6/hdbscan\r\n  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -I/home/tom/portaal/env/include/python3.6m -I/home/tom/portaal/env/lib/python3.6/site-packages/numpy/core/include -c hdbscan/_hdbscan_tree.c -o build/temp.linux-x86_64-3.6/hdbscan/_hdbscan_tree.o\r\n  hdbscan/_hdbscan_tree.c:4:20: fatal error: Python.h: No such file or directory\r\n  compilation terminated.\r\n  error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\r\n\r\n  ----------------------------------------\r\n  Failed building wheel for hdbscan\r\n```\r\n \r\n## Attempts\r\n\r\nIn trying to get `hdbscan` to install correctly I discovered that both `0.8.14` and `0.8.15` result in this error, whereas `0.8.13` installs perfectly fine. Even more, `0.8.13` also installs without explicitly installing `cython` first.\r\n\r\n## Versions\r\n\r\n- Ubuntu 16.04.4 LTS\r\n- Python 3.6.5\r\n- virtualenv 15.2.0\r\n- pip 18.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/227", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/227/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/227/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/227/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/227", "id": 345513119, "node_id": "MDU6SXNzdWUzNDU1MTMxMTk=", "number": 227, "title": "How can I set up the k value in HDBSCAN class that could compute points' core distance?", "user": {"login": "OseongKwon", "id": 12179056, "node_id": "MDQ6VXNlcjEyMTc5MDU2", "avatar_url": "https://avatars1.githubusercontent.com/u/12179056?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OseongKwon", "html_url": "https://github.com/OseongKwon", "followers_url": "https://api.github.com/users/OseongKwon/followers", "following_url": "https://api.github.com/users/OseongKwon/following{/other_user}", "gists_url": "https://api.github.com/users/OseongKwon/gists{/gist_id}", "starred_url": "https://api.github.com/users/OseongKwon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OseongKwon/subscriptions", "organizations_url": "https://api.github.com/users/OseongKwon/orgs", "repos_url": "https://api.github.com/users/OseongKwon/repos", "events_url": "https://api.github.com/users/OseongKwon/events{/privacy}", "received_events_url": "https://api.github.com/users/OseongKwon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-29T08:31:41Z", "updated_at": "2018-07-30T04:38:42Z", "closed_at": "2018-07-30T04:38:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "At 'Transform the space' section in 'How HDBSCAN Works' document, you said:\r\nthe core distance defined for parameter k for a point x and denote as \\mathrm{core}_k(x).\r\n\r\nBut I couldn't  find the k parameter in the HDBSCAN class, But in the RobustSingleLinkage class.\r\nHowever, don't HDBSCAN use RobustSingleLinkage!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/225", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/225/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/225/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/225/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/225", "id": 339168461, "node_id": "MDU6SXNzdWUzMzkxNjg0NjE=", "number": 225, "title": "Do I need to standarize my data if I am using hdbscan or hiererchichal clustering?", "user": {"login": "alonsopg", "id": 13632106, "node_id": "MDQ6VXNlcjEzNjMyMTA2", "avatar_url": "https://avatars1.githubusercontent.com/u/13632106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alonsopg", "html_url": "https://github.com/alonsopg", "followers_url": "https://api.github.com/users/alonsopg/followers", "following_url": "https://api.github.com/users/alonsopg/following{/other_user}", "gists_url": "https://api.github.com/users/alonsopg/gists{/gist_id}", "starred_url": "https://api.github.com/users/alonsopg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alonsopg/subscriptions", "organizations_url": "https://api.github.com/users/alonsopg/orgs", "repos_url": "https://api.github.com/users/alonsopg/repos", "events_url": "https://api.github.com/users/alonsopg/events{/privacy}", "received_events_url": "https://api.github.com/users/alonsopg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-07-07T19:17:44Z", "updated_at": "2019-10-09T01:10:08Z", "closed_at": "2019-10-09T01:10:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "As far as I know, when using a linear model you need to put in the same scale your data. However, HDBSCAN, is a density based clustering method, this do I need to normalize or scale my dataset? \r\n\r\nI tried both options and when standard scaling my data I have two clusters, on the other hand when I dont scale my data I end with 89. My fear is that if I scale, I can bias my results.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/222", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/222/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/222/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/222/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/222", "id": 336944527, "node_id": "MDU6SXNzdWUzMzY5NDQ1Mjc=", "number": 222, "title": "import hdbscan failure after pip install on linux", "user": {"login": "braymp", "id": 1186440, "node_id": "MDQ6VXNlcjExODY0NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1186440?v=4", "gravatar_id": "", "url": "https://api.github.com/users/braymp", "html_url": "https://github.com/braymp", "followers_url": "https://api.github.com/users/braymp/followers", "following_url": "https://api.github.com/users/braymp/following{/other_user}", "gists_url": "https://api.github.com/users/braymp/gists{/gist_id}", "starred_url": "https://api.github.com/users/braymp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/braymp/subscriptions", "organizations_url": "https://api.github.com/users/braymp/orgs", "repos_url": "https://api.github.com/users/braymp/repos", "events_url": "https://api.github.com/users/braymp/events{/privacy}", "received_events_url": "https://api.github.com/users/braymp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-06-29T10:18:54Z", "updated_at": "2019-01-17T18:22:48Z", "closed_at": "2018-07-05T09:41:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Recently, I used <code>pip install hdbscan</code> under Python 2.7.11 which was successful. However, upon trying to <code>import hdbscan</code>, I get the following error:\r\n```\r\nPython 2.7.11 (default, Dec 22 2016, 01:51:28) \r\n[GCC 4.8.2] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import hdbscan\r\nRuntimeError: module compiled against API version 0xc but this version of numpy is 0xa\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/brayma2/.local/lib/python2.7/site-packages/hdbscan/__init__.py\", line 1, in <module>\r\n    from .hdbscan_ import HDBSCAN, hdbscan\r\n  File \"/home/brayma2/.local/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 21, in <module>\r\n    from ._hdbscan_linkage import (single_linkage,\r\n  File \"hdbscan/dist_metrics.pxd\", line 61, in init hdbscan._hdbscan_linkage (hdbscan/_hdbscan_linkage.c:20936)\r\nImportError: numpy.core.multiarray failed to import\r\n```\r\nThough it appears to be a numpy error/version mismatch, <code>pip list</code> shows the following:\r\n```\r\nCython (0.25.2)\r\nnumpy (1.11.2)\r\nscikit-learn (0.19.1)\r\nscipy (0.18.1)\r\n```\r\nwhich satisfies the requirements.txt file.\r\n\r\nFurthermore, attempting to compile from source yields the following error (sorry for the length):\r\n```\r\nrunning install\r\nrunning bdist_egg\r\nrunning egg_info\r\nwriting requirements to hdbscan.egg-info/requires.txt\r\nwriting hdbscan.egg-info/PKG-INFO\r\nwriting top-level names to hdbscan.egg-info/top_level.txt\r\nwriting dependency_links to hdbscan.egg-info/dependency_links.txt\r\nreading manifest file 'hdbscan.egg-info/SOURCES.txt'\r\nreading manifest template 'MANIFEST.in'\r\nwriting manifest file 'hdbscan.egg-info/SOURCES.txt'\r\ninstalling library code to build/bdist.linux-x86_64/egg\r\nrunning install_lib\r\nrunning build_py\r\ncreating build\r\ncreating build/lib.linux-x86_64-2.7\r\ncreating build/lib.linux-x86_64-2.7/hdbscan\r\ncopying hdbscan/robust_single_linkage_.py -> build/lib.linux-x86_64-2.7/hdbscan\r\ncopying hdbscan/prediction.py -> build/lib.linux-x86_64-2.7/hdbscan\r\ncopying hdbscan/hdbscan_.py -> build/lib.linux-x86_64-2.7/hdbscan\r\ncopying hdbscan/validity.py -> build/lib.linux-x86_64-2.7/hdbscan\r\ncopying hdbscan/__init__.py -> build/lib.linux-x86_64-2.7/hdbscan\r\ncopying hdbscan/plots.py -> build/lib.linux-x86_64-2.7/hdbscan\r\ncreating build/lib.linux-x86_64-2.7/hdbscan/tests\r\ncopying hdbscan/tests/test_hdbscan.py -> build/lib.linux-x86_64-2.7/hdbscan/tests\r\ncopying hdbscan/tests/test_rsl.py -> build/lib.linux-x86_64-2.7/hdbscan/tests\r\ncopying hdbscan/tests/__init__.py -> build/lib.linux-x86_64-2.7/hdbscan/tests\r\nrunning build_ext\r\ncythoning hdbscan/_hdbscan_tree.pyx to hdbscan/_hdbscan_tree.c\r\nbuilding 'hdbscan._hdbscan_tree' extension\r\ncreating build/temp.linux-x86_64-2.7\r\ncreating build/temp.linux-x86_64-2.7/hdbscan\r\ngcc -fno-strict-aliasing -O3 -march=x86-64 -mtune=generic -fPIC -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/include/python2.7 -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include -c hdbscan/_hdbscan_tree.c -o build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_tree.o\r\nIn file included from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarraytypes.h:1777:0,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:18,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from hdbscan/_hdbscan_tree.c:435:\r\n/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n #warning \"Using deprecated NumPy API, disable it by \" \\\r\n  ^\r\ngcc -shared -Wl,-rpath=$ORIGIN/../lib build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_tree.o -L/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib -lpython2.7 -o build/lib.linux-x86_64-2.7/hdbscan/_hdbscan_tree.so\r\nskipping 'hdbscan/_hdbscan_linkage.c' Cython extension (up-to-date)\r\nbuilding 'hdbscan._hdbscan_linkage' extension\r\ngcc -fno-strict-aliasing -O3 -march=x86-64 -mtune=generic -fPIC -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/include/python2.7 -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include -c hdbscan/_hdbscan_linkage.c -o build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_linkage.o\r\nIn file included from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarraytypes.h:1777:0,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:18,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from hdbscan/_hdbscan_linkage.c:274:\r\n/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n #warning \"Using deprecated NumPy API, disable it by \" \\\r\n  ^\r\nIn file included from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:27:0,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from hdbscan/_hdbscan_linkage.c:274:\r\n/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/__multiarray_api.h:1448:1: warning: \u00e2\u20ac\u02dc_import_array\u00e2\u20ac\u2122 defined but not used [-Wunused-function]\r\n _import_array(void)\r\n ^\r\ngcc -shared -Wl,-rpath=$ORIGIN/../lib build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_linkage.o -L/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib -lpython2.7 -o build/lib.linux-x86_64-2.7/hdbscan/_hdbscan_linkage.so\r\ncythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan/_hdbscan_boruvka.c\r\nbuilding 'hdbscan._hdbscan_boruvka' extension\r\ngcc -fno-strict-aliasing -O3 -march=x86-64 -mtune=generic -fPIC -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/include/python2.7 -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include -c hdbscan/_hdbscan_boruvka.c -o build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_boruvka.o\r\nIn file included from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarraytypes.h:1777:0,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:18,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from hdbscan/_hdbscan_boruvka.c:435:\r\n/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n #warning \"Using deprecated NumPy API, disable it by \" \\\r\n  ^\r\nhdbscan/_hdbscan_boruvka.c: In function \u00e2\u20ac\u02dc__pyx_f_7hdbscan_16_hdbscan_boruvka_24BallTreeBoruvkaAlgorithm_dual_tree_traversal\u00e2\u20ac\u2122:\r\nhdbscan/_hdbscan_boruvka.c:14509:16: warning: \u00e2\u20ac\u02dc__pyx_v_new_lower_bound\u00e2\u20ac\u2122 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     __pyx_t_19 = (__pyx_v_new_lower_bound + (2.0 * __pyx_v_node1_info.radius));\r\n                ^\r\nhdbscan/_hdbscan_boruvka.c:14527:8: warning: \u00e2\u20ac\u02dc__pyx_v_new_upper_bound\u00e2\u20ac\u2122 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if (((__pyx_t_19 < __pyx_t_5) != 0)) {\r\n        ^\r\ngcc -shared -Wl,-rpath=$ORIGIN/../lib build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_boruvka.o -L/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib -lpython2.7 -o build/lib.linux-x86_64-2.7/hdbscan/_hdbscan_boruvka.so\r\nskipping 'hdbscan/_hdbscan_reachability.c' Cython extension (up-to-date)\r\nbuilding 'hdbscan._hdbscan_reachability' extension\r\ngcc -fno-strict-aliasing -O3 -march=x86-64 -mtune=generic -fPIC -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/include/python2.7 -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include -c hdbscan/_hdbscan_reachability.c -o build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_reachability.o\r\nIn file included from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarraytypes.h:1777:0,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:18,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from hdbscan/_hdbscan_reachability.c:274:\r\n/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n #warning \"Using deprecated NumPy API, disable it by \" \\\r\n  ^\r\nIn file included from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:27:0,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from hdbscan/_hdbscan_reachability.c:274:\r\n/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/__multiarray_api.h:1448:1: warning: \u00e2\u20ac\u02dc_import_array\u00e2\u20ac\u2122 defined but not used [-Wunused-function]\r\n _import_array(void)\r\n ^\r\ngcc -shared -Wl,-rpath=$ORIGIN/../lib build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_reachability.o -L/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib -lpython2.7 -o build/lib.linux-x86_64-2.7/hdbscan/_hdbscan_reachability.so\r\ncythoning hdbscan/_prediction_utils.pyx to hdbscan/_prediction_utils.c\r\nbuilding 'hdbscan._prediction_utils' extension\r\ngcc -fno-strict-aliasing -O3 -march=x86-64 -mtune=generic -fPIC -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/include/python2.7 -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include -c hdbscan/_prediction_utils.c -o build/temp.linux-x86_64-2.7/hdbscan/_prediction_utils.o\r\nIn file included from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarraytypes.h:1777:0,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/ndarrayobject.h:18,\r\n                 from /usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/arrayobject.h:4,\r\n                 from hdbscan/_prediction_utils.c:435:\r\n/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning \"Using deprecated NumPy API, disable it by \" \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\r\n #warning \"Using deprecated NumPy API, disable it by \" \\\r\n  ^\r\ngcc -shared -Wl,-rpath=$ORIGIN/../lib build/temp.linux-x86_64-2.7/hdbscan/_prediction_utils.o -L/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib -lpython2.7 -o build/lib.linux-x86_64-2.7/hdbscan/_prediction_utils.so\r\ncythoning hdbscan/dist_metrics.pyx to hdbscan/dist_metrics.c\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n             ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1138:14: Assignment of Python object not allowed without gil\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n        x2arr = _buffer_to_ndarray(x2, size)\r\n             ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1139:14: Assignment of Python object not allowed without gil\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n\r\n    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n                                 ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1138:34: Calling gil-requiring function not allowed without gil\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n        x2arr = _buffer_to_ndarray(x2, size)\r\n                                 ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1139:34: Calling gil-requiring function not allowed without gil\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n        x2arr = _buffer_to_ndarray(x2, size)\r\n        return self.func(x1arr, x2arr, **self.kwargs)\r\n                       ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1140:24: Coercion from Python not allowed without the GIL\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n        x2arr = _buffer_to_ndarray(x2, size)\r\n        return self.func(x1arr, x2arr, **self.kwargs)\r\n                       ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1140:24: Calling gil-requiring function not allowed without gil\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n        x2arr = _buffer_to_ndarray(x2, size)\r\n        return self.func(x1arr, x2arr, **self.kwargs)\r\n                       ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1140:24: Constructing Python tuple not allowed without gil\r\n\r\nError compiling Cython file:\r\n------------------------------------------------------------\r\n...\r\n                             ITYPE_t size) except -1 with gil:\r\n        cdef np.ndarray x1arr\r\n        cdef np.ndarray x2arr\r\n        x1arr = _buffer_to_ndarray(x1, size)\r\n        x2arr = _buffer_to_ndarray(x2, size)\r\n        return self.func(x1arr, x2arr, **self.kwargs)\r\n                       ^\r\n------------------------------------------------------------\r\n\r\nhdbscan/dist_metrics.pyx:1140:24: Constructing Python dict not allowed without gil\r\nbuilding 'hdbscan.dist_metrics' extension\r\ngcc -fno-strict-aliasing -O3 -march=x86-64 -mtune=generic -fPIC -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/include/python2.7 -I/usr/prog/python/2.7.11-goolf-1.5.14-NX/lib/python2.7/site-packages/numpy-1.11.2-py2.7-linux-x86_64.egg/numpy/core/include -c hdbscan/dist_metrics.c -o build/temp.linux-x86_64-2.7/hdbscan/dist_metrics.o\r\nhdbscan/dist_metrics.c:1:2: error: #error Do not use this file, it is the result of a failed Cython compilation.\r\n #error Do not use this file, it is the result of a failed Cython compilation.\r\n  ^\r\nerror: command 'gcc' failed with exit status 1\r\n```\r\nAny suggestions?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/216", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/216/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/216/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/216/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/216", "id": 331712098, "node_id": "MDU6SXNzdWUzMzE3MTIwOTg=", "number": 216, "title": "Dataset", "user": {"login": "mashuklya", "id": 18324597, "node_id": "MDQ6VXNlcjE4MzI0NTk3", "avatar_url": "https://avatars2.githubusercontent.com/u/18324597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mashuklya", "html_url": "https://github.com/mashuklya", "followers_url": "https://api.github.com/users/mashuklya/followers", "following_url": "https://api.github.com/users/mashuklya/following{/other_user}", "gists_url": "https://api.github.com/users/mashuklya/gists{/gist_id}", "starred_url": "https://api.github.com/users/mashuklya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mashuklya/subscriptions", "organizations_url": "https://api.github.com/users/mashuklya/orgs", "repos_url": "https://api.github.com/users/mashuklya/repos", "events_url": "https://api.github.com/users/mashuklya/events{/privacy}", "received_events_url": "https://api.github.com/users/mashuklya/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-12T18:57:22Z", "updated_at": "2018-07-10T07:17:15Z", "closed_at": "2018-07-10T07:17:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Could you please share your dataset `clusterable_data.npy` from http://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/214", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/214/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/214/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/214/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/214", "id": 329536035, "node_id": "MDU6SXNzdWUzMjk1MzYwMzU=", "number": 214, "title": "Buffer dtype mismatch issue ", "user": {"login": "ujjwalaananth", "id": 28482281, "node_id": "MDQ6VXNlcjI4NDgyMjgx", "avatar_url": "https://avatars1.githubusercontent.com/u/28482281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ujjwalaananth", "html_url": "https://github.com/ujjwalaananth", "followers_url": "https://api.github.com/users/ujjwalaananth/followers", "following_url": "https://api.github.com/users/ujjwalaananth/following{/other_user}", "gists_url": "https://api.github.com/users/ujjwalaananth/gists{/gist_id}", "starred_url": "https://api.github.com/users/ujjwalaananth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ujjwalaananth/subscriptions", "organizations_url": "https://api.github.com/users/ujjwalaananth/orgs", "repos_url": "https://api.github.com/users/ujjwalaananth/repos", "events_url": "https://api.github.com/users/ujjwalaananth/events{/privacy}", "received_events_url": "https://api.github.com/users/ujjwalaananth/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-05T16:36:03Z", "updated_at": "2018-06-05T17:11:23Z", "closed_at": "2018-06-05T17:09:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/212", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/212/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/212/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/212/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/212", "id": 327433998, "node_id": "MDU6SXNzdWUzMjc0MzM5OTg=", "number": 212, "title": "Optimizing HDBSCAN for huge datasets", "user": {"login": "Karvi95", "id": 8742642, "node_id": "MDQ6VXNlcjg3NDI2NDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/8742642?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Karvi95", "html_url": "https://github.com/Karvi95", "followers_url": "https://api.github.com/users/Karvi95/followers", "following_url": "https://api.github.com/users/Karvi95/following{/other_user}", "gists_url": "https://api.github.com/users/Karvi95/gists{/gist_id}", "starred_url": "https://api.github.com/users/Karvi95/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Karvi95/subscriptions", "organizations_url": "https://api.github.com/users/Karvi95/orgs", "repos_url": "https://api.github.com/users/Karvi95/repos", "events_url": "https://api.github.com/users/Karvi95/events{/privacy}", "received_events_url": "https://api.github.com/users/Karvi95/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-29T18:21:12Z", "updated_at": "2019-02-06T15:03:13Z", "closed_at": "2018-06-01T23:31:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! Thank you very much for this innovative algorithm; heretofore have gotten a lot of mileage out of it and the accompanying well-written docs.\r\n\r\nI'm attempting to cluster a dataset of shape: (8870443, 2); it's basically an numpy array of lat/longs converted into radians.\r\n\r\nThe corresponding code looks like:\r\n```\r\nimport hdbscan\r\nfrom joblib import Memory\r\nmem = Memory(cachedir='/tmp/joblib')\r\n\r\nInRadians = np.radians(df[['lat', 'lon']])\r\ndata = np.array(InRadians)\r\n\r\nclusterer = hdbscan.HDBSCAN(metric='haversine', min_cluster_size=3, algorithm='prims_balltree', core_dist_n_jobs=8, memory=mem).fit(data) \r\n```\r\n.\r\n\r\nThis hasn't finished even though it's been running for 3 hours.\r\n\r\nI have read Issue https://github.com/scikit-learn-contrib/hdbscan/issues/27 but because I'm using the haversine metric I had to switch the algorithm from 'prims_kdtree' to be 'prims_balltree'. The other useful part of that thread was including the core_dist_n_jobs which I set to 8. Other than that though, I haven't been able to find anything to resolve my problem.\r\n\r\nIs this there anything more that I could do to speed up the performance (short of upgrading hardware)?\r\n\r\nHardware Specs:\r\nOS Name:                   Microsoft Windows 10 Enterprise\r\nOS Version:                10.0.16299 N/A Build 16299\r\nProcessor(s):               Intel64 Family 6 Model 78 Stepping 3 GenuineIntel ~802 Mhz\r\nTotal Physical Memory:     16,239 MB\r\nAvailable Physical Memory: 5,646 MB\r\nVirtual Memory: Max Size:  21,359 MB\r\nVirtual Memory: Available: 8,495 MB\r\nVirtual Memory: In Use:    12,864 MB\r\n\r\nI tried attaching the file in the comment but it exceeds 10MB...\r\n\r\n\r\nUPDATE: \r\nIt finished; here's the stack trace:\r\n```\r\n[Memory] Calling hdbscan.hdbscan_._hdbscan_prims_balltree...\r\n_hdbscan_prims_balltree(array([[ 0.830357, -2.132278],\r\n       ...,\r\n       [ 0.831464, -2.131725]]), 1000, 1.0, 'haversine', None, 40, False)\r\n---------------------------------------------------------------------------\r\nMemoryError                               Traceback (most recent call last)\r\n<ipython-input-5-b1938e51c86b> in <module>()\r\n      8 #### 5. Extract the stable clusters from the condensed tree.\r\n      9 \r\n---> 10 clusterer = hdbscan.HDBSCAN(metric='haversine', min_cluster_size=3, min_samples=1000, algorithm='prims_balltree', core_dist_n_jobs=8, memory=mem).fit(data)\r\n     11 labels = clusterer.labels_\r\n\r\nc:\\program files (x86)\\microsoft visual studio\\shared\\python36_64\\lib\\site-packages\\hdbscan\\hdbscan_.py in fit(self, X, y)\r\n    814          self._condensed_tree,\r\n    815          self._single_linkage_tree,\r\n--> 816          self._min_spanning_tree) = hdbscan(X, **kwargs)\r\n    817 \r\n    818         if self.prediction_data:\r\n\r\nc:\\program files (x86)\\microsoft visual studio\\shared\\python36_64\\lib\\site-packages\\hdbscan\\hdbscan_.py in hdbscan(X, min_cluster_size, min_samples, alpha, metric, p, leaf_size, algorithm, memory, approx_min_span_tree, gen_min_span_tree, core_dist_n_jobs, cluster_selection_method, allow_single_cluster, match_reference_implementation, **kwargs)\r\n    496                 _hdbscan_prims_balltree)(X, min_samples, alpha,\r\n    497                                          metric, p, leaf_size,\r\n--> 498                                          gen_min_span_tree, **kwargs)\r\n    499         elif algorithm == 'boruvka_kdtree':\r\n    500             if metric not in BallTree.valid_metrics:\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\joblib\\memory.py in __call__(self, *args, **kwargs)\r\n    560 \r\n    561     def __call__(self, *args, **kwargs):\r\n--> 562         return self._cached_call(args, kwargs)[0]\r\n    563 \r\n    564     def __reduce__(self):\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\joblib\\memory.py in _cached_call(self, args, kwargs)\r\n    508                           'directory %s'\r\n    509                         % (name, argument_hash, output_dir))\r\n--> 510             out, metadata = self.call(*args, **kwargs)\r\n    511             if self.mmap_mode is not None:\r\n    512                 # Memmap the output at the first call to be consistent with\r\n\r\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\joblib\\memory.py in call(self, *args, **kwargs)\r\n    742         if self._verbose > 0:\r\n    743             print(format_call(self.func, args, kwargs))\r\n--> 744         output = self.func(*args, **kwargs)\r\n    745         self._persist_output(output, output_dir)\r\n    746         duration = time.time() - start_time\r\n\r\nc:\\program files (x86)\\microsoft visual studio\\shared\\python36_64\\lib\\site-packages\\hdbscan\\hdbscan_.py in _hdbscan_prims_balltree(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)\r\n    205     core_distances = tree.query(X, k=min_samples,\r\n    206                                 dualtree=True,\r\n--> 207                                 breadth_first=True)[0][:, -1].copy(order='C')\r\n    208 \r\n    209     # Mutual reachability distance is implicit in mst_linkage_core_vector\r\n\r\nsklearn\\neighbors\\binary_tree.pxi in sklearn.neighbors.ball_tree.BinaryTree.query()\r\n\r\nsklearn\\neighbors\\binary_tree.pxi in sklearn.neighbors.ball_tree.NeighborsHeap.__init__()\r\n\r\nMemoryError: \r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/208", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/208/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/208/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/208/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/208", "id": 323954010, "node_id": "MDU6SXNzdWUzMjM5NTQwMTA=", "number": 208, "title": "Extracting Nearest Neighbors from HDBSCAN", "user": {"login": "ixxie", "id": 20320695, "node_id": "MDQ6VXNlcjIwMzIwNjk1", "avatar_url": "https://avatars0.githubusercontent.com/u/20320695?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ixxie", "html_url": "https://github.com/ixxie", "followers_url": "https://api.github.com/users/ixxie/followers", "following_url": "https://api.github.com/users/ixxie/following{/other_user}", "gists_url": "https://api.github.com/users/ixxie/gists{/gist_id}", "starred_url": "https://api.github.com/users/ixxie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ixxie/subscriptions", "organizations_url": "https://api.github.com/users/ixxie/orgs", "repos_url": "https://api.github.com/users/ixxie/repos", "events_url": "https://api.github.com/users/ixxie/events{/privacy}", "received_events_url": "https://api.github.com/users/ixxie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-17T10:00:26Z", "updated_at": "2018-05-20T19:57:27Z", "closed_at": "2018-05-20T19:57:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Our pipeline leverages HDBSCAN to cluster points in a vector space. We are also interested in getting the K nearest neighbors, and knowing that HDBSCAN already uses ball trees to cluster I was wondering if they can also be made available to infer the nearest neighbors, saving us duplicated computation.\r\n\r\nThe documentation mentions the minimal spanning tree and the single linkage tree are available, but it is not entirely clear to me if these contain the information needed to infer the nearest neighbors. Is there a way to obtain it? I am open to hacking on HDBSCAN and submitting a PR if you could give some pointers on where to begin!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/202", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/202/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/202/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/202/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/202", "id": 318552672, "node_id": "MDU6SXNzdWUzMTg1NTI2NzI=", "number": 202, "title": "HDBSCAN with pca features", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-27T21:17:17Z", "updated_at": "2019-08-06T04:04:22Z", "closed_at": "2019-08-06T04:04:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am doing unsupervised learning, and using the anomalies or clusters that were formed few events as an approach to tune the hyper parameters of hdbscan. \r\n\r\nBut, even before starting with hdbscan, I am doing dimensionality reduction using PCA. But lately, if am pondering on whether using PCA is a good approach or not for dimensionality reduction before using clustering. \r\n\r\nThe reason is because, the components in PCA have different degrees of contribution from several features. The variance contribution explained by, lets say first two components can be different from next two and is also highly dependent on the feature scaling method used before PCA. \r\n\r\nSo, the reduced dimension from PCA, are not all equally important but have different weightage associated to it. Is there any way to include this varying degree of contribution from components before doing hdbscan on the compressed data.\r\n\r\nAlso, can you share you opinion on dimensionality reduction techniques that can be applied on large scale with noise especially before doing any clustering activity on it. One technique that gained lot of traction is using auto-encoders to extract `non-linear` components in the chosen latent space. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/199", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/199/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/199/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/199/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/199", "id": 316981099, "node_id": "MDU6SXNzdWUzMTY5ODEwOTk=", "number": 199, "title": "How to avoid distance matrix computation?", "user": {"login": "mattyd2", "id": 3060067, "node_id": "MDQ6VXNlcjMwNjAwNjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3060067?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mattyd2", "html_url": "https://github.com/mattyd2", "followers_url": "https://api.github.com/users/mattyd2/followers", "following_url": "https://api.github.com/users/mattyd2/following{/other_user}", "gists_url": "https://api.github.com/users/mattyd2/gists{/gist_id}", "starred_url": "https://api.github.com/users/mattyd2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mattyd2/subscriptions", "organizations_url": "https://api.github.com/users/mattyd2/orgs", "repos_url": "https://api.github.com/users/mattyd2/repos", "events_url": "https://api.github.com/users/mattyd2/events{/privacy}", "received_events_url": "https://api.github.com/users/mattyd2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-04-23T21:00:08Z", "updated_at": "2018-05-23T23:57:41Z", "closed_at": "2018-05-23T23:57:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nWhen I attempt to load a pre-computed distance matrix for HDBSCAN, the `fit()` call still runs as though I'm doing it for the first time.\r\n\r\n```\r\nimport hdbscan\r\nimport multiprocessing\r\n\r\nhdbs = hdbscan.HDBSCAN(\r\n    min_cluster_size=20,\r\n    min_samples=40,\r\n    metric=\"euclidean\",\r\n    alpha=1.0,\r\n    p=None,\r\n    memory=\"/path_to_memory_dir/\",\r\n    algorithm=\"best\",\r\n    leaf_size=40,\r\n    approx_min_span_tree=True,\r\n    gen_min_span_tree=False,\r\n    core_dist_n_jobs=multiprocessing.cpu_count(),\r\n    cluster_selection_method=\"eom\",\r\n    allow_single_cluster=False,\r\n    prediction_data=False,\r\n    match_reference_implementation=False)\r\n\r\nhdbs.fit(blobs)\r\n```\r\n\r\nNot sure what I'm doing wrong, but any guidance would be appreciated, and I'm happy to make a tutorial of it afterwards.\r\n\r\nThanks,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/195", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/195/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/195/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/195/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/195", "id": 314908882, "node_id": "MDU6SXNzdWUzMTQ5MDg4ODI=", "number": 195, "title": "Important features in a cluster", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-04-17T05:19:30Z", "updated_at": "2018-04-27T20:29:37Z", "closed_at": "2018-04-27T20:29:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nThis question is in general related to any density based clustering approach. \r\n\r\nHow to identify important features in a density based cluster since the density based approach doesn't focus solely on reducing the variance within the cluster as it's main objective. \r\n\r\nCan the core points of a cluster formed using HDBSCAN be used to identify significant features that helped in forming the cluster? \r\n\r\nCan you point me to any literature that you know of which discusses about the feature importance of density based clusters in general or for HDBSCAN ?  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/190", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/190/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/190/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/190/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/190", "id": 310310808, "node_id": "MDU6SXNzdWUzMTAzMTA4MDg=", "number": 190, "title": "Difference between normal and cosine distance variants unclear", "user": {"login": "ixxie", "id": 20320695, "node_id": "MDQ6VXNlcjIwMzIwNjk1", "avatar_url": "https://avatars0.githubusercontent.com/u/20320695?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ixxie", "html_url": "https://github.com/ixxie", "followers_url": "https://api.github.com/users/ixxie/followers", "following_url": "https://api.github.com/users/ixxie/following{/other_user}", "gists_url": "https://api.github.com/users/ixxie/gists{/gist_id}", "starred_url": "https://api.github.com/users/ixxie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ixxie/subscriptions", "organizations_url": "https://api.github.com/users/ixxie/orgs", "repos_url": "https://api.github.com/users/ixxie/repos", "events_url": "https://api.github.com/users/ixxie/events{/privacy}", "received_events_url": "https://api.github.com/users/ixxie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-01T09:28:07Z", "updated_at": "2018-04-01T16:41:16Z", "closed_at": "2018-04-01T16:41:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "HDBSCAN has two variant packages in Pypi, both with identical descriptions: the [normal variant](https://pypi.python.org/pypi/hdbscan/0.8.12), and the [cosine distance variant](https://pypi.python.org/pypi/hdbscan-with-cosine-distance/0.8.1). Both point to https://github.com/lmcinnes/hdbscan but I understand the main repo is here now, so these might need to be updated as well. Since there is no branch called *cosine distance* or something like that, I cannot see any documentation of the difference between these packages. This is being requested by reviewers downstream as I am packaging this for Nix.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/188", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/188/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/188/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/188/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/188", "id": 306079242, "node_id": "MDU6SXNzdWUzMDYwNzkyNDI=", "number": 188, "title": "How to prevent \"AttributeError: No minimum spanning tree was generated.\"", "user": {"login": "aleereza", "id": 16723702, "node_id": "MDQ6VXNlcjE2NzIzNzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16723702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aleereza", "html_url": "https://github.com/aleereza", "followers_url": "https://api.github.com/users/aleereza/followers", "following_url": "https://api.github.com/users/aleereza/following{/other_user}", "gists_url": "https://api.github.com/users/aleereza/gists{/gist_id}", "starred_url": "https://api.github.com/users/aleereza/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aleereza/subscriptions", "organizations_url": "https://api.github.com/users/aleereza/orgs", "repos_url": "https://api.github.com/users/aleereza/repos", "events_url": "https://api.github.com/users/aleereza/events{/privacy}", "received_events_url": "https://api.github.com/users/aleereza/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-16T21:16:24Z", "updated_at": "2018-03-16T23:55:09Z", "closed_at": "2018-03-16T23:55:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi\r\nhow can I find out minimum spanning tree will not be generated for the data. I have set gen_min_span_tree=True, but sometimes when I want to cluster large number of features I get this error:\r\n```\r\nAttributeError: No minimum spanning tree was generated.This may be due to optimized algorithm variations that skip explicit generation of the spanning tree.\r\n```\r\nCan I change something to have minimum spanning tree anyway?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/187", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/187/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/187/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/187/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/187", "id": 305162970, "node_id": "MDU6SXNzdWUzMDUxNjI5NzA=", "number": 187, "title": "allow inf or nan in precomputed distances", "user": {"login": "LGro", "id": 4706479, "node_id": "MDQ6VXNlcjQ3MDY0Nzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/4706479?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LGro", "html_url": "https://github.com/LGro", "followers_url": "https://api.github.com/users/LGro/followers", "following_url": "https://api.github.com/users/LGro/following{/other_user}", "gists_url": "https://api.github.com/users/LGro/gists{/gist_id}", "starred_url": "https://api.github.com/users/LGro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LGro/subscriptions", "organizations_url": "https://api.github.com/users/LGro/orgs", "repos_url": "https://api.github.com/users/LGro/repos", "events_url": "https://api.github.com/users/LGro/events{/privacy}", "received_events_url": "https://api.github.com/users/LGro/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-14T13:42:21Z", "updated_at": "2018-03-23T13:13:16Z", "closed_at": "2018-03-23T13:13:16Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi @lmcinnes, do you see a reason that speaks against allowing nan or inf in the precomputed distance matrix to indicate missing information about pairwise distances. Looking at the `mst_linkage_core` source I'm actually wondering whether that might work straight away without much adaptation.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/185", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/185/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/185/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/185/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/185", "id": 304485607, "node_id": "MDU6SXNzdWUzMDQ0ODU2MDc=", "number": 185, "title": "outlier label to an observation with low outlier score", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-12T18:17:41Z", "updated_at": "2018-03-26T17:48:59Z", "closed_at": "2018-03-26T17:48:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI ran hdbscan on some dataset and computed the membership vector and outlier score for all the observations. I noticed that some of the observations which are marked as outliers, have lower GOSH outlier score than compared to observations which are marked to a cluster label. \r\n\r\nIs that expected behavior and if so what can be the conceptual reasons behind label an observation as outlier even-though it has lower GOSH outlier score? Is membership vector taken into consideration while classifying the observation as outlier or not ?\r\n\r\nBrief info on the image attached. I have created a dataframe with membership vectors, outlier_score, cluster label. I have 16 clusters for the dataset and hence 16 integer labels for dataframe columns. \r\n\r\n![membership vector](https://user-images.githubusercontent.com/23462596/37301527-cc35c910-25f6-11e8-96fe-d163773cc4e8.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/184", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/184/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/184/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/184/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/184", "id": 304141455, "node_id": "MDU6SXNzdWUzMDQxNDE0NTU=", "number": 184, "title": "Local vs global outlier", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-11T08:52:49Z", "updated_at": "2018-03-12T15:12:11Z", "closed_at": "2018-03-12T15:12:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI read through the docs and understood that an outlier from hdbscan can either be a local or a global outlier. \r\n\r\nLocal outliers are those which are distant (or equidistant) from few cluster centers but not so much when compared to global outliers which are distant from all of the clusters center generated by hdbscan. Is there any approach to identify them distinctively using the membership vector available for outliers or any methods available in hdbscan. \r\n\r\nOne approach is to use a threshold on the membership vector. But identifying the threshold to classify an outlier into local or global can be tricky. Second approach is to use the `lambda` at which the outlier is separated from a cluster. If it is separated in the first few levels (at the top of the condensed tree), then it has high chance of being global clusters. Is it easy to find the distance/`lambda` at which it is separated from the cluster ?\r\n\r\nI am asking this question as there is a fundamental difference in these outliers positions/properties even though they are classified with a single label -1. So the outlier analysis, if done on all the available outliers, can have different interpretations than when doing the analysis distinctly on both local/global outliers\r\n\r\nThanks\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/183", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/183/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/183/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/183/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/183", "id": 303490930, "node_id": "MDU6SXNzdWUzMDM0OTA5MzA=", "number": 183, "title": "Impact of adding extreme values?", "user": {"login": "joshjacobson", "id": 1466632, "node_id": "MDQ6VXNlcjE0NjY2MzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1466632?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joshjacobson", "html_url": "https://github.com/joshjacobson", "followers_url": "https://api.github.com/users/joshjacobson/followers", "following_url": "https://api.github.com/users/joshjacobson/following{/other_user}", "gists_url": "https://api.github.com/users/joshjacobson/gists{/gist_id}", "starred_url": "https://api.github.com/users/joshjacobson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joshjacobson/subscriptions", "organizations_url": "https://api.github.com/users/joshjacobson/orgs", "repos_url": "https://api.github.com/users/joshjacobson/repos", "events_url": "https://api.github.com/users/joshjacobson/events{/privacy}", "received_events_url": "https://api.github.com/users/joshjacobson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-03-08T14:06:25Z", "updated_at": "2018-03-08T17:30:12Z", "closed_at": "2018-03-08T17:30:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've struggled with a difficult decision that's not unique to this algorithm: How to treat my missing data. That said, I believe my implementation may be uniquely situated to perform well under `hdbscan`, and am looking for feedback.\r\n\r\nMy missing data is not actually 'missing'; it does not exist because it is not applicable to that entry. Therefore, imputation does not make sense, since that would assign data where there should not be any.\r\n\r\nI also do not want to convert fields with missing entries into categoricals; I don't want to lose the meaning of the data values in that field.\r\n\r\nTherefore, I'm generating an extreme, far-outlying value for each field, and I fill that field's missing entries with that extreme, far-outlying value. This seemingly should ensure that, from a KNN perspective, 'missing' is not clustered with non-missing values for that specific field. These datapoints with 'missing' also seemingly will not have a greater possibility of being classified as outliers, unless being 'missing' is a very rare situation for other points with a similar profile.\r\n\r\nDoes this treatment seem sound? What are the drawbacks?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/182", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/182/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/182/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/182/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/182", "id": 303488732, "node_id": "MDU6SXNzdWUzMDM0ODg3MzI=", "number": 182, "title": "Is normalization beneficial for hdbscan?", "user": {"login": "joshjacobson", "id": 1466632, "node_id": "MDQ6VXNlcjE0NjY2MzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1466632?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joshjacobson", "html_url": "https://github.com/joshjacobson", "followers_url": "https://api.github.com/users/joshjacobson/followers", "following_url": "https://api.github.com/users/joshjacobson/following{/other_user}", "gists_url": "https://api.github.com/users/joshjacobson/gists{/gist_id}", "starred_url": "https://api.github.com/users/joshjacobson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joshjacobson/subscriptions", "organizations_url": "https://api.github.com/users/joshjacobson/orgs", "repos_url": "https://api.github.com/users/joshjacobson/repos", "events_url": "https://api.github.com/users/joshjacobson/events{/privacy}", "received_events_url": "https://api.github.com/users/joshjacobson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-08T13:59:46Z", "updated_at": "2018-03-08T17:30:44Z", "closed_at": "2018-03-08T17:30:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "This algorithm has the appearance of being robust to features of vastly different scale without normalization; is this reading of it correct?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/181", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/181/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/181/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/181/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/181", "id": 303451697, "node_id": "MDU6SXNzdWUzMDM0NTE2OTc=", "number": 181, "title": "Windows to Ubuntu: out of memory", "user": {"login": "dburke1984", "id": 18658674, "node_id": "MDQ6VXNlcjE4NjU4Njc0", "avatar_url": "https://avatars1.githubusercontent.com/u/18658674?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dburke1984", "html_url": "https://github.com/dburke1984", "followers_url": "https://api.github.com/users/dburke1984/followers", "following_url": "https://api.github.com/users/dburke1984/following{/other_user}", "gists_url": "https://api.github.com/users/dburke1984/gists{/gist_id}", "starred_url": "https://api.github.com/users/dburke1984/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dburke1984/subscriptions", "organizations_url": "https://api.github.com/users/dburke1984/orgs", "repos_url": "https://api.github.com/users/dburke1984/repos", "events_url": "https://api.github.com/users/dburke1984/events{/privacy}", "received_events_url": "https://api.github.com/users/dburke1984/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-08T11:44:42Z", "updated_at": "2018-03-08T15:47:30Z", "closed_at": "2018-03-08T15:47:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Firstly, thank you so much for the very useful code which you have created.\r\n\r\nI have written some clustering code which works great in windows, however when executing the code in ubuntu, the system runs out of memory. Below is the error i am getting. \r\n\r\nFile \"pipeline.py\", line 179, in <module>\r\n    main()\r\n  File \"pipeline.py\", line 173, in main\r\n    cluster_objects, xlocs0, ylocs0, xcents0, ycents0, cluster_labels, clusterer = get_clustering_tool(input_array)\r\n  File \"/home/dan/Documents/Git_Repos/pythoni/PythonApplication1/pythoni/tools/get_clusters.py\", line 71, in get_clustering_tool\r\n    cluster_objects, xlocs0, ylocs0, xcents0, ycents0, labels, clusterer = Clusterer.get_clusters(input_array)\r\n  File \"/home/dan/Documents/Git_Repos/pythoni/PythonApplication1/pythoni/localisation_processing/Clustering/Clusterer.py\", line 147, in get_clusters\r\n    get_clusters_from_locs(location_obj, number_of_frames, min_cluster_size)\r\n  File \"/home/dan/Documents/Git_Repos/pythoni/PythonApplication1/pythoni/localisation_processing/Clustering/Clusterer.py\", line 67, in get_clusters_from_locs\r\n    labels = clusterer.fit_predict(point_array)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/hdbscan/hdbscan_.py\", line 838, in fit_predict\r\n    self.fit(X)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/hdbscan/hdbscan_.py\", line 816, in fit\r\n    self._min_spanning_tree) = hdbscan(X, **kwargs)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/hdbscan/hdbscan_.py\", line 543, in hdbscan\r\n    core_dist_n_jobs, **kwargs)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/sklearn/externals/joblib/memory.py\", line 362, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/hdbscan/hdbscan_.py\", line 239, in _hdbscan_boruvka_kdtree\r\n    n_jobs=core_dist_n_jobs, **kwargs)\r\n  File \"hdbscan/_hdbscan_boruvka.pyx\", line 375, in hdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm.__init__ (hdbscan/_hdbscan_boruvka.c:5195)\r\n  File \"hdbscan/_hdbscan_boruvka.pyx\", line 411, in hdbscan._hdbscan_boruvka.KDTreeBoruvkaAlgorithm._compute_bounds (hdbscan/_hdbscan_boruvka.c:5915)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 749, in __call__\r\n    n_jobs = self._initialize_backend()\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\", line 547, in _initialize_backend\r\n    **self._backend_args)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 317, in configure\r\n    self._pool = MemmapingPool(n_jobs, **backend_args)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 600, in __init__\r\n    super(MemmapingPool, self).__init__(**poolargs)\r\n  File \"/home/dan/.local/lib/python3.5/site-packages/sklearn/externals/joblib/pool.py\", line 420, in __init__\r\n    super(PicklingPool, self).__init__(**poolargs)\r\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 168, in __init__\r\n    self._repopulate_pool()\r\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 233, in _repopulate_pool\r\n    w.start()\r\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/usr/lib/python3.5/multiprocessing/context.py\", line 267, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/usr/lib/python3.5/multiprocessing/popen_fork.py\", line 20, in __init__\r\n    self._launch(process_obj)\r\n  File \"/usr/lib/python3.5/multiprocessing/popen_fork.py\", line 67, in _launch\r\n    self.pid = os.fork()\r\nOSError: [Errno 12] Cannot allocate memory\r\n\r\nAny ideas how i can resolve this error?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/179", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/179/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/179/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/179/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/179", "id": 298480092, "node_id": "MDU6SXNzdWUyOTg0ODAwOTI=", "number": 179, "title": "Errors while exporting condensed tree to networkx", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-02-20T05:56:36Z", "updated_at": "2019-02-26T21:19:05Z", "closed_at": "2018-02-20T18:03:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to export my clusterer object to networkx and I received the following errors\r\n\r\n`clusterer.condensed_tree_.to_networkx()`\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n~/anaconda/envs/env_name/lib/python3.6/site-packages/networkx/classes/function.py in set_node_attributes(G, values, name)\r\n    646         try:  # `values` is a dict\r\n--> 647             for n, v in values.items():\r\n    648                 try:\r\n\r\nAttributeError: 'str' object has no attribute 'items'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-62-0417c92212b3> in <module>()\r\n----> 1 g = clusterer.condensed_tree_.to_networkx()\r\n\r\n~/anaconda/envs/env_name/lib/python3.6/site-packages/hdbscan/plots.py in to_networkx(self)\r\n    450             result.add_edge(row['parent'], row['child'], weight=row['lambda_val'])\r\n    451 \r\n--> 452         set_node_attributes(result, 'size', dict(self._raw_tree[['child', 'child_size']]))\r\n    453 \r\n    454         return result\r\n\r\n~/anaconda/envs/env_name/lib/python3.6/site-packages/networkx/classes/function.py in set_node_attributes(G, values, name)\r\n    652         except AttributeError:  # `values` is a constant\r\n    653             for n in G:\r\n--> 654                 G.nodes[n][name] = values\r\n    655     else:  # `values` must be dict of dict\r\n    656         for n, d in values.items():\r\n\r\nTypeError: unhashable type: 'dict'\r\n```\r\n\r\nI am not sure what is happening with exceptions in networkx caused by hdbscan ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/178", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/178/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/178/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/178/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/178", "id": 297658383, "node_id": "MDU6SXNzdWUyOTc2NTgzODM=", "number": 178, "title": "Cluster similarity", "user": {"login": "ravimulpuri", "id": 23462596, "node_id": "MDQ6VXNlcjIzNDYyNTk2", "avatar_url": "https://avatars0.githubusercontent.com/u/23462596?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ravimulpuri", "html_url": "https://github.com/ravimulpuri", "followers_url": "https://api.github.com/users/ravimulpuri/followers", "following_url": "https://api.github.com/users/ravimulpuri/following{/other_user}", "gists_url": "https://api.github.com/users/ravimulpuri/gists{/gist_id}", "starred_url": "https://api.github.com/users/ravimulpuri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ravimulpuri/subscriptions", "organizations_url": "https://api.github.com/users/ravimulpuri/orgs", "repos_url": "https://api.github.com/users/ravimulpuri/repos", "events_url": "https://api.github.com/users/ravimulpuri/events{/privacy}", "received_events_url": "https://api.github.com/users/ravimulpuri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-02-16T01:49:15Z", "updated_at": "2018-02-23T17:04:04Z", "closed_at": "2018-02-23T17:04:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I used hdscan clustering on large time series data. The dataset has a million rows and 20 dimensions  after performing dimensionality reduction. I get the results in reasonable amount of time.\r\n\r\nI can plot clusters vs time to see how the clusters are moving or emerging from one time state to another. When I look at some of the important features of a cluster using a statistical test, I find something interesting. Let's say there is a cluster c1 in time interval t1 with top feature set as f1 (ex: 5 features and all of them are high in this cluster). A new cluster c2 appears in the time interval t2 at a later stage but with same feature set as f1. And the top features in c2 are also high in value (similar characteristic to f1). \r\n\r\nIs there any way to find the similarity of these two clusters with hdbscan or with any of the function outputs from hdbscan. Since the dataset is big, I am afraid that can't use any distance matrix based analysis to find similarity unless I under-sample heavily. \r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/175", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/175/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/175/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/175/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/175", "id": 295892875, "node_id": "MDU6SXNzdWUyOTU4OTI4NzU=", "number": 175, "title": "clusterer.condensed_tree_.plot() pathologically slow for high no. clusters", "user": {"login": "thclark", "id": 7223170, "node_id": "MDQ6VXNlcjcyMjMxNzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7223170?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thclark", "html_url": "https://github.com/thclark", "followers_url": "https://api.github.com/users/thclark/followers", "following_url": "https://api.github.com/users/thclark/following{/other_user}", "gists_url": "https://api.github.com/users/thclark/gists{/gist_id}", "starred_url": "https://api.github.com/users/thclark/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thclark/subscriptions", "organizations_url": "https://api.github.com/users/thclark/orgs", "repos_url": "https://api.github.com/users/thclark/repos", "events_url": "https://api.github.com/users/thclark/events{/privacy}", "received_events_url": "https://api.github.com/users/thclark/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-09T14:42:00Z", "updated_at": "2018-02-09T19:07:29Z", "closed_at": "2018-02-09T19:07:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "## TL;DR\r\nWith a large number of clusters (high dimensional data, wide graph hierarchy), the process hangs whilst plotting the condensed tree.\r\n\r\n## Background and use case\r\nI've been using `HDBSCAN` to cluster related strings of text. My test case matrix size`X.shape` is `(8535 x 1820)`. The dataset pops out `~800` clusters, then plotting the condensed tree yields a graph with `~3000` splits.\r\n\r\n`hdbscan.plots.CondensedTree.plot()` uses the following code to render the black branch lines:\r\n```python\r\n        for xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):\r\n            axis.plot(xs, ys, color='black', linewidth=1)\r\n```\r\nHowever, the looped call to axis.plot() can exhibit pathologically slow behaviour for such high-split cases.\r\n\r\n## Reproduction\r\n\r\nRecreate the [condensed tree plot](http://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html#extract-the-clusters) for a dataset where `len(plot_data['line_xs'])` is `~3000` (`plot_data` is the dict returned from calling `clusterer.gen_plot_data()`).\r\n\r\nThe symptom (may be platform dependent??) is simply a hang of the process (or at least extremely long execution time), due to the extreme number of calls to `axis.plot()`.\r\n\r\n## Solution\r\n\r\nPull request inbound. Will be something like:\r\n```\r\ndrawlines = []\r\nfor xs, ys in zip(plot_data['line_xs'], plot_data['line_ys']):\r\n    drawlines.append(xs)\r\n    drawlines.append(ys)\r\naxis.plot(*drawlines, color='black', linewidth=1)\r\n```\r\n(with thanks to [this SO post](https://stackoverflow.com/questions/43216111/anyway-to-draw-many-lines-fast-in-python-matploblib-is-preferred-if-possible))\r\n\r\n~~HELP WANTED: Can someone tell me if that `*` argument unpacking works in python 2.7? I've only ever learned Python3!~~", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/172", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/172/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/172/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/172/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/172", "id": 294134123, "node_id": "MDU6SXNzdWUyOTQxMzQxMjM=", "number": 172, "title": "Is there a way to save the model for future prediction?", "user": {"login": "econkc", "id": 17934731, "node_id": "MDQ6VXNlcjE3OTM0NzMx", "avatar_url": "https://avatars1.githubusercontent.com/u/17934731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/econkc", "html_url": "https://github.com/econkc", "followers_url": "https://api.github.com/users/econkc/followers", "following_url": "https://api.github.com/users/econkc/following{/other_user}", "gists_url": "https://api.github.com/users/econkc/gists{/gist_id}", "starred_url": "https://api.github.com/users/econkc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/econkc/subscriptions", "organizations_url": "https://api.github.com/users/econkc/orgs", "repos_url": "https://api.github.com/users/econkc/repos", "events_url": "https://api.github.com/users/econkc/events{/privacy}", "received_events_url": "https://api.github.com/users/econkc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-02-03T18:32:06Z", "updated_at": "2018-02-04T19:10:05Z", "closed_at": "2018-02-04T19:10:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI wonder if there is a way to save the final model for future prediction? I understand that we can save a joblib object for tuning purpose that might be able to speed up the calculation but is there a way that we can just import the model back into python and use it to predict new data points without refitting the model. I am not sure if the \"generate_prediction_data()\" function is for this purpose and I cannot find a clear explanation of this function anywhere in the documentation.\r\n\r\nThanks,", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/171", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/171/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/171/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/171/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/171", "id": 292228261, "node_id": "MDU6SXNzdWUyOTIyMjgyNjE=", "number": 171, "title": "Suspicious clustering result", "user": {"login": "econkc", "id": 17934731, "node_id": "MDQ6VXNlcjE3OTM0NzMx", "avatar_url": "https://avatars1.githubusercontent.com/u/17934731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/econkc", "html_url": "https://github.com/econkc", "followers_url": "https://api.github.com/users/econkc/followers", "following_url": "https://api.github.com/users/econkc/following{/other_user}", "gists_url": "https://api.github.com/users/econkc/gists{/gist_id}", "starred_url": "https://api.github.com/users/econkc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/econkc/subscriptions", "organizations_url": "https://api.github.com/users/econkc/orgs", "repos_url": "https://api.github.com/users/econkc/repos", "events_url": "https://api.github.com/users/econkc/events{/privacy}", "received_events_url": "https://api.github.com/users/econkc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-28T18:52:36Z", "updated_at": "2018-01-30T02:58:40Z", "closed_at": "2018-01-30T02:58:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nThe clusters I have got from the algorithm looks kind of weird. Every cluster has a persistence of 1.0 and when I try to plot condense tree, it throw an error (matplotlib error): \r\n\r\n> OverflowError: cannot convert float infinity to integer\r\n\r\nI know it is hard to answer but can you guess what could be the problem? \r\n\r\nThanks,\r\n ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/167", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/167/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/167/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/167/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/167", "id": 291446305, "node_id": "MDU6SXNzdWUyOTE0NDYzMDU=", "number": 167, "title": "Playing with dimension numbers", "user": {"login": "snakers4", "id": 12515440, "node_id": "MDQ6VXNlcjEyNTE1NDQw", "avatar_url": "https://avatars0.githubusercontent.com/u/12515440?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snakers4", "html_url": "https://github.com/snakers4", "followers_url": "https://api.github.com/users/snakers4/followers", "following_url": "https://api.github.com/users/snakers4/following{/other_user}", "gists_url": "https://api.github.com/users/snakers4/gists{/gist_id}", "starred_url": "https://api.github.com/users/snakers4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snakers4/subscriptions", "organizations_url": "https://api.github.com/users/snakers4/orgs", "repos_url": "https://api.github.com/users/snakers4/repos", "events_url": "https://api.github.com/users/snakers4/events{/privacy}", "received_events_url": "https://api.github.com/users/snakers4/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-25T04:22:11Z", "updated_at": "2020-01-29T08:30:19Z", "closed_at": "2018-01-30T05:08:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "@lmcinnes \r\nThis is more of a practical observation.\r\n\r\nAll the examples [http://hdbscan.readthedocs.io/en/latest/performance_and_scalability.html](here) are based on 10-dimension vectors.\r\n\r\nIn practice I wanted to apply HDBSCAN to clustering images using CNN activations and skip connections (5000+ dimension vector). So I tried PCA and observed the following:\r\n\r\n- 500k samples * 10 - 50 dimensions seem to work fine;\r\n- Anything above 100 dimensions just stalls w/o significant memory and CPU consumption;\r\n\r\nDid not see the mentioning of dimensionality in the docs explicitly, so may be this will be of help to anybody.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/166", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/166/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/166/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/166/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/166", "id": 289790071, "node_id": "MDU6SXNzdWUyODk3OTAwNzE=", "number": 166, "title": "Change clustering based on minimum spanning tree", "user": {"login": "aleereza", "id": 16723702, "node_id": "MDQ6VXNlcjE2NzIzNzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16723702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aleereza", "html_url": "https://github.com/aleereza", "followers_url": "https://api.github.com/users/aleereza/followers", "following_url": "https://api.github.com/users/aleereza/following{/other_user}", "gists_url": "https://api.github.com/users/aleereza/gists{/gist_id}", "starred_url": "https://api.github.com/users/aleereza/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aleereza/subscriptions", "organizations_url": "https://api.github.com/users/aleereza/orgs", "repos_url": "https://api.github.com/users/aleereza/repos", "events_url": "https://api.github.com/users/aleereza/events{/privacy}", "received_events_url": "https://api.github.com/users/aleereza/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-18T21:57:54Z", "updated_at": "2018-01-18T23:56:49Z", "closed_at": "2018-01-18T23:56:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi\r\nIs there a way to change clustering result by changing minimum spanning tree data? for example I need to change distance value of an edge in minimum spanning tree and recalculate the clustering based on that, or I need to use completely different tree and do the clustering based on that.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/157", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/157/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/157/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/157/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/157", "id": 283975147, "node_id": "MDU6SXNzdWUyODM5NzUxNDc=", "number": 157, "title": "all_points_membership_vectors fails when no clusters are found", "user": {"login": "m-dz", "id": 15376817, "node_id": "MDQ6VXNlcjE1Mzc2ODE3", "avatar_url": "https://avatars2.githubusercontent.com/u/15376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/m-dz", "html_url": "https://github.com/m-dz", "followers_url": "https://api.github.com/users/m-dz/followers", "following_url": "https://api.github.com/users/m-dz/following{/other_user}", "gists_url": "https://api.github.com/users/m-dz/gists{/gist_id}", "starred_url": "https://api.github.com/users/m-dz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/m-dz/subscriptions", "organizations_url": "https://api.github.com/users/m-dz/orgs", "repos_url": "https://api.github.com/users/m-dz/repos", "events_url": "https://api.github.com/users/m-dz/events{/privacy}", "received_events_url": "https://api.github.com/users/m-dz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-21T19:03:37Z", "updated_at": "2017-12-22T14:59:40Z", "closed_at": "2017-12-22T14:59:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When no clusters are found (i.e. all points are outliers) the `all_points_membership_vectors` function fails with `ValueError: Invalid shape in axis 1: 0.` from lines:\r\nhttps://github.com/scikit-learn-contrib/hdbscan/blob/98eef9987800d3f2dd790b0d2e6834ed365ef151/hdbscan/_prediction_utils.pyx#L276-L277\r\n\r\nA quick and pretty efficient fix could be returning an array of zeros when `clusters.size == 0`:\r\n```\r\n    # Throws errors when no clusters, possible solution: return array of 0's\r\n    if clusters.size == 0:\r\n        return np.zeros(all_points.shape[0])\r\n```\r\nafter those lines:\r\nhttps://github.com/scikit-learn-contrib/hdbscan/blob/98eef9987800d3f2dd790b0d2e6834ed365ef151/hdbscan/prediction.py#L522-L524\r\n\r\nWhat would you say?\r\n\r\nEdit: Please ignore the commit referenced below, it's a bit WIP.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/154", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/154/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/154/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/154/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/154", "id": 282528544, "node_id": "MDU6SXNzdWUyODI1Mjg1NDQ=", "number": 154, "title": "Principled Way to reduce size of noise class: URGENT", "user": {"login": "pGit1", "id": 13975114, "node_id": "MDQ6VXNlcjEzOTc1MTE0", "avatar_url": "https://avatars0.githubusercontent.com/u/13975114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pGit1", "html_url": "https://github.com/pGit1", "followers_url": "https://api.github.com/users/pGit1/followers", "following_url": "https://api.github.com/users/pGit1/following{/other_user}", "gists_url": "https://api.github.com/users/pGit1/gists{/gist_id}", "starred_url": "https://api.github.com/users/pGit1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pGit1/subscriptions", "organizations_url": "https://api.github.com/users/pGit1/orgs", "repos_url": "https://api.github.com/users/pGit1/repos", "events_url": "https://api.github.com/users/pGit1/events{/privacy}", "received_events_url": "https://api.github.com/users/pGit1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-15T19:44:29Z", "updated_at": "2018-03-29T16:34:03Z", "closed_at": "2017-12-15T20:17:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! I am playing around with HDBSCAN on a dataset to see if it will work for our needs and no matter how I change the hyperparameters I keep getting a giant noise class. Ive set the cluster_selection_method to \"leaf\", Ive set min_samples=1 and even lowered the value of alpha **all** to no avail. Ideally I would want to see if the algorithm can create relatively small noise clusters but I cannot find the parameters to do so. \r\n\r\nAny help would be much appreciated.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/151", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/151/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/151/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/151/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/151", "id": 281437198, "node_id": "MDU6SXNzdWUyODE0MzcxOTg=", "number": 151, "title": "\"ValueError: zero-size array to reduction operation minimum which has no identity\" with no leafs", "user": {"login": "m-dz", "id": 15376817, "node_id": "MDQ6VXNlcjE1Mzc2ODE3", "avatar_url": "https://avatars2.githubusercontent.com/u/15376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/m-dz", "html_url": "https://github.com/m-dz", "followers_url": "https://api.github.com/users/m-dz/followers", "following_url": "https://api.github.com/users/m-dz/following{/other_user}", "gists_url": "https://api.github.com/users/m-dz/gists{/gist_id}", "starred_url": "https://api.github.com/users/m-dz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/m-dz/subscriptions", "organizations_url": "https://api.github.com/users/m-dz/orgs", "repos_url": "https://api.github.com/users/m-dz/repos", "events_url": "https://api.github.com/users/m-dz/events{/privacy}", "received_events_url": "https://api.github.com/users/m-dz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 22, "created_at": "2017-12-12T15:58:55Z", "updated_at": "2018-04-09T23:20:29Z", "closed_at": "2017-12-13T15:59:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi, I think I might by accident tracked an error related to #115 and #144, please see below:\r\n\r\nUsing the current master branch on Win 10 64 bits and Python 2.7.14\r\n\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom hdbscan import HDBSCAN\r\n\r\n\r\n# Generate data\r\ntest_data = np.array([\r\n    [0.0, 0.0],\r\n    [1.0, 1.0],\r\n    [0.8, 1.0],\r\n    [1.0, 0.8],\r\n    [0.8, 0.8]])\r\n\r\n# HDBSCAN\r\nnp.random.seed(1)\r\nhdb_unweighted = HDBSCAN(min_cluster_size=3, gen_min_span_tree=True, allow_single_cluster=True)\r\nhdb_unweighted.fit(test_data)\r\n\r\nfig = plt.figure()\r\ncd = hdb_unweighted.condensed_tree_\r\ncd.plot()\r\nfig.suptitle('Unweighted HDBSCAN condensed tree plot'); plt.show()\r\n```\r\n\r\nWhole traceback (\"anonymised\"):\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"...\\JetBrains\\PyCharm 2017.2.4\\helpers\\pydev\\pydev_run_in_console.py\", line 37, in run_file\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \".../.PyCharm2017.3/config/scratches/scratch_2.py\", line 22, in <module>\r\n    cd.plot()\r\n  File \"build\\bdist.win-amd64\\egg\\hdbscan\\plots.py\", line 321, in plot\r\n    max_rectangle_per_icicle=max_rectangles_per_icicle)\r\n  File \"build\\bdist.win-amd64\\egg\\hdbscan\\plots.py\", line 104, in get_plot_data\r\n    leaves = _get_leaves(self._raw_tree)\r\n  File \"build\\bdist.win-amd64\\egg\\hdbscan\\plots.py\", line 44, in _get_leaves\r\n    root = cluster_tree['parent'].min()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\venv_temp_hdbscan_dev_py27\\lib\\site-packages\\numpy\\core\\_methods.py\", line 29, in _amin\r\n    return umr_minimum(a, axis, None, out, keepdims)\r\nValueError: zero-size array to reduction operation minimum which has no identity\r\n```\r\n\r\nI have tracked down the problem to lines 42-45 of `plots.py`:\r\n```\r\ndef _get_leaves(condensed_tree):\r\n    cluster_tree = condensed_tree[condensed_tree['child_size'] > 1]\r\n    root = cluster_tree['parent'].min()\r\n    return _recurse_leaf_dfs(cluster_tree, root)\r\n```\r\n`cluster_tree` created here is empty, so line 44 throws an error.\r\n\r\nI am not sure if there is any solution to this except maybe plotting the `single_linkage_tree_`?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/150", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/150/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/150/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/150/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/150", "id": 281330802, "node_id": "MDU6SXNzdWUyODEzMzA4MDI=", "number": 150, "title": "Cluster color codes not matching for selected clusters in condensed_tree_ and clusterer.labels_", "user": {"login": "Sieboldianus", "id": 13646666, "node_id": "MDQ6VXNlcjEzNjQ2NjY2", "avatar_url": "https://avatars1.githubusercontent.com/u/13646666?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sieboldianus", "html_url": "https://github.com/Sieboldianus", "followers_url": "https://api.github.com/users/Sieboldianus/followers", "following_url": "https://api.github.com/users/Sieboldianus/following{/other_user}", "gists_url": "https://api.github.com/users/Sieboldianus/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sieboldianus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sieboldianus/subscriptions", "organizations_url": "https://api.github.com/users/Sieboldianus/orgs", "repos_url": "https://api.github.com/users/Sieboldianus/repos", "events_url": "https://api.github.com/users/Sieboldianus/events{/privacy}", "received_events_url": "https://api.github.com/users/Sieboldianus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 202788228, "node_id": "MDU6TGFiZWwyMDI3ODgyMjg=", "url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/labels/bug", "name": "bug", "color": "fc2929", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-12-12T10:25:21Z", "updated_at": "2017-12-13T12:37:13Z", "closed_at": "2017-12-13T12:10:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "For some reason, the colors of selected clusters in condensed_tree_ don't seem to match the colors assigned to clusterer.labels_:\r\n![compcolor1](https://user-images.githubusercontent.com/13646666/33879091-a1c90fb2-df2d-11e7-963e-2f9697ae7eea.png)\r\n![comcolor2](https://user-images.githubusercontent.com/13646666/33879097-a367e1f4-df2d-11e7-8852-ecf70f04d7cf.png)\r\n\r\nOrange in Fig. 2 is assigned to a relatively small cluster (below 50 points), but in Fig. 1, orange is assigned to the largest cluster of about 128 points (of total 277 points in my test dataset). I have the feeling red in Fig. 2 refers to the orange cluster in Fig. 1.\r\n\r\nThis is the output of all clusters from the tree:\r\n\r\n```python\r\npdtree = clusterer.condensed_tree_.to_pandas()\r\nprint(pdtree[pdtree.child_size > 1])\r\n```\r\n\r\n```\r\n     parent  child    lambda_val  child_size\r\n14      277    278  14263.422977          20\r\n15      277    279  14263.422977         243\r\n57      279    280  22791.396195         130\r\n58      279    281  22791.396195          92\r\n70      281    282  24987.788939          12\r\n71      281    283  24987.788939          75\r\n77      283    284  27216.038608          28\r\n78      283    285  27216.038608          46\r\n163     280    286  53545.282797          12\r\n164     280    287  53545.282797          87\r\n229     287    288  80239.500772          36\r\n230     287    289  80239.500772          22\r\n```\r\n\r\nAnd this is my code for assigning colors to both the condensed_tree_ and clusterer.labels_:\r\n```python\r\nclusterer = hdbscan.HDBSCAN(min_cluster_size=10)\r\nclusterer.fit(test_data)\r\npalette = sns.color_palette()\r\ncluster_colors = [sns.desaturate(palette[col], sat) \r\n                  if col >= 0 else (0.5, 0.5, 0.5) for col, sat in \r\n                  zip(clusterer.labels_, clusterer.probabilities_)]\r\nplt.scatter(test_data.T[0], test_data.T[1], c=cluster_colors, **plot_kwds)\r\nclusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())\r\n```\r\n\r\nI am referring to the description [here](http://hdbscan.readthedocs.io/en/latest/advanced_hdbscan.html)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/145", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/145/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/145/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/145/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/145", "id": 277903862, "node_id": "MDU6SXNzdWUyNzc5MDM4NjI=", "number": 145, "title": "Problem importing HDBSCAN in Jupyter and Spyder", "user": {"login": "econkc", "id": 17934731, "node_id": "MDQ6VXNlcjE3OTM0NzMx", "avatar_url": "https://avatars1.githubusercontent.com/u/17934731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/econkc", "html_url": "https://github.com/econkc", "followers_url": "https://api.github.com/users/econkc/followers", "following_url": "https://api.github.com/users/econkc/following{/other_user}", "gists_url": "https://api.github.com/users/econkc/gists{/gist_id}", "starred_url": "https://api.github.com/users/econkc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/econkc/subscriptions", "organizations_url": "https://api.github.com/users/econkc/orgs", "repos_url": "https://api.github.com/users/econkc/repos", "events_url": "https://api.github.com/users/econkc/events{/privacy}", "received_events_url": "https://api.github.com/users/econkc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-11-29T20:57:00Z", "updated_at": "2017-11-30T00:28:52Z", "closed_at": "2017-11-30T00:28:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI try to import hdbscan in both Jupyter 5.0 and Spyder 3.2 on macOSSierra 10.12.6 but I have got this error message.\r\n\r\n```\r\n/Library/Python/2.7/site-packages/hdbscan/__init__.py in <module>()\r\n----> 1 from .hdbscan_ import HDBSCAN, hdbscan\r\n      2 from .robust_single_linkage_ import RobustSingleLinkage, robust_single_linkage\r\n      3 from .validity import validity_index\r\n      4 from .prediction import approximate_predict, membership_vector, all_points_membership_vectors\r\n      5 \r\n\r\n/Library/Python/2.7/site-packages/hdbscan/hdbscan_.py in <module>()\r\n     19 from scipy.sparse import csgraph\r\n     20 \r\n---> 21 from ._hdbscan_linkage import (single_linkage,\r\n     22                                mst_linkage_core,\r\n     23                                mst_linkage_core_vector,\r\n\r\nImportError: dlopen(/Library/Python/2.7/site-packages/hdbscan/_hdbscan_linkage.so, 2): Symbol not found: _PyInt_Type\r\n  Referenced from: /Library/Python/2.7/site-packages/hdbscan/_hdbscan_linkage.so\r\n  Expected in: flat namespace\r\n in /Library/Python/2.7/site-packages/hdbscan/_hdbscan_linkage.so\r\n```\r\n\r\nIt works fine on Spyder 2.7 on my another mac (same OS)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/140", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/140/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/140/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/140/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/140", "id": 274858463, "node_id": "MDU6SXNzdWUyNzQ4NTg0NjM=", "number": 140, "title": "pip install error under python2.7", "user": {"login": "fjaellet", "id": 6987187, "node_id": "MDQ6VXNlcjY5ODcxODc=", "avatar_url": "https://avatars2.githubusercontent.com/u/6987187?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjaellet", "html_url": "https://github.com/fjaellet", "followers_url": "https://api.github.com/users/fjaellet/followers", "following_url": "https://api.github.com/users/fjaellet/following{/other_user}", "gists_url": "https://api.github.com/users/fjaellet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjaellet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjaellet/subscriptions", "organizations_url": "https://api.github.com/users/fjaellet/orgs", "repos_url": "https://api.github.com/users/fjaellet/repos", "events_url": "https://api.github.com/users/fjaellet/events{/privacy}", "received_events_url": "https://api.github.com/users/fjaellet/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-17T13:02:53Z", "updated_at": "2017-11-17T14:23:29Z", "closed_at": "2017-11-17T14:23:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI tried to install hdbscan with pip2, and ran into the following error that I can't seem to resolve.\r\n\r\n```\r\n> sudo pip2 install hdbscan\r\nCollecting hdbscan\r\n  Using cached hdbscan-0.8.11.tar.gz\r\nRequirement already satisfied: scikit-learn>=0.16 in /usr/lib64/python2.7/site-packages (from hdbscan)\r\nInstalling collected packages: hdbscan\r\n  Running setup.py install for hdbscan ... error\r\n    Complete output from command /usr/bin/python2 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-nQcYx1/hdbscan/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-jwPnNV-record/install-record.txt --single-version-externally-managed --compile:\r\n    /tmp/pip-build-nQcYx1/hdbscan/setup.py:8: UserWarning: No module named Cython.Distutils\r\n      warnings.warn(e.args[0])\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.linux-x86_64-2.7\r\n    creating build/lib.linux-x86_64-2.7/hdbscan\r\n    copying hdbscan/hdbscan_.py -> build/lib.linux-x86_64-2.7/hdbscan\r\n    copying hdbscan/plots.py -> build/lib.linux-x86_64-2.7/hdbscan\r\n    copying hdbscan/prediction.py -> build/lib.linux-x86_64-2.7/hdbscan\r\n    copying hdbscan/validity.py -> build/lib.linux-x86_64-2.7/hdbscan\r\n    copying hdbscan/robust_single_linkage_.py -> build/lib.linux-x86_64-2.7/hdbscan\r\n    copying hdbscan/__init__.py -> build/lib.linux-x86_64-2.7/hdbscan\r\n    running build_ext\r\n    building 'hdbscan._hdbscan_tree' extension\r\n    creating build/temp.linux-x86_64-2.7\r\n    creating build/temp.linux-x86_64-2.7/hdbscan\r\n    gcc -pthread -fno-strict-aliasing -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables -fasynchronous-unwind-tables -g -DNDEBUG -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables -fasynchronous-unwind-tables -g -DOPENSSL_LOAD_CONF -fwrapv -fPIC -I/usr/lib64/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c hdbscan/_hdbscan_tree.c -o build/temp.linux-x86_64-2.7/hdbscan/_hdbscan_tree.o\r\n    hdbscan/_hdbscan_tree.c:274:31: fatal error: numpy/arrayobject.h: No such file or directory\r\n     #include \"numpy/arrayobject.h\"\r\n                                   ^\r\n    compilation terminated.\r\n    error: command 'gcc' failed with exit status 1\r\n    \r\n    ----------------------------------------\r\nCommand \"/usr/bin/python2 -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-nQcYx1/hdbscan/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-jwPnNV-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-nQcYx1/hdbscan/\r\n```\r\n\r\nTo me it looks like there is a missing space after the \"-I\" in the line \r\n`gcc ... -I/usr/lib64/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7`\r\nAny idea how to fix this? Manual install yielded the same error. \r\n\r\nI am on OpenSUSE 42.3; python 2.7, numpy 1.13.3, scipy 1.0.0, cython 0.27.3, sklearn 0.19.1\r\n\r\nThanks!\r\n\\friedrich.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/138", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/138/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/138/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/138/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/138", "id": 274108604, "node_id": "MDU6SXNzdWUyNzQxMDg2MDQ=", "number": 138, "title": "Example BUG: Bug in the example in the Readme", "user": {"login": "ronpik", "id": 6642570, "node_id": "MDQ6VXNlcjY2NDI1NzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6642570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ronpik", "html_url": "https://github.com/ronpik", "followers_url": "https://api.github.com/users/ronpik/followers", "following_url": "https://api.github.com/users/ronpik/following{/other_user}", "gists_url": "https://api.github.com/users/ronpik/gists{/gist_id}", "starred_url": "https://api.github.com/users/ronpik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ronpik/subscriptions", "organizations_url": "https://api.github.com/users/ronpik/orgs", "repos_url": "https://api.github.com/users/ronpik/repos", "events_url": "https://api.github.com/users/ronpik/events{/privacy}", "received_events_url": "https://api.github.com/users/ronpik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-11-15T10:37:16Z", "updated_at": "2017-11-19T15:49:36Z", "closed_at": "2017-11-19T15:49:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "In line 7 of the example presented in the Readme, there is a bug:\r\nrunning the line `alt_labels = hierarchy.get_clusters(0.100, 5)` resulted with the following exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n...\r\n    alt_labels = hierarchy.get_clusters(0.100, 5)\r\n  File \"/home/dev/.local/lib/python2.7/site-packages/hdbscan/plots.py\", line 686, in get_clusters\r\n    return labelling_at_cut(self._linkage, cut_distance, min_cluster_size)\r\nTypeError: Argument 'linkage' has incorrect type (expected numpy.ndarray, got SingleLinkageTree)\r\n```\r\n\r\nIt can be solved by changing line 6 to `hierarchy = clusterer.cluster_hierarchy_._linkage`\r\n\r\nrunning python 2.7\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/134", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/134/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/134/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/134/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/134", "id": 270654591, "node_id": "MDU6SXNzdWUyNzA2NTQ1OTE=", "number": 134, "title": "Pickled hdbscan object is very large", "user": {"login": "eivindlm", "id": 511531, "node_id": "MDQ6VXNlcjUxMTUzMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/511531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eivindlm", "html_url": "https://github.com/eivindlm", "followers_url": "https://api.github.com/users/eivindlm/followers", "following_url": "https://api.github.com/users/eivindlm/following{/other_user}", "gists_url": "https://api.github.com/users/eivindlm/gists{/gist_id}", "starred_url": "https://api.github.com/users/eivindlm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eivindlm/subscriptions", "organizations_url": "https://api.github.com/users/eivindlm/orgs", "repos_url": "https://api.github.com/users/eivindlm/repos", "events_url": "https://api.github.com/users/eivindlm/events{/privacy}", "received_events_url": "https://api.github.com/users/eivindlm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-02T13:34:19Z", "updated_at": "2017-11-03T08:14:13Z", "closed_at": "2017-11-03T08:14:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "After calling cluster_obj = hdbscan.HDBSCAN(...).fit(data), I am pickling cluster_obj for later prediction use. The resulting pickle file is quite large, and I suspect the reason is the member \"_raw_data\" of class HDBSCAN. My data is typically 150,000 rows and 15 columns.\r\n\r\nCan HDBSCAN perform prediction without access to the raw data, such that it would be safe to in some way remove the raw data from the pickle file?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/132", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/132/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/132/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/132/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/132", "id": 263203201, "node_id": "MDU6SXNzdWUyNjMyMDMyMDE=", "number": 132, "title": "Identifying the influence of a feature on a sample's cluster assignment", "user": {"login": "dremekie", "id": 2873935, "node_id": "MDQ6VXNlcjI4NzM5MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/2873935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dremekie", "html_url": "https://github.com/dremekie", "followers_url": "https://api.github.com/users/dremekie/followers", "following_url": "https://api.github.com/users/dremekie/following{/other_user}", "gists_url": "https://api.github.com/users/dremekie/gists{/gist_id}", "starred_url": "https://api.github.com/users/dremekie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dremekie/subscriptions", "organizations_url": "https://api.github.com/users/dremekie/orgs", "repos_url": "https://api.github.com/users/dremekie/repos", "events_url": "https://api.github.com/users/dremekie/events{/privacy}", "received_events_url": "https://api.github.com/users/dremekie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-05T17:19:46Z", "updated_at": "2017-10-14T17:12:01Z", "closed_at": "2017-10-14T17:12:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Taken a set of features: `[a, b, c]` for a set of samples: X, Y, Z.\r\n\r\nIf X is \"assigned\" to cluster 1, can I answer the following question:\r\n\r\nHow much did feature `a` contribute to sample X's assignment to cluster 1?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/126", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/126/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/126/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/126/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/126", "id": 248612502, "node_id": "MDU6SXNzdWUyNDg2MTI1MDI=", "number": 126, "title": "Time Complexity / Runtime Speed for \"generic\" algorithm using Precomputed Sparse Distance Matrix", "user": {"login": "windweller", "id": 4699797, "node_id": "MDQ6VXNlcjQ2OTk3OTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/4699797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/windweller", "html_url": "https://github.com/windweller", "followers_url": "https://api.github.com/users/windweller/followers", "following_url": "https://api.github.com/users/windweller/following{/other_user}", "gists_url": "https://api.github.com/users/windweller/gists{/gist_id}", "starred_url": "https://api.github.com/users/windweller/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/windweller/subscriptions", "organizations_url": "https://api.github.com/users/windweller/orgs", "repos_url": "https://api.github.com/users/windweller/repos", "events_url": "https://api.github.com/users/windweller/events{/privacy}", "received_events_url": "https://api.github.com/users/windweller/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-08T06:25:12Z", "updated_at": "2017-08-08T19:47:17Z", "closed_at": "2017-08-08T19:47:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using a custom distance function (edit distance) so I computed sparse distance matrix (thank @lmcinnes for the advice). My other choice is to use cosine distance on feature vectors but it seems like cosine distance is not usable at this moment...\r\n\r\nMay I ask if there is a slowdown for using the precomputed sparse distance matrix? (Is it as slow as the single linkage from fastcluster/scipy?)\r\n\r\nIs there still a way to use cosine distance on feature vectors? (It was mentioned in another issue that there's a \"bug\", but I was wondering if somehow I can still use it).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/125", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/125/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/125/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/125/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/125", "id": 247882203, "node_id": "MDU6SXNzdWUyNDc4ODIyMDM=", "number": 125, "title": "Taking in Condensed Distance Array for HBSCAN.fit()", "user": {"login": "windweller", "id": 4699797, "node_id": "MDQ6VXNlcjQ2OTk3OTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/4699797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/windweller", "html_url": "https://github.com/windweller", "followers_url": "https://api.github.com/users/windweller/followers", "following_url": "https://api.github.com/users/windweller/following{/other_user}", "gists_url": "https://api.github.com/users/windweller/gists{/gist_id}", "starred_url": "https://api.github.com/users/windweller/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/windweller/subscriptions", "organizations_url": "https://api.github.com/users/windweller/orgs", "repos_url": "https://api.github.com/users/windweller/repos", "events_url": "https://api.github.com/users/windweller/events{/privacy}", "received_events_url": "https://api.github.com/users/windweller/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-08-04T01:57:05Z", "updated_at": "2017-08-07T20:55:16Z", "closed_at": "2017-08-07T20:37:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to cluster around 80000 points, and a pair-wise distance matrix would be huge and painful to compute. `sklearn` and `fastcluster`'s `linkage` method takes in a condensed distance array and it saves me time to only compute half of the distance matrix (since it's symmetric).\r\n\r\n```python\r\ndistance_matrix = pairwise_distances(blobs)\r\nclusterer = hdbscan.HDBSCAN(metric='precomputed')\r\nclusterer.fit(distance_matrix)\r\n```\r\n\r\nDoes HBSCAN take in a condensed array that's the upper triangular area of the pairwise distance matrix?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/124", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/124/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/124/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/124/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/124", "id": 247155209, "node_id": "MDU6SXNzdWUyNDcxNTUyMDk=", "number": 124, "title": "from sklearn.utils import check_array", "user": {"login": "tessaberry", "id": 13989429, "node_id": "MDQ6VXNlcjEzOTg5NDI5", "avatar_url": "https://avatars2.githubusercontent.com/u/13989429?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tessaberry", "html_url": "https://github.com/tessaberry", "followers_url": "https://api.github.com/users/tessaberry/followers", "following_url": "https://api.github.com/users/tessaberry/following{/other_user}", "gists_url": "https://api.github.com/users/tessaberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/tessaberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tessaberry/subscriptions", "organizations_url": "https://api.github.com/users/tessaberry/orgs", "repos_url": "https://api.github.com/users/tessaberry/repos", "events_url": "https://api.github.com/users/tessaberry/events{/privacy}", "received_events_url": "https://api.github.com/users/tessaberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-08-01T18:11:27Z", "updated_at": "2017-08-01T20:31:01Z", "closed_at": "2017-08-01T19:18:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "lastest versions of sklearn.utils seems to have moved to check_arrays\r\n\r\nI changed this in the following locations:\r\n\r\nobust_single_linkage_.py\r\nhdbscan_.py", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/111", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/111/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/111/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/111/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/111", "id": 230775554, "node_id": "MDU6SXNzdWUyMzA3NzU1NTQ=", "number": 111, "title": "Is this the implementation of \"Accelerated Hierarchical Density Clustering\"?", "user": {"login": "chrisranderson", "id": 5461398, "node_id": "MDQ6VXNlcjU0NjEzOTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/5461398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrisranderson", "html_url": "https://github.com/chrisranderson", "followers_url": "https://api.github.com/users/chrisranderson/followers", "following_url": "https://api.github.com/users/chrisranderson/following{/other_user}", "gists_url": "https://api.github.com/users/chrisranderson/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrisranderson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrisranderson/subscriptions", "organizations_url": "https://api.github.com/users/chrisranderson/orgs", "repos_url": "https://api.github.com/users/chrisranderson/repos", "events_url": "https://api.github.com/users/chrisranderson/events{/privacy}", "received_events_url": "https://api.github.com/users/chrisranderson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-23T16:49:25Z", "updated_at": "2017-05-23T17:36:22Z", "closed_at": "2017-05-23T17:14:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "From https://arxiv.org/pdf/1705.07321.pdf. If not, do you know where I can find it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/109", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/109/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/109/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/109/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/109", "id": 229003154, "node_id": "MDU6SXNzdWUyMjkwMDMxNTQ=", "number": 109, "title": "approximate_predict() fails on several dasets", "user": {"login": "remidomingues", "id": 5238619, "node_id": "MDQ6VXNlcjUyMzg2MTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5238619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/remidomingues", "html_url": "https://github.com/remidomingues", "followers_url": "https://api.github.com/users/remidomingues/followers", "following_url": "https://api.github.com/users/remidomingues/following{/other_user}", "gists_url": "https://api.github.com/users/remidomingues/gists{/gist_id}", "starred_url": "https://api.github.com/users/remidomingues/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/remidomingues/subscriptions", "organizations_url": "https://api.github.com/users/remidomingues/orgs", "repos_url": "https://api.github.com/users/remidomingues/repos", "events_url": "https://api.github.com/users/remidomingues/events{/privacy}", "received_events_url": "https://api.github.com/users/remidomingues/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-16T11:45:03Z", "updated_at": "2017-05-19T09:40:12Z", "closed_at": "2017-05-19T09:40:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI'm using the `approximate_predict` function on a fitted HDSBSCAN model with Python 3.4 and version 0.8.10 of the package. Everything runs well most of the time, though for quite a few dataset the prediction cannot be computed and raises an error (see the stacktrace below).\r\n\r\nI've looked a bit into the code, and it seems `cluster_tree['parent']` is empty at the prediction step which causes the error. I've also checked the fitted model, and the training data has been correctly distributed among several clusters. Note that this issue is tightly linked to the value assigned to `min_cluster_size` and that the error disappears most of the time when reducing the parameter value (though it appears with `min_cluster_size~=15` for some datasets of a few hundred samples).\r\n\r\nThis issue can be reproduced using `min_cluster_size=int(len(X) * 0.05)` and the other default parameter values for the following datasets (I'm using a sample of 80% of data that is scaled by `sklearn.preprocessing.scale`): [yeast](https://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data), [magic gamma](https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data), [wine quality](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv) and [german](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data)\r\n\r\nStacktrace below:\r\n```\r\n  File \"/usr/local/lib/python3.4/dist-packages/hdbscan/prediction.py\", line 398, in \r\n    min_samples\r\n  File \"/usr/local/lib/python3.4/dist-packages/hdbscan/prediction.py\", line 293, in _find_cluster_and_probability\r\n    tree_root = cluster_tree['parent'].min()\r\n  File \"/usr/local/lib/python3.4/dist-packages/numpy/core/_methods.py\", line 29, in _amin\r\n    return umr_minimum(a, axis, None, out, keepdims)\r\nValueError: zero-size array to reduction operation minimum which has no identity\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/107", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/107/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/107/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/107/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/107", "id": 228090038, "node_id": "MDU6SXNzdWUyMjgwOTAwMzg=", "number": 107, "title": "Extracting flat clustering while keeping the hierarchical structure but NOT using the plot?", "user": {"login": "jessiejamieson", "id": 27699407, "node_id": "MDQ6VXNlcjI3Njk5NDA3", "avatar_url": "https://avatars0.githubusercontent.com/u/27699407?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jessiejamieson", "html_url": "https://github.com/jessiejamieson", "followers_url": "https://api.github.com/users/jessiejamieson/followers", "following_url": "https://api.github.com/users/jessiejamieson/following{/other_user}", "gists_url": "https://api.github.com/users/jessiejamieson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jessiejamieson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jessiejamieson/subscriptions", "organizations_url": "https://api.github.com/users/jessiejamieson/orgs", "repos_url": "https://api.github.com/users/jessiejamieson/repos", "events_url": "https://api.github.com/users/jessiejamieson/events{/privacy}", "received_events_url": "https://api.github.com/users/jessiejamieson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-11T19:03:31Z", "updated_at": "2017-05-11T22:56:22Z", "closed_at": "2017-05-11T22:56:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Greetings!\r\n\r\nFirst, thanks for this project. It's pretty fantastic and has helped me tremendously.\r\n\r\nSecondly, I'm using a different (but similar) vis for the condensed_tree_ plot that is more effective with large and crazy data sets, but I would like to see what the selected clusters would look like in the vis I was using. However, I don't directly see how to tell which clusters were chosen without using the plot and circling the icicles that were picked, then trying to see in the data where the clustering occured. Ideally, I could do something like ...condensed_tree_.to_networkx() and an attribute for each node in the tree may be 0 if the node was not in the HDBSCAN clustering, and 1 if the node WAS in the HDBSCAN clustering. Perhaps I just don't directly see how to find the flat clustering while keeping the hierarchical data, and if that's the case, I apologize and would appreciate being pointed in the right direction!\r\n\r\nThank you!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/104", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/104/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/104/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/104/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/104", "id": 222695127, "node_id": "MDU6SXNzdWUyMjI2OTUxMjc=", "number": 104, "title": "Conda Forge Python 3.6 issue", "user": {"login": "mickohara23", "id": 27767916, "node_id": "MDQ6VXNlcjI3NzY3OTE2", "avatar_url": "https://avatars0.githubusercontent.com/u/27767916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mickohara23", "html_url": "https://github.com/mickohara23", "followers_url": "https://api.github.com/users/mickohara23/followers", "following_url": "https://api.github.com/users/mickohara23/following{/other_user}", "gists_url": "https://api.github.com/users/mickohara23/gists{/gist_id}", "starred_url": "https://api.github.com/users/mickohara23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mickohara23/subscriptions", "organizations_url": "https://api.github.com/users/mickohara23/orgs", "repos_url": "https://api.github.com/users/mickohara23/repos", "events_url": "https://api.github.com/users/mickohara23/events{/privacy}", "received_events_url": "https://api.github.com/users/mickohara23/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-04-19T10:42:08Z", "updated_at": "2017-04-26T15:00:00Z", "closed_at": "2017-04-26T15:00:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "Using lastest version of Anaconda, which is using python 3.6.\r\nIs hdbscan unavailable for python 3.6\r\n\r\nUnsatisfiableError: The following specifications were found to be in conflict:\r\n  - hdbscan -> python 3.4*\r\n  - python 3.6*\r\nUse \"conda info <package>\" to see the dependencies for each package.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/99", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/99/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/99/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/99/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/99", "id": 218314957, "node_id": "MDU6SXNzdWUyMTgzMTQ5NTc=", "number": 99, "title": "Update pypi with newer tags", "user": {"login": "loisaidasam", "id": 213281, "node_id": "MDQ6VXNlcjIxMzI4MQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/213281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loisaidasam", "html_url": "https://github.com/loisaidasam", "followers_url": "https://api.github.com/users/loisaidasam/followers", "following_url": "https://api.github.com/users/loisaidasam/following{/other_user}", "gists_url": "https://api.github.com/users/loisaidasam/gists{/gist_id}", "starred_url": "https://api.github.com/users/loisaidasam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loisaidasam/subscriptions", "organizations_url": "https://api.github.com/users/loisaidasam/orgs", "repos_url": "https://api.github.com/users/loisaidasam/repos", "events_url": "https://api.github.com/users/loisaidasam/events{/privacy}", "received_events_url": "https://api.github.com/users/loisaidasam/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-03-30T20:16:09Z", "updated_at": "2017-03-31T16:11:05Z", "closed_at": "2017-03-31T00:14:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I noticed that the latest version of hdbscan on [pypi](https://pypi.python.org/pypi/hdbscan) is `0.8.8`.\r\n\r\nAny reason why it doesn't have the updates to `0.8.10`?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/98", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/98/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/98/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/98/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/98", "id": 217905193, "node_id": "MDU6SXNzdWUyMTc5MDUxOTM=", "number": 98, "title": "MemoryError when inputting list of 100,000 strings", "user": {"login": "rf987", "id": 22036990, "node_id": "MDQ6VXNlcjIyMDM2OTkw", "avatar_url": "https://avatars3.githubusercontent.com/u/22036990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rf987", "html_url": "https://github.com/rf987", "followers_url": "https://api.github.com/users/rf987/followers", "following_url": "https://api.github.com/users/rf987/following{/other_user}", "gists_url": "https://api.github.com/users/rf987/gists{/gist_id}", "starred_url": "https://api.github.com/users/rf987/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rf987/subscriptions", "organizations_url": "https://api.github.com/users/rf987/orgs", "repos_url": "https://api.github.com/users/rf987/repos", "events_url": "https://api.github.com/users/rf987/events{/privacy}", "received_events_url": "https://api.github.com/users/rf987/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-03-29T14:58:39Z", "updated_at": "2018-11-28T09:36:54Z", "closed_at": "2017-05-24T19:13:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting a memory error when trying to cluster a large list of strings.  Here's my code:\r\n\r\n`    \r\n    from sklearn.feature_extraction.text import TfidfVectorizer\r\n    import numpy as np\r\n    import hdbscan    \r\n\r\n    vect = TfidfVectorizer(stop_words=\"english\",analyzer =\"word\",max_features=5000)\r\n    X = vect.fit_transform(text_list)\r\n    clusterer = hdbscan.HDBSCAN(min_cluster_size=smallest_cluster_size)\r\n    clusterer.fit(X)\r\n`\r\n\r\nhere is the full error I receive when calling clusterer.fit:\r\n    clusterer.fit(X)\r\n  File \"C:\\Python27\\lib\\site-packages\\hdbscan\\hdbscan_.py\", line 854, in fit\r\n    self._min_spanning_tree) = hdbscan(X, **kwargs)\r\n  File \"C:\\Python27\\lib\\site-packages\\hdbscan\\hdbscan_.py\", line 576, in hdbscan\r\n    gen_min_span_tree, **kwargs)\r\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\", line 283, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"C:\\Python27\\lib\\site-packages\\hdbscan\\hdbscan_.py\", line 123, in _hdbscan_generic\r\n    distance_matrix = pairwise_distances(X, metric=metric, **kwargs)\r\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 1240, in pairwise_distances\r\n    return _parallel_pairwise(X, Y, func, n_jobs, **kwds)\r\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 1083, in _parallel_pairwise\r\n    return func(X, Y, **kwds)\r\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 245, in euclidean_distances\r\n    distances = safe_sparse_dot(X, Y.T, dense_output=True)\r\n  File \"C:\\Python27\\lib\\site-packages\\sklearn\\utils\\extmath.py\", line 184, in safe_sparse_dot\r\n    ret = a * b\r\n  File \"C:\\Python27\\lib\\site-packages\\scipy\\sparse\\base.py\", line 369, in __mul__\r\n    return self._mul_sparse_matrix(other)\r\n  File \"C:\\Python27\\lib\\site-packages\\scipy\\sparse\\compressed.py\", line 540, in _mul_sparse_matrix\r\n    indices = np.empty(nnz, dtype=idx_dtype)\r\nMemoryError\r\n\r\n\r\nIs there a limit to how much data hdbscan can process?  Should I keep the lists small?  Or is there a way to fix this error so it can handle large datasets?\r\n\r\nI'm running Windows, 64bit, with 18Gb of ram\r\n\r\nThank you", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/96", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/96/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/96/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/96/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/96", "id": 216483905, "node_id": "MDU6SXNzdWUyMTY0ODM5MDU=", "number": 96, "title": "Prediction.approximate_predict returns nan probability values", "user": {"login": "matt24smith", "id": 8880061, "node_id": "MDQ6VXNlcjg4ODAwNjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/8880061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matt24smith", "html_url": "https://github.com/matt24smith", "followers_url": "https://api.github.com/users/matt24smith/followers", "following_url": "https://api.github.com/users/matt24smith/following{/other_user}", "gists_url": "https://api.github.com/users/matt24smith/gists{/gist_id}", "starred_url": "https://api.github.com/users/matt24smith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matt24smith/subscriptions", "organizations_url": "https://api.github.com/users/matt24smith/orgs", "repos_url": "https://api.github.com/users/matt24smith/repos", "events_url": "https://api.github.com/users/matt24smith/events{/privacy}", "received_events_url": "https://api.github.com/users/matt24smith/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-23T16:09:29Z", "updated_at": "2017-03-27T17:46:10Z", "closed_at": "2017-03-27T17:46:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I run hdbscan prediction.approximate_predict() on new points to predict, some values are returned as nan. Since these values still evaluate as float, it can be difficult to find the source of the error when using the approximate_predict function in code. Perhaps these values should be returned as 0?\r\n\r\nSome info about my data used:\r\nhdbscan clusterer object is created with bounded coordinate data. Object is generated with the following code: \r\n\r\n    clusterer = hdbscan.HDBSCAN(algorithm = 'prims_kdtree',\r\n                                min_cluster_size=8, \r\n                                min_samples=10,\r\n                                prediction_data=True, \r\n                                cluster_selection_method='leaf',\r\n                                alpha=1,\r\n                                gen_min_span_tree=False).fit(data)\r\n\r\n`data` is of type zip(longitudeData, latitudeData)\r\n`points_to_predict` is of type zip(newLongitudeData, newLatitudeData)\r\n\r\n\r\nlongitudeData, latitudeData, newLongitudeData, newLatitudeData are all verified as not containing nan values (all floats)\r\n\r\nhdbscan version 0.8.8", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/95", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/95/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/95/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/95/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/95", "id": 215604197, "node_id": "MDU6SXNzdWUyMTU2MDQxOTc=", "number": 95, "title": "Problems while running outlier detection example?", "user": {"login": "alonsopg", "id": 13632106, "node_id": "MDQ6VXNlcjEzNjMyMTA2", "avatar_url": "https://avatars1.githubusercontent.com/u/13632106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alonsopg", "html_url": "https://github.com/alonsopg", "followers_url": "https://api.github.com/users/alonsopg/followers", "following_url": "https://api.github.com/users/alonsopg/following{/other_user}", "gists_url": "https://api.github.com/users/alonsopg/gists{/gist_id}", "starred_url": "https://api.github.com/users/alonsopg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alonsopg/subscriptions", "organizations_url": "https://api.github.com/users/alonsopg/orgs", "repos_url": "https://api.github.com/users/alonsopg/repos", "events_url": "https://api.github.com/users/alonsopg/events{/privacy}", "received_events_url": "https://api.github.com/users/alonsopg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-21T01:37:05Z", "updated_at": "2017-03-21T03:55:11Z", "closed_at": "2017-03-21T03:55:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was reading the docs about outlier detection. However, when I try:\r\n```\r\nthreshold = pd.Series(clusterer.outlier_scores_).quantile(0.9)\r\noutliers = np.where(clusterer.outlier_scores_ > threshold)[0]\r\nplt.scatter(*data.T, s=50, linewidth=0, c='gray', alpha=0.25)\r\nplt.scatter(*data[outliers].T, s=50, linewidth=0, c='red', alpha=0.5)\r\n```\r\nI geta type error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-10-2dde5cfd670f> in <module>()\r\n      2 threshold = pd.Series(clusterer.outlier_scores_).quantile(0.9)\r\n      3 outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\r\n----> 4 plt.scatter(*data.T, s=50, linewidth=0, c='gray', alpha=0.25)\r\n      5 plt.scatter(*data[outliers].T, s=50, linewidth=0, c='red', alpha=0.5)\r\n      6 plt.show\r\n\r\nTypeError: scatter() got multiple values for argument 's'\r\n\r\n\r\n```\r\n\r\nAre the docs ritght?..", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/89", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/89/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/89/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/89/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/89", "id": 211208981, "node_id": "MDU6SXNzdWUyMTEyMDg5ODE=", "number": 89, "title": "Outlier detection question", "user": {"login": "dlop3469", "id": 5630214, "node_id": "MDQ6VXNlcjU2MzAyMTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/5630214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dlop3469", "html_url": "https://github.com/dlop3469", "followers_url": "https://api.github.com/users/dlop3469/followers", "following_url": "https://api.github.com/users/dlop3469/following{/other_user}", "gists_url": "https://api.github.com/users/dlop3469/gists{/gist_id}", "starred_url": "https://api.github.com/users/dlop3469/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dlop3469/subscriptions", "organizations_url": "https://api.github.com/users/dlop3469/orgs", "repos_url": "https://api.github.com/users/dlop3469/repos", "events_url": "https://api.github.com/users/dlop3469/events{/privacy}", "received_events_url": "https://api.github.com/users/dlop3469/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-01T21:09:58Z", "updated_at": "2017-03-03T19:32:41Z", "closed_at": "2017-03-03T19:32:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nFirst of all, thanks for the implementation, I think you are doing an amazing job on this project.\r\n\r\nMy question is regarding outlier/anomaly detection. I am currently building an application to identify anomalies from a set of records and sort them based on their \"importance\" (so they can be reviewed later), and the GLOSH implementation is helping a lot since it has the notion of scoring.\r\n\r\nAs documented, I can just focus on the 90th percentile, and list the results in descending order from the outlier_scores_ values.\r\n\r\nHowever, since the algorithm also labels some data as \"noise\" (-1), I am confused in how to sort those results, whether noise (global outlier) should take preference than a value with the highest outlier score. \r\n\r\nCould you give me an explanation of what would be the right approach to sort those results?\r\n\r\n Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/87", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/87/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/87/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/87/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/87", "id": 209126374, "node_id": "MDU6SXNzdWUyMDkxMjYzNzQ=", "number": 87, "title": "Experiments on clustering tweets", "user": {"login": "bluemonk482", "id": 6764450, "node_id": "MDQ6VXNlcjY3NjQ0NTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6764450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bluemonk482", "html_url": "https://github.com/bluemonk482", "followers_url": "https://api.github.com/users/bluemonk482/followers", "following_url": "https://api.github.com/users/bluemonk482/following{/other_user}", "gists_url": "https://api.github.com/users/bluemonk482/gists{/gist_id}", "starred_url": "https://api.github.com/users/bluemonk482/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bluemonk482/subscriptions", "organizations_url": "https://api.github.com/users/bluemonk482/orgs", "repos_url": "https://api.github.com/users/bluemonk482/repos", "events_url": "https://api.github.com/users/bluemonk482/events{/privacy}", "received_events_url": "https://api.github.com/users/bluemonk482/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-02-21T12:08:34Z", "updated_at": "2017-03-02T13:41:54Z", "closed_at": "2017-03-02T13:41:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Excellent implementation! Thanks guys!!\r\n\r\nI have done some experiments now trying to cluster a bunch of tweets (about 350) using hdbscan but the results I have to say, are mush worse than other more 'mainstream' ones from sklearn..\r\n\r\nI have tried:\r\n- bow with tfidf (288 dimensions)\r\n- - tfidf then computing JSD pairwise distance matrix\r\n- - tfidf then perform LSHForest (from sklearn) and generate a Euclidean distance matrix between points\r\n- tweet2vec (500 dimensions)\r\n-- tweet2vec then computing cosine pairwise distance matrix\r\n-- tweet2vec then computing Euclidean pairwise distance matrix\r\n- - tweet2vec then perform LSHForest (from sklearn) and generate a Euclidean distance matrix between point\r\n\r\n```\r\nIn [62]: clusterer = hdbscan.HDBSCAN(min_cluster_size=N, metric='precomputed')\r\n    ...: clusters = clusterer.fit_predict(sims)\r\n    ...: print('Number of clusters =', clusters.max())\r\n    ...: print(clusters)\r\n```\r\n\r\nIn most times I am getting '-1' for cluster labels (is this normal?) and quite often all instances are labelled as '-1's if my N is over 10.. I wonder where the difficulty is? Has hdbscan proven to be below average clustering tool for short and noisy text like tweets?\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/85", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/85/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/85/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/85/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/85", "id": 205621887, "node_id": "MDU6SXNzdWUyMDU2MjE4ODc=", "number": 85, "title": "Robust Single Linkage hierarchy", "user": {"login": "peekxc", "id": 11429028, "node_id": "MDQ6VXNlcjExNDI5MDI4", "avatar_url": "https://avatars2.githubusercontent.com/u/11429028?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peekxc", "html_url": "https://github.com/peekxc", "followers_url": "https://api.github.com/users/peekxc/followers", "following_url": "https://api.github.com/users/peekxc/following{/other_user}", "gists_url": "https://api.github.com/users/peekxc/gists{/gist_id}", "starred_url": "https://api.github.com/users/peekxc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peekxc/subscriptions", "organizations_url": "https://api.github.com/users/peekxc/orgs", "repos_url": "https://api.github.com/users/peekxc/repos", "events_url": "https://api.github.com/users/peekxc/events{/privacy}", "received_events_url": "https://api.github.com/users/peekxc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-06T15:40:02Z", "updated_at": "2017-02-08T04:23:12Z", "closed_at": "2017-02-08T04:23:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is less an issue and more a question for @lmcinnes, since github doesn't have messaging. \r\n\r\nIt was noted in the main page that the \"Robust Single Linkage Hierarchy\" is also available for performing cutting. It seems to me that this is equivalent to the \"HDBSCAN* hierarchy.\" But I randomly skimmed the 2015 HDBSCAN paper and noticed there was not reference in the paper to the 2010 paper \"Rates of convergence of the cluster tree\" by Chaudhuri and DasGupta. \r\n\r\nIt seems intuitive to make this jump that they are equivalent, but seeing as it wasn't \"proven\" or anything in the paper, who figured that out? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/84", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/84/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/84/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/84/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/84", "id": 203901579, "node_id": "MDU6SXNzdWUyMDM5MDE1Nzk=", "number": 84, "title": "some cluster_persistence_ outputs are greater than 1?", "user": {"login": "titaniumrain", "id": 4322892, "node_id": "MDQ6VXNlcjQzMjI4OTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/4322892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/titaniumrain", "html_url": "https://github.com/titaniumrain", "followers_url": "https://api.github.com/users/titaniumrain/followers", "following_url": "https://api.github.com/users/titaniumrain/following{/other_user}", "gists_url": "https://api.github.com/users/titaniumrain/gists{/gist_id}", "starred_url": "https://api.github.com/users/titaniumrain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/titaniumrain/subscriptions", "organizations_url": "https://api.github.com/users/titaniumrain/orgs", "repos_url": "https://api.github.com/users/titaniumrain/repos", "events_url": "https://api.github.com/users/titaniumrain/events{/privacy}", "received_events_url": "https://api.github.com/users/titaniumrain/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-01-29T22:29:14Z", "updated_at": "2017-02-04T09:33:22Z", "closed_at": "2017-02-03T03:53:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nQuick question though, according to HDBSCAN documentation, if my interpretation is correct cluster_persistence_ outputs should be [0, 1]. \r\n\r\nHowever, I got some weird results like 2.81 as shown below.\r\n\r\n\r\nimport numpy as np\r\nnp.round(clusters.cluster_persistence_, 2)\r\n\u200b\r\nOut[34]:\r\narray([ 2.81,  0.  ,  0.44,  0.  ,  0.  ,  0.  ,  0.02,  0.  ,  0.  ,\r\n        0.  ,  0.  ,  0.11,  0.01,  0.  ,  0.01,  0.  ,  0.  ,  0.  ,\r\n        0.01,  0.  ,  0.04,  0.07,  0.  ,  0.03,  0.  ,  0.  ,  0.  ,\r\n        0.01,  0.  ,  0.  ,  0.01,  0.  ,  0.  ,  0.  ,  0.01,  0.01,\r\n        0.03,  0.  ,  0.03,  0.  ,  0.01,  0.01,  0.01,  0.01,  0.  ,\r\n        0.02,  0.  ,  0.18,  0.  ,  0.  ,  0.  ,  1.06,  0.37,  0.61,\r\n        0.77,  0.  ,  0.09,  0.  ,  0.23,  0.74,  0.  ,  0.24,  1.1 ,\r\n        0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.01,  0.01,  0.01,\r\n        0.01,  0.  ,  0.  ,  0.  ,  0.  ,  0.06,  0.  ,  0.  ,  0.  ,\r\n        0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.03,  0.01,  0.  ,\r\n        0.  ,  0.  ,  0.  ,  0.  ,  0.01,  0.  ,  0.03,  0.03,  0.01,\r\n        0.04,  0.  ,  0.  ,  0.  ,  0.01,  0.  ,  0.01,  0.  ,  0.  ,\r\n        0.  ,  0.  ,  0.01,  0.  ,  0.01,  0.  ,  0.  ,  0.02,  0.  ,  0.  ])\r\n\r\n\r\nAm I doing things wrong?\r\n\r\nCheers,\r\n\r\nTitan\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/82", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/82/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/82/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/82/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/82", "id": 199964931, "node_id": "MDU6SXNzdWUxOTk5NjQ5MzE=", "number": 82, "title": "ValueError: Sparse distance matrix has multiple connected components!", "user": {"login": "eugmandel", "id": 80363, "node_id": "MDQ6VXNlcjgwMzYz", "avatar_url": "https://avatars2.githubusercontent.com/u/80363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eugmandel", "html_url": "https://github.com/eugmandel", "followers_url": "https://api.github.com/users/eugmandel/followers", "following_url": "https://api.github.com/users/eugmandel/following{/other_user}", "gists_url": "https://api.github.com/users/eugmandel/gists{/gist_id}", "starred_url": "https://api.github.com/users/eugmandel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eugmandel/subscriptions", "organizations_url": "https://api.github.com/users/eugmandel/orgs", "repos_url": "https://api.github.com/users/eugmandel/repos", "events_url": "https://api.github.com/users/eugmandel/events{/privacy}", "received_events_url": "https://api.github.com/users/eugmandel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-01-10T23:39:56Z", "updated_at": "2017-07-20T15:31:40Z", "closed_at": "2017-01-12T02:32:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was trying to run HDBSCAN with metric='precomputed' and getting this error: \r\n\r\n```\r\n..../python2.7/site-packages/hdbscan/hdbscan_.pyc in _hdbscan_sparse_distance_matrix(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)\r\n    172     if csgraph.connected_components(mutual_reachability_, directed=False,\r\n    173                                     return_labels=False) > 1:\r\n--> 174         raise ValueError('Sparse distance matrix has multiple connected'\r\n    175                          ' components!\\nRun hdbscan on each component.')\r\n    176 \r\n\r\nValueError: Sparse distance matrix has multiple connected components!\r\nRun hdbscan on each component.\r\n```\r\n\r\nThe pairwise distance matrix that I pass as an argument to fit method is a sparse SciPy matrix in CSR format. Could you please explain the error? Thank you!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/77", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/77/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/77/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/77/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/77", "id": 195577015, "node_id": "MDU6SXNzdWUxOTU1NzcwMTU=", "number": 77, "title": "Threshold for noisy points", "user": {"login": "rth", "id": 630936, "node_id": "MDQ6VXNlcjYzMDkzNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/630936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rth", "html_url": "https://github.com/rth", "followers_url": "https://api.github.com/users/rth/followers", "following_url": "https://api.github.com/users/rth/following{/other_user}", "gists_url": "https://api.github.com/users/rth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rth/subscriptions", "organizations_url": "https://api.github.com/users/rth/orgs", "repos_url": "https://api.github.com/users/rth/repos", "events_url": "https://api.github.com/users/rth/events{/privacy}", "received_events_url": "https://api.github.com/users/rth/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-12-14T16:23:14Z", "updated_at": "2016-12-16T10:28:42Z", "closed_at": "2016-12-16T10:28:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I was wondering if  there is a way to control how many points will be marked as \"noisy\" in HBSCAN? For instance, in [`DBSCAN`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) there is an `eps` parameter that affects the cluster structure, but also effectively how many points will be marked as noisy. \r\n\r\nLet's consider the following example (which doesn't have clearly defined dense clusters),\r\n```py\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.manifold import TSNE\r\nfrom hdbscan import HDBSCAN\r\n\r\nnp.random.seed(3)\r\nX = np.random.rand(100, 20)\r\nprojection = TSNE().fit_transform(X)\r\n\r\nplt.scatter(*projection.T)\r\n\r\nmodel = HDBSCAN(min_cluster_size=2, )\r\nmodel.fit(X)\r\ncm = plt.scatter(*projection.T, c=model.labels_, cmap='brg')\r\ncb = plt.colorbar(cm)\r\ncb.set_label('Cluster label')\r\nmask_noisy = (model.labels_ == -1)\r\nprint('Clusters found:', len(np.unique(model.labels_[~mask_noisy])))\r\nprint('Noisy points: {} / {}'.format(mask_noisy.sum(), len(mask_noisy)))\r\n```\r\nwhich returns\r\n```\r\nClusters found: 2\r\nNoisy points: 58 / 100\r\n```\r\n![test](https://cloud.githubusercontent.com/assets/630936/21188628/42a9be34-c21c-11e6-96ce-e192b3b73c85.png)\r\n\r\nmost points will be marked as noisy, and if we look at the outliers scores, `model.outlier_scores_.max() == 0.19`. \r\n  1. Is there a way to indicate to HBSCAN that a point should not be considered noisy (e.g. based on some `outlier_scores_` threshold)?\r\n  2. what would be the easiest/most efficient way to merge a noisy point back to the nearest cluster? Would it be to walk though the nodes of the minimum spanning tree, until a cluster is encountered?\r\n  3. I was also wondering how noisy points are handled in the [cluster hierarchy](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html#build-the-cluster-hierarchy) are they considered as a separate cluster or just ignored?\r\n\r\nThanks a lot!\r\nP.S: also all the classes from `hdbscan/plots.py` such as `SingleLinkageTree` etc appear to be missing from the [API documentation](https://hdbscan.readthedocs.io/en/latest/api.html)...\r\n\r\n**Edit:** OK, I just realized that this might be a duplicate of issue #72 ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/74", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/74/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/74/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/74/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/74", "id": 190916403, "node_id": "MDU6SXNzdWUxOTA5MTY0MDM=", "number": 74, "title": "Is hdbscan using Parallel from joblib", "user": {"login": "sachinruk", "id": 1410927, "node_id": "MDQ6VXNlcjE0MTA5Mjc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1410927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sachinruk", "html_url": "https://github.com/sachinruk", "followers_url": "https://api.github.com/users/sachinruk/followers", "following_url": "https://api.github.com/users/sachinruk/following{/other_user}", "gists_url": "https://api.github.com/users/sachinruk/gists{/gist_id}", "starred_url": "https://api.github.com/users/sachinruk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sachinruk/subscriptions", "organizations_url": "https://api.github.com/users/sachinruk/orgs", "repos_url": "https://api.github.com/users/sachinruk/repos", "events_url": "https://api.github.com/users/sachinruk/events{/privacy}", "received_events_url": "https://api.github.com/users/sachinruk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-11-22T06:16:08Z", "updated_at": "2016-11-24T16:27:03Z", "closed_at": "2016-11-24T16:27:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "when I try to run the following code I'm getting the warning `UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1`.\r\n\r\nJust wondering if the `RobustSingleLinkage` class specifically is using the `Parallel` class from `joblib`. Because that's the only way I can think of how parallel loops are being nested if anything.\r\n\r\nIf it is parallelising, is there a way to set the number of cores the same way it does in a normal HDBSCAN class?\r\n```\r\ndef clusterParallel(i):\r\n    global labels\r\n    \r\n    np.random.seed(i)\r\n    idx = np.random.choice(len(data2), 25000, replace=False)\r\n    clusterer = hdbscan.RobustSingleLinkage(k=30, cut=0.1)\r\n    labels[idx,i] = clusterer.fit_predict(data2[idx])[:,None]\r\n\r\nParallel(n_jobs=4)(delayed(clusterParallel)(i) for i in range(8))\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/68", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/68/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/68/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/68/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/68", "id": 183819937, "node_id": "MDU6SXNzdWUxODM4MTk5Mzc=", "number": 68, "title": "Simple 3 blobs not separatable", "user": {"login": "michaelaye", "id": 69774, "node_id": "MDQ6VXNlcjY5Nzc0", "avatar_url": "https://avatars1.githubusercontent.com/u/69774?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelaye", "html_url": "https://github.com/michaelaye", "followers_url": "https://api.github.com/users/michaelaye/followers", "following_url": "https://api.github.com/users/michaelaye/following{/other_user}", "gists_url": "https://api.github.com/users/michaelaye/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelaye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelaye/subscriptions", "organizations_url": "https://api.github.com/users/michaelaye/orgs", "repos_url": "https://api.github.com/users/michaelaye/repos", "events_url": "https://api.github.com/users/michaelaye/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelaye/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-10-18T22:30:41Z", "updated_at": "2017-01-08T19:17:23Z", "closed_at": "2016-10-30T15:23:53Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm surprised that I cannot seem to separate these blobs easily in 3 clusters?\n\n![example_blobs](https://cloud.githubusercontent.com/assets/69774/19498748/94a255ae-9547-11e6-83fe-003ad11c3218.png)\n\nI looped over all reasonable min_cluster_size values (until 30, with 90 samples for these 3 clusters) and only `min_cluster_size=3` and `min_samples_size=1` brought anything larger than 2, but it created 11, so far too extreme.\n\nHere's the example notebook for debugging:\n\nhttps://gist.github.com/9bf06c2b796b93771da85b57785009b5\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/62", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/62/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/62/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/62/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/62", "id": 174953615, "node_id": "MDU6SXNzdWUxNzQ5NTM2MTU=", "number": 62, "title": "Error in `python': double free or corruption", "user": {"login": "whllnd", "id": 1707027, "node_id": "MDQ6VXNlcjE3MDcwMjc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1707027?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whllnd", "html_url": "https://github.com/whllnd", "followers_url": "https://api.github.com/users/whllnd/followers", "following_url": "https://api.github.com/users/whllnd/following{/other_user}", "gists_url": "https://api.github.com/users/whllnd/gists{/gist_id}", "starred_url": "https://api.github.com/users/whllnd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whllnd/subscriptions", "organizations_url": "https://api.github.com/users/whllnd/orgs", "repos_url": "https://api.github.com/users/whllnd/repos", "events_url": "https://api.github.com/users/whllnd/events{/privacy}", "received_events_url": "https://api.github.com/users/whllnd/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-09-04T15:14:37Z", "updated_at": "2016-09-12T15:04:36Z", "closed_at": "2016-09-12T15:04:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\n\nfirst, let me thank you for your work. It's been great to work with so far! Today I ran into a segmentation fault regarding a specific data set. It's available at https://userpage.fu-berlin.de/mdetlefsen/_dist_p10_new.csv (18mb, though). \n\nWhen loading the file and then wrapping the original 939x939 matrix into a 940x940 matrix, it runs fine. I played around with it for a while, but couldn't figure out anything more interesting. Sample code has been the following:\n\n`dist = np.genfromtxt(\"_dist_p10_new.csv\", delimiter=\",\")`\n`clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=3, metric='precomputed')`\n`clusterer.fit(dist)`\n\nApart from this specific file, I haven't run into other problems so far. But I don't see the problem at first glance, since it's just another data file.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/61", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/61/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/61/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/61/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/61", "id": 174893179, "node_id": "MDU6SXNzdWUxNzQ4OTMxNzk=", "number": 61, "title": "PEP 8", "user": {"login": "glemaitre", "id": 7454015, "node_id": "MDQ6VXNlcjc0NTQwMTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7454015?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glemaitre", "html_url": "https://github.com/glemaitre", "followers_url": "https://api.github.com/users/glemaitre/followers", "following_url": "https://api.github.com/users/glemaitre/following{/other_user}", "gists_url": "https://api.github.com/users/glemaitre/gists{/gist_id}", "starred_url": "https://api.github.com/users/glemaitre/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glemaitre/subscriptions", "organizations_url": "https://api.github.com/users/glemaitre/orgs", "repos_url": "https://api.github.com/users/glemaitre/repos", "events_url": "https://api.github.com/users/glemaitre/events{/privacy}", "received_events_url": "https://api.github.com/users/glemaitre/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 202788230, "node_id": "MDU6TGFiZWwyMDI3ODgyMzA=", "url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "glemaitre", "id": 7454015, "node_id": "MDQ6VXNlcjc0NTQwMTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7454015?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glemaitre", "html_url": "https://github.com/glemaitre", "followers_url": "https://api.github.com/users/glemaitre/followers", "following_url": "https://api.github.com/users/glemaitre/following{/other_user}", "gists_url": "https://api.github.com/users/glemaitre/gists{/gist_id}", "starred_url": "https://api.github.com/users/glemaitre/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glemaitre/subscriptions", "organizations_url": "https://api.github.com/users/glemaitre/orgs", "repos_url": "https://api.github.com/users/glemaitre/repos", "events_url": "https://api.github.com/users/glemaitre/events{/privacy}", "received_events_url": "https://api.github.com/users/glemaitre/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "glemaitre", "id": 7454015, "node_id": "MDQ6VXNlcjc0NTQwMTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7454015?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glemaitre", "html_url": "https://github.com/glemaitre", "followers_url": "https://api.github.com/users/glemaitre/followers", "following_url": "https://api.github.com/users/glemaitre/following{/other_user}", "gists_url": "https://api.github.com/users/glemaitre/gists{/gist_id}", "starred_url": "https://api.github.com/users/glemaitre/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glemaitre/subscriptions", "organizations_url": "https://api.github.com/users/glemaitre/orgs", "repos_url": "https://api.github.com/users/glemaitre/repos", "events_url": "https://api.github.com/users/glemaitre/events{/privacy}", "received_events_url": "https://api.github.com/users/glemaitre/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2016-09-03T11:48:02Z", "updated_at": "2016-10-20T15:59:11Z", "closed_at": "2016-10-20T15:59:11Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "I was wondering if you were interested to make the code following the PEP8 style.\n\nI could take care of that if you are interested.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/58", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/58/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/58/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/58/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/58", "id": 174040417, "node_id": "MDU6SXNzdWUxNzQwNDA0MTc=", "number": 58, "title": "Cosine Metric - Memory Error", "user": {"login": "brunoalano", "id": 810642, "node_id": "MDQ6VXNlcjgxMDY0Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/810642?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brunoalano", "html_url": "https://github.com/brunoalano", "followers_url": "https://api.github.com/users/brunoalano/followers", "following_url": "https://api.github.com/users/brunoalano/following{/other_user}", "gists_url": "https://api.github.com/users/brunoalano/gists{/gist_id}", "starred_url": "https://api.github.com/users/brunoalano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brunoalano/subscriptions", "organizations_url": "https://api.github.com/users/brunoalano/orgs", "repos_url": "https://api.github.com/users/brunoalano/repos", "events_url": "https://api.github.com/users/brunoalano/events{/privacy}", "received_events_url": "https://api.github.com/users/brunoalano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-08-30T14:49:23Z", "updated_at": "2016-08-30T19:33:20Z", "closed_at": "2016-08-30T19:33:20Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I think that I've one problem at my implementation. The `cosine` metric it's giving `Memory Error` in large datasets (60k of entries with 100 dimensions). I'm trying to investigate what is happening\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/57", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/57/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/57/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/57/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/57", "id": 173298271, "node_id": "MDU6SXNzdWUxNzMyOTgyNzE=", "number": 57, "title": "A \"Predict\" Function", "user": {"login": "tobyfrancis", "id": 12557896, "node_id": "MDQ6VXNlcjEyNTU3ODk2", "avatar_url": "https://avatars3.githubusercontent.com/u/12557896?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobyfrancis", "html_url": "https://github.com/tobyfrancis", "followers_url": "https://api.github.com/users/tobyfrancis/followers", "following_url": "https://api.github.com/users/tobyfrancis/following{/other_user}", "gists_url": "https://api.github.com/users/tobyfrancis/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobyfrancis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobyfrancis/subscriptions", "organizations_url": "https://api.github.com/users/tobyfrancis/orgs", "repos_url": "https://api.github.com/users/tobyfrancis/repos", "events_url": "https://api.github.com/users/tobyfrancis/events{/privacy}", "received_events_url": "https://api.github.com/users/tobyfrancis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 202788230, "node_id": "MDU6TGFiZWwyMDI3ODgyMzA=", "url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}, {"id": 202788231, "node_id": "MDU6TGFiZWwyMDI3ODgyMzE=", "url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/labels/help%20wanted", "name": "help wanted", "color": "159818", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-08-25T20:13:41Z", "updated_at": "2017-03-09T16:44:36Z", "closed_at": "2017-03-09T16:44:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to integrate a \"predict\" function, which allows for the prediction of labels on new data? Currently, to do this task I'm using a nearest neighbor search but that's particularly inefficient. I might work on building this for myself from your codebase if my schedule is forgiving!\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/56", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/56/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/56/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/56/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/56", "id": 172899241, "node_id": "MDU6SXNzdWUxNzI4OTkyNDE=", "number": 56, "title": "Problem with special input data", "user": {"login": "PHadc", "id": 21157708, "node_id": "MDQ6VXNlcjIxMTU3NzA4", "avatar_url": "https://avatars0.githubusercontent.com/u/21157708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PHadc", "html_url": "https://github.com/PHadc", "followers_url": "https://api.github.com/users/PHadc/followers", "following_url": "https://api.github.com/users/PHadc/following{/other_user}", "gists_url": "https://api.github.com/users/PHadc/gists{/gist_id}", "starred_url": "https://api.github.com/users/PHadc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PHadc/subscriptions", "organizations_url": "https://api.github.com/users/PHadc/orgs", "repos_url": "https://api.github.com/users/PHadc/repos", "events_url": "https://api.github.com/users/PHadc/events{/privacy}", "received_events_url": "https://api.github.com/users/PHadc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-08-24T08:57:09Z", "updated_at": "2016-08-25T18:21:04Z", "closed_at": "2016-08-25T18:21:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I have implemented the code successfully with the data provided by make_blobs. \nHowever, when I change the input data with the array [[0],[0]....[0],[20]] which are a set of 0 and a 20(noise), the output (cluster.labels_) is a set of -1 which means that the input data are all noises. \nI am wondering if there is any problem.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/55", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/55/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/55/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/55/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/55", "id": 171786987, "node_id": "MDU6SXNzdWUxNzE3ODY5ODc=", "number": 55, "title": "Extract tree structure from clustering", "user": {"login": "tkosciol", "id": 6714454, "node_id": "MDQ6VXNlcjY3MTQ0NTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/6714454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tkosciol", "html_url": "https://github.com/tkosciol", "followers_url": "https://api.github.com/users/tkosciol/followers", "following_url": "https://api.github.com/users/tkosciol/following{/other_user}", "gists_url": "https://api.github.com/users/tkosciol/gists{/gist_id}", "starred_url": "https://api.github.com/users/tkosciol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tkosciol/subscriptions", "organizations_url": "https://api.github.com/users/tkosciol/orgs", "repos_url": "https://api.github.com/users/tkosciol/repos", "events_url": "https://api.github.com/users/tkosciol/events{/privacy}", "received_events_url": "https://api.github.com/users/tkosciol/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-08-17T23:50:16Z", "updated_at": "2016-08-25T18:21:28Z", "closed_at": "2016-08-25T18:21:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm wondering if it is possible to get a tree-like structure of the resulting clustering. What I'm ultimately trying to do is to get a dendogram overlaid on top of a heat map (e.g. seaborn.clustermap)\n\nFrom looking at the source code, it seems like there's a condensed tree attribute and that there's a `_raw_tree` attribute, is that what I should be looking at?\n\ncc @ElDeveloper\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/52", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/52/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/52/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/52/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/52", "id": 171520792, "node_id": "MDU6SXNzdWUxNzE1MjA3OTI=", "number": 52, "title": "Is there an R implementation of this algorithm?", "user": {"login": "zachmayer", "id": 581590, "node_id": "MDQ6VXNlcjU4MTU5MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/581590?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zachmayer", "html_url": "https://github.com/zachmayer", "followers_url": "https://api.github.com/users/zachmayer/followers", "following_url": "https://api.github.com/users/zachmayer/following{/other_user}", "gists_url": "https://api.github.com/users/zachmayer/gists{/gist_id}", "starred_url": "https://api.github.com/users/zachmayer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zachmayer/subscriptions", "organizations_url": "https://api.github.com/users/zachmayer/orgs", "repos_url": "https://api.github.com/users/zachmayer/repos", "events_url": "https://api.github.com/users/zachmayer/events{/privacy}", "received_events_url": "https://api.github.com/users/zachmayer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-08-16T21:19:53Z", "updated_at": "2016-09-22T00:20:57Z", "closed_at": "2016-08-25T18:21:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is probably a dumb question, but I found an [R implementation of dbscan](https://cran.r-project.org/web/packages/dbscan/index.html), so I'm hoping there's one of hdbscan.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/50", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/50/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/50/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/50/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/50", "id": 169984733, "node_id": "MDU6SXNzdWUxNjk5ODQ3MzM=", "number": 50, "title": "hdbscan.HDBSCAN().fit('group')", "user": {"login": "ericgcoker", "id": 8006556, "node_id": "MDQ6VXNlcjgwMDY1NTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/8006556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericgcoker", "html_url": "https://github.com/ericgcoker", "followers_url": "https://api.github.com/users/ericgcoker/followers", "following_url": "https://api.github.com/users/ericgcoker/following{/other_user}", "gists_url": "https://api.github.com/users/ericgcoker/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericgcoker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericgcoker/subscriptions", "organizations_url": "https://api.github.com/users/ericgcoker/orgs", "repos_url": "https://api.github.com/users/ericgcoker/repos", "events_url": "https://api.github.com/users/ericgcoker/events{/privacy}", "received_events_url": "https://api.github.com/users/ericgcoker/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-08-08T17:54:32Z", "updated_at": "2017-07-30T04:56:31Z", "closed_at": "2016-08-25T18:22:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "I know that the algorithm will fit very easily using a column from a full pandas dataframe, but is there an elegant solution for 'fitting' across categorical groups, either by using 'groupby.transform' or through iteration?\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/48", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/48/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/48/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/48/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/48", "id": 166224389, "node_id": "MDU6SXNzdWUxNjYyMjQzODk=", "number": 48, "title": "min_cluster_size=1", "user": {"login": "legel", "id": 1915466, "node_id": "MDQ6VXNlcjE5MTU0NjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1915466?v=4", "gravatar_id": "", "url": "https://api.github.com/users/legel", "html_url": "https://github.com/legel", "followers_url": "https://api.github.com/users/legel/followers", "following_url": "https://api.github.com/users/legel/following{/other_user}", "gists_url": "https://api.github.com/users/legel/gists{/gist_id}", "starred_url": "https://api.github.com/users/legel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/legel/subscriptions", "organizations_url": "https://api.github.com/users/legel/orgs", "repos_url": "https://api.github.com/users/legel/repos", "events_url": "https://api.github.com/users/legel/events{/privacy}", "received_events_url": "https://api.github.com/users/legel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-07-19T00:42:00Z", "updated_at": "2016-11-24T23:48:36Z", "closed_at": "2016-07-20T20:11:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Greetings,\n\nFirst, thank you for the awesome library, I've found great success with it riding on top of a word2vec -> t-SNE pipeline for a new natural language processing project called [words2map](https://github.com/overlap-ai/words2map).  I'm just a few days from publishing about our code, but am finding suddenly an unusual response around the min_cluster_size=1 edge case.\n\nSpecifically, here is a sample from my code:\n\n```\n        print vectors\n        print vectors.shape\n        clusters = HDBSCAN(min_cluster_size=1).fit_predict(vectors)\n```\n\n...which generates the following:\n\n```\n[[-6.45791257 -4.44567396]\n [-6.44261124 -4.46679372]\n [ 1.56015613  2.66220251]\n [-2.85201212 -6.22262758]\n [-2.09304712 -7.14152838]\n [-1.79593637 -5.40181427]\n [-2.85714682 -6.80640389]\n [ 4.3957132   0.9440575 ]\n [ 1.81048591  1.02327832]\n [ 5.16887682  6.7110603 ]\n [ 5.37719927  6.28808868]\n [ 0.11477621 -1.11785618]\n [ 5.04218427  6.9646349 ]\n [ 5.00133672 -1.09453604]\n [ 3.72641649 -7.25523152]\n [ 4.04915813 -6.63786898]\n [ 1.2898098  -0.4886845 ]\n [ 3.83732358 -6.94612688]\n [-5.76981671 -0.61409089]\n [-4.01244246 -0.87056213]\n [ 6.22234736  0.58425667]\n [-6.04886453  3.37668561]\n [ 5.72806943  0.43197586]\n [-5.69073762 -0.53585563]\n [-6.05174687  3.38048791]\n [-6.1931812  -7.1423447 ]\n [-6.58477538 -7.32116243]\n [-2.1166777  -6.3211911 ]\n [-7.34847743 -7.71195486]\n [ 2.35076966 -0.85595291]\n [ 1.84758212  0.54659307]\n [-5.19133828  0.36389009]\n [-1.31086939 -4.1921722 ]\n [ 1.79962948  1.58228398]\n [-8.89372872 -6.65828121]]\n(35, 2)\nTraceback (most recent call last):\n  File \"clusters.py\", line 47, in <module>\n    compute_clusters()\n  File \"clusters.py\", line 25, in compute_clusters\n    clusters = generate_clusters(topic_names, vectors_in_2D)\n  File \"/home/ubuntu/words2map/words2map.py\", line 202, in generate_clusters\n    clusters = HDBSCAN(min_cluster_size=1).fit_predict(vectors)\n  File \"/home/ubuntu/miniconda/envs/words2map/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 667, in fit_predict\n    self.fit(X)\n  File \"/home/ubuntu/miniconda/envs/words2map/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 649, in fit\n    self._min_spanning_tree) = hdbscan(X, **kwargs)\n  File \"/home/ubuntu/miniconda/envs/words2map/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 469, in hdbscan\n    return _tree_to_labels(X, single_linkage_tree, min_cluster_size) + (result_min_span_tree,)\n  File \"/home/ubuntu/miniconda/envs/words2map/lib/python2.7/site-packages/hdbscan/hdbscan_.py\", line 55, in _tree_to_labels\n    labels, probabilities, stabilities = get_clusters(condensed_tree, stability_dict)\n  File \"hdbscan/_hdbscan_tree.pyx\", line 477, in hdbscan._hdbscan_tree.get_clusters (hdbscan/_hdbscan_tree.c:9977)\n  File \"hdbscan/_hdbscan_tree.pyx\", line 520, in hdbscan._hdbscan_tree.get_clusters (hdbscan/_hdbscan_tree.c:9812)\n  File \"hdbscan/_hdbscan_tree.pyx\", line 371, in hdbscan._hdbscan_tree.do_labelling (hdbscan/_hdbscan_tree.c:7355)\n  File \"hdbscan/_hdbscan_tree.pyx\", line 269, in hdbscan._hdbscan_tree.TreeUnionFind.union_ (hdbscan/_hdbscan_tree.c:5911)\n  File \"hdbscan/_hdbscan_tree.pyx\", line 284, in hdbscan._hdbscan_tree.TreeUnionFind.find (hdbscan/_hdbscan_tree.c:6127)\nIndexError: index 98 is out of bounds for axis 0 with size 98\n```\n\nI don't receive any error when setting min_cluster_size=2, and am having trouble figuring out why and what I might need to do try to debug further.\n\nOtherwise everything has been great and I am very grateful for everything; I also recall making the min_cluster_size=1 parameter work previously, and so am not sure in what way this behavior may be data-dependent...\n\nCheers!\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/47", "repository_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan", "labels_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/47/labels{/name}", "comments_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/47/comments", "events_url": "https://api.github.com/repos/scikit-learn-contrib/hdbscan/issues/47/events", "html_url": "https://github.com/scikit-learn-contrib/hdbscan/issues/47", "id": 165987365, "node_id": "MDU6SXNzdWUxNjU5ODczNjU=", "number": 47, "title": "move to scikit-learn contrib?", "user": {"login": "amueller", "id": 449558, "node_id": "MDQ6VXNlcjQ0OTU1OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/449558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amueller", "html_url": "https://github.com/amueller", "followers_url": "https://api.github.com/users/amueller/followers", "following_url": "https://api.github.com/users/amueller/following{/other_user}", "gists_url": "https://api.github.com/users/amueller/gists{/gist_id}", "starred_url": "https://api.github.com/users/amueller/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amueller/subscriptions", "organizations_url": "https://api.github.com/users/amueller/orgs", "repos_url": "https://api.github.com/users/amueller/repos", "events_url": "https://api.github.com/users/amueller/events{/privacy}", "received_events_url": "https://api.github.com/users/amueller/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-07-17T17:49:57Z", "updated_at": "2016-09-30T16:39:49Z", "closed_at": "2016-09-30T16:39:49Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Hey.\nScikit-learn recently created a \"contrib\" repo to house related projects:\nhttps://github.com/scikit-learn-contrib/scikit-learn-contrib/blob/master/README.md\n\nThe idea is to collect scikit-learn compatible algorithms in a single place, and provide more exposure.\nIt would be great to have hdbscan there, if you want to move the repo.\n\nBest,\nAndy\n", "performed_via_github_app": null, "score": 1.0}]}