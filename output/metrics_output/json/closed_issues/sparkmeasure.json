{"total_count": 16, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/30", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/30/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/30/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/30/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/30", "id": 651741884, "node_id": "MDU6SXNzdWU2NTE3NDE4ODQ=", "number": 30, "title": "Notable difference to REST API", "user": {"login": "mansenfranzen", "id": 18086180, "node_id": "MDQ6VXNlcjE4MDg2MTgw", "avatar_url": "https://avatars0.githubusercontent.com/u/18086180?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mansenfranzen", "html_url": "https://github.com/mansenfranzen", "followers_url": "https://api.github.com/users/mansenfranzen/followers", "following_url": "https://api.github.com/users/mansenfranzen/following{/other_user}", "gists_url": "https://api.github.com/users/mansenfranzen/gists{/gist_id}", "starred_url": "https://api.github.com/users/mansenfranzen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mansenfranzen/subscriptions", "organizations_url": "https://api.github.com/users/mansenfranzen/orgs", "repos_url": "https://api.github.com/users/mansenfranzen/repos", "events_url": "https://api.github.com/users/mansenfranzen/events{/privacy}", "received_events_url": "https://api.github.com/users/mansenfranzen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-06T18:36:41Z", "updated_at": "2020-07-30T07:59:50Z", "closed_at": "2020-07-30T07:59:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi Luca and community, \r\n\r\nfirst of all - thanks for the great work - very much appreciate it! \r\n\r\nThis is not a real issue but rather an user question asking for clarification. I've been wondering if sparkMeasure provides any additional metrics than the default REST API?\r\n\r\nI would like to collect spark job metrics while keeping any dependencies as minimal as possible. Using the default REST API to collect the metrics seems simple without needing to rely on an additional package. Of course sparkMeasure provides additional abstractions to aggregate on stage/task level and to compute many relevant metrics. That is of great use. We are likely to be interested in only a few core metrics and we don't need all of them.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/29", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/29/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/29/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/29/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/29", "id": 606493243, "node_id": "MDU6SXNzdWU2MDY0OTMyNDM=", "number": 29, "title": "TaskMetrics and StageMetrics does not extend a common trait", "user": {"login": "RodrigoCSoares", "id": 34926320, "node_id": "MDQ6VXNlcjM0OTI2MzIw", "avatar_url": "https://avatars3.githubusercontent.com/u/34926320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RodrigoCSoares", "html_url": "https://github.com/RodrigoCSoares", "followers_url": "https://api.github.com/users/RodrigoCSoares/followers", "following_url": "https://api.github.com/users/RodrigoCSoares/following{/other_user}", "gists_url": "https://api.github.com/users/RodrigoCSoares/gists{/gist_id}", "starred_url": "https://api.github.com/users/RodrigoCSoares/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RodrigoCSoares/subscriptions", "organizations_url": "https://api.github.com/users/RodrigoCSoares/orgs", "repos_url": "https://api.github.com/users/RodrigoCSoares/repos", "events_url": "https://api.github.com/users/RodrigoCSoares/events{/privacy}", "received_events_url": "https://api.github.com/users/RodrigoCSoares/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-24T18:09:11Z", "updated_at": "2020-07-10T13:04:22Z", "closed_at": "2020-07-10T13:04:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "TasksMetrics and StageMetrics classes does not extend a common trait and this can cause some trouble while implementing those metrics in a generic way.\r\n\r\nAn example of a code that does not compile because of that:\r\n\r\n```\r\nval someExternalConfiguration = ExternalConfiguration.read();\r\nval dataframe = spark.sql(\"SELECT * FROM SOME_WHERE\")\r\n\r\nsomeExternalConfiguration match {\r\n    case \"stages\" => \r\n        val stagesMetrics = ch.cern.sparkmeasure.StageMetrics(spark)\r\n        Writer.doWrite(dataFrame, stagesMetrics)\r\n    case \"tasks\" => \r\n        val tasksMetrics = ch.cern.sparkmeasure.TaskMetrics(spark)\r\n        Writer.doWrite(dataFrame, tasksMetrics)\r\n}\r\n\r\nobject Writer {\r\n    def doWrite(dataFrame: DataFrame, metrics: <Here should be the common trait>) {\r\n        metrics.runAndMeasure(dataFrame.write.format(\"parquet\").save(\"/tmp/any_where\"))\r\n    }\r\n}\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/28", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/28/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/28/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/28/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/28", "id": 560126720, "node_id": "MDU6SXNzdWU1NjAxMjY3MjA=", "number": 28, "title": "Send to Prometheus not available from Python", "user": {"login": "nadenf", "id": 657041, "node_id": "MDQ6VXNlcjY1NzA0MQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/657041?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nadenf", "html_url": "https://github.com/nadenf", "followers_url": "https://api.github.com/users/nadenf/followers", "following_url": "https://api.github.com/users/nadenf/following{/other_user}", "gists_url": "https://api.github.com/users/nadenf/gists{/gist_id}", "starred_url": "https://api.github.com/users/nadenf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nadenf/subscriptions", "organizations_url": "https://api.github.com/users/nadenf/orgs", "repos_url": "https://api.github.com/users/nadenf/repos", "events_url": "https://api.github.com/users/nadenf/events{/privacy}", "received_events_url": "https://api.github.com/users/nadenf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-05T04:04:12Z", "updated_at": "2020-07-20T09:13:44Z", "closed_at": "2020-07-20T09:13:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "It is not possible to call:\r\n\r\n```\r\n  def sendReportPrometheus(serverIPnPort: String,\r\n                 metricsJob: String,\r\n                 labelName: String = sparkSession.sparkContext.appName,\r\n                 labelValue: String = sparkSession.sparkContext.applicationId): Unit\r\n```\r\n\r\nFrom Python.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/23", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/23/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/23/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/23/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/23", "id": 412443782, "node_id": "MDU6SXNzdWU0MTI0NDM3ODI=", "number": 23, "title": "spark 3.2.3 problems", "user": {"login": "dmitrybugakov", "id": 21037644, "node_id": "MDQ6VXNlcjIxMDM3NjQ0", "avatar_url": "https://avatars3.githubusercontent.com/u/21037644?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmitrybugakov", "html_url": "https://github.com/dmitrybugakov", "followers_url": "https://api.github.com/users/dmitrybugakov/followers", "following_url": "https://api.github.com/users/dmitrybugakov/following{/other_user}", "gists_url": "https://api.github.com/users/dmitrybugakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmitrybugakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmitrybugakov/subscriptions", "organizations_url": "https://api.github.com/users/dmitrybugakov/orgs", "repos_url": "https://api.github.com/users/dmitrybugakov/repos", "events_url": "https://api.github.com/users/dmitrybugakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dmitrybugakov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-02-20T14:08:00Z", "updated_at": "2019-02-25T13:08:53Z", "closed_at": "2019-02-25T13:08:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi. I'm trying use this library. \r\nSamples of code: \r\n\r\nbuild.sbt\r\n\r\n```\r\nval hdpMinorVersion = \"3.1.0.0-78\"\r\nval hadoopVersion = \"3.1.1\" + \".\" + hdpMinorVersion\r\nval sparkVersion = \"2.3.2\" + \".\" + hdpMinorVersion\r\n\r\n\r\nlazy val localResolvers = Seq(\r\n  \"mvnrepository\" at \"https://mvnrepository.com/artifact/\",\r\n  \"Hortonworks HDP\" at \"http://repo.hortonworks.com/content/repositories/releases/\",\r\n  \"Hortonworks Other Dependencies\" at \"http://repo.hortonworks.com/content/groups/public\"\r\n)\r\n\r\nval projectResolvers: Seq[Resolver] = Seq(Resolver.defaultLocal, Resolver.mavenLocal) ++ localResolvers\r\n\r\nresolvers := projectResolvers\r\n\r\nlazy val sparkDependencies = Seq(\r\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion % Provided,\r\n  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion % Provided\r\n)\r\n\r\nlazy val hdpDependencies = Seq(\r\n  \"com.hortonworks.hive\" %% \"hive-warehouse-connector\" % \"1.0.0.3.0.1.0-187\" % Provided intransitive()\r\n)\r\n\r\nlazy val staticAnalyzer = Seq(\r\n  compilerPlugin(dependency = \"org.wartremover\" %% \"wartremover\" % \"2.3.4\")\r\n)\r\n\r\nlibraryDependencies ++= sparkDependencies ++ hdpDependencies ++ staticAnalyzer ++ Seq(\r\n  \"io.monix\" %% \"monix\" % \"2.3.3\",\r\n  \"org.typelevel\" %% \"cats-core\" % \"0.9.0\",\r\n  //  \"io.monix\" %% \"monix-eval\" % \"2.3.3\",\r\n  \"ch.cern.sparkmeasure\" %% \"spark-measure\" % \"0.13\",\r\n  \"io.monix\" %% \"monix-cats\" % \"2.3.3\",\r\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.6\" % \"test\"\r\n)\r\n```\r\n\r\n\r\n\r\nspark-shell \r\n\r\n```\r\nscala> val taskMetrics = ch.cern.sparkmeasure.TaskMetrics(spark)\r\ntaskMetrics: ch.cern.sparkmeasure.TaskMetrics = TaskMetrics(org.apache.spark.sql.SparkSession@69c0bae6,false)\r\n\r\nscala> taskMetrics.runAndMeasure\r\n   def runAndMeasure[T](f: => T): T\r\n\r\nscala> taskMetrics.runAndMeasure(spark.sql(\"select * from test.test\"))\r\nHive Session ID = 04e7280b-0a45-4fad-867f-f1447faf6bf4\r\nTime taken: 4895 ms\r\n19/02/20 09:01:58 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\r\n19/02/20 09:01:58 WARN TaskMetrics: Stage metrics data refreshed into temp view PerfTaskMetrics\r\nscala.MatchError: (elapsedTime,null) (of class scala.Tuple2)                    \r\n  at ch.cern.sparkmeasure.TaskMetrics$$anonfun$report$1.apply(taskmetrics.scala:206)\r\n  at ch.cern.sparkmeasure.TaskMetrics$$anonfun$report$1.apply(taskmetrics.scala:206)\r\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\r\n  at ch.cern.sparkmeasure.TaskMetrics.report(taskmetrics.scala:206)\r\n  at ch.cern.sparkmeasure.TaskMetrics.printReport(taskmetrics.scala:215)\r\n  at ch.cern.sparkmeasure.TaskMetrics.runAndMeasure(taskmetrics.scala:282)\r\n  ... 49 elided\r\n```\r\n\r\nWhat I'm doing wrong? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/22", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/22/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/22/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/22/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/22", "id": 396380639, "node_id": "MDU6SXNzdWUzOTYzODA2Mzk=", "number": 22, "title": "jobId field only comes as \"0\" or \"1\" in stage and task Metrics.", "user": {"login": "Amittyagi007", "id": 25293890, "node_id": "MDQ6VXNlcjI1MjkzODkw", "avatar_url": "https://avatars3.githubusercontent.com/u/25293890?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Amittyagi007", "html_url": "https://github.com/Amittyagi007", "followers_url": "https://api.github.com/users/Amittyagi007/followers", "following_url": "https://api.github.com/users/Amittyagi007/following{/other_user}", "gists_url": "https://api.github.com/users/Amittyagi007/gists{/gist_id}", "starred_url": "https://api.github.com/users/Amittyagi007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Amittyagi007/subscriptions", "organizations_url": "https://api.github.com/users/Amittyagi007/orgs", "repos_url": "https://api.github.com/users/Amittyagi007/repos", "events_url": "https://api.github.com/users/Amittyagi007/events{/privacy}", "received_events_url": "https://api.github.com/users/Amittyagi007/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-01-07T07:44:31Z", "updated_at": "2019-01-09T04:19:13Z", "closed_at": "2019-01-09T04:19:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nI am evaluating sparkMeasure for my use case but I always gets jobID filed as \"0\" or a series of 1. Is this expected behavior or I am missing something here?\r\n\r\nBelow are the outputs of a spark2-submit pi job with sparkMeasure and from a spark-shell job resp.\r\n\r\n+-----+--------+-------+--------------------+\r\n|jobId|jobGroup|stageId|                name|\r\n+-----+--------+-------+--------------------+\r\n|    0|    null|      0|reduce at SparkPi...|\r\n+-----+--------+-------+--------------------+\r\n\r\n+-----+--------+\r\n|jobId|jobGroup|\r\n+-----+--------+\r\n|    0|    null|\r\n|    1|    null|\r\n|    1|    null|\r\n|    1|    null|\r\n|    1|    null|\r\n|    1|    null|\r\n|    1|    null|\r\n\r\nAlso, how can we figure out to which task a particular metric belongs to?\r\n\r\nThanks\r\nAmit ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/20", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/20/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/20/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/20/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/20", "id": 392210789, "node_id": "MDU6SXNzdWUzOTIyMTA3ODk=", "number": 20, "title": "Uncaught throwable from user code: scala.MatchError: (elapsedTime,null) (of class scala.Tuple2) (taskmetrics.scala:206)", "user": {"login": "turtlemonvh", "id": 889442, "node_id": "MDQ6VXNlcjg4OTQ0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/889442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/turtlemonvh", "html_url": "https://github.com/turtlemonvh", "followers_url": "https://api.github.com/users/turtlemonvh/followers", "following_url": "https://api.github.com/users/turtlemonvh/following{/other_user}", "gists_url": "https://api.github.com/users/turtlemonvh/gists{/gist_id}", "starred_url": "https://api.github.com/users/turtlemonvh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/turtlemonvh/subscriptions", "organizations_url": "https://api.github.com/users/turtlemonvh/orgs", "repos_url": "https://api.github.com/users/turtlemonvh/repos", "events_url": "https://api.github.com/users/turtlemonvh/events{/privacy}", "received_events_url": "https://api.github.com/users/turtlemonvh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-12-18T15:35:21Z", "updated_at": "2018-12-20T07:34:27Z", "closed_at": "2018-12-20T07:34:27Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I am seeing an occasional exception when using the `.report` method on a `TaskMetrics` to render the metrics data as a string.\r\n\r\nThe problem seems to be associated with this line:\r\nhttps://github.com/LucaCanali/sparkMeasure/blob/master/src/main/scala/ch/cern/sparkmeasure/taskmetrics.scala#L206\r\n\r\nHere is the top of the stacktrace.\r\n\r\n```\r\n18/12/17 20:53:14 ERROR Uncaught throwable from user code: scala.MatchError: (elapsedTime,null) (of class scala.Tuple2)\r\n\tat ch.cern.sparkmeasure.TaskMetrics$$anonfun$report$1.apply(taskmetrics.scala:206)\r\n\tat ch.cern.sparkmeasure.TaskMetrics$$anonfun$report$1.apply(taskmetrics.scala:206)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\r\n\tat ch.cern.sparkmeasure.TaskMetrics.report(taskmetrics.scala:206)\r\n\tat com.ionic.helperfunctions.SparkMeasureHelpers$.save(SparkMeasure.scala:116)\r\n```\r\n\r\nIt looks like this is probably likely caused by calling `.report` when there are no records in `listenerTask.taskMetricsData` (I'll try to confirm this).\r\n\r\nIf this is the case, there are a few options for fixing.  The most obvious one is to change the match statement to something like:\r\n\r\n```\r\n      .map {\r\n        case ((n: String, v: Long)) => Utils.prettyPrintValues(n, v)\r\n        case ((n: String, null)) => n + \" => null\"\r\n      }\r\n     ).mkString(\"\\n\")\r\n```\r\n\r\nBut there are, of course, other options.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/19", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/19/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/19/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/19/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/19", "id": 391578416, "node_id": "MDU6SXNzdWUzOTE1Nzg0MTY=", "number": 19, "title": "jupyter notebook example missing", "user": {"login": "catch93", "id": 8515135, "node_id": "MDQ6VXNlcjg1MTUxMzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8515135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/catch93", "html_url": "https://github.com/catch93", "followers_url": "https://api.github.com/users/catch93/followers", "following_url": "https://api.github.com/users/catch93/following{/other_user}", "gists_url": "https://api.github.com/users/catch93/gists{/gist_id}", "starred_url": "https://api.github.com/users/catch93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/catch93/subscriptions", "organizations_url": "https://api.github.com/users/catch93/orgs", "repos_url": "https://api.github.com/users/catch93/repos", "events_url": "https://api.github.com/users/catch93/events{/privacy}", "received_events_url": "https://api.github.com/users/catch93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-12-17T06:48:26Z", "updated_at": "2019-01-07T09:03:12Z", "closed_at": "2019-01-07T09:03:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am wondering if you still have a sample of jupyter notebook with sparkmeasure\r\nThe following SparkMeasure_Jupyer_Python_getting_started.ipynb is not found as of today\r\nThanks in advance", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/17", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/17/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/17/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/17/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/17", "id": 375860319, "node_id": "MDU6SXNzdWUzNzU4NjAzMTk=", "number": 17, "title": "printReport error on the complex query", "user": {"login": "Jirapong", "id": 36509, "node_id": "MDQ6VXNlcjM2NTA5", "avatar_url": "https://avatars2.githubusercontent.com/u/36509?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jirapong", "html_url": "https://github.com/Jirapong", "followers_url": "https://api.github.com/users/Jirapong/followers", "following_url": "https://api.github.com/users/Jirapong/following{/other_user}", "gists_url": "https://api.github.com/users/Jirapong/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jirapong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jirapong/subscriptions", "organizations_url": "https://api.github.com/users/Jirapong/orgs", "repos_url": "https://api.github.com/users/Jirapong/repos", "events_url": "https://api.github.com/users/Jirapong/events{/privacy}", "received_events_url": "https://api.github.com/users/Jirapong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-31T09:02:16Z", "updated_at": "2018-12-17T07:30:16Z", "closed_at": "2018-12-17T07:30:16Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I got an error after run following command \r\n\r\n```\r\nval stageMetrics = ch.cern.sparkmeasure.StageMetrics(spark) \r\n\r\nstageMetrics.begin()\r\n\r\nspark.sql(\"SELECT 1=1\")\r\n\r\nstageMetrics.end()\r\nstageMetrics.printReport()\r\n```\r\n\r\ngot following error\r\n\r\n```\r\nat ch.cern.sparkmeasure.StageMetrics$$anonfun$report$1.apply(stagemetrics.scala:184)\r\n\tat ch.cern.sparkmeasure.StageMetrics$$anonfun$report$1.apply(stagemetrics.scala:184)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\r\n\tat ch.cern.sparkmeasure.StageMetrics.report(stagemetrics.scala:184)\r\n\tat ch.cern.sparkmeasure.StageMetrics.printReport(stagemetrics.scala:193)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-652119476452024:3)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-652119476452024:48)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$$iw$$iw$$iw$$iw.<init>(command-652119476452024:50)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$$iw$$iw$$iw.<init>(command-652119476452024:52)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$$iw$$iw.<init>(command-652119476452024:54)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$$iw.<init>(command-652119476452024:56)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read.<init>(command-652119476452024:58)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$.<init>(command-652119476452024:62)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$read$.<clinit>(command-652119476452024)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$eval$.$print$lzycompute(<notebook>:7)\r\n\tat linea8b963cf2ca141a2ac1e991add9914c629.$eval$.$print(<notebook>:6)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/14", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/14/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/14/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/14/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/14", "id": 357040599, "node_id": "MDU6SXNzdWUzNTcwNDA1OTk=", "number": 14, "title": "New logo/icon proposal", "user": {"login": "mansya", "id": 33461607, "node_id": "MDQ6VXNlcjMzNDYxNjA3", "avatar_url": "https://avatars1.githubusercontent.com/u/33461607?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mansya", "html_url": "https://github.com/mansya", "followers_url": "https://api.github.com/users/mansya/followers", "following_url": "https://api.github.com/users/mansya/following{/other_user}", "gists_url": "https://api.github.com/users/mansya/gists{/gist_id}", "starred_url": "https://api.github.com/users/mansya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mansya/subscriptions", "organizations_url": "https://api.github.com/users/mansya/orgs", "repos_url": "https://api.github.com/users/mansya/repos", "events_url": "https://api.github.com/users/mansya/events{/privacy}", "received_events_url": "https://api.github.com/users/mansya/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-05T02:20:23Z", "updated_at": "2018-11-01T09:51:54Z", "closed_at": "2018-11-01T09:51:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Good day sir. I am a graphic designer and i am interested in designing a logo for your good project. I will be doing it as a gift for free. I just need your permission first before I begin my design. Hoping for your positive feedback. Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/12", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/12/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/12/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/12/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/12", "id": 348039582, "node_id": "MDU6SXNzdWUzNDgwMzk1ODI=", "number": 12, "title": "Error creating a TaskMetrics dataframe", "user": {"login": "franciscojfs", "id": 30843274, "node_id": "MDQ6VXNlcjMwODQzMjc0", "avatar_url": "https://avatars2.githubusercontent.com/u/30843274?v=4", "gravatar_id": "", "url": "https://api.github.com/users/franciscojfs", "html_url": "https://github.com/franciscojfs", "followers_url": "https://api.github.com/users/franciscojfs/followers", "following_url": "https://api.github.com/users/franciscojfs/following{/other_user}", "gists_url": "https://api.github.com/users/franciscojfs/gists{/gist_id}", "starred_url": "https://api.github.com/users/franciscojfs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/franciscojfs/subscriptions", "organizations_url": "https://api.github.com/users/franciscojfs/orgs", "repos_url": "https://api.github.com/users/franciscojfs/repos", "events_url": "https://api.github.com/users/franciscojfs/events{/privacy}", "received_events_url": "https://api.github.com/users/franciscojfs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-06T18:46:43Z", "updated_at": "2018-08-07T14:27:43Z", "closed_at": "2018-08-07T14:27:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello!\r\n\r\nI'm trying to create a dataframe to save my task metrics in Hive. However, when I call the create_taskmetrics_DF method, as below, the error occurs on the print screen.\r\n\r\n`tm = taskmetrics.create_taskmetrics_DF(\"PerfTaskMetrics\")`\r\n\r\n![image](https://user-images.githubusercontent.com/30843274/43734775-315dee7e-998f-11e8-9150-c98c13044533.png)\r\n\r\nIt looks like an error in the class, looking only at the error message.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/11", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/11/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/11/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/11/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/11", "id": 342288248, "node_id": "MDU6SXNzdWUzNDIyODgyNDg=", "number": 11, "title": "taskVals.toDF  java.lang.ClassCastException: ch.cern.sparkmeasure.StageVals incompatible with ch.cern.sparkmeasure.TaskVals", "user": {"login": "KennethNagin", "id": 1453210, "node_id": "MDQ6VXNlcjE0NTMyMTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1453210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KennethNagin", "html_url": "https://github.com/KennethNagin", "followers_url": "https://api.github.com/users/KennethNagin/followers", "following_url": "https://api.github.com/users/KennethNagin/following{/other_user}", "gists_url": "https://api.github.com/users/KennethNagin/gists{/gist_id}", "starred_url": "https://api.github.com/users/KennethNagin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KennethNagin/subscriptions", "organizations_url": "https://api.github.com/users/KennethNagin/orgs", "repos_url": "https://api.github.com/users/KennethNagin/repos", "events_url": "https://api.github.com/users/KennethNagin/events{/privacy}", "received_events_url": "https://api.github.com/users/KennethNagin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-18T11:28:00Z", "updated_at": "2018-07-24T08:38:09Z", "closed_at": "2018-07-24T08:38:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to use the deserializer.  However the toDF that follows is throwing an exception:\r\n\r\nDetails:\r\n val taskVals = ch.cern.sparkmeasure.Utils.readSerializedTaskMetrics(\"/tmp/stageMetrics.serialized\")\r\nscala> val taskVals = ch.cern.sparkmeasure.Utils.readSerializedTaskMetrics(\"/tmp/stageMetrics.serialized\")\r\ntaskVals: scala.collection.mutable.ListBuffer[ch.cern.sparkmeasure.TaskVals] = ListBuffer(StageVals(0,0,parquet at NativeMethodAccessorImpl.java:0,1531905390534,1531905392912,2378,1,1878,123,368,259,5,78,4802,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0), StageVals(1,1,count at NativeMethodAccessorImpl.java:0,1531905395225,1531905405201,9976,1003,1028843,104666,103972,21334,218,35221,1714153,144,0,0,0,100818815,11697322,0,0,0,0,0,0,0,1123,56844,1003), StageVals(1,2,count at NativeMethodAccessorImpl.java:0,1531905405221,1531905405591,370,1,336,176,26,19,1,0,2382,2,0,0,0,0,0,0,0,55,56844,1003,28,975,0,0,0), StageVals(2,3,collect at /root/SparCle/workload/sqlquery/data-layout-read.py:20,1531905405925,1531905413411,7486,1003,813482,25942,6350,1765,58,9238,1707192,144,0,0,0,100818815,156304569,0,0,0,0,0,...\r\nscala> val taskMetricsDF = taskVals.toDF()\r\njava.lang.RuntimeException: Error while encoding: java.lang.ClassCastException: ch.cern.sparkmeasure.StageVals incompatible with ch.cern.sparkmeasure.TaskVals\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).jobId AS jobId#444\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).stageId AS stageId#445\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).index AS index#446L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).launchTime AS launchTime#447L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).finishTime AS finishTime#448L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).duration AS duration#449L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).schedulerDelay AS schedulerDelay#450L\r\nstaticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).executorId, true) AS executorId#451\r\nstaticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).host, true) AS host#452\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).taskLocality AS taskLocality#453\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).speculative AS speculative#454\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).gettingResultTime AS gettingResultTime#455L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).successful AS successful#456\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).executorRunTime AS executorRunTime#457L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).executorCpuTime AS executorCpuTime#458L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).executorDeserializeTime AS executorDeserializeTime#459L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).executorDeserializeCpuTime AS executorDeserializeCpuTime#460L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).resultSerializationTime AS resultSerializationTime#461L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).jvmGCTime AS jvmGCTime#462L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).resultSize AS resultSize#463L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).numUpdatedBlockStatuses AS numUpdatedBlockStatuses#464\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).diskBytesSpilled AS diskBytesSpilled#465L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).memoryBytesSpilled AS memoryBytesSpilled#466L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).peakExecutionMemory AS peakExecutionMemory#467L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).recordsRead AS recordsRead#468L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).bytesRead AS bytesRead#469L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).recordsWritten AS recordsWritten#470L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).bytesWritten AS bytesWritten#471L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleFetchWaitTime AS shuffleFetchWaitTime#472L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleTotalBytesRead AS shuffleTotalBytesRead#473L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleTotalBlocksFetched AS shuffleTotalBlocksFetched#474L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleLocalBlocksFetched AS shuffleLocalBlocksFetched#475L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleRemoteBlocksFetched AS shuffleRemoteBlocksFetched#476L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleWriteTime AS shuffleWriteTime#477L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleBytesWritten AS shuffleBytesWritten#478L\r\nassertnotnull(assertnotnull(input[0, ch.cern.sparkmeasure.TaskVals, true])).shuffleRecordsWritten AS shuffleRecordsWritten#479L\r\n  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:290)\r\n  at org.apache.spark.sql.SparkSession$$anonfun$2.apply(SparkSession.scala:464)\r\n  at org.apache.spark.sql.SparkSession$$anonfun$2.apply(SparkSession.scala:464)\r\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n  at scala.collection.immutable.List.foreach(List.scala:381)\r\n  at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\r\n  at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)\r\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n  at org.apache.spark.sql.SparkSession.createDataset(SparkSession.scala:464)\r\n  at org.apache.spark.sql.SQLContext.createDataset(SQLContext.scala:377)\r\n  at org.apache.spark.sql.SQLImplicits.localSeqToDatasetHolder(SQLImplicits.scala:213)\r\n  ... 48 elided\r\nCaused by: java.lang.ClassCastException: ch.cern.sparkmeasure.StageVals incompatible with ch.cern.sparkmeasure.TaskVals\r\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_0$(Unknown Source)\r\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\r\n  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.toRow(ExpressionEncoder.scala:287)\r\n  ... 60 more\r\n\r\nscala>\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/8", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/8/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/8/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/8/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/8", "id": 329724843, "node_id": "MDU6SXNzdWUzMjk3MjQ4NDM=", "number": 8, "title": "Why sum(bytesWritten) => 0 (0 Bytes) always 0 ?", "user": {"login": "gilv", "id": 6321715, "node_id": "MDQ6VXNlcjYzMjE3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/6321715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gilv", "html_url": "https://github.com/gilv", "followers_url": "https://api.github.com/users/gilv/followers", "following_url": "https://api.github.com/users/gilv/following{/other_user}", "gists_url": "https://api.github.com/users/gilv/gists{/gist_id}", "starred_url": "https://api.github.com/users/gilv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gilv/subscriptions", "organizations_url": "https://api.github.com/users/gilv/orgs", "repos_url": "https://api.github.com/users/gilv/repos", "events_url": "https://api.github.com/users/gilv/events{/privacy}", "received_events_url": "https://api.github.com/users/gilv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-06T06:16:43Z", "updated_at": "2018-06-06T16:14:13Z", "closed_at": "2018-06-06T16:14:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I used Spark to read local Parquet file and store it under different name.\r\nI used file:// as i accessed file locally.\r\n\r\nsum(bytesWritten) => 0 (0 Bytes) was reported 0. Why is this?\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/7", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/7/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/7/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/7/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/7", "id": 295107679, "node_id": "MDU6SXNzdWUyOTUxMDc2Nzk=", "number": 7, "title": "Dropping SparkListenerEvent because no remaining room in event queue", "user": {"login": "bteeuwen", "id": 517598, "node_id": "MDQ6VXNlcjUxNzU5OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/517598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bteeuwen", "html_url": "https://github.com/bteeuwen", "followers_url": "https://api.github.com/users/bteeuwen/followers", "following_url": "https://api.github.com/users/bteeuwen/following{/other_user}", "gists_url": "https://api.github.com/users/bteeuwen/gists{/gist_id}", "starred_url": "https://api.github.com/users/bteeuwen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bteeuwen/subscriptions", "organizations_url": "https://api.github.com/users/bteeuwen/orgs", "repos_url": "https://api.github.com/users/bteeuwen/repos", "events_url": "https://api.github.com/users/bteeuwen/events{/privacy}", "received_events_url": "https://api.github.com/users/bteeuwen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-07T11:46:11Z", "updated_at": "2018-07-18T15:56:02Z", "closed_at": "2018-07-18T15:56:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I launched sparkMeasure in a large job. Immediately I got:\r\n18/02/07 08:21:56 ERROR org.apache.spark.scheduler.LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.\r\n18/02/07 08:21:56 WARN org.apache.spark.scheduler.LiveListenerBus: Dropped 1 SparkListenerEvents since Thu Jan 01 00:00:00 UTC 1970\r\n18/02/07 08:22:56 WARN org.apache.spark.scheduler.LiveListenerBus: Dropped 13971 SparkListenerEvents since Wed Feb 07 08:21:56 UTC 2018\r\n18/02/07 08:23:51 ERROR org.apache.spark.network.server.TransportRequestHandler: Error opening block StreamChunkId{streamId=1999850815777, chunkIndex=0} for request from /10.205.151.192:37514 org.apache.spark.storage.BlockNotFoundException: Block broadcast_32_piece0 not found\r\n\r\nThe job does continue, but it seems to be overloading the listenerbus.\r\nI'll try --conf spark.scheduler.listenerbus.eventqueue.size=100000. \r\n\r\nDid you already encounter this somewhere?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/6", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/6/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/6/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/6/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/6", "id": 275458756, "node_id": "MDU6SXNzdWUyNzU0NTg3NTY=", "number": 6, "title": "enable travis-ci", "user": {"login": "pjfanning", "id": 11783444, "node_id": "MDQ6VXNlcjExNzgzNDQ0", "avatar_url": "https://avatars0.githubusercontent.com/u/11783444?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pjfanning", "html_url": "https://github.com/pjfanning", "followers_url": "https://api.github.com/users/pjfanning/followers", "following_url": "https://api.github.com/users/pjfanning/following{/other_user}", "gists_url": "https://api.github.com/users/pjfanning/gists{/gist_id}", "starred_url": "https://api.github.com/users/pjfanning/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pjfanning/subscriptions", "organizations_url": "https://api.github.com/users/pjfanning/orgs", "repos_url": "https://api.github.com/users/pjfanning/repos", "events_url": "https://api.github.com/users/pjfanning/events{/privacy}", "received_events_url": "https://api.github.com/users/pjfanning/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-20T18:46:20Z", "updated_at": "2017-12-11T18:22:44Z", "closed_at": "2017-12-11T18:22:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "@LucaCanali would it be possible for you to login to Travis CI with your github credentials and enable Travis builds for sparkMeasure?\r\n\r\nYou could then add these badges at the top of your README.md.\r\nThese are copied from https://github.com/swagger-akka-http/swagger-akka-http/blob/master/README.md\r\n\r\n```\r\n[![Build Status](https://travis-ci.org/LucaCanali/sparkMeasure.svg?branch=master)](https://travis-ci.org/LucaCanali/sparkMeasure)\r\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/LucaCanali/sparkMeasure/badge.svg)](https://maven-badges.herokuapp.com/maven-central/LucaCanali/sparkMeasure)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/3", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/3/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/3/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/3/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/3", "id": 271157943, "node_id": "MDU6SXNzdWUyNzExNTc5NDM=", "number": 3, "title": "use json for the serialized data", "user": {"login": "pjfanning", "id": 11783444, "node_id": "MDQ6VXNlcjExNzgzNDQ0", "avatar_url": "https://avatars0.githubusercontent.com/u/11783444?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pjfanning", "html_url": "https://github.com/pjfanning", "followers_url": "https://api.github.com/users/pjfanning/followers", "following_url": "https://api.github.com/users/pjfanning/following{/other_user}", "gists_url": "https://api.github.com/users/pjfanning/gists{/gist_id}", "starred_url": "https://api.github.com/users/pjfanning/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pjfanning/subscriptions", "organizations_url": "https://api.github.com/users/pjfanning/orgs", "repos_url": "https://api.github.com/users/pjfanning/repos", "events_url": "https://api.github.com/users/pjfanning/events{/privacy}", "received_events_url": "https://api.github.com/users/pjfanning/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-11-04T01:51:07Z", "updated_at": "2017-11-08T22:52:14Z", "closed_at": "2017-11-08T22:52:13Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I think this would be easier for users to check the serialized metrics if it was saved in a text format like JSON. I understand that the processing is done by code but it useful if the formats are human readable.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/1", "repository_url": "https://api.github.com/repos/LucaCanali/sparkMeasure", "labels_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/1/labels{/name}", "comments_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/1/comments", "events_url": "https://api.github.com/repos/LucaCanali/sparkMeasure/issues/1/events", "html_url": "https://github.com/LucaCanali/sparkMeasure/issues/1", "id": 271151574, "node_id": "MDU6SXNzdWUyNzExNTE1NzQ=", "number": 1, "title": "switch to slf4j for logging", "user": {"login": "pjfanning", "id": 11783444, "node_id": "MDQ6VXNlcjExNzgzNDQ0", "avatar_url": "https://avatars0.githubusercontent.com/u/11783444?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pjfanning", "html_url": "https://github.com/pjfanning", "followers_url": "https://api.github.com/users/pjfanning/followers", "following_url": "https://api.github.com/users/pjfanning/following{/other_user}", "gists_url": "https://api.github.com/users/pjfanning/gists{/gist_id}", "starred_url": "https://api.github.com/users/pjfanning/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pjfanning/subscriptions", "organizations_url": "https://api.github.com/users/pjfanning/orgs", "repos_url": "https://api.github.com/users/pjfanning/repos", "events_url": "https://api.github.com/users/pjfanning/events{/privacy}", "received_events_url": "https://api.github.com/users/pjfanning/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-04T00:27:14Z", "updated_at": "2017-11-05T17:25:15Z", "closed_at": "2017-11-05T17:25:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "[SLF4J](https://www.slf4j.org/manual.html) is a nice logging abstraction that allows developers include logging but lets users choose their preferred logging framework.\r\nCould you switch to having a compile dependency on slf4j-api jar and a test dependency on log4j and the [log4j-slf4j-impl](https://logging.apache.org/log4j/2.0/log4j-slf4j-impl/index.html)?", "performed_via_github_app": null, "score": 1.0}]}