{"total_count": 2, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/4", "repository_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune", "labels_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/4/labels{/name}", "comments_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/4/comments", "events_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/4/events", "html_url": "https://github.com/GoogleCloudPlatform/cloudml-hypertune/issues/4", "id": 604648988, "node_id": "MDU6SXNzdWU2MDQ2NDg5ODg=", "number": 4, "title": "Why does this work when tf.summary.scalar doesn't?", "user": {"login": "JulianFerry", "id": 24207830, "node_id": "MDQ6VXNlcjI0MjA3ODMw", "avatar_url": "https://avatars0.githubusercontent.com/u/24207830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JulianFerry", "html_url": "https://github.com/JulianFerry", "followers_url": "https://api.github.com/users/JulianFerry/followers", "following_url": "https://api.github.com/users/JulianFerry/following{/other_user}", "gists_url": "https://api.github.com/users/JulianFerry/gists{/gist_id}", "starred_url": "https://api.github.com/users/JulianFerry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JulianFerry/subscriptions", "organizations_url": "https://api.github.com/users/JulianFerry/orgs", "repos_url": "https://api.github.com/users/JulianFerry/repos", "events_url": "https://api.github.com/users/JulianFerry/events{/privacy}", "received_events_url": "https://api.github.com/users/JulianFerry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-22T10:37:10Z", "updated_at": "2020-04-30T06:27:51Z", "closed_at": "2020-04-30T06:27:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Thanks for the repo!\r\n\r\n### Problem\r\n\r\nI have been trying to display the accuracy of a Keras model which I run with hyperparameter tuning on the AI platform. The method outlined in the AI platform documentation does not work, whereas yours does, and I would like to know what this stems from (it might be a bug?).\r\n\r\n### Documentation\r\n\r\nI have been following the following documentation pages:\r\n**1. [Overview of hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview)**\r\n**2. [Using hyperparameter tuning](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning)**\r\n\r\n\r\nAccording to **[1]**:\r\n\r\n> **How AI Platform Training gets your metric**\r\n> You may notice that there are no instructions in this documentation for passing your hyperparameter metric to the AI Platform Training training service. That's because the service monitors TensorFlow summary events generated by your training application and retrieves the metric.\"\r\n\r\nAnd according to **[2]**, one way of generating such a Tensorflow summary event is by creating a callback class as so:\r\n```\r\nclass MyMetricCallback(tf.keras.callbacks.Callback):\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        tf.summary.scalar('metric1', logs['RootMeanSquaredError'], epoch)\r\n```\r\n\r\n### My code\r\n\r\nSo in my code I included:\r\n\r\n\r\n```\r\n# hptuning_config.yaml\r\n\r\ntrainingInput:\r\n  hyperparameters:\r\n    goal: MAXIMIZE\r\n    maxTrials: 4\r\n    maxParallelTrials: 2\r\n    hyperparameterMetricTag: val_accuracy\r\n    params:\r\n    - parameterName: learning_rate\r\n      type: DOUBLE\r\n      minValue: 0.001\r\n      maxValue: 0.01\r\n      scaleType: UNIT_LOG_SCALE\r\n```\r\n\r\n```\r\n# model.py\r\n\r\nclass MetricCallback(tf.keras.callbacks.Callback):\r\n\r\n    def on_epoch_end(self, epoch, logs):\r\n        tf.summary.scalar('val_accuracy', logs['val_accuracy'], epoch)\r\n```\r\n\r\nI even tried\r\n```\r\n# model.py\r\n\r\nclass MetricCallback(tf.keras.callbacks.Callback):\r\n    def __init__(self, logdir):\r\n        self.writer = tf.summary.create_file_writer(logdir)\r\n\r\n    def on_epoch_end(self, epoch, logs):\r\n        with writer.as_default():\r\n            tf.summary.scalar('val_accuracy', logs['val_accuracy'], epoch)\r\n```\r\n\r\nWhich successfully saved the 'val_accuracy' metric to Google storage, which I could also see with TensorBoard. But this was not picked up by AI platform jobs, despite the claim made in **[1]**.\r\n\r\nUsing your package, I created the following class:\r\n\r\n```\r\n# model.py\r\n\r\nclass MetricCallback(tf.keras.callbacks.Callback):\r\n    def __init__(self):\r\n        self.hpt = hypertune.HyperTune()\r\n\r\n    def on_epoch_end(self, epoch, logs):\r\n        self.hpt.report_hyperparameter_tuning_metric(\r\n            hyperparameter_metric_tag='val_accuracy',\r\n            metric_value=logs['val_accuracy'],\r\n            global_step=epoch\r\n        )\r\n```\r\n\r\nwhich works! But I don't see how, since it all it seems to do is write to a file on the AI platform worker at `/tmp/hypertune/*`. There is nothing in the documentation that explains how this is getting picked up by the AI platform...\r\n\r\nCould you please explain why your `HyperTune.report_hyperparameter_tuning_metric` works? Are the docs wrong or out of date?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/1", "repository_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune", "labels_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/1/labels{/name}", "comments_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/1/comments", "events_url": "https://api.github.com/repos/GoogleCloudPlatform/cloudml-hypertune/issues/1/events", "html_url": "https://github.com/GoogleCloudPlatform/cloudml-hypertune/issues/1", "id": 502080357, "node_id": "MDU6SXNzdWU1MDIwODAzNTc=", "number": 1, "title": "How to `report_hyperparameter_tuning_metric` inside `tf.estimator.train_and_evaluate`?", "user": {"login": "JakeTheWise", "id": 13136287, "node_id": "MDQ6VXNlcjEzMTM2Mjg3", "avatar_url": "https://avatars2.githubusercontent.com/u/13136287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JakeTheWise", "html_url": "https://github.com/JakeTheWise", "followers_url": "https://api.github.com/users/JakeTheWise/followers", "following_url": "https://api.github.com/users/JakeTheWise/following{/other_user}", "gists_url": "https://api.github.com/users/JakeTheWise/gists{/gist_id}", "starred_url": "https://api.github.com/users/JakeTheWise/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JakeTheWise/subscriptions", "organizations_url": "https://api.github.com/users/JakeTheWise/orgs", "repos_url": "https://api.github.com/users/JakeTheWise/repos", "events_url": "https://api.github.com/users/JakeTheWise/events{/privacy}", "received_events_url": "https://api.github.com/users/JakeTheWise/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-03T13:51:47Z", "updated_at": "2019-12-13T00:33:02Z", "closed_at": "2019-12-13T00:33:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm running hyperparameter tuning with a custom container. I've followed the [tutorial](https://cloud.google.com/ml-engine/docs/custom-containers-training#submit_a_hyperparameter_tuning_job) but I'd like some guidance on how I might be able to report metrics to the hypertune service during the `train_and_estimate` call instead of afterwards.", "performed_via_github_app": null, "score": 1.0}]}