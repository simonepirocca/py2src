{"total_count": 159, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/huggingface/tokenizers/issues/367", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/367/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/367/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/367/events", "html_url": "https://github.com/huggingface/tokenizers/issues/367", "id": 673382073, "node_id": "MDU6SXNzdWU2NzMzODIwNzM=", "number": 367, "title": "Unicode problem, Exception: Split pre-tokenized string must represent entire orignal String", "user": {"login": "wbqtac", "id": 804445, "node_id": "MDQ6VXNlcjgwNDQ0NQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/804445?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wbqtac", "html_url": "https://github.com/wbqtac", "followers_url": "https://api.github.com/users/wbqtac/followers", "following_url": "https://api.github.com/users/wbqtac/following{/other_user}", "gists_url": "https://api.github.com/users/wbqtac/gists{/gist_id}", "starred_url": "https://api.github.com/users/wbqtac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wbqtac/subscriptions", "organizations_url": "https://api.github.com/users/wbqtac/orgs", "repos_url": "https://api.github.com/users/wbqtac/repos", "events_url": "https://api.github.com/users/wbqtac/events{/privacy}", "received_events_url": "https://api.github.com/users/wbqtac/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-08-05T09:08:09Z", "updated_at": "2020-08-09T05:51:26Z", "closed_at": "2020-08-09T05:51:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there,\r\n\r\nI got an issue when tokenizing the following sentence:\r\n\r\nRika Noguchi ( \u91ce\u53e3\u91cc\u4f73 Noguchi Rika, born 1971 in Tokyo, Japan) is a Japanese photographic artist\r\n\r\nException: Split pre-tokenized string must represent the entire original string. \r\n\r\nThe code is shown as follows,\r\n#####################################################\r\nfrom tokenizers import (ByteLevelBPETokenizer,\r\n                            CharBPETokenizer,\r\n                            SentencePieceBPETokenizer,\r\n                            BertWordPieceTokenizer)\r\ntokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\r\noriginal_str = \"Rika Noguchi ( \u91ce\u53e3\u91cc\u4f73 Noguchi Rika, born 1971 in Tokyo, Japan) is a Japanese photographic artist \"\r\noutput = tokenizer.encode(original_str)\r\n\r\nI think the problem comes from the chinese unicode \u91ce\u53e3\u91cc\u4f73, as the tokenizer works fine if these words are removed.\r\n\r\nIs there any way to deal with it? \r\n\r\nThank you. Sorry if the question should be post somewhere else. \r\n\r\nBest\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/361", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/361/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/361/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/361/events", "html_url": "https://github.com/huggingface/tokenizers/issues/361", "id": 669721930, "node_id": "MDU6SXNzdWU2Njk3MjE5MzA=", "number": 361, "title": "How to produce an encoder.json and vocab.bpe for GPT-2 encoding ?", "user": {"login": "Skylixia", "id": 12053610, "node_id": "MDQ6VXNlcjEyMDUzNjEw", "avatar_url": "https://avatars1.githubusercontent.com/u/12053610?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Skylixia", "html_url": "https://github.com/Skylixia", "followers_url": "https://api.github.com/users/Skylixia/followers", "following_url": "https://api.github.com/users/Skylixia/following{/other_user}", "gists_url": "https://api.github.com/users/Skylixia/gists{/gist_id}", "starred_url": "https://api.github.com/users/Skylixia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Skylixia/subscriptions", "organizations_url": "https://api.github.com/users/Skylixia/orgs", "repos_url": "https://api.github.com/users/Skylixia/repos", "events_url": "https://api.github.com/users/Skylixia/events{/privacy}", "received_events_url": "https://api.github.com/users/Skylixia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-31T11:33:49Z", "updated_at": "2020-07-31T18:29:08Z", "closed_at": "2020-07-31T13:28:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, \r\n\r\nI would like to produce an encoder.json and vocab.bpe file to make the GPT-2 preprocessing/encoding as done for using fairseq library as seen in the [documentation](https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.pretraining.md)\r\nIt seems to be possible to produce such files with this library but I am not sure how ... any tips ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/359", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/359/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/359/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/359/events", "html_url": "https://github.com/huggingface/tokenizers/issues/359", "id": 667954583, "node_id": "MDU6SXNzdWU2Njc5NTQ1ODM=", "number": 359, "title": "Custom chinese pre-tokenizer", "user": {"login": "adendek", "id": 5308413, "node_id": "MDQ6VXNlcjUzMDg0MTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/5308413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adendek", "html_url": "https://github.com/adendek", "followers_url": "https://api.github.com/users/adendek/followers", "following_url": "https://api.github.com/users/adendek/following{/other_user}", "gists_url": "https://api.github.com/users/adendek/gists{/gist_id}", "starred_url": "https://api.github.com/users/adendek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adendek/subscriptions", "organizations_url": "https://api.github.com/users/adendek/orgs", "repos_url": "https://api.github.com/users/adendek/repos", "events_url": "https://api.github.com/users/adendek/events{/privacy}", "received_events_url": "https://api.github.com/users/adendek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-29T15:56:02Z", "updated_at": "2020-08-03T20:17:18Z", "closed_at": "2020-08-03T20:17:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to use a custom pre-tokenizer based on a [jieba](https://github.com/fxsjy/jieba) library. It is a tool that allows splitting strings into meaningful words. \r\nHere is the code that I wrote in order to combine jieba tokens with tokenizers.  \r\n\r\n```python \r\nimport jieba\r\n\r\nclass JiebaPreTokenizer:\r\n    \"\"\"This class is a jieba adapter that mimic the interface of PreTokenizer, \r\n    which is a component responsible for a initial spliting a sentence into tokens.\r\n    \"\"\"\r\n    def __init__(self, jieba_init_fn=None):\r\n        \"\"\"\r\n        :param jieba_init_fn: is a function pointner that is executed in order to initialize jieba tokenizer engine, \r\n        see https://github.com/fxsjy/jieba#initialization\r\n        \"\"\"\r\n        if jieba_init_fn is not None:\r\n            jieba_init_fn()\r\n    \r\n    def pre_tokenize(self, sentence):\r\n        \"\"\"this function return tokenized version of the sentence, \r\n        it has to adjust the format of the output generated by jieba to be consistent with the one required by the tokenizers \"\"\"\r\n        return [(token[0],(token[1],token[2])) for token in jieba.tokenize(sentence)]\r\n    \r\n    def decode(self, tokens):\r\n        return \"\".join(tokens)\r\n    \r\n    \r\ndef build_tokenizer():\r\n        \r\n    tokenizer = Tokenizer(BPE())\r\n    jieba_pre_tokenizer = JiebaPreTokenizer(jieba.disable_parallel)\r\n    tokenizer.pre_tokenizer = pre_tokenizers.PreTokenizer.custom(jieba_pre_tokenizer)\r\n    tokenizer.decoder = decoders.Decoder.custom(jieba_pre_tokenizer)\r\n        \r\n    return tokenizer\r\n\r\ndef create_trainer():\r\n    return BpeTrainer(vocab_size=15000, show_progress=True,\r\n                    min_frequency=0, special_tokens=[\r\n                    \"<s>\",\r\n                    \"<pad>\",\r\n                    \"</s>\",\r\n                    \"<unk>\",\r\n                    \"<mask>\",\r\n                    ]\r\n          )\r\n               \r\n        \r\nif __name__ ==\"__main__\":\r\n    tokenizer = build_tokenizer()\r\n    trainer = create_trainer()\r\n    tokenizer.train(trainer, [\"./descriptions_cn.txt\"])\r\n    tokenizer.model.save('./tokenizer_cn')\r\n```\r\n\r\nWhen running the code, I encounter the deadlock issue. Is there any way to make it work? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/354", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/354/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/354/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/354/events", "html_url": "https://github.com/huggingface/tokenizers/issues/354", "id": 663980452, "node_id": "MDU6SXNzdWU2NjM5ODA0NTI=", "number": 354, "title": "Segfaults / dangling pointers", "user": {"login": "sebpuetz", "id": 30175181, "node_id": "MDQ6VXNlcjMwMTc1MTgx", "avatar_url": "https://avatars2.githubusercontent.com/u/30175181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sebpuetz", "html_url": "https://github.com/sebpuetz", "followers_url": "https://api.github.com/users/sebpuetz/followers", "following_url": "https://api.github.com/users/sebpuetz/following{/other_user}", "gists_url": "https://api.github.com/users/sebpuetz/gists{/gist_id}", "starred_url": "https://api.github.com/users/sebpuetz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sebpuetz/subscriptions", "organizations_url": "https://api.github.com/users/sebpuetz/orgs", "repos_url": "https://api.github.com/users/sebpuetz/repos", "events_url": "https://api.github.com/users/sebpuetz/events{/privacy}", "received_events_url": "https://api.github.com/users/sebpuetz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-07-22T19:00:39Z", "updated_at": "2020-08-05T16:52:49Z", "closed_at": "2020-08-05T16:52:48Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "~~~Python\r\nimport tokenizers\r\nmod = tokenizers.models.BPE()\r\ntok = tokenizers.Tokenizer(mod)\r\ntok.encode(\"Test\")\r\ndel tok\r\nmod.encode([\"Test\"])\r\n~~~\r\n\r\n~~~\r\n[1]    7487 segmentation fault (core dumped)  ipython\r\n~~~\r\n\r\nThis happens because the `Tokenizer` steals the `Box` from the `Model` passed to the constructor and replaces it with a raw pointer to that `Box`. When the `Tokenizer` is dropped, the `Box` with the `Model` gets deallocated and the raw pointer in the container is dangling.\r\n\r\nOne workaround for this issue would be the use of a managed reference (e.g. `Arc` since threading is required) on the Rust-side. \r\n\r\nE.g. (untested):\r\n\r\n~~~Rust\r\n\r\n#[derive(Clone)]\r\npub struct PyModel {\r\n    pub model: Arc<dyn tk::tokenizer::Model>,\r\n}\r\n\r\nimpl Model for PyModel {\r\n    [...]\r\n}\r\n\r\n#[pymethods]\r\nimpl Tokenizer {\r\n    #[new]\r\n    fn new(model: PyRefMut<PyModel>) -> PyResult<Self> {\r\n        let tokenizer = tk::tokenizer::Tokenizer::new(model.clone());\r\n        Ok(Tokenizer { tokenizer })\r\n    }\r\n}\r\n~~~\r\n\r\n------\r\n\r\nAnother issue with `Container` is that it dereference whatever pointer gets thrown in there, e.g. this will also cause segmentation faults:\r\n\r\n~~~Rust\r\nlet c: Container<String>::Pointer(std::ptr::null_mut());\r\nc.execute(|s| s.to_string());\r\n~~~", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/349", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/349/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/349/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/349/events", "html_url": "https://github.com/huggingface/tokenizers/issues/349", "id": 659547958, "node_id": "MDU6SXNzdWU2NTk1NDc5NTg=", "number": 349, "title": "Giving invalid file location to train a tokenizer results in exiting the process and fatal runtime error", "user": {"login": "llStringll", "id": 30209072, "node_id": "MDQ6VXNlcjMwMjA5MDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/30209072?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llStringll", "html_url": "https://github.com/llStringll", "followers_url": "https://api.github.com/users/llStringll/followers", "following_url": "https://api.github.com/users/llStringll/following{/other_user}", "gists_url": "https://api.github.com/users/llStringll/gists{/gist_id}", "starred_url": "https://api.github.com/users/llStringll/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llStringll/subscriptions", "organizations_url": "https://api.github.com/users/llStringll/orgs", "repos_url": "https://api.github.com/users/llStringll/repos", "events_url": "https://api.github.com/users/llStringll/events{/privacy}", "received_events_url": "https://api.github.com/users/llStringll/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-17T19:13:47Z", "updated_at": "2020-07-17T23:39:36Z", "closed_at": "2020-07-17T23:39:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "While training a tokenizer like so:\r\n```\r\nfrom tokenizers import ByteLevelBPETokenizer as bpe\r\ntok = bpe(add_prefix_space=True, lowercase=True)\r\ntok.train([train_corpus], vocab_size=16000, special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\"])\r\n```\r\nIf the train_corpus is invalid, i.e., the file does not exist, it exhibits the under shown behavior, the error could be handled in a better way.\r\n```\r\nthread '<unnamed>' panicked at 'called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: \"No such file or directory\" }', /__w/tokenizers/tokenizers/tokenizers/src/tokenizer/mod.rs:593:29\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\nfatal runtime error: failed to initiate panic, error 5\r\nAborted (core dumped)\r\n```\r\nBacktrace shown as:\r\n```\r\nstack backtrace:\r\n   0: backtrace::backtrace::libunwind::trace\r\n             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.46/src/backtrace/libunwind.rs:86\r\n   1: backtrace::backtrace::trace_unsynchronized\r\n             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.46/src/backtrace/mod.rs:66\r\n   2: std::sys_common::backtrace::_print_fmt\r\n             at src/libstd/sys_common/backtrace.rs:78\r\n   3: <std::sys_common::backtrace::_print::DisplayBacktrace as core::fmt::Display>::fmt\r\n             at src/libstd/sys_common/backtrace.rs:59\r\n   4: core::fmt::write\r\n             at src/libcore/fmt/mod.rs:1069\r\n   5: std::io::Write::write_fmt\r\n             at src/libstd/io/mod.rs:1537\r\n   6: std::sys_common::backtrace::_print\r\n             at src/libstd/sys_common/backtrace.rs:62\r\n   7: std::sys_common::backtrace::print\r\n             at src/libstd/sys_common/backtrace.rs:49\r\n   8: std::panicking::default_hook::{{closure}}\r\n             at src/libstd/panicking.rs:198\r\n   9: std::panicking::default_hook\r\n             at src/libstd/panicking.rs:218\r\n  10: std::panicking::rust_panic_with_hook\r\n             at src/libstd/panicking.rs:477\r\n  11: rust_begin_unwind\r\n             at src/libstd/panicking.rs:385\r\n  12: core::panicking::panic_fmt\r\n             at src/libcore/panicking.rs:89\r\n  13: core::option::expect_none_failed\r\n             at src/libcore/option.rs:1272\r\n  14: <core::iter::adapters::Map<I,F> as core::iter::traits::iterator::Iterator>::fold\r\n  15: tokenizers::tokenizer::Tokenizer::train\r\n  16: tokenizers::tokenizer::Tokenizer::train::{{closure}}\r\n  17: tokenizers::tokenizer::__init11742626496714830824::__init11742626496714830824::__wrap\r\n  18: <unknown>\r\n  19: _PyEval_EvalFrameDefault\r\n  20: <unknown>\r\n  21: <unknown>\r\n  22: <unknown>\r\n  23: _PyEval_EvalFrameDefault\r\n  24: <unknown>\r\n  25: PyEval_EvalCode\r\n  26: <unknown>\r\n  27: <unknown>\r\n  28: PyRun_InteractiveLoopFlags\r\n  29: PyRun_AnyFileExFlags\r\n  30: Py_Main\r\n  31: main\r\n  32: __libc_start_main\r\n  33: _start\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\n```\r\nI ran the full backtrace, it was too long, to post here, it is reproducible as shown above", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/342", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/342/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/342/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/342/events", "html_url": "https://github.com/huggingface/tokenizers/issues/342", "id": 656790064, "node_id": "MDU6SXNzdWU2NTY3OTAwNjQ=", "number": 342, "title": "TypeError: __init__() got an unexpected keyword argument 'add_special_tokens'", "user": {"login": "Samar-080301", "id": 35553535, "node_id": "MDQ6VXNlcjM1NTUzNTM1", "avatar_url": "https://avatars3.githubusercontent.com/u/35553535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Samar-080301", "html_url": "https://github.com/Samar-080301", "followers_url": "https://api.github.com/users/Samar-080301/followers", "following_url": "https://api.github.com/users/Samar-080301/following{/other_user}", "gists_url": "https://api.github.com/users/Samar-080301/gists{/gist_id}", "starred_url": "https://api.github.com/users/Samar-080301/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Samar-080301/subscriptions", "organizations_url": "https://api.github.com/users/Samar-080301/orgs", "repos_url": "https://api.github.com/users/Samar-080301/repos", "events_url": "https://api.github.com/users/Samar-080301/events{/privacy}", "received_events_url": "https://api.github.com/users/Samar-080301/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-14T17:50:30Z", "updated_at": "2020-07-15T13:10:40Z", "closed_at": "2020-07-15T13:10:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "`import tokenizers\r\n    bwpt = tokenizers.BertWordPieceTokenizer(\r\n    vocab_file=None,\r\n    add_special_tokens=True,\r\n    unk_token='[UNK]',\r\n    sep_token='[SEP]',\r\n    cls_token='[CLS]',\r\n    clean_text=True,\r\n    handle_chinese_chars=True,\r\n    strip_accents=True,\r\n    lowercase=True,\r\n    wordpieces_prefix='##'\r\n)`\r\nI was trying this code, but the above error occured please help me!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/341", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/341/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/341/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/341/events", "html_url": "https://github.com/huggingface/tokenizers/issues/341", "id": 656333128, "node_id": "MDU6SXNzdWU2NTYzMzMxMjg=", "number": 341, "title": "from_file method of tokenizers.Tokenizer does not exist", "user": {"login": "llStringll", "id": 30209072, "node_id": "MDQ6VXNlcjMwMjA5MDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/30209072?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llStringll", "html_url": "https://github.com/llStringll", "followers_url": "https://api.github.com/users/llStringll/followers", "following_url": "https://api.github.com/users/llStringll/following{/other_user}", "gists_url": "https://api.github.com/users/llStringll/gists{/gist_id}", "starred_url": "https://api.github.com/users/llStringll/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llStringll/subscriptions", "organizations_url": "https://api.github.com/users/llStringll/orgs", "repos_url": "https://api.github.com/users/llStringll/repos", "events_url": "https://api.github.com/users/llStringll/events{/privacy}", "received_events_url": "https://api.github.com/users/llStringll/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-14T05:19:09Z", "updated_at": "2020-07-17T16:50:46Z", "closed_at": "2020-07-17T16:50:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "I trained a Byte-level BPE tokenizer on reddit pushshift, and I saved it like so:\r\n```\r\ntok=tokenizers.ByteLevelBPETokenizer()\r\ntok.train(['my_file_here.txt'])\r\ntok.save(\"my_dir_here\")\r\n```\r\nThis worked as expected, saved vocab.json and merges.txt\r\n\r\nNow,  as shown in tokenizer repo's readme for python, to load a saved tokenizer one can do so:\r\n```\r\ntokenizer = Tokenizer.from_file(\"byte-level-bpe.tokenizer.json\")\r\n```\r\nIt throws an error stating that Tokenizer doesn't have an attribute \"from_file\"\r\nWell, now how can one load a trained tokenizer?\r\n\r\nPS- Can one just parse vocab.json into newline-separated tokens similar to vocab.txt of transformers.PreTrainedTokenizers and load it using transformers.PreTrainedTokenizer.from_pretrained() ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/340", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/340/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/340/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/340/events", "html_url": "https://github.com/huggingface/tokenizers/issues/340", "id": 656163963, "node_id": "MDU6SXNzdWU2NTYxNjM5NjM=", "number": 340, "title": "Rust and Python implementation treat cleanup differently", "user": {"login": "sebpuetz", "id": 30175181, "node_id": "MDQ6VXNlcjMwMTc1MTgx", "avatar_url": "https://avatars2.githubusercontent.com/u/30175181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sebpuetz", "html_url": "https://github.com/sebpuetz", "followers_url": "https://api.github.com/users/sebpuetz/followers", "following_url": "https://api.github.com/users/sebpuetz/following{/other_user}", "gists_url": "https://api.github.com/users/sebpuetz/gists{/gist_id}", "starred_url": "https://api.github.com/users/sebpuetz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sebpuetz/subscriptions", "organizations_url": "https://api.github.com/users/sebpuetz/orgs", "repos_url": "https://api.github.com/users/sebpuetz/repos", "events_url": "https://api.github.com/users/sebpuetz/events{/privacy}", "received_events_url": "https://api.github.com/users/sebpuetz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-13T21:40:58Z", "updated_at": "2020-07-20T18:17:46Z", "closed_at": "2020-07-15T13:17:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\n```Py\r\n>>> tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=True)\r\n>>> [tokenizer.tokenize(t) for t in ['Test', '\\xad', 'test']]\r\n[['Test'], [], ['test']]\r\n\r\n>>> tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=False)\r\n>>> [tokenizer.tokenize(t) for t in ['Test', '\\xad', 'test']]\r\n[['Test'], ['[UNK]'], ['test']]\r\n```\r\n\r\nDigging a bit in the code, `tokenizers::normalizers::bert::is_control` returns `true` for `SOFT-HYPHEN (\\xad)`, so it gets filtered in `tokenizers::normalizers::bert::BertNormalizer::do_clean_text`.\r\n\r\nIt would be nice if the Rust implementation could also return `'[UNK]'` when it removes all characters of a token. While having a control character in the dataset is clearly an issue with the data, I still think consistency would be nice.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/339", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/339/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/339/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/339/events", "html_url": "https://github.com/huggingface/tokenizers/issues/339", "id": 655390389, "node_id": "MDU6SXNzdWU2NTUzOTAzODk=", "number": 339, "title": "Different behaviors of tokenizer", "user": {"login": "kushalj001", "id": 32245327, "node_id": "MDQ6VXNlcjMyMjQ1MzI3", "avatar_url": "https://avatars2.githubusercontent.com/u/32245327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kushalj001", "html_url": "https://github.com/kushalj001", "followers_url": "https://api.github.com/users/kushalj001/followers", "following_url": "https://api.github.com/users/kushalj001/following{/other_user}", "gists_url": "https://api.github.com/users/kushalj001/gists{/gist_id}", "starred_url": "https://api.github.com/users/kushalj001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kushalj001/subscriptions", "organizations_url": "https://api.github.com/users/kushalj001/orgs", "repos_url": "https://api.github.com/users/kushalj001/repos", "events_url": "https://api.github.com/users/kushalj001/events{/privacy}", "received_events_url": "https://api.github.com/users/kushalj001/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-12T12:24:26Z", "updated_at": "2020-07-16T17:45:29Z", "closed_at": "2020-07-16T17:45:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am fine-tuning a DistilBERT for some downstream tasks. While preparing the data, I came across this weird behavior of the tokenizer `__call__` function and the `convert_tokens_to_ids` functions. \r\nWhen I use `convert_tokens_to_ids` function on an individual token, it returns an index of 1 i.e. the `[UNK]` token as shown below:  \r\n  \r\n![image](https://user-images.githubusercontent.com/32245327/87246095-91cd1180-c468-11ea-97e5-841f04e51146.png)\r\n\r\nHowever, when I use the tokenizer's `__call__` method, it subtly changes the words and returns some indices as shown below  \r\n\r\n![image](https://user-images.githubusercontent.com/32245327/87246102-9e516a00-c468-11ea-9ccb-01869c6b7623.png)\r\n\r\nIs this behavior documented somewhere or specific to my language model? This is the Hindi language.\r\nThank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/337", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/337/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/337/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/337/events", "html_url": "https://github.com/huggingface/tokenizers/issues/337", "id": 654886030, "node_id": "MDU6SXNzdWU2NTQ4ODYwMzA=", "number": 337, "title": "TypeError: sep_token not found in the vocabulary", "user": {"login": "cahya-wirawan", "id": 7669893, "node_id": "MDQ6VXNlcjc2Njk4OTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7669893?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cahya-wirawan", "html_url": "https://github.com/cahya-wirawan", "followers_url": "https://api.github.com/users/cahya-wirawan/followers", "following_url": "https://api.github.com/users/cahya-wirawan/following{/other_user}", "gists_url": "https://api.github.com/users/cahya-wirawan/gists{/gist_id}", "starred_url": "https://api.github.com/users/cahya-wirawan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cahya-wirawan/subscriptions", "organizations_url": "https://api.github.com/users/cahya-wirawan/orgs", "repos_url": "https://api.github.com/users/cahya-wirawan/repos", "events_url": "https://api.github.com/users/cahya-wirawan/events{/privacy}", "received_events_url": "https://api.github.com/users/cahya-wirawan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-10T16:10:29Z", "updated_at": "2020-07-16T17:40:32Z", "closed_at": "2020-07-16T17:40:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI have tokenizer version 0.8.1rc1. I have a problem to load the vocabulary with BertWordPieceTokenizer, it says\r\nTypeError: sep_token not found in the vocabulary, even the vocab is just trained and saved with the same version.\r\nPlease have a look to this code snippet:\r\n\r\n```\r\n>>> from tokenizers import BertWordPieceTokenizer\r\n>>> tokenizer = BertWordPieceTokenizer()\r\n>>> tokenizer.train([\"train.txt\"], vocab_size=10000)\r\n[00:00:00] Reading files (1 Mo)                     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 100\r\n[00:00:00] Tokenize words                           \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 16090    /    16090\r\n[00:00:00] Count pairs                              \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 16090    /    16090\r\n[00:00:00] Compute merges                           \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 9617     /     9617\r\n\r\n>>> tokenizer.save(\"vocab.txt\")\r\n>>> tokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase=True)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/wirawan/miniconda3/envs/transformers/lib/python3.7/site-packages/tokenizers/implementations/bert_wordpiece.py\", line 57, in __init__\r\n    raise TypeError(\"sep_token not found in the vocabulary\")\r\nTypeError: sep_token not found in the vocabulary\r\n\r\n```\r\nDid I miss something? I don't have problem if I train,save and load with ByteLevelBPETokenizer. Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/335", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/335/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/335/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/335/events", "html_url": "https://github.com/huggingface/tokenizers/issues/335", "id": 653215252, "node_id": "MDU6SXNzdWU2NTMyMTUyNTI=", "number": 335, "title": "Cannot save BPE files", "user": {"login": "scheiblr", "id": 2667575, "node_id": "MDQ6VXNlcjI2Njc1NzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/2667575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scheiblr", "html_url": "https://github.com/scheiblr", "followers_url": "https://api.github.com/users/scheiblr/followers", "following_url": "https://api.github.com/users/scheiblr/following{/other_user}", "gists_url": "https://api.github.com/users/scheiblr/gists{/gist_id}", "starred_url": "https://api.github.com/users/scheiblr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scheiblr/subscriptions", "organizations_url": "https://api.github.com/users/scheiblr/orgs", "repos_url": "https://api.github.com/users/scheiblr/repos", "events_url": "https://api.github.com/users/scheiblr/events{/privacy}", "received_events_url": "https://api.github.com/users/scheiblr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-08T11:26:52Z", "updated_at": "2020-07-09T07:11:57Z", "closed_at": "2020-07-08T14:17:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nfirst of all thanks a lot for this amzingly fast implementation!\r\n\r\nI just encountered a problem, when trying to save a generated BPE:\r\n```python\r\nfrom tokenizers import ByteLevelBPETokenizer\r\n\r\n# Initialize a tokenizer\r\ntokenizer = ByteLevelBPETokenizer()\r\n\r\n# Customize training\r\ntokenizer.train(files=[\"data.txt\"], vocab_size=52_000, min_frequency=2, special_tokens=[\r\n    \"<s>\",\r\n    \"<pad>\",\r\n    \"</s>\",\r\n    \"<unk>\",\r\n    \"<mask>\",\r\n])\r\n\r\n# Save files to disk\r\ntokenizer.save('.') # <- error here!\r\n```\r\n\r\nthe error I get: \r\n```\r\nTraceback (most recent call last):\r\n  File \"bpe/create_dict_hf.py\", line 21, in <module>\r\n    tokenizer.save(dir_raw)\r\n  File \"/somepath/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py\", line 332, in save\r\n    return self._tokenizer.save(path, pretty)\r\nException: Is a directory (os error 21)\r\n```\r\n\r\nWith version 0.7.0 it works.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/332", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/332/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/332/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/332/events", "html_url": "https://github.com/huggingface/tokenizers/issues/332", "id": 651747060, "node_id": "MDU6SXNzdWU2NTE3NDcwNjA=", "number": 332, "title": "Truncation warning when tokenizing inputs with T5Tokenizer", "user": {"login": "mrm8488", "id": 3653789, "node_id": "MDQ6VXNlcjM2NTM3ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3653789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrm8488", "html_url": "https://github.com/mrm8488", "followers_url": "https://api.github.com/users/mrm8488/followers", "following_url": "https://api.github.com/users/mrm8488/following{/other_user}", "gists_url": "https://api.github.com/users/mrm8488/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrm8488/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrm8488/subscriptions", "organizations_url": "https://api.github.com/users/mrm8488/orgs", "repos_url": "https://api.github.com/users/mrm8488/repos", "events_url": "https://api.github.com/users/mrm8488/events{/privacy}", "received_events_url": "https://api.github.com/users/mrm8488/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-06T18:47:00Z", "updated_at": "2020-07-08T00:28:40Z", "closed_at": "2020-07-08T00:28:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi guys,\r\nI had this code:\r\n\r\n```python\r\n # tokenize inputs\r\n      tokenized_inputs = self.tokenizer.batch_encode_plus(\r\n          [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\r\n      )\r\n```\r\nAnd it worked without any problem.\r\nBut, Maybe because of new updates on the library I get the following message tons of times:\r\n```Truncation was not explicitly activated but max_length is provide, please...```\r\nAm I missing smth?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/331", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/331/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/331/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/331/events", "html_url": "https://github.com/huggingface/tokenizers/issues/331", "id": 651720041, "node_id": "MDU6SXNzdWU2NTE3MjAwNDE=", "number": 331, "title": "Matching published performance numbers", "user": {"login": "albertoa", "id": 2904025, "node_id": "MDQ6VXNlcjI5MDQwMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/2904025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertoa", "html_url": "https://github.com/albertoa", "followers_url": "https://api.github.com/users/albertoa/followers", "following_url": "https://api.github.com/users/albertoa/following{/other_user}", "gists_url": "https://api.github.com/users/albertoa/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertoa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertoa/subscriptions", "organizations_url": "https://api.github.com/users/albertoa/orgs", "repos_url": "https://api.github.com/users/albertoa/repos", "events_url": "https://api.github.com/users/albertoa/events{/privacy}", "received_events_url": "https://api.github.com/users/albertoa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-06T17:55:00Z", "updated_at": "2020-07-06T19:48:21Z", "closed_at": "2020-07-06T19:48:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Could you give more details on how you obtained \"20 seconds to tokenize a GB of text\"?\r\n\r\nIt would be great if you could provide the exact input file, the code used and some information as to the server used.\r\n\r\nUsing the first 100MB of wikitext-103 I am getting 36 seconds on a Intel(R) Xeon(R) W-2155 CPU @ 3.30GHz. \r\n\r\nI can see only 1 CPU core in use pegged at 100%, so it is unclear if the rust implementation is being used.\r\n\r\nhere is my code\r\n\r\n```\r\nfrom tokenizers import (ByteLevelBPETokenizer,\r\n                            CharBPETokenizer,\r\n                            SentencePieceBPETokenizer,\r\n                            BertWordPieceTokenizer)\r\nimport time\r\nimport os\r\n\r\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\r\n\r\nfn=\"wikitext-103-100M.txt\"\r\n\r\nt1 = time.time()\r\nwith open(fn, 'rt') as inf:\r\n    txt = inf.read()\r\nt2 = time.time()\r\nprint(f\"Reading txt took {t2-t1} sec.\")\r\n\r\ntokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\r\n\r\nt1 = time.time()\r\noutput = tokenizer.encode(txt)\r\nt2 = time.time()\r\nprint(f\"Encoding took {t2-t1} sec.\")\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/328", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/328/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/328/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/328/events", "html_url": "https://github.com/huggingface/tokenizers/issues/328", "id": 651531808, "node_id": "MDU6SXNzdWU2NTE1MzE4MDg=", "number": 328, "title": "Improve experience with TOKENIZERS_PARALLELISM", "user": {"login": "n1t0", "id": 1217986, "node_id": "MDQ6VXNlcjEyMTc5ODY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1217986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/n1t0", "html_url": "https://github.com/n1t0", "followers_url": "https://api.github.com/users/n1t0/followers", "following_url": "https://api.github.com/users/n1t0/following{/other_user}", "gists_url": "https://api.github.com/users/n1t0/gists{/gist_id}", "starred_url": "https://api.github.com/users/n1t0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/n1t0/subscriptions", "organizations_url": "https://api.github.com/users/n1t0/orgs", "repos_url": "https://api.github.com/users/n1t0/repos", "events_url": "https://api.github.com/users/n1t0/events{/privacy}", "received_events_url": "https://api.github.com/users/n1t0/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-06T13:18:36Z", "updated_at": "2020-07-06T18:52:08Z", "closed_at": "2020-07-06T18:52:08Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "The message is not explicit enough, so some users don't understand that they should set an environment variable. Also, we could probably provide a function that, given a boolean, would set it for the user.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/326", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/326/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/326/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/326/events", "html_url": "https://github.com/huggingface/tokenizers/issues/326", "id": 651065833, "node_id": "MDU6SXNzdWU2NTEwNjU4MzM=", "number": 326, "title": "tokenizer for masked lm question", "user": {"login": "siheming", "id": 30624256, "node_id": "MDQ6VXNlcjMwNjI0MjU2", "avatar_url": "https://avatars1.githubusercontent.com/u/30624256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/siheming", "html_url": "https://github.com/siheming", "followers_url": "https://api.github.com/users/siheming/followers", "following_url": "https://api.github.com/users/siheming/following{/other_user}", "gists_url": "https://api.github.com/users/siheming/gists{/gist_id}", "starred_url": "https://api.github.com/users/siheming/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/siheming/subscriptions", "organizations_url": "https://api.github.com/users/siheming/orgs", "repos_url": "https://api.github.com/users/siheming/repos", "events_url": "https://api.github.com/users/siheming/events{/privacy}", "received_events_url": "https://api.github.com/users/siheming/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-05T13:00:16Z", "updated_at": "2020-07-06T13:09:22Z", "closed_at": "2020-07-06T13:09:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI am using transformer version 3.0.0 for my project and have some questions.\r\n\r\nI want to use a bert model with masked lm pretraining for protein sequences.\r\nTo get a character level tokenizer I derived from the BertTokenizer\r\n```\r\nfrom transformers import BertTokenizer\r\nclass DerivedBertTok(BertTokenizer):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n    def tokenize(self, text):\r\n        if isinstance(text, np.ndarray):\r\n            assert len(text) == 1\r\n            text = text[0]\r\n        return [x if x in self.vocab else self.unk_token for x in text]\r\n```\r\nmy vocab looks like this\r\n```\r\n[PAD]\r\n[CLS]\r\n[SEP]\r\n[UNK]\r\n[MASK]\r\nA\r\nR\r\nN\r\nD\r\nB\r\nC\r\nE\r\nQ\r\nZ\r\nG\r\nH\r\nI\r\nL\r\nK\r\nM\r\nF\r\nP\r\nS\r\nT\r\nW\r\nY\r\nV\r\n```\r\nThe usage seems quite similar to what i have seen in the docs:\r\n```\r\nd_tokenizer = DerivedBertTok(\r\n    vocab_file=vocab_path,\r\n    do_lower_case=False,\r\n    do_basic_tokenize=False,\r\n    tokenize_chinese_chars=False\r\n)\r\nd_tokenizer.encode_plus(np.array([\"AXEF\"])[0], \r\n                      max_length=20,\r\n                      pad_to_max_length=True,\r\n                      add_special_tokens=True,\r\n                      truncation=True,\r\n                      return_tensors='pt')\r\n```\r\n\r\nFrom this I was building a pytorch Dataset with a custom collate function.\r\nall the collate function does is taking all input tensors and stacking them\r\n```\r\nfrom transformers import BatchEncoding\r\n    def collate_fn(self, batch):\r\n        # this function will not work for higher dimension inputs\r\n        elem = batch[0]\r\n        elem_type = type(elem)\r\n        if isinstance(elem, BatchEncoding):\r\n            new_shapes = {key: (len(batch), value.shape[1]) for key, value in elem.items()}\r\n            outs = {key: value.new_empty(new_shapes[key]) for key, value in elem.items()}\r\n            if torch.utils.data.get_worker_info() is not None:\r\n                [v.share_memory_() for v in outs.values()]\r\n            return {key: torch.stack(tuple((d[key].view(-1) for d in batch)), 0, out=outs[key]) for key in elem.keys()}\r\n        else:\r\n            raise ValueError(f\"type: {elem_type} not understood\")\r\n```\r\nSo I was wondering if the BatchEncoding or another class is already capable of doing this (and doing it possibly better?)\r\n\r\nAdditionally, I want to mask some of the Inputs as required for the masked LM, however I did not manage find any implementation in the transformer library. Are there any recommendations for doing this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/324", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/324/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/324/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/324/events", "html_url": "https://github.com/huggingface/tokenizers/issues/324", "id": 648519404, "node_id": "MDU6SXNzdWU2NDg1MTk0MDQ=", "number": 324, "title": "Encoded Chinese strings become unrecognizable characters", "user": {"login": "amyxie361", "id": 26669286, "node_id": "MDQ6VXNlcjI2NjY5Mjg2", "avatar_url": "https://avatars2.githubusercontent.com/u/26669286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amyxie361", "html_url": "https://github.com/amyxie361", "followers_url": "https://api.github.com/users/amyxie361/followers", "following_url": "https://api.github.com/users/amyxie361/following{/other_user}", "gists_url": "https://api.github.com/users/amyxie361/gists{/gist_id}", "starred_url": "https://api.github.com/users/amyxie361/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amyxie361/subscriptions", "organizations_url": "https://api.github.com/users/amyxie361/orgs", "repos_url": "https://api.github.com/users/amyxie361/repos", "events_url": "https://api.github.com/users/amyxie361/events{/privacy}", "received_events_url": "https://api.github.com/users/amyxie361/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-30T21:52:40Z", "updated_at": "2020-07-10T14:37:43Z", "closed_at": "2020-07-10T14:37:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there,\r\n\r\nI want to train a language model from scratch in Chinese. In the first step of tokenization training, I follow [this page](https://github.com/huggingface/transformers/blob/master/notebooks/01-training-tokenizers.ipynb)\r\n\r\nWhen the tokenizer finishes training and I check the results and found the encoder's results are not understandable.\r\n\r\nHere are the codes:\r\n\r\n```\r\nfrom tokenizers import Tokenizer\r\nfrom tokenizers.decoders import ByteLevel as ByteLevelDecoder\r\nfrom tokenizers.models import BPE\r\nfrom tokenizers.normalizers import Lowercase, NFKC, Sequence\r\nfrom tokenizers.pre_tokenizers import ByteLevel\r\n\r\ntokenizer = Tokenizer(BPE())\r\ntokenizer.normalizer = Sequence([\r\n    NFKC()\r\n])\r\ntokenizer.pre_tokenizer = ByteLevel()\r\ntokenizer.decoder = ByteLevelDecoder()\r\n\r\nfrom tokenizers.trainers import BpeTrainer\r\n\r\ntrainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\r\ntokenizer.train(trainer, [\"big.txt\"])\r\n\r\nprint(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))\r\n\r\ntokenizer.model.save('.')\r\n\r\ntokenizer.model = BPE('vocab.json', 'merges.txt')\r\nencoding = tokenizer.encode(\"\u8fd9\u662f\u4e00\u53e5\u4e2d\u6587\")\r\n\r\nprint(\"Encoded string: {}\".format(encoding.tokens))\r\n\r\ndecoded = tokenizer.decode(encoding.ids)\r\nprint(\"Decoded string: {}\".format(decoded))\r\n```\r\nAnd the output is \r\n```\r\nEncoded string: ['\u0120\u00e8\u00bf\u013b\u00e6\u013a\u00af', '\u00e4\u00b8\u0122\u00e5\u0131\u00a5', '\u00e4\u00b8\u0143\u00e6\u0138\u0129']\r\nDecoded string:  \u8fd9\u662f\u4e00\u53e5\u4e2d\u6587\r\n```\r\nThe decoded string is correct but the characters in the encoded string are not understandable.\r\nHow can I solve this? Or am I supposed to use this results to do the next step?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/322", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/322/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/322/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/322/events", "html_url": "https://github.com/huggingface/tokenizers/issues/322", "id": 646949958, "node_id": "MDU6SXNzdWU2NDY5NDk5NTg=", "number": 322, "title": "tokenizer.train encounter core dump when the file does not exist.", "user": {"login": "fuzihaofzh", "id": 1419566, "node_id": "MDQ6VXNlcjE0MTk1NjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1419566?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fuzihaofzh", "html_url": "https://github.com/fuzihaofzh", "followers_url": "https://api.github.com/users/fuzihaofzh/followers", "following_url": "https://api.github.com/users/fuzihaofzh/following{/other_user}", "gists_url": "https://api.github.com/users/fuzihaofzh/gists{/gist_id}", "starred_url": "https://api.github.com/users/fuzihaofzh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fuzihaofzh/subscriptions", "organizations_url": "https://api.github.com/users/fuzihaofzh/orgs", "repos_url": "https://api.github.com/users/fuzihaofzh/repos", "events_url": "https://api.github.com/users/fuzihaofzh/events{/privacy}", "received_events_url": "https://api.github.com/users/fuzihaofzh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1652177653, "node_id": "MDU6TGFiZWwxNjUyMTc3NjUz", "url": "https://api.github.com/repos/huggingface/tokenizers/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-28T15:40:46Z", "updated_at": "2020-08-03T16:08:18Z", "closed_at": "2020-08-03T16:08:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "The tokenizer.train encounter core dump when the file does not exist. I hope it could be more elegant to show the error.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/321", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/321/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/321/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/321/events", "html_url": "https://github.com/huggingface/tokenizers/issues/321", "id": 646936389, "node_id": "MDU6SXNzdWU2NDY5MzYzODk=", "number": 321, "title": "import error Symbol not found: ____chkstk_darwin", "user": {"login": "sumitjha4321", "id": 11829867, "node_id": "MDQ6VXNlcjExODI5ODY3", "avatar_url": "https://avatars0.githubusercontent.com/u/11829867?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sumitjha4321", "html_url": "https://github.com/sumitjha4321", "followers_url": "https://api.github.com/users/sumitjha4321/followers", "following_url": "https://api.github.com/users/sumitjha4321/following{/other_user}", "gists_url": "https://api.github.com/users/sumitjha4321/gists{/gist_id}", "starred_url": "https://api.github.com/users/sumitjha4321/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sumitjha4321/subscriptions", "organizations_url": "https://api.github.com/users/sumitjha4321/orgs", "repos_url": "https://api.github.com/users/sumitjha4321/repos", "events_url": "https://api.github.com/users/sumitjha4321/events{/privacy}", "received_events_url": "https://api.github.com/users/sumitjha4321/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 24, "created_at": "2020-06-28T14:40:56Z", "updated_at": "2020-07-24T01:34:31Z", "closed_at": "2020-07-18T12:23:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am getting below error when importing `tokenizers`.\r\n\r\nDetails:\r\n\r\n- Platform: MacOS 10.13.6\r\n- Installed using `pip` inside `conda env`.\r\n\r\n>ImportError: dlopen(/Users/sumit.jha/.conda/envs/kaggle/lib/python3.7/site-packages/tokenizers/tokenizers.cpython-37m-darwin.so, 2): Symbol not found: ____chkstk_darwin\r\n  Referenced from: /Users/sumit.jha/.conda/envs/kaggle/lib/python3.7/site-packages/tokenizers/tokenizers.cpython-37m-darwin.so (which was built for Mac OS X 10.15)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /Users/sumit.jha/.conda/envs/kaggle/lib/python3.7/site-packages/tokenizers/tokenizers.cpython-37m-darwin.so", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/319", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/319/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/319/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/319/events", "html_url": "https://github.com/huggingface/tokenizers/issues/319", "id": 646579343, "node_id": "MDU6SXNzdWU2NDY1NzkzNDM=", "number": 319, "title": "CharBPETokenizer.add_tokens misses first argument", "user": {"login": "timothyjlaurent", "id": 2000204, "node_id": "MDQ6VXNlcjIwMDAyMDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2000204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timothyjlaurent", "html_url": "https://github.com/timothyjlaurent", "followers_url": "https://api.github.com/users/timothyjlaurent/followers", "following_url": "https://api.github.com/users/timothyjlaurent/following{/other_user}", "gists_url": "https://api.github.com/users/timothyjlaurent/gists{/gist_id}", "starred_url": "https://api.github.com/users/timothyjlaurent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timothyjlaurent/subscriptions", "organizations_url": "https://api.github.com/users/timothyjlaurent/orgs", "repos_url": "https://api.github.com/users/timothyjlaurent/repos", "events_url": "https://api.github.com/users/timothyjlaurent/events{/privacy}", "received_events_url": "https://api.github.com/users/timothyjlaurent/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-27T01:11:28Z", "updated_at": "2020-06-27T01:33:03Z", "closed_at": "2020-06-27T01:33:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/318", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/318/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/318/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/318/events", "html_url": "https://github.com/huggingface/tokenizers/issues/318", "id": 646549351, "node_id": "MDU6SXNzdWU2NDY1NDkzNTE=", "number": 318, "title": "Tokenizers 0.8.0 is version incompatible with transformers 2.11", "user": {"login": "timothyjlaurent", "id": 2000204, "node_id": "MDQ6VXNlcjIwMDAyMDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2000204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timothyjlaurent", "html_url": "https://github.com/timothyjlaurent", "followers_url": "https://api.github.com/users/timothyjlaurent/followers", "following_url": "https://api.github.com/users/timothyjlaurent/following{/other_user}", "gists_url": "https://api.github.com/users/timothyjlaurent/gists{/gist_id}", "starred_url": "https://api.github.com/users/timothyjlaurent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timothyjlaurent/subscriptions", "organizations_url": "https://api.github.com/users/timothyjlaurent/orgs", "repos_url": "https://api.github.com/users/timothyjlaurent/repos", "events_url": "https://api.github.com/users/timothyjlaurent/events{/privacy}", "received_events_url": "https://api.github.com/users/timothyjlaurent/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-26T22:52:13Z", "updated_at": "2020-06-29T15:49:04Z", "closed_at": "2020-06-29T15:49:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "```shell\r\n15:48:51 \u276f poetry add \"tokenizers>=0.8\"\r\n\r\nUpdating dependencies\r\n\r\nResolving dependencies... (1.0s)\r\n[SolverProblemError]\r\nBecause transformers (2.11.0) depends on tokenizers (0.7.0)\r\n and no versions of transformers match >2.11.0,<3.0.0, transformers (>=2.11.0,<3.0.0) requires tokenizers (0.7.0).\r\nSo, because inv-text2struct depends on both transformers (^2.11.0) and tokenizers (>=0.8), version solving failed.\r\n\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/317", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/317/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/317/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/317/events", "html_url": "https://github.com/huggingface/tokenizers/issues/317", "id": 645028683, "node_id": "MDU6SXNzdWU2NDUwMjg2ODM=", "number": 317, "title": "support for subword-nmt-style glossaries?", "user": {"login": "timothyjlaurent", "id": 2000204, "node_id": "MDQ6VXNlcjIwMDAyMDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2000204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timothyjlaurent", "html_url": "https://github.com/timothyjlaurent", "followers_url": "https://api.github.com/users/timothyjlaurent/followers", "following_url": "https://api.github.com/users/timothyjlaurent/following{/other_user}", "gists_url": "https://api.github.com/users/timothyjlaurent/gists{/gist_id}", "starred_url": "https://api.github.com/users/timothyjlaurent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timothyjlaurent/subscriptions", "organizations_url": "https://api.github.com/users/timothyjlaurent/orgs", "repos_url": "https://api.github.com/users/timothyjlaurent/repos", "events_url": "https://api.github.com/users/timothyjlaurent/events{/privacy}", "received_events_url": "https://api.github.com/users/timothyjlaurent/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-06-24T22:56:27Z", "updated_at": "2020-08-19T12:12:16Z", "closed_at": "2020-06-26T22:50:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "We are currently using subword-nmt bpe tokenizer for a job and are using its \"Glossary\" parameters to be able ignore certain symbols using regular expressions.\r\n\r\nI understand that Tokenizers has the ability to specify special tokens, but these are removed during decoding.\r\n\r\nIs there any good way to add a glossary a la subword-nmt bpe, using regexes that will be left alone on encode and decode.\r\n\r\nhttps://github.com/rsennrich/subword-nmt\r\n> support for glossaries: use the argument --glossaries for subword-nmt apply-bpe to provide a list of words and/or regular expressions that should always be passed to the output without subword segmentation", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/316", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/316/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/316/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/316/events", "html_url": "https://github.com/huggingface/tokenizers/issues/316", "id": 644862960, "node_id": "MDU6SXNzdWU2NDQ4NjI5NjA=", "number": 316, "title": "Process automatically get killed during Training ByteLevelBPETokenizer on large datasets", "user": {"login": "ad6398", "id": 38162294, "node_id": "MDQ6VXNlcjM4MTYyMjk0", "avatar_url": "https://avatars1.githubusercontent.com/u/38162294?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ad6398", "html_url": "https://github.com/ad6398", "followers_url": "https://api.github.com/users/ad6398/followers", "following_url": "https://api.github.com/users/ad6398/following{/other_user}", "gists_url": "https://api.github.com/users/ad6398/gists{/gist_id}", "starred_url": "https://api.github.com/users/ad6398/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ad6398/subscriptions", "organizations_url": "https://api.github.com/users/ad6398/orgs", "repos_url": "https://api.github.com/users/ad6398/repos", "events_url": "https://api.github.com/users/ad6398/events{/privacy}", "received_events_url": "https://api.github.com/users/ad6398/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-06-24T19:18:53Z", "updated_at": "2020-06-25T05:09:10Z", "closed_at": "2020-06-25T05:09:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to train it on the open web text corpus,( 30k files, 31GB). but after a few minutes(30 min approx) this process gets killed. Initially, I thought it to be a machine issue, but later on, trying with a better machine I am still getting the same error. The following is code to reproduce this error. I am not getting any other error log/output other than the `killed` message.\r\n```python\r\ntokenizer = ByteLevelBPETokenizer(lowercase=True, add_prefix_space=True)\r\ntokenizer.train(\r\n        files= file_paths_to_open_web_text_corpus,\r\n        vocab_size= vocab_size,\r\n        min_frequency= min_fre,\r\n        special_tokens= [\"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[UNK]\",],\r\n    )\r\n```\r\nThis just gets killed during reading files step.\r\nI have also checked limits on the opening of number of files at a time of the machine, etc.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/314", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/314/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/314/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/314/events", "html_url": "https://github.com/huggingface/tokenizers/issues/314", "id": 642944051, "node_id": "MDU6SXNzdWU2NDI5NDQwNTE=", "number": 314, "title": "tokenizer.save produces invalid json (python binding, 0.8.0.rc2)", "user": {"login": "pdufter", "id": 13961899, "node_id": "MDQ6VXNlcjEzOTYxODk5", "avatar_url": "https://avatars2.githubusercontent.com/u/13961899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pdufter", "html_url": "https://github.com/pdufter", "followers_url": "https://api.github.com/users/pdufter/followers", "following_url": "https://api.github.com/users/pdufter/following{/other_user}", "gists_url": "https://api.github.com/users/pdufter/gists{/gist_id}", "starred_url": "https://api.github.com/users/pdufter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pdufter/subscriptions", "organizations_url": "https://api.github.com/users/pdufter/orgs", "repos_url": "https://api.github.com/users/pdufter/repos", "events_url": "https://api.github.com/users/pdufter/events{/privacy}", "received_events_url": "https://api.github.com/users/pdufter/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-22T10:18:14Z", "updated_at": "2020-06-22T16:52:57Z", "closed_at": "2020-06-22T16:52:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "*Version info:*\r\n`tokenizers==0.8.0.rc2`\r\n\r\n*Description:*\r\ntokenizer.save produces invalid json in the field `added_tokens`. As a result it is not possible to load a stored tokenizer from the json file with `tokenizers.Tokenizer.from_file`\r\n\r\nThe json file should probably be \r\n```\"added_tokens\":[{\"id\":0,\"special\":true,\"content\":\" ...``` instead of\r\n```\"added_tokens\":[],{\"id\":0,\"special\":true,\"content\":\" ...```.\r\n\r\n\r\n*Minimal Example:*\r\n```\r\nimport tokenizers\r\n\r\nwith open(\"corpus.txt\", \"w\") as fp:\r\n\tfp.write(\"This is an example sentence.\")\r\ntok = tokenizers.BertWordPieceTokenizer()\r\ntok.train([\"corpus.txt\"])\r\ntok.save(\"tokenizer.json\")\r\n\r\n# try to load the tokenizer\r\ntok_loaded = tokenizers.Tokenizer.from_file(\"tokenizer.json\")\r\n# breaks with \r\n# >>> Exception: key must be a string at line 1 column 69\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/313", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/313/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/313/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/313/events", "html_url": "https://github.com/huggingface/tokenizers/issues/313", "id": 642829111, "node_id": "MDU6SXNzdWU2NDI4MjkxMTE=", "number": 313, "title": "Tokenizer/ByteLevelBPETokenizer objects are not serializable", "user": {"login": "ad6398", "id": 38162294, "node_id": "MDQ6VXNlcjM4MTYyMjk0", "avatar_url": "https://avatars1.githubusercontent.com/u/38162294?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ad6398", "html_url": "https://github.com/ad6398", "followers_url": "https://api.github.com/users/ad6398/followers", "following_url": "https://api.github.com/users/ad6398/following{/other_user}", "gists_url": "https://api.github.com/users/ad6398/gists{/gist_id}", "starred_url": "https://api.github.com/users/ad6398/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ad6398/subscriptions", "organizations_url": "https://api.github.com/users/ad6398/orgs", "repos_url": "https://api.github.com/users/ad6398/repos", "events_url": "https://api.github.com/users/ad6398/events{/privacy}", "received_events_url": "https://api.github.com/users/ad6398/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-22T07:31:00Z", "updated_at": "2020-06-24T13:04:15Z", "closed_at": "2020-06-24T13:04:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to do tokenization step of a file by converting into chunks and distributing it to multiple processes. But the main process is not able to pass tokenizer between subprocess.\r\n\r\n```python\r\ndef single_work(wid, tokenizer, chunk):\r\n    print(\"call single process\")\r\n\r\ndef func1(arguments):\r\n    tokenizer = ByteLevelBPETokenizer(\r\n            vocab_file, merges_file, lowercase=lowercase, add_prefix_space=True\r\n        )\r\n    pool = Pool()\r\n    for i in range(workers):\r\n        pool.apply_async(single_work, args=(i,tokenizer, chunks[i]))\r\n    pool.close()\r\n    pool.join()\r\n```\r\n\r\n`single_work` function is not being called by pool.apply_async, the reason is that multi-process uses pickling to pass arguments between all the subprocesses, so all the arguments should be serializable.  tokenizer seems here to be that non-serializable argument. \r\nif I used `tokenizer = BertTokenizer.from_pretrained(bert_model_type)` from transformers this is working fine. \r\n\r\n*update*\r\nusing `tokenizer= RobertaTokenizerFast()` or `tokenizer= RobertaTokenizerFast.from_pretrained()` from transformers library are also not serializable, as these are built over tokenizer library. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/304", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/304/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/304/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/304/events", "html_url": "https://github.com/huggingface/tokenizers/issues/304", "id": 638097853, "node_id": "MDU6SXNzdWU2MzgwOTc4NTM=", "number": 304, "title": "Progress not getting displayed while training ByteLevelBPETokenizer", "user": {"login": "sayakpaul", "id": 22957388, "node_id": "MDQ6VXNlcjIyOTU3Mzg4", "avatar_url": "https://avatars3.githubusercontent.com/u/22957388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sayakpaul", "html_url": "https://github.com/sayakpaul", "followers_url": "https://api.github.com/users/sayakpaul/followers", "following_url": "https://api.github.com/users/sayakpaul/following{/other_user}", "gists_url": "https://api.github.com/users/sayakpaul/gists{/gist_id}", "starred_url": "https://api.github.com/users/sayakpaul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sayakpaul/subscriptions", "organizations_url": "https://api.github.com/users/sayakpaul/orgs", "repos_url": "https://api.github.com/users/sayakpaul/repos", "events_url": "https://api.github.com/users/sayakpaul/events{/privacy}", "received_events_url": "https://api.github.com/users/sayakpaul/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-13T05:07:12Z", "updated_at": "2020-06-15T12:35:38Z", "closed_at": "2020-06-15T12:35:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am training a `ByteLevelBPETokenizer` and while training it the progress is not getting displayed even though I have explicitly set `show_progress=True`. Here's the [Colab Notebook](https://colab.research.google.com/gist/sayakpaul/939b16bb4d5376822a46089bd52e0bd8/tokenization.ipynb) that can reproduce this issue.  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/302", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/302/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/302/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/302/events", "html_url": "https://github.com/huggingface/tokenizers/issues/302", "id": 636915176, "node_id": "MDU6SXNzdWU2MzY5MTUxNzY=", "number": 302, "title": "Some rough edges with add_tokens", "user": {"login": "thomwolf", "id": 7353373, "node_id": "MDQ6VXNlcjczNTMzNzM=", "avatar_url": "https://avatars2.githubusercontent.com/u/7353373?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomwolf", "html_url": "https://github.com/thomwolf", "followers_url": "https://api.github.com/users/thomwolf/followers", "following_url": "https://api.github.com/users/thomwolf/following{/other_user}", "gists_url": "https://api.github.com/users/thomwolf/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomwolf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomwolf/subscriptions", "organizations_url": "https://api.github.com/users/thomwolf/orgs", "repos_url": "https://api.github.com/users/thomwolf/repos", "events_url": "https://api.github.com/users/thomwolf/events{/privacy}", "received_events_url": "https://api.github.com/users/thomwolf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-11T10:48:39Z", "updated_at": "2020-06-19T14:42:59Z", "closed_at": "2020-06-19T14:42:59Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Here are some rough edges I identified when comparing with the python-based tokenizers in `transformers`. Maybe you would want to fix them?\r\n\r\n- **Lower-casing**: adding not-lower-case tokens to the vocabulary with `add_tokens` when `do_lower_case` is `True` means these tokens won't be recognized later as they are stored without being lower-cased. Also, if we store them lower-cased then we don't recognize upper cased token that are identical <= I found no work-around for this issue in `transformers`.\r\n- **Adding several identical tokens**: currently giving a list of identical tokens will add all of them instead of adding just a single token.\r\n- **Adding tokens with spaces**: at the moment I don't think we can add tokens with spaces inside them.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/301", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/301/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/301/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/301/events", "html_url": "https://github.com/huggingface/tokenizers/issues/301", "id": 636601362, "node_id": "MDU6SXNzdWU2MzY2MDEzNjI=", "number": 301, "title": "Python manylinux wheel files include compiled libraries for multiple Python versions", "user": {"login": "Stranger6667", "id": 1236561, "node_id": "MDQ6VXNlcjEyMzY1NjE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1236561?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stranger6667", "html_url": "https://github.com/Stranger6667", "followers_url": "https://api.github.com/users/Stranger6667/followers", "following_url": "https://api.github.com/users/Stranger6667/following{/other_user}", "gists_url": "https://api.github.com/users/Stranger6667/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stranger6667/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stranger6667/subscriptions", "organizations_url": "https://api.github.com/users/Stranger6667/orgs", "repos_url": "https://api.github.com/users/Stranger6667/repos", "events_url": "https://api.github.com/users/Stranger6667/events{/privacy}", "received_events_url": "https://api.github.com/users/Stranger6667/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-10T23:04:45Z", "updated_at": "2020-06-11T16:44:57Z", "closed_at": "2020-06-11T16:44:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello!\r\n\r\nIn the Python build output, there are the following [lines](https://github.com/huggingface/tokenizers/runs/736452365?check_suite_focus=true#step:4:962)\r\n\r\n```\r\nadding 'tokenizers/tokenizers.cpython-35m-x86_64-linux-gnu.so'\r\nadding 'tokenizers/tokenizers.cpython-36m-x86_64-linux-gnu.so'\r\nadding 'tokenizers/tokenizers.cpython-37m-x86_64-linux-gnu.so'\r\nadding 'tokenizers/tokenizers.cpython-38-x86_64-linux-gnu.so\r\n```\r\n\r\nAnd the [published wheel](https://files.pythonhosted.org/packages/04/44/1da09bb16357fb7502b20631170e7b278df4d8567931bd104180dd62d2d8/tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl) indeed includes all these files. As far as my understanding goes - since the wheel is for CPython 3.8, `.so` files for other Python versions are not used anyway and can be removed. They also occupy significant space - each one uncompressed is ~6.4MB. What do you think about removing them? It should be 1 line (probably `rm` call after creating a wheel) in `build-wheels.sh` but will reduce traffic for the users\r\n\r\nCheers!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/300", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/300/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/300/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/300/events", "html_url": "https://github.com/huggingface/tokenizers/issues/300", "id": 636580709, "node_id": "MDU6SXNzdWU2MzY1ODA3MDk=", "number": 300, "title": "BertWordPieceTokenizer returns invalid offsets after .add_tokens() is called", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-10T22:10:10Z", "updated_at": "2020-06-11T20:32:13Z", "closed_at": "2020-06-11T20:32:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "- torch / tokenizers / transformers versions: `('1.5.0+cu101', '0.5.2', '2.8.0')`\r\n\r\nI'm using `BertWordPieceTokenizer` for ELECTRA model with custom tokens. However, when calling `add_tokens()`, offsets start to shift by 1 and original text can not be reconstructed.\r\n\r\nHere is a snippet to reproduce the issue:\r\n```python\r\n# Download tokenizer\r\ntokenizer = transformers.ElectraTokenizerFast.from_pretrained(\"google/electra-base-generator\")\r\ntokenizer.save_pretrained('.')  # saves vocab.txt file\r\n\r\n\r\n# Without `add_tokens()` call, works as expected\r\ntokenizer = tokenizers.BertWordPieceTokenizer('vocab.txt')\r\n\r\ntext = 'hello *** world'\r\nencoding = tokenizer.encode(text)\r\ntokens_from_offsets = [text[s:e] for s,e in encoding.offsets]\r\n\r\nassert encoding.tokens == ['[CLS]', 'hello', '*', '*', '*', 'world', '[SEP]']\r\nassert tokens_from_offsets == ['', 'hello', '*', '*', '*', 'world', '']\r\n\r\n\r\n# `add_tokens()`, offsets shift by 1\r\ntokenizer = tokenizers.BertWordPieceTokenizer('vocab.txt')\r\ntokenizer.add_tokens(['***'])\r\n\r\ntext = 'hello *** world'\r\nencoding = tokenizer.encode(text)\r\ntokens_from_offsets = [text[s:e] for s,e in encoding.offsets]\r\n\r\nassert encoding.tokens == ['[CLS]', 'hello', '***', 'world', '[SEP]']\r\n\r\n# This is not expected, it should be ['', 'hello', '***', 'world', '']\r\nassert tokens_from_offsets == ['', 'hello', ' **', ' worl', '']\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/297", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/297/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/297/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/297/events", "html_url": "https://github.com/huggingface/tokenizers/issues/297", "id": 635649751, "node_id": "MDU6SXNzdWU2MzU2NDk3NTE=", "number": 297, "title": "Bug with tokenizer save in 0.8.0.dev2", "user": {"login": "sarahwie", "id": 8027676, "node_id": "MDQ6VXNlcjgwMjc2NzY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8027676?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sarahwie", "html_url": "https://github.com/sarahwie", "followers_url": "https://api.github.com/users/sarahwie/followers", "following_url": "https://api.github.com/users/sarahwie/following{/other_user}", "gists_url": "https://api.github.com/users/sarahwie/gists{/gist_id}", "starred_url": "https://api.github.com/users/sarahwie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sarahwie/subscriptions", "organizations_url": "https://api.github.com/users/sarahwie/orgs", "repos_url": "https://api.github.com/users/sarahwie/repos", "events_url": "https://api.github.com/users/sarahwie/events{/privacy}", "received_events_url": "https://api.github.com/users/sarahwie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-06-09T18:02:24Z", "updated_at": "2020-08-02T02:53:53Z", "closed_at": "2020-06-29T16:14:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Version info: \r\n`transformers==2.9.1`\r\n`tokenizers==0.8.0.dev2`\r\n\r\n```\r\ntokenizer = GPT2TokenizerFast.from_pretrained('gpt2', cache_dir=cache_dir)\r\ntokenizer.save_pretrained(output_directory)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 1117, in save_pretrained\r\n    vocab_files = self.save_vocabulary(save_directory)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/transformers/tokenization_utils.py\", line 2657, in save_vocabulary\r\n    files = self._tokenizer.save(save_directory)\r\n  File \"/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/tokenizers/implementations/base_tokenizer.py\", line 312, in save\r\n    return self._tokenizer.save(path, pretty)\r\nException: Is a directory (os error 21)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/296", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/296/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/296/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/296/events", "html_url": "https://github.com/huggingface/tokenizers/issues/296", "id": 634388260, "node_id": "MDU6SXNzdWU2MzQzODgyNjA=", "number": 296, "title": "\u2753 How to see the token frequency ?", "user": {"login": "Colanim", "id": 43774355, "node_id": "MDQ6VXNlcjQzNzc0MzU1", "avatar_url": "https://avatars2.githubusercontent.com/u/43774355?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Colanim", "html_url": "https://github.com/Colanim", "followers_url": "https://api.github.com/users/Colanim/followers", "following_url": "https://api.github.com/users/Colanim/following{/other_user}", "gists_url": "https://api.github.com/users/Colanim/gists{/gist_id}", "starred_url": "https://api.github.com/users/Colanim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Colanim/subscriptions", "organizations_url": "https://api.github.com/users/Colanim/orgs", "repos_url": "https://api.github.com/users/Colanim/repos", "events_url": "https://api.github.com/users/Colanim/events{/privacy}", "received_events_url": "https://api.github.com/users/Colanim/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-08T08:51:38Z", "updated_at": "2020-06-08T23:41:44Z", "closed_at": "2020-06-08T23:41:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I trained a tokenizer on my own data :\r\n\r\n```\r\nfrom tokenizers import ByteLevelBPETokenizer\r\n\r\ntokenizer = ByteLevelBPETokenizer()\r\ntokenizer.train([\"./data.txt\"])\r\n```\r\n\r\nI know vocabulary is already ordered based on token frequency, but I'm interested on the precise number of occurence of each vocabulary word. **How can I access this information ?**", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/295", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/295/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/295/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/295/events", "html_url": "https://github.com/huggingface/tokenizers/issues/295", "id": 634065396, "node_id": "MDU6SXNzdWU2MzQwNjUzOTY=", "number": 295, "title": " add_special_tokens does not work with ByteLevelBPETokenizer (RoBERTa)", "user": {"login": "dsanyal", "id": 10409513, "node_id": "MDQ6VXNlcjEwNDA5NTEz", "avatar_url": "https://avatars0.githubusercontent.com/u/10409513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dsanyal", "html_url": "https://github.com/dsanyal", "followers_url": "https://api.github.com/users/dsanyal/followers", "following_url": "https://api.github.com/users/dsanyal/following{/other_user}", "gists_url": "https://api.github.com/users/dsanyal/gists{/gist_id}", "starred_url": "https://api.github.com/users/dsanyal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dsanyal/subscriptions", "organizations_url": "https://api.github.com/users/dsanyal/orgs", "repos_url": "https://api.github.com/users/dsanyal/repos", "events_url": "https://api.github.com/users/dsanyal/events{/privacy}", "received_events_url": "https://api.github.com/users/dsanyal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-08T03:31:54Z", "updated_at": "2020-06-12T19:17:37Z", "closed_at": "2020-06-12T19:17:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nWhen I call the `encode` method with an object of the `ByteLevelBPETokenizer` class, the `add_special_tokens` flag  does not work (does not add the special tokens) even when it is set to True. But while using `BertWordPieceTokenizer` it works as expected.\r\n\r\nExample:\r\n```\r\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=ROBERTA_PATH+'vocab-roberta-base.json', \r\n                                             merges_file=ROBERTA_PATH+'merges-roberta-base.txt', \r\n                                             lowercase=True,add_prefix_space=True)\r\n    \r\ntext = \"hello there\"\r\nenc = tokenizer.encode(text, add_special_tokens = True) \r\nenc.tokens\r\n```\r\nThe output is:\r\n`['\u0120hello', '\u0120there']`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/294", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/294/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/294/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/294/events", "html_url": "https://github.com/huggingface/tokenizers/issues/294", "id": 633832546, "node_id": "MDU6SXNzdWU2MzM4MzI1NDY=", "number": 294, "title": "Exception raised while trying to encode List[List[str]] with ElectraTokenizerFast", "user": {"login": "setu4993", "id": 1833708, "node_id": "MDQ6VXNlcjE4MzM3MDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1833708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/setu4993", "html_url": "https://github.com/setu4993", "followers_url": "https://api.github.com/users/setu4993/followers", "following_url": "https://api.github.com/users/setu4993/following{/other_user}", "gists_url": "https://api.github.com/users/setu4993/gists{/gist_id}", "starred_url": "https://api.github.com/users/setu4993/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/setu4993/subscriptions", "organizations_url": "https://api.github.com/users/setu4993/orgs", "repos_url": "https://api.github.com/users/setu4993/repos", "events_url": "https://api.github.com/users/setu4993/events{/privacy}", "received_events_url": "https://api.github.com/users/setu4993/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-06-07T22:40:33Z", "updated_at": "2020-06-08T16:43:05Z", "closed_at": "2020-06-08T16:43:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Seeing an unexpected issue being raised when trying to batch encode input of the type `List[List[str]]`.\r\n\r\nExample:\r\n```python\r\nfrom transformers import ElectraTokenizerFast\r\nfast_tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-discriminator\")\r\nfast_tokenizer.batch_encode_plus([[\"this is\", \"a test\"], [\"this is\", \"another test\"]])\r\n```\r\n\r\nRunning this raises the exception:\r\n`Exception: Input must be a list[str] or list[(str, str)]`\r\n\r\nReplacing the inner list with a tuple works, but this was an unexpected error, since `List[List[str]]` gets encoded fine with the normal tokenizer.\r\n```python\r\nfrom transformers import ElectraTokenizer\r\ntokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\r\ntokenizer.batch_encode_plus([[\"this is\", \"a test\"], [\"this is\", \"another test\"]])\r\n```\r\n\r\nTo make matters more weird, initializing the normal tokenizer with `use_fast` does not raise the same exception:\r\n```python\r\nfrom transformers import ElectraTokenizerFast\r\nfast_tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator', use_fast=True)\r\nfast_tokenizer.batch_encode_plus([[\"this is\", \"a test\"], [\"this is\", \"another test\"]])\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/293", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/293/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/293/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/293/events", "html_url": "https://github.com/huggingface/tokenizers/issues/293", "id": 633809384, "node_id": "MDU6SXNzdWU2MzM4MDkzODQ=", "number": 293, "title": "GPT2 for fixed length next word prediction", "user": {"login": "shampp", "id": 55344772, "node_id": "MDQ6VXNlcjU1MzQ0Nzcy", "avatar_url": "https://avatars1.githubusercontent.com/u/55344772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shampp", "html_url": "https://github.com/shampp", "followers_url": "https://api.github.com/users/shampp/followers", "following_url": "https://api.github.com/users/shampp/following{/other_user}", "gists_url": "https://api.github.com/users/shampp/gists{/gist_id}", "starred_url": "https://api.github.com/users/shampp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shampp/subscriptions", "organizations_url": "https://api.github.com/users/shampp/orgs", "repos_url": "https://api.github.com/users/shampp/repos", "events_url": "https://api.github.com/users/shampp/events{/privacy}", "received_events_url": "https://api.github.com/users/shampp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-07T22:08:59Z", "updated_at": "2020-06-15T08:23:18Z", "closed_at": "2020-06-15T08:23:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to use huggingface GPT2 implementation to predict next word of fixed size ? I do not see an option in huggingface documentaion. I think one way to get this done is to use a tokenizer which produces vocabulary containing fixed length words only. But by default ByteLevel BPE tokenizer used in GPT models are not based on words. Is it possible to make ByteLevelBPE tokenizer to create vocabulary containing fixed length words only ?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/291", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/291/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/291/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/291/events", "html_url": "https://github.com/huggingface/tokenizers/issues/291", "id": 630057750, "node_id": "MDU6SXNzdWU2MzAwNTc3NTA=", "number": 291, "title": "Cannot load SentencePieceBPETokenizer pretrained model ", "user": {"login": "Ierezell", "id": 30974685, "node_id": "MDQ6VXNlcjMwOTc0Njg1", "avatar_url": "https://avatars2.githubusercontent.com/u/30974685?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ierezell", "html_url": "https://github.com/Ierezell", "followers_url": "https://api.github.com/users/Ierezell/followers", "following_url": "https://api.github.com/users/Ierezell/following{/other_user}", "gists_url": "https://api.github.com/users/Ierezell/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ierezell/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ierezell/subscriptions", "organizations_url": "https://api.github.com/users/Ierezell/orgs", "repos_url": "https://api.github.com/users/Ierezell/repos", "events_url": "https://api.github.com/users/Ierezell/events{/privacy}", "received_events_url": "https://api.github.com/users/Ierezell/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-03T14:44:42Z", "updated_at": "2020-06-08T20:35:41Z", "closed_at": "2020-06-06T17:07:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "First, thanks a lot for the binding in node. As my company is doing web stuff and want to keep the stack it's really useful to be able to run NLP tasks in node. \r\n\r\nI get inspired with the [node question answering](https://github.com/huggingface/node-question-answering) repo you made and I wanted to compute embeddings on node the same way (then maybe do other tasks as well)\r\n\r\nI get to load hugging face models in `TFJS-node` as you did, same with `BertWordPieceTokenizer` (loaded from the vocabulary text file dumped by python method `save_pretrained`)\r\n\r\nI would like to load a `SentencePieceBPETokenizer` but the only pretrained file I can have is the `sentencepiece.bpe.model` given by the python `save_pretrained` method (I'm using `Camembert-base`)\r\n\r\nI try to load it with the node  `SentencePieceBPETokenizer.fromOptions` (which is the only function I found) but i cannot encode sentences.\r\n\r\n```\r\nasync function test_tokenizer(path_tok: string) {\r\n  const tokenizer = await SentencePieceBPETokenizer.fromOptions({\r\n    vocabFile: path.join(path_tok, \"sentencepiece.bpe.model\"),\r\n  });\r\n  const tok = await tokenizer.encode(\r\n    \"Qui est Pierre ?\",\r\n    \"Pierre est un enseignant\"\r\n  );\r\n  console.log(tok);\r\n  console.log(tok.length);\r\n  console.log(tok.tokens);\r\n  console.log(tok.ids);\r\n  console.log(tok.attentionMask);\r\n  console.log(tok.offsets);\r\n  console.log(tok.overflowing);\r\n  console.log(tok.specialTokensMask);\r\n  console.log(tok.typeIds);\r\n  console.log(tok.wordIndexes);\r\n}\r\n\r\ntest_tokenizer(\"path_to_camembert_savepretrained\")\r\n  .catch((e) => console.log(e))\r\n  .then((_) => console.log(\"done\"));\r\n```\r\n\r\nOutput : \r\n\r\n```\r\nEncoding { _rawEncoding: Encoding {} }\r\n0\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\ndone\r\n```\r\n\r\nThanks in advance for any help or documentation about the node bindings. I'm not a fan of web stuff for NLP but I guess it's a huge demand in the industry ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/290", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/290/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/290/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/290/events", "html_url": "https://github.com/huggingface/tokenizers/issues/290", "id": 628646125, "node_id": "MDU6SXNzdWU2Mjg2NDYxMjU=", "number": 290, "title": "How to get offsets for ElectraTokenizer", "user": {"login": "Aktsvigun", "id": 36672861, "node_id": "MDQ6VXNlcjM2NjcyODYx", "avatar_url": "https://avatars2.githubusercontent.com/u/36672861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aktsvigun", "html_url": "https://github.com/Aktsvigun", "followers_url": "https://api.github.com/users/Aktsvigun/followers", "following_url": "https://api.github.com/users/Aktsvigun/following{/other_user}", "gists_url": "https://api.github.com/users/Aktsvigun/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aktsvigun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aktsvigun/subscriptions", "organizations_url": "https://api.github.com/users/Aktsvigun/orgs", "repos_url": "https://api.github.com/users/Aktsvigun/repos", "events_url": "https://api.github.com/users/Aktsvigun/events{/privacy}", "received_events_url": "https://api.github.com/users/Aktsvigun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-01T18:42:03Z", "updated_at": "2020-06-09T08:40:58Z", "closed_at": "2020-06-09T08:40:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Good afternoon.\r\nIs it possible to get offsets for the text when using models-stacked tokenizers? For instance, I am using ElectraTokenizerFast and I would like it to return the offsets for my texts (the indexes for the first and the last letters of token) - do you probably have any templates of how to do it?\r\nThanks in advance!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/288", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/288/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/288/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/288/events", "html_url": "https://github.com/huggingface/tokenizers/issues/288", "id": 627304632, "node_id": "MDU6SXNzdWU2MjczMDQ2MzI=", "number": 288, "title": "Unable to load pretrained CharBPETokenizer", "user": {"login": "jeremyjordan", "id": 13970565, "node_id": "MDQ6VXNlcjEzOTcwNTY1", "avatar_url": "https://avatars3.githubusercontent.com/u/13970565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeremyjordan", "html_url": "https://github.com/jeremyjordan", "followers_url": "https://api.github.com/users/jeremyjordan/followers", "following_url": "https://api.github.com/users/jeremyjordan/following{/other_user}", "gists_url": "https://api.github.com/users/jeremyjordan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeremyjordan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeremyjordan/subscriptions", "organizations_url": "https://api.github.com/users/jeremyjordan/orgs", "repos_url": "https://api.github.com/users/jeremyjordan/repos", "events_url": "https://api.github.com/users/jeremyjordan/events{/privacy}", "received_events_url": "https://api.github.com/users/jeremyjordan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-29T14:17:47Z", "updated_at": "2020-05-29T15:19:41Z", "closed_at": "2020-05-29T15:19:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "In the `0.8` release there is a new feature for serializing models in a single file - this is a really nice feature! However, I seem to be unable to deserialize back into a Python object. I am getting an error: `Exception: invalid type: string \"version\", expected a borrowed string at line 1 column 11`\r\n\r\n\r\nMinimal reproducible example:\r\n```\r\nimport os\r\nfrom pathlib import Path\r\nfrom tokenizers import CharBPETokenizer\r\n\r\nos.environ['RUST_BACKTRACE'] = 'full'\r\n\r\na = Path('a.txt')\r\na.write_text(\"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer condimentum accumsan orci, sit amet euismod dui \r\nefficitur eu. Nam quis dolor sem. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. \r\nCurabitur volutpat felis tellus, at tempor magna mollis tincidunt. Pellentesque finibus nec mauris vitae tempor. Phasellus hendrerit \r\neu ipsum vitae mattis.\"\"\")\r\n\r\nb = Path('b.txt')\r\nb.write_text(\"\"\"In vehicula risus nec sagittis. In ipsum lectus, luctus nec tellus id, volutpat ultricies lorem. Nunc suscipit, \r\nerat id dignissim iaculis, dolor dui volutpat mi, sed tincidunt odio purus id dui. Sed sit amet dictum libero. Fusce sodales fermentum \r\nnibh eget finibus. Duis viverra, nisi ac ornare viverra, orci nibh dignissim enim, quis luctus dolor nisl at tellus. Sed quis congue \r\nquam scelerisque justo tellus. Morbi facilisis magna in diam feugiat vulputate eu vitae lorem. Pellentesque placerat quam in \r\nneque elementum. Suspendisse facilisis lectus et vestibulum. Curabitur ut eros tellus.\"\"\")\r\n\r\ntext_files = [str(a), str(b)]\r\n\r\ntokenizer = CharBPETokenizer(split_on_whitespace_only=True)\r\ntokenizer.train(\r\n    text_files,\r\n)\r\n\r\nfilename = f\"bpe.tokenizer.json\"\r\ntokenizer.save(filename)\r\n\r\nfrom tokenizers import Tokenizer\r\ntokenizer = Tokenizer.from_file(filename)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/284", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/284/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/284/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/284/events", "html_url": "https://github.com/huggingface/tokenizers/issues/284", "id": 626627289, "node_id": "MDU6SXNzdWU2MjY2MjcyODk=", "number": 284, "title": "Thread panic when saving tokenizer", "user": {"login": "jeremyjordan", "id": 13970565, "node_id": "MDQ6VXNlcjEzOTcwNTY1", "avatar_url": "https://avatars3.githubusercontent.com/u/13970565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeremyjordan", "html_url": "https://github.com/jeremyjordan", "followers_url": "https://api.github.com/users/jeremyjordan/followers", "following_url": "https://api.github.com/users/jeremyjordan/following{/other_user}", "gists_url": "https://api.github.com/users/jeremyjordan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeremyjordan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeremyjordan/subscriptions", "organizations_url": "https://api.github.com/users/jeremyjordan/orgs", "repos_url": "https://api.github.com/users/jeremyjordan/repos", "events_url": "https://api.github.com/users/jeremyjordan/events{/privacy}", "received_events_url": "https://api.github.com/users/jeremyjordan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-28T16:02:04Z", "updated_at": "2020-05-30T01:42:19Z", "closed_at": "2020-05-30T01:42:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm using the `CharBPETokenizer(split_on_whitespace_only=True)` for a project. It trains on my corpus without an issue but when I try to save the tokenizer, I'm getting the following error which is killing the Python process :`thread '<unnamed>' panicked at 'no entry found for key', /__w/tokenizers/tokenizers/tokenizers/src/models/mod.rs:27:66`\r\n\r\nMinimal reproducible example:\r\n\r\n```\r\nimport os\r\nfrom pathlib import Path\r\nfrom tokenizers import CharBPETokenizer\r\n\r\nos.environ['RUST_BACKTRACE'] = 'full'\r\n\r\na = Path('a.txt')\r\na.write_text(\"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer condimentum accumsan orci, sit amet euismod dui \r\nefficitur eu. Nam quis dolor sem. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. \r\nCurabitur volutpat felis tellus, at tempor magna mollis tincidunt. Pellentesque finibus nec mauris vitae tempor. Phasellus hendrerit \r\neu ipsum vitae mattis. Ut id tristique purus. Cras ac #HELLO odio, eget rhoncus eros. Donec risus turpis, accumsan nec fermentum \r\net, pharetra id massa. Praesent facilisis egestas urna nec scelerisque. Sed ullamcorper consectetur diam, ac accumsan lacus \r\npellentesque in. Nulla facilisi. Curabitur risus massa, auctor et feugiat ut, ultricies ac magna. Class aptent taciti sociosqu ad \r\nlitora torquent per conubia nostra, per inceptos himenaeos. Morbi consectetur nibh ut tincidunt suscipit. Aliquam bibendum lectus in \r\nodio ornare malesuada. Suspendisse dui libero, aliquet a est in, volutpat consequat metus. Fusce sodales imperdiet velit, et egestas \r\nante maximus at. Vivamus urna dolor, lacinia eu nibh tincidunt, euismod posuere ante. Sed in elit hendrerit, vehicula nulla et, \r\nelementum magna.\"\"\")\r\n\r\nb = Path('b.txt')\r\nb.write_text(\"\"\"In #HELLO vehicula risus nec sagittis. In ipsum lectus, luctus nec tellus id, volutpat ultricies lorem. Nunc suscipit, \r\nerat id dignissim iaculis, dolor dui volutpat mi, sed tincidunt odio purus id dui. Sed sit amet dictum libero. Fusce sodales fermentum \r\nnibh eget finibus. Duis viverra, nisi ac ornare viverra, orci nibh dignissim enim, quis luctus dolor nisl at tellus. Sed quis congue \r\nquam. #WORLD/#WORLD scelerisque justo tellus. Morbi facilisis magna in diam feugiat vulputate eu vitae lorem. Pellentesque placerat quam in \r\nneque elementum. Suspendisse facilisis lectus et 1-#WORLD-14 vestibulum. Curabitur ut eros tellus.\"\"\")\r\n\r\ntext_files = [str(a), str(b)]\r\n\r\ntokenizer = CharBPETokenizer(split_on_whitespace_only=True)\r\ntokenizer.train(\r\n    text_files,\r\n    special_tokens=[\r\n        '#HELLO',\r\n        '#WORLD',\r\n    ],\r\n)\r\n\r\nfilename = f\"bpe.tokenizer.json\"\r\ntokenizer.save(filename)\r\n```\r\n\r\nI recognize there are some oddities in this example (`#WORLD/#WORLD` and `1-#WORLD-14` with `split_on_whitespace_only=True`), but I'm not sure why this only resulted in an error when serializing the model.\r\n\r\nThis example is using `tokenizers==0.8.0.dev1` but the same behavior is occurring on the latest stable release.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/283", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/283/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/283/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/283/events", "html_url": "https://github.com/huggingface/tokenizers/issues/283", "id": 626373436, "node_id": "MDU6SXNzdWU2MjYzNzM0MzY=", "number": 283, "title": "can't load trained tokenizer, invalid type: string \"version\"", "user": {"login": "deepTransformer", "id": 7314679, "node_id": "MDQ6VXNlcjczMTQ2Nzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7314679?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepTransformer", "html_url": "https://github.com/deepTransformer", "followers_url": "https://api.github.com/users/deepTransformer/followers", "following_url": "https://api.github.com/users/deepTransformer/following{/other_user}", "gists_url": "https://api.github.com/users/deepTransformer/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepTransformer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepTransformer/subscriptions", "organizations_url": "https://api.github.com/users/deepTransformer/orgs", "repos_url": "https://api.github.com/users/deepTransformer/repos", "events_url": "https://api.github.com/users/deepTransformer/events{/privacy}", "received_events_url": "https://api.github.com/users/deepTransformer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-28T09:48:00Z", "updated_at": "2020-05-28T13:50:31Z", "closed_at": "2020-05-28T13:50:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "I followed the python code in the README to train my own tokenizer\r\n```python\r\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\r\n\r\n# Initialize a tokenizer\r\ntokenizer = Tokenizer(models.BPE())\r\n\r\n# Customize pre-tokenization and decoding\r\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\r\ntokenizer.decoder = decoders.ByteLevel()\r\ntokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\r\n\r\n# And then train\r\ntrainer = trainers.BpeTrainer(vocab_size=20000, min_frequency=2)\r\ntokenizer.train(trainer, [\r\n\t\"./path/to/dataset/1.txt\",\r\n\t\"./path/to/dataset/2.txt\",\r\n\t\"./path/to/dataset/3.txt\"\r\n])\r\n\r\n# And Save it\r\ntokenizer.save(\"byte-level-bpe.tokenizer.json\", pretty=True)\r\n```\r\n\r\nbut when I load the tokenizer, the error **\"invalid type: string \"version\", expected a borrowed string at line 1 column 11\"** occured, how to solve the problem?\r\n```python\r\nfrom tokenizers import Tokenizer\r\ntokenizer = Tokenizer.from_file(\"byte-level-bpe.tokenizer.json\")\r\nencoded = tokenizer.encode(\"I can feel the magic, can you?\")\r\n```\r\n\r\nmy tokenizers version is '0.8.0.dev1'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/282", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/282/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/282/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/282/events", "html_url": "https://github.com/huggingface/tokenizers/issues/282", "id": 626338059, "node_id": "MDU6SXNzdWU2MjYzMzgwNTk=", "number": 282, "title": "Exception: stream did not contain valid UTF-8", "user": {"login": "phamdinhkhanh", "id": 24260166, "node_id": "MDQ6VXNlcjI0MjYwMTY2", "avatar_url": "https://avatars3.githubusercontent.com/u/24260166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phamdinhkhanh", "html_url": "https://github.com/phamdinhkhanh", "followers_url": "https://api.github.com/users/phamdinhkhanh/followers", "following_url": "https://api.github.com/users/phamdinhkhanh/following{/other_user}", "gists_url": "https://api.github.com/users/phamdinhkhanh/gists{/gist_id}", "starred_url": "https://api.github.com/users/phamdinhkhanh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phamdinhkhanh/subscriptions", "organizations_url": "https://api.github.com/users/phamdinhkhanh/orgs", "repos_url": "https://api.github.com/users/phamdinhkhanh/repos", "events_url": "https://api.github.com/users/phamdinhkhanh/events{/privacy}", "received_events_url": "https://api.github.com/users/phamdinhkhanh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-28T08:54:32Z", "updated_at": "2020-06-29T16:29:13Z", "closed_at": "2020-06-29T16:29:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I get bug when tokenize ByteLevelBPETokenizer() for diacritic language in utf-16 such as 'Viet Nam' language. Bellow are my code initialize tokenizer.\r\n\r\n\r\n```\r\n%%time \r\nfrom pathlib import Path\r\n\r\nfrom tokenizers import ByteLevelBPETokenizer\r\n\r\npaths = ['file1.txt', 'file2.txt']\r\nprint(paths)\r\n# Initialize a tokenizer\r\ntokenizer = ByteLevelBPETokenizer()\r\n# Customize training\r\ntokenizer.train(files=paths, vocab_size=52000, min_frequency=2, special_tokens=[\r\n    \"<s>\",\r\n    \"<pad>\",\r\n    \"</s>\",\r\n    \"<unk>\",\r\n    \"<mask>\",\r\n])\r\n```\r\n\r\nAnd bug log:\r\n\r\n> <ipython-input-78-66e6ec31bd7b> in train(self, files, vocab_size, min_frequency, show_progress, special_tokens)\r\n>      90             files = [files]\r\n>      91         print('files list: \\n', files)\r\n> ---> 92         self._tokenizer.train(trainer, files)\r\n> \r\n> Exception: stream did not contain valid UTF-8\r\n\r\nmy `file1.txt` and `file2.txt` contain words like:\r\n\r\n`xin ch\u00e0o t\u00f4i \u0111\u1ebfn t\u1eeb Vi\u1ec7t Nam, t\u00f4i g\u1eb7p v\u1ea5n \u0111\u1ec1 v\u1edbi tokenizer.`\r\n\r\n\r\nI try to find what self._tokenizer.train() does to fix it myself but project code are complicated. Can you explain what i was wrong?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/281", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/281/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/281/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/281/events", "html_url": "https://github.com/huggingface/tokenizers/issues/281", "id": 626130450, "node_id": "MDU6SXNzdWU2MjYxMzA0NTA=", "number": 281, "title": "Prevent changes in the characters", "user": {"login": "sinaahmadi", "id": 12765529, "node_id": "MDQ6VXNlcjEyNzY1NTI5", "avatar_url": "https://avatars0.githubusercontent.com/u/12765529?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sinaahmadi", "html_url": "https://github.com/sinaahmadi", "followers_url": "https://api.github.com/users/sinaahmadi/followers", "following_url": "https://api.github.com/users/sinaahmadi/following{/other_user}", "gists_url": "https://api.github.com/users/sinaahmadi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sinaahmadi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sinaahmadi/subscriptions", "organizations_url": "https://api.github.com/users/sinaahmadi/orgs", "repos_url": "https://api.github.com/users/sinaahmadi/repos", "events_url": "https://api.github.com/users/sinaahmadi/events{/privacy}", "received_events_url": "https://api.github.com/users/sinaahmadi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-05-28T00:44:25Z", "updated_at": "2020-05-28T19:19:18Z", "closed_at": "2020-05-28T19:19:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nSorry, I could not find a better title for this issue.\r\n\r\nI am training a model using the WordPiece method as follows:\r\n\r\n```\r\n    vocab_size = 50000\r\n    # Initialize a tokenizer\r\n    tokenizer = BertWordPieceTokenizer(clean_text=True, handle_chinese_chars=False, strip_accents=True, lowercase=False)\r\n\r\n    # And then train\r\n    tokenizer.train(\r\n        files,\r\n        vocab_size,\r\n        min_frequency=2,\r\n        show_progress=True,\r\n        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\r\n        limit_alphabet=1000,\r\n        wordpieces_prefix=\"##\",\r\n    )\r\n\r\n    tokenizer.save('./', 'output-wordpiece_%s'%str(vocab_size))\r\n```\r\n\r\n\r\nEverything is fine and the tokenizer works too. However, some of the characters are automatically changed. This looks like an automatic normalization. Is there any way to prevent such modifications? I also tried `BertWordPieceTokenizer(clean_text=False, handle_chinese_chars=False, strip_accents=False, lowercase=False)` but the problem does not get solved either. I only want the text get tokenized without any further modification.\r\n\r\nThanks. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/279", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/279/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/279/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/279/events", "html_url": "https://github.com/huggingface/tokenizers/issues/279", "id": 625509481, "node_id": "MDU6SXNzdWU2MjU1MDk0ODE=", "number": 279, "title": "[Question/Doubt]", "user": {"login": "mrm8488", "id": 3653789, "node_id": "MDQ6VXNlcjM2NTM3ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3653789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrm8488", "html_url": "https://github.com/mrm8488", "followers_url": "https://api.github.com/users/mrm8488/followers", "following_url": "https://api.github.com/users/mrm8488/following{/other_user}", "gists_url": "https://api.github.com/users/mrm8488/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrm8488/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrm8488/subscriptions", "organizations_url": "https://api.github.com/users/mrm8488/orgs", "repos_url": "https://api.github.com/users/mrm8488/repos", "events_url": "https://api.github.com/users/mrm8488/events{/privacy}", "received_events_url": "https://api.github.com/users/mrm8488/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-27T08:55:44Z", "updated_at": "2020-05-28T14:48:34Z", "closed_at": "2020-05-28T14:48:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "How can it be create a \"spiece.model\" with tokenizers library? @n1t0 ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/278", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/278/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/278/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/278/events", "html_url": "https://github.com/huggingface/tokenizers/issues/278", "id": 624673266, "node_id": "MDU6SXNzdWU2MjQ2NzMyNjY=", "number": 278, "title": "ByteLevelBPETokenizer drops umlaute", "user": {"login": "KappaDistributive", "id": 18640507, "node_id": "MDQ6VXNlcjE4NjQwNTA3", "avatar_url": "https://avatars2.githubusercontent.com/u/18640507?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KappaDistributive", "html_url": "https://github.com/KappaDistributive", "followers_url": "https://api.github.com/users/KappaDistributive/followers", "following_url": "https://api.github.com/users/KappaDistributive/following{/other_user}", "gists_url": "https://api.github.com/users/KappaDistributive/gists{/gist_id}", "starred_url": "https://api.github.com/users/KappaDistributive/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KappaDistributive/subscriptions", "organizations_url": "https://api.github.com/users/KappaDistributive/orgs", "repos_url": "https://api.github.com/users/KappaDistributive/repos", "events_url": "https://api.github.com/users/KappaDistributive/events{/privacy}", "received_events_url": "https://api.github.com/users/KappaDistributive/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-26T07:51:50Z", "updated_at": "2020-05-27T09:20:20Z", "closed_at": "2020-05-27T09:20:19Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "ByteLevelBPETokenizer seems to ignore all umlaute. It doesn't raise an exception either -- just silently ignores them and tokenizes the remaining text.\r\n\r\nIs this intentional?\r\n\r\nCode:\r\n\r\n```python\r\nimport json\r\nimport pathlib\r\n\r\nimport tokenizers\r\n# Create dummy vocab and merges files to build tokeniser from \r\nwith open('vocab.json', 'w') as f:\r\n    json.dump({'A': 0, '\u00e4': 1, 'A\u00e4': 2}, f)\r\n\r\nwith open('merges.txt', 'w') as f:\r\n    f.write('#version 1\\nA \u00e4')\r\n\r\nbpe_tokeniser_from_files = tokenizers.ByteLevelBPETokenizer(\r\n    vocab_file='vocab.json',\r\n    merges_file='merges.txt'\r\n)\r\n\r\nprint(bpe_tokeniser_from_files.encode(\"A\u00e4\").ids)\r\n```\r\n\r\nExpected Result:\r\n`[2]`\r\n\r\nActual Result:\r\n`[0]`\r\n\r\nRelated: #164", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/275", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/275/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/275/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/275/events", "html_url": "https://github.com/huggingface/tokenizers/issues/275", "id": 623290634, "node_id": "MDU6SXNzdWU2MjMyOTA2MzQ=", "number": 275, "title": "SentencePieceBPETokenizer splits <mask> and <pad> tokens even after adding them to special tokens", "user": {"login": "kuppulur", "id": 3698879, "node_id": "MDQ6VXNlcjM2OTg4Nzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/3698879?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kuppulur", "html_url": "https://github.com/kuppulur", "followers_url": "https://api.github.com/users/kuppulur/followers", "following_url": "https://api.github.com/users/kuppulur/following{/other_user}", "gists_url": "https://api.github.com/users/kuppulur/gists{/gist_id}", "starred_url": "https://api.github.com/users/kuppulur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kuppulur/subscriptions", "organizations_url": "https://api.github.com/users/kuppulur/orgs", "repos_url": "https://api.github.com/users/kuppulur/repos", "events_url": "https://api.github.com/users/kuppulur/events{/privacy}", "received_events_url": "https://api.github.com/users/kuppulur/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-22T15:29:50Z", "updated_at": "2020-05-22T17:49:03Z", "closed_at": "2020-05-22T17:49:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n \r\nI am trying to train a **_SentencePieceBPETokenizer_** on my data, which I want to further use in creating a RoBERTa language model. Tokens look good, but the tokenizer is also splitting mask and pad tokens **_('\u2581<', 'm', 'as', 'k', '>')_**. Could anyone explain if this is what is expected or if I am making a mistake somewhere?\r\n\r\n```\r\nfrom tokenizers import SentencePieceBPETokenizer\r\npaths = [str(x) for x in Path(\"./my_data/\").glob(\"**/*.txt\")]\r\ntokenizer = SentencePieceBPETokenizer()\r\ntokenizer.train(files=paths, vocab_size=25000, min_frequency=2, special_tokens=[\r\n    \"<s>\",\r\n    \"<pad>\",\r\n    \"</s>\",\r\n    \"<unk>\",\r\n    \"<mask>\",\r\n])\r\ntokenizer.save(\".\", \"MyBERTu\")\r\n```\r\ntokenizers == 0.7.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/268", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/268/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/268/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/268/events", "html_url": "https://github.com/huggingface/tokenizers/issues/268", "id": 619674573, "node_id": "MDU6SXNzdWU2MTk2NzQ1NzM=", "number": 268, "title": "Is ByteLevelBPETokenizer working with cyrillics?", "user": {"login": "purgenetik", "id": 9008124, "node_id": "MDQ6VXNlcjkwMDgxMjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/9008124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/purgenetik", "html_url": "https://github.com/purgenetik", "followers_url": "https://api.github.com/users/purgenetik/followers", "following_url": "https://api.github.com/users/purgenetik/following{/other_user}", "gists_url": "https://api.github.com/users/purgenetik/gists{/gist_id}", "starred_url": "https://api.github.com/users/purgenetik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/purgenetik/subscriptions", "organizations_url": "https://api.github.com/users/purgenetik/orgs", "repos_url": "https://api.github.com/users/purgenetik/repos", "events_url": "https://api.github.com/users/purgenetik/events{/privacy}", "received_events_url": "https://api.github.com/users/purgenetik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-17T11:12:32Z", "updated_at": "2020-05-27T02:00:17Z", "closed_at": "2020-05-27T02:00:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to run tokenizer with Russian language, but have problem with encodings. The full colab notebook is available here https://colab.research.google.com/drive/1aURDJAt1HqDCZVlYH1_K8YCBrbot5Wx5?usp=sharing\r\nThe encode of input file is ok:\r\n\r\n![image](https://user-images.githubusercontent.com/9008124/82142767-643c6100-9847-11ea-8834-0c6643cf5994.png)\r\n\r\nBut after passing though tokenizer I see non-cyrillic symbols in vocab.json and merges.txt and tokenizer run gives the same problem.\r\n\r\n![image](https://user-images.githubusercontent.com/9008124/82142882-6b17a380-9848-11ea-9872-be741e026e89.png)\r\n\r\nHow one can correctly process Cyrillic language and is it possible?\r\nCheers,\r\nYuri\r\n\r\n ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/267", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/267/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/267/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/267/events", "html_url": "https://github.com/huggingface/tokenizers/issues/267", "id": 618119065, "node_id": "MDU6SXNzdWU2MTgxMTkwNjU=", "number": 267, "title": "Memory Limit", "user": {"login": "NaxAlpha", "id": 11090613, "node_id": "MDQ6VXNlcjExMDkwNjEz", "avatar_url": "https://avatars3.githubusercontent.com/u/11090613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NaxAlpha", "html_url": "https://github.com/NaxAlpha", "followers_url": "https://api.github.com/users/NaxAlpha/followers", "following_url": "https://api.github.com/users/NaxAlpha/following{/other_user}", "gists_url": "https://api.github.com/users/NaxAlpha/gists{/gist_id}", "starred_url": "https://api.github.com/users/NaxAlpha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NaxAlpha/subscriptions", "organizations_url": "https://api.github.com/users/NaxAlpha/orgs", "repos_url": "https://api.github.com/users/NaxAlpha/repos", "events_url": "https://api.github.com/users/NaxAlpha/events{/privacy}", "received_events_url": "https://api.github.com/users/NaxAlpha/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-14T10:36:11Z", "updated_at": "2020-06-13T08:11:15Z", "closed_at": "2020-05-15T05:09:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI have 100+GB of text corpus and want to train sentence piece tokenizer. But when I tried to train on those files, it first tried to read all of them in memory which made it impossible to train on entire dataset. \r\n\r\nNext I tried to sample a ~10GB corpus and tried to train on that. It is successfully able to read all the files in memory but Tokenize word phase takes a lot more memory (in my case it used up all the 100GB of RAM while still at ~30%).\r\n\r\nI would prefer to train tokenizer on entire dataset rather than just a chunk but chunk would be fine if it works. Can anyone please help me out on how to reduce this memory requirement?\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/266", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/266/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/266/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/266/events", "html_url": "https://github.com/huggingface/tokenizers/issues/266", "id": 617402795, "node_id": "MDU6SXNzdWU2MTc0MDI3OTU=", "number": 266, "title": "BertWordPieceTokenizer is not reversible for quotation marks", "user": {"login": "tomhosking", "id": 9419158, "node_id": "MDQ6VXNlcjk0MTkxNTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/9419158?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomhosking", "html_url": "https://github.com/tomhosking", "followers_url": "https://api.github.com/users/tomhosking/followers", "following_url": "https://api.github.com/users/tomhosking/following{/other_user}", "gists_url": "https://api.github.com/users/tomhosking/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomhosking/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomhosking/subscriptions", "organizations_url": "https://api.github.com/users/tomhosking/orgs", "repos_url": "https://api.github.com/users/tomhosking/repos", "events_url": "https://api.github.com/users/tomhosking/events{/privacy}", "received_events_url": "https://api.github.com/users/tomhosking/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-13T12:22:57Z", "updated_at": "2020-05-20T08:51:05Z", "closed_at": "2020-05-20T08:51:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Encoding then decoding a string containing single and double quotation marks does not return the original string. Single quotes are combined with adjacent words and doubles are padded with an additional space.\r\n\r\n```\r\nimport tokenizers\r\nfrom tokenizers import BertWordPieceTokenizer\r\n\r\ntok = BertWordPieceTokenizer(\"../data/bert-vocabs/bert-base-uncased-vocab.txt\", lowercase=True)\r\n\r\ntoks = tok.encode(\"this sentence contains 'single quotes' and \\\"double quotes\\\" in the middle.\")\r\n\r\nprint(tokenizers.__version__)\r\nprint(tok.decode(toks.ids))\r\n```\r\n\r\nOutput:\r\n> 0.7.0\r\n> this sentence contains'single quotes'and \" double quotes \" in the middle.\r\n\r\nExpected:\r\n> this sentence contains 'single quotes' and \"double quotes\" in the middle.\r\n\r\nEdit: I think I see why this may be more complex than I thought:\r\n`print(tok.decode(tok.encode(\"It's not clear whether peoples' use of 'single quotes' and \\\"double quotes\\\" is reversible.\").ids))`\r\n> it's not clear whether peoples'use of'single quotes'and \" double quotes \" is reversible.\r\n\r\nGiven that the BERT vocab is fixed, I guess there's no way of getting this right for all use cases?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/264", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/264/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/264/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/264/events", "html_url": "https://github.com/huggingface/tokenizers/issues/264", "id": 616022904, "node_id": "MDU6SXNzdWU2MTYwMjI5MDQ=", "number": 264, "title": "Issues with ByteLevelBPETokenizer encoder.", "user": {"login": "iyor", "id": 15113131, "node_id": "MDQ6VXNlcjE1MTEzMTMx", "avatar_url": "https://avatars1.githubusercontent.com/u/15113131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iyor", "html_url": "https://github.com/iyor", "followers_url": "https://api.github.com/users/iyor/followers", "following_url": "https://api.github.com/users/iyor/following{/other_user}", "gists_url": "https://api.github.com/users/iyor/gists{/gist_id}", "starred_url": "https://api.github.com/users/iyor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iyor/subscriptions", "organizations_url": "https://api.github.com/users/iyor/orgs", "repos_url": "https://api.github.com/users/iyor/repos", "events_url": "https://api.github.com/users/iyor/events{/privacy}", "received_events_url": "https://api.github.com/users/iyor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-11T16:40:05Z", "updated_at": "2020-05-12T06:51:40Z", "closed_at": "2020-05-12T06:51:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI used hf/tokenizers to make a custom vocabulary. As a sanity check, I'm trying to run the ByteLevelBPETokenizer on the vocabulary but things aren't adding up. One token in said vocabulary is `\u0120OH\u00c3\u00a2`. Running,\r\n```python3\r\ntokenizer = ByteLevelBPETokenizer('custom_vocab.json', 'custom_vocab_merges.txt')\r\ntokenizer.encode(' OH\u00c3\u00a2').tokens\r\ntokenizer.encode(' OH\u00c3\u00a2').ids\r\n```\r\nyields,\r\n```python3\r\n['\u0120OH', '\u00c3\u0125', '\u00c2\u00a2']\r\n[0, 11700, 820, 17600, 2]\r\n```\r\nShouldn't I be getting a single token id (plus the start-of-sentence and end-of-sentence tokens), since I'm trying to encode a token right out of the vocabulary? Also the tokens don't actually look correct. Decoding again gets me back the original string though...", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/263", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/263/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/263/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/263/events", "html_url": "https://github.com/huggingface/tokenizers/issues/263", "id": 615967456, "node_id": "MDU6SXNzdWU2MTU5Njc0NTY=", "number": 263, "title": "Automatically Adding SOS/EOS Tokens", "user": {"login": "glennkroegel", "id": 16552111, "node_id": "MDQ6VXNlcjE2NTUyMTEx", "avatar_url": "https://avatars3.githubusercontent.com/u/16552111?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glennkroegel", "html_url": "https://github.com/glennkroegel", "followers_url": "https://api.github.com/users/glennkroegel/followers", "following_url": "https://api.github.com/users/glennkroegel/following{/other_user}", "gists_url": "https://api.github.com/users/glennkroegel/gists{/gist_id}", "starred_url": "https://api.github.com/users/glennkroegel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glennkroegel/subscriptions", "organizations_url": "https://api.github.com/users/glennkroegel/orgs", "repos_url": "https://api.github.com/users/glennkroegel/repos", "events_url": "https://api.github.com/users/glennkroegel/events{/privacy}", "received_events_url": "https://api.github.com/users/glennkroegel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-11T15:18:40Z", "updated_at": "2020-06-05T12:21:15Z", "closed_at": "2020-06-05T12:21:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi guys,\r\n\r\nIs there a way to prepend/append start of text + end of text tokens?\r\n\r\nI'm basically trying to recreate the functionality of the BertTokenizer (on a smaller, completely different dataset) with the [CLS] and [SEP] tokens automatically being added with no changes to the raw text.  \r\n\r\nThe decoder should also just remove them and return the original text as expected.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/262", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/262/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/262/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/262/events", "html_url": "https://github.com/huggingface/tokenizers/issues/262", "id": 614120975, "node_id": "MDU6SXNzdWU2MTQxMjA5NzU=", "number": 262, "title": "Feature request: train on many fiiles (instead of having to cat them manually)", "user": {"login": "jchwenger", "id": 34098722, "node_id": "MDQ6VXNlcjM0MDk4NzIy", "avatar_url": "https://avatars1.githubusercontent.com/u/34098722?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jchwenger", "html_url": "https://github.com/jchwenger", "followers_url": "https://api.github.com/users/jchwenger/followers", "following_url": "https://api.github.com/users/jchwenger/following{/other_user}", "gists_url": "https://api.github.com/users/jchwenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/jchwenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jchwenger/subscriptions", "organizations_url": "https://api.github.com/users/jchwenger/orgs", "repos_url": "https://api.github.com/users/jchwenger/repos", "events_url": "https://api.github.com/users/jchwenger/events{/privacy}", "received_events_url": "https://api.github.com/users/jchwenger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-07T14:53:11Z", "updated_at": "2020-05-07T15:24:37Z", "closed_at": "2020-05-07T15:24:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi again,\r\n\r\nGrand, I've been able to produce OpenAI-looking vocabulary & merge tables thanks to your tokenizer, that's great!\r\n\r\nHowever, I had to cat my files into one large text beforehand, as the os complained when I loaded my folder:\r\n```\r\nIn [1]: from tokenizers import  ByteLevelBPETokenizer                                                \r\nIn [6]: to = ByteLevelBPETokenizer()                                                                 \r\nIn [10]: files = os.listdir('.')                                                                     \r\nIn [11]: len(files)                                                                                  \r\nOut[11]: 2999\r\nIn [12]: to.train(files, vocab_size=50000)                                                           \r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-12-5cef96dfddcd> in <module>\r\n----> 1 to.train(files, vocab_size=50000)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/tokenizers/implementations/byte_level_bpe.py in train(self, files, vocab_size, min_frequency, show_progress, special_tokens)\r\n     87         if isinstance(files, str):\r\n     88             files = [files]\r\n---> 89         self._tokenizer.train(trainer, files)\r\n\r\nException: Too many open files (os error 24)\r\n```\r\n\r\nThis worked fine after passing just one file (although it took more than the advertised 20 seconds on my Asus Rog Zephyrus G GU502DU_GA502DU):\r\n```\r\nIn [3]: tokenizer.train('all_my_files.txt', vocab_size=50000)                                           \r\n[00:03:03] all_my_files.txt                            \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 1.17GB   /   1.17GB\r\n[00:00:14] Tokenize words                           \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 1329291  /  1329291\r\n[00:00:16] Count pairs                              \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 1329291  /  1329291\r\n[00:02:13] Compute merges                           \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 49744    /    49744\r\n```\r\n\r\nStill, great work! And I love that it uses Rust, I'll have a look at the implementation when I have some time.\r\n\r\nPerhaps something to consider, for convenience's sake? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/261", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/261/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/261/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/261/events", "html_url": "https://github.com/huggingface/tokenizers/issues/261", "id": 614064794, "node_id": "MDU6SXNzdWU2MTQwNjQ3OTQ=", "number": 261, "title": "Exception: Unk token `<unk>` not found in the vocabulary", "user": {"login": "jchwenger", "id": 34098722, "node_id": "MDQ6VXNlcjM0MDk4NzIy", "avatar_url": "https://avatars1.githubusercontent.com/u/34098722?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jchwenger", "html_url": "https://github.com/jchwenger", "followers_url": "https://api.github.com/users/jchwenger/followers", "following_url": "https://api.github.com/users/jchwenger/following{/other_user}", "gists_url": "https://api.github.com/users/jchwenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/jchwenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jchwenger/subscriptions", "organizations_url": "https://api.github.com/users/jchwenger/orgs", "repos_url": "https://api.github.com/users/jchwenger/repos", "events_url": "https://api.github.com/users/jchwenger/events{/privacy}", "received_events_url": "https://api.github.com/users/jchwenger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-07T13:37:22Z", "updated_at": "2020-05-07T14:24:16Z", "closed_at": "2020-05-07T14:24:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI've been looking into BPE tokenizing methods while studying OpenAI's GPT-2, and am surprised to see this error after installing (`pip install tokenizers`):\r\n\r\n```\r\nfrom tokenizers import  CharBPETokenizer \r\nvocab = \"/path-to-gpt-2/models/117M/encoder.json\"                                    \r\nmerges = \"/path-to-gpt-2/models/117M/vocab.bpe\"                                      \r\ntokenizer = CharBPETokenizer(vocab, merges)                                                  \r\ntokenizer.encode('Hello there!')                                           \r\n```\r\nWhich yields:\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-9-b92cb27e03e1> in <module>\r\n----> 1 tokenizer.encode('Hello there, trying that again')\r\n\r\n~/anaconda3/envs/tf14/lib/python3.7/site-packages/tokenizers/implementations/base_tokenizer.py in encode(self, sequence, pair, add_special_tokens)\r\n    228             raise ValueError(\"None input is not valid. Should be a string.\")\r\n    229 \r\n--> 230         return self._tokenizer.encode(sequence, pair, add_special_tokens)\r\n    231 \r\n    232     def encode_batch(\r\n\r\nException: Unk token `<unk>` not found in the vocabulary\r\n```\r\n\r\nI was hoping this would work out of the box since you write in the code that the default behaviour is precisely OpenAI's encoding? (I'm looking into a BPE tokenizer that encodes beginnings of words instead of ends of words, which seems to be one of OpenAI's tricks not present in Sennrich or FastBPE - as for sentencepiece, see [my question](https://github.com/google/sentencepiece/issues/455).\r\n\r\nWhat would be the fix for that?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/260", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/260/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/260/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/260/events", "html_url": "https://github.com/huggingface/tokenizers/issues/260", "id": 613197673, "node_id": "MDU6SXNzdWU2MTMxOTc2NzM=", "number": 260, "title": "no entry found for key", "user": {"login": "djstrong", "id": 1849959, "node_id": "MDQ6VXNlcjE4NDk5NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1849959?v=4", "gravatar_id": "", "url": "https://api.github.com/users/djstrong", "html_url": "https://github.com/djstrong", "followers_url": "https://api.github.com/users/djstrong/followers", "following_url": "https://api.github.com/users/djstrong/following{/other_user}", "gists_url": "https://api.github.com/users/djstrong/gists{/gist_id}", "starred_url": "https://api.github.com/users/djstrong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/djstrong/subscriptions", "organizations_url": "https://api.github.com/users/djstrong/orgs", "repos_url": "https://api.github.com/users/djstrong/repos", "events_url": "https://api.github.com/users/djstrong/events{/privacy}", "received_events_url": "https://api.github.com/users/djstrong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-05-06T10:01:18Z", "updated_at": "2020-06-04T18:42:35Z", "closed_at": "2020-05-10T10:30:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am finetuning a model and for some hyperparameters (different number of epochs or learning rate) I get an error:\r\n```\r\nthread '<unnamed>' panicked at 'no entry found for key', /rustc/6d0e58bff88f620c1a4f641a627f046bf4cde4ad/src/libstd/collections/hash/map.rs:1023:9\r\nstack backtrace:\r\n   0: backtrace::backtrace::libunwind::trace\r\n             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.44/src/backtrace/libunwind.rs:86\r\n   1: backtrace::backtrace::trace_unsynchronized\r\n             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.44/src/backtrace/mod.rs:66\r\n   2: std::sys_common::backtrace::_print_fmt\r\n             at src/libstd/sys_common/backtrace.rs:78\r\n   3: <std::sys_common::backtrace::_print::DisplayBacktrace as core::fmt::Display>::fmt\r\n             at src/libstd/sys_common/backtrace.rs:59\r\n   4: core::fmt::write\r\n             at src/libcore/fmt/mod.rs:1052\r\n   5: std::io::Write::write_fmt\r\n             at src/libstd/io/mod.rs:1428\r\n   6: std::sys_common::backtrace::_print\r\n             at src/libstd/sys_common/backtrace.rs:62\r\n   7: std::sys_common::backtrace::print\r\n             at src/libstd/sys_common/backtrace.rs:49\r\n   8: std::panicking::default_hook::{{closure}}\r\n             at src/libstd/panicking.rs:204\r\n   9: std::panicking::default_hook\r\n             at src/libstd/panicking.rs:224\r\n  10: std::panicking::rust_panic_with_hook\r\n             at src/libstd/panicking.rs:470\r\n  11: rust_begin_unwind\r\n             at src/libstd/panicking.rs:378\r\n  12: core::panicking::panic_fmt\r\n             at src/libcore/panicking.rs:85\r\n  13: core::option::expect_failed\r\n             at src/libcore/option.rs:1203\r\n  14: serde::ser::Serializer::collect_map\r\n  15: <tokenizers::models::bpe::model::BPE as tokenizers::tokenizer::Model>::save\r\n  16: tokenizers::models::__init2023689508296420652::__init2023689508296420652::__wrap\r\n  17: _PyMethodDef_RawFastCallKeywords\r\n             at Objects/call.c:694\r\n  18: _PyCFunction_FastCallKeywords\r\n             at Objects/call.c:734\r\n  19: call_function\r\n             at Python/ceval.c:4568\r\n  20: _PyEval_EvalFrameDefault\r\n             at Python/ceval.c:3139\r\n  21: _PyEval_EvalCodeWithName\r\n             at Python/ceval.c:3930\r\n  22: _PyFunction_FastCallKeywords\r\n             at Objects/call.c:433\r\n  23: call_function\r\n             at Python/ceval.c:4616\r\n  24: _PyEval_EvalFrameDefault\r\n             at Python/ceval.c:3110\r\n  25: function_code_fastcall\r\n             at Objects/call.c:283\r\n  26: _PyFunction_FastCallKeywords\r\n             at Objects/call.c:408\r\n  27: call_function\r\n             at Python/ceval.c:4616\r\n  28: _PyEval_EvalFrameDefault\r\n             at Python/ceval.c:3110\r\n  29: function_code_fastcall\r\n             at Objects/call.c:283\r\n  30: _PyFunction_FastCallKeywords\r\n             at Objects/call.c:408\r\n  31: call_function\r\n             at Python/ceval.c:4616\r\n  32: _PyEval_EvalFrameDefault\r\n             at Python/ceval.c:3110\r\n  33: _PyEval_EvalCodeWithName\r\n             at Python/ceval.c:3930\r\n  34: _PyFunction_FastCallKeywords\r\n             at Objects/call.c:433\r\n  35: call_function\r\n             at Python/ceval.c:4616\r\n  36: _PyEval_EvalFrameDefault\r\n             at Python/ceval.c:3139\r\n  37: _PyEval_EvalCodeWithName\r\n             at Python/ceval.c:3930\r\n  38: _PyFunction_FastCallDict\r\n             at Objects/call.c:376\r\n  39: _PyObject_Call_Prepend\r\n             at Objects/call.c:908\r\n  40: PyObject_Call\r\n             at Objects/call.c:245\r\n  41: do_call_core\r\n             at Python/ceval.c:4645\r\n  42: _PyEval_EvalFrameDefault\r\n             at Python/ceval.c:3191\r\n  43: _PyEval_EvalCodeWithName\r\n             at Python/ceval.c:3930\r\n  44: _PyFunction_FastCallKeywords\r\n             at Objects/call.c:433\r\n  45: call_function\r\n             at Python/ceval.c:4616\r\n  46: _PyEval_EvalFrameDefault\r\n             at Python/ceval.c:3139\r\n  47: _PyEval_EvalCodeWithName\r\n             at Python/ceval.c:3930\r\n  48: PyEval_EvalCodeEx\r\n             at Python/ceval.c:3959\r\n  49: PyEval_EvalCode\r\n             at Python/ceval.c:524\r\n  50: run_mod\r\n             at Python/pythonrun.c:1035\r\n  51: PyRun_FileExFlags\r\n             at Python/pythonrun.c:988\r\n  52: PyRun_SimpleFileExFlags\r\n             at Python/pythonrun.c:429\r\n  53: pymain_run_file\r\n             at Modules/main.c:427\r\n  54: pymain_run_filename\r\n             at Modules/main.c:1606\r\n  55: pymain_run_python\r\n             at Modules/main.c:2867\r\n  56: pymain_main\r\n             at Modules/main.c:3028\r\n  57: _Py_UnixMain\r\n             at Modules/main.c:3063\r\n  58: __libc_start_main\r\n  59: <unknown>\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\nfatal runtime error: failed to initiate panic, error 5\r\n/var/spool/slurmd/job18823086/slurm_script: line 29:  4069 Aborted                 $1\r\n```\r\nVersion: 0.5.2", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/258", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/258/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/258/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/258/events", "html_url": "https://github.com/huggingface/tokenizers/issues/258", "id": 612490389, "node_id": "MDU6SXNzdWU2MTI0OTAzODk=", "number": 258, "title": "Tokenizer stalls / hangs when used in DataLoader (multiprocessing issue)", "user": {"login": "jiahuei", "id": 16820751, "node_id": "MDQ6VXNlcjE2ODIwNzUx", "avatar_url": "https://avatars3.githubusercontent.com/u/16820751?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiahuei", "html_url": "https://github.com/jiahuei", "followers_url": "https://api.github.com/users/jiahuei/followers", "following_url": "https://api.github.com/users/jiahuei/following{/other_user}", "gists_url": "https://api.github.com/users/jiahuei/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiahuei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiahuei/subscriptions", "organizations_url": "https://api.github.com/users/jiahuei/orgs", "repos_url": "https://api.github.com/users/jiahuei/repos", "events_url": "https://api.github.com/users/jiahuei/events{/privacy}", "received_events_url": "https://api.github.com/users/jiahuei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-05T10:15:40Z", "updated_at": "2020-05-23T08:58:31Z", "closed_at": "2020-05-23T08:58:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, the tokenizer seems to stall when `self.train()` or even `self.encode()` is run before loading into DataLoader when `num_workers > 0`.\r\n\r\nIs there any way to solve this?  Thanks !\r\n```\r\n# -*- coding: utf-8 -*-\r\n\r\nimport os\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom tokenizers import ByteLevelBPETokenizer\r\n\r\n\r\nclass ListDataset(Dataset):\r\n    def __init__(self, data):\r\n        self.data = data\r\n\r\n    def __getitem__(self, index):\r\n        return self.data[index]\r\n\r\n    def __len__(self):\r\n        return len(self.data)\r\n\r\n\r\nclass Collate:\r\n    def __init__(self, tokenizer):\r\n        self.tokenizer = tokenizer\r\n\r\n    def __call__(self, batch):\r\n        encoding = self.tokenizer.encode_batch(\r\n            list(batch),\r\n            add_special_tokens=True\r\n        )\r\n        return {'ids': [_.ids for _ in encoding], 'attention_mask': [_.attention_mask for _ in encoding]}\r\n\r\n\r\nCURR_DIR = os.path.dirname(os.path.realpath(__file__))\r\n\r\ndata = ['This is a test sentence that is long, on purpose, YOLO right?.',\r\n        'Tokenize this please.',\r\n        ' Random \u4e2d\u6587 words\u3002', ]\r\n\r\n######################\r\n# Enabling this chunk causes code to stall, with or without `del tokenizer`\r\n######################\r\n# tokenizer = ByteLevelBPETokenizer()\r\n# tokenizer.train('big.txt')\r\n# tokenizer.save(directory=CURR_DIR, name=None)\r\n# del tokenizer\r\n######################\r\n\r\nprint('`ByteLevelBPETokenizer` init.')\r\ntokenizer = ByteLevelBPETokenizer(\r\n    os.path.join(CURR_DIR, 'vocab.json'),\r\n    os.path.join(CURR_DIR, 'merges.txt'),\r\n)\r\n######################\r\n# Enabling this next line also causes code to stall\r\n######################\r\n# print(tokenizer.encode('Testing tokenizer.').ids)\r\n\r\ncollate = Collate(tokenizer=tokenizer)\r\ndata_loader = DataLoader(dataset=ListDataset(data),\r\n                         batch_size=2,\r\n                         shuffle=False,\r\n                         num_workers=1,\r\n                         collate_fn=collate,\r\n                         pin_memory=False)\r\nbatches = list(data_loader)\r\nprint(batches)\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/256", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/256/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/256/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/256/events", "html_url": "https://github.com/huggingface/tokenizers/issues/256", "id": 610252875, "node_id": "MDU6SXNzdWU2MTAyNTI4NzU=", "number": 256, "title": "Is tokenizers suitable for sentence segmentation?", "user": {"login": "mustaszewski", "id": 13680208, "node_id": "MDQ6VXNlcjEzNjgwMjA4", "avatar_url": "https://avatars1.githubusercontent.com/u/13680208?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mustaszewski", "html_url": "https://github.com/mustaszewski", "followers_url": "https://api.github.com/users/mustaszewski/followers", "following_url": "https://api.github.com/users/mustaszewski/following{/other_user}", "gists_url": "https://api.github.com/users/mustaszewski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mustaszewski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mustaszewski/subscriptions", "organizations_url": "https://api.github.com/users/mustaszewski/orgs", "repos_url": "https://api.github.com/users/mustaszewski/repos", "events_url": "https://api.github.com/users/mustaszewski/events{/privacy}", "received_events_url": "https://api.github.com/users/mustaszewski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-30T17:20:50Z", "updated_at": "2020-05-17T19:13:34Z", "closed_at": "2020-05-17T19:13:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nrather than posting an issue, I'd like to ask whether Tokenizers can be used for the task of sentence segmentation (i.e. splitting raw text input into sentences). For my task in mind, only the sentence boundary detection would be required. I am interested in Tokenizers for this task because it supports multilingual models and is trainable.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/254", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/254/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/254/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/254/events", "html_url": "https://github.com/huggingface/tokenizers/issues/254", "id": 608181902, "node_id": "MDU6SXNzdWU2MDgxODE5MDI=", "number": 254, "title": "encoding problem when training for Russian ", "user": {"login": "janyfe", "id": 34155871, "node_id": "MDQ6VXNlcjM0MTU1ODcx", "avatar_url": "https://avatars0.githubusercontent.com/u/34155871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/janyfe", "html_url": "https://github.com/janyfe", "followers_url": "https://api.github.com/users/janyfe/followers", "following_url": "https://api.github.com/users/janyfe/following{/other_user}", "gists_url": "https://api.github.com/users/janyfe/gists{/gist_id}", "starred_url": "https://api.github.com/users/janyfe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/janyfe/subscriptions", "organizations_url": "https://api.github.com/users/janyfe/orgs", "repos_url": "https://api.github.com/users/janyfe/repos", "events_url": "https://api.github.com/users/janyfe/events{/privacy}", "received_events_url": "https://api.github.com/users/janyfe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-04-28T09:53:25Z", "updated_at": "2020-05-12T13:55:12Z", "closed_at": "2020-05-12T13:55:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\nfrom tokenizers import ByteLevelBPETokenizer\r\n\r\n# path = [txt files with some text in Russian]\r\n\r\n# Initialize a tokenizer\r\ntokenizer = ByteLevelBPETokenizer()\r\n\r\n# Customize training\r\ntokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\r\n    \"<s>\",\r\n    \"<pad>\",\r\n    \"</s>\",\r\n    \"<unk>\",\r\n    \"<mask>\",\r\n])\r\n\r\n# Save files to disk\r\ntokenizer.save(\".\", \"dbg_bpe\")\r\n\r\nfrom tokenizers.implementations import ByteLevelBPETokenizer\r\n\r\ntokenizer = ByteLevelBPETokenizer(\r\n    './dbg_bpe-vocab.json',\r\n './dbg_bpe-merges.txt'\r\n)\r\n\r\nz = tokenizer.encode('\u0447\u0435\u0441\u0442\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043e \u043f\u0440\u044f\u043c \u0440\u0443\u043a\u0430\u043c\u0438')\r\n\r\nprint(z.tokens)\r\n\r\n['\u00d1\u0129\u00d0\u00b5\u00d1\u0123\u00d1\u0124', '\u00d0\u00bd\u00d0\u00be\u00d0\u00b5', '\u0120\u00d1\u0123\u00d0\u00bb\u00d0\u00be\u00d0\u00b2\u00d0\u00be', '\u0120\u00d0\u00bd\u00d0\u00b0\u00d0\u00bf\u00d0\u00b8\u00d1\u0123\u00d0\u00b0\u00d0\u00bd\u00d0\u00be', '\u0120\u00d0\u00bf\u00d1\u0122\u00d1\u0131\u00d0\u00bc', '\u0120\u00d1\u0122\u00d1\u0125\u00d0\u00ba\u00d0\u00b0\u00d0\u00bc\u00d0\u00b8']\r\n```\r\n\r\n\r\nIs it possible to train tokenizer for Russian?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/253", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/253/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/253/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/253/events", "html_url": "https://github.com/huggingface/tokenizers/issues/253", "id": 608075829, "node_id": "MDU6SXNzdWU2MDgwNzU4Mjk=", "number": 253, "title": "ValueError: logits and labels must have the same shape ((None, 1) vs ())", "user": {"login": "Rose-25", "id": 63998629, "node_id": "MDQ6VXNlcjYzOTk4NjI5", "avatar_url": "https://avatars1.githubusercontent.com/u/63998629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Rose-25", "html_url": "https://github.com/Rose-25", "followers_url": "https://api.github.com/users/Rose-25/followers", "following_url": "https://api.github.com/users/Rose-25/following{/other_user}", "gists_url": "https://api.github.com/users/Rose-25/gists{/gist_id}", "starred_url": "https://api.github.com/users/Rose-25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Rose-25/subscriptions", "organizations_url": "https://api.github.com/users/Rose-25/orgs", "repos_url": "https://api.github.com/users/Rose-25/repos", "events_url": "https://api.github.com/users/Rose-25/events{/privacy}", "received_events_url": "https://api.github.com/users/Rose-25/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-28T07:02:14Z", "updated_at": "2020-05-25T10:44:23Z", "closed_at": "2020-05-12T14:05:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "a part of relative code\r\n\r\nimport tensorflow_datasets as tfds\r\nimdb, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\r\n\r\ntrain_data, test_data = imdb['train'], imdb['test']\r\ntokenizer = info.features['text'].encoder\r\n\r\nimport tensorflow.keras as keras\r\nembedding_dim = 64\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.Embedding(tokenizer.vocab_size, embedding_dim),\r\n    keras.layers.GlobalAveragePooling1D(),\r\n    keras.layers.Dense(6, activation='relu'),\r\n    keras.layers.Dense(1, activation='sigmoid')\r\n])\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_data, epochs=10, validation_data=test_data)\r\n\r\nValueError: logits and labels must have the same shape ((None, 1) vs ())\r\nthere are  not any code of padding sequences,so how can i resolve it\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/252", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/252/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/252/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/252/events", "html_url": "https://github.com/huggingface/tokenizers/issues/252", "id": 607480156, "node_id": "MDU6SXNzdWU2MDc0ODAxNTY=", "number": 252, "title": "Training a BPE model in Rust", "user": {"login": "Ayuei", "id": 18545131, "node_id": "MDQ6VXNlcjE4NTQ1MTMx", "avatar_url": "https://avatars0.githubusercontent.com/u/18545131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ayuei", "html_url": "https://github.com/Ayuei", "followers_url": "https://api.github.com/users/Ayuei/followers", "following_url": "https://api.github.com/users/Ayuei/following{/other_user}", "gists_url": "https://api.github.com/users/Ayuei/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ayuei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ayuei/subscriptions", "organizations_url": "https://api.github.com/users/Ayuei/orgs", "repos_url": "https://api.github.com/users/Ayuei/repos", "events_url": "https://api.github.com/users/Ayuei/events{/privacy}", "received_events_url": "https://api.github.com/users/Ayuei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-27T11:44:57Z", "updated_at": "2020-05-17T19:15:20Z", "closed_at": "2020-05-17T19:15:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I couldn't find any examples on how to do this. So I crunched up some code by looking at the test cases and type hints. I'm not sure if this is the correct way to do this however, would like some help.\r\n\r\n(Build model, pretokenize text, count text/process, train the model with counts hashmap, save).\r\n```rust \r\n// Build trainer with fields\r\nlet trainer = BpeTrainerBuilder::new()`\r\n        .show_progress(true)\r\n        .vocab_size(vocab_size)\r\n        .min_frequency(0)\r\n        .special_tokens(vec![AddedToken::from(\"<s>\".into()),\r\n        AddedToken::from(\"<pad>\".into()),\r\n        AddedToken::from(\"</s>\".into()),\r\n        AddedToken::from(\"<unk>\".into()),\r\n        AddedToken::from(\"<mask>\".into())])\r\n        .build();\r\n\r\nlet mut words = HashMap::new();\r\nlet b = ByteLevel::default();   // Pretokenizer\r\n\r\nfor (i, entry) in glob_files.enumerate(){\r\n        pbar.inc(1);\r\n        if let Ok(path) = entry {\r\n            let file = File::open(path);\r\n            let reader = BufReader::new(file.unwrap());\r\n\r\n            for line in reader.lines() {\r\n                let mut text = NormalizedString::from(&line.unwrap()[..]);\r\n               \r\n                // Clean text\r\n                text.strip();\r\n                text.nfc();\r\n\r\n                // Pretokenize\r\n                let byte_text = b.pre_tokenize(&mut text)\r\n                    .unwrap()\r\n                    .into_iter()\r\n                    .map(|f| f.0)\r\n                    .collect();\r\n\r\n                // Get counts\r\n                trainer.process_tokens(&mut words, byte_text);\r\n            }\r\n      }\r\n}\r\n\r\nlet (model, _special_tokens) = trainer.train(words).unwrap();\r\nmodel.save(folder, Some(\"my_tokenizer\")).unwrap();\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/250", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/250/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/250/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/250/events", "html_url": "https://github.com/huggingface/tokenizers/issues/250", "id": 606708448, "node_id": "MDU6SXNzdWU2MDY3MDg0NDg=", "number": 250, "title": "token_to_id return None", "user": {"login": "lingdavid", "id": 25095662, "node_id": "MDQ6VXNlcjI1MDk1NjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/25095662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lingdavid", "html_url": "https://github.com/lingdavid", "followers_url": "https://api.github.com/users/lingdavid/followers", "following_url": "https://api.github.com/users/lingdavid/following{/other_user}", "gists_url": "https://api.github.com/users/lingdavid/gists{/gist_id}", "starred_url": "https://api.github.com/users/lingdavid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lingdavid/subscriptions", "organizations_url": "https://api.github.com/users/lingdavid/orgs", "repos_url": "https://api.github.com/users/lingdavid/repos", "events_url": "https://api.github.com/users/lingdavid/events{/privacy}", "received_events_url": "https://api.github.com/users/lingdavid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-25T07:11:23Z", "updated_at": "2020-05-12T14:17:51Z", "closed_at": "2020-05-12T14:17:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "tokenizers version: 0.7.0\r\n`tokenizer = BertWordPieceTokenizer(vocab_file='./vocabulary/vocab_small.txt', lowercase=True)\r\nprint(tokenizer.token_to_id(\"...\")`\r\noutput is 'None',not a int number.\r\noov token will return None,it is a bug or?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/248", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/248/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/248/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/248/events", "html_url": "https://github.com/huggingface/tokenizers/issues/248", "id": 606260501, "node_id": "MDU6SXNzdWU2MDYyNjA1MDE=", "number": 248, "title": "Is it a better alternative to Tokenizers in transformers or how are the two related? And I wander why do you implement this as a new project?", "user": {"login": "dxzmpk", "id": 34058412, "node_id": "MDQ6VXNlcjM0MDU4NDEy", "avatar_url": "https://avatars2.githubusercontent.com/u/34058412?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dxzmpk", "html_url": "https://github.com/dxzmpk", "followers_url": "https://api.github.com/users/dxzmpk/followers", "following_url": "https://api.github.com/users/dxzmpk/following{/other_user}", "gists_url": "https://api.github.com/users/dxzmpk/gists{/gist_id}", "starred_url": "https://api.github.com/users/dxzmpk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dxzmpk/subscriptions", "organizations_url": "https://api.github.com/users/dxzmpk/orgs", "repos_url": "https://api.github.com/users/dxzmpk/repos", "events_url": "https://api.github.com/users/dxzmpk/events{/privacy}", "received_events_url": "https://api.github.com/users/dxzmpk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-24T11:58:59Z", "updated_at": "2020-05-04T01:41:25Z", "closed_at": "2020-05-04T01:41:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/247", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/247/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/247/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/247/events", "html_url": "https://github.com/huggingface/tokenizers/issues/247", "id": 605598244, "node_id": "MDU6SXNzdWU2MDU1OTgyNDQ=", "number": 247, "title": "How to add some new special tokens to a pretrained tokenizer?", "user": {"login": "ky941122", "id": 35162145, "node_id": "MDQ6VXNlcjM1MTYyMTQ1", "avatar_url": "https://avatars1.githubusercontent.com/u/35162145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ky941122", "html_url": "https://github.com/ky941122", "followers_url": "https://api.github.com/users/ky941122/followers", "following_url": "https://api.github.com/users/ky941122/following{/other_user}", "gists_url": "https://api.github.com/users/ky941122/gists{/gist_id}", "starred_url": "https://api.github.com/users/ky941122/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ky941122/subscriptions", "organizations_url": "https://api.github.com/users/ky941122/orgs", "repos_url": "https://api.github.com/users/ky941122/repos", "events_url": "https://api.github.com/users/ky941122/events{/privacy}", "received_events_url": "https://api.github.com/users/ky941122/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-04-23T14:43:55Z", "updated_at": "2020-08-18T13:19:54Z", "closed_at": "2020-05-12T14:20:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi guys. I want to add some new special tokens like `[XXX]` to a pretrained `ByteLevelBPETokenizer`, but I can't find how to do this in python. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/246", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/246/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/246/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/246/events", "html_url": "https://github.com/huggingface/tokenizers/issues/246", "id": 604973605, "node_id": "MDU6SXNzdWU2MDQ5NzM2MDU=", "number": 246, "title": "Need help ASAP \ud83d\ude4f", "user": {"login": "mrm8488", "id": 3653789, "node_id": "MDQ6VXNlcjM2NTM3ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3653789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrm8488", "html_url": "https://github.com/mrm8488", "followers_url": "https://api.github.com/users/mrm8488/followers", "following_url": "https://api.github.com/users/mrm8488/following{/other_user}", "gists_url": "https://api.github.com/users/mrm8488/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrm8488/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrm8488/subscriptions", "organizations_url": "https://api.github.com/users/mrm8488/orgs", "repos_url": "https://api.github.com/users/mrm8488/repos", "events_url": "https://api.github.com/users/mrm8488/events{/privacy}", "received_events_url": "https://api.github.com/users/mrm8488/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2020-04-22T18:34:02Z", "updated_at": "2020-05-12T15:25:25Z", "closed_at": "2020-05-12T15:19:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, guys.\r\nI was admitted in TFRC. I have 30 days to use their TPUs for researching. I have found they have a template for training a RoBERTa like model in its TPUs.\r\nThe problem is that it is for English and they use GPT2/encoder.json and GPT2/vocab.bpe.\r\nI need to create both files for my Spanish corpus. Huggingface/Transformers has a tuto to do It but it is not generated a bpe file. Do you know how could I create that files or something that works instead? I will add follow the code they use:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/245", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/245/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/245/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/245/events", "html_url": "https://github.com/huggingface/tokenizers/issues/245", "id": 604672568, "node_id": "MDU6SXNzdWU2MDQ2NzI1Njg=", "number": 245, "title": "wasm as compiler target for browser support", "user": {"login": "lhk", "id": 1475537, "node_id": "MDQ6VXNlcjE0NzU1Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1475537?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhk", "html_url": "https://github.com/lhk", "followers_url": "https://api.github.com/users/lhk/followers", "following_url": "https://api.github.com/users/lhk/following{/other_user}", "gists_url": "https://api.github.com/users/lhk/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhk/subscriptions", "organizations_url": "https://api.github.com/users/lhk/orgs", "repos_url": "https://api.github.com/users/lhk/repos", "events_url": "https://api.github.com/users/lhk/events{/privacy}", "received_events_url": "https://api.github.com/users/lhk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-22T11:15:25Z", "updated_at": "2020-04-22T19:46:11Z", "closed_at": "2020-04-22T15:17:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "It would be great to be able to use this code on the client, or on-device in a hybrid app.\r\n\r\nSince I've never compiled Rust to wasm myself, I'm not aware of the limitations :)\r\nFrom a quick web search, io and threading seem to be off-limits.\r\nWould it be feasible to add wasm as a compilation target to this library?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/244", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/244/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/244/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/244/events", "html_url": "https://github.com/huggingface/tokenizers/issues/244", "id": 604167043, "node_id": "MDU6SXNzdWU2MDQxNjcwNDM=", "number": 244, "title": "Simple WordLevelTokenizer", "user": {"login": "jaymody", "id": 26451316, "node_id": "MDQ6VXNlcjI2NDUxMzE2", "avatar_url": "https://avatars3.githubusercontent.com/u/26451316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jaymody", "html_url": "https://github.com/jaymody", "followers_url": "https://api.github.com/users/jaymody/followers", "following_url": "https://api.github.com/users/jaymody/following{/other_user}", "gists_url": "https://api.github.com/users/jaymody/gists{/gist_id}", "starred_url": "https://api.github.com/users/jaymody/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jaymody/subscriptions", "organizations_url": "https://api.github.com/users/jaymody/orgs", "repos_url": "https://api.github.com/users/jaymody/repos", "events_url": "https://api.github.com/users/jaymody/events{/privacy}", "received_events_url": "https://api.github.com/users/jaymody/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-21T17:45:03Z", "updated_at": "2020-08-06T20:54:54Z", "closed_at": "2020-05-12T14:29:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I was wondering if there are plans to implement a plain `WordLevelTokenizer` implementation that splits only via whitespace (similar to https://github.com/huggingface/tokenizers/issues/3774 and https://github.com/huggingface/tokenizers/issues/232).\r\n\r\nI see that much of the infrastructure for that is already in place, so I attempted to create it myself (this one is specific to BERT for post processing):\r\n\r\n``` python\r\nclass WordLevelBertTokenizer(BaseTokenizer):\r\n    \"\"\" WordLevelBertTokenizer\r\n    Represents a simple word level tokenization for BERT.\r\n    \"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        vocab_file: Optional[str] = None,\r\n        unk_token: Union[str, AddedToken] = \"[UNK]\",\r\n        sep_token: Union[str, AddedToken] = \"[SEP]\",\r\n        cls_token: Union[str, AddedToken] = \"[CLS]\",\r\n        pad_token: Union[str, AddedToken] = \"[PAD]\",\r\n        mask_token: Union[str, AddedToken] = \"[MASK]\",\r\n        lowercase: bool = False,\r\n        unicode_normalizer: Optional[str] = None,\r\n    ):\r\n        if vocab_file is not None:\r\n            tokenizer = Tokenizer(WordLevel(vocab_file))\r\n        else:\r\n            tokenizer = Tokenizer(WordLevel())\r\n\r\n        # Let the tokenizer know about special tokens if they are part of the vocab\r\n        if tokenizer.token_to_id(str(unk_token)) is not None:\r\n            tokenizer.add_special_tokens([str(unk_token)])\r\n        if tokenizer.token_to_id(str(sep_token)) is not None:\r\n            tokenizer.add_special_tokens([str(sep_token)])\r\n        if tokenizer.token_to_id(str(cls_token)) is not None:\r\n            tokenizer.add_special_tokens([str(cls_token)])\r\n        if tokenizer.token_to_id(str(pad_token)) is not None:\r\n            tokenizer.add_special_tokens([str(pad_token)])\r\n        if tokenizer.token_to_id(str(mask_token)) is not None:\r\n            tokenizer.add_special_tokens([str(mask_token)])\r\n\r\n        # Check for Unicode normalization first (before everything else)\r\n        normalizers = []\r\n\r\n        if unicode_normalizer:\r\n            normalizers += [unicode_normalizer_from_str(unicode_normalizer)]\r\n\r\n        if lowercase:\r\n            normalizers += [Lowercase()]\r\n\r\n        # Create the normalizer structure\r\n        if len(normalizers) > 0:\r\n            if len(normalizers) > 1:\r\n                tokenizer.normalizer = Sequence(normalizers)\r\n            else:\r\n                tokenizer.normalizer = normalizers[0]\r\n\r\n        tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\r\n\r\n        if vocab_file is not None:\r\n            sep_token_id = tokenizer.token_to_id(str(sep_token))\r\n            if sep_token_id is None:\r\n                raise TypeError(\"sep_token not found in the vocabulary\")\r\n            cls_token_id = tokenizer.token_to_id(str(cls_token))\r\n            if cls_token_id is None:\r\n                raise TypeError(\"cls_token not found in the vocabulary\")\r\n\r\n            tokenizer.post_processor = processors.BertProcessing(\r\n                (str(sep_token), sep_token_id), (str(cls_token), cls_token_id)\r\n            )\r\n\r\n        parameters = {\r\n            \"model\": \"WordLevel\",\r\n            \"unk_token\": unk_token,\r\n            \"sep_token\": sep_token,\r\n            \"cls_token\": cls_token,\r\n            \"pad_token\": pad_token,\r\n            \"mask_token\": mask_token,\r\n            \"lowercase\": lowercase,\r\n            \"unicode_normalizer\": unicode_normalizer,\r\n        }\r\n\r\n        super().__init__(tokenizer, parameters)\r\n```\r\n\r\nHowever when I pass in a valid `vocab_file` that looks like this:\r\n```\r\n{\r\n  \"0\": \"[PAD]\",\r\n  \"1\": \"[UNK]\",\r\n  \"2\": \"[CLS]\",\r\n  \"3\": \"[SEP]\",\r\n  \"4\": \"[MASK]\",\r\n  \"5\": \"violence\",\r\n  \"6\": \"threes\",\r\n  ...\r\n  \"44040\": \"fanaticism\",\r\n  \"44041\": \"tendency\",\r\n  \"44042\": \"stopover\",\r\n  \"44043\": \"peacemaker\"\r\n}\r\n```\r\nit doesn't actually load the vocab dict in this line `tokenizer = Tokenizer(WordLevel(vocab_file))`. This results in an error being thrown at \r\n```python\r\nif sep_token_id is None:\r\n    raise TypeError(\"sep_token not found in the vocabulary\")\r\n```\r\nAs the tokenizers internal vocab is empty.\r\n\r\nI know that this is a WIP repository, so what I'm asking may not be possible atm, but if it is, would anyone be able to shed some light on what I need to fix? Are there plans to add tokenizer implementations for simple tokenizers like the one above?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/243", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/243/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/243/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/243/events", "html_url": "https://github.com/huggingface/tokenizers/issues/243", "id": 603421047, "node_id": "MDU6SXNzdWU2MDM0MjEwNDc=", "number": 243, "title": "Training own Tokenizer", "user": {"login": "yeozertas", "id": 32851901, "node_id": "MDQ6VXNlcjMyODUxOTAx", "avatar_url": "https://avatars3.githubusercontent.com/u/32851901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yeozertas", "html_url": "https://github.com/yeozertas", "followers_url": "https://api.github.com/users/yeozertas/followers", "following_url": "https://api.github.com/users/yeozertas/following{/other_user}", "gists_url": "https://api.github.com/users/yeozertas/gists{/gist_id}", "starred_url": "https://api.github.com/users/yeozertas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yeozertas/subscriptions", "organizations_url": "https://api.github.com/users/yeozertas/orgs", "repos_url": "https://api.github.com/users/yeozertas/repos", "events_url": "https://api.github.com/users/yeozertas/events{/privacy}", "received_events_url": "https://api.github.com/users/yeozertas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-20T17:51:06Z", "updated_at": "2020-05-17T19:20:04Z", "closed_at": "2020-05-17T19:20:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI can find some tokenizers which I can use as tokenizer and create my own vocabulary (BertWordPieceTokenizer, ByteLevelBPETokenizer, SentencePieceBPETokenizer and like that). \r\nHowever, I have one question? Is it possible to train my own tokenizer with my rules? \r\nFor example; Don't split \"-\" words , but split \"_\" words .  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/240", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/240/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/240/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/240/events", "html_url": "https://github.com/huggingface/tokenizers/issues/240", "id": 602398282, "node_id": "MDU6SXNzdWU2MDIzOTgyODI=", "number": 240, "title": "Difference to Roberta transformers tokenizer", "user": {"login": "psinger", "id": 1677826, "node_id": "MDQ6VXNlcjE2Nzc4MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1677826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/psinger", "html_url": "https://github.com/psinger", "followers_url": "https://api.github.com/users/psinger/followers", "following_url": "https://api.github.com/users/psinger/following{/other_user}", "gists_url": "https://api.github.com/users/psinger/gists{/gist_id}", "starred_url": "https://api.github.com/users/psinger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/psinger/subscriptions", "organizations_url": "https://api.github.com/users/psinger/orgs", "repos_url": "https://api.github.com/users/psinger/repos", "events_url": "https://api.github.com/users/psinger/events{/privacy}", "received_events_url": "https://api.github.com/users/psinger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-18T07:20:42Z", "updated_at": "2020-05-27T18:15:21Z", "closed_at": "2020-05-27T18:15:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi all -\r\n\r\nI am trying to understand some difference I receive when using transformers RobertaTokenizer vs. ByteLevelBPETokenizer.\r\n\r\n```\r\ntokenizer1 = transformers.RobertaTokenizer(\r\n        vocab_file=f\"vocab.json\", \r\n        merges_file=f\"merges.txt\"\r\n)\r\n\r\ntokenizer2 = tokenizers.ByteLevelBPETokenizer(\r\n        vocab_file=f\"vocab.json\", \r\n        merges_file=f\"merges.txt\", \r\n        lowercase=False,\r\n        add_prefix_space=True\r\n    )\r\n\r\ns = \"  'there\"\r\n\r\ntokenizer1.encode(s, add_special_tokens=False, add_prefix_space=True)\r\n# [1437, 128, 8585]\r\n# ['\u0120', \"\u0120'\", 'there']\r\n\r\ntokenizer2.encode(s).ids\r\n# [1437, 1437, 75, 10859]\r\n# ['\u0120', '\u0120', \"'t\", 'here']\r\n```\r\n\r\nThere are two spaces at the beginning of the string, followed by an apostrophe. \r\n\r\nAm I using some different settings somehow? Any other explanation? Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/235", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/235/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/235/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/235/events", "html_url": "https://github.com/huggingface/tokenizers/issues/235", "id": 600017475, "node_id": "MDU6SXNzdWU2MDAwMTc0NzU=", "number": 235, "title": "how can we use  the tokenizers  to load  albert  model?", "user": {"login": "bestpredicts", "id": 12403152, "node_id": "MDQ6VXNlcjEyNDAzMTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/12403152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bestpredicts", "html_url": "https://github.com/bestpredicts", "followers_url": "https://api.github.com/users/bestpredicts/followers", "following_url": "https://api.github.com/users/bestpredicts/following{/other_user}", "gists_url": "https://api.github.com/users/bestpredicts/gists{/gist_id}", "starred_url": "https://api.github.com/users/bestpredicts/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bestpredicts/subscriptions", "organizations_url": "https://api.github.com/users/bestpredicts/orgs", "repos_url": "https://api.github.com/users/bestpredicts/repos", "events_url": "https://api.github.com/users/bestpredicts/events{/privacy}", "received_events_url": "https://api.github.com/users/bestpredicts/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-15T04:48:13Z", "updated_at": "2020-04-22T16:22:47Z", "closed_at": "2020-04-22T16:22:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/233", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/233/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/233/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/233/events", "html_url": "https://github.com/huggingface/tokenizers/issues/233", "id": 599715897, "node_id": "MDU6SXNzdWU1OTk3MTU4OTc=", "number": 233, "title": "New Tokens Aren't Saved", "user": {"login": "AdityaSoni19031997", "id": 22738086, "node_id": "MDQ6VXNlcjIyNzM4MDg2", "avatar_url": "https://avatars2.githubusercontent.com/u/22738086?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AdityaSoni19031997", "html_url": "https://github.com/AdityaSoni19031997", "followers_url": "https://api.github.com/users/AdityaSoni19031997/followers", "following_url": "https://api.github.com/users/AdityaSoni19031997/following{/other_user}", "gists_url": "https://api.github.com/users/AdityaSoni19031997/gists{/gist_id}", "starred_url": "https://api.github.com/users/AdityaSoni19031997/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AdityaSoni19031997/subscriptions", "organizations_url": "https://api.github.com/users/AdityaSoni19031997/orgs", "repos_url": "https://api.github.com/users/AdityaSoni19031997/repos", "events_url": "https://api.github.com/users/AdityaSoni19031997/events{/privacy}", "received_events_url": "https://api.github.com/users/AdityaSoni19031997/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-14T16:53:27Z", "updated_at": "2020-04-22T16:27:30Z", "closed_at": "2020-04-22T16:27:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "Let's say i am new tokens to existing `tokenizers.ByteLevelBPETokenizer`; Upon calling `tokenizers.ByteLevelBPETokenizer.save()`, I was expecting it will save the vocab_file etc including the new tokens i have added so that i can load it from there next time going forward; But to my surprise, it doesn't happen that way;\r\n\r\nCode to reproduce the same,\r\n\r\n```\r\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\r\n        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \r\n        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \r\n        lowercase=True,\r\n        add_prefix_space=True\r\n)\r\nnew_tks = [\"your_fav_tok_1\",  \"your_fav_tok_2\", ...]\r\nTOKENIZER.add_tokens(new_tks)\r\nTOKENIZER.save(\".\") # save them\r\n>Tokenizer(vocabulary_size=50268, model=ByteLevelBPE, add_prefix_space=True, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None)\r\n\r\n# now let's reload from the cached version;\r\n\r\nNEW_TOKENIZER = tokenizers.ByteLevelBPETokenizer(\r\n    vocab_file=f\"./vocab.json\", # saved file\r\n    merges_file=f\"./merges.txt\", #saved_file\r\n    lowercase=True,\r\n    add_prefix_space=True\r\n)\r\nNEW_TOKENIZER\r\n>Tokenizer(vocabulary_size=50265, model=ByteLevelBPE, add_prefix_space=True, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None)\r\n```\r\n\r\ntokenizers_version -> '0.5.2'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/231", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/231/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/231/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/231/events", "html_url": "https://github.com/huggingface/tokenizers/issues/231", "id": 598698577, "node_id": "MDU6SXNzdWU1OTg2OTg1Nzc=", "number": 231, "title": "Do raw text files require sentence segmentation?", "user": {"login": "ck37", "id": 50770, "node_id": "MDQ6VXNlcjUwNzcw", "avatar_url": "https://avatars3.githubusercontent.com/u/50770?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ck37", "html_url": "https://github.com/ck37", "followers_url": "https://api.github.com/users/ck37/followers", "following_url": "https://api.github.com/users/ck37/following{/other_user}", "gists_url": "https://api.github.com/users/ck37/gists{/gist_id}", "starred_url": "https://api.github.com/users/ck37/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ck37/subscriptions", "organizations_url": "https://api.github.com/users/ck37/orgs", "repos_url": "https://api.github.com/users/ck37/repos", "events_url": "https://api.github.com/users/ck37/events{/privacy}", "received_events_url": "https://api.github.com/users/ck37/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-13T06:14:46Z", "updated_at": "2020-04-22T20:50:52Z", "closed_at": "2020-04-22T20:50:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nWhen training a byte-level BPE tokenizer from scratch, do the raw text files need to include only one sentence per line as in SentencePiece? I looked but was not able to find anything in the documentation or source code to clarify this. Apologies if I just missed it though.\r\n\r\nEither way, it would be nice to note it here: https://github.com/huggingface/tokenizers#quick-examples-using-python (I can do a PR if it would be helpful).\r\n\r\nThanks,\r\nChris", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/228", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/228/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/228/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/228/events", "html_url": "https://github.com/huggingface/tokenizers/issues/228", "id": 598080966, "node_id": "MDU6SXNzdWU1OTgwODA5NjY=", "number": 228, "title": "gpt2 returns word-level tokens with byte-level bpe", "user": {"login": "petulla", "id": 3466817, "node_id": "MDQ6VXNlcjM0NjY4MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/3466817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petulla", "html_url": "https://github.com/petulla", "followers_url": "https://api.github.com/users/petulla/followers", "following_url": "https://api.github.com/users/petulla/following{/other_user}", "gists_url": "https://api.github.com/users/petulla/gists{/gist_id}", "starred_url": "https://api.github.com/users/petulla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petulla/subscriptions", "organizations_url": "https://api.github.com/users/petulla/orgs", "repos_url": "https://api.github.com/users/petulla/repos", "events_url": "https://api.github.com/users/petulla/events{/privacy}", "received_events_url": "https://api.github.com/users/petulla/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-10T20:30:17Z", "updated_at": "2020-04-26T22:27:39Z", "closed_at": "2020-04-26T22:27:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "hi I'm using huggingface's pretrained gpt2-large vocab.json and merge.txt to tokenize at the byte level. But i seem to be getting back word-level tokens. \r\n\r\nIs this the level of subword tokenization expected? I wasn't able to find an example of loading a pre-trained gpt2 vocab for tokenization.\r\n\r\n```\r\nbgpt2 = ByteLevelBPETokenizer(vocab_file='/gpt2/large/vocab.json', merges_file=\"/gpt2/large/merges.txt\", add_prefix_space=True)\r\n\r\nb = bgpt2.encode(\"This is a combination of a sentence about separate Apple products.\")\r\nprint(b.tokens)\r\nprint(b.ids)\r\n```\r\nOutput\r\n```\r\n['\u0120This', '\u0120is', '\u0120a', '\u0120combination', '\u0120of', '\u0120a', '\u0120sentence', '\u0120about', '\u0120separate', '\u0120Apple', '\u0120products', '.']\r\n[770, 318, 257, 6087, 286, 257, 6827, 546, 4553, 4196, 3186, 13]\r\n```\r\n\r\nI do see if I try:\r\n\r\n```\r\nb = bgpt2.encode(\"Ilikeeatingapple\")\r\nprint(b.tokens)\r\nprint(b.ids)\r\n```\r\n\r\nBPE returns:\r\n\r\n```\r\n['\u0120I', 'like', 'eating', 'apple']\r\n[314, 2339, 30041, 18040]\r\n```\r\n\r\nI just assumed the words would be broken down into finer subunits.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/225", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/225/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/225/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/225/events", "html_url": "https://github.com/huggingface/tokenizers/issues/225", "id": 596985042, "node_id": "MDU6SXNzdWU1OTY5ODUwNDI=", "number": 225, "title": "How to load sentencepiece model file into SentencePieceBPETokenizer?", "user": {"login": "acmilannesta", "id": 47703762, "node_id": "MDQ6VXNlcjQ3NzAzNzYy", "avatar_url": "https://avatars1.githubusercontent.com/u/47703762?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acmilannesta", "html_url": "https://github.com/acmilannesta", "followers_url": "https://api.github.com/users/acmilannesta/followers", "following_url": "https://api.github.com/users/acmilannesta/following{/other_user}", "gists_url": "https://api.github.com/users/acmilannesta/gists{/gist_id}", "starred_url": "https://api.github.com/users/acmilannesta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acmilannesta/subscriptions", "organizations_url": "https://api.github.com/users/acmilannesta/orgs", "repos_url": "https://api.github.com/users/acmilannesta/repos", "events_url": "https://api.github.com/users/acmilannesta/events{/privacy}", "received_events_url": "https://api.github.com/users/acmilannesta/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-09T02:57:09Z", "updated_at": "2020-05-08T20:56:15Z", "closed_at": "2020-04-22T17:07:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried to load pretrained Xlnet sentencepiece model file (spiece.model), But the ```SentencePieceBPETokenizer``` requires vocab and merges file. How can I create these two files?\r\nThanks!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/223", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/223/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/223/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/223/events", "html_url": "https://github.com/huggingface/tokenizers/issues/223", "id": 596389393, "node_id": "MDU6SXNzdWU1OTYzODkzOTM=", "number": 223, "title": "ByteLevelBPETokenizer with Greek gives weird symbols.", "user": {"login": "gdet", "id": 49757110, "node_id": "MDQ6VXNlcjQ5NzU3MTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/49757110?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gdet", "html_url": "https://github.com/gdet", "followers_url": "https://api.github.com/users/gdet/followers", "following_url": "https://api.github.com/users/gdet/following{/other_user}", "gists_url": "https://api.github.com/users/gdet/gists{/gist_id}", "starred_url": "https://api.github.com/users/gdet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gdet/subscriptions", "organizations_url": "https://api.github.com/users/gdet/orgs", "repos_url": "https://api.github.com/users/gdet/repos", "events_url": "https://api.github.com/users/gdet/events{/privacy}", "received_events_url": "https://api.github.com/users/gdet/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-08T08:07:19Z", "updated_at": "2020-04-10T13:53:17Z", "closed_at": "2020-04-10T13:53:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI have followed your steps in this article https://huggingface.co/blog/how-to-train to train a model in Greek language.All files I used is in UTF-8 encoding. When using ByteLevelBPETokenizer  I get weird symbols. I read in other issues here that this is normal but there is no normal character in my file merges.txt. Also when I try to print it to see if it tokenizes a word correctly it prints this:\r\n\r\n     result = tokenizer.encode(\"\u0393\u03b5\u03b9\u03b1 \u03c4\u03b9 \u03ba\u03ac\u03bd\u03b5\u03b9\u03c2;\")\r\n     print (result .tokens)\r\n     ['<s>', '\u00ce\u0135\u00ce\u00b5\u00ce\u00b9\u00ce\u00b1', '\u0120\u00cf\u0126\u00ce\u00b9', '\u0120\u00ce\u00ba\u00ce\u00ac\u00ce\u00bd\u00ce\u00b5\u00ce\u00b9\u00cf\u0124', ';', '</s>']\r\n\r\nIs this normal? Or ByteLevelBPETokenizer  is not suitable for Greek characters? Also is it possible to tranform this output to readable string to check if it is correct?\r\nExample of merges.txt:\r\n\r\n     \u0120\u00cf\u0126 \u00ce\u00bf\r\n     \u00cf\u0126 \u00ce\u00b7\r\n     \u0120\u00ce \u00bd\r\n     \u0120\u00cf\u0126 \u00ce\u00bf\u00cf\u0127\r\n\r\nThank you", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/219", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/219/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/219/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/219/events", "html_url": "https://github.com/huggingface/tokenizers/issues/219", "id": 594801139, "node_id": "MDU6SXNzdWU1OTQ4MDExMzk=", "number": 219, "title": "Unexpected behavior of BertWordPieceTokenizer with accents", "user": {"login": "kldarek", "id": 15803781, "node_id": "MDQ6VXNlcjE1ODAzNzgx", "avatar_url": "https://avatars1.githubusercontent.com/u/15803781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kldarek", "html_url": "https://github.com/kldarek", "followers_url": "https://api.github.com/users/kldarek/followers", "following_url": "https://api.github.com/users/kldarek/following{/other_user}", "gists_url": "https://api.github.com/users/kldarek/gists{/gist_id}", "starred_url": "https://api.github.com/users/kldarek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kldarek/subscriptions", "organizations_url": "https://api.github.com/users/kldarek/orgs", "repos_url": "https://api.github.com/users/kldarek/repos", "events_url": "https://api.github.com/users/kldarek/events{/privacy}", "received_events_url": "https://api.github.com/users/kldarek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-06T04:53:15Z", "updated_at": "2020-04-06T12:56:49Z", "closed_at": "2020-04-06T12:56:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "I trained BertWordPieceTokenizer with _strip_accents=False_ and _lowercase=True_, I'm working with it now and cannot fully understand the behavior - depending on whether I pass a string or an array of strings, it behaves differently. I'm not sure if I trained my Bert model correctly for this reason, the behavior around Polish special letters is sometimes unexpected. \r\n\r\nCan you clarify if this is expected or a bug, and what's the right way to handle special characters like \"\u0105 \u017c \u0119\"?\r\n\r\n`tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")`\r\n`indices1 = tokenizer.encode('\u017ce wysok\u0105 izb\u0119', add_special_tokens=False)`\r\n`print(tokenizer.decode(indices1))`\r\n`indices2 = tokenizer.encode('\u017ce wysok\u0105 izb\u0119'.split(\" \"), add_special_tokens=False)`\r\n`print(tokenizer.decode(indices2))`\r\n\r\nOutput:\r\n`ze wysoka izbe`\r\n`\u017ce wysok\u0105 izb\u0119`\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/218", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/218/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/218/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/218/events", "html_url": "https://github.com/huggingface/tokenizers/issues/218", "id": 594690015, "node_id": "MDU6SXNzdWU1OTQ2OTAwMTU=", "number": 218, "title": "Issue setting up RobertA tokenizer, \"Exception: Merges text file invalid at line 1\"", "user": {"login": "Santosh-Gupta", "id": 5524261, "node_id": "MDQ6VXNlcjU1MjQyNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5524261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Santosh-Gupta", "html_url": "https://github.com/Santosh-Gupta", "followers_url": "https://api.github.com/users/Santosh-Gupta/followers", "following_url": "https://api.github.com/users/Santosh-Gupta/following{/other_user}", "gists_url": "https://api.github.com/users/Santosh-Gupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/Santosh-Gupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Santosh-Gupta/subscriptions", "organizations_url": "https://api.github.com/users/Santosh-Gupta/orgs", "repos_url": "https://api.github.com/users/Santosh-Gupta/repos", "events_url": "https://api.github.com/users/Santosh-Gupta/events{/privacy}", "received_events_url": "https://api.github.com/users/Santosh-Gupta/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-05T22:47:01Z", "updated_at": "2020-04-13T00:23:03Z", "closed_at": "2020-04-13T00:23:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to set up a RobertA tokenizer using the HFTransformer's vocab and merge files. \r\n\r\n```\r\nimport tokenizers                                                                                                                                                                                                                                                                 \r\nimport transformers                                                                                                                                                                                                                                                               \r\n\r\ntok1 = tokenizers.ByteLevelBPETokenizer( \r\n     vocab_file=\"../../roberta-base/vocab.json\",  \r\n     merges_file=\"../../roberta-base/merges.txt\",  \r\n     lowercase=True, \r\n     add_prefix_space=True \r\n )        \r\n```\r\n\r\nWhere the merges and vocab file are from the transformers library\r\n\r\n'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt'\r\n\r\nand \r\n\r\n\"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json\"\r\n\r\n\r\nI get this error\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n<ipython-input-36-5925f7cca3c7> in <module>()\r\n      3      merges_file='merges.txt',\r\n      4      lowercase=True,\r\n----> 5      add_prefix_space=True\r\n      6 )               \r\n\r\n/usr/local/lib/python3.6/dist-packages/tokenizers/implementations/byte_level_bpe.py in __init__(self, vocab_file, merges_file, add_prefix_space, lowercase, dropout, unicode_normalizer, continuing_subword_prefix, end_of_word_suffix)\r\n     31                     dropout=dropout,\r\n     32                     continuing_subword_prefix=continuing_subword_prefix or \"\",\r\n---> 33                     end_of_word_suffix=end_of_word_suffix or \"\",\r\n     34                 )\r\n     35             )\r\n\r\nException: Merges text file invalid at line 1\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/217", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/217/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/217/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/217/events", "html_url": "https://github.com/huggingface/tokenizers/issues/217", "id": 593548280, "node_id": "MDU6SXNzdWU1OTM1NDgyODA=", "number": 217, "title": "NodeJS Build Error", "user": {"login": "ayir", "id": 22117010, "node_id": "MDQ6VXNlcjIyMTE3MDEw", "avatar_url": "https://avatars3.githubusercontent.com/u/22117010?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ayir", "html_url": "https://github.com/ayir", "followers_url": "https://api.github.com/users/ayir/followers", "following_url": "https://api.github.com/users/ayir/following{/other_user}", "gists_url": "https://api.github.com/users/ayir/gists{/gist_id}", "starred_url": "https://api.github.com/users/ayir/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ayir/subscriptions", "organizations_url": "https://api.github.com/users/ayir/orgs", "repos_url": "https://api.github.com/users/ayir/repos", "events_url": "https://api.github.com/users/ayir/events{/privacy}", "received_events_url": "https://api.github.com/users/ayir/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-03T17:58:40Z", "updated_at": "2020-06-29T16:28:41Z", "closed_at": "2020-06-29T16:28:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have been trying to use the node bindings of tokenizers for a project with tf-js but when I build the project I run into a weird issue. \r\n\r\n```\r\n/PycharmProjects/chrome-extension/node_modules/tokenizers/bindings/native.js:1:23: \r\nCannot resolve dependency '../bin-package' at '/PycharmProjects/chrome-extension/node_modules/tokenizers/bin-package'\r\n> 1 | const native = require(\"../bin-package\");\r\n    |                       ^\r\n  2 | module.exports = native;\r\n```\r\n\r\nPlatform - Mac OS10.14 \r\nTokenizers Version: 0.6.1", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/216", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/216/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/216/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/216/events", "html_url": "https://github.com/huggingface/tokenizers/issues/216", "id": 593255485, "node_id": "MDU6SXNzdWU1OTMyNTU0ODU=", "number": 216, "title": "decoding bug with bert word piece tokenizer?", "user": {"login": "abhishekkrthakur", "id": 1183441, "node_id": "MDQ6VXNlcjExODM0NDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1183441?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhishekkrthakur", "html_url": "https://github.com/abhishekkrthakur", "followers_url": "https://api.github.com/users/abhishekkrthakur/followers", "following_url": "https://api.github.com/users/abhishekkrthakur/following{/other_user}", "gists_url": "https://api.github.com/users/abhishekkrthakur/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhishekkrthakur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhishekkrthakur/subscriptions", "organizations_url": "https://api.github.com/users/abhishekkrthakur/orgs", "repos_url": "https://api.github.com/users/abhishekkrthakur/repos", "events_url": "https://api.github.com/users/abhishekkrthakur/events{/privacy}", "received_events_url": "https://api.github.com/users/abhishekkrthakur/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-03T09:42:59Z", "updated_at": "2020-04-08T18:46:27Z", "closed_at": "2020-04-08T18:46:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Recently, I came across the following:\r\n\r\n```\r\nfrom tokenizers import BertWordPieceTokenizer\r\n\r\ntokenizer = BertWordPieceTokenizer(path_to_vocab, lowercase=True)\r\ns = \"foo (bar)\"\r\ns_token_ids = tokenizer.encode(s).ids\r\n\r\nprint(tokenizer.decode(s_token_ids))\r\n>> foo ( bar )\r\n```\r\n\r\nvs.\r\n\r\n```\r\ns_offs = tokenizer.encode(s).offsets\r\n\r\ndecoded_s  = \"\"\r\nfor i in range(len(s_offs)):\r\n    decoded_s += s[s_offs[i][0]: s_offs[i][1]]\r\n    if (i+1) < len(s_offs) and s_offs[i][1] < s_offs[i+1][0]:\r\n        decoded_s += \" \"\r\nprint(decoded_s)\r\n>> foo (bar)\r\n```\r\n\r\nThis however does not happen when using BPE tokenizer (as far as I checked).\r\n\r\nThe above snippets were shared by @akensert, in the comments to this kaggle kernel: https://www.kaggle.com/akensert/complete-tf2-1-mixed-precision-implementation\r\n\r\nIs this a bug or expected behavior?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/215", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/215/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/215/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/215/events", "html_url": "https://github.com/huggingface/tokenizers/issues/215", "id": 593129507, "node_id": "MDU6SXNzdWU1OTMxMjk1MDc=", "number": 215, "title": "encode() of BertWordPieceTokenizer has wrong behavior upon japanese symbols", "user": {"login": "hymzoque", "id": 34158908, "node_id": "MDQ6VXNlcjM0MTU4OTA4", "avatar_url": "https://avatars0.githubusercontent.com/u/34158908?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hymzoque", "html_url": "https://github.com/hymzoque", "followers_url": "https://api.github.com/users/hymzoque/followers", "following_url": "https://api.github.com/users/hymzoque/following{/other_user}", "gists_url": "https://api.github.com/users/hymzoque/gists{/gist_id}", "starred_url": "https://api.github.com/users/hymzoque/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hymzoque/subscriptions", "organizations_url": "https://api.github.com/users/hymzoque/orgs", "repos_url": "https://api.github.com/users/hymzoque/repos", "events_url": "https://api.github.com/users/hymzoque/events{/privacy}", "received_events_url": "https://api.github.com/users/hymzoque/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-03T06:28:25Z", "updated_at": "2020-04-04T05:05:04Z", "closed_at": "2020-04-04T05:05:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\nimport tokenizers # 0.6.0\r\nimport json\r\nwith open('temp_corpus', 'w', encoding='utf-8') as f:\r\n    f.write('\u30a2\u30ea\u30d0\u30d0\u682a\u5f0f\u4f1a\u793e\u306e\u516c\u5f0f\u30b5\u30a4\u30c8\u3067\u3059\u3002\u30b5\u30fc\u30d3\u30b9\u3001\u4f1a\u793e\u60c5\u5831\u3001\u63a1\u7528\u60c5\u5831\u306a\u3069\u3092\u3054\u89a7\u3044\u305f\u3060\u3051\u307e\u3059')\r\ntokenizer = tokenizers.BertWordPieceTokenizer()\r\ntokenizer.train('temp_corpus', vocab_size=20)\r\nres = tokenizer.encode('\u30a2\u30ea\u30d0\u30d0\u682a\u5f0f\u4f1a\u793e\u306e\u516c\u5f0f\u30b5\u30a4\u30c8\u3067\u3059')\r\nprint(json.dumps([res.tokens, res.offsets], ensure_ascii=False))\r\n>> [[\"\u30a2\", \"##\u30ea\", \"##\u30cf\", \"##\u30cf\", \"\u682a\", \"\u5f0f\", \"\u4f1a\", \"\u793e\", \"\u306e\", \"\u516c\", \"\u5f0f\", \\\r\n     \"\u30b5\", \"##\u30a4\", \"##\u30c8\", \"##\u3066\", \"##\u3059\"], \\\r\n   [[0, 1], [1, 2], [2, 3], [3, 4], [5, 6], [8, 9], [11, 12], [14, 15], [16, 17], \\\r\n    [18, 19], [21, 22], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28]]]\r\n```\r\n\r\n1. The symbol '\u30d0' is encoded to '\u30cf', '\u3067' is encoded to '\u3066', and they are different symbols\r\n2. Offsets are confusing\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/214", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/214/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/214/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/214/events", "html_url": "https://github.com/huggingface/tokenizers/issues/214", "id": 591075498, "node_id": "MDU6SXNzdWU1OTEwNzU0OTg=", "number": 214, "title": "tokenizer fails for large dataset", "user": {"login": "vr25", "id": 22553367, "node_id": "MDQ6VXNlcjIyNTUzMzY3", "avatar_url": "https://avatars2.githubusercontent.com/u/22553367?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vr25", "html_url": "https://github.com/vr25", "followers_url": "https://api.github.com/users/vr25/followers", "following_url": "https://api.github.com/users/vr25/following{/other_user}", "gists_url": "https://api.github.com/users/vr25/gists{/gist_id}", "starred_url": "https://api.github.com/users/vr25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vr25/subscriptions", "organizations_url": "https://api.github.com/users/vr25/orgs", "repos_url": "https://api.github.com/users/vr25/repos", "events_url": "https://api.github.com/users/vr25/events{/privacy}", "received_events_url": "https://api.github.com/users/vr25/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-31T12:40:52Z", "updated_at": "2020-06-29T16:28:17Z", "closed_at": "2020-06-29T16:28:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI am trying to replicate this blog-post: https://huggingface.co/blog/how-to-train\r\n\r\nI am doing okay for a small set of files but it throws the following error for a larger dataset:\r\n\r\n[00:00:00] Count pairs                              \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 11219    /    11219\r\nthread '<unnamed>' panicked at 'called `Option::unwrap()` on a `None` value', /home/conda/feedstock_root/build_artifacts/tokenizers_1583876217402/work/tokenizers/src/models/bpe/trainer.rs:373:34\r\nstack backtrace:\r\n   0: backtrace::backtrace::libunwind::trace\r\n             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.44/src/backtrace/libunwind.rs:86\r\n   1: backtrace::backtrace::trace_unsynchronized\r\n             at /cargo/registry/src/github.com-1ecc6299db9ec823/backtrace-0.3.44/src/backtrace/mod.rs:66\r\n   2: std::sys_common::backtrace::_print_fmt\r\n             at src/libstd/sys_common/backtrace.rs:78\r\n   3: <std::sys_common::backtrace::_print::DisplayBacktrace as core::fmt::Display>::fmt\r\n             at src/libstd/sys_common/backtrace.rs:59\r\n   4: core::fmt::write\r\n             at src/libcore/fmt/mod.rs:1063\r\n   5: std::io::Write::write_fmt\r\n             at src/libstd/io/mod.rs:1428\r\n   6: std::sys_common::backtrace::_print\r\n             at src/libstd/sys_common/backtrace.rs:62\r\n   7: std::sys_common::backtrace::print\r\n             at src/libstd/sys_common/backtrace.rs:49\r\n   8: std::panicking::default_hook::{{closure}}\r\n             at src/libstd/panicking.rs:204\r\n   9: std::panicking::default_hook\r\n             at src/libstd/panicking.rs:224\r\n  10: std::panicking::rust_panic_with_hook\r\n             at src/libstd/panicking.rs:470\r\n  11: rust_begin_unwind\r\n             at src/libstd/panicking.rs:378\r\n  12: core::panicking::panic_fmt\r\n             at src/libcore/panicking.rs:85\r\n  13: core::panicking::panic\r\n             at src/libcore/panicking.rs:52\r\n  14: core::ops::function::impls::<impl core::ops::function::FnMut<A> for &F>::call_mut\r\n  15: rayon::iter::plumbing::Folder::consume_iter\r\n  16: rayon::iter::plumbing::Producer::fold_with\r\n  17: rayon::iter::plumbing::bridge_producer_consumer::helper\r\n  18: std::panicking::try::do_call\r\n  19: __rust_try\r\n  20: __rust_maybe_catch_panic\r\n             at src/libpanic_unwind/lib.rs:86\r\n  21: <rayon_core::job::StackJob<L,F,R> as rayon_core::job::Job>::execute\r\n  22: rayon_core::registry::WorkerThread::wait_until_cold\r\n  23: rayon_core::registry::ThreadBuilder::run\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\nfatal runtime error: failed to initiate panic, error 3613222800\r\nAborted\r\n\r\n\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/210", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/210/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/210/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/210/events", "html_url": "https://github.com/huggingface/tokenizers/issues/210", "id": 590333333, "node_id": "MDU6SXNzdWU1OTAzMzMzMzM=", "number": 210, "title": "[Help] Is it possible to split up a chunk of text into sentences?", "user": {"login": "martinnormark", "id": 67565, "node_id": "MDQ6VXNlcjY3NTY1", "avatar_url": "https://avatars3.githubusercontent.com/u/67565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinnormark", "html_url": "https://github.com/martinnormark", "followers_url": "https://api.github.com/users/martinnormark/followers", "following_url": "https://api.github.com/users/martinnormark/following{/other_user}", "gists_url": "https://api.github.com/users/martinnormark/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinnormark/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinnormark/subscriptions", "organizations_url": "https://api.github.com/users/martinnormark/orgs", "repos_url": "https://api.github.com/users/martinnormark/repos", "events_url": "https://api.github.com/users/martinnormark/events{/privacy}", "received_events_url": "https://api.github.com/users/martinnormark/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-30T14:23:48Z", "updated_at": "2020-03-31T06:54:58Z", "closed_at": "2020-03-31T06:54:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it correctly understood, that the `SentencePieceBPETokenizer` can be used to tokenize a chunck of text into sentences?\r\n\r\n```javascript\r\nconst tokenizer = await tokenizers.SentencePieceBPETokenizer.fromOptions({ vocabFile: \"./bert-large-uncased-vocab.txt\" });\r\nconst wpEncoded = await tokenizer.encode(text);\r\n\r\nconsole.log(wpEncoded.length);\r\nconsole.log(wpEncoded.tokens);\r\nconsole.log(wpEncoded.ids);\r\nconsole.log(wpEncoded.attentionMask);\r\nconsole.log(wpEncoded.offsets);\r\nconsole.log(wpEncoded.overflowing);\r\nconsole.log(wpEncoded.specialTokensMask);\r\nconsole.log(wpEncoded.typeIds);\r\n```\r\n\r\nThe above returns empty results:\r\n\r\n```\r\n0\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n[]\r\n```\r\n\r\nI am probably doing something wrong?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/209", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/209/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/209/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/209/events", "html_url": "https://github.com/huggingface/tokenizers/issues/209", "id": 589555420, "node_id": "MDU6SXNzdWU1ODk1NTU0MjA=", "number": 209, "title": "Different token ids in tokenizers and transformers", "user": {"login": "abhishekkrthakur", "id": 1183441, "node_id": "MDQ6VXNlcjExODM0NDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1183441?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abhishekkrthakur", "html_url": "https://github.com/abhishekkrthakur", "followers_url": "https://api.github.com/users/abhishekkrthakur/followers", "following_url": "https://api.github.com/users/abhishekkrthakur/following{/other_user}", "gists_url": "https://api.github.com/users/abhishekkrthakur/gists{/gist_id}", "starred_url": "https://api.github.com/users/abhishekkrthakur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abhishekkrthakur/subscriptions", "organizations_url": "https://api.github.com/users/abhishekkrthakur/orgs", "repos_url": "https://api.github.com/users/abhishekkrthakur/repos", "events_url": "https://api.github.com/users/abhishekkrthakur/events{/privacy}", "received_events_url": "https://api.github.com/users/abhishekkrthakur/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-28T11:06:01Z", "updated_at": "2020-03-28T13:24:36Z", "closed_at": "2020-03-28T13:24:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\nIn [1]: import tokenizers                                                                                                                                                                                                                                                                 \r\n\r\nIn [2]: import transformers                                                                                                                                                                                                                                                               \r\n\r\nIn [3]: tok1 = tokenizers.ByteLevelBPETokenizer( \r\n   ...:     vocab_file=\"../../roberta-base/vocab.json\",  \r\n   ...:     merges_file=\"../../roberta-base/merges.txt\",  \r\n   ...:     lowercase=True, \r\n   ...:     add_prefix_space=True \r\n   ...: )                                                                                                                                                                                                                                                                                 \r\n\r\nIn [4]: tok2 = transformers.RobertaTokenizer.from_pretrained(\"../../roberta-base/\")                                                                                                                                                                                                       \r\n\r\nIn [5]: tok1.encode(\"hello\").ids                                                                                                                                                                                                                                                          \r\nOut[5]: [20760]\r\n\r\nIn [6]: tok2.encode(\"hello\")                                                                                                                                                                                                                                                              \r\nOut[6]: [0, 42891, 2]\r\n```\r\n\r\nSo, using `tokenizers`, the id for \"hello\" is \"20760\" and using `transformers`, the id for \"hello\" is \"42891\". \r\n\r\nShouldn\u2019t both these should be same?\r\n\r\n\r\ntokenizers==0.6.0\r\ntransformers==2.3.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/207", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/207/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/207/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/207/events", "html_url": "https://github.com/huggingface/tokenizers/issues/207", "id": 589089370, "node_id": "MDU6SXNzdWU1ODkwODkzNzA=", "number": 207, "title": "Token <-> ids mapping", "user": {"login": "cbaziotis", "id": 5629093, "node_id": "MDQ6VXNlcjU2MjkwOTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5629093?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbaziotis", "html_url": "https://github.com/cbaziotis", "followers_url": "https://api.github.com/users/cbaziotis/followers", "following_url": "https://api.github.com/users/cbaziotis/following{/other_user}", "gists_url": "https://api.github.com/users/cbaziotis/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbaziotis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbaziotis/subscriptions", "organizations_url": "https://api.github.com/users/cbaziotis/orgs", "repos_url": "https://api.github.com/users/cbaziotis/repos", "events_url": "https://api.github.com/users/cbaziotis/events{/privacy}", "received_events_url": "https://api.github.com/users/cbaziotis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-27T12:08:35Z", "updated_at": "2020-03-27T21:38:03Z", "closed_at": "2020-03-27T21:38:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is there a way to get the tok2id and id2tok mappings (dictionaries) for a trained tokenizer? \r\nAll I've found is how to get the ids of a tokenized sentence. I need to know how to get the id for every token in the tokenizer's vocabulary.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/205", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/205/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/205/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/205/events", "html_url": "https://github.com/huggingface/tokenizers/issues/205", "id": 588444426, "node_id": "MDU6SXNzdWU1ODg0NDQ0MjY=", "number": 205, "title": "Extra space in decoded string (CharBPETokenizer)", "user": {"login": "samsontmr", "id": 15007950, "node_id": "MDQ6VXNlcjE1MDA3OTUw", "avatar_url": "https://avatars0.githubusercontent.com/u/15007950?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samsontmr", "html_url": "https://github.com/samsontmr", "followers_url": "https://api.github.com/users/samsontmr/followers", "following_url": "https://api.github.com/users/samsontmr/following{/other_user}", "gists_url": "https://api.github.com/users/samsontmr/gists{/gist_id}", "starred_url": "https://api.github.com/users/samsontmr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samsontmr/subscriptions", "organizations_url": "https://api.github.com/users/samsontmr/orgs", "repos_url": "https://api.github.com/users/samsontmr/repos", "events_url": "https://api.github.com/users/samsontmr/events{/privacy}", "received_events_url": "https://api.github.com/users/samsontmr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-26T14:01:48Z", "updated_at": "2020-03-27T21:59:32Z", "closed_at": "2020-03-27T21:59:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "The decoder appears to be adding an extra space at the end of the string:\r\n\r\n`tokenizer.decode(tokenizer.encode('I can feel the magic, can you?').ids)` produces `'I can feel the magic, can you? '` instead of `'I can feel the magic, can you?'`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/204", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/204/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/204/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/204/events", "html_url": "https://github.com/huggingface/tokenizers/issues/204", "id": 588144274, "node_id": "MDU6SXNzdWU1ODgxNDQyNzQ=", "number": 204, "title": "Direct subtokenization for BertWordPieceTokenizer, and tokenizer that records original whitespace for reconstruction", "user": {"login": "Santosh-Gupta", "id": 5524261, "node_id": "MDQ6VXNlcjU1MjQyNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5524261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Santosh-Gupta", "html_url": "https://github.com/Santosh-Gupta", "followers_url": "https://api.github.com/users/Santosh-Gupta/followers", "following_url": "https://api.github.com/users/Santosh-Gupta/following{/other_user}", "gists_url": "https://api.github.com/users/Santosh-Gupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/Santosh-Gupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Santosh-Gupta/subscriptions", "organizations_url": "https://api.github.com/users/Santosh-Gupta/orgs", "repos_url": "https://api.github.com/users/Santosh-Gupta/repos", "events_url": "https://api.github.com/users/Santosh-Gupta/events{/privacy}", "received_events_url": "https://api.github.com/users/Santosh-Gupta/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-26T04:22:08Z", "updated_at": "2020-04-11T07:54:57Z", "closed_at": "2020-04-11T07:54:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Context: I am trying to incorporate this tokenizer library into my preprocessing and prediction pipelines for Bert question and answering. This involves taking position IDs from the Bert output, and making them back to where they were in the original non-normalized text. \r\n\r\nFor now, my pipeline does the following:\r\n\r\noriginaltext -> tokenize (including white space tokens) -> filter out white space tokens -> use transformers tokenizer to normalize subtokenize tokens.\r\n\r\nDoing tokenization this way allows to make maps which can point an index from one set of tokens to another; this is ultimately used for highlighting an answer within it's original text using Bert outputs which gives indexes that point to answers in the bert subtokens. \r\n\r\nFor the last step, I am using `tokenizer.tokenize(token)`. I am wondering if there's an equivalent for this library. I see that BertWordPieceTokenizer has a `token_to_id` attribute, but this seems to be for tokens where are already in Bert's vocab; it does not subtokenize/normalize tokens. For example, `BertWordPieceTokenizer.token_to_id('biologicalificationize')` will not give an id. \r\n\r\nAs for recording whitespaces, currently I am using spaCy for the original split, which also includes whitespaces as tokens in cases where there just isn't a simple ' ' between words. For example, the spaCy tokenized version of 'this \\n sentence \\t now', will be tokenized to 'this', '\\n', 'sentence', '\\t', 'now'. These tokens also have a 'whitespace_' attribute to which contains the exact whitespaces in between words. \r\n\r\nI am wondering if any of the tokenizers have something like a 'whitespace_' attribute, to assist in recreation of text from the tokens. A usable alternative would be to record everything that was filtered out, and \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/199", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/199/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/199/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/199/events", "html_url": "https://github.com/huggingface/tokenizers/issues/199", "id": 584021149, "node_id": "MDU6SXNzdWU1ODQwMjExNDk=", "number": 199, "title": "Encode text dataset to items with equal length ", "user": {"login": "jwallat", "id": 24674150, "node_id": "MDQ6VXNlcjI0Njc0MTUw", "avatar_url": "https://avatars2.githubusercontent.com/u/24674150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwallat", "html_url": "https://github.com/jwallat", "followers_url": "https://api.github.com/users/jwallat/followers", "following_url": "https://api.github.com/users/jwallat/following{/other_user}", "gists_url": "https://api.github.com/users/jwallat/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwallat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwallat/subscriptions", "organizations_url": "https://api.github.com/users/jwallat/orgs", "repos_url": "https://api.github.com/users/jwallat/repos", "events_url": "https://api.github.com/users/jwallat/events{/privacy}", "received_events_url": "https://api.github.com/users/jwallat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-18T21:43:02Z", "updated_at": "2020-03-18T22:41:33Z", "closed_at": "2020-03-18T22:41:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nI am trying to load a text dataset (wikitext-103) for language modeling with the transformers library. Since pre-processing the dataset with the transformers tokenizer takes fairly long (> 30 minutes), I am looking at using the faster tokenizers library. \r\n\r\nWhat I want to do is getting my dataset split into encoded chunks of equal size (in my case with BERT the block size is 512). Is there a function that does exactly that? \r\n\r\nThe transformers' run_language_modeling [example](https://github.com/huggingface/transformers/blob/20139b7c8d88f380f1e4e0ae2baf0b0ac9351039/examples/run_language_modeling.py#L86) does it this way: \r\n\r\n`\r\n            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))`\r\n\r\n`\r\n                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  \r\n                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))`\r\n\r\nFrom what I can tell, in the tokenizers lib, there are no functions for the single steps of tokenization, converting lists of tokens to ids and only adding special tokens to an otherwise prepared encoded list of ids.\r\n\r\nIs there maybe another way to get this done?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/195", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/195/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/195/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/195/events", "html_url": "https://github.com/huggingface/tokenizers/issues/195", "id": 579799648, "node_id": "MDU6SXNzdWU1Nzk3OTk2NDg=", "number": 195, "title": "How to train a BertWordPieceTokenizer", "user": {"login": "thak123", "id": 3891859, "node_id": "MDQ6VXNlcjM4OTE4NTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3891859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thak123", "html_url": "https://github.com/thak123", "followers_url": "https://api.github.com/users/thak123/followers", "following_url": "https://api.github.com/users/thak123/following{/other_user}", "gists_url": "https://api.github.com/users/thak123/gists{/gist_id}", "starred_url": "https://api.github.com/users/thak123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thak123/subscriptions", "organizations_url": "https://api.github.com/users/thak123/orgs", "repos_url": "https://api.github.com/users/thak123/repos", "events_url": "https://api.github.com/users/thak123/events{/privacy}", "received_events_url": "https://api.github.com/users/thak123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-12T09:22:07Z", "updated_at": "2020-03-27T22:11:48Z", "closed_at": "2020-03-27T22:11:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "What are the special tokesn that should be passed to train a BertWordPieceTokenizer ?\r\n\r\nBPE tokenizer does not work with Bert style LM as the bert requires masks and other features from input", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/192", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/192/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/192/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/192/events", "html_url": "https://github.com/huggingface/tokenizers/issues/192", "id": 577183277, "node_id": "MDU6SXNzdWU1NzcxODMyNzc=", "number": 192, "title": "ByteLevelBPETokenizer doesn't work when file contains incorrect bytes", "user": {"login": "andriihomiak", "id": 35844847, "node_id": "MDQ6VXNlcjM1ODQ0ODQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/35844847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andriihomiak", "html_url": "https://github.com/andriihomiak", "followers_url": "https://api.github.com/users/andriihomiak/followers", "following_url": "https://api.github.com/users/andriihomiak/following{/other_user}", "gists_url": "https://api.github.com/users/andriihomiak/gists{/gist_id}", "starred_url": "https://api.github.com/users/andriihomiak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andriihomiak/subscriptions", "organizations_url": "https://api.github.com/users/andriihomiak/orgs", "repos_url": "https://api.github.com/users/andriihomiak/repos", "events_url": "https://api.github.com/users/andriihomiak/events{/privacy}", "received_events_url": "https://api.github.com/users/andriihomiak/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-06T21:24:26Z", "updated_at": "2020-03-08T10:06:06Z", "closed_at": "2020-03-08T10:06:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "version: `0.6.0`\r\n\r\nI have a 4GB ukrainian corpus, it is splitted up into 4 files, and whenever I try to `.train()` `ByteLevelBPETokenizer` it starts to read the files and then suddenly stops. \r\n\r\nFrom the progressbar I found the byte range on which it struggles and turns out there are some corrupted words like `\u0440\u0435\u0437\u043e\u043d\ufffd`. I assumed it was the culprit, so I moved this word to the beginning of the file, ran `.train()` on this file only and indeed - the progress stopped right at the start, showing 0B progress like so:\r\n```\r\n[00:00:00] files_to_use.split03                     \u2591\u2591\u2591\u2591\u2591\u2591\u2591 0B       / 602.41MB\r\n```\r\n\r\nIs this expected behaviour?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/191", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/191/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/191/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/191/events", "html_url": "https://github.com/huggingface/tokenizers/issues/191", "id": 577159075, "node_id": "MDU6SXNzdWU1NzcxNTkwNzU=", "number": 191, "title": "Questions about token ids in my trained vocabulary vs. Transformers pre-trained vocabulary", "user": {"login": "dizzySummer", "id": 49494825, "node_id": "MDQ6VXNlcjQ5NDk0ODI1", "avatar_url": "https://avatars1.githubusercontent.com/u/49494825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dizzySummer", "html_url": "https://github.com/dizzySummer", "followers_url": "https://api.github.com/users/dizzySummer/followers", "following_url": "https://api.github.com/users/dizzySummer/following{/other_user}", "gists_url": "https://api.github.com/users/dizzySummer/gists{/gist_id}", "starred_url": "https://api.github.com/users/dizzySummer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dizzySummer/subscriptions", "organizations_url": "https://api.github.com/users/dizzySummer/orgs", "repos_url": "https://api.github.com/users/dizzySummer/repos", "events_url": "https://api.github.com/users/dizzySummer/events{/privacy}", "received_events_url": "https://api.github.com/users/dizzySummer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-06T20:28:40Z", "updated_at": "2020-03-12T09:34:51Z", "closed_at": "2020-03-12T09:28:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have tried different encoding methods that you offered. There are two questions I would like to check with you:\r\n\r\n1) Taking the example of BertWordPieceTokenizer: In my vocabulary, the id for [CLS] is 1 and for [SEP] is 2. In your pre-trained vocabulary, the id for [CLS] is 101 and for [SEP] is 102. So the question is when feeding my trained tokens to your pre-trained models e.g. Bert-Base uncased, will my tokens cause any confusion to your pre-trained model and then produce a lower accuracy as the result?  In other words,  the model expected to get 101 as a starting point but got 1 ...\r\n\r\n2) What is the right way to set \"add_prefix_space\" to True in ByteLevelBPETokenizer? I have tried:\r\n\r\n     a) from tokenizers import ByteLevelBPETokenizer\r\n         tokenizer = BertWordPieceTokenizer(add_prefix_space=True)\r\n\r\n     b)  from tokenizers import ByteLevelBPETokenizer\r\n          tokenizer = BertWordPieceTokenizer()\r\n         tokenizer.add_prefix_space=True\r\n\r\n None of them works. Any comments? Thank you!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/189", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/189/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/189/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/189/events", "html_url": "https://github.com/huggingface/tokenizers/issues/189", "id": 576692024, "node_id": "MDU6SXNzdWU1NzY2OTIwMjQ=", "number": 189, "title": "[Bug] Error in saving tokenizer after adding tokens", "user": {"login": "ahnjaewoo", "id": 15323600, "node_id": "MDQ6VXNlcjE1MzIzNjAw", "avatar_url": "https://avatars1.githubusercontent.com/u/15323600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahnjaewoo", "html_url": "https://github.com/ahnjaewoo", "followers_url": "https://api.github.com/users/ahnjaewoo/followers", "following_url": "https://api.github.com/users/ahnjaewoo/following{/other_user}", "gists_url": "https://api.github.com/users/ahnjaewoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahnjaewoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahnjaewoo/subscriptions", "organizations_url": "https://api.github.com/users/ahnjaewoo/orgs", "repos_url": "https://api.github.com/users/ahnjaewoo/repos", "events_url": "https://api.github.com/users/ahnjaewoo/events{/privacy}", "received_events_url": "https://api.github.com/users/ahnjaewoo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-06T04:33:09Z", "updated_at": "2020-04-22T17:09:22Z", "closed_at": "2020-04-22T17:09:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Below is a part of the code with an error.\r\n```\r\nfrom tokenizers import SentencePieceBPETokenizer\r\n\r\ntokenizer = SentencePieceBPETokenizer()\r\ntokenizer.train([data_dir], vocab_size=32000)\r\ntokenizer.add_tokens(['<some tokens>'])\r\ntokenizer.save('.')  # save it to ['vocab.json', 'merges.txt']\r\n```\r\nAfter I saved the tokenizer, I found that `vocab.json` file didn't contain any tokens (i.e., `<some tokens>` above).\r\nI think this `save` method should be improved.\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/187", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/187/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/187/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/187/events", "html_url": "https://github.com/huggingface/tokenizers/issues/187", "id": 575662301, "node_id": "MDU6SXNzdWU1NzU2NjIzMDE=", "number": 187, "title": "python version hangs on encode_batch when run in subprocess", "user": {"login": "kretes", "id": 1224887, "node_id": "MDQ6VXNlcjEyMjQ4ODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1224887?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kretes", "html_url": "https://github.com/kretes", "followers_url": "https://api.github.com/users/kretes/followers", "following_url": "https://api.github.com/users/kretes/following{/other_user}", "gists_url": "https://api.github.com/users/kretes/gists{/gist_id}", "starred_url": "https://api.github.com/users/kretes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kretes/subscriptions", "organizations_url": "https://api.github.com/users/kretes/orgs", "repos_url": "https://api.github.com/users/kretes/repos", "events_url": "https://api.github.com/users/kretes/events{/privacy}", "received_events_url": "https://api.github.com/users/kretes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2020-03-04T18:42:57Z", "updated_at": "2020-06-29T16:22:23Z", "closed_at": "2020-06-29T16:22:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nWhen I run encode_batch with at least two texts in an array in python subprocess - the call hangs.\r\nThis happens in real-life when used inside pytorch dataloaders with multiple workers.\r\nA self-contained reproducible  script is here: https://gist.github.com/kretes/1a51bb8b936fc4e6277f71931b886bed\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/186", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/186/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/186/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/186/events", "html_url": "https://github.com/huggingface/tokenizers/issues/186", "id": 575642968, "node_id": "MDU6SXNzdWU1NzU2NDI5Njg=", "number": 186, "title": "Special tokens not removed by ByteLevelBPETokenizer decoder ", "user": {"login": "tomhosking", "id": 9419158, "node_id": "MDQ6VXNlcjk0MTkxNTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/9419158?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomhosking", "html_url": "https://github.com/tomhosking", "followers_url": "https://api.github.com/users/tomhosking/followers", "following_url": "https://api.github.com/users/tomhosking/following{/other_user}", "gists_url": "https://api.github.com/users/tomhosking/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomhosking/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomhosking/subscriptions", "organizations_url": "https://api.github.com/users/tomhosking/orgs", "repos_url": "https://api.github.com/users/tomhosking/repos", "events_url": "https://api.github.com/users/tomhosking/events{/privacy}", "received_events_url": "https://api.github.com/users/tomhosking/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-04T18:18:49Z", "updated_at": "2020-03-05T10:51:47Z", "closed_at": "2020-03-05T10:51:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following code snippet doesn't behave as I would have expected (compare to eg BertWordPieceTokenizer):\r\n\r\n```\r\nfrom tokenizers import ByteLevelBPETokenizer\r\n\r\nmodel_slug = 'bart-large'\r\ntokenizer = ByteLevelBPETokenizer(\"./data/bert-vocabs/{:}-vocab.json\".format(model_slug), \"./data/bert-vocabs/{:}-merges.txt\".format(model_slug))\r\noutput = tokenizer.encode(' Here is a sentence with some special tokens <unk> <mask>.')\r\ntokenizer.decode([0] + output.ids + [2] + [1]*3, skip_special_tokens=True)\r\n>>> '<s> Here is a sentence with some special tokens <unk> <mask>.</s><pad><pad><pad>'\r\n```\r\n\r\nI would have expected all the special tokens to be stripped by the decoder?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/182", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/182/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/182/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/182/events", "html_url": "https://github.com/huggingface/tokenizers/issues/182", "id": 574876604, "node_id": "MDU6SXNzdWU1NzQ4NzY2MDQ=", "number": 182, "title": "Format for .txt Files for Training BERT WordPiece Tokenizer? ", "user": {"login": "siddk", "id": 2498509, "node_id": "MDQ6VXNlcjI0OTg1MDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/2498509?v=4", "gravatar_id": "", "url": "https://api.github.com/users/siddk", "html_url": "https://github.com/siddk", "followers_url": "https://api.github.com/users/siddk/followers", "following_url": "https://api.github.com/users/siddk/following{/other_user}", "gists_url": "https://api.github.com/users/siddk/gists{/gist_id}", "starred_url": "https://api.github.com/users/siddk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/siddk/subscriptions", "organizations_url": "https://api.github.com/users/siddk/orgs", "repos_url": "https://api.github.com/users/siddk/repos", "events_url": "https://api.github.com/users/siddk/events{/privacy}", "received_events_url": "https://api.github.com/users/siddk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-03T19:03:58Z", "updated_at": "2020-06-29T16:27:24Z", "closed_at": "2020-06-29T16:27:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "What should the format of files be for training the BERT Wordpiece Tokenizer? Specifically for training the tokenizer on WikiText-103 or similar Wikipedia dumps?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/181", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/181/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/181/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/181/events", "html_url": "https://github.com/huggingface/tokenizers/issues/181", "id": 574473481, "node_id": "MDU6SXNzdWU1NzQ0NzM0ODE=", "number": 181, "title": "Question marks disappeared from the vocab for BertWordPieceTokenizer", "user": {"login": "13717680252", "id": 18718184, "node_id": "MDQ6VXNlcjE4NzE4MTg0", "avatar_url": "https://avatars0.githubusercontent.com/u/18718184?v=4", "gravatar_id": "", "url": "https://api.github.com/users/13717680252", "html_url": "https://github.com/13717680252", "followers_url": "https://api.github.com/users/13717680252/followers", "following_url": "https://api.github.com/users/13717680252/following{/other_user}", "gists_url": "https://api.github.com/users/13717680252/gists{/gist_id}", "starred_url": "https://api.github.com/users/13717680252/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/13717680252/subscriptions", "organizations_url": "https://api.github.com/users/13717680252/orgs", "repos_url": "https://api.github.com/users/13717680252/repos", "events_url": "https://api.github.com/users/13717680252/events{/privacy}", "received_events_url": "https://api.github.com/users/13717680252/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-03T07:36:47Z", "updated_at": "2020-03-05T02:12:42Z", "closed_at": "2020-03-05T02:12:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Recently I use the wordpiece tokenizer to build the vocabulary, and I am sure that there are more than 30000 appearances of question mark (?) in my raw corpus, but it ended up missed in the trained vocab file.\r\nIt seems that the basic tokenizer (normalizer) works well, and where might be the problem?\r\nLooking forward to reply. Thanks !", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/178", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/178/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/178/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/178/events", "html_url": "https://github.com/huggingface/tokenizers/issues/178", "id": 573661814, "node_id": "MDU6SXNzdWU1NzM2NjE4MTQ=", "number": 178, "title": "\"Too many open files\" when training a ByteLevel BPETokenizer", "user": {"login": "surisdi", "id": 15468123, "node_id": "MDQ6VXNlcjE1NDY4MTIz", "avatar_url": "https://avatars3.githubusercontent.com/u/15468123?v=4", "gravatar_id": "", "url": "https://api.github.com/users/surisdi", "html_url": "https://github.com/surisdi", "followers_url": "https://api.github.com/users/surisdi/followers", "following_url": "https://api.github.com/users/surisdi/following{/other_user}", "gists_url": "https://api.github.com/users/surisdi/gists{/gist_id}", "starred_url": "https://api.github.com/users/surisdi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/surisdi/subscriptions", "organizations_url": "https://api.github.com/users/surisdi/orgs", "repos_url": "https://api.github.com/users/surisdi/repos", "events_url": "https://api.github.com/users/surisdi/events{/privacy}", "received_events_url": "https://api.github.com/users/surisdi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-02T01:23:54Z", "updated_at": "2020-03-02T20:22:54Z", "closed_at": "2020-03-02T20:22:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, when training a ByteLevelBPETokenizer given a list of `.txt` files, using a code like the following one:\r\n```\r\nfrom tokenizers import ByteLevelBPETokenizer\r\ntokenizer = ByteLevelBPETokenizer()\r\ntokenizer.train(text_files)\r\n```\r\nif the `text_files` list of `.txt` files is larger than ~1000 files, an \"Exception: Too many open files (os error 24)\" error appears. A possible solution would be to join all files into a larger one, but sometimes it is not convenient. Is there anything that can be done from the `tokenizers` library? \r\n\r\nThanks!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/177", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/177/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/177/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/177/events", "html_url": "https://github.com/huggingface/tokenizers/issues/177", "id": 573331401, "node_id": "MDU6SXNzdWU1NzMzMzE0MDE=", "number": 177, "title": "Get most frequent tokens in pre-trained tokenizers", "user": {"login": "amir-rahnama", "id": 6009583, "node_id": "MDQ6VXNlcjYwMDk1ODM=", "avatar_url": "https://avatars1.githubusercontent.com/u/6009583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amir-rahnama", "html_url": "https://github.com/amir-rahnama", "followers_url": "https://api.github.com/users/amir-rahnama/followers", "following_url": "https://api.github.com/users/amir-rahnama/following{/other_user}", "gists_url": "https://api.github.com/users/amir-rahnama/gists{/gist_id}", "starred_url": "https://api.github.com/users/amir-rahnama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amir-rahnama/subscriptions", "organizations_url": "https://api.github.com/users/amir-rahnama/orgs", "repos_url": "https://api.github.com/users/amir-rahnama/repos", "events_url": "https://api.github.com/users/amir-rahnama/events{/privacy}", "received_events_url": "https://api.github.com/users/amir-rahnama/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-29T16:43:10Z", "updated_at": "2020-03-31T19:11:41Z", "closed_at": "2020-03-27T21:47:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to get the most frequent tokens in the vocabulary of pretrained tokenizers? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/176", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/176/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/176/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/176/events", "html_url": "https://github.com/huggingface/tokenizers/issues/176", "id": 572877132, "node_id": "MDU6SXNzdWU1NzI4NzcxMzI=", "number": 176, "title": "Installing Tokenizers without Internet connection", "user": {"login": "jordiae", "id": 2944532, "node_id": "MDQ6VXNlcjI5NDQ1MzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2944532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jordiae", "html_url": "https://github.com/jordiae", "followers_url": "https://api.github.com/users/jordiae/followers", "following_url": "https://api.github.com/users/jordiae/following{/other_user}", "gists_url": "https://api.github.com/users/jordiae/gists{/gist_id}", "starred_url": "https://api.github.com/users/jordiae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jordiae/subscriptions", "organizations_url": "https://api.github.com/users/jordiae/orgs", "repos_url": "https://api.github.com/users/jordiae/repos", "events_url": "https://api.github.com/users/jordiae/events{/privacy}", "received_events_url": "https://api.github.com/users/jordiae/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-02-28T16:59:42Z", "updated_at": "2020-03-04T15:12:58Z", "closed_at": "2020-03-04T15:12:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I'm trying to install the Tokenizers package in a cluster that doesn't have access to the Internet. Specifically, I'm interested in the Python bindings. So far, I have installed some dependencies, but when I run the setup.py script I get the following log:\r\n\r\n```\r\nrunning install\r\nrunning bdist_egg\r\nrunning egg_info\r\nwriting tokenizers.egg-info/PKG-INFO\r\nwriting dependency_links to tokenizers.egg-info/dependency_links.txt\r\nwriting top-level names to tokenizers.egg-info/top_level.txt\r\nreading manifest file 'tokenizers.egg-info/SOURCES.txt'\r\nreading manifest template 'MANIFEST.in'\r\nwarning: no previously-included files matching '*' found under directory 'tokenizers-lib/target'\r\nwriting manifest file 'tokenizers.egg-info/SOURCES.txt'\r\ninstalling library code to build/bdist.linux-ppc64le/egg\r\nrunning install_lib\r\nrunning build_py\r\nrunning build_ext\r\nrunning build_rust\r\n    Updating crates.io index\r\n```\r\n\r\nIt gets stuck since it has no connection but it's still trying to update the crates.io index.\r\n\r\nAny suggestions on how could I prevent the script from trying to connect to the internet or which dependencies should I install or configure locally?\r\n\r\nUnlike a [recent, related issue](https://github.com/huggingface/tokenizers/issues/173), in my case, it's a Power9 architecture so there is no wheel available and I must install directly from the tar file. \r\n\r\nTL;DR: How to install Tokenizers if `pip install tokenizers` is not an option because there is no connection to the Internet.\r\n\r\nMany thanks in advance.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/175", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/175/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/175/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/175/events", "html_url": "https://github.com/huggingface/tokenizers/issues/175", "id": 572154641, "node_id": "MDU6SXNzdWU1NzIxNTQ2NDE=", "number": 175, "title": "Can not add large custom vocab to RobertaTokenizerFast", "user": {"login": "DarshanPatel11", "id": 26226718, "node_id": "MDQ6VXNlcjI2MjI2NzE4", "avatar_url": "https://avatars2.githubusercontent.com/u/26226718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DarshanPatel11", "html_url": "https://github.com/DarshanPatel11", "followers_url": "https://api.github.com/users/DarshanPatel11/followers", "following_url": "https://api.github.com/users/DarshanPatel11/following{/other_user}", "gists_url": "https://api.github.com/users/DarshanPatel11/gists{/gist_id}", "starred_url": "https://api.github.com/users/DarshanPatel11/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DarshanPatel11/subscriptions", "organizations_url": "https://api.github.com/users/DarshanPatel11/orgs", "repos_url": "https://api.github.com/users/DarshanPatel11/repos", "events_url": "https://api.github.com/users/DarshanPatel11/events{/privacy}", "received_events_url": "https://api.github.com/users/DarshanPatel11/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-27T15:22:42Z", "updated_at": "2020-03-26T15:57:03Z", "closed_at": "2020-03-26T15:57:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have around **65K** tokens in my custom vocab file and I want to include in the DistilRoberta model to finetune it on my corpus.  I added it on RobertaTokenizer and it was added successfully, but the Tokenization process is very slow. Even with using multiprocessing it is showing that It will take around **200 hrs**.  So I tried to use RobertaTokenizerFast but couldn't add my custom vocab. It failed with an error as follows:\r\n```thread '<unnamed>' panicked at 'called `Result::unwrap()` on an `Err` value: CompiledTooBig(10485760)', /__w/tokenizers/tokenizers/tokenizers/src/tokenizer/mod.rs:672:22```\r\nSo is there any workaround to add my custom vocab and fast tokenization.\r\nAlso, I have a question that Roberta/GPT2 has a \"\u0120\" character in the vocab, so should I add it as a prefix to my vocab, or not?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/174", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/174/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/174/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/174/events", "html_url": "https://github.com/huggingface/tokenizers/issues/174", "id": 571857187, "node_id": "MDU6SXNzdWU1NzE4NTcxODc=", "number": 174, "title": "Boundary problems of BertWordPieceTokenizer (rust fatal error, max length)", "user": {"login": "wxupjack", "id": 23218717, "node_id": "MDQ6VXNlcjIzMjE4NzE3", "avatar_url": "https://avatars2.githubusercontent.com/u/23218717?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wxupjack", "html_url": "https://github.com/wxupjack", "followers_url": "https://api.github.com/users/wxupjack/followers", "following_url": "https://api.github.com/users/wxupjack/following{/other_user}", "gists_url": "https://api.github.com/users/wxupjack/gists{/gist_id}", "starred_url": "https://api.github.com/users/wxupjack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wxupjack/subscriptions", "organizations_url": "https://api.github.com/users/wxupjack/orgs", "repos_url": "https://api.github.com/users/wxupjack/repos", "events_url": "https://api.github.com/users/wxupjack/events{/privacy}", "received_events_url": "https://api.github.com/users/wxupjack/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-27T06:40:20Z", "updated_at": "2020-02-29T21:38:29Z", "closed_at": "2020-02-29T21:38:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "The simple code is here for reproducing:\r\n> tokenizers version:  0.5.2\r\n```python\r\nwith open('./vocab.txt', 'w') as f:\r\n    f.write('\\n'.join( ['[SEP]', '[UNK]', '[CLS]', '[MASK]', '[PAD]', ',', '.']))\r\n\r\nfrom tokenizers import BertWordPieceTokenizer\r\ntokenizer = BertWordPieceTokenizer('./vocab.txt')\r\ntokenizer.enable_truncation(max_length=128)\r\n\r\ntext1, text2 = ',' * 125, '.' * 130\r\nencodings = tokenizer.encode_batch([(text1, text2)])\r\n\r\nprint(len(encodings[0].ids))\r\n```\r\nHere, **max_length=128** (125=max_length-3 special tokens)\r\n- When `len(text2)<125`, no problems happens whatever the length of text1. The normal encoding should be like `[2, (part of) token indexes of text1 ..., 0,  (part of) token indexes of text2..., 0]` (0 for [SEP], 2 for [CLS]).\r\n- When `len(text2)>=125`, \r\n1.  `len(text1)<125`,  every case also work well.\r\n2.  `len(text1)=125`,  the error of Rust happens.\r\n\r\n> thread '<unnamed>' panicked at 'assertion failed: stride < max_len', /__w/tokenizers/tokenizers/tokenizers/src/tokenizer/encoding.rs:109:9\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\nfatal runtime error: failed to initiate panic, error 5\r\nAborted (core dumped)\r\n3. `len(text1)>125`, the length of encodings are out of 'max_length', maybe some asserts required here.\r\n\r\n\r\nIt maybe meaningless for the last case, so I ignored it at first. But when I found the fatal runtime error, I think this should be fixed just in case.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/173", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/173/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/173/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/173/events", "html_url": "https://github.com/huggingface/tokenizers/issues/173", "id": 571734259, "node_id": "MDU6SXNzdWU1NzE3MzQyNTk=", "number": 173, "title": "What's the best way to install a standalone version?", "user": {"login": "ylmeng", "id": 12010653, "node_id": "MDQ6VXNlcjEyMDEwNjUz", "avatar_url": "https://avatars0.githubusercontent.com/u/12010653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ylmeng", "html_url": "https://github.com/ylmeng", "followers_url": "https://api.github.com/users/ylmeng/followers", "following_url": "https://api.github.com/users/ylmeng/following{/other_user}", "gists_url": "https://api.github.com/users/ylmeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/ylmeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ylmeng/subscriptions", "organizations_url": "https://api.github.com/users/ylmeng/orgs", "repos_url": "https://api.github.com/users/ylmeng/repos", "events_url": "https://api.github.com/users/ylmeng/events{/privacy}", "received_events_url": "https://api.github.com/users/ylmeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-27T00:00:47Z", "updated_at": "2020-03-27T16:21:02Z", "closed_at": "2020-03-27T16:21:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I need to install the tokenizers on a high-security server, where internet access is impossible. The only thing I can do is to upload files there with scp.\r\n\r\nI managed to download (and upload) the src, set environment etc. However when I run \r\n\r\n> cd tokenizers/bindings/python\r\n> python setup.py install\r\n\r\nI always got \r\n\r\n> error: Can not find Rust compiler\r\n\r\nI did install Rust. So I suppose it is some configuration problem. But with the limited instructions it is very difficult to figure out what happened. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/170", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/170/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/170/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/170/events", "html_url": "https://github.com/huggingface/tokenizers/issues/170", "id": 570716226, "node_id": "MDU6SXNzdWU1NzA3MTYyMjY=", "number": 170, "title": "new release for node-binding and general question w.r.t. node releases", "user": {"login": "schiller-manuel", "id": 6340397, "node_id": "MDQ6VXNlcjYzNDAzOTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/6340397?v=4", "gravatar_id": "", "url": "https://api.github.com/users/schiller-manuel", "html_url": "https://github.com/schiller-manuel", "followers_url": "https://api.github.com/users/schiller-manuel/followers", "following_url": "https://api.github.com/users/schiller-manuel/following{/other_user}", "gists_url": "https://api.github.com/users/schiller-manuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/schiller-manuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/schiller-manuel/subscriptions", "organizations_url": "https://api.github.com/users/schiller-manuel/orgs", "repos_url": "https://api.github.com/users/schiller-manuel/repos", "events_url": "https://api.github.com/users/schiller-manuel/events{/privacy}", "received_events_url": "https://api.github.com/users/schiller-manuel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1872688216, "node_id": "MDU6TGFiZWwxODcyNjg4MjE2", "url": "https://api.github.com/repos/huggingface/tokenizers/labels/node", "name": "node", "color": "3ac95b", "default": false, "description": "Issue related to node bindings"}], "state": "closed", "locked": false, "assignee": {"login": "Pierrci", "id": 5020707, "node_id": "MDQ6VXNlcjUwMjA3MDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5020707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pierrci", "html_url": "https://github.com/Pierrci", "followers_url": "https://api.github.com/users/Pierrci/followers", "following_url": "https://api.github.com/users/Pierrci/following{/other_user}", "gists_url": "https://api.github.com/users/Pierrci/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pierrci/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pierrci/subscriptions", "organizations_url": "https://api.github.com/users/Pierrci/orgs", "repos_url": "https://api.github.com/users/Pierrci/repos", "events_url": "https://api.github.com/users/Pierrci/events{/privacy}", "received_events_url": "https://api.github.com/users/Pierrci/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "Pierrci", "id": 5020707, "node_id": "MDQ6VXNlcjUwMjA3MDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5020707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Pierrci", "html_url": "https://github.com/Pierrci", "followers_url": "https://api.github.com/users/Pierrci/followers", "following_url": "https://api.github.com/users/Pierrci/following{/other_user}", "gists_url": "https://api.github.com/users/Pierrci/gists{/gist_id}", "starred_url": "https://api.github.com/users/Pierrci/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Pierrci/subscriptions", "organizations_url": "https://api.github.com/users/Pierrci/orgs", "repos_url": "https://api.github.com/users/Pierrci/repos", "events_url": "https://api.github.com/users/Pierrci/events{/privacy}", "received_events_url": "https://api.github.com/users/Pierrci/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-02-25T17:14:08Z", "updated_at": "2020-02-27T20:32:18Z", "closed_at": "2020-02-27T20:32:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "The last release (0.4.1) of node binding is from 14 days ago, so it does not contain the latest fixes. Could you please release a new version for node as well?\r\n\r\nWhat is the versioning scheme used for tokenizers? The rust lib is at 0.7, the python binding at 0.5.2.\r\n\r\nIdeally, a new release of both node and python bindings should be released when the rust library is updated, right?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/168", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/168/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/168/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/168/events", "html_url": "https://github.com/huggingface/tokenizers/issues/168", "id": 570173398, "node_id": "MDU6SXNzdWU1NzAxNzMzOTg=", "number": 168, "title": "Exception: WordPiece error: Missing [UNK] token from the vocabulary", "user": {"login": "dizzySummer", "id": 49494825, "node_id": "MDQ6VXNlcjQ5NDk0ODI1", "avatar_url": "https://avatars1.githubusercontent.com/u/49494825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dizzySummer", "html_url": "https://github.com/dizzySummer", "followers_url": "https://api.github.com/users/dizzySummer/followers", "following_url": "https://api.github.com/users/dizzySummer/following{/other_user}", "gists_url": "https://api.github.com/users/dizzySummer/gists{/gist_id}", "starred_url": "https://api.github.com/users/dizzySummer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dizzySummer/subscriptions", "organizations_url": "https://api.github.com/users/dizzySummer/orgs", "repos_url": "https://api.github.com/users/dizzySummer/repos", "events_url": "https://api.github.com/users/dizzySummer/events{/privacy}", "received_events_url": "https://api.github.com/users/dizzySummer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-24T21:59:42Z", "updated_at": "2020-02-24T22:29:52Z", "closed_at": "2020-02-24T22:14:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! \r\n\r\nAs to the above-mentioned issue, I 've got this error message:\r\n\r\nhttps://github.com/dizzySummer/temp/blob/master/errorMessage#L1-L30\r\n\r\nAny comments?\r\n\r\nThank you!\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/166", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/166/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/166/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/166/events", "html_url": "https://github.com/huggingface/tokenizers/issues/166", "id": 570013353, "node_id": "MDU6SXNzdWU1NzAwMTMzNTM=", "number": 166, "title": "Creating a ByteLevelBPETokenizer from own corpora, special tokens problem", "user": {"login": "simonefrancia", "id": 7140210, "node_id": "MDQ6VXNlcjcxNDAyMTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/7140210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simonefrancia", "html_url": "https://github.com/simonefrancia", "followers_url": "https://api.github.com/users/simonefrancia/followers", "following_url": "https://api.github.com/users/simonefrancia/following{/other_user}", "gists_url": "https://api.github.com/users/simonefrancia/gists{/gist_id}", "starred_url": "https://api.github.com/users/simonefrancia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simonefrancia/subscriptions", "organizations_url": "https://api.github.com/users/simonefrancia/orgs", "repos_url": "https://api.github.com/users/simonefrancia/repos", "events_url": "https://api.github.com/users/simonefrancia/events{/privacy}", "received_events_url": "https://api.github.com/users/simonefrancia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-24T17:40:52Z", "updated_at": "2020-02-24T22:48:16Z", "closed_at": "2020-02-24T22:48:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI would like to create a GPT2 tokenizer based on my own data.\r\n\r\nMy data are formatted in a certain way and I would like to recreate the structure; for this reason I want to add a special token <nl> instead of \\n because I want GPT2 to learn also when it has to put newlines inside text. This is my code:\r\n\r\n```\r\n# Initialize a tokenizer\r\ntokenizer = ByteLevelBPETokenizer(add_prefix_space=False)\r\n\r\n# Customize training\r\ntokenizer.train(files=paths, \r\n                vocab_size=8_000, \r\n                min_frequency=2, \r\n                special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\", \"<nl>\"])\r\n\r\nprint(tokenizer.encode(\"Training <nl> hi ciao <nl> \").tokens)\r\ntokenizer.save(\"./my-gpt2/\", name=\"my\")\r\n```\r\nOutput of last code line is:\r\n```\r\n['T', 'ra', 'ining', '\u0120', '<nl>', '\u0120hi', '\u0120c', 'ia', 'o', '\u0120', '<nl>', '\u0120']\r\n```\r\nMy doubt is that <nl> is encoded ok but it has the special token before as single token and I don't understand if it's ok.\r\n\r\nAfter I create my own tokenizer, I would like to use it as input tokenizer for language model training for the script: https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py\r\nwith the --tokenizer_name parameter pointing at my own tokenizer.\r\nThanks\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/huggingface/tokenizers/issues/163", "repository_url": "https://api.github.com/repos/huggingface/tokenizers", "labels_url": "https://api.github.com/repos/huggingface/tokenizers/issues/163/labels{/name}", "comments_url": "https://api.github.com/repos/huggingface/tokenizers/issues/163/comments", "events_url": "https://api.github.com/repos/huggingface/tokenizers/issues/163/events", "html_url": "https://github.com/huggingface/tokenizers/issues/163", "id": 569422131, "node_id": "MDU6SXNzdWU1Njk0MjIxMzE=", "number": 163, "title": "Allow handle a large number of files for Tokenizer training", "user": {"login": "andy-yangz", "id": 23011317, "node_id": "MDQ6VXNlcjIzMDExMzE3", "avatar_url": "https://avatars3.githubusercontent.com/u/23011317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andy-yangz", "html_url": "https://github.com/andy-yangz", "followers_url": "https://api.github.com/users/andy-yangz/followers", "following_url": "https://api.github.com/users/andy-yangz/following{/other_user}", "gists_url": "https://api.github.com/users/andy-yangz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andy-yangz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andy-yangz/subscriptions", "organizations_url": "https://api.github.com/users/andy-yangz/orgs", "repos_url": "https://api.github.com/users/andy-yangz/repos", "events_url": "https://api.github.com/users/andy-yangz/events{/privacy}", "received_events_url": "https://api.github.com/users/andy-yangz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-23T01:54:07Z", "updated_at": "2020-02-26T15:48:10Z", "closed_at": "2020-02-26T15:48:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently, I am handling a dataset with more than 10000 txt files. When I use \"**/*.txt\" I get the following error.\r\n![image](https://user-images.githubusercontent.com/23011317/75102123-9875fc00-5621-11ea-8b94-6f8ad31179ef.png)\r\nAlthough I know it will be quite simple to just handle it by cat them together, I still wonder can there be an improvement to handle this directly.", "performed_via_github_app": null, "score": 1.0}]}