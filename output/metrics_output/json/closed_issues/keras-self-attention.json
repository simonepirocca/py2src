{"total_count": 41, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/43", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/43/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/43/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/43/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/43", "id": 671047127, "node_id": "MDU6SXNzdWU2NzEwNDcxMjc=", "number": 43, "title": "reference for multiplicative attention", "user": {"login": "indi297", "id": 69086905, "node_id": "MDQ6VXNlcjY5MDg2OTA1", "avatar_url": "https://avatars1.githubusercontent.com/u/69086905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/indi297", "html_url": "https://github.com/indi297", "followers_url": "https://api.github.com/users/indi297/followers", "following_url": "https://api.github.com/users/indi297/following{/other_user}", "gists_url": "https://api.github.com/users/indi297/gists{/gist_id}", "starred_url": "https://api.github.com/users/indi297/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/indi297/subscriptions", "organizations_url": "https://api.github.com/users/indi297/orgs", "repos_url": "https://api.github.com/users/indi297/repos", "events_url": "https://api.github.com/users/indi297/events{/privacy}", "received_events_url": "https://api.github.com/users/indi297/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-01T17:29:06Z", "updated_at": "2020-08-10T03:01:34Z", "closed_at": "2020-08-10T03:01:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Sir/Ma'am\r\n\r\nKindly point me to some reference for the multiplicative self-attention mechanism.\r\n\r\nRegards", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/41", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/41/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/41/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/41/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/41", "id": 641463498, "node_id": "MDU6SXNzdWU2NDE0NjM0OTg=", "number": 41, "title": "Masking implementation", "user": {"login": "tomasmenezes", "id": 22925356, "node_id": "MDQ6VXNlcjIyOTI1MzU2", "avatar_url": "https://avatars0.githubusercontent.com/u/22925356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomasmenezes", "html_url": "https://github.com/tomasmenezes", "followers_url": "https://api.github.com/users/tomasmenezes/followers", "following_url": "https://api.github.com/users/tomasmenezes/following{/other_user}", "gists_url": "https://api.github.com/users/tomasmenezes/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomasmenezes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomasmenezes/subscriptions", "organizations_url": "https://api.github.com/users/tomasmenezes/orgs", "repos_url": "https://api.github.com/users/tomasmenezes/repos", "events_url": "https://api.github.com/users/tomasmenezes/events{/privacy}", "received_events_url": "https://api.github.com/users/tomasmenezes/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-18T18:30:50Z", "updated_at": "2020-06-28T05:04:39Z", "closed_at": "2020-06-28T05:04:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi @CyberZHG, I'm using self-attention over an RNN for a classification problem, however I'm a bit confused with the masking implementation and their differences among the provided attention types. I apologize for the size of the post in advance.\r\n\r\nTo test the masking, I created a placeholder tensor to represent the output hidden states from an RNN with T=6 timesteps [t0,...,t5] and D=3 units, where timesteps t2, t4 and t5 are masked:\r\n```python\r\nh_states = tf.convert_to_tensor(np.array([[[0.5,0.2,0.1],[0.4,0.9,0.3],[-1,-1,-1],[0.1,0.2,0.1], [-1,-1,-1], [-1,-1,-1]]]), dtype='float32')\r\nmasked_states = Masking(mask_value=-1)(h_states)\r\n```\r\n\r\n### SeqSelfAttention\r\n`SeqSelfAttention(return_attention=True)(masked_states)`\r\nWhen calling the additive or dot attention, I was surprised to find that only a_{i,j}, with i,j = [2,4,5] in the [TxT] attention matrix were masked:\r\n```python\r\nSeqSelfAttention(return_attention=True)(masked_states)\r\n[<tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=\r\n array([[[0.13978598, 0.16256869, 0.06499083],\r\n         [0.1412907 , 0.16534805, 0.06593135],\r\n         [0.32337117, 0.3761466 , 0.15032762],\r\n         [0.14026345, 0.163282  , 0.06523413],\r\n         [0.32337117, 0.3761466 , 0.15032762],\r\n         [0.32337117, 0.3761466 , 0.15032762]]], dtype=float32)>,\r\n <tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=\r\n array([[[0.159832  , 0.10862342, 0.18911283, 0.16420609, 0.18911283, 0.18911283],\r\n         [0.16049388, 0.11161783, 0.18797404, 0.16396616, 0.18797404, 0.18797404],\r\n         [0.36969936, 0.25163805, 0.        , 0.37866256, 0.        , 0.        ],\r\n         [0.16022852, 0.10937916, 0.18880568, 0.16397531, 0.18880568, 0.18880568],\r\n         [0.36969936, 0.25163805, 0.        , 0.37866256, 0.        , 0.        ],\r\n         [0.36969936, 0.25163805, 0.        , 0.37866256, 0.        , 0.        ]]], dtype=float32)>]\r\n```\r\n- **Q1:** Shouldn't the [2,4,5] rows and columns be masked entirely instead since the values result from alignments with masked timesteps? \r\n\r\n### SeqWeightedAttention\r\n`SeqWeightedAttention` seems to mask the padding timesteps completely:\r\n```python\r\nSeqWeightedAttention(return_attention=True)(masked_states)\r\n[<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.33272028, 0.43565503, 0.16733001]], dtype=float32)>,\r\n <tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[0.32931313, 0.33665004, 0.        , 0.33403683, 0.        , 0.        ]], dtype=float32)>]\r\n```\r\n\r\n### ScaledDotProductAttention\r\n`ScaledDotProductAttention` expectedly returned similar values to Keras' implementation `tf.keras.layers.Attention(use_scale=True)`, except for the existing masked timestep values:\r\n```python\r\nScaledDotProductAttention(return_attention=True)(masked_states)\r\n[<tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=\r\n array([[[0.34341848, 0.4522895 , 0.17208272],\r\n         [0.3484643 , 0.5025628 , 0.18644652],\r\n         [0.33333334, 0.43333334, 0.16666667],\r\n         [0.33703578, 0.4488316 , 0.17109475],\r\n         [0.33333334, 0.43333334, 0.16666667],\r\n         [0.33333334, 0.43333334, 0.16666667]]], dtype=float32)>,\r\n <tf.Tensor: shape=(1, 6, 6), dtype=float32, numpy=\r\n array([[[0.33823597, 0.3604136 , 0.        , 0.3013504 , 0.        , 0.        ],\r\n         [0.29698637, 0.43223262, 0.        , 0.27078095, 0.        , 0.        ],\r\n         [0.33333334, 0.33333334, 0.        , 0.33333334, 0.        , 0.        ],\r\n         [0.32598415, 0.3554737 , 0.        , 0.31854212, 0.        , 0.        ],\r\n         [0.33333334, 0.33333334, 0.        , 0.33333334, 0.        , 0.        ],\r\n         [0.33333334, 0.33333334, 0.        , 0.33333334, 0.        , 0.        ]]], dtype=float32)>]\r\n```\r\nHere the mask propagates over the columns but not the rows.\r\n\r\n### Keras Dot Attention\r\nFinally, even though its implementation is supposedly not supported for RNN (as per code documentation), the final result is more aligned with my expected behavior, where the values for the masked timesteps are removed:\r\n```python\r\nAttention(use_scale=True)([masked_states,masked_states])\r\n<tf.Tensor: shape=(1, 6, 3), dtype=float32, numpy=\r\narray([[[0.35038543, 0.46623248, 0.17606643],\r\n        [0.35869   , 0.5558893 , 0.20168266],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.3397184 , 0.46044892, 0.17441398],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ]]], dtype=float32)>\r\n```\r\n\r\n- **Q2:** Is there a need to multiply the output of `SeqSelfAttention` or `ScaledDotAttention` by the initial mask before summing over the timestep dimension to obtain a final vector?\r\n---\r\n[edit: question wording, example removed]", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/40", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/40/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/40/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/40/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/40", "id": 611279136, "node_id": "MDU6SXNzdWU2MTEyNzkxMzY=", "number": 40, "title": "Supporting convolutional LSTM? ", "user": {"login": "adanacademic", "id": 34600067, "node_id": "MDQ6VXNlcjM0NjAwMDY3", "avatar_url": "https://avatars0.githubusercontent.com/u/34600067?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adanacademic", "html_url": "https://github.com/adanacademic", "followers_url": "https://api.github.com/users/adanacademic/followers", "following_url": "https://api.github.com/users/adanacademic/following{/other_user}", "gists_url": "https://api.github.com/users/adanacademic/gists{/gist_id}", "starred_url": "https://api.github.com/users/adanacademic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adanacademic/subscriptions", "organizations_url": "https://api.github.com/users/adanacademic/orgs", "repos_url": "https://api.github.com/users/adanacademic/repos", "events_url": "https://api.github.com/users/adanacademic/events{/privacy}", "received_events_url": "https://api.github.com/users/adanacademic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-02T21:38:50Z", "updated_at": "2020-05-10T02:43:03Z", "closed_at": "2020-05-10T02:43:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Does it support ConvLSTM? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/39", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/39/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/39/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/39/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/39", "id": 606221236, "node_id": "MDU6SXNzdWU2MDYyMjEyMzY=", "number": 39, "title": "tf.keras.layers.Attention\uff1f", "user": {"login": "Jie-Yuan", "id": 20265321, "node_id": "MDQ6VXNlcjIwMjY1MzIx", "avatar_url": "https://avatars2.githubusercontent.com/u/20265321?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jie-Yuan", "html_url": "https://github.com/Jie-Yuan", "followers_url": "https://api.github.com/users/Jie-Yuan/followers", "following_url": "https://api.github.com/users/Jie-Yuan/following{/other_user}", "gists_url": "https://api.github.com/users/Jie-Yuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jie-Yuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jie-Yuan/subscriptions", "organizations_url": "https://api.github.com/users/Jie-Yuan/orgs", "repos_url": "https://api.github.com/users/Jie-Yuan/repos", "events_url": "https://api.github.com/users/Jie-Yuan/events{/privacy}", "received_events_url": "https://api.github.com/users/Jie-Yuan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-24T10:46:01Z", "updated_at": "2020-05-01T12:19:44Z", "closed_at": "2020-05-01T12:19:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/38", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/38/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/38/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/38/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/38", "id": 589660537, "node_id": "MDU6SXNzdWU1ODk2NjA1Mzc=", "number": 38, "title": "Compatibility with Tensorflow 2.0", "user": {"login": "mohamedScikitLearn", "id": 19480658, "node_id": "MDQ6VXNlcjE5NDgwNjU4", "avatar_url": "https://avatars0.githubusercontent.com/u/19480658?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mohamedScikitLearn", "html_url": "https://github.com/mohamedScikitLearn", "followers_url": "https://api.github.com/users/mohamedScikitLearn/followers", "following_url": "https://api.github.com/users/mohamedScikitLearn/following{/other_user}", "gists_url": "https://api.github.com/users/mohamedScikitLearn/gists{/gist_id}", "starred_url": "https://api.github.com/users/mohamedScikitLearn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mohamedScikitLearn/subscriptions", "organizations_url": "https://api.github.com/users/mohamedScikitLearn/orgs", "repos_url": "https://api.github.com/users/mohamedScikitLearn/repos", "events_url": "https://api.github.com/users/mohamedScikitLearn/events{/privacy}", "received_events_url": "https://api.github.com/users/mohamedScikitLearn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-03-28T21:11:02Z", "updated_at": "2020-07-16T01:56:30Z", "closed_at": "2020-04-10T00:20:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to create a model using `keras-self-attention`on Google colab, and since the default Tensorflow version is 2.0 now, this error prompt : \r\n```\r\nmodel = models.Sequential()\r\nmodel.add( Embedding(max_features, 32))\r\nmodel.add(Bidirectional( LSTM(32, return_sequences=True)))\r\n# adding an attention layer\r\nmodel.add(SeqWeightedAttention())\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py in _get_default_graph()\r\n     65     try:\r\n---> 66         return tf.get_default_graph()\r\n     67     except AttributeError:\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'get_default_graph'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n4 frames\r\n<ipython-input-7-9c4e625938a2> in <module>()\r\n      3 model.add(Bidirectional( LSTM(32, return_sequences=True)))\r\n      4 # adding an attention layer\r\n----> 5 model.add(SeqWeightedAttention())\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_self_attention/seq_weighted_attention.py in __init__(self, use_bias, return_attention, **kwargs)\r\n     10 \r\n     11     def __init__(self, use_bias=True, return_attention=False, **kwargs):\r\n---> 12         super(SeqWeightedAttention, self).__init__(**kwargs)\r\n     13         self.supports_masking = True\r\n     14         self.use_bias = use_bias\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py in __init__(self, **kwargs)\r\n    130         if not name:\r\n    131             prefix = self.__class__.__name__\r\n--> 132             name = _to_snake_case(prefix) + '_' + str(K.get_uid(prefix))\r\n    133         self.name = name\r\n    134 \r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py in get_uid(prefix)\r\n     84     \"\"\"\r\n     85     global _GRAPH_UID_DICTS\r\n---> 86     graph = _get_default_graph()\r\n     87     if graph not in _GRAPH_UID_DICTS:\r\n     88         _GRAPH_UID_DICTS[graph] = defaultdict(int)\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py in _get_default_graph()\r\n     67     except AttributeError:\r\n     68         raise RuntimeError(\r\n---> 69             'It looks like you are trying to use '\r\n     70             'a version of multi-backend Keras that '\r\n     71             'does not support TensorFlow 2.0. We recommend '\r\n\r\n**RuntimeError: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14.**\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/37", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/37/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/37/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/37/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/37", "id": 575292679, "node_id": "MDU6SXNzdWU1NzUyOTI2Nzk=", "number": 37, "title": "ScaledDotProductAttention not returning attention properly.", "user": {"login": "deepettas", "id": 20043465, "node_id": "MDQ6VXNlcjIwMDQzNDY1", "avatar_url": "https://avatars2.githubusercontent.com/u/20043465?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepettas", "html_url": "https://github.com/deepettas", "followers_url": "https://api.github.com/users/deepettas/followers", "following_url": "https://api.github.com/users/deepettas/following{/other_user}", "gists_url": "https://api.github.com/users/deepettas/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepettas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepettas/subscriptions", "organizations_url": "https://api.github.com/users/deepettas/orgs", "repos_url": "https://api.github.com/users/deepettas/repos", "events_url": "https://api.github.com/users/deepettas/events{/privacy}", "received_events_url": "https://api.github.com/users/deepettas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-04T10:29:36Z", "updated_at": "2020-03-11T13:24:58Z", "closed_at": "2020-03-11T13:24:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have used your MultiheadAttention model on a complex input, outputting attention/self attention with success.\r\nAfter that, I have tried experimenting with the ScaledDotProductAttention model that is used from Multihead with the aim to increase interpretability and decrease inference/training times.\r\n\r\nWhile building a simple example such as the one bellow, I was unable to extract a correct attention vector even after training the model. \r\n**The attention vector was always:  [0.99, 0.99, 0.99, .... , 0.99] for single dimentional input.**\r\n\r\nAny clues to what I am doing that prevents the model outputting a correct attention vector?\r\n\r\nReference code bellow:\r\n\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\nimport cProfile\r\nimport keras\r\nimport keras.backend as K\r\nfrom keras_self_attention import ScaledDotProductAttention\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport keras\r\n\r\n\r\ninput_query = keras.layers.Input(\r\n    shape=(9,),\r\n    name='Input-Q',\r\n)\r\ninput_key = keras.layers.Input(\r\n    shape=(1, ),\r\n    name='Input-K',\r\n)\r\ninput_value = keras.layers.Input(\r\n    shape=(1, ),\r\n    name='Input-V',\r\n)\r\nemb = keras.layers.Embedding(input_dim=20, output_dim=10)\r\n\r\nquery = emb(input_query)\r\nkey = emb(input_key)\r\nvalue = emb(input_value)\r\n\r\natt_layer, a = ScaledDotProductAttention(\r\n    name='ScaledDotProductAttention',\r\n    return_attention= True\r\n)([query, key, value])\r\n\r\natt_layer = keras.layers.GlobalAveragePooling1D(name='AttAvg')(att_layer)\r\n\r\nquery = keras.layers.GlobalAveragePooling1D()(query)\r\n\r\nquery = keras.layers.Reshape(target_shape=(1, 10))(query)\r\n\r\natt_layer = keras.layers.Reshape(target_shape=(1, 10))(att_layer)\r\n\r\ny = keras.layers.concatenate([att_layer, query])\r\ny = keras.layers.Reshape(target_shape=(20,))(y)\r\ny = keras.layers.Dense(300, activation='relu')(y)\r\n\r\ny = keras.layers.Dense(1, activation='sigmoid', name='output_sigmoid')(y)\r\n\r\n\r\nmodel = keras.models.Model(\r\n    inputs=[input_query, input_key, input_value], outputs=y)\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss='binary_crossentropy',\r\n    metrics=['accuracy'],\r\n)\r\nmodel.summary()\r\n```\r\n\r\n    Attention is:  Tensor(\"ScaledDotProductAttention_2/truediv_1:0\", shape=(?, 9, 1), dtype=float32) Tensor(\"ScaledDotProductAttention_2/MatMul_1:0\", shape=(?, 9, 10), dtype=float32) Tensor(\"embedding_4_2/GatherV2:0\", shape=(?, 1, 10), dtype=float32)\r\n    __________________________________________________________________________________________________\r\n    Layer (type)                    Output Shape         Param #     Connected to                     \r\n    ==================================================================================================\r\n    Input-Q (InputLayer)            (None, 9)            0                                            \r\n    __________________________________________________________________________________________________\r\n    Input-K (InputLayer)            (None, 1)            0                                            \r\n    __________________________________________________________________________________________________\r\n    Input-V (InputLayer)            (None, 1)            0                                            \r\n    __________________________________________________________________________________________________\r\n    embedding_4 (Embedding)         multiple             200         Input-Q[0][0]                    \r\n                                                                     Input-K[0][0]                    \r\n                                                                     Input-V[0][0]                    \r\n    __________________________________________________________________________________________________\r\n    ScaledDotProductAttention (Scal [(None, 9, 10), (Non 0           embedding_4[0][0]                \r\n                                                                     embedding_4[1][0]                \r\n                                                                     embedding_4[2][0]                \r\n    __________________________________________________________________________________________________\r\n    AttAvg (GlobalAveragePooling1D) (None, 10)           0           ScaledDotProductAttention[0][0]  \r\n    __________________________________________________________________________________________________\r\n    global_average_pooling1d_3 (Glo (None, 10)           0           embedding_4[0][0]                \r\n    __________________________________________________________________________________________________\r\n    reshape_7 (Reshape)             (None, 1, 10)        0           AttAvg[0][0]                     \r\n    __________________________________________________________________________________________________\r\n    reshape_6 (Reshape)             (None, 1, 10)        0           global_average_pooling1d_3[0][0] \r\n    __________________________________________________________________________________________________\r\n    concatenate_3 (Concatenate)     (None, 1, 20)        0           reshape_7[0][0]                  \r\n                                                                     reshape_6[0][0]                  \r\n    __________________________________________________________________________________________________\r\n    reshape_8 (Reshape)             (None, 20)           0           concatenate_3[0][0]              \r\n    __________________________________________________________________________________________________\r\n    dense_2 (Dense)                 (None, 300)          6300        reshape_8[0][0]                  \r\n    __________________________________________________________________________________________________\r\n    output_sigmoid (Dense)          (None, 1)            301         dense_2[0][0]                    \r\n    ==================================================================================================\r\n    Total params: 6,801\r\n    Trainable params: 6,801\r\n    Non-trainable params: 0\r\n    __________________________________________________________________________________________________\r\n\r\n\r\n# Data generation\r\n\r\n\r\n```python\r\n# Data Generation.\r\n\r\nfrom random import randint\r\ndef get_q(m,n):\r\n    \r\n    q = []\r\n    \r\n    for i in range(n):\r\n        a = []\r\n        k = randint(1,10)\r\n        for i in range(m):\r\n            a.append(k)\r\n        q.append(a)\r\n    return q\r\n\r\ndef get_k(n):\r\n    l = randint(1,10)\r\n    k = []\r\n    for i in range(n):\r\n        k.append((l))\r\n    \r\n    return k\r\n\r\n\r\n\r\ndef potato_gun(iterations):\r\n    i_q = np.array([get_q(1,9) for i in range(0,iterations)])\r\n    i_k = np.array([get_q(1,1) for i in range(0,iterations)])\r\n\r\n\r\n    i_y = []\r\n    for q, v in zip(i_q, i_k):\r\n        y = 0\r\n\r\n        for row in q:\r\n            if np.array_equal(v[0],row):\r\n                y = 1\r\n        i_y.append(y)\r\n    \r\n    return [np.reshape(i_q, newshape=(iterations, 9)), \r\n            np.reshape(i_k, newshape=(iterations,)), \r\n            np.reshape(i_k, newshape=(iterations,))], i_y\r\n\r\n\r\ndummy_dataset = potato_gun(240000)\r\nx_train, y_train = (dummy_dataset[0],dummy_dataset[1])\r\ndummy_dataset = potato_gun(80000)\r\nx_test, y_test = (dummy_dataset[0],dummy_dataset[1])\r\n\r\n```\r\n\r\n# Simple rule: \r\n## If input[1] exists in the input[0] , y = 1, else y = 0\r\n\r\n\r\n\r\n```python\r\nidx = 5\r\nprint(f'If {x_train[0][idx]} contains {x_train[1][idx]}, Then y = {y_train[idx]}')\r\n```\r\n\r\n    If [ 9  8  9  6  4  2  6  7 10] contains [7], Then y = 1\r\n\r\n\r\n# Training\r\n\r\n\r\n```python\r\nmodel.fit(x=x_train,\r\n      y=y_train,\r\n      batch_size=32,\r\n      epochs=10,\r\n      verbose=1,\r\n      validation_data=(x_test, y_test),\r\n      shuffle=True,\r\n      class_weight=None,\r\n      sample_weight=None\r\n          )\r\n```\r\n\r\n    Train on 240000 samples, validate on 80000 samples\r\n    Epoch 1/10\r\n    240000/240000 [==============================] - 11s 47us/step - loss: 0.4857 - acc: 0.7541 - val_loss: 0.3153 - val_acc: 0.8487\r\n    Epoch 2/10\r\n    240000/240000 [==============================] - 11s 45us/step - loss: 0.1943 - acc: 0.9161 - val_loss: 0.0770 - val_acc: 0.9788\r\n    Epoch 3/10\r\n    240000/240000 [==============================] - 11s 46us/step - loss: 0.0113 - acc: 0.9986 - val_loss: 3.9026e-04 - val_acc: 1.0000\r\n    Epoch 4/10\r\n    240000/240000 [==============================] - 11s 46us/step - loss: 2.3738e-04 - acc: 1.0000 - val_loss: 9.9840e-05 - val_acc: 1.0000\r\n    Epoch 5/10\r\n    240000/240000 [==============================] - 12s 50us/step - loss: 6.5733e-05 - acc: 1.0000 - val_loss: 2.9516e-05 - val_acc: 1.0000\r\n    Epoch 6/10\r\n    240000/240000 [==============================] - 13s 56us/step - loss: 3.2543e-05 - acc: 1.0000 - val_loss: 1.5050e-05 - val_acc: 1.0000\r\n    Epoch 7/10\r\n    240000/240000 [==============================] - 13s 56us/step - loss: 2.5813e-06 - acc: 1.0000 - val_loss: 1.8419e-06 - val_acc: 1.0000\r\n    Epoch 8/10\r\n    240000/240000 [==============================] - 12s 50us/step - loss: 5.1268e-05 - acc: 1.0000 - val_loss: 2.1687e-06 - val_acc: 1.0000\r\n    Epoch 9/10\r\n    240000/240000 [==============================] - 12s 49us/step - loss: 5.6301e-07 - acc: 1.0000 - val_loss: 3.2817e-07 - val_acc: 1.0000\r\n    Epoch 10/10\r\n    240000/240000 [==============================] - 12s 51us/step - loss: 7.6280e-05 - acc: 1.0000 - val_loss: 9.8148e-07 - val_acc: 1.0000\r\n\r\n\r\n\r\n\r\n\r\n    <keras.callbacks.History at 0x7fa5630a7630>\r\n\r\n\r\n\r\n# Extracting the submodel & confirming the output shapes:\r\n\r\n\r\n```python\r\ntarget_layer = model.get_layer('ScaledDotProductAttention')\r\nlayer_output = target_layer.output\r\nfrom keras.models import Model\r\n\r\nsubmodel_emb = Model(inputs=model.input, outputs=layer_output)\r\nprint(submodel_emb.summary())\r\nprint(f'Shapes: \\n Values: {submodel_emb.output_shape[0]}, Attention: {submodel_emb.output_shape[1]}')\r\n```\r\n\r\n    __________________________________________________________________________________________________\r\n    Layer (type)                    Output Shape         Param #     Connected to                     \r\n    ==================================================================================================\r\n    Input-Q (InputLayer)            (None, 9)            0                                            \r\n    __________________________________________________________________________________________________\r\n    Input-K (InputLayer)            (None, 1)            0                                            \r\n    __________________________________________________________________________________________________\r\n    Input-V (InputLayer)            (None, 1)            0                                            \r\n    __________________________________________________________________________________________________\r\n    embedding_3 (Embedding)         multiple             200         Input-Q[0][0]                    \r\n                                                                     Input-K[0][0]                    \r\n                                                                     Input-V[0][0]                    \r\n    __________________________________________________________________________________________________\r\n    ScaledDotProductAttention (Scal [(None, 9, 10), (Non 0           embedding_3[0][0]                \r\n                                                                     embedding_3[1][0]                \r\n                                                                     embedding_3[2][0]                \r\n    ==================================================================================================\r\n    Total params: 200\r\n    Trainable params: 200\r\n    Non-trainable params: 0\r\n    __________________________________________________________________________________________________\r\n    None\r\n    Shapes: \r\n     Values: (None, 9, 10), Attention: (None, 9, 1)\r\n\r\n\r\n# Expected behaviour: \r\n### Model should export an attention vector containing attention weights summing up to 1 during .predict() \r\n\r\n# Observed behaviour:\r\n# With input array [2,2,2,2,2,2,2,2,1] the attention should be directed in the last position of the input.\r\n\r\n## Contrary to the expectations, the attention vector is [0.99, 0.99, 0.99, .... , 0.99]\r\n\r\n\r\n```python\r\nidx = 5\r\nvalues, attention = submodel_emb.predict(\r\n    (\r\n        [np.array([[2]*8 + [1]]),\r\n        np.array([1,]),\r\n        np.array([1])]\r\n    )\r\n)\r\nprint(attention)\r\n```\r\n\r\n    [[[0.9999999]\r\n      [0.9999999]\r\n      [0.9999999]\r\n      [0.9999999]\r\n      [0.9999999]\r\n      [0.9999999]\r\n      [0.9999999]\r\n      [0.9999999]\r\n      [0.9999999]]]\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/36", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/36/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/36/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/36/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/36", "id": 554936297, "node_id": "MDU6SXNzdWU1NTQ5MzYyOTc=", "number": 36, "title": "IndexError, tuple index out of range", "user": {"login": "elsheikh21", "id": 26064109, "node_id": "MDQ6VXNlcjI2MDY0MTA5", "avatar_url": "https://avatars1.githubusercontent.com/u/26064109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elsheikh21", "html_url": "https://github.com/elsheikh21", "followers_url": "https://api.github.com/users/elsheikh21/followers", "following_url": "https://api.github.com/users/elsheikh21/following{/other_user}", "gists_url": "https://api.github.com/users/elsheikh21/gists{/gist_id}", "starred_url": "https://api.github.com/users/elsheikh21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elsheikh21/subscriptions", "organizations_url": "https://api.github.com/users/elsheikh21/orgs", "repos_url": "https://api.github.com/users/elsheikh21/repos", "events_url": "https://api.github.com/users/elsheikh21/events{/privacy}", "received_events_url": "https://api.github.com/users/elsheikh21/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-24T20:22:59Z", "updated_at": "2020-02-01T02:26:50Z", "closed_at": "2020-02-01T02:26:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "```{python}\r\ndef multitask_seq2seq_model(output_size, pos_vocab_size,\r\n                            lex_vocab_size, config_params,\r\n                            visualize=False, plot=False):\r\n    hidden_size = int(config_params['hidden_size'])\r\n    batch_size = int(config_params['batch_size'])\r\n    embedding_size = 768\r\n    max_seq_len = 512\r\n    in_id = Input(shape=(max_seq_len,), name=\"input_ids\")\r\n    in_mask = Input(shape=(max_seq_len,), name=\"input_masks\")\r\n    in_segment = Input(shape=(max_seq_len,), name=\"segment_ids\")\r\n    bert_inputs_ = [in_id, in_mask, in_segment]\r\n\r\n    bert_output_ = BertEmbeddingLayer(n_fine_tune_layers=3,\r\n                                      pooling=\"mean\")(bert_inputs_)\r\n    bert_output = Reshape((max_seq_len, embedding_size))(bert_output_)\r\n\r\n    input_mask = Input(shape=(None, output_size),\r\n                       batch_size=batch_size, name='Candidate_Synsets_Mask')\r\n\r\n    bert_inputs_.append(input_mask)\r\n\r\n    bilstm, forward_h, _, backward_h, _ = Bidirectional(LSTM(hidden_size,\r\n                                                             return_sequences=True,\r\n                                                             return_state=True,\r\n                                                             dropout=0.2,\r\n                                                             recurrent_dropout=0.2,\r\n                                                             input_shape=(None, None, embedding_size)\r\n                                                             ),\r\n                                                        merge_mode='sum', name='Encoder_BiLSTM'\r\n                                                        )(bert_output)\r\n\r\n    state_h = Concatenate()([forward_h, backward_h])\r\n\r\n    context = SeqSelfAttention(units=128)([bilstm, state_h])\r\n\r\n    concat = Concatenate()([bilstm, context])\r\n\r\n    decoder_fwd_lstm = LSTM(hidden_size, dropout=0.2,\r\n                            recurrent_dropout=0.2,\r\n                            return_sequences=True,\r\n                            input_shape=(None, None, embedding_size),\r\n                            name='Decoder_FWD_LSTM')(concat)\r\n\r\n    decoder_bck_lstm = LSTM(hidden_size,\r\n                            dropout=0.2,\r\n                            recurrent_dropout=0.2,\r\n                            return_sequences=True,\r\n                            input_shape=(None, None, embedding_size),\r\n                            go_backwards=True,\r\n                            name='Decoder_BWD_LSTM')(decoder_fwd_lstm)\r\n\r\n    decoder_bilstm = Concatenate()([decoder_fwd_lstm, decoder_bck_lstm])\r\n\r\n    logits = TimeDistributed(Dense(output_size), name='WSD_logits')(decoder_bilstm)\r\n    logits_mask = Add(name=\"Masked_logits\")([logits, input_mask])\r\n\r\n    pos_logits = TimeDistributed(Dense(pos_vocab_size), name='POS_logits')(decoder_bilstm)\r\n    lex_logits = TimeDistributed(Dense(lex_vocab_size), name='LEX_logits')(decoder_bilstm)\r\n\r\n    wsd_output = Softmax(name=\"WSD_output\")(logits_mask)\r\n    pos_output = Softmax(name=\"POS_output\")(pos_logits)\r\n    lex_output = Softmax(name=\"LEX_output\")(lex_logits)\r\n\r\n    model = Model(inputs=bert_inputs_, outputs=[wsd_output, pos_output, lex_output],\r\n                  name='Bert_Attention_Seq2Seq_MultiTask')\r\n\r\n    model.compile(loss=\"sparse_categorical_crossentropy\",\r\n                  optimizer=Adadelta(), metrics=['accuracy'])\r\n\r\n    visualize_plot_mdl(visualize, plot, model)\r\n\r\n    return model\r\n```\r\n\r\nAnd every time I get \r\n```\r\n    feature_dim = int(input_shape[2])\r\nIndexError: tuple index out of range\r\n```\r\nI tried, none of them worked:\r\n\r\n1. `return_sequence=True` in the preceding LSTM\r\n2. Tried `SeqWeightedAttention()` instead of the currently used\r\n\r\nI am using \r\n```\r\nPython 3.6.4\r\ntensorflow 1.12\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/35", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/35/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/35/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/35/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/35", "id": 532601697, "node_id": "MDU6SXNzdWU1MzI2MDE2OTc=", "number": 35, "title": "Examples for a basic NMT model?", "user": {"login": "John-8704", "id": 58213113, "node_id": "MDQ6VXNlcjU4MjEzMTEz", "avatar_url": "https://avatars1.githubusercontent.com/u/58213113?v=4", "gravatar_id": "", "url": "https://api.github.com/users/John-8704", "html_url": "https://github.com/John-8704", "followers_url": "https://api.github.com/users/John-8704/followers", "following_url": "https://api.github.com/users/John-8704/following{/other_user}", "gists_url": "https://api.github.com/users/John-8704/gists{/gist_id}", "starred_url": "https://api.github.com/users/John-8704/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/John-8704/subscriptions", "organizations_url": "https://api.github.com/users/John-8704/orgs", "repos_url": "https://api.github.com/users/John-8704/repos", "events_url": "https://api.github.com/users/John-8704/events{/privacy}", "received_events_url": "https://api.github.com/users/John-8704/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-04T10:48:42Z", "updated_at": "2019-12-11T11:33:55Z", "closed_at": "2019-12-11T11:33:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "I couldn't understand how to use it to build a sequence to sequence machine translation task. An example/tutorial will be really helpful.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/34", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/34/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/34/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/34/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/34", "id": 532344103, "node_id": "MDU6SXNzdWU1MzIzNDQxMDM=", "number": 34, "title": "Is it only for sequential data?", "user": {"login": "xuzhang5788", "id": 36318415, "node_id": "MDQ6VXNlcjM2MzE4NDE1", "avatar_url": "https://avatars0.githubusercontent.com/u/36318415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuzhang5788", "html_url": "https://github.com/xuzhang5788", "followers_url": "https://api.github.com/users/xuzhang5788/followers", "following_url": "https://api.github.com/users/xuzhang5788/following{/other_user}", "gists_url": "https://api.github.com/users/xuzhang5788/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuzhang5788/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuzhang5788/subscriptions", "organizations_url": "https://api.github.com/users/xuzhang5788/orgs", "repos_url": "https://api.github.com/users/xuzhang5788/repos", "events_url": "https://api.github.com/users/xuzhang5788/events{/privacy}", "received_events_url": "https://api.github.com/users/xuzhang5788/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-03T23:47:25Z", "updated_at": "2019-12-11T00:23:11Z", "closed_at": "2019-12-11T00:23:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "May I use it for CNN models? How to add this layer into it? Many thanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/33", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/33/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/33/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/33/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/33", "id": 528567749, "node_id": "MDU6SXNzdWU1Mjg1Njc3NDk=", "number": 33, "title": "AttributeError: module 'tensorflow' has no attribute 'get_default_graph' while using 'SeqSelfAttention'", "user": {"login": "octolis", "id": 46808738, "node_id": "MDQ6VXNlcjQ2ODA4NzM4", "avatar_url": "https://avatars0.githubusercontent.com/u/46808738?v=4", "gravatar_id": "", "url": "https://api.github.com/users/octolis", "html_url": "https://github.com/octolis", "followers_url": "https://api.github.com/users/octolis/followers", "following_url": "https://api.github.com/users/octolis/following{/other_user}", "gists_url": "https://api.github.com/users/octolis/gists{/gist_id}", "starred_url": "https://api.github.com/users/octolis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/octolis/subscriptions", "organizations_url": "https://api.github.com/users/octolis/orgs", "repos_url": "https://api.github.com/users/octolis/repos", "events_url": "https://api.github.com/users/octolis/events{/privacy}", "received_events_url": "https://api.github.com/users/octolis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-26T08:34:12Z", "updated_at": "2020-03-09T16:53:20Z", "closed_at": "2019-12-03T10:43:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey CyberZHG, \r\nthank you for your cool packages. I've used keras-self-attention, and firstly it worked okay but the other day 'AttributeError: module 'tensorflow' has no attribute 'get_default_graph'' started to appear every time I try to use SeqSelfAttention. Without using your code, the error disappears. \r\nI couldn't figure out what was the problem. I tried to upgrade/downgrade and reinstall tf and Keras (following the posts from StackOverFlow) but it didn't help. \r\nSo maybe you can explain to me what's wrong? The problem seems to be somehow connected with keras-self-attention. I'm new to neural networks and programming in general, so I hope if this question is stupid, you'll be patient to answer in detail (because several days of googling did not help). Thank you in advance! \r\nHere is my code: \r\n```\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\r\nfrom tensorflow.keras.layers import LSTM\r\nfrom tensorflow.keras.layers import GRU\r\nfrom keras_self_attention import SeqSelfAttention\r\n\r\nmax_features = 4 #number of words in the dictionary\r\nnum_classes = 2\r\nmodel = Sequential()\r\nmodel.add(GRU(128, input_shape=(70, max_features), return_sequences=True, activation='tanh'))\r\nmodel.add(SeqSelfAttention(attention_activation='sigmoid')) \r\nmodel.add(Flatten())\r\nmodel.add(Dense(num_classes, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\n```\r\n\r\nHere is my error: \r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-15-1807f7e55fc9> in <module>\r\n     11 model.add(GRU(128, input_shape=(70, max_features), return_sequences=True, activation='tanh'))\r\n     12 # model.add(LSTM(128, input_shape=(70, max_features), return_sequences=True)) #return_sequences: output for att.layer\r\n---> 13 model.add(SeqSelfAttention(attention_activation='sigmoid'))\r\n     14 # model.add(Dropout(0.5))\r\n     15 model.add(Flatten())\r\n\r\n~/anaconda3/lib/python3.7/site-packages/keras_self_attention/seq_self_attention.py in __init__(self, units, attention_width, attention_type, return_attention, history_only, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, kernel_constraint, bias_constraint, use_additive_bias, use_attention_bias, attention_activation, attention_regularizer_weight, **kwargs)\r\n     47         :param kwargs: Parameters for parent class.\r\n     48         \"\"\"\r\n---> 49         super(SeqSelfAttention, self).__init__(**kwargs)\r\n     50         self.supports_masking = True\r\n     51         self.units = units\r\n\r\n~/anaconda3/lib/python3.7/site-packages/keras/engine/base_layer.py in __init__(self, **kwargs)\r\n    130         if not name:\r\n    131             prefix = self.__class__.__name__\r\n--> 132             name = _to_snake_case(prefix) + '_' + str(K.get_uid(prefix))\r\n    133         self.name = name\r\n    134 \r\n\r\n~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py in get_uid(prefix)\r\n     72     \"\"\"\r\n     73     global _GRAPH_UID_DICTS\r\n---> 74     graph = tf.get_default_graph()\r\n     75     if graph not in _GRAPH_UID_DICTS:\r\n     76         _GRAPH_UID_DICTS[graph] = defaultdict(int)\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'get_default_graph'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/32", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/32/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/32/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/32/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/32", "id": 519047173, "node_id": "MDU6SXNzdWU1MTkwNDcxNzM=", "number": 32, "title": "Attention Weights", "user": {"login": "BigMasonFang", "id": 14925813, "node_id": "MDQ6VXNlcjE0OTI1ODEz", "avatar_url": "https://avatars3.githubusercontent.com/u/14925813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BigMasonFang", "html_url": "https://github.com/BigMasonFang", "followers_url": "https://api.github.com/users/BigMasonFang/followers", "following_url": "https://api.github.com/users/BigMasonFang/following{/other_user}", "gists_url": "https://api.github.com/users/BigMasonFang/gists{/gist_id}", "starred_url": "https://api.github.com/users/BigMasonFang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BigMasonFang/subscriptions", "organizations_url": "https://api.github.com/users/BigMasonFang/orgs", "repos_url": "https://api.github.com/users/BigMasonFang/repos", "events_url": "https://api.github.com/users/BigMasonFang/events{/privacy}", "received_events_url": "https://api.github.com/users/BigMasonFang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-07T05:19:08Z", "updated_at": "2020-01-07T12:20:19Z", "closed_at": "2019-11-14T06:17:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "How to get the final attention weights when setting return_attention = True, suppose you set a the word vector as the input.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/31", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/31/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/31/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/31/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/31", "id": 516181122, "node_id": "MDU6SXNzdWU1MTYxODExMjI=", "number": 31, "title": "Tensorflow 2.0 Compatibility", "user": {"login": "SamanehSaadat", "id": 1986164, "node_id": "MDQ6VXNlcjE5ODYxNjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1986164?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SamanehSaadat", "html_url": "https://github.com/SamanehSaadat", "followers_url": "https://api.github.com/users/SamanehSaadat/followers", "following_url": "https://api.github.com/users/SamanehSaadat/following{/other_user}", "gists_url": "https://api.github.com/users/SamanehSaadat/gists{/gist_id}", "starred_url": "https://api.github.com/users/SamanehSaadat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SamanehSaadat/subscriptions", "organizations_url": "https://api.github.com/users/SamanehSaadat/orgs", "repos_url": "https://api.github.com/users/SamanehSaadat/repos", "events_url": "https://api.github.com/users/SamanehSaadat/events{/privacy}", "received_events_url": "https://api.github.com/users/SamanehSaadat/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-01T15:25:44Z", "updated_at": "2020-03-28T16:02:23Z", "closed_at": "2019-11-08T16:29:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nWhen I tried to use this package in Tensorflow 2.0, I got the following error:\r\n\r\n> RuntimeError: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/30", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/30/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/30/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/30/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/30", "id": 506322855, "node_id": "MDU6SXNzdWU1MDYzMjI4NTU=", "number": 30, "title": "keras-self-attention for time series forecasting", "user": {"login": "rahimikia", "id": 6575477, "node_id": "MDQ6VXNlcjY1NzU0Nzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/6575477?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rahimikia", "html_url": "https://github.com/rahimikia", "followers_url": "https://api.github.com/users/rahimikia/followers", "following_url": "https://api.github.com/users/rahimikia/following{/other_user}", "gists_url": "https://api.github.com/users/rahimikia/gists{/gist_id}", "starred_url": "https://api.github.com/users/rahimikia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rahimikia/subscriptions", "organizations_url": "https://api.github.com/users/rahimikia/orgs", "repos_url": "https://api.github.com/users/rahimikia/repos", "events_url": "https://api.github.com/users/rahimikia/events{/privacy}", "received_events_url": "https://api.github.com/users/rahimikia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-10-13T13:04:38Z", "updated_at": "2019-10-17T11:52:48Z", "closed_at": "2019-10-17T11:52:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a  time series case, and the input shape of network iss `2000*214` (`2000` samples \u2013 here, every sample is a day, and I have `214` features). Also, X as the input of the fit function is `(2000, 1, 214)`, and the output is `(2000, 1)`.\r\n\r\n\r\n```\r\nipt   = Input(shape = (2000, 214))\r\nx     = LSTM(250, activation='tanh', return_sequences=True)(ipt)\r\nx     = SeqSelfAttention(return_attention=True, name='att')(x)\r\nx     = concatenate(x)\r\nx     = Flatten()(x)\r\nout   = Dense(1, activation='relu')(x)\r\ndl_model = Model(ipt, out)\r\ndl_model.compile(optimizer = 'adam', loss = 'mse')\r\n```\r\nAfter training the model, I use this:\r\n```\r\n\r\noutputs   = [layer.output for layer in dl_model.layers if 'att' in layer.name]\r\nlayers_fn = K.function([dl_model.input, K.learning_phase()], outputs[0])\r\nww = layers_fn([X_train, 1])\r\n```\r\nThe problem is that all values in `ww[1]` are one. I used this package with a sample of random numbers, and it behaved similarly. In this particular case, how can I extract attention to understand the importance of past samples.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/29", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/29/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/29/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/29/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/29", "id": 493669292, "node_id": "MDU6SXNzdWU0OTM2NjkyOTI=", "number": 29, "title": "__init__() missing 3 required positional arguments: 'node_def', 'op', and 'message'", "user": {"login": "dingtine", "id": 7880782, "node_id": "MDQ6VXNlcjc4ODA3ODI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7880782?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dingtine", "html_url": "https://github.com/dingtine", "followers_url": "https://api.github.com/users/dingtine/followers", "following_url": "https://api.github.com/users/dingtine/following{/other_user}", "gists_url": "https://api.github.com/users/dingtine/gists{/gist_id}", "starred_url": "https://api.github.com/users/dingtine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dingtine/subscriptions", "organizations_url": "https://api.github.com/users/dingtine/orgs", "repos_url": "https://api.github.com/users/dingtine/repos", "events_url": "https://api.github.com/users/dingtine/events{/privacy}", "received_events_url": "https://api.github.com/users/dingtine/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-14T21:39:21Z", "updated_at": "2019-09-21T23:27:03Z", "closed_at": "2019-09-21T23:27:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "when i used the SeqSelfAttention function, the code return this error: __init__() missing 3 required positional arguments: 'node_def', 'op', and 'message', how to fix this ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/28", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/28/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/28/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/28/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/28", "id": 490176024, "node_id": "MDU6SXNzdWU0OTAxNzYwMjQ=", "number": 28, "title": "\u5728\u52a0\u6cd5\u6a21\u5f0f\u548c\u4e58\u6cd5\u6a21\u5f0f\u91cc\uff0c\u4e00\u4e2a\u52a0 ba,\u4e00\u4e2a\u52a0ba[0]", "user": {"login": "xiayangdi", "id": 9064102, "node_id": "MDQ6VXNlcjkwNjQxMDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/9064102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiayangdi", "html_url": "https://github.com/xiayangdi", "followers_url": "https://api.github.com/users/xiayangdi/followers", "following_url": "https://api.github.com/users/xiayangdi/following{/other_user}", "gists_url": "https://api.github.com/users/xiayangdi/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiayangdi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiayangdi/subscriptions", "organizations_url": "https://api.github.com/users/xiayangdi/orgs", "repos_url": "https://api.github.com/users/xiayangdi/repos", "events_url": "https://api.github.com/users/xiayangdi/events{/privacy}", "received_events_url": "https://api.github.com/users/xiayangdi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-06T07:32:18Z", "updated_at": "2019-09-09T03:11:41Z", "closed_at": "2019-09-09T03:11:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/CyberZHG/keras-self-attention/blob/2d4f5e67ffcbccf0a7bb5d22faec266253e9fd7e/keras_self_attention/seq_self_attention.py#L215\r\n\r\nba\u7684shape\u4e3a(1,)\uff0c\u5728\u52a0\u6cd5\u6a21\u5f0f\u4e2d\u662f\u76f4\u63a5\u52a0 self.ba\uff0c\u5728\u4e58\u6cd5\u6a21\u5f0f\u91cc\u5374\u662f\u52a0 self.ba[0]\uff0c\u611f\u89c9\u5e94\u8be5\u6ca1\u5fc5\u8981\u53d6[0]", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/27", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/27/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/27/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/27/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/27", "id": 490115797, "node_id": "MDU6SXNzdWU0OTAxMTU3OTc=", "number": 27, "title": "\u521d\u59cb\u5316\u5668\u5199\u6210\u4e86\u6b63\u5219\u5316\u5668", "user": {"login": "xiayangdi", "id": 9064102, "node_id": "MDQ6VXNlcjkwNjQxMDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/9064102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiayangdi", "html_url": "https://github.com/xiayangdi", "followers_url": "https://api.github.com/users/xiayangdi/followers", "following_url": "https://api.github.com/users/xiayangdi/following{/other_user}", "gists_url": "https://api.github.com/users/xiayangdi/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiayangdi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiayangdi/subscriptions", "organizations_url": "https://api.github.com/users/xiayangdi/orgs", "repos_url": "https://api.github.com/users/xiayangdi/repos", "events_url": "https://api.github.com/users/xiayangdi/events{/privacy}", "received_events_url": "https://api.github.com/users/xiayangdi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-06T03:56:06Z", "updated_at": "2019-09-07T01:04:29Z", "closed_at": "2019-09-07T01:04:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/CyberZHG/keras-self-attention/blob/2d4f5e67ffcbccf0a7bb5d22faec266253e9fd7e/keras_self_attention/seq_self_attention.py#L88-L89\r\n\r\n\u4e24\u4e2a keras.regularizers.serialize\uff0c\u5e94\u8be5\u662f  keras.initializers.serialize", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/26", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/26/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/26/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/26/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/26", "id": 490096317, "node_id": "MDU6SXNzdWU0OTAwOTYzMTc=", "number": 26, "title": "results[2]\u5e94\u8be5\u6539\u4e3aresults[0]", "user": {"login": "xiayangdi", "id": 9064102, "node_id": "MDQ6VXNlcjkwNjQxMDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/9064102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiayangdi", "html_url": "https://github.com/xiayangdi", "followers_url": "https://api.github.com/users/xiayangdi/followers", "following_url": "https://api.github.com/users/xiayangdi/following{/other_user}", "gists_url": "https://api.github.com/users/xiayangdi/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiayangdi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiayangdi/subscriptions", "organizations_url": "https://api.github.com/users/xiayangdi/orgs", "repos_url": "https://api.github.com/users/xiayangdi/repos", "events_url": "https://api.github.com/users/xiayangdi/events{/privacy}", "received_events_url": "https://api.github.com/users/xiayangdi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-06T02:29:33Z", "updated_at": "2019-09-07T01:04:24Z", "closed_at": "2019-09-07T01:04:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://github.com/CyberZHG/keras-self-attention/blob/2d4f5e67ffcbccf0a7bb5d22faec266253e9fd7e/tests/scaled_dot_attention/test_history.py#L43\r\n\r\n\u611f\u89c9results[2]\u5e94\u8be5\u6539\u4e3aresults[0]\u5427\uff1f\u4e0d\u8fc7\u81ea\u5df1\u662f\u65b0\u624b\uff0c\u4e0d\u592a\u786e\u5b9a\u81ea\u5df1\u5bf9\u4e0d\u5bf9\uff0c\u7ed9\u5927\u795e\u53cd\u9988\u4e0b", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/25", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/25/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/25/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/25/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/25", "id": 481908551, "node_id": "MDU6SXNzdWU0ODE5MDg1NTE=", "number": 25, "title": "\"Tuple index out of range\" when using SeqWeightedAttention", "user": {"login": "Hellisotherpeople", "id": 12686966, "node_id": "MDQ6VXNlcjEyNjg2OTY2", "avatar_url": "https://avatars0.githubusercontent.com/u/12686966?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hellisotherpeople", "html_url": "https://github.com/Hellisotherpeople", "followers_url": "https://api.github.com/users/Hellisotherpeople/followers", "following_url": "https://api.github.com/users/Hellisotherpeople/following{/other_user}", "gists_url": "https://api.github.com/users/Hellisotherpeople/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hellisotherpeople/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hellisotherpeople/subscriptions", "organizations_url": "https://api.github.com/users/Hellisotherpeople/orgs", "repos_url": "https://api.github.com/users/Hellisotherpeople/repos", "events_url": "https://api.github.com/users/Hellisotherpeople/events{/privacy}", "received_events_url": "https://api.github.com/users/Hellisotherpeople/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-08-17T17:30:38Z", "updated_at": "2020-01-24T20:22:03Z", "closed_at": "2019-08-18T08:54:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python \r\n\r\nelif keras_mode == \"RNN\":\r\n            model.add(Reshape((1, list_of_embeddings[1].size), input_shape = Emb_train.shape[1:])) \r\n            model.add(Bidirectional(GRU(list_of_embeddings[1].size, activation = 'relu'))) ##this works too - seems to be better for smaller datasets too!\r\n            model.add(SeqWeightedAttention())\r\n            model.add(Dense(len(np.unique(Y_val)),activation='softmax',kernel_initializer=kernel_initializer, use_bias = False))\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"classification.py\", line 182, in <module>\r\n    pipe.fit(X_train, Y_train)\r\n  File \"/usr/lib/python3.7/site-packages/sklearn/pipeline.py\", line 267, in fit\r\n    self._final_estimator.fit(Xt, y, **fit_params)\r\n  File \"/usr/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 210, in fit\r\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\", line 141, in fit\r\n    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\r\n  File \"classification.py\", line 144, in create_model\r\n    model.add(SeqWeightedAttention())\r\n  File \"/usr/lib/python3.7/site-packages/keras/engine/sequential.py\", line 181, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/usr/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 431, in __call__\r\n    self.build(unpack_singleton(input_shapes))\r\n  File \"/usr/lib/python3.7/site-packages/keras_self_attention/seq_weighted_attention.py\", line 27, in build\r\n    self.W = self.add_weight(shape=(int(input_shape[2]), 1),\r\nIndexError: tuple index out of range\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/24", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/24/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/24/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/24/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/24", "id": 479363970, "node_id": "MDU6SXNzdWU0NzkzNjM5NzA=", "number": 24, "title": "\"tuple index out of range\" when using self attention layer in imdb dataset", "user": {"login": "Larry955", "id": 15189425, "node_id": "MDQ6VXNlcjE1MTg5NDI1", "avatar_url": "https://avatars0.githubusercontent.com/u/15189425?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Larry955", "html_url": "https://github.com/Larry955", "followers_url": "https://api.github.com/users/Larry955/followers", "following_url": "https://api.github.com/users/Larry955/following{/other_user}", "gists_url": "https://api.github.com/users/Larry955/gists{/gist_id}", "starred_url": "https://api.github.com/users/Larry955/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Larry955/subscriptions", "organizations_url": "https://api.github.com/users/Larry955/orgs", "repos_url": "https://api.github.com/users/Larry955/repos", "events_url": "https://api.github.com/users/Larry955/events{/privacy}", "received_events_url": "https://api.github.com/users/Larry955/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-08-11T12:23:48Z", "updated_at": "2020-01-20T00:19:11Z", "closed_at": "2019-08-19T05:16:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I followed the steps to use self-attention layer just as README.md did but I got an error when I created my own model.\r\nHere is my code:\r\n\r\n```\r\nfrom keras.datasets import imdb # the dataset I used\r\n# ....\r\nreview_input = Input(shape=(MAX_WORDS_PER_REVIEW,), dtype='int32')\r\nembedding_layer = Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_WORDS_PER_REVIEW)\r\nembedding_review = embedding_layer(review_input)\r\nlstm = LSTM(100)(embedding_review)\r\natt_lstm = SelfAttention(units=100, attention_activation=\"sigmoid\")(lstm)  # I used Attention Layer after LSTM layer\r\npreds = Dense(1, activation='sigmoid')(att_lstm)\r\n\r\nmodel = Model(review_input, preds)\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n```\r\nHere is the error:\r\n```\r\n\"self_attention.py\", line 106, in **_build_additive_attention**\r\n    feature_dim = int(input_shape[2])\r\nIndexError: tuple index out of range\r\n```\r\n\r\nIt seems that the dimension is not suitable, but I don't know why this happened, since the code worked without the attention layer.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/23", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/23/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/23/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/23/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/23", "id": 479330487, "node_id": "MDU6SXNzdWU0NzkzMzA0ODc=", "number": 23, "title": "\u60a8\u597d\u8bf7\u95ee\u60a8\u8fd9\u4e2a\u5bf9\u5e94\u7684\u8bba\u6587\u662f\u90a3\u4e00\u7bc7\u3002", "user": {"login": "dashujuzha", "id": 26001693, "node_id": "MDQ6VXNlcjI2MDAxNjkz", "avatar_url": "https://avatars0.githubusercontent.com/u/26001693?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashujuzha", "html_url": "https://github.com/dashujuzha", "followers_url": "https://api.github.com/users/dashujuzha/followers", "following_url": "https://api.github.com/users/dashujuzha/following{/other_user}", "gists_url": "https://api.github.com/users/dashujuzha/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashujuzha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashujuzha/subscriptions", "organizations_url": "https://api.github.com/users/dashujuzha/orgs", "repos_url": "https://api.github.com/users/dashujuzha/repos", "events_url": "https://api.github.com/users/dashujuzha/events{/privacy}", "received_events_url": "https://api.github.com/users/dashujuzha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-11T04:12:08Z", "updated_at": "2019-08-19T05:16:54Z", "closed_at": "2019-08-19T05:16:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "\u60a8\u597d\u8bf7\u95ee\u60a8\u8fd9\u4e2a\u5bf9\u5e94\u7684\u8bba\u6587\u662f\u90a3\u4e00\u7bc7\u3002", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/22", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/22/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/22/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/22/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/22", "id": 479075285, "node_id": "MDU6SXNzdWU0NzkwNzUyODU=", "number": 22, "title": "error in building my bi-lstm with attention, help ", "user": {"login": "denglizong", "id": 7865325, "node_id": "MDQ6VXNlcjc4NjUzMjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/7865325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/denglizong", "html_url": "https://github.com/denglizong", "followers_url": "https://api.github.com/users/denglizong/followers", "following_url": "https://api.github.com/users/denglizong/following{/other_user}", "gists_url": "https://api.github.com/users/denglizong/gists{/gist_id}", "starred_url": "https://api.github.com/users/denglizong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/denglizong/subscriptions", "organizations_url": "https://api.github.com/users/denglizong/orgs", "repos_url": "https://api.github.com/users/denglizong/repos", "events_url": "https://api.github.com/users/denglizong/events{/privacy}", "received_events_url": "https://api.github.com/users/denglizong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-08-09T16:43:20Z", "updated_at": "2019-12-01T22:27:37Z", "closed_at": "2019-08-19T05:16:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Dear author,\r\n        Thanks for your keras-self-attention.\r\n   \r\n        Recently I am learning to develop a bi-lstm with attention model, and meet a mistake when use self-attention:\r\n\r\n(for imdb dataset)\r\n`model3 = Sequential()\r\n\r\nmodel3.add( Embedding(max_features, 32) )\r\nmodel3.add( layers.Bidirectional( layers.LSTM(32, return_sequences=True) ) )\r\n**model3.add(SeqSelfAttention(activation='sigmoid')  )**\r\nmodel3.add(Dense(1, activation='sigmoid') )\r\n\r\nmodel3.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\r\nhistory = model3.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)`\r\n\r\n     when I run model.fit, the value error comes,\r\n`---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-86-e6eb02d043c4> in <module>()\r\n----> 1 history = model3.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\r\n\r\n~/denglz/venv4re/lib/python3.6/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n    950             sample_weight=sample_weight,\r\n    951             class_weight=class_weight,\r\n--> 952             batch_size=batch_size)\r\n    953         # Prepare validation data.\r\n    954         do_validation = False\r\n\r\n~/denglz/venv4re/lib/python3.6/site-packages/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\r\n    787                 feed_output_shapes,\r\n    788                 check_batch_axis=False,  # Don't enforce the batch size.\r\n--> 789                 exception_prefix='target')\r\n    790 \r\n    791             # Generate sample-wise weight values given the `sample_weight` and\r\n\r\n~/denglz/venv4re/lib/python3.6/site-packages/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    126                         ': expected ' + names[i] + ' to have ' +\r\n    127                         str(len(shape)) + ' dimensions, but got array '\r\n--> 128                         'with shape ' + str(data_shape))\r\n    129                 if not check_batch_axis:\r\n    130                     data_shape = data_shape[1:]\r\n\r\nValueError: Error when checking target: expected dense_6 to have 3 dimensions, but got array with shape (25000, 1)`\r\n\r\n     Am i using the keras-self-attention in wrong way? Need your help , thanks a lot..", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/21", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/21/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/21/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/21/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/21", "id": 470929196, "node_id": "MDU6SXNzdWU0NzA5MjkxOTY=", "number": 21, "title": "how to apply Attention between two LSTM layers?", "user": {"login": "convman", "id": 39067766, "node_id": "MDQ6VXNlcjM5MDY3NzY2", "avatar_url": "https://avatars1.githubusercontent.com/u/39067766?v=4", "gravatar_id": "", "url": "https://api.github.com/users/convman", "html_url": "https://github.com/convman", "followers_url": "https://api.github.com/users/convman/followers", "following_url": "https://api.github.com/users/convman/following{/other_user}", "gists_url": "https://api.github.com/users/convman/gists{/gist_id}", "starred_url": "https://api.github.com/users/convman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/convman/subscriptions", "organizations_url": "https://api.github.com/users/convman/orgs", "repos_url": "https://api.github.com/users/convman/repos", "events_url": "https://api.github.com/users/convman/events{/privacy}", "received_events_url": "https://api.github.com/users/convman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-22T07:07:42Z", "updated_at": "2019-07-22T07:19:10Z", "closed_at": "2019-07-22T07:19:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/20", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/20/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/20/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/20/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/20", "id": 470651904, "node_id": "MDU6SXNzdWU0NzA2NTE5MDQ=", "number": 20, "title": "The gradient is missing sometimes", "user": {"login": "SUNBERG010", "id": 16748554, "node_id": "MDQ6VXNlcjE2NzQ4NTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/16748554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SUNBERG010", "html_url": "https://github.com/SUNBERG010", "followers_url": "https://api.github.com/users/SUNBERG010/followers", "following_url": "https://api.github.com/users/SUNBERG010/following{/other_user}", "gists_url": "https://api.github.com/users/SUNBERG010/gists{/gist_id}", "starred_url": "https://api.github.com/users/SUNBERG010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SUNBERG010/subscriptions", "organizations_url": "https://api.github.com/users/SUNBERG010/orgs", "repos_url": "https://api.github.com/users/SUNBERG010/repos", "events_url": "https://api.github.com/users/SUNBERG010/events{/privacy}", "received_events_url": "https://api.github.com/users/SUNBERG010/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-20T08:43:11Z", "updated_at": "2019-07-29T09:06:56Z", "closed_at": "2019-07-29T09:06:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "Dear Zhao,\r\n Recently I tried your self-attention layer for my work, I really appreciate it.\r\n\r\nHowever, the decrease of gradient sometimes got stuck from the beginning; I tried some ways, however, it is not stable, could you please give some suggestions? \r\n\r\nBest wishes,\r\nSunberg\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/19", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/19/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/19/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/19/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/19", "id": 460665613, "node_id": "MDU6SXNzdWU0NjA2NjU2MTM=", "number": 19, "title": "T", "user": {"login": "SUNBERG010", "id": 16748554, "node_id": "MDQ6VXNlcjE2NzQ4NTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/16748554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SUNBERG010", "html_url": "https://github.com/SUNBERG010", "followers_url": "https://api.github.com/users/SUNBERG010/followers", "following_url": "https://api.github.com/users/SUNBERG010/following{/other_user}", "gists_url": "https://api.github.com/users/SUNBERG010/gists{/gist_id}", "starred_url": "https://api.github.com/users/SUNBERG010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SUNBERG010/subscriptions", "organizations_url": "https://api.github.com/users/SUNBERG010/orgs", "repos_url": "https://api.github.com/users/SUNBERG010/repos", "events_url": "https://api.github.com/users/SUNBERG010/events{/privacy}", "received_events_url": "https://api.github.com/users/SUNBERG010/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-06-25T22:03:31Z", "updated_at": "2019-06-27T23:52:48Z", "closed_at": "2019-06-27T11:27:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/18", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/18/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/18/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/18/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/18", "id": 448910012, "node_id": "MDU6SXNzdWU0NDg5MTAwMTI=", "number": 18, "title": "Compatibility with `tf.keras`", "user": {"login": "nshaud", "id": 2974890, "node_id": "MDQ6VXNlcjI5NzQ4OTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2974890?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nshaud", "html_url": "https://github.com/nshaud", "followers_url": "https://api.github.com/users/nshaud/followers", "following_url": "https://api.github.com/users/nshaud/following{/other_user}", "gists_url": "https://api.github.com/users/nshaud/gists{/gist_id}", "starred_url": "https://api.github.com/users/nshaud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nshaud/subscriptions", "organizations_url": "https://api.github.com/users/nshaud/orgs", "repos_url": "https://api.github.com/users/nshaud/repos", "events_url": "https://api.github.com/users/nshaud/events{/privacy}", "received_events_url": "https://api.github.com/users/nshaud/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-27T15:22:55Z", "updated_at": "2019-05-28T08:08:47Z", "closed_at": "2019-05-28T08:08:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have been looking into self-attention using TensorFlow. More specifically I use the Keras API which is integrated the `tf.keras` module.\r\n\r\nI have tried both the `Sequential` and `Functional` API to no avail:\r\n```python\r\ntext_inputs = tf.keras.layers.Input(shape=(None,))\r\nembd_layer = tf.keras.layers.Embedding(input_dim=VOCAB_SIZE,\r\n                                output_dim=EMBEDDING_DIM,\r\n                                mask_zero=True,\r\n                                weights=None,\r\n                                trainable=None is None,\r\n                                name='Embedding')(text_inputs)\r\nlstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=512,\r\n                                                      recurrent_dropout=0.4,\r\n                                                      return_sequences=True),\r\n                                                      name='Bi-LSTM')(embd_layer)\r\nattention_layer = SeqSelfAttention(attention_activation='sigmoid',\r\n                               attention_width=9,\r\n                               return_attention=False,\r\n                               name='Attention')(lstm_layer)\r\n```\r\n\r\nreturns `TypeError: The added layer must be an instance of class Layer. Found: <keras_self_attention.seq_self_attention.SeqSelfAttention object at 0x7f87ee16bd30>` (I think because TensorFlow expects a `tf.keras.Layer` object).\r\n\r\nAnd using the Functional API:\r\n```python\r\ntext_inputs = tf.keras.layers.Input(shape=(SEQ_LENGTH,))\r\nx = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=SEQ_LENGTH)(text_inputs)\r\nx = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True))(x)\r\nx = SeqSelfAttention(attention_activation='sigmoid')(x)\r\n```\r\n\r\nreturns `ValueError: Layer Attention was called with an input that isn't a symbolic tensor. Received type: <class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'>. Full input: [<DeferredTensor 'None' shape=(?, ?, 1024) dtype=float32>]. All inputs to the layer should be tensors`\r\n\r\nAny clue? Is it because I am not using Keras but `tf.keras` instead?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/16", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/16/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/16/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/16/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/16", "id": 431833363, "node_id": "MDU6SXNzdWU0MzE4MzMzNjM=", "number": 16, "title": "Error (with multiplication?)", "user": {"login": "GadL", "id": 41967815, "node_id": "MDQ6VXNlcjQxOTY3ODE1", "avatar_url": "https://avatars0.githubusercontent.com/u/41967815?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GadL", "html_url": "https://github.com/GadL", "followers_url": "https://api.github.com/users/GadL/followers", "following_url": "https://api.github.com/users/GadL/following{/other_user}", "gists_url": "https://api.github.com/users/GadL/gists{/gist_id}", "starred_url": "https://api.github.com/users/GadL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GadL/subscriptions", "organizations_url": "https://api.github.com/users/GadL/orgs", "repos_url": "https://api.github.com/users/GadL/repos", "events_url": "https://api.github.com/users/GadL/events{/privacy}", "received_events_url": "https://api.github.com/users/GadL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-11T05:13:13Z", "updated_at": "2019-04-18T06:32:46Z", "closed_at": "2019-04-18T06:32:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "I keep getting value errors when working with your attention mechamism like such:\r\n\r\nValueError: Dimensions must be equal, but are 128 and 32 for 'Attention/MatMul' (op: 'MatMul') with input shapes: [?,128], [32,32].", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/15", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/15/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/15/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/15/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/15", "id": 427680844, "node_id": "MDU6SXNzdWU0Mjc2ODA4NDQ=", "number": 15, "title": "can this be used in a seq 2 seq task?", "user": {"login": "cristianmtr", "id": 8330330, "node_id": "MDQ6VXNlcjgzMzAzMzA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8330330?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cristianmtr", "html_url": "https://github.com/cristianmtr", "followers_url": "https://api.github.com/users/cristianmtr/followers", "following_url": "https://api.github.com/users/cristianmtr/following{/other_user}", "gists_url": "https://api.github.com/users/cristianmtr/gists{/gist_id}", "starred_url": "https://api.github.com/users/cristianmtr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cristianmtr/subscriptions", "organizations_url": "https://api.github.com/users/cristianmtr/orgs", "repos_url": "https://api.github.com/users/cristianmtr/repos", "events_url": "https://api.github.com/users/cristianmtr/events{/privacy}", "received_events_url": "https://api.github.com/users/cristianmtr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-01T12:36:59Z", "updated_at": "2019-04-09T08:24:40Z", "closed_at": "2019-04-09T08:24:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Can this be used in a seq 2 seq task? With an encoder LSTM and a decoder LSTM? The examples don't seem to cover this", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/14", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/14/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/14/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/14/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/14", "id": 427417628, "node_id": "MDU6SXNzdWU0Mjc0MTc2Mjg=", "number": 14, "title": "Scaled Dot Product attention error", "user": {"login": "AliOsm", "id": 7662492, "node_id": "MDQ6VXNlcjc2NjI0OTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7662492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AliOsm", "html_url": "https://github.com/AliOsm", "followers_url": "https://api.github.com/users/AliOsm/followers", "following_url": "https://api.github.com/users/AliOsm/following{/other_user}", "gists_url": "https://api.github.com/users/AliOsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/AliOsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AliOsm/subscriptions", "organizations_url": "https://api.github.com/users/AliOsm/orgs", "repos_url": "https://api.github.com/users/AliOsm/repos", "events_url": "https://api.github.com/users/AliOsm/events{/privacy}", "received_events_url": "https://api.github.com/users/AliOsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-03-31T16:54:10Z", "updated_at": "2019-04-09T11:24:41Z", "closed_at": "2019-04-09T11:24:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "When applying scaled dot product attention is gives the following error:\r\n\r\n```\r\nTypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\r\n```\r\n\r\nAny idea?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/13", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/13/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/13/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/13/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/13", "id": 427401962, "node_id": "MDU6SXNzdWU0Mjc0MDE5NjI=", "number": 13, "title": "ignore this issue. my mistake", "user": {"login": "bsugerman", "id": 22836928, "node_id": "MDQ6VXNlcjIyODM2OTI4", "avatar_url": "https://avatars1.githubusercontent.com/u/22836928?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bsugerman", "html_url": "https://github.com/bsugerman", "followers_url": "https://api.github.com/users/bsugerman/followers", "following_url": "https://api.github.com/users/bsugerman/following{/other_user}", "gists_url": "https://api.github.com/users/bsugerman/gists{/gist_id}", "starred_url": "https://api.github.com/users/bsugerman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bsugerman/subscriptions", "organizations_url": "https://api.github.com/users/bsugerman/orgs", "repos_url": "https://api.github.com/users/bsugerman/repos", "events_url": "https://api.github.com/users/bsugerman/events{/privacy}", "received_events_url": "https://api.github.com/users/bsugerman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-03-31T14:20:45Z", "updated_at": "2019-03-31T14:24:39Z", "closed_at": "2019-03-31T14:24:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Your attention codes all call self.add_weight, but I don't see that function defined anywhere in this repo...", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/12", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/12/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/12/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/12/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/12", "id": 411191411, "node_id": "MDU6SXNzdWU0MTExOTE0MTE=", "number": 12, "title": "Dense layer after Self Attention Layer throws an error", "user": {"login": "TarunTater", "id": 11738012, "node_id": "MDQ6VXNlcjExNzM4MDEy", "avatar_url": "https://avatars3.githubusercontent.com/u/11738012?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TarunTater", "html_url": "https://github.com/TarunTater", "followers_url": "https://api.github.com/users/TarunTater/followers", "following_url": "https://api.github.com/users/TarunTater/following{/other_user}", "gists_url": "https://api.github.com/users/TarunTater/gists{/gist_id}", "starred_url": "https://api.github.com/users/TarunTater/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TarunTater/subscriptions", "organizations_url": "https://api.github.com/users/TarunTater/orgs", "repos_url": "https://api.github.com/users/TarunTater/repos", "events_url": "https://api.github.com/users/TarunTater/events{/privacy}", "received_events_url": "https://api.github.com/users/TarunTater/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-02-17T14:08:51Z", "updated_at": "2019-02-23T06:31:33Z", "closed_at": "2019-02-23T06:31:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "If I try the code given in the example with text data : \r\n\r\n```\r\nimport keras\r\nfrom keras_self_attention import SeqSelfAttention\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Embedding(input_dim=10000,\r\n                                 output_dim=300,\r\n                                 mask_zero=True))\r\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(units=128,\r\n                                                       return_sequences=True)))\r\nmodel.add(SeqSelfAttention(attention_activation='sigmoid'))\r\nmodel.add(keras.layers.Dense(units=5))\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss='categorical_crossentropy',\r\n    metrics=['categorical_accuracy'],\r\n)\r\nmodel.summary()\r\n```\r\n\r\nIt throws an error : \r\n\r\n> ValueError: Error when checking target: expected dense_10 to have 3 dimensions, but got array with shape (28696, 2)\r\n\r\nI need to add another lstm layer after selfattention with return_sequence=False, to make this run. Am i missing something? The output from self attention layer is of shape (maxlen, number of units in lstm layer(say units=200)), how do we get a vector out of the layer of size (units=200)? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/11", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/11/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/11/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/11/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/11", "id": 407713357, "node_id": "MDU6SXNzdWU0MDc3MTMzNTc=", "number": 11, "title": "How to plot an attention heat map ? ", "user": {"login": "Bertorob", "id": 46813433, "node_id": "MDQ6VXNlcjQ2ODEzNDMz", "avatar_url": "https://avatars1.githubusercontent.com/u/46813433?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bertorob", "html_url": "https://github.com/Bertorob", "followers_url": "https://api.github.com/users/Bertorob/followers", "following_url": "https://api.github.com/users/Bertorob/following{/other_user}", "gists_url": "https://api.github.com/users/Bertorob/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bertorob/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bertorob/subscriptions", "organizations_url": "https://api.github.com/users/Bertorob/orgs", "repos_url": "https://api.github.com/users/Bertorob/repos", "events_url": "https://api.github.com/users/Bertorob/events{/privacy}", "received_events_url": "https://api.github.com/users/Bertorob/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-02-07T13:59:32Z", "updated_at": "2020-07-14T18:02:48Z", "closed_at": "2019-02-14T15:36:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried the flag return_attention = true but it returns an array filled with ones.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/10", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/10/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/10/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/10/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/10", "id": 404767515, "node_id": "MDU6SXNzdWU0MDQ3Njc1MTU=", "number": 10, "title": "\u201cOverflowError: Python int too large to convert to C long\u201d on windows but not linux", "user": {"login": "rnd-shtar", "id": 46990021, "node_id": "MDQ6VXNlcjQ2OTkwMDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/46990021?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rnd-shtar", "html_url": "https://github.com/rnd-shtar", "followers_url": "https://api.github.com/users/rnd-shtar/followers", "following_url": "https://api.github.com/users/rnd-shtar/following{/other_user}", "gists_url": "https://api.github.com/users/rnd-shtar/gists{/gist_id}", "starred_url": "https://api.github.com/users/rnd-shtar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rnd-shtar/subscriptions", "organizations_url": "https://api.github.com/users/rnd-shtar/orgs", "repos_url": "https://api.github.com/users/rnd-shtar/repos", "events_url": "https://api.github.com/users/rnd-shtar/events{/privacy}", "received_events_url": "https://api.github.com/users/rnd-shtar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886882, "node_id": "MDU6TGFiZWwxMDIzODg2ODgy", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-30T13:21:00Z", "updated_at": "2019-02-07T14:38:00Z", "closed_at": "2019-02-07T14:38:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "        attention = SeqSelfAttention(attention_activation='sigmoid',\r\n                                     history_only=True,\r\n                                     attention_type=SeqSelfAttention.ATTENTION_TYPE_ADD\r\n                                     )(biLSTM_LSTM_H)\r\nraises:\r\nOverflowError: Python int too large to convert to C long.\r\nThis happens on windows, on Linux there is not problem.\r\nWhen removing history_only=True parameter it works.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/9", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/9/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/9/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/9/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/9", "id": 398681171, "node_id": "MDU6SXNzdWUzOTg2ODExNzE=", "number": 9, "title": "Output of the layer is 3 Dimension and not 2 Dimension", "user": {"login": "hardikmeisheri", "id": 13063765, "node_id": "MDQ6VXNlcjEzMDYzNzY1", "avatar_url": "https://avatars0.githubusercontent.com/u/13063765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hardikmeisheri", "html_url": "https://github.com/hardikmeisheri", "followers_url": "https://api.github.com/users/hardikmeisheri/followers", "following_url": "https://api.github.com/users/hardikmeisheri/following{/other_user}", "gists_url": "https://api.github.com/users/hardikmeisheri/gists{/gist_id}", "starred_url": "https://api.github.com/users/hardikmeisheri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hardikmeisheri/subscriptions", "organizations_url": "https://api.github.com/users/hardikmeisheri/orgs", "repos_url": "https://api.github.com/users/hardikmeisheri/repos", "events_url": "https://api.github.com/users/hardikmeisheri/events{/privacy}", "received_events_url": "https://api.github.com/users/hardikmeisheri/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-01-13T17:41:44Z", "updated_at": "2019-01-23T02:57:15Z", "closed_at": "2019-01-23T02:57:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Compute_out_shape outputs shape = (?, max seqeunce length, units of bidirectional layer)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/8", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/8/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/8/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/8/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/8", "id": 396035213, "node_id": "MDU6SXNzdWUzOTYwMzUyMTM=", "number": 8, "title": "It is ready to work with multivariate  ?", "user": {"login": "rjpg", "id": 22857941, "node_id": "MDQ6VXNlcjIyODU3OTQx", "avatar_url": "https://avatars1.githubusercontent.com/u/22857941?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rjpg", "html_url": "https://github.com/rjpg", "followers_url": "https://api.github.com/users/rjpg/followers", "following_url": "https://api.github.com/users/rjpg/following{/other_user}", "gists_url": "https://api.github.com/users/rjpg/gists{/gist_id}", "starred_url": "https://api.github.com/users/rjpg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rjpg/subscriptions", "organizations_url": "https://api.github.com/users/rjpg/orgs", "repos_url": "https://api.github.com/users/rjpg/repos", "events_url": "https://api.github.com/users/rjpg/events{/privacy}", "received_events_url": "https://api.github.com/users/rjpg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-04T19:12:36Z", "updated_at": "2019-01-08T03:15:03Z", "closed_at": "2019-01-08T03:15:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "hello,\r\n\r\nI have this model : \r\n```\r\n        variables=9\r\n        timeSteps=128 \r\n        inputNet = Input(shape=(timeSteps,variables)) \r\n               lstm=Bidirectional(LSTM(100,recurrent_dropout=0.4,dropout=0.4,return_sequences=True),merge_mode='concat')(inputNet) #worse using stateful=True\r\n        lstm=SeqSelfAttention(attention_activation='sigmoid')(lstm)  \r\n      lstm=Bidirectional(LSTM(50,recurrent_dropout=0.4,dropout=0.4,return_sequences=False),merge_mode='concat')(lstm) #worse using stateful=True         \r\n        classificationLayer=Dense(classes,activation='softmax')(lstm)\r\n        model=Model(inputNet,classificationLayer)\r\n```\r\n\r\nIt do not seam to improve to the same model but without the line :\r\n` lstm=SeqSelfAttention(attention_activation='sigmoid')(lstm)    `   \r\n\r\nMy code makes sense or my problem/data does not need attention? \r\n\r\nThanks !", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/7", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/7/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/7/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/7/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/7", "id": 387899997, "node_id": "MDU6SXNzdWUzODc4OTk5OTc=", "number": 7, "title": "No attribute like return sequence = True/Flase", "user": {"login": "VedantYadav", "id": 13516636, "node_id": "MDQ6VXNlcjEzNTE2NjM2", "avatar_url": "https://avatars2.githubusercontent.com/u/13516636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VedantYadav", "html_url": "https://github.com/VedantYadav", "followers_url": "https://api.github.com/users/VedantYadav/followers", "following_url": "https://api.github.com/users/VedantYadav/following{/other_user}", "gists_url": "https://api.github.com/users/VedantYadav/gists{/gist_id}", "starred_url": "https://api.github.com/users/VedantYadav/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VedantYadav/subscriptions", "organizations_url": "https://api.github.com/users/VedantYadav/orgs", "repos_url": "https://api.github.com/users/VedantYadav/repos", "events_url": "https://api.github.com/users/VedantYadav/events{/privacy}", "received_events_url": "https://api.github.com/users/VedantYadav/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-12-05T19:03:01Z", "updated_at": "2018-12-10T02:54:26Z", "closed_at": "2018-12-10T02:54:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI'm using the attention model with Bidirectional LSTMs for sequence classification.\r\nHere is the code:\r\n\r\n```\r\n    inp = Input(shape=(maxlen, ), name=\"Input\")\r\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\r\n                  trainable=False, name=\"embedding\")(inp)\r\n    x = Bidirectional(LSTM(300, return_sequences=True, dropout=0.25,\r\n                           recurrent_dropout=0.25),name=\"lstm_1\")(x)\r\n    x = Bidirectional(LSTM(300, return_sequences=True, dropout=0.25,\r\n                           recurrent_dropout=0.25),name=\"lstm_2\")(x)\r\n    x = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, \r\n                         return_sequences = False,\r\n                         kernel_regularizer=keras.regularizers.l2(1e-4),\r\n                         bias_regularizer=keras.regularizers.l1(1e-4),\r\n                         attention_regularizer_weight=1e-4,\r\n                         name=\"Attention\")(x)\r\n    \r\n    x = Dense(256, activation=\"relu\")(x)\r\n    x = Dropout(0.25)(x)\r\n    x = Dense(6, activation=\"sigmoid\")(x)\r\n    model = Model(inputs=inp, outputs=x, name=\"Model\")\r\n```\r\n\r\nand the output I'm getting is:\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nInput (InputLayer)           (None, 150)               0         \r\n_________________________________________________________________\r\nembedding (Embedding)        (None, 150, 300)          30000000  \r\n_________________________________________________________________\r\nlstm_1 (Bidirectional)       (None, 150, 600)          1442400   \r\n_________________________________________________________________\r\nlstm_2 (Bidirectional)       (None, 150, 600)          2162400   \r\n_________________________________________________________________\r\nAttention (SeqSelfAttention) (None, 150, 600)          360001    \r\n_________________________________________________________________\r\ndense_8 (Dense)              (None, 150, 256)          153856    \r\n_________________________________________________________________\r\ndropout_4 (Dropout)          (None, 150, 256)          0         \r\n_________________________________________________________________\r\ndense_9 (Dense)              (None, 150, 6)            1542      \r\n=================================================================\r\nTotal params: 34,120,199\r\nTrainable params: 4,120,199\r\nNon-trainable params: 30,000,000\r\n_________________________________________________________________\r\n```\r\n\r\nAs you can see that output of attention model is (None,150,600) inplace of (None,600). Please tell me how I can do that ?\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/6", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/6/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/6/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/6/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/6", "id": 380586020, "node_id": "MDU6SXNzdWUzODA1ODYwMjA=", "number": 6, "title": "attention w/ TimeDistributed", "user": {"login": "phiweger", "id": 3918306, "node_id": "MDQ6VXNlcjM5MTgzMDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/3918306?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phiweger", "html_url": "https://github.com/phiweger", "followers_url": "https://api.github.com/users/phiweger/followers", "following_url": "https://api.github.com/users/phiweger/following{/other_user}", "gists_url": "https://api.github.com/users/phiweger/gists{/gist_id}", "starred_url": "https://api.github.com/users/phiweger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phiweger/subscriptions", "organizations_url": "https://api.github.com/users/phiweger/orgs", "repos_url": "https://api.github.com/users/phiweger/repos", "events_url": "https://api.github.com/users/phiweger/events{/privacy}", "received_events_url": "https://api.github.com/users/phiweger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-14T08:28:29Z", "updated_at": "2018-11-19T01:06:10Z", "closed_at": "2018-11-19T01:06:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nIs it possible to use the attention layer w/ a `TimeDistributed(Dense(...))`?\r\n\r\nIf so, how would one have to modify the example in the README?\r\n\r\nThank you,\r\nAdrian", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/5", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/5/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/5/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/5/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/5", "id": 377862471, "node_id": "MDU6SXNzdWUzNzc4NjI0NzE=", "number": 5, "title": "Does it support functional API?", "user": {"login": "ErmiaAzarkhalili", "id": 16982297, "node_id": "MDQ6VXNlcjE2OTgyMjk3", "avatar_url": "https://avatars2.githubusercontent.com/u/16982297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ErmiaAzarkhalili", "html_url": "https://github.com/ErmiaAzarkhalili", "followers_url": "https://api.github.com/users/ErmiaAzarkhalili/followers", "following_url": "https://api.github.com/users/ErmiaAzarkhalili/following{/other_user}", "gists_url": "https://api.github.com/users/ErmiaAzarkhalili/gists{/gist_id}", "starred_url": "https://api.github.com/users/ErmiaAzarkhalili/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ErmiaAzarkhalili/subscriptions", "organizations_url": "https://api.github.com/users/ErmiaAzarkhalili/orgs", "repos_url": "https://api.github.com/users/ErmiaAzarkhalili/repos", "events_url": "https://api.github.com/users/ErmiaAzarkhalili/events{/privacy}", "received_events_url": "https://api.github.com/users/ErmiaAzarkhalili/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-06T14:08:10Z", "updated_at": "2018-11-08T11:03:25Z", "closed_at": "2018-11-08T11:03:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Could you please provide real-world examples of the models with combination of keras functional API? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/4", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/4/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/4/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/4/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/4", "id": 377640888, "node_id": "MDU6SXNzdWUzNzc2NDA4ODg=", "number": 4, "title": "Does it support masking?", "user": {"login": "LincLabUCCS", "id": 30666434, "node_id": "MDQ6VXNlcjMwNjY2NDM0", "avatar_url": "https://avatars3.githubusercontent.com/u/30666434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LincLabUCCS", "html_url": "https://github.com/LincLabUCCS", "followers_url": "https://api.github.com/users/LincLabUCCS/followers", "following_url": "https://api.github.com/users/LincLabUCCS/following{/other_user}", "gists_url": "https://api.github.com/users/LincLabUCCS/gists{/gist_id}", "starred_url": "https://api.github.com/users/LincLabUCCS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LincLabUCCS/subscriptions", "organizations_url": "https://api.github.com/users/LincLabUCCS/orgs", "repos_url": "https://api.github.com/users/LincLabUCCS/repos", "events_url": "https://api.github.com/users/LincLabUCCS/events{/privacy}", "received_events_url": "https://api.github.com/users/LincLabUCCS/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-11-06T00:25:38Z", "updated_at": "2018-11-08T01:51:31Z", "closed_at": "2018-11-08T01:51:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello CyberZHG \r\n\r\nI have a sequence of inputs and sequence of outputs where each input has an associated output(Label). lets say (part of speech tagging (POS tagging))\r\n\r\nSeq_in[0][0:3]\r\narray([[15],[28], [23]])\r\n\r\nSeq_out[0][0:3]\r\narray([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\r\n          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\r\ndtype=float32)\r\n\r\nI am using the following code for training: \r\n\r\nX_train, X_val, Y_train, Y_val = train_test_split(Seq_in,Seq_out, test_size=0.20)\r\n\r\nmodel = Sequential()\r\nmodel.add(Masking(mask_value=5, input_shape= (Seq_in.shape[1],1))) # time steps is 500\r\nmodel.add(Bidirectional(LSTM(256,  return_sequences=True)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Bidirectional(LSTM(256,  return_sequences=True)))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(seq_self_attention.SeqSelfAttention())\r\nmodel.add(Dense(15, activation='softmax'))  \r\n\r\nsgd = optimizers.SGD(lr=.1,momentum=0.9,decay=1e-3,nesterov=True)\r\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\r\n\r\nmodel.fit(X_train,Y_train,epochs=2, validation_data=(X_val, Y_val),verbose=2)\r\n\r\nI have a couple of concerns:\r\n it seems that the implementation supports masking, but what I am doing in the code is a correct way to use masking or there is another way?\r\n\r\nwhy do we need the variable units in the constructor? does not the code figuer it out itself?\r\n\r\nfollowing the equations posted in the readme file, the process is to sum each neighbor states ht` with the state of the current time step ht, then taking the tanh of each unit in each state, which produce the same shape. first equation.\r\n\r\nsecond, each states ht` is squashed to one value (scalar) using sigmoid function. Second equation. \r\n\r\nThird, we find the softmax between the current state of the current time step with the other states ht`.\r\n\r\nFinally, we multiply the softmax probability (attention weight) with each unit and then taking the weighted sum. \r\n\r\n\r\nis my understanding correct? if so, why do we need the unit in the constructor? \r\n\r\nAlso, we have to methods multiplicative and additive, where can I see the difference in regard to the equations\r\n\r\nSorry, too many questions, I would appreciate your answers...\r\nThank you\r\n\r\n\r\n \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/3", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/3/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/3/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/3/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/3", "id": 373483175, "node_id": "MDU6SXNzdWUzNzM0ODMxNzU=", "number": 3, "title": "SeqSelfAttention returning tuple (tensor, weights) raise TypeError on Tensor object not iterable", "user": {"login": "McKracken", "id": 7049505, "node_id": "MDQ6VXNlcjcwNDk1MDU=", "avatar_url": "https://avatars0.githubusercontent.com/u/7049505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/McKracken", "html_url": "https://github.com/McKracken", "followers_url": "https://api.github.com/users/McKracken/followers", "following_url": "https://api.github.com/users/McKracken/following{/other_user}", "gists_url": "https://api.github.com/users/McKracken/gists{/gist_id}", "starred_url": "https://api.github.com/users/McKracken/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/McKracken/subscriptions", "organizations_url": "https://api.github.com/users/McKracken/orgs", "repos_url": "https://api.github.com/users/McKracken/repos", "events_url": "https://api.github.com/users/McKracken/events{/privacy}", "received_events_url": "https://api.github.com/users/McKracken/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-10-24T13:28:22Z", "updated_at": "2018-11-05T01:53:30Z", "closed_at": "2018-11-05T01:53:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI'm running the code Regularizer example from your README, and I getting the following error from SeqSelAttention layer\r\n\r\n`TypeError: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn. `\r\n\r\nraised in \"lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 436, in __iter__\r\n\r\nI'm using:\r\n- python 3.6.5 \r\n- tensorflow 1.9.0\r\n\r\nAny clue?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/2", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/2/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/2/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/2/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/2", "id": 358977930, "node_id": "MDU6SXNzdWUzNTg5Nzc5MzA=", "number": 2, "title": "How to implement \u201cMulti-Head\u201d", "user": {"login": "guofengjpggwk", "id": 32236422, "node_id": "MDQ6VXNlcjMyMjM2NDIy", "avatar_url": "https://avatars2.githubusercontent.com/u/32236422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guofengjpggwk", "html_url": "https://github.com/guofengjpggwk", "followers_url": "https://api.github.com/users/guofengjpggwk/followers", "following_url": "https://api.github.com/users/guofengjpggwk/following{/other_user}", "gists_url": "https://api.github.com/users/guofengjpggwk/gists{/gist_id}", "starred_url": "https://api.github.com/users/guofengjpggwk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guofengjpggwk/subscriptions", "organizations_url": "https://api.github.com/users/guofengjpggwk/orgs", "repos_url": "https://api.github.com/users/guofengjpggwk/repos", "events_url": "https://api.github.com/users/guofengjpggwk/events{/privacy}", "received_events_url": "https://api.github.com/users/guofengjpggwk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023886876, "node_id": "MDU6TGFiZWwxMDIzODg2ODc2", "url": "https://api.github.com/repos/CyberZHG/keras-self-attention/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-11T10:10:44Z", "updated_at": "2018-09-20T09:25:00Z", "closed_at": "2018-09-20T09:25:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "I want to use 'Multi-Head', but I don't know how. Is stacking SelfAttention OK?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/1", "repository_url": "https://api.github.com/repos/CyberZHG/keras-self-attention", "labels_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/1/labels{/name}", "comments_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/1/comments", "events_url": "https://api.github.com/repos/CyberZHG/keras-self-attention/issues/1/events", "html_url": "https://github.com/CyberZHG/keras-self-attention/issues/1", "id": 358106964, "node_id": "MDU6SXNzdWUzNTgxMDY5NjQ=", "number": 1, "title": "About Keras self-attention", "user": {"login": "yinmingwang", "id": 13360913, "node_id": "MDQ6VXNlcjEzMzYwOTEz", "avatar_url": "https://avatars1.githubusercontent.com/u/13360913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinmingwang", "html_url": "https://github.com/yinmingwang", "followers_url": "https://api.github.com/users/yinmingwang/followers", "following_url": "https://api.github.com/users/yinmingwang/following{/other_user}", "gists_url": "https://api.github.com/users/yinmingwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinmingwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinmingwang/subscriptions", "organizations_url": "https://api.github.com/users/yinmingwang/orgs", "repos_url": "https://api.github.com/users/yinmingwang/repos", "events_url": "https://api.github.com/users/yinmingwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yinmingwang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-07T15:12:52Z", "updated_at": "2018-09-10T06:41:27Z", "closed_at": "2018-09-10T06:41:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, Thank you for your contribution\uff0cI want to know if this is a self-attention or a general attention.I hope to get your reply.Thank you", "performed_via_github_app": null, "score": 1.0}]}