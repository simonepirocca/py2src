{"total_count": 381, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/databricks/koalas/issues/1710", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1710/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1710/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1710/events", "html_url": "https://github.com/databricks/koalas/issues/1710", "id": 679282042, "node_id": "MDU6SXNzdWU2NzkyODIwNDI=", "number": 1710, "title": "Docs Update: pivot_table does not have columns concatenated by \u201c_\u201d", "user": {"login": "nitishsinghal29", "id": 34052485, "node_id": "MDQ6VXNlcjM0MDUyNDg1", "avatar_url": "https://avatars2.githubusercontent.com/u/34052485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nitishsinghal29", "html_url": "https://github.com/nitishsinghal29", "followers_url": "https://api.github.com/users/nitishsinghal29/followers", "following_url": "https://api.github.com/users/nitishsinghal29/following{/other_user}", "gists_url": "https://api.github.com/users/nitishsinghal29/gists{/gist_id}", "starred_url": "https://api.github.com/users/nitishsinghal29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nitishsinghal29/subscriptions", "organizations_url": "https://api.github.com/users/nitishsinghal29/orgs", "repos_url": "https://api.github.com/users/nitishsinghal29/repos", "events_url": "https://api.github.com/users/nitishsinghal29/events{/privacy}", "received_events_url": "https://api.github.com/users/nitishsinghal29/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-14T16:55:05Z", "updated_at": "2020-08-14T23:54:47Z", "closed_at": "2020-08-14T23:54:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "As per the docs for pivot_table (v_1.1.0): https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.DataFrame.pivot_table.html#databricks.koalas.DataFrame.pivot_table\r\n\r\nI was trying to get the pivoted dataframe with final columns concatenated by \u201c\\_\u201d as the docs says in `aggfunc` parameter documentation. But the final dataframe has the multilevel columns instead of concatenated by \u201c_\u201d. \r\n\r\n\r\nThis is the example:\r\n```\r\n>> df = ks.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\r\n                         \"bar\", \"bar\", \"bar\", \"bar\"],\r\n                   \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\r\n                         \"one\", \"one\", \"two\", \"two\"],\r\n                   \"C\": [\"small\", \"large\", \"large\", \"small\",\r\n                         \"small\", \"large\", \"small\", \"small\",\r\n                         \"large\"],\r\n                   \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\r\n                   \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]},\r\n                  columns=['A', 'B', 'C', 'D', 'E'])\r\n>> df\r\n     A    B      C  D  E\r\n0  foo  one  small  1  2\r\n1  foo  one  large  2  4\r\n2  foo  one  large  2  5\r\n3  foo  two  small  3  5\r\n4  foo  two  small  3  6\r\n5  bar  one  large  4  6\r\n6  bar  one  small  5  8\r\n7  bar  two  small  6  9\r\n8  bar  two  large  7  9\r\n```\r\nOutput after pivot:\r\n```\r\ntable = df.pivot_table(index=['C'], columns=\"A\", values=['D', 'E'],\r\n                        aggfunc={'D': 'mean', 'E': 'sum'})\r\ntable.sort_index() \r\n         D             E\r\nA      bar       foo bar foo\r\nC\r\nlarge  5.5  2.000000  15   9\r\nsmall  5.5  2.333333  17  13\r\n```\r\nI think updated documentation aligned with the implementation would help!!\r\n\r\nWhat I was interested in the final columns concatenated by \u201c\\_\u201d in the pivoted dataframe, not sure if it's still supported ?? I have to manually do it after getting the pivot dataframe for now.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1690", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1690/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1690/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1690/events", "html_url": "https://github.com/databricks/koalas/issues/1690", "id": 669620880, "node_id": "MDU6SXNzdWU2Njk2MjA4ODA=", "number": 1690, "title": "reset_index is super slow", "user": {"login": "luistelmocosta", "id": 7799750, "node_id": "MDQ6VXNlcjc3OTk3NTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luistelmocosta", "html_url": "https://github.com/luistelmocosta", "followers_url": "https://api.github.com/users/luistelmocosta/followers", "following_url": "https://api.github.com/users/luistelmocosta/following{/other_user}", "gists_url": "https://api.github.com/users/luistelmocosta/gists{/gist_id}", "starred_url": "https://api.github.com/users/luistelmocosta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luistelmocosta/subscriptions", "organizations_url": "https://api.github.com/users/luistelmocosta/orgs", "repos_url": "https://api.github.com/users/luistelmocosta/repos", "events_url": "https://api.github.com/users/luistelmocosta/events{/privacy}", "received_events_url": "https://api.github.com/users/luistelmocosta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-31T09:36:10Z", "updated_at": "2020-08-04T01:35:44Z", "closed_at": "2020-08-04T01:35:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I am trying to load a dataset that does not have a particular index and I would like to know if my approach is correct.\r\n\r\nI read that due to koalas index issues we should always specify the index, so I did it like this:\r\n\r\n`df = df_tmp.to_koalas(index_col = ['A', 'B'])` since A and B are the columns that make a value unique.\r\n\r\nHowever I need to drop the index because I will need these columns further and I tried to reset_index():\r\n\r\n`df = df.sort_values('Date').reset_index()`\r\n\r\nBut this opperation takes 32 minutes which is completely infeasible. \r\n\r\nMy dataset comes from a csv file\r\n\r\n`df_tmp = spark.read.option(\"header\", \"true\").csv(data_lake_path+\"test.csv\")`\r\n\r\nAnd I added this line already:\r\n\r\n`ks.set_option('compute.default_index_type', 'distributed-sequence')`\r\n\r\nAny idea what I am doing wrong?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1687", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1687/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1687/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1687/events", "html_url": "https://github.com/databricks/koalas/issues/1687", "id": 668403907, "node_id": "MDU6SXNzdWU2Njg0MDM5MDc=", "number": 1687, "title": "Warnings while running Koalas operations in python environment", "user": {"login": "nitishsinghal29", "id": 34052485, "node_id": "MDQ6VXNlcjM0MDUyNDg1", "avatar_url": "https://avatars2.githubusercontent.com/u/34052485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nitishsinghal29", "html_url": "https://github.com/nitishsinghal29", "followers_url": "https://api.github.com/users/nitishsinghal29/followers", "following_url": "https://api.github.com/users/nitishsinghal29/following{/other_user}", "gists_url": "https://api.github.com/users/nitishsinghal29/gists{/gist_id}", "starred_url": "https://api.github.com/users/nitishsinghal29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nitishsinghal29/subscriptions", "organizations_url": "https://api.github.com/users/nitishsinghal29/orgs", "repos_url": "https://api.github.com/users/nitishsinghal29/repos", "events_url": "https://api.github.com/users/nitishsinghal29/events{/privacy}", "received_events_url": "https://api.github.com/users/nitishsinghal29/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1351987327, "node_id": "MDU6TGFiZWwxMzUxOTg3MzI3", "url": "https://api.github.com/repos/databricks/koalas/labels/not%20a%20koalas%20issue", "name": "not a koalas issue", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-30T06:15:35Z", "updated_at": "2020-07-30T06:20:51Z", "closed_at": "2020-07-30T06:20:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "I gave it a try using koalas for my Data Science project and implemented couple of usecases using koalas api. When I try running these logic in my python environment, it gives couple of warnings, as below:\r\n```\r\n20/07/30 01:39:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n20/07/30 01:39:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\r\n```\r\n\r\nBut when I run the same logic on my spark cluster it works fine without any warnings which is expected. \r\nI am wondering is it something that I need to take care of setting some options while running in Spark vs pure Python environment, so that I can avoid the above warnings.\r\n\r\nAny help would be appreciated.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1684", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1684/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1684/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1684/events", "html_url": "https://github.com/databricks/koalas/issues/1684", "id": 668040836, "node_id": "MDU6SXNzdWU2NjgwNDA4MzY=", "number": 1684, "title": "ValueError: no signature found for builtin type <class 'dict'> when trying to create object column with dicts ", "user": {"login": "paulochf", "id": 107470, "node_id": "MDQ6VXNlcjEwNzQ3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/107470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/paulochf", "html_url": "https://github.com/paulochf", "followers_url": "https://api.github.com/users/paulochf/followers", "following_url": "https://api.github.com/users/paulochf/following{/other_user}", "gists_url": "https://api.github.com/users/paulochf/gists{/gist_id}", "starred_url": "https://api.github.com/users/paulochf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/paulochf/subscriptions", "organizations_url": "https://api.github.com/users/paulochf/orgs", "repos_url": "https://api.github.com/users/paulochf/repos", "events_url": "https://api.github.com/users/paulochf/events{/privacy}", "received_events_url": "https://api.github.com/users/paulochf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-29T18:13:41Z", "updated_at": "2020-07-31T03:54:54Z", "closed_at": "2020-07-31T03:54:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello!\r\n\r\nI'm trying to get the same results as I would have with a pandas DataFrame, but I got this error instead.\r\n\r\nIs it expected? Any chance to get this working with koalas in the future using an official way?\r\n\r\nI'm using\r\npandas 1.1.0\r\nkoalas 1.1.0\r\nspark 2.4.5\r\n\r\nThanks!\r\n\r\n```python\r\ntest_pdf = pd.DataFrame([[1.*i, 2.*i, 3.*i] for i in range(3)], columns=[\"col_1\", \"col_2\", \"col_3\"])\r\ntest_kdf = ks.from_pandas(test_pdf)\r\n\r\n# pandas goes fine\r\ntest_pdf[\"new_col\"] = test_pdf.apply(dict, axis=1)\r\nprint(test_pdf)\r\n>>>    col_1  col_2  col_3                                     new_col\r\n>>> 0    0.0    0.0    0.0  {'col_1': 0.0, 'col_2': 0.0, 'col_3': 0.0}\r\n>>> 1    1.0    2.0    3.0  {'col_1': 1.0, 'col_2': 2.0, 'col_3': 3.0}\r\n>>> 2    2.0    4.0    6.0  {'col_1': 2.0, 'col_2': 4.0, 'col_3': 6.0}\r\n\r\n# koalas doesn't\r\ntest_kdf[\"new_col\"] = test_kdf.apply(dict, axis=1)\r\nprint(test_kdf)\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/lib/python3.7/inspect.py in getfullargspec(func)\r\n   1125                                        skip_bound_arg=False,\r\n-> 1126                                        sigcls=Signature)\r\n   1127     except Exception as ex:\r\n\r\n/usr/lib/python3.7/inspect.py in _signature_from_callable(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\r\n   2363                     raise ValueError(\r\n-> 2364                         'no signature found for builtin type {!r}'.format(obj))\r\n   2365 \r\n\r\nValueError: no signature found for builtin type <class 'dict'>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<command-6884238> in <module>\r\n      6 # display(test_pdf)\r\n      7 \r\n----> 8 test_kdf[\"new_col\"] = test_kdf.apply(dict, axis=1)\r\n      9 print(test_kdf)\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-88f94e2c-4907-4af0-adac-903a130015df/lib/python3.7/site-packages/databricks/koalas/usage_logging/__init__.py in wrapper(*args, **kwargs)\r\n    178             start = time.perf_counter()\r\n    179             try:\r\n--> 180                 res = func(*args, **kwargs)\r\n    181                 logger.log_success(\r\n    182                     class_name, function_name, time.perf_counter() - start, signature\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-88f94e2c-4907-4af0-adac-903a130015df/lib/python3.7/site-packages/databricks/koalas/frame.py in apply(self, func, axis, args, **kwds)\r\n   2313         axis = validate_axis(axis)\r\n   2314         should_return_series = False\r\n-> 2315         spec = inspect.getfullargspec(func)\r\n   2316         return_sig = spec.annotations.get(\"return\", None)\r\n   2317         should_infer_schema = return_sig is None\r\n\r\n/usr/lib/python3.7/inspect.py in getfullargspec(func)\r\n   1130         # else. So to be fully backwards compatible, we catch all\r\n   1131         # possible exceptions here, and reraise a TypeError.\r\n-> 1132         raise TypeError('unsupported callable') from ex\r\n   1133 \r\n   1134     args = []\r\n\r\nTypeError: unsupported callable\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1680", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1680/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1680/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1680/events", "html_url": "https://github.com/databricks/koalas/issues/1680", "id": 667504683, "node_id": "MDU6SXNzdWU2Njc1MDQ2ODM=", "number": 1680, "title": "Use PyArrow 1.0.0 in CI.", "user": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-29T02:43:08Z", "updated_at": "2020-07-30T01:21:36Z", "closed_at": "2020-07-30T01:21:36Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Now that PyArrow 1.0.0 is available. We should add the test env with PyArrow 1.0.0.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1679", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1679/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1679/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1679/events", "html_url": "https://github.com/databricks/koalas/issues/1679", "id": 667443942, "node_id": "MDU6SXNzdWU2Njc0NDM5NDI=", "number": 1679, "title": "Unable to install koalas on lubuntu--segmentation fault", "user": {"login": "harnalashok", "id": 47495816, "node_id": "MDQ6VXNlcjQ3NDk1ODE2", "avatar_url": "https://avatars3.githubusercontent.com/u/47495816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harnalashok", "html_url": "https://github.com/harnalashok", "followers_url": "https://api.github.com/users/harnalashok/followers", "following_url": "https://api.github.com/users/harnalashok/following{/other_user}", "gists_url": "https://api.github.com/users/harnalashok/gists{/gist_id}", "starred_url": "https://api.github.com/users/harnalashok/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harnalashok/subscriptions", "organizations_url": "https://api.github.com/users/harnalashok/orgs", "repos_url": "https://api.github.com/users/harnalashok/repos", "events_url": "https://api.github.com/users/harnalashok/events{/privacy}", "received_events_url": "https://api.github.com/users/harnalashok/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1351987327, "node_id": "MDU6TGFiZWwxMzUxOTg3MzI3", "url": "https://api.github.com/repos/databricks/koalas/labels/not%20a%20koalas%20issue", "name": "not a koalas issue", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-28T23:26:53Z", "updated_at": "2020-08-12T06:17:03Z", "closed_at": "2020-08-03T03:10:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "On ubuntu, I have no problem in installing koalas and invoking the library as:\r\n\r\n`import` databricks.koalas as ks\r\n\r\nBut, on lubuntu (version 18, 64bit), lightweight brother of ubuntu, conda installation is successful. But when I open ipython and call koalas as above, ipython breaks and I get a message of 'Segmentation fault'. I have tried to install koalas through pip also but to no avail.  I have tried this installation process on three different Virtual Machines (of lubuntu 64bit) but same result appears each time.\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1674", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1674/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1674/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1674/events", "html_url": "https://github.com/databricks/koalas/issues/1674", "id": 663698677, "node_id": "MDU6SXNzdWU2NjM2OTg2Nzc=", "number": 1674, "title": "Upgraded koalas and Spark, the same datasets but out of memoy", "user": {"login": "Tom-Deng", "id": 17919002, "node_id": "MDQ6VXNlcjE3OTE5MDAy", "avatar_url": "https://avatars2.githubusercontent.com/u/17919002?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tom-Deng", "html_url": "https://github.com/Tom-Deng", "followers_url": "https://api.github.com/users/Tom-Deng/followers", "following_url": "https://api.github.com/users/Tom-Deng/following{/other_user}", "gists_url": "https://api.github.com/users/Tom-Deng/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tom-Deng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tom-Deng/subscriptions", "organizations_url": "https://api.github.com/users/Tom-Deng/orgs", "repos_url": "https://api.github.com/users/Tom-Deng/repos", "events_url": "https://api.github.com/users/Tom-Deng/events{/privacy}", "received_events_url": "https://api.github.com/users/Tom-Deng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-22T11:45:14Z", "updated_at": "2020-08-05T08:31:35Z", "closed_at": "2020-08-05T08:31:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "Dataset: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HG7NV7\r\n\r\nWe used the 2005, 2006, 2007 dataset. The size of these datasets are about 2GB.\r\n\r\nIn the same configuration\uff0cwhen we handle these datasets with koalas 0.32 and spark 2.4.4, the resut is nomal.\r\n![image](https://user-images.githubusercontent.com/17919002/88172197-2a2a7980-cc53-11ea-8866-8ba341c48c6d.png)\r\nWhen we handle these datasets with koalas 1.1.0 and spark 3.0.0, the result is abnormal.\r\n![image](https://user-images.githubusercontent.com/17919002/88172250-3dd5e000-cc53-11ea-9f9e-d3ba55ca3d91.png)\r\n\r\nSpecific error messages:\r\nERROR:root:Exception while sending command.\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1207, in send_command\r\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\r\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1033, in send_command\r\n    response = connection.send_command(command)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1212, in send_command\r\n    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\r\npy4j.protocol.Py4JNetworkError: Error while receiving\r\n/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py:134: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\r\n  An error occurred while calling o1304.getResult\r\n  warnings.warn(msg)\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\r\n    r = method(self, *args, **kwargs)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 702, in __call__\r\n    printer.pretty(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 394, in pretty\r\n    return _repr_pprint(obj, self, cycle)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 684, in _repr_pprint\r\n    output = repr(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9965, in __repr__\r\n    pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9957, in _get_or_create_repr_pandas_cache\r\n    self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9953, in _to_internal_pandas\r\n    return self._internal.to_pandas_frame\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py\", line 477, in wrapped_lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py\", line 743, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 108, in toPandas\r\n    batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 244, in _collect_as_arrow\r\n    jsocket_auth_server.getResult()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 131, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py\", line 336, in get_return_value\r\n    format(target_id, \".\", name))\r\npy4j.protocol.Py4JError: An error occurred while calling o1304.getResult\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'Py4JError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\r\n    r = method(self, *args, **kwargs)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 702, in __call__\r\n    printer.pretty(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 394, in pretty\r\n    return _repr_pprint(obj, self, cycle)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 684, in _repr_pprint\r\n    output = repr(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9965, in __repr__\r\n    pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9957, in _get_or_create_repr_pandas_cache\r\n    self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9953, in _to_internal_pandas\r\n    return self._internal.to_pandas_frame\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py\", line 477, in wrapped_lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py\", line 743, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 108, in toPandas\r\n    batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 244, in _collect_as_arrow\r\n    jsocket_auth_server.getResult()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 131, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py\", line 336, in get_return_value\r\n    format(target_id, \".\", name))\r\npy4j.protocol.Py4JError: An error occurred while calling o1304.getResult\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'Py4JError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\r\n    r = method(self, *args, **kwargs)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 702, in __call__\r\n    printer.pretty(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 394, in pretty\r\n    return _repr_pprint(obj, self, cycle)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 684, in _repr_pprint\r\n    output = repr(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9965, in __repr__\r\n    pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9957, in _get_or_create_repr_pandas_cache\r\n    self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9953, in _to_internal_pandas\r\n    return self._internal.to_pandas_frame\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py\", line 477, in wrapped_lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py\", line 743, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 108, in toPandas\r\n    batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 244, in _collect_as_arrow\r\n    jsocket_auth_server.getResult()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 131, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py\", line 336, in get_return_value\r\n    format(target_id, \".\", name))\r\npy4j.protocol.Py4JError: An error occurred while calling o1304.getResult\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'Py4JError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\r\n    r = method(self, *args, **kwargs)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 702, in __call__\r\n    printer.pretty(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 394, in pretty\r\n    return _repr_pprint(obj, self, cycle)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 684, in _repr_pprint\r\n    output = repr(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9965, in __repr__\r\n    pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9957, in _get_or_create_repr_pandas_cache\r\n    self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9953, in _to_internal_pandas\r\n    return self._internal.to_pandas_frame\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py\", line 477, in wrapped_lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py\", line 743, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 108, in toPandas\r\n    batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 244, in _collect_as_arrow\r\n    jsocket_auth_server.getResult()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 131, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py\", line 336, in get_return_value\r\n    format(target_id, \".\", name))\r\npy4j.protocol.Py4JError: An error occurred while calling o1304.getResult\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'Py4JError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\r\n    r = method(self, *args, **kwargs)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 702, in __call__\r\n    printer.pretty(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 394, in pretty\r\n    return _repr_pprint(obj, self, cycle)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 684, in _repr_pprint\r\n    output = repr(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9965, in __repr__\r\n    pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9957, in _get_or_create_repr_pandas_cache\r\n    self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9953, in _to_internal_pandas\r\n    return self._internal.to_pandas_frame\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py\", line 477, in wrapped_lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py\", line 743, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 108, in toPandas\r\n    batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 244, in _collect_as_arrow\r\n    jsocket_auth_server.getResult()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 131, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py\", line 336, in get_return_value\r\n    format(target_id, \".\", name))\r\npy4j.protocol.Py4JError: An error occurred while calling o1304.getResult\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'Py4JError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\r\n    r = method(self, *args, **kwargs)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 702, in __call__\r\n    printer.pretty(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 394, in pretty\r\n    return _repr_pprint(obj, self, cycle)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 684, in _repr_pprint\r\n    output = repr(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9965, in __repr__\r\n    pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9957, in _get_or_create_repr_pandas_cache\r\n    self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9953, in _to_internal_pandas\r\n    return self._internal.to_pandas_frame\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py\", line 477, in wrapped_lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py\", line 743, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 108, in toPandas\r\n    batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 244, in _collect_as_arrow\r\n    jsocket_auth_server.getResult()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 131, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py\", line 336, in get_return_value\r\n    format(target_id, \".\", name))\r\npy4j.protocol.Py4JError: An error occurred while calling o1304.getResult\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'Py4JError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 241, in _collect_as_arrow\r\n    results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 60, in load_stream\r\n    for batch in self.serializer.load_stream(stream):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py\", line 98, in load_stream\r\n    reader = pa.ipc.open_stream(stream)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 146, in open_stream\r\n    return RecordBatchStreamReader(source)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py\", line 62, in __init__\r\n    self._open(source)\r\n  File \"pyarrow/ipc.pxi\", line 360, in pyarrow.lib._RecordBatchStreamReader._open\r\n  File \"pyarrow/error.pxi\", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\r\n    r = method(self, *args, **kwargs)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py\", line 702, in __call__\r\n    printer.pretty(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 394, in pretty\r\n    return _repr_pprint(obj, self, cycle)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py\", line 684, in _repr_pprint\r\n    output = repr(obj)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9965, in __repr__\r\n    pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9957, in _get_or_create_repr_pandas_cache\r\n    self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py\", line 9953, in _to_internal_pandas\r\n    return self._internal.to_pandas_frame\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py\", line 477, in wrapped_lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py\", line 743, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 108, in toPandas\r\n    batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py\", line 244, in _collect_as_arrow\r\n    jsocket_auth_server.getResult()\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1305, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py\", line 131, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py\", line 336, in get_return_value\r\n    format(target_id, \".\", name))\r\npy4j.protocol.Py4JError: An error occurred while calling o1304.getResult\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'Py4JError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py in _collect_as_arrow(self)\r\n    240         try:\r\n--> 241             results = list(_load_from_socket((port, auth_secret), ArrowCollectSerializer()))\r\n    242         finally:\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py in load_stream(self, stream)\r\n     59         # load the batches\r\n---> 60         for batch in self.serializer.load_stream(stream):\r\n     61             yield batch\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/serializers.py in load_stream(self, stream)\r\n     97         import pyarrow as pa\r\n---> 98         reader = pa.ipc.open_stream(stream)\r\n     99         for batch in reader:\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py in open_stream(source)\r\n    145     \"\"\"\r\n--> 146     return RecordBatchStreamReader(source)\r\n    147 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.py in __init__(self, source)\r\n     61     def __init__(self, source):\r\n---> 62         self._open(source)\r\n     63 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/ipc.pxi in pyarrow.lib._RecordBatchStreamReader._open()\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Tried reading schema message, was null or length 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nPy4JError                                 Traceback (most recent call last)\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    700                 type_pprinters=self.type_printers,\r\n    701                 deferred_pprinters=self.deferred_printers)\r\n--> 702             printer.pretty(obj)\r\n    703             printer.flush()\r\n    704             return stream.getvalue()\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    392                         if cls is not object \\\r\n    393                                 and callable(cls.__dict__.get('__repr__')):\r\n--> 394                             return _repr_pprint(obj, self, cycle)\r\n    395 \r\n    396             return _default_pprint(obj, self, cycle)\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    682     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\r\n    683     # Find newlines and replace them with p.break_()\r\n--> 684     output = repr(obj)\r\n    685     lines = output.splitlines()\r\n    686     with p.group():\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py in __repr__(self)\r\n   9963             return self._to_internal_pandas().to_string()\r\n   9964 \r\n-> 9965         pdf = self._get_or_create_repr_pandas_cache(max_display_count)\r\n   9966         pdf_length = len(pdf)\r\n   9967         pdf = pdf.iloc[:max_display_count]\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py in _get_or_create_repr_pandas_cache(self, n)\r\n   9955     def _get_or_create_repr_pandas_cache(self, n):\r\n   9956         if not hasattr(self, \"_repr_pandas_cache\") or n not in self._repr_pandas_cache:\r\n-> 9957             self._repr_pandas_cache = {n: self.head(n + 1)._to_internal_pandas()}\r\n   9958         return self._repr_pandas_cache[n]\r\n   9959 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py in _to_internal_pandas(self)\r\n   9951         This method is for internal use only.\r\n   9952         \"\"\"\r\n-> 9953         return self._internal.to_pandas_frame\r\n   9954 \r\n   9955     def _get_or_create_repr_pandas_cache(self, n):\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py in wrapped_lazy_property(self)\r\n    475     def wrapped_lazy_property(self):\r\n    476         if not hasattr(self, attr_name):\r\n--> 477             setattr(self, attr_name, fn(self))\r\n    478         return getattr(self, attr_name)\r\n    479 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/internal.py in to_pandas_frame(self)\r\n    741         \"\"\" Return as pandas DataFrame. \"\"\"\r\n    742         sdf = self.to_internal_spark_frame\r\n--> 743         pdf = sdf.toPandas()\r\n    744         if len(pdf) == 0 and len(sdf.schema) > 0:\r\n    745             pdf = pdf.astype(\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py in toPandas(self)\r\n    106                     # Rename columns to avoid duplicated column names.\r\n    107                     tmp_column_names = ['col_{}'.format(i) for i in range(len(self.columns))]\r\n--> 108                     batches = self.toDF(*tmp_column_names)._collect_as_arrow()\r\n    109                     if len(batches) > 0:\r\n    110                         table = pyarrow.Table.from_batches(batches)\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/pandas/conversion.py in _collect_as_arrow(self)\r\n    242         finally:\r\n    243             # Join serving thread and raise any exceptions from collectAsArrowToPython\r\n--> 244             jsocket_auth_server.getResult()\r\n    245 \r\n    246         # Separate RecordBatches from batch order indices in results\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)\r\n   1303         answer = self.gateway_client.send_command(command)\r\n   1304         return_value = get_return_value(\r\n-> 1305             answer, self.gateway_client, self.target_id, self.name)\r\n   1306 \r\n   1307         for temp_arg in temp_args:\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\r\n    129     def deco(*a, **kw):\r\n    130         try:\r\n--> 131             return f(*a, **kw)\r\n    132         except py4j.protocol.Py4JJavaError as e:\r\n    133             converted = convert_exception(e.java_exception)\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\r\n    334             raise Py4JError(\r\n    335                 \"An error occurred while calling {0}{1}{2}\".\r\n--> 336                 format(target_id, \".\", name))\r\n    337     else:\r\n    338         type = answer[1]\r\n\r\nPy4JError: An error occurred while calling o1304.getResult\r\nERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\r\n    connection = self.deque.pop()\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/dengxinhuan/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1115, in start\r\n    self.socket.connect((self.address, self.port))\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in _get_connection(self)\r\n    976         try:\r\n--> 977             connection = self.deque.pop()\r\n    978         except IndexError:\r\n\r\nIndexError: pop from an empty deque\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConnectionRefusedError                    Traceback (most recent call last)\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in start(self)\r\n   1114         try:\r\n-> 1115             self.socket.connect((self.address, self.port))\r\n   1116             self.stream = self.socket.makefile(\"rb\")\r\n\r\nConnectionRefusedError: [Errno 111] Connection refused\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nPy4JNetworkError                          Traceback (most recent call last)\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    343             method = get_real_method(obj, self.print_method)\r\n    344             if method is not None:\r\n--> 345                 return method()\r\n    346             return None\r\n    347         else:\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/frame.py in _repr_html_(self)\r\n   9979 \r\n   9980     def _repr_html_(self):\r\n-> 9981         max_display_count = get_option(\"display.max_rows\")\r\n   9982         # pandas 0.25.1 has a regression about HTML representation so 'bold_rows'\r\n   9983         # has to be set as False explicitly. See https://github.com/pandas-dev/pandas/issues/28204\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/config.py in get_option(key, default)\r\n    299     _options_dict[key].validate(default)\r\n    300 \r\n--> 301     return json.loads(default_session().conf.get(_key_format(key), default=json.dumps(default)))\r\n    302 \r\n    303 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/databricks/koalas/utils.py in default_session(conf)\r\n    379     # configuration. This is needed with Spark 3.0+.\r\n    380     builder.config(\"spark.sql.analyzer.failAmbiguousSelfJoin\", False)\r\n--> 381     session = builder.getOrCreate()\r\n    382 \r\n    383     if not should_use_legacy_ipc:\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/pyspark/sql/session.py in getOrCreate(self)\r\n    189                     session = SparkSession(sc)\r\n    190                 for key, value in self._options.items():\r\n--> 191                     session._jsparkSession.sessionState().conf().setConfString(key, value)\r\n    192                 return session\r\n    193 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)\r\n   1301             proto.END_COMMAND_PART\r\n   1302 \r\n-> 1303         answer = self.gateway_client.send_command(command)\r\n   1304         return_value = get_return_value(\r\n   1305             answer, self.gateway_client, self.target_id, self.name)\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in send_command(self, command, retry, binary)\r\n   1029          if `binary` is `True`.\r\n   1030         \"\"\"\r\n-> 1031         connection = self._get_connection()\r\n   1032         try:\r\n   1033             response = connection.send_command(command)\r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in _get_connection(self)\r\n    977             connection = self.deque.pop()\r\n    978         except IndexError:\r\n--> 979             connection = self._create_connection()\r\n    980         return connection\r\n    981 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in _create_connection(self)\r\n    983         connection = GatewayConnection(\r\n    984             self.gateway_parameters, self.gateway_property)\r\n--> 985         connection.start()\r\n    986         return connection\r\n    987 \r\n\r\n~/miniconda3/envs/blackhole_qatest0507/lib/python3.6/site-packages/py4j/java_gateway.py in start(self)\r\n   1125                 \"server ({0}:{1})\".format(self.address, self.port)\r\n   1126             logger.exception(msg)\r\n-> 1127             raise Py4JNetworkError(msg, e)\r\n   1128 \r\n   1129     def _authenticate_connection(self):\r\n\r\nPy4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:50804)\r\n\r\nConfiguration information:\r\n![image](https://user-images.githubusercontent.com/17919002/88172430-8e4d3d80-cc53-11ea-934e-8dfaf8cccdda.png)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1673", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1673/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1673/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1673/events", "html_url": "https://github.com/databricks/koalas/issues/1673", "id": 663576536, "node_id": "MDU6SXNzdWU2NjM1NzY1MzY=", "number": 1673, "title": "Is DataFame have union or some equivalent method?", "user": {"login": "CaoTianze", "id": 10096919, "node_id": "MDQ6VXNlcjEwMDk2OTE5", "avatar_url": "https://avatars0.githubusercontent.com/u/10096919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CaoTianze", "html_url": "https://github.com/CaoTianze", "followers_url": "https://api.github.com/users/CaoTianze/followers", "following_url": "https://api.github.com/users/CaoTianze/following{/other_user}", "gists_url": "https://api.github.com/users/CaoTianze/gists{/gist_id}", "starred_url": "https://api.github.com/users/CaoTianze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CaoTianze/subscriptions", "organizations_url": "https://api.github.com/users/CaoTianze/orgs", "repos_url": "https://api.github.com/users/CaoTianze/repos", "events_url": "https://api.github.com/users/CaoTianze/events{/privacy}", "received_events_url": "https://api.github.com/users/CaoTianze/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-22T08:29:58Z", "updated_at": "2020-07-22T14:45:43Z", "closed_at": "2020-07-22T14:44:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "In **pyspark.sql.DataFrame**, it have 3 methods : union, unionAll, unionByName.But i can't find some equivalent method in databricks.koalas.DataFrame.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1666", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1666/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1666/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1666/events", "html_url": "https://github.com/databricks/koalas/issues/1666", "id": 661906972, "node_id": "MDU6SXNzdWU2NjE5MDY5NzI=", "number": 1666, "title": "df.to_parquet doesn't create partitions", "user": {"login": "FredericJames", "id": 6051126, "node_id": "MDQ6VXNlcjYwNTExMjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6051126?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FredericJames", "html_url": "https://github.com/FredericJames", "followers_url": "https://api.github.com/users/FredericJames/followers", "following_url": "https://api.github.com/users/FredericJames/following{/other_user}", "gists_url": "https://api.github.com/users/FredericJames/gists{/gist_id}", "starred_url": "https://api.github.com/users/FredericJames/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FredericJames/subscriptions", "organizations_url": "https://api.github.com/users/FredericJames/orgs", "repos_url": "https://api.github.com/users/FredericJames/repos", "events_url": "https://api.github.com/users/FredericJames/events{/privacy}", "received_events_url": "https://api.github.com/users/FredericJames/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-20T14:44:36Z", "updated_at": "2020-07-21T13:57:19Z", "closed_at": "2020-07-21T13:57:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "My environment: Databrick platform: runtime 7.0 ML\r\nKoalas: 1.0.1\r\n\r\nI'm trying to write parquets from a koalas dataframe to S3 with partitions. The partitions are not created (I tried with a single or multiple partition cols).\r\nIf I'm using the pyspark API, the partitions are created.\r\n\r\ncode:\r\nno partition created\r\ndf.to_parquet(path='s3://{bucket}/{Path_to_data}, mode='overwrite', compression='gzip', partition_cols=['year','month','day'])\r\n\r\nPartition created\r\ndf.to_spark().write.mode('overwrite').partitionBy('year', 'month', 'day').parquet('s3://{bucket}/{Path_to_data}')\r\n\r\nI would like to have the possibilities to use the partitions in koalas in the same way I'm doing it in spark.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1665", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1665/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1665/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1665/events", "html_url": "https://github.com/databricks/koalas/issues/1665", "id": 661546461, "node_id": "MDU6SXNzdWU2NjE1NDY0NjE=", "number": 1665, "title": "Throw an PythonException  when displaying the results returned by the koalas dataframe operation with iloc", "user": {"login": "Tom-Deng", "id": 17919002, "node_id": "MDQ6VXNlcjE3OTE5MDAy", "avatar_url": "https://avatars2.githubusercontent.com/u/17919002?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tom-Deng", "html_url": "https://github.com/Tom-Deng", "followers_url": "https://api.github.com/users/Tom-Deng/followers", "following_url": "https://api.github.com/users/Tom-Deng/following{/other_user}", "gists_url": "https://api.github.com/users/Tom-Deng/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tom-Deng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tom-Deng/subscriptions", "organizations_url": "https://api.github.com/users/Tom-Deng/orgs", "repos_url": "https://api.github.com/users/Tom-Deng/repos", "events_url": "https://api.github.com/users/Tom-Deng/events{/privacy}", "received_events_url": "https://api.github.com/users/Tom-Deng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-20T08:44:39Z", "updated_at": "2020-08-11T11:57:49Z", "closed_at": "2020-08-11T11:57:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "According to https://www.kaggle.com/c/ashrae-energy-prediction/overview\uff0cI want to finish the data analysis with koalas. When I operate dataframe with iloc\uff0cthe returned result can not only be displayed but also calculated.\r\n\r\n![image](https://user-images.githubusercontent.com/17919002/87917546-bac45680-caa7-11ea-956c-2e84bac266c0.png)\r\n\r\nfull code\uff1a\r\n[input_less_dataframe.tar.gz](https://github.com/databricks/koalas/files/4946508/input_less_dataframe.tar.gz)\r\n\r\n\r\nSoftware versions\r\npython 3.6.9\r\nkoalas 1.0.1\r\npyspark 3.0.0\r\nspark 3.0.0\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1664", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1664/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1664/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1664/events", "html_url": "https://github.com/databricks/koalas/issues/1664", "id": 661500236, "node_id": "MDU6SXNzdWU2NjE1MDAyMzY=", "number": 1664, "title": "Throw an AnalysisException when displaying the results returned by the koalas series operation with numpy ", "user": {"login": "Tom-Deng", "id": 17919002, "node_id": "MDQ6VXNlcjE3OTE5MDAy", "avatar_url": "https://avatars2.githubusercontent.com/u/17919002?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tom-Deng", "html_url": "https://github.com/Tom-Deng", "followers_url": "https://api.github.com/users/Tom-Deng/followers", "following_url": "https://api.github.com/users/Tom-Deng/following{/other_user}", "gists_url": "https://api.github.com/users/Tom-Deng/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tom-Deng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tom-Deng/subscriptions", "organizations_url": "https://api.github.com/users/Tom-Deng/orgs", "repos_url": "https://api.github.com/users/Tom-Deng/repos", "events_url": "https://api.github.com/users/Tom-Deng/events{/privacy}", "received_events_url": "https://api.github.com/users/Tom-Deng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-20T07:54:34Z", "updated_at": "2020-07-21T02:17:30Z", "closed_at": "2020-07-21T02:17:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "According to https://www.kaggle.com/c/ashrae-energy-prediction/overview\uff0cI want to finish the data analysis with koalas. When I operate series with numpy\uff0cthe returned result can not only be displayed but also calculated.\r\n\r\n![image](https://user-images.githubusercontent.com/17919002/87913166-31aa2100-caa1-11ea-8108-d594c3e54f46.png)\r\n\r\nfull code\uff1a\r\n[input_less.tar.gz](https://github.com/databricks/koalas/files/4946271/input_less.tar.gz)\r\n\r\n\r\nSoftware versions\r\npython 3.6.9\r\nkoalas 1.0.1\r\npyspark 3.0.0\r\nspark 3.0.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1663", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1663/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1663/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1663/events", "html_url": "https://github.com/databricks/koalas/issues/1663", "id": 661169323, "node_id": "MDU6SXNzdWU2NjExNjkzMjM=", "number": 1663, "title": "java.lang.IllegalArgumentException when using toPandas", "user": {"login": "polocorona", "id": 51968716, "node_id": "MDQ6VXNlcjUxOTY4NzE2", "avatar_url": "https://avatars2.githubusercontent.com/u/51968716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/polocorona", "html_url": "https://github.com/polocorona", "followers_url": "https://api.github.com/users/polocorona/followers", "following_url": "https://api.github.com/users/polocorona/following{/other_user}", "gists_url": "https://api.github.com/users/polocorona/gists{/gist_id}", "starred_url": "https://api.github.com/users/polocorona/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/polocorona/subscriptions", "organizations_url": "https://api.github.com/users/polocorona/orgs", "repos_url": "https://api.github.com/users/polocorona/repos", "events_url": "https://api.github.com/users/polocorona/events{/privacy}", "received_events_url": "https://api.github.com/users/polocorona/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-19T23:42:34Z", "updated_at": "2020-07-21T01:43:59Z", "closed_at": "2020-07-20T16:22:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! I bootstrapped an EMR 5.30.1 cluster using koalas with conda. I'm getting in a dead-end here with the following error whenever trying to convert a koalas dataframe to a pandas dataframe after computing some operations. Here's the full traceback:\r\n\r\n```\r\n/usr/lib/spark/python/pyspark/sql/dataframe.py:2148: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.fallback.enabled' does not have an effect on failures in the middle of computation.\r\n  An error occurred while calling o988.getResult.\r\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\r\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:874)\r\n\tat org.apache.spark.api.python.PythonServer.getResult(PythonRDD.scala:870)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 4 times, most recent failure: Lost task 0.3 in stage 10.0 (TID 10, ip-172-31-76-239.ec2.internal, executor 3): java.lang.IllegalArgumentException\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\r\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\r\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3322)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsArrowToPython$1$$anonfun$apply$17.apply(Dataset.scala:3291)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply$mcV$sp(PythonRDD.scala:456)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7$$anonfun$apply$3.apply(PythonRDD.scala:456)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:457)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$7.apply(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:994)\r\n\tat org.apache.spark.api.python.SocketFuncServer.handleConnection(PythonRDD.scala:988)\r\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11$$anonfun$apply$9.apply(PythonRDD.scala:853)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:853)\r\n\tat org.apache.spark.api.python.PythonServer$$anonfun$11.apply(PythonRDD.scala:852)\r\n\tat org.apache.spark.api.python.PythonServer$$anon$1.run(PythonRDD.scala:908)\r\nCaused by: java.lang.IllegalArgumentException\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\r\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\r\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n  warnings.warn(msg)\r\n```\r\nany ideas on how to fix it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1661", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1661/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1661/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1661/events", "html_url": "https://github.com/databricks/koalas/issues/1661", "id": 659569456, "node_id": "MDU6SXNzdWU2NTk1Njk0NTY=", "number": 1661, "title": "Simplify the Plot Assessors", "user": {"login": "DumbMachine", "id": 23381512, "node_id": "MDQ6VXNlcjIzMzgxNTEy", "avatar_url": "https://avatars1.githubusercontent.com/u/23381512?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DumbMachine", "html_url": "https://github.com/DumbMachine", "followers_url": "https://api.github.com/users/DumbMachine/followers", "following_url": "https://api.github.com/users/DumbMachine/following{/other_user}", "gists_url": "https://api.github.com/users/DumbMachine/gists{/gist_id}", "starred_url": "https://api.github.com/users/DumbMachine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DumbMachine/subscriptions", "organizations_url": "https://api.github.com/users/DumbMachine/orgs", "repos_url": "https://api.github.com/users/DumbMachine/repos", "events_url": "https://api.github.com/users/DumbMachine/events{/privacy}", "received_events_url": "https://api.github.com/users/DumbMachine/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-17T19:38:47Z", "updated_at": "2020-08-12T19:01:05Z", "closed_at": "2020-08-12T19:01:05Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Current `plot` accessors have separate implementations for both `Series` and `DataFrame` despite having a very similar API structure.  Since there exist separate functions for both `plot_series` and `plot_frame`, a single class can handle plots for both.\r\n\r\nI will open a PR for to solve this issue, ([cross ref](https://github.com/databricks/koalas/pull/1639#issuecomment-659824221))", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1657", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1657/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1657/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1657/events", "html_url": "https://github.com/databricks/koalas/issues/1657", "id": 657826478, "node_id": "MDU6SXNzdWU2NTc4MjY0Nzg=", "number": 1657, "title": "How to create a DataFrame with column name case-sensitive ", "user": {"login": "shengjh", "id": 46514371, "node_id": "MDQ6VXNlcjQ2NTE0Mzcx", "avatar_url": "https://avatars0.githubusercontent.com/u/46514371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shengjh", "html_url": "https://github.com/shengjh", "followers_url": "https://api.github.com/users/shengjh/followers", "following_url": "https://api.github.com/users/shengjh/following{/other_user}", "gists_url": "https://api.github.com/users/shengjh/gists{/gist_id}", "starred_url": "https://api.github.com/users/shengjh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shengjh/subscriptions", "organizations_url": "https://api.github.com/users/shengjh/orgs", "repos_url": "https://api.github.com/users/shengjh/repos", "events_url": "https://api.github.com/users/shengjh/events{/privacy}", "received_events_url": "https://api.github.com/users/shengjh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-16T03:59:35Z", "updated_at": "2020-07-16T13:33:45Z", "closed_at": "2020-07-16T13:33:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hello, I find it is not easy to create a case-sensitive columns  DataFrame.\r\n```py\r\ndata = {'a':[1,2], 'A': [2,3]}\r\nkdf = ks.DataFrame(data)\r\n...\r\npyspark.sql.utils.AnalysisException: Reference 'a' is ambiguous, could be: a, a.;\r\n```\r\nI googled this error and i should to set pyspark option `spark.sql.caseSensitive=true`.\r\nSo I tried `ks.set_option`, but it failed by `_check_option`.\r\nIs there any way to set pyspark option? Or how to create a case-sensitive DataFrame?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1653", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1653/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1653/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1653/events", "html_url": "https://github.com/databricks/koalas/issues/1653", "id": 657527675, "node_id": "MDU6SXNzdWU2NTc1Mjc2NzU=", "number": 1653, "title": "Tutorial notebook isn't loading right now", "user": {"login": "boatcoder", "id": 18578897, "node_id": "MDQ6VXNlcjE4NTc4ODk3", "avatar_url": "https://avatars1.githubusercontent.com/u/18578897?v=4", "gravatar_id": "", "url": "https://api.github.com/users/boatcoder", "html_url": "https://github.com/boatcoder", "followers_url": "https://api.github.com/users/boatcoder/followers", "following_url": "https://api.github.com/users/boatcoder/following{/other_user}", "gists_url": "https://api.github.com/users/boatcoder/gists{/gist_id}", "starred_url": "https://api.github.com/users/boatcoder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/boatcoder/subscriptions", "organizations_url": "https://api.github.com/users/boatcoder/orgs", "repos_url": "https://api.github.com/users/boatcoder/repos", "events_url": "https://api.github.com/users/boatcoder/events{/privacy}", "received_events_url": "https://api.github.com/users/boatcoder/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-15T17:30:01Z", "updated_at": "2020-07-15T17:56:42Z", "closed_at": "2020-07-15T17:56:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "Says to check for logs, but that window/panel is blank.\r\n\r\nReally interested in kicking the tires....\r\n\r\nI've tried it twice and gave the first one 30 minutes (because I was in a meeting) and it never got initialized.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1652", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1652/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1652/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1652/events", "html_url": "https://github.com/databricks/koalas/issues/1652", "id": 657277727, "node_id": "MDU6SXNzdWU2NTcyNzc3Mjc=", "number": 1652, "title": "StackOverflowError when using  ks.append", "user": {"login": "Lukas012", "id": 2796837, "node_id": "MDQ6VXNlcjI3OTY4Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/2796837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lukas012", "html_url": "https://github.com/Lukas012", "followers_url": "https://api.github.com/users/Lukas012/followers", "following_url": "https://api.github.com/users/Lukas012/following{/other_user}", "gists_url": "https://api.github.com/users/Lukas012/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lukas012/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lukas012/subscriptions", "organizations_url": "https://api.github.com/users/Lukas012/orgs", "repos_url": "https://api.github.com/users/Lukas012/repos", "events_url": "https://api.github.com/users/Lukas012/events{/privacy}", "received_events_url": "https://api.github.com/users/Lukas012/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1351987327, "node_id": "MDU6TGFiZWwxMzUxOTg3MzI3", "url": "https://api.github.com/repos/databricks/koalas/labels/not%20a%20koalas%20issue", "name": "not a koalas issue", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-15T11:29:53Z", "updated_at": "2020-08-10T15:51:15Z", "closed_at": "2020-08-10T15:51:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\n\r\nos.environ[\"PYSPARK_PYTHON\"] = \"/opt/conda/envs/spark3/bin/python\"\r\nos.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages io.delta:delta-core_2.12:0.7.0 pyspark-shell \"\r\nspark = SparkSession \\\r\n        .builder \\\r\n        .master(\"spark://my_master:7077\") \\\r\n        .config(\"spark.executor.memory\", \"15g\") \\\r\n        .config(\"spark.driver.memory\", \"3g\") \\\r\n        .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\r\n        .config(\"spark.sql.execution.arrow.fallback.enabled\", \"true\") \\\r\n        .config(\"spark.cores.max\", \"55\") \\\r\n        .config(\"spark.executor.cores\", \"11\") \\\r\n        .appName(\"my_app\") \\\r\n        .getOrCreate()\r\nks.set_option('compute.default_index_type', 'distributed')\r\n\r\nTABLE = \"/my/delta/table/\" \r\nkdf = ks.read_delta(TABLE) # 10 million rows, 100 columns, numbers only\r\nkdfbig = ks.read_delta(TABLE) \r\n\r\nfor x in range(300):\r\n    kdfbig = kdfbig.append(kdf, ignore_index=True) # Driver CPU usage goes to 100 %.\r\n#kdfbig should now have 3 billion rows\r\n\r\nkdfbig.count()\r\n```\r\n\r\nWhile invoking .count() following exception occurs:\r\n```\r\n--------------------------------------------------------------------------\r\nPy4JJavaError                             Traceback (most recent call last)\r\n<ipython-input-9-8f0bba6fd428> in <module>\r\n----> 1 kdfbig.count()\r\n\r\n/opt/conda/envs/spark3/lib/python3.8/site-packages/databricks/koalas/frame.py in count(self, axis)\r\n   6172         Name: 0, dtype: int64\r\n   6173         \"\"\"\r\n-> 6174         return self._reduce_for_stat_function(\r\n   6175             Frame._count_expr, name=\"count\", axis=axis, numeric_only=False\r\n   6176         )\r\n\r\n/opt/conda/envs/spark3/lib/python3.8/site-packages/databricks/koalas/frame.py in _reduce_for_stat_function(self, sfun, name, axis, numeric_only)\r\n    635                 )\r\n    636 \r\n--> 637                 return first_series(DataFrame(internal).transpose())\r\n    638 \r\n    639         elif axis == 1:\r\n\r\n/opt/conda/envs/spark3/lib/python3.8/site-packages/databricks/koalas/frame.py in transpose(self)\r\n   2080             SPARK_INDEX_NAME_FORMAT(i) for i in range(self._internal.column_labels_level)\r\n   2081         ]\r\n-> 2082         pivoted_df = exploded_df.groupBy(internal_index_columns).pivot(\"index\")\r\n   2083 \r\n   2084         transposed_df = pivoted_df.agg(F.first(F.col(\"value\")))\r\n\r\n/opt/conda/envs/spark3/lib/python3.8/site-packages/pyspark/sql/group.py in pivot(self, pivot_col, values)\r\n    215         \"\"\"\r\n    216         if values is None:\r\n--> 217             jgd = self._jgd.pivot(pivot_col)\r\n    218         else:\r\n    219             jgd = self._jgd.pivot(pivot_col, values)\r\n\r\n/opt/conda/envs/spark3/lib/python3.8/site-packages/py4j/java_gateway.py in __call__(self, *args)\r\n   1302 \r\n   1303         answer = self.gateway_client.send_command(command)\r\n-> 1304         return_value = get_return_value(\r\n   1305             answer, self.gateway_client, self.target_id, self.name)\r\n   1306 \r\n\r\n/opt/conda/envs/spark3/lib/python3.8/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\r\n    129     def deco(*a, **kw):\r\n    130         try:\r\n--> 131             return f(*a, **kw)\r\n    132         except py4j.protocol.Py4JJavaError as e:\r\n    133             converted = convert_exception(e.java_exception)\r\n\r\n/opt/conda/envs/spark3/lib/python3.8/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\r\n    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n    325             if answer[1] == REFERENCE_TYPE:\r\n--> 326                 raise Py4JJavaError(\r\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\r\n    328                     format(target_id, \".\", name), value)\r\n\r\nPy4JJavaError: An error occurred while calling o120307.pivot.\r\n: java.lang.StackOverflowError\r\n\tat java.util.HashMap.hash(HashMap.java:339)\r\n\tat java.util.HashMap.get(HashMap.java:557)\r\n\tat java.util.Collections$SynchronizedMap.get(Collections.java:2584)\r\n\tat org.apache.spark.internal.config.MapProvider.get(ConfigProvider.scala:47)\r\n\tat org.apache.spark.internal.config.ConfigReader.get(ConfigReader.scala:79)\r\n\tat org.apache.spark.internal.config.ConfigEntry.readString(ConfigEntry.scala:65)\r\n\tat org.apache.spark.internal.config.ConfigEntryWithDefault.readFrom(ConfigEntry.scala:112)\r\n\tat org.apache.spark.sql.internal.SQLConf.getConf(SQLConf.scala:3210)\r\n\tat org.apache.spark.sql.internal.SQLConf.maxToStringFields(SQLConf.scala:3143)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.toString(Expression.scala:276)\r\n\tat java.lang.String.valueOf(String.java:2994)\r\n\tat java.lang.StringBuilder.append(StringBuilder.java:131)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.toString(namedExpressions.scala:193)\r\n\tat java.lang.String.valueOf(String.java:2994)\r\n\tat scala.collection.mutable.StringBuilder.append(StringBuilder.scala:203)\r\n\tat scala.collection.TraversableOnce.$anonfun$addString$1(TraversableOnce.scala:364)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:362)\r\n\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractTraversable.addString(Traversable.scala:108)\r\n\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:328)\r\n\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:327)\r\n\tat scala.collection.AbstractTraversable.mkString(Traversable.scala:108)\r\n\tat org.apache.spark.sql.catalyst.util.package$.truncatedString(package.scala:181)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$argString$1(TreeNode.scala:535)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.addString(TraversableOnce.scala:362)\r\n\tat scala.collection.TraversableOnce.addString$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.addString(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:328)\r\n\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:327)\r\n\tat scala.collection.AbstractIterator.mkString(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.mkString(TraversableOnce.scala:330)\r\n\tat scala.collection.TraversableOnce.mkString$(TraversableOnce.scala:330)\r\n\tat scala.collection.AbstractIterator.mkString(Iterator.scala:1429)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.argString(TreeNode.scala:549)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.simpleString(TreeNode.scala:556)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.simpleString(QueryPlan.scala:188)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.verboseString(QueryPlan.scala:190)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:669)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$3(TreeNode.scala:693)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$3$adapted(TreeNode.scala:691)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:691)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$3(TreeNode.scala:693)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$3$adapted(TreeNode.scala:691)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:691)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:697)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$3(TreeNode.scala:693)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$generateTreeString$3$adapted(TreeNode.scala:691)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n(......)\r\n```\r\n\r\nWhy do I get a stackoverflow exception? On my spark worker nodes there is enough Memory. It seems to me, that the driver memory is the limit, but I don't understand why it does not execute the .append on the workers?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1651", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1651/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1651/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1651/events", "html_url": "https://github.com/databricks/koalas/issues/1651", "id": 657128263, "node_id": "MDU6SXNzdWU2NTcxMjgyNjM=", "number": 1651, "title": "Fillna doesn't get the correct result when arg \"method\" is \"ffill\".", "user": {"login": "xiaocai2333", "id": 46207236, "node_id": "MDQ6VXNlcjQ2MjA3MjM2", "avatar_url": "https://avatars3.githubusercontent.com/u/46207236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiaocai2333", "html_url": "https://github.com/xiaocai2333", "followers_url": "https://api.github.com/users/xiaocai2333/followers", "following_url": "https://api.github.com/users/xiaocai2333/following{/other_user}", "gists_url": "https://api.github.com/users/xiaocai2333/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiaocai2333/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiaocai2333/subscriptions", "organizations_url": "https://api.github.com/users/xiaocai2333/orgs", "repos_url": "https://api.github.com/users/xiaocai2333/repos", "events_url": "https://api.github.com/users/xiaocai2333/events{/privacy}", "received_events_url": "https://api.github.com/users/xiaocai2333/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-15T07:28:35Z", "updated_at": "2020-07-16T04:56:35Z", "closed_at": "2020-07-16T04:56:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "Fillna the method of Koalas Series  doesn't get the correct result when method \"ffill\" not work.\r\n\r\nAfter I execute fillna with method \"ffill\", print the whole result, it is right. But when I print the value which is filled, it is None.\r\n\r\nexample:\r\n```python\r\n>>> from databricks.koalas import Series\r\n>>> import numpy as np\r\n>>> a = [\"Bob\", \"John\", None]\r\n>>> b = [\"Bob\", \"John\", np.nan]\r\n>>> s1 = Series(a)\r\n>>> s2 = Series(b)\r\n>>> r1 = s1.fillna(method=\"ffill\")\r\n>>> r1\r\n20/07/15 15:18:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\r\n0     Bob\r\n1    John\r\n2    John\r\nName: 0, dtype: object\r\n>>> print(r1[2])\r\n20/07/15 15:19:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\r\nNone\r\n>>> r2 = s2.fillna(method=\"ffill\")\r\n>>> r2\r\n20/07/15 15:19:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\r\n0     Bob\r\n1    John\r\n2    John\r\nName: 0, dtype: object\r\n>>> print(r2[2])\r\n20/07/15 15:19:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\r\nNone\r\n```\r\n\r\nI think r1[2] and r2[2] should be \"John\", but I got \"None\".\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1645", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1645/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1645/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1645/events", "html_url": "https://github.com/databricks/koalas/issues/1645", "id": 654789795, "node_id": "MDU6SXNzdWU2NTQ3ODk3OTU=", "number": 1645, "title": "Cannot read parquet file from Pandas", "user": {"login": "lfdversluis", "id": 3618917, "node_id": "MDQ6VXNlcjM2MTg5MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3618917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lfdversluis", "html_url": "https://github.com/lfdversluis", "followers_url": "https://api.github.com/users/lfdversluis/followers", "following_url": "https://api.github.com/users/lfdversluis/following{/other_user}", "gists_url": "https://api.github.com/users/lfdversluis/gists{/gist_id}", "starred_url": "https://api.github.com/users/lfdversluis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lfdversluis/subscriptions", "organizations_url": "https://api.github.com/users/lfdversluis/orgs", "repos_url": "https://api.github.com/users/lfdversluis/repos", "events_url": "https://api.github.com/users/lfdversluis/events{/privacy}", "received_events_url": "https://api.github.com/users/lfdversluis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}, {"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-10T13:38:04Z", "updated_at": "2020-08-04T05:58:02Z", "closed_at": "2020-08-04T05:58:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I computed some data using pandas which I wrote to a file. The df looks like this https://i.imgur.com/pxCLrUh.png \r\nI wrote it to parquet using `plot_df.to_parquet(\"test.parquet\", flavor='spark')`\r\n\r\nNow when I read it in using spark, there is no issue, yet note there being one additional column which apparently pandas adds:\r\n```\r\ndf = spark.read.parquet(\"test.parquet\")\r\ndf.printSchema()\r\nroot\r\n |-- time: timestamp (nullable = true)\r\n |-- node: string (nullable = true)\r\n |-- power_usage: double (nullable = true)\r\n |-- temperature: double (nullable = true)\r\n |-- processes_running: double (nullable = true)\r\n |-- memory_used: double (nullable = true)\r\n |-- load1: double (nullable = true)\r\n |-- __index_level_0__: long (nullable = true)\r\n```\r\n\r\nNow if I run `kdf = df.to_koalas()` I get\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-29-4c46820eb041> in <module>\r\n      1 df = spark.read.parquet(\"test.parquet\")\r\n      2 df.printSchema()\r\n----> 3 kdf = df.to_koalas()\r\n\r\n/var/scratch/lvs215/miniconda3/lib/python3.8/site-packages/databricks/koalas/frame.py in to_koalas(self, index_col)\r\n   4449 \r\n   4450             index_map = _get_index_map(self, index_col)\r\n-> 4451             internal = InternalFrame(spark_frame=self, index_map=index_map)\r\n   4452             return DataFrame(internal)\r\n   4453 \r\n\r\n/var/scratch/lvs215/miniconda3/lib/python3.8/site-packages/databricks/koalas/internal.py in __init__(self, spark_frame, index_map, column_labels, data_spark_columns, column_label_names)\r\n    436 \r\n    437         if index_map is None:\r\n--> 438             assert not any(SPARK_INDEX_NAME_PATTERN.match(name) for name in spark_frame.columns), (\r\n    439                 \"Index columns should not appear in columns of the Spark DataFrame. Avoid \"\r\n    440                 \"index column names [%s].\" % SPARK_INDEX_NAME_PATTERN\r\n\r\nAssertionError: Index columns should not appear in columns of the Spark DataFrame. Avoid index column names [re.compile('__index_level_[0-9]+__')].\r\n```\r\n\r\nThe issue looks similar to https://github.com/xitongsys/parquet-go/issues/233\r\n\r\nHow to deal with this? Should I follow the suggestion in https://github.com/apache/arrow/issues/2244 and don't preserve the index?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1633", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1633/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1633/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1633/events", "html_url": "https://github.com/databricks/koalas/issues/1633", "id": 651396442, "node_id": "MDU6SXNzdWU2NTEzOTY0NDI=", "number": 1633, "title": "Binary operation got \"AnalysisException\" after doing kdf[key] = kdf[key] + 1", "user": {"login": "shengjh", "id": 46514371, "node_id": "MDQ6VXNlcjQ2NTE0Mzcx", "avatar_url": "https://avatars0.githubusercontent.com/u/46514371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shengjh", "html_url": "https://github.com/shengjh", "followers_url": "https://api.github.com/users/shengjh/followers", "following_url": "https://api.github.com/users/shengjh/following{/other_user}", "gists_url": "https://api.github.com/users/shengjh/gists{/gist_id}", "starred_url": "https://api.github.com/users/shengjh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shengjh/subscriptions", "organizations_url": "https://api.github.com/users/shengjh/orgs", "repos_url": "https://api.github.com/users/shengjh/repos", "events_url": "https://api.github.com/users/shengjh/events{/privacy}", "received_events_url": "https://api.github.com/users/shengjh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-06T09:38:56Z", "updated_at": "2020-07-07T02:20:04Z", "closed_at": "2020-07-07T02:20:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hello, I'm new to koalas and trying to process spatial data based on Series or DataFrame. \r\n\r\nWhen i modify a colum in a DataFrame like code sample below, an `AnalysisException`  is raised.\r\n\r\n\r\n**Code Sample**\r\n```\r\nfrom databricks.koalas import DataFrame\r\n\r\ndata = {\"a\": [1, 2, 3, 4]}\r\nkdf = DataFrame(data)\r\nkdf['a'] = kdf['a'] + 1\r\nprint(kdf['a'] + 1)\r\n```\r\n\r\n**Error Output**\r\n```\r\npyspark.sql.utils.AnalysisException: syntax error in attribute name: `((a + 1) AS `a` + 1)`;\r\n```\r\n\r\n**Software versions**\r\n```\r\npython 3.7.6\r\nkoalas 1.0.0\r\npyspark 3.0.0\r\nspark 3.0.0\r\n```\r\nIs it a BUG? Hope for some help.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1626", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1626/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1626/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1626/events", "html_url": "https://github.com/databricks/koalas/issues/1626", "id": 650267137, "node_id": "MDU6SXNzdWU2NTAyNjcxMzc=", "number": 1626, "title": "Plotly on koalas dataframes", "user": {"login": "jainayush007", "id": 31528721, "node_id": "MDQ6VXNlcjMxNTI4NzIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31528721?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jainayush007", "html_url": "https://github.com/jainayush007", "followers_url": "https://api.github.com/users/jainayush007/followers", "following_url": "https://api.github.com/users/jainayush007/following{/other_user}", "gists_url": "https://api.github.com/users/jainayush007/gists{/gist_id}", "starred_url": "https://api.github.com/users/jainayush007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jainayush007/subscriptions", "organizations_url": "https://api.github.com/users/jainayush007/orgs", "repos_url": "https://api.github.com/users/jainayush007/repos", "events_url": "https://api.github.com/users/jainayush007/events{/privacy}", "received_events_url": "https://api.github.com/users/jainayush007/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1355475110, "node_id": "MDU6TGFiZWwxMzU1NDc1MTEw", "url": "https://api.github.com/repos/databricks/koalas/labels/discussions", "name": "discussions", "color": "f29721", "default": false, "description": ""}, {"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "DumbMachine", "id": 23381512, "node_id": "MDQ6VXNlcjIzMzgxNTEy", "avatar_url": "https://avatars1.githubusercontent.com/u/23381512?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DumbMachine", "html_url": "https://github.com/DumbMachine", "followers_url": "https://api.github.com/users/DumbMachine/followers", "following_url": "https://api.github.com/users/DumbMachine/following{/other_user}", "gists_url": "https://api.github.com/users/DumbMachine/gists{/gist_id}", "starred_url": "https://api.github.com/users/DumbMachine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DumbMachine/subscriptions", "organizations_url": "https://api.github.com/users/DumbMachine/orgs", "repos_url": "https://api.github.com/users/DumbMachine/repos", "events_url": "https://api.github.com/users/DumbMachine/events{/privacy}", "received_events_url": "https://api.github.com/users/DumbMachine/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "DumbMachine", "id": 23381512, "node_id": "MDQ6VXNlcjIzMzgxNTEy", "avatar_url": "https://avatars1.githubusercontent.com/u/23381512?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DumbMachine", "html_url": "https://github.com/DumbMachine", "followers_url": "https://api.github.com/users/DumbMachine/followers", "following_url": "https://api.github.com/users/DumbMachine/following{/other_user}", "gists_url": "https://api.github.com/users/DumbMachine/gists{/gist_id}", "starred_url": "https://api.github.com/users/DumbMachine/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DumbMachine/subscriptions", "organizations_url": "https://api.github.com/users/DumbMachine/orgs", "repos_url": "https://api.github.com/users/DumbMachine/repos", "events_url": "https://api.github.com/users/DumbMachine/events{/privacy}", "received_events_url": "https://api.github.com/users/DumbMachine/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2020-07-03T00:22:39Z", "updated_at": "2020-07-14T18:06:22Z", "closed_at": "2020-07-14T10:45:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I can easily use plotly's interactive charts by below 2 lines of code:\r\n\r\nimport pandas as pd\r\npd.options.plotting.backend = \"plotly\"\r\n\r\nHowever, unable to use Plotly with Koalas Dataframes. Is there a workaround or can there be an enhancement to include this feature?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1618", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1618/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1618/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1618/events", "html_url": "https://github.com/databricks/koalas/issues/1618", "id": 647438395, "node_id": "MDU6SXNzdWU2NDc0MzgzOTU=", "number": 1618, "title": "Koalas very slow compared to pandas", "user": {"login": "Lukas012", "id": 2796837, "node_id": "MDQ6VXNlcjI3OTY4Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/2796837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lukas012", "html_url": "https://github.com/Lukas012", "followers_url": "https://api.github.com/users/Lukas012/followers", "following_url": "https://api.github.com/users/Lukas012/following{/other_user}", "gists_url": "https://api.github.com/users/Lukas012/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lukas012/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lukas012/subscriptions", "organizations_url": "https://api.github.com/users/Lukas012/orgs", "repos_url": "https://api.github.com/users/Lukas012/repos", "events_url": "https://api.github.com/users/Lukas012/events{/privacy}", "received_events_url": "https://api.github.com/users/Lukas012/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-29T14:46:53Z", "updated_at": "2020-07-01T11:32:39Z", "closed_at": "2020-06-30T17:05:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, \r\n\r\nwe want to use koalas. We did some tests and compared it to pandas. However, koalas was in all cases significantly slower. We tried koalas in local[32]-Mode (but the results are similar in our distributed spark cluster):\r\n\r\nEnvironment:\r\nKoalas 1.0.1\r\nPySpark 2.4.5 (similar results with PySpark 3.0.0)\r\n\r\nFollowing Code:\r\n```python\r\nos.environ[\"PYSPARK_PYTHON\"] = \"/opt/conda/envs/MY_ENV/bin/python\"\r\nspark = SparkSession \\\r\n        .builder \\\r\n        .master(\"local[32]\") \\\r\n        .config(\"spark.executor.memory\", \"20g\") \\\r\n        .config(\"spark.driver.memory\", \"50g\") \\\r\n        .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\r\n        .config(\"spark.sql.execution.arrow.fallback.enabled\", \"true\") \\\r\n        .config(\"spark.cores.max\", \"32\") \\\r\n        .appName(\"MY_APP\") \\\r\n        .getOrCreate()\r\nks.set_option('compute.default_index_type', 'distributed')\r\n```\r\n\r\nWe made these performance comparision with koalas and pandas:\r\n```python\r\nNROWS = 1000000000\r\n```\r\nFrom now on we executed the code with\r\n```python\r\nimport pandas as ks\r\n#and\r\nimport databrick.koalas as ks\r\n```\r\n\r\nPerformance:\r\n```python\r\nt = time()\r\ndata = ks.DataFrame(columns=['col'], data=np.random.randn(NROWS))\r\nprint(time()-t)\r\n```\r\nPandas: 3,9 s\r\nKoalas: 12,19 s\r\n\r\n```python\r\nt = time()\r\nprint(data['col'].sum())\r\nprint((time()-t))\r\n```\r\nPandas: 0,87 s\r\nKoalas: 3,7 s\r\n\r\n```python\r\nt = time()\r\nprint(data.describe())\r\nprint((time()-t))\r\n```\r\nPandas: 8,13 s\r\nKoalas: 8,1 s\r\n\r\n```python\r\nt = time()\r\nx = len(data.loc[data['col'] == 1])\r\nprint(x)\r\nprint((time()-t))\r\n```\r\nPandas: 0,43 s\r\nKoalas: 2,3 s\r\n\r\n\r\n\r\nCan you give us some hints, why koalas is so slow in our test with 100 Million Random Numbers?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1615", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1615/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1615/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1615/events", "html_url": "https://github.com/databricks/koalas/issues/1615", "id": 646523361, "node_id": "MDU6SXNzdWU2NDY1MjMzNjE=", "number": 1615, "title": "Series.droplevel", "user": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}, {"id": 1179552130, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMw", "url": "https://api.github.com/repos/databricks/koalas/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-26T21:33:42Z", "updated_at": "2020-07-15T20:54:22Z", "closed_at": "2020-07-15T20:54:22Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Implement `Series.droplevel`.\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.droplevel.html", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1614", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1614/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1614/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1614/events", "html_url": "https://github.com/databricks/koalas/issues/1614", "id": 646523173, "node_id": "MDU6SXNzdWU2NDY1MjMxNzM=", "number": 1614, "title": "DataFrame.droplevel", "user": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}, {"id": 1179552130, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMw", "url": "https://api.github.com/repos/databricks/koalas/labels/good%20first%20issue", "name": "good first issue", "color": "7057ff", "default": true, "description": "Good for newcomers"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-06-26T21:33:10Z", "updated_at": "2020-07-09T12:14:29Z", "closed_at": "2020-07-09T12:14:29Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Implement `DataFrame.droplevel`.\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.droplevel.html", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1612", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1612/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1612/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1612/events", "html_url": "https://github.com/databricks/koalas/issues/1612", "id": 645721384, "node_id": "MDU6SXNzdWU2NDU3MjEzODQ=", "number": 1612, "title": "Provide an utility to add an increasing id column", "user": {"login": "candalfigomoro", "id": 50733646, "node_id": "MDQ6VXNlcjUwNzMzNjQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/50733646?v=4", "gravatar_id": "", "url": "https://api.github.com/users/candalfigomoro", "html_url": "https://github.com/candalfigomoro", "followers_url": "https://api.github.com/users/candalfigomoro/followers", "following_url": "https://api.github.com/users/candalfigomoro/following{/other_user}", "gists_url": "https://api.github.com/users/candalfigomoro/gists{/gist_id}", "starred_url": "https://api.github.com/users/candalfigomoro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/candalfigomoro/subscriptions", "organizations_url": "https://api.github.com/users/candalfigomoro/orgs", "repos_url": "https://api.github.com/users/candalfigomoro/repos", "events_url": "https://api.github.com/users/candalfigomoro/events{/privacy}", "received_events_url": "https://api.github.com/users/candalfigomoro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-25T17:07:12Z", "updated_at": "2020-07-03T02:17:26Z", "closed_at": "2020-07-03T02:17:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "In pandas, adding a new column with an increasing sequential id is trivial:\r\n```\r\ndf.insert(0, \"id\", range(len(df)))\r\n```\r\nso we don't need any specific function.\r\n\r\nKoalas itself does create an increasing id when it generates a default index (see https://github.com/databricks/koalas/blob/master/databricks/koalas/internal.py#L514) but  `attach_default_index`, `attach_distributed_column` and `attach_distributed_sequence_column` methods are internal methods and they are not supposed to be directly used (see the comment at https://github.com/databricks/koalas/blob/master/databricks/koalas/internal.py#L74).\r\n\r\nMoreover, those internal methods work on Spark dataframes (not Koalas dataframes), so to use them we should call `to_spark()` (paying attention to indexes), add the increasing id column and then convert the Spark dataframe back to a Koalas dataframe.\r\n\r\nPlease expose utilities to add increasing ids (sequential or not) to Koalas dataframes, as we can't do it the pandas way.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1611", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1611/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1611/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1611/events", "html_url": "https://github.com/databricks/koalas/issues/1611", "id": 644457504, "node_id": "MDU6SXNzdWU2NDQ0NTc1MDQ=", "number": 1611, "title": "What does  API coverage mentioned in the 1.0.0 version released reaches 75% actually mean\uff1f", "user": {"login": "chi2liu", "id": 19861628, "node_id": "MDQ6VXNlcjE5ODYxNjI4", "avatar_url": "https://avatars2.githubusercontent.com/u/19861628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chi2liu", "html_url": "https://github.com/chi2liu", "followers_url": "https://api.github.com/users/chi2liu/followers", "following_url": "https://api.github.com/users/chi2liu/following{/other_user}", "gists_url": "https://api.github.com/users/chi2liu/gists{/gist_id}", "starred_url": "https://api.github.com/users/chi2liu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chi2liu/subscriptions", "organizations_url": "https://api.github.com/users/chi2liu/orgs", "repos_url": "https://api.github.com/users/chi2liu/repos", "events_url": "https://api.github.com/users/chi2liu/events{/privacy}", "received_events_url": "https://api.github.com/users/chi2liu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-24T09:19:51Z", "updated_at": "2020-06-25T13:38:36Z", "closed_at": "2020-06-25T01:50:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "The API coverage mentioned in the 1.0.0 version released reaches 75%. \r\nWhat specific classes and methods does API coverage count here?\r\nFor example, the API coverage of DataFrame class includes only public methods and attributes, not private methods\uff1f", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1608", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1608/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1608/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1608/events", "html_url": "https://github.com/databricks/koalas/issues/1608", "id": 643938788, "node_id": "MDU6SXNzdWU2NDM5Mzg3ODg=", "number": 1608, "title": "Rename changes logical plan", "user": {"login": "sebastianvermaas", "id": 39453388, "node_id": "MDQ6VXNlcjM5NDUzMzg4", "avatar_url": "https://avatars2.githubusercontent.com/u/39453388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sebastianvermaas", "html_url": "https://github.com/sebastianvermaas", "followers_url": "https://api.github.com/users/sebastianvermaas/followers", "following_url": "https://api.github.com/users/sebastianvermaas/following{/other_user}", "gists_url": "https://api.github.com/users/sebastianvermaas/gists{/gist_id}", "starred_url": "https://api.github.com/users/sebastianvermaas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sebastianvermaas/subscriptions", "organizations_url": "https://api.github.com/users/sebastianvermaas/orgs", "repos_url": "https://api.github.com/users/sebastianvermaas/repos", "events_url": "https://api.github.com/users/sebastianvermaas/events{/privacy}", "received_events_url": "https://api.github.com/users/sebastianvermaas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-23T15:29:40Z", "updated_at": "2020-06-24T00:13:47Z", "closed_at": "2020-06-24T00:13:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Please let me know if I am doing something wrong here, but I would not expect that the `rename()` operation would redact previous operations.\r\n\r\n\r\n```python\r\ndf = ks.DataFrame([[\"2020-05-01\", 1], [\"2020-05-02\", 2]], columns=[\"date\", \"value\"])\r\ndf[\"date\"] = F.to_date(F.col(\"date\"))\r\n\r\ndf.spark.print_schema()\r\n```\r\neverything looks good, \r\n\r\n```\r\nroot\r\n |-- date: date (nullable = true)\r\n |-- value: long (nullable = false)\r\n```\r\n\r\n```python\r\ndf_add = df.copy()\r\ndf_add[\"date\"] = F.add_months(F.col(\"date\"), 12)\r\ndf_add.spark.print_schema()\r\n```\r\n```\r\n== Physical Plan ==\r\n*(1) Project [__index_level_0__#945L, add_months(cast(date#946 as date), 12) AS date#961, value#947L]\r\n+- Scan ExistingRDD[__index_level_0__#945L,date#946,value#947L]\r\n      \r\n| date | value |\r\n| ---- | ----- |\r\n|2021-05-01 | 1 | \r\n| 2021-05-02 | 2| \r\n```\r\n\r\nOnce I rename the `value` column, however, it seems that the `add_months` no longer applies. \r\n```python\r\ndf_add = df_add.rename(columns={\"value\": \"new_value\"})\r\ndf_add.spark.print_schema()\r\n```\r\n\r\n```\r\n== Physical Plan ==\r\n*(1) Project [__index_level_0__#993L, date#994, value#995L AS new_value#1030L]\r\n+- Scan ExistingRDD[__index_level_0__#993L,date#994,value#995L]\r\n                    \r\n| date | value |\r\n| ---- | ----- |\r\n|2020-05-01 | 1 | \r\n| 2020-05-02 | 2| \r\n```\r\n\r\nEDIT: \r\nI receive the same result when trying to use the new `kss.spark.transform()` accessor. \r\n```python\r\ndf = ks.DataFrame([[\"2020-05-01\", 1], [\"2020-05-02\", 2]], columns=[\"date\", \"value\"])\r\ndf[\"date\"] = df[\"date\"].spark.transform(lambda col: F.to_date(col))\r\n\r\ndf_add = df.copy()\r\ndf_add[\"date\"] = df_add[\"date\"].spark.transform(lambda col: F.add_months(col, 12))\r\n\r\ndf_add = df_add.rename(columns={\"value\": \"new_value\"})\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1607", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1607/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1607/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1607/events", "html_url": "https://github.com/databricks/koalas/issues/1607", "id": 643471226, "node_id": "MDU6SXNzdWU2NDM0NzEyMjY=", "number": 1607, "title": "How to can I set it up for driver memory for pyspark on Koalas?", "user": {"login": "krazyeom", "id": 442477, "node_id": "MDQ6VXNlcjQ0MjQ3Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/442477?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krazyeom", "html_url": "https://github.com/krazyeom", "followers_url": "https://api.github.com/users/krazyeom/followers", "following_url": "https://api.github.com/users/krazyeom/following{/other_user}", "gists_url": "https://api.github.com/users/krazyeom/gists{/gist_id}", "starred_url": "https://api.github.com/users/krazyeom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krazyeom/subscriptions", "organizations_url": "https://api.github.com/users/krazyeom/orgs", "repos_url": "https://api.github.com/users/krazyeom/repos", "events_url": "https://api.github.com/users/krazyeom/events{/privacy}", "received_events_url": "https://api.github.com/users/krazyeom/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552127, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI3", "url": "https://api.github.com/repos/databricks/koalas/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-23T02:02:32Z", "updated_at": "2020-06-23T05:33:46Z", "closed_at": "2020-06-23T05:33:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I test a sample code on [https://gist.github.com/patryk-oleniuk/043f97ae9c405cbd13b6977e7e6d6fbc](https://gist.github.com/patryk-oleniuk/043f97ae9c405cbd13b6977e7e6d6fbc)\r\n\r\nbut I've got the following error message after running the code.\r\n\r\n```\r\n2020-06-23 10:30:51 WARN  TaskSetManager:66 - Stage 0 contains a task of very large size (73693 KB). The maximum recommended task size is 100 KB.\r\n[Stage 0:>                                                          (0 + 1) / 8]Exception in thread \"dispatcher-event-loop-3\" java.lang.OutOfMemoryError: Java heap space\r\n        at java.util.Arrays.copyOf(Arrays.java:3236)\r\n        at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\r\n        at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\r\n        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\r\n        at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\r\n```\r\n\r\nI think it has only 1g memory to run pyspark for driver. I'd like to config it for more than 1g. But I cannot find a config file on Koalas. I tried to set the config file up for *5g* on `spark/conf/spark-defaults.conf`. It still run with *1g* memory.\r\n\r\n```\r\nbrighti+  4127  3302  1 10:29 ?        00:00:22 /home/brightics/brightics/packages/java/bin/java -cp /home/brightics/brightics/packages/conda/lib/python3.6/site-packages/pyspark/conf:/home/brightics/brightics/packages/conda/lib/python3.6/site-packages/pyspark/jars/* -Xmx1g org.apache.spark.deploy.SparkSubmit --conf spark.sql.analyzer.failAmbiguousSelfJoin=False --conf spark.app.name=Koalas pyspark-shell\r\n```\r\n\r\nHow can I set it up for a driver memory for pyspark?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1579", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1579/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1579/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1579/events", "html_url": "https://github.com/databricks/koalas/issues/1579", "id": 638469977, "node_id": "MDU6SXNzdWU2Mzg0Njk5Nzc=", "number": 1579, "title": "Bug import", "user": {"login": "GCPBigData", "id": 488409, "node_id": "MDQ6VXNlcjQ4ODQwOQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/488409?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GCPBigData", "html_url": "https://github.com/GCPBigData", "followers_url": "https://api.github.com/users/GCPBigData/followers", "following_url": "https://api.github.com/users/GCPBigData/following{/other_user}", "gists_url": "https://api.github.com/users/GCPBigData/gists{/gist_id}", "starred_url": "https://api.github.com/users/GCPBigData/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GCPBigData/subscriptions", "organizations_url": "https://api.github.com/users/GCPBigData/orgs", "repos_url": "https://api.github.com/users/GCPBigData/repos", "events_url": "https://api.github.com/users/GCPBigData/events{/privacy}", "received_events_url": "https://api.github.com/users/GCPBigData/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1351987327, "node_id": "MDU6TGFiZWwxMzUxOTg3MzI3", "url": "https://api.github.com/repos/databricks/koalas/labels/not%20a%20koalas%20issue", "name": "not a koalas issue", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-15T00:56:06Z", "updated_at": "2020-06-16T10:25:46Z", "closed_at": "2020-06-16T10:25:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "https://colab.research.google.com/drive/1CmWY3mKhhlnLEy7G9yGAoIMORwrj3T9R?usp=sharing\r\n\r\nimport databricks.koalas as ks\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-20-0376b7c81ae0> in <module>()\r\n----> 1 import databricks.koalas as ks\r\n\r\n6 frames\r\n/content/spark-3.0.0-preview2-bin-hadoop3.2/python/pyspark/sql/utils.py in require_minimum_pyarrow_version()\r\n    175     if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\r\n    176         raise ImportError(\"PyArrow >= %s must be installed; however, \"\r\n--> 177                           \"your version was %s.\" % (minimum_pyarrow_version, pyarrow.__version__))\r\n    178     if os.environ.get(\"ARROW_PRE_0_15_IPC_FORMAT\", \"0\") == \"1\":\r\n    179         raise RuntimeError(\"Arrow legacy IPC format is not supported in PySpark, \"\r\n\r\nImportError: PyArrow >= 0.15.1 must be installed; however, your version was 0.14.1.\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1523", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1523/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1523/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1523/events", "html_url": "https://github.com/databricks/koalas/issues/1523", "id": 622261776, "node_id": "MDU6SXNzdWU2MjIyNjE3NzY=", "number": 1523, "title": "Apply Spark Column operations directly on a Series", "user": {"login": "Callum027", "id": 2997843, "node_id": "MDQ6VXNlcjI5OTc4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2997843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Callum027", "html_url": "https://github.com/Callum027", "followers_url": "https://api.github.com/users/Callum027/followers", "following_url": "https://api.github.com/users/Callum027/following{/other_user}", "gists_url": "https://api.github.com/users/Callum027/gists{/gist_id}", "starred_url": "https://api.github.com/users/Callum027/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Callum027/subscriptions", "organizations_url": "https://api.github.com/users/Callum027/orgs", "repos_url": "https://api.github.com/users/Callum027/repos", "events_url": "https://api.github.com/users/Callum027/events{/privacy}", "received_events_url": "https://api.github.com/users/Callum027/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2020-05-21T05:40:22Z", "updated_at": "2020-05-24T08:25:57Z", "closed_at": "2020-05-24T08:17:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "On issue #1492, I noticed it was noted the following operation was possible on a column of a Koalas `DataFrame`:\r\n```python\r\n>>> import databricks.koalas as ks\r\n>>> df = ks.DataFrame([\"example\"], columns=[\"column\"])\r\n>>> from pyspark.sql import functions as F\r\n>>> df[\"column\"] = F.trim(F.upper(F.col(\"column\")))\r\n>>> df\r\n    column\r\n0  EXAMPLE\r\n```\r\n\r\nBy any chance, is there currently a way to apply Spark SQL/Column functions to a Koalas `Series`? I can imagine it looking something like so:\r\n```python\r\n>>> import databricks.koalas as ks\r\n>>> import pyspark.sql.functions as F\r\n>>> kss = ks.Series([\"example\"])\r\n>>> kss.apply(F.trim(F.upper(kss.spark_column)))\r\n0    EXAMPLE\r\nName: 0, dtype: object\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1516", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1516/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1516/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1516/events", "html_url": "https://github.com/databricks/koalas/issues/1516", "id": 620661484, "node_id": "MDU6SXNzdWU2MjA2NjE0ODQ=", "number": 1516, "title": " subset parameter in DataFrame.replace", "user": {"login": "beobest2", "id": 7010554, "node_id": "MDQ6VXNlcjcwMTA1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7010554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beobest2", "html_url": "https://github.com/beobest2", "followers_url": "https://api.github.com/users/beobest2/followers", "following_url": "https://api.github.com/users/beobest2/following{/other_user}", "gists_url": "https://api.github.com/users/beobest2/gists{/gist_id}", "starred_url": "https://api.github.com/users/beobest2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beobest2/subscriptions", "organizations_url": "https://api.github.com/users/beobest2/orgs", "repos_url": "https://api.github.com/users/beobest2/repos", "events_url": "https://api.github.com/users/beobest2/events{/privacy}", "received_events_url": "https://api.github.com/users/beobest2/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-19T04:09:27Z", "updated_at": "2020-06-11T19:23:47Z", "closed_at": "2020-06-11T19:23:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```python\r\n>>> kdf.replace('Mjolnir', 'Stormbuster', subset=('weapon',))\r\n                     name       weapon\r\n0.342778          Ironman      Mark-45\r\n0.087444  Captain America       Shield\r\n0.179212             Thor  Stormbuster\r\n0.522174             Hulk        Smash\r\n\r\n>>> pdf.replace('Mjolnir', 'Stormbuster', subset=('weapon',))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: replace() got an unexpected keyword argument 'subset'\r\n```\r\n\r\n@HyukjinKwon \r\nWhen I tried adding the above test to the test case, I found that pandas does not support the `subset` parameter. So when I looked into the old version, pandas didn't support the `subset` parameter from the beginning. I found that the current `replace` parameter matches the spark `replace`. So, what are your thoughts on deleting a `subset` from Koalas for pandas?\r\n\r\nref>\r\n\r\n[pandas 0.9.0 DataFrame.replace](https://pandas.pydata.org/pandas-docs/version/0.9.0/generated/pandas.DataFrame.replace.html?highlight=replace#pandas.DataFrame.replace)\r\n\r\n[pandas 1.0.1 DataFrame.replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html)\r\n\r\n[pyspark 2.1.3 DataFrame.replace](https://spark.apache.org/docs/2.1.3/api/python/pyspark.sql.html#pyspark.sql.DataFrame.replace)\r\n\r\n_Originally posted by @beobest2 in https://github.com/_render_node/MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAxNjQ0Mw==/comments/review_comment_", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1513", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1513/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1513/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1513/events", "html_url": "https://github.com/databricks/koalas/issues/1513", "id": 620643562, "node_id": "MDU6SXNzdWU2MjA2NDM1NjI=", "number": 1513, "title": "read_csv parse column error when value contains comma", "user": {"login": "Tokkiu", "id": 13414571, "node_id": "MDQ6VXNlcjEzNDE0NTcx", "avatar_url": "https://avatars2.githubusercontent.com/u/13414571?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tokkiu", "html_url": "https://github.com/Tokkiu", "followers_url": "https://api.github.com/users/Tokkiu/followers", "following_url": "https://api.github.com/users/Tokkiu/following{/other_user}", "gists_url": "https://api.github.com/users/Tokkiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tokkiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tokkiu/subscriptions", "organizations_url": "https://api.github.com/users/Tokkiu/orgs", "repos_url": "https://api.github.com/users/Tokkiu/repos", "events_url": "https://api.github.com/users/Tokkiu/events{/privacy}", "received_events_url": "https://api.github.com/users/Tokkiu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-19T03:14:16Z", "updated_at": "2020-05-19T03:55:04Z", "closed_at": "2020-05-19T03:55:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a CSV file for example:\r\n(I need to use double quotes to specify a json str)\r\n\r\nname,text\r\nary,\"{\"\"k1\"\":\"\"a1\"\",\"\"k2\"\":\"\"a2\"\"}\"\r\nalice,\"{\"\"k1\"\":\"\"a3\"\",\"\"k2\"\":\"\"a4\"\"}\"\r\n\r\nWhen using pandas, it works perfectly.\r\nBut when moved to koalas, using ks.read_csv().\r\nthis file would be recognized to:\r\n\r\nname, text\r\nary,'{\"k1\":\"a1\"\r\nalice,'{\"k1\":\"a3\" \r\n\r\nThe rest part of 'text' column is missing.\r\n\r\nI think this should be a lethal issue and should be fixed quickly.\r\n\r\nThanks.\r\n\r\n<img width=\"523\" alt=\"Screen Shot 2020-05-19 at 11 12 28 AM\" src=\"https://user-images.githubusercontent.com/13414571/82280534-a25d9000-99c1-11ea-8768-0f5ce587c01c.png\">\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1506", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1506/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1506/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1506/events", "html_url": "https://github.com/databricks/koalas/issues/1506", "id": 619846722, "node_id": "MDU6SXNzdWU2MTk4NDY3MjI=", "number": 1506, "title": "List type not supported for annotating functions for apply", "user": {"login": "Callum027", "id": 2997843, "node_id": "MDQ6VXNlcjI5OTc4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2997843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Callum027", "html_url": "https://github.com/Callum027", "followers_url": "https://api.github.com/users/Callum027/followers", "following_url": "https://api.github.com/users/Callum027/following{/other_user}", "gists_url": "https://api.github.com/users/Callum027/gists{/gist_id}", "starred_url": "https://api.github.com/users/Callum027/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Callum027/subscriptions", "organizations_url": "https://api.github.com/users/Callum027/orgs", "repos_url": "https://api.github.com/users/Callum027/repos", "events_url": "https://api.github.com/users/Callum027/events{/privacy}", "received_events_url": "https://api.github.com/users/Callum027/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-18T01:26:29Z", "updated_at": "2020-05-18T02:35:46Z", "closed_at": "2020-05-18T01:34:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\nfrom typing import List\r\nimport databricks.koalas as ks\r\n\r\n# Equivalent Spark UDF return type: ArrayType(StringType())\r\ndef to_list(x: str) -> List[str]:\r\n    return [x]\r\n\r\nks.Series([\"x\", \"y\"]).apply(to_list)\r\n```\r\nResult:\r\n```\r\nTraceback (most recent call last)\r\n<ipython-input-17-13d49d0c81a1> in <module>\r\n      6     return [x]\r\n      7 \r\n----> 8 ks.Series([\"x\", \"y\"]).apply(to_list)\r\n\r\n/opt/conda/lib/python3.7/site-packages/databricks/koalas/series.py in apply(self, func, args, **kwds)\r\n   2664                 raise ValueError(\r\n   2665                     \"Expected the return type of this function to be of scalar type, \"\r\n-> 2666                     \"but found type {}\".format(sig_return)\r\n   2667                 )\r\n   2668             return_schema = sig_return.tpe\r\n\r\nValueError: Expected the return type of this function to be of scalar type, but found type UnknownType[typing.List[str]]\r\n```\r\nUsing the actual type doesn't seem to work either:\r\n```\r\nValueError: Expected the return type of this function to be of scalar type, but found type UnknownType[<class 'list'>]\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1498", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1498/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1498/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1498/events", "html_url": "https://github.com/databricks/koalas/issues/1498", "id": 618326885, "node_id": "MDU6SXNzdWU2MTgzMjY4ODU=", "number": 1498, "title": "Support 'max_rows' parameter in DataFrame.to_markdown()", "user": {"login": "beobest2", "id": 7010554, "node_id": "MDQ6VXNlcjcwMTA1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7010554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beobest2", "html_url": "https://github.com/beobest2", "followers_url": "https://api.github.com/users/beobest2/followers", "following_url": "https://api.github.com/users/beobest2/following{/other_user}", "gists_url": "https://api.github.com/users/beobest2/gists{/gist_id}", "starred_url": "https://api.github.com/users/beobest2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beobest2/subscriptions", "organizations_url": "https://api.github.com/users/beobest2/orgs", "repos_url": "https://api.github.com/users/beobest2/repos", "events_url": "https://api.github.com/users/beobest2/events{/privacy}", "received_events_url": "https://api.github.com/users/beobest2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-14T15:32:20Z", "updated_at": "2020-05-15T03:11:18Z", "closed_at": "2020-05-15T03:11:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "https://github.com/databricks/koalas/blob/master/databricks/koalas/frame.py (line: 1767)\r\n![image](https://user-images.githubusercontent.com/7010554/81952614-a85e1480-9641-11ea-9718-14de6c18fc25.png)\r\n\r\nAccording to the code above, the `to_markdown` function in Koalas DataFrame seems to support the `max_rows` parameter. However, `to_markdown`, which can be supported by pandas version from 1.0.0, did not support 'max_rows' from the beginning. (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_markdown.html)\r\n\r\nTherefore, the following error occurs. \r\n```python\r\n>>> kdf = ks.DataFrame(data={\"animal_1\": [\"elk\", \"pig\"], \"animal_2\": [\"dog\", \"quetzal\"]})\r\n>>> kdf.to_markdown(max_rows=1)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/hwpark/Desktop/git_koalas/koalas/databricks/koalas/frame.py\", line 1814, in to_markdown\r\n    kdf._to_internal_pandas(), self.to_markdown, pd.DataFrame.to_markdown, args\r\n  File \"/Users/hwpark/Desktop/git_koalas/koalas/databricks/koalas/utils.py\", line 413, in validate_arguments_and_invoke_function\r\n    % (pd.__version__, param.name, pandas_func.__name__)\r\nTypeError: The pandas version [1.0.3] available does not support parameter 'max_rows' for function 'to_markdown'.\r\n```\r\n\r\nIn general, I'm curious whether to remove `max_rows` parameter support to match pandas in this case, or bypass `validate_arguments_and_invoke_function` to only use Koalas.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1493", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1493/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1493/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1493/events", "html_url": "https://github.com/databricks/koalas/issues/1493", "id": 617319789, "node_id": "MDU6SXNzdWU2MTczMTk3ODk=", "number": 1493, "title": "Best way to Fill NA's on large dataset", "user": {"login": "sebastianvermaas", "id": 39453388, "node_id": "MDQ6VXNlcjM5NDUzMzg4", "avatar_url": "https://avatars2.githubusercontent.com/u/39453388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sebastianvermaas", "html_url": "https://github.com/sebastianvermaas", "followers_url": "https://api.github.com/users/sebastianvermaas/followers", "following_url": "https://api.github.com/users/sebastianvermaas/following{/other_user}", "gists_url": "https://api.github.com/users/sebastianvermaas/gists{/gist_id}", "starred_url": "https://api.github.com/users/sebastianvermaas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sebastianvermaas/subscriptions", "organizations_url": "https://api.github.com/users/sebastianvermaas/orgs", "repos_url": "https://api.github.com/users/sebastianvermaas/repos", "events_url": "https://api.github.com/users/sebastianvermaas/events{/privacy}", "received_events_url": "https://api.github.com/users/sebastianvermaas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-13T10:08:17Z", "updated_at": "2020-05-13T10:57:55Z", "closed_at": "2020-05-13T10:57:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Would the below be an appropriate way to fill na's? Or is there a better way? \r\n\r\n```python\r\nsdf = kdf.to_spark(index_col=\"\")\r\nsdf = sdf.fillna(\"<fill-val>\", subset=[\"column1\", \"column2\"])\r\nkdf = sdf.to_koalas(index_col=\"\")\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1492", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1492/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1492/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1492/events", "html_url": "https://github.com/databricks/koalas/issues/1492", "id": 617281856, "node_id": "MDU6SXNzdWU2MTcyODE4NTY=", "number": 1492, "title": "Pyspark functions vs Koalas methods", "user": {"login": "sebastianvermaas", "id": 39453388, "node_id": "MDQ6VXNlcjM5NDUzMzg4", "avatar_url": "https://avatars2.githubusercontent.com/u/39453388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sebastianvermaas", "html_url": "https://github.com/sebastianvermaas", "followers_url": "https://api.github.com/users/sebastianvermaas/followers", "following_url": "https://api.github.com/users/sebastianvermaas/following{/other_user}", "gists_url": "https://api.github.com/users/sebastianvermaas/gists{/gist_id}", "starred_url": "https://api.github.com/users/sebastianvermaas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sebastianvermaas/subscriptions", "organizations_url": "https://api.github.com/users/sebastianvermaas/orgs", "repos_url": "https://api.github.com/users/sebastianvermaas/repos", "events_url": "https://api.github.com/users/sebastianvermaas/events{/privacy}", "received_events_url": "https://api.github.com/users/sebastianvermaas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-13T09:11:50Z", "updated_at": "2020-06-16T12:08:24Z", "closed_at": "2020-05-13T11:03:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, this is more of a question, but I was wondering why so many Koalas methods are explained as `pythonUDF`s?\r\n\r\nI guess my questions are: \r\n- Is this intended behavior? If so, could you explain why?\r\n- What is the \"optimal\" pick here? Does this change when data is larger, smaller, etc?\r\n\r\n\r\nExamples\r\n\r\n**String methods**\r\n\r\n*Koalas*\r\n```python\r\n\r\ndf = ks.DataFrame([\"example\"], columns=[\"column\"])\r\ndf[\"column\"] = df[\"column\"].str.upper().str.strip()\r\ndf.explain()\r\n\r\n== Physical Plan ==\r\n*(1) Project [__index_level_0__#720L, pythonUDF0#746 AS column#738]\r\n+- ArrowEvalPython [clean_fun(clean_fun(column#721))], [__index_level_0__#720L, column#721, pythonUDF0#746]\r\n   +- Scan ExistingRDD[__index_level_0__#720L,column#721]\r\n```\r\n\r\n*Pyspark*\r\n```python \r\ndf = ks.DataFrame([\"example\"], columns=[\"column\"])\r\ndf = df.to_spark().withColumn(\"column\", F.trim(F.upper(F.col(\"column\")))).to_koalas()\r\ndf.explain()\r\n\r\n== Physical Plan ==\r\n*(3) Project [(_we0#709 - 1) AS __index_level_0__#707, column#704]\r\n+- Window [row_number() windowspecdefinition(_w0#708L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _we0#709], [_w0#708L ASC NULLS FIRST]\r\n   +- *(2) Sort [_w0#708L ASC NULLS FIRST], false, 0\r\n      +- Exchange SinglePartition\r\n         +- *(1) Project [trim(upper(column#694), None) AS column#704, monotonically_increasing_id() AS _w0#708L]\r\n            +- Scan ExistingRDD[__index_level_0__#693L,column#694]\r\n```\r\n\r\n**Datetime methods**\r\n\r\n*Koalas*\r\n```python\r\ndf = ks.DataFrame([\"2020-01-01\"], columns=[\"date\"])\r\ndf[\"date\"] = ks.to_datetime(df[\"date\"])\r\ndf.explain()\r\n\r\n\r\n== Physical Plan ==\r\n*(1) Project [__index_level_0__#756L, pythonUDF0#776 AS date#768]\r\n+- ArrowEvalPython [clean_fun(date#757)], [__index_level_0__#756L, date#757, pythonUDF0#776]\r\n   +- Scan ExistingRDD[__index_level_0__#756L,date#757]\r\n```\r\n\r\n*Pyspark*\r\n\r\n```python\r\ndf = ks.DataFrame([\"2020-01-01\"], columns=[\"date\"])\r\ndf = df.to_spark().withColumn(\"date\", F.to_date(F.col(\"date\"))).to_koalas()\r\ndf.explain()\r\n\r\n\r\n== Physical Plan ==\r\n*(3) Project [(_we0#806 - 1) AS __index_level_0__#804, date#801]\r\n+- Window [row_number() windowspecdefinition(_w0#805L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _we0#806], [_w0#805L ASC NULLS FIRST]\r\n   +- *(2) Sort [_w0#805L ASC NULLS FIRST], false, 0\r\n      +- Exchange SinglePartition\r\n         +- *(1) Project [cast(date#791 as date) AS date#801, monotonically_increasing_id() AS _w0#805L]\r\n            +- Scan ExistingRDD[__index_level_0__#790L,date#791]\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1469", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1469/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1469/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1469/events", "html_url": "https://github.com/databricks/koalas/issues/1469", "id": 614470887, "node_id": "MDU6SXNzdWU2MTQ0NzA4ODc=", "number": 1469, "title": "get_dummies uses the prefix parameter whose type is dict return KeyError", "user": {"login": "Tom-Deng", "id": 17919002, "node_id": "MDQ6VXNlcjE3OTE5MDAy", "avatar_url": "https://avatars2.githubusercontent.com/u/17919002?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tom-Deng", "html_url": "https://github.com/Tom-Deng", "followers_url": "https://api.github.com/users/Tom-Deng/followers", "following_url": "https://api.github.com/users/Tom-Deng/following{/other_user}", "gists_url": "https://api.github.com/users/Tom-Deng/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tom-Deng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tom-Deng/subscriptions", "organizations_url": "https://api.github.com/users/Tom-Deng/orgs", "repos_url": "https://api.github.com/users/Tom-Deng/repos", "events_url": "https://api.github.com/users/Tom-Deng/events{/privacy}", "received_events_url": "https://api.github.com/users/Tom-Deng/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-08T03:06:31Z", "updated_at": "2020-05-11T06:51:03Z", "closed_at": "2020-05-11T06:51:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "Use get_dummies demo with prefix  and columns\r\n![image](https://user-images.githubusercontent.com/17919002/81365829-c5638680-911b-11ea-8896-cdfb8f3cc68d.png)\r\npandas result like this:\r\n![image](https://user-images.githubusercontent.com/17919002/81365863-e1672800-911b-11ea-901d-9ab3467ce93b.png)\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1465", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1465/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1465/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1465/events", "html_url": "https://github.com/databricks/koalas/issues/1465", "id": 613518625, "node_id": "MDU6SXNzdWU2MTM1MTg2MjU=", "number": 1465, "title": "Support for API Extensions", "user": {"login": "scook12", "id": 23006174, "node_id": "MDQ6VXNlcjIzMDA2MTc0", "avatar_url": "https://avatars1.githubusercontent.com/u/23006174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scook12", "html_url": "https://github.com/scook12", "followers_url": "https://api.github.com/users/scook12/followers", "following_url": "https://api.github.com/users/scook12/following{/other_user}", "gists_url": "https://api.github.com/users/scook12/gists{/gist_id}", "starred_url": "https://api.github.com/users/scook12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scook12/subscriptions", "organizations_url": "https://api.github.com/users/scook12/orgs", "repos_url": "https://api.github.com/users/scook12/repos", "events_url": "https://api.github.com/users/scook12/events{/privacy}", "received_events_url": "https://api.github.com/users/scook12/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1355475110, "node_id": "MDU6TGFiZWwxMzU1NDc1MTEw", "url": "https://api.github.com/repos/databricks/koalas/labels/discussions", "name": "discussions", "color": "f29721", "default": false, "description": ""}, {"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-06T18:21:09Z", "updated_at": "2020-07-01T11:42:49Z", "closed_at": "2020-07-01T11:42:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Issue\r\npandas exposes a pretty simple API to let library developers extend pandas objects via registering accessors. It would be awesome if koalas would support a similar feature.\r\n\r\n### Resources\r\nDocs: https://pandas.pydata.org/pandas-docs/stable/reference/extensions.html\r\n\r\nPublic API: https://github.com/pandas-dev/pandas/blob/master/pandas/api/extensions/__init__.py\r\n\r\nAccessors: https://github.com/pandas-dev/pandas/blob/master/pandas/core/accessor.py\r\n\r\n#420 has a possibly related discussion on pandas extension dtypes. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1464", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1464/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1464/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1464/events", "html_url": "https://github.com/databricks/koalas/issues/1464", "id": 613132296, "node_id": "MDU6SXNzdWU2MTMxMzIyOTY=", "number": 1464, "title": "read_csv appear \uff1aTypeError: 'bool' object is not iterable", "user": {"login": "mao-ouyang", "id": 46299753, "node_id": "MDQ6VXNlcjQ2Mjk5NzUz", "avatar_url": "https://avatars0.githubusercontent.com/u/46299753?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mao-ouyang", "html_url": "https://github.com/mao-ouyang", "followers_url": "https://api.github.com/users/mao-ouyang/followers", "following_url": "https://api.github.com/users/mao-ouyang/following{/other_user}", "gists_url": "https://api.github.com/users/mao-ouyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/mao-ouyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mao-ouyang/subscriptions", "organizations_url": "https://api.github.com/users/mao-ouyang/orgs", "repos_url": "https://api.github.com/users/mao-ouyang/repos", "events_url": "https://api.github.com/users/mao-ouyang/events{/privacy}", "received_events_url": "https://api.github.com/users/mao-ouyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-06T08:13:31Z", "updated_at": "2020-05-07T00:38:49Z", "closed_at": "2020-05-07T00:38:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "When will index_col be equal to Flase?\r\nThis problem is caused by index _col equal to False. At present, the judgment that increasing index_col is not equal to False temporarily solves the problem.\r\n![image](https://user-images.githubusercontent.com/46299753/81151188-52d79700-8fb3-11ea-92e1-4ae02a0b2116.png)\r\n![image](https://user-images.githubusercontent.com/46299753/81151419-8e726100-8fb3-11ea-90c6-4314f453d3f9.png)\r\n\r\nNormal after modification\uff1a\r\n![image](https://user-images.githubusercontent.com/46299753/81151843-f628ac00-8fb3-11ea-8963-a56bab904c03.png)\r\n![image](https://user-images.githubusercontent.com/46299753/81152244-5a4b7000-8fb4-11ea-8fd8-1dfe5f0ffead.png)\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1458", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1458/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1458/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1458/events", "html_url": "https://github.com/databricks/koalas/issues/1458", "id": 611354960, "node_id": "MDU6SXNzdWU2MTEzNTQ5NjA=", "number": 1458, "title": "ImportError: No module named 'numpy.testing.nosetester'", "user": {"login": "hanman2016", "id": 25268672, "node_id": "MDQ6VXNlcjI1MjY4Njcy", "avatar_url": "https://avatars1.githubusercontent.com/u/25268672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hanman2016", "html_url": "https://github.com/hanman2016", "followers_url": "https://api.github.com/users/hanman2016/followers", "following_url": "https://api.github.com/users/hanman2016/following{/other_user}", "gists_url": "https://api.github.com/users/hanman2016/gists{/gist_id}", "starred_url": "https://api.github.com/users/hanman2016/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hanman2016/subscriptions", "organizations_url": "https://api.github.com/users/hanman2016/orgs", "repos_url": "https://api.github.com/users/hanman2016/repos", "events_url": "https://api.github.com/users/hanman2016/events{/privacy}", "received_events_url": "https://api.github.com/users/hanman2016/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1351987327, "node_id": "MDU6TGFiZWwxMzUxOTg3MzI3", "url": "https://api.github.com/repos/databricks/koalas/labels/not%20a%20koalas%20issue", "name": "not a koalas issue", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-03T07:42:51Z", "updated_at": "2020-05-06T08:19:36Z", "closed_at": "2020-05-06T08:19:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "I got this error after installing koalas library on my cluster.\r\nI tried importing numpy library then this encountered.\r\nPlease help to sort it out ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1456", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1456/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1456/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1456/events", "html_url": "https://github.com/databricks/koalas/issues/1456", "id": 610700615, "node_id": "MDU6SXNzdWU2MTA3MDA2MTU=", "number": 1456, "title": "Add Spark JDBC read ", "user": {"login": "sebastianvermaas", "id": 39453388, "node_id": "MDQ6VXNlcjM5NDUzMzg4", "avatar_url": "https://avatars2.githubusercontent.com/u/39453388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sebastianvermaas", "html_url": "https://github.com/sebastianvermaas", "followers_url": "https://api.github.com/users/sebastianvermaas/followers", "following_url": "https://api.github.com/users/sebastianvermaas/following{/other_user}", "gists_url": "https://api.github.com/users/sebastianvermaas/gists{/gist_id}", "starred_url": "https://api.github.com/users/sebastianvermaas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sebastianvermaas/subscriptions", "organizations_url": "https://api.github.com/users/sebastianvermaas/orgs", "repos_url": "https://api.github.com/users/sebastianvermaas/repos", "events_url": "https://api.github.com/users/sebastianvermaas/events{/privacy}", "received_events_url": "https://api.github.com/users/sebastianvermaas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-01T11:14:12Z", "updated_at": "2020-05-01T18:30:19Z", "closed_at": "2020-05-01T18:30:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "For enterprise use, I'd like to poll the extension of read methods to JDBC, given that drivers are available in the Spark Context. \r\n\r\n**Current Solutions**\r\n```python\r\nfrom pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.getOrCreate()\r\n\r\nimport databricks.koalas as ks\r\n\r\njdbc_options = dict()\r\njdbc_options[\"driver\"] = \"<my-driver>\"\r\n\r\n# JDBC\r\ndf = ks.DataFrame(\r\n    spark.read.format(\"jdbc\").options(**jdbc_options).option(\"dbtable\", \"<sql>\").load()\r\n)\r\n\r\n# Snowflake\r\nsf_options = dict()\r\ndf = ks.DataFrame(\r\n    spark.read.format(\"net.snowflake.spark.snowflake\")\r\n    .options(**sf_options)\r\n    .option(\"dbtable\", \"<sql>\")\r\n    .load()\r\n)\r\n```\r\n\r\n**New Solution**\r\n```python \r\nimport databricks.koalas as ks\r\n\r\ndf = ks.read_jdbc(dbtable=\"<sql>\", driver=\"<my-driver>\", **options)\r\n\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1451", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1451/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1451/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1451/events", "html_url": "https://github.com/databricks/koalas/issues/1451", "id": 607311362, "node_id": "MDU6SXNzdWU2MDczMTEzNjI=", "number": 1451, "title": "Python Kernel with remote Spark cluster", "user": {"login": "user919lx", "id": 22161559, "node_id": "MDQ6VXNlcjIyMTYxNTU5", "avatar_url": "https://avatars1.githubusercontent.com/u/22161559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/user919lx", "html_url": "https://github.com/user919lx", "followers_url": "https://api.github.com/users/user919lx/followers", "following_url": "https://api.github.com/users/user919lx/following{/other_user}", "gists_url": "https://api.github.com/users/user919lx/gists{/gist_id}", "starred_url": "https://api.github.com/users/user919lx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/user919lx/subscriptions", "organizations_url": "https://api.github.com/users/user919lx/orgs", "repos_url": "https://api.github.com/users/user919lx/repos", "events_url": "https://api.github.com/users/user919lx/events{/privacy}", "received_events_url": "https://api.github.com/users/user919lx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-04-27T07:36:11Z", "updated_at": "2020-05-07T00:39:53Z", "closed_at": "2020-05-07T00:39:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have a JupyterHub on a server,and my Spark clustes are on other servers. They comunicate with Jupyterhub by sparkmagic and livy.\r\n\r\nIt works fine in PySpark Kernel by sparkmagic as I install koalas in PySpark environment.\r\n\r\nBut I can't make koalas work in JupyterHub origin Python kernel, error info:`ImportError: Unable to import pyspark - consider doing a pip install with [spark] extra to install pyspark with pip `. \r\n\r\nJupyterhub isn't on Spark cluster servers, so how to make Python Kernel work with koalas?\r\nI really want to show dataframe in a pretty style, which can be done in Python kernel but can't in PySpark Kernel.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1450", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1450/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1450/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1450/events", "html_url": "https://github.com/databricks/koalas/issues/1450", "id": 607193682, "node_id": "MDU6SXNzdWU2MDcxOTM2ODI=", "number": 1450, "title": "Is there something wrong with concat when axis=1?", "user": {"login": "Tom-Deng", "id": 17919002, "node_id": "MDQ6VXNlcjE3OTE5MDAy", "avatar_url": "https://avatars2.githubusercontent.com/u/17919002?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tom-Deng", "html_url": "https://github.com/Tom-Deng", "followers_url": "https://api.github.com/users/Tom-Deng/followers", "following_url": "https://api.github.com/users/Tom-Deng/following{/other_user}", "gists_url": "https://api.github.com/users/Tom-Deng/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tom-Deng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tom-Deng/subscriptions", "organizations_url": "https://api.github.com/users/Tom-Deng/orgs", "repos_url": "https://api.github.com/users/Tom-Deng/repos", "events_url": "https://api.github.com/users/Tom-Deng/events{/privacy}", "received_events_url": "https://api.github.com/users/Tom-Deng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-04-27T02:57:33Z", "updated_at": "2020-04-28T07:56:12Z", "closed_at": "2020-04-28T07:56:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "![image](https://user-images.githubusercontent.com/17919002/80329598-6d07cb80-8875-11ea-9208-a3b571dd292a.png)\r\n\r\ntmp3.shape is expected to be (195638, 32)\r\n\r\ndtype info\r\n![image](https://user-images.githubusercontent.com/17919002/80329737-d1c32600-8875-11ea-99d7-3fd3c6db21c5.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1449", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1449/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1449/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1449/events", "html_url": "https://github.com/databricks/koalas/issues/1449", "id": 606580409, "node_id": "MDU6SXNzdWU2MDY1ODA0MDk=", "number": 1449, "title": "Timezone issue.", "user": {"login": "dk09630", "id": 3115389, "node_id": "MDQ6VXNlcjMxMTUzODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3115389?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dk09630", "html_url": "https://github.com/dk09630", "followers_url": "https://api.github.com/users/dk09630/followers", "following_url": "https://api.github.com/users/dk09630/following{/other_user}", "gists_url": "https://api.github.com/users/dk09630/gists{/gist_id}", "starred_url": "https://api.github.com/users/dk09630/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dk09630/subscriptions", "organizations_url": "https://api.github.com/users/dk09630/orgs", "repos_url": "https://api.github.com/users/dk09630/repos", "events_url": "https://api.github.com/users/dk09630/events{/privacy}", "received_events_url": "https://api.github.com/users/dk09630/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-24T20:58:55Z", "updated_at": "2020-04-24T21:16:16Z", "closed_at": "2020-04-24T21:16:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "While reading a csv file which contains the date in format 2018-09-27 01:05:10, it gives an error UnknownTimeZoneError: 'GMT+05:30'. The same works fine while reading from pandas", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1443", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1443/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1443/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1443/events", "html_url": "https://github.com/databricks/koalas/issues/1443", "id": 605388445, "node_id": "MDU6SXNzdWU2MDUzODg0NDU=", "number": 1443, "title": "Image storage", "user": {"login": "HyukjinKwon", "id": 6477701, "node_id": "MDQ6VXNlcjY0Nzc3MDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6477701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HyukjinKwon", "html_url": "https://github.com/HyukjinKwon", "followers_url": "https://api.github.com/users/HyukjinKwon/followers", "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}", "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}", "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions", "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs", "repos_url": "https://api.github.com/users/HyukjinKwon/repos", "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}", "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-23T09:39:20Z", "updated_at": "2020-06-18T02:24:49Z", "closed_at": "2020-04-23T09:39:23Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "![Screen Shot 2020-04-23 at 5 26 21 PM](https://user-images.githubusercontent.com/6477701/80076779-9f6cac80-8587-11ea-8c92-07d7b992733b.png)\r\n\r\n![Screen Shot 2020-04-23 at 5 27 38 PM](https://user-images.githubusercontent.com/6477701/80076898-c2975c00-8587-11ea-9b2c-69c9729e9294.png)\r\n\r\n![Screen Shot 2020-04-23 at 5 26 37 PM](https://user-images.githubusercontent.com/6477701/80076790-a1cf0680-8587-11ea-8b08-8dc694071ba0.png)\r\n\r\n![Screen Shot 2020-04-23 at 5 26 41 PM](https://user-images.githubusercontent.com/6477701/80076795-a3003380-8587-11ea-8b73-186e4047f8c0.png)\r\n\r\n![Screen Shot 2020-04-23 at 6 02 26 PM](https://user-images.githubusercontent.com/6477701/80080469-a518c100-858c-11ea-8a6c-fff469dd00a6.png)\r\n\r\n![Screen Shot 2020-04-23 at 6 02 35 PM](https://user-images.githubusercontent.com/6477701/80080472-a6e28480-858c-11ea-9840-76452f73bacb.png)\r\n\r\n\r\nPPTX: https://drive.google.com/open?id=19ruC0CuMQv70Slt-Spe8KgnF-Ot18dOr\r\nKeynote: https://drive.google.com/open?id=1DNW-k0zR1P8KX9XMQ6QY6-7y3yY4mbsJ\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1433", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1433/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1433/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1433/events", "html_url": "https://github.com/databricks/koalas/issues/1433", "id": 603224247, "node_id": "MDU6SXNzdWU2MDMyMjQyNDc=", "number": 1433, "title": "kdf.head(10) vs df.limit(10).toPandas()", "user": {"login": "Lukas012", "id": 2796837, "node_id": "MDQ6VXNlcjI3OTY4Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/2796837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lukas012", "html_url": "https://github.com/Lukas012", "followers_url": "https://api.github.com/users/Lukas012/followers", "following_url": "https://api.github.com/users/Lukas012/following{/other_user}", "gists_url": "https://api.github.com/users/Lukas012/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lukas012/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lukas012/subscriptions", "organizations_url": "https://api.github.com/users/Lukas012/orgs", "repos_url": "https://api.github.com/users/Lukas012/repos", "events_url": "https://api.github.com/users/Lukas012/events{/privacy}", "received_events_url": "https://api.github.com/users/Lukas012/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-20T13:07:25Z", "updated_at": "2020-04-21T08:35:19Z", "closed_at": "2020-04-21T08:35:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I've got a PySpark DataFrame df.\r\n`df = ...`\r\nWhen I invoke:\r\n`df.limit(10).toPandas()`\r\nI get the result very quick.\r\n\r\nWhen I invoke:\r\n`kdf = df.to_koalas()`\r\n`kdf.head(10)`\r\n\r\nThe results takes a very long time (if I even get it). Furthermore disk is filled until job get crashed due to \"No space left on device\".\r\n\r\nWhy? From my point of view both commands should do the same.\r\n\r\nEnvironment:\r\n- JupyterHub\r\n- Spark 2.4.5\r\n- Delta Lake\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1430", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1430/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1430/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1430/events", "html_url": "https://github.com/databricks/koalas/issues/1430", "id": 601853718, "node_id": "MDU6SXNzdWU2MDE4NTM3MTg=", "number": 1430, "title": "Series.astype() does not not respect \"truthyness\" as in pandas", "user": {"login": "Callum027", "id": 2997843, "node_id": "MDQ6VXNlcjI5OTc4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2997843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Callum027", "html_url": "https://github.com/Callum027", "followers_url": "https://api.github.com/users/Callum027/followers", "following_url": "https://api.github.com/users/Callum027/following{/other_user}", "gists_url": "https://api.github.com/users/Callum027/gists{/gist_id}", "starred_url": "https://api.github.com/users/Callum027/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Callum027/subscriptions", "organizations_url": "https://api.github.com/users/Callum027/orgs", "repos_url": "https://api.github.com/users/Callum027/repos", "events_url": "https://api.github.com/users/Callum027/events{/privacy}", "received_events_url": "https://api.github.com/users/Callum027/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-17T10:02:33Z", "updated_at": "2020-04-20T10:03:31Z", "closed_at": "2020-04-20T10:03:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "As you can see in the code below, I'm trying to convert a `str` to a `bool` below, as part of a quick way to see if a string is not `None`, empty or only consisting of whitespace. Since Koalas seems to hook directly into Spark's `Column.cast` and Java does not have the concept of \"truthy\" and \"falsey\" as in Python, the conversion fails and `None` is inserted instead.\r\n\r\nThis is a very tricky issue and might be very difficult to solve, so I'm not necessarily expecting a fix, but a note relating to the limitations of `Series.astype` in the documentation is probably necessary.\r\n\r\n```python\r\nimport pandas as pd\r\nimport databricks.koalas as ks\r\n\r\nprint(\"koalas version: {}\\n\".format(ks.__version__))\r\n\r\ndata = [\"hi\", \"hi \", \" \", \" \\t\", \"\", None]\r\npds = pd.Series(data)\r\nkss = ks.Series(data)\r\n\r\nprint(\"pds:\\n{}\\n\".format(pds.astype(bool)))\r\nprint(\"kss:\\n{}\\n\".format(kss.astype(bool)))\r\n\r\nprint(\"pds (stripped):\\n{}\\n\".format(pds.str.strip().astype(bool)))\r\nprint(\"kss (stripped):\\n{}\".format(kss.str.strip().astype(bool)))\r\n```\r\n\r\nOutput:\r\n```\r\nkoalas version: 0.31.0\r\n\r\npds:\r\n0     True\r\n1     True\r\n2     True\r\n3     True\r\n4    False\r\n5    False\r\ndtype: bool\r\n\r\nkss:\r\n0    None\r\n1    None\r\n2    None\r\n3    None\r\n4    None\r\n5    None\r\nName: 0, dtype: object\r\n\r\npds (stripped):\r\n0     True\r\n1     True\r\n2    False\r\n3    False\r\n4    False\r\n5    False\r\ndtype: bool\r\n\r\nkss (stripped):\r\n0    None\r\n1    None\r\n2    None\r\n3    None\r\n4    None\r\n5    None\r\nName: 0, dtype: object\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1428", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1428/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1428/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1428/events", "html_url": "https://github.com/databricks/koalas/issues/1428", "id": 601653990, "node_id": "MDU6SXNzdWU2MDE2NTM5OTA=", "number": 1428, "title": "Incompatible with pandas when floordiv with np.nan", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-17T02:17:15Z", "updated_at": "2020-04-20T10:04:00Z", "closed_at": "2020-04-20T10:04:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "There is an incompatible behaviour with pandas when floordiv with np.nan\r\n\r\n```python\r\n>>> pser = pd.Series([-100, 0, 100, None, np.nan], name=\"Koalas\")\r\n>>> kser = ks.from_pandas(pser)\r\n\r\n>>> pser // np.nan\r\n0   NaN\r\n1   NaN\r\n2   NaN\r\n3   NaN\r\n4   NaN\r\nName: Koalas, dtype: float64\r\n\r\n>>> kser // np.nan                                                                                                                \r\n0    0.0\r\n1    0.0\r\n2    0.0\r\n3    NaN\r\n4    NaN\r\nName: Koalas, dtype: float64\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1425", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1425/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1425/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1425/events", "html_url": "https://github.com/databricks/koalas/issues/1425", "id": 600641836, "node_id": "MDU6SXNzdWU2MDA2NDE4MzY=", "number": 1425, "title": "Handling missing values in datatime field throws error.", "user": {"login": "hanzigs", "id": 30790120, "node_id": "MDQ6VXNlcjMwNzkwMTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/30790120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hanzigs", "html_url": "https://github.com/hanzigs", "followers_url": "https://api.github.com/users/hanzigs/followers", "following_url": "https://api.github.com/users/hanzigs/following{/other_user}", "gists_url": "https://api.github.com/users/hanzigs/gists{/gist_id}", "starred_url": "https://api.github.com/users/hanzigs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hanzigs/subscriptions", "organizations_url": "https://api.github.com/users/hanzigs/orgs", "repos_url": "https://api.github.com/users/hanzigs/repos", "events_url": "https://api.github.com/users/hanzigs/events{/privacy}", "received_events_url": "https://api.github.com/users/hanzigs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-04-15T23:12:46Z", "updated_at": "2020-05-07T00:40:19Z", "closed_at": "2020-05-07T00:40:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have the missing values in datetime column in koalas dataframe, \r\n```\r\ncleaned_Data['Valuation_Date'].head()\r\n(1) Spark Jobs\r\nOut[12]: 0     1/01/2013 0:00\r\n1    16/08/2013 0:00\r\n2    16/08/2013 0:00\r\n3               None\r\n4               None\r\nName: Valuation_Date, dtype: object\r\n```\r\nI convert to koalas datatime, but thats not working, thorwing error\r\n```\r\ncleaned_Data[\"Valuation_Date1\"] = ks.to_datetime(cleaned_Data[\"Valuation_Date\"], errors='coerce')\r\ncleaned_Data['Valuation_Date1'].head()\r\n\r\norg.apache.spark.SparkException: Exception thrown in awaitResult: \r\nPy4JJavaError                             Traceback (most recent call last)\r\n/databricks/python/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj)\r\n    700                 type_pprinters=self.type_printers,\r\n    701                 deferred_pprinters=self.deferred_printers)\r\n--> 702             printer.pretty(obj)\r\n    703             printer.flush()\r\n    704             return stream.getvalue()\r\n\r\n/databricks/python/lib/python3.7/site-packages/IPython/lib/pretty.py in pretty(self, obj)\r\n    400                         if cls is not object \\\r\n    401                                 and callable(cls.__dict__.get('__repr__')):\r\n--> 402                             return _repr_pprint(obj, self, cycle)\r\n    403 \r\n    404             return _default_pprint(obj, self, cycle)\r\n\r\n/databricks/python/lib/python3.7/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\r\n    695     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\r\n    696     # Find newlines and replace them with p.break_()\r\n--> 697     output = repr(obj)\r\n    698     for idx,output_line in enumerate(output.splitlines()):\r\n```\r\nAny help on error please, if there is no missing values then same works charm.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1422", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1422/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1422/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1422/events", "html_url": "https://github.com/databricks/koalas/issues/1422", "id": 600024124, "node_id": "MDU6SXNzdWU2MDAwMjQxMjQ=", "number": 1422, "title": "TypeError: copy() got an unexpected keyword argument 'deep'", "user": {"login": "hanzigs", "id": 30790120, "node_id": "MDQ6VXNlcjMwNzkwMTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/30790120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hanzigs", "html_url": "https://github.com/hanzigs", "followers_url": "https://api.github.com/users/hanzigs/followers", "following_url": "https://api.github.com/users/hanzigs/following{/other_user}", "gists_url": "https://api.github.com/users/hanzigs/gists{/gist_id}", "starred_url": "https://api.github.com/users/hanzigs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hanzigs/subscriptions", "organizations_url": "https://api.github.com/users/hanzigs/orgs", "repos_url": "https://api.github.com/users/hanzigs/repos", "events_url": "https://api.github.com/users/hanzigs/events{/privacy}", "received_events_url": "https://api.github.com/users/hanzigs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-04-15T05:07:53Z", "updated_at": "2020-04-16T01:28:46Z", "closed_at": "2020-04-16T01:28:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nIn koalas I understand .copy() is implemented not like pandas, but I get error from the outside package scorecardpy\r\n```\r\n/databricks/python/lib/python3.7/site-packages/scorecardpy/woebin.py in woebin(dt, y, x, var_skip, breaks_list, special_values, stop_limit, count_distr_limit, bin_num_limit, positive, no_cores, print_step, method, ignore_const_cols, ignore_datetime_cols, check_cate_num, replace_blank, save_breaks_list, **kwargs)\r\n    886     if print_info: print('[INFO] creating woe binning ...')\r\n    887 \r\n--> 888     dt = dt.copy(deep=True)\r\n    889     if isinstance(y, str):\r\n    890         y = [y]\r\n\r\n/databricks/python/lib/python3.7/site-packages/databricks/koalas/usage_logging/__init__.py in wrapper(*args, **kwargs)\r\n    168             start = time.perf_counter()\r\n    169             try:\r\n--> 170                 res = func(*args, **kwargs)\r\n    171                 logger.log_success(\r\n    172                     class_name, function_name, time.perf_counter() - start, signature\r\n\r\nTypeError: copy() got an unexpected keyword argument 'deep'\r\n```\r\nIs there any fix for it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1421", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1421/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1421/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1421/events", "html_url": "https://github.com/databricks/koalas/issues/1421", "id": 599994029, "node_id": "MDU6SXNzdWU1OTk5OTQwMjk=", "number": 1421, "title": "str.split(expand = True)", "user": {"login": "mks2192", "id": 21050738, "node_id": "MDQ6VXNlcjIxMDUwNzM4", "avatar_url": "https://avatars1.githubusercontent.com/u/21050738?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mks2192", "html_url": "https://github.com/mks2192", "followers_url": "https://api.github.com/users/mks2192/followers", "following_url": "https://api.github.com/users/mks2192/following{/other_user}", "gists_url": "https://api.github.com/users/mks2192/gists{/gist_id}", "starred_url": "https://api.github.com/users/mks2192/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mks2192/subscriptions", "organizations_url": "https://api.github.com/users/mks2192/orgs", "repos_url": "https://api.github.com/users/mks2192/repos", "events_url": "https://api.github.com/users/mks2192/events{/privacy}", "received_events_url": "https://api.github.com/users/mks2192/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-15T03:34:03Z", "updated_at": "2020-04-20T10:01:10Z", "closed_at": "2020-04-20T10:01:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "in dataframe string split into multiple columns is not available. what is alternative to that", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1417", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1417/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1417/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1417/events", "html_url": "https://github.com/databricks/koalas/issues/1417", "id": 597990681, "node_id": "MDU6SXNzdWU1OTc5OTA2ODE=", "number": 1417, "title": "Implement resample", "user": {"login": "Titan0022", "id": 32593837, "node_id": "MDQ6VXNlcjMyNTkzODM3", "avatar_url": "https://avatars0.githubusercontent.com/u/32593837?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Titan0022", "html_url": "https://github.com/Titan0022", "followers_url": "https://api.github.com/users/Titan0022/followers", "following_url": "https://api.github.com/users/Titan0022/following{/other_user}", "gists_url": "https://api.github.com/users/Titan0022/gists{/gist_id}", "starred_url": "https://api.github.com/users/Titan0022/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Titan0022/subscriptions", "organizations_url": "https://api.github.com/users/Titan0022/orgs", "repos_url": "https://api.github.com/users/Titan0022/repos", "events_url": "https://api.github.com/users/Titan0022/events{/privacy}", "received_events_url": "https://api.github.com/users/Titan0022/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552127, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI3", "url": "https://api.github.com/repos/databricks/koalas/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}, {"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-10T17:01:36Z", "updated_at": "2020-06-08T19:35:52Z", "closed_at": "2020-06-08T19:35:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Have a lot of workflows that depends on resampling time series data\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1414", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1414/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1414/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1414/events", "html_url": "https://github.com/databricks/koalas/issues/1414", "id": 597234567, "node_id": "MDU6SXNzdWU1OTcyMzQ1Njc=", "number": 1414, "title": "Document that we don't support the compatibility with non-Koalas APIs yet.", "user": {"login": "HyukjinKwon", "id": 6477701, "node_id": "MDQ6VXNlcjY0Nzc3MDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6477701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HyukjinKwon", "html_url": "https://github.com/HyukjinKwon", "followers_url": "https://api.github.com/users/HyukjinKwon/followers", "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}", "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}", "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions", "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs", "repos_url": "https://api.github.com/users/HyukjinKwon/repos", "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}", "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-04-09T12:04:17Z", "updated_at": "2020-04-15T10:48:30Z", "closed_at": "2020-04-15T10:48:30Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Seems like people want to convert their codes directly from pandas to Koalas. One case I often observe is, they want to convert the codes that works together with other Python standard functions such as `max`, `min`, or list/generator comprehensions, e.g.)\r\n\r\n```python\r\nimport pandas as pd\r\ndata = []\r\nfor a in pd.Series([1, 2, 3]):\r\n    data.append(a)\r\n\r\npd.DataFrame(data)\r\n```\r\n\r\nIn Koalas, such example does not work. We should preemptively document and guide users to stick to Koalas APIs only. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1413", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1413/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1413/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1413/events", "html_url": "https://github.com/databricks/koalas/issues/1413", "id": 597056938, "node_id": "MDU6SXNzdWU1OTcwNTY5Mzg=", "number": 1413, "title": "Py4JJavaError: An error occurred while calling o1446.filter.", "user": {"login": "hanzigs", "id": 30790120, "node_id": "MDQ6VXNlcjMwNzkwMTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/30790120?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hanzigs", "html_url": "https://github.com/hanzigs", "followers_url": "https://api.github.com/users/hanzigs/followers", "following_url": "https://api.github.com/users/hanzigs/following{/other_user}", "gists_url": "https://api.github.com/users/hanzigs/gists{/gist_id}", "starred_url": "https://api.github.com/users/hanzigs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hanzigs/subscriptions", "organizations_url": "https://api.github.com/users/hanzigs/orgs", "repos_url": "https://api.github.com/users/hanzigs/repos", "events_url": "https://api.github.com/users/hanzigs/events{/privacy}", "received_events_url": "https://api.github.com/users/hanzigs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-09T06:38:58Z", "updated_at": "2020-05-07T00:39:16Z", "closed_at": "2020-05-07T00:39:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI have a proc_cnt which koalas df with column count, (coding in databricks cluster)\r\n```\r\nthres = 50\r\ndrop_list = list(set(proc_cnt.query('count >= @thres').index))\r\nks_df_drop = ks_df[ks_df.product_id.isin(drop_list)]\r\n```\r\nMy query function throws ParseException\r\n```\r\norg.apache.spark.sql.catalyst.parser.ParseException: \r\nPy4JJavaError                             Traceback (most recent call last)\r\n/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\r\n     62         try:\r\n---> 63             return f(*a, **kw)\r\n     64         except py4j.protocol.Py4JJavaError as e:\r\n\r\n/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\r\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\r\n--> 328                     format(target_id, \".\", name), value)\r\n    329             else:\r\n\r\nPy4JJavaError: An error occurred while calling o1446.filter.\r\n: org.apache.spark.sql.catalyst.parser.ParseException: \r\nextraneous input '@' expecting {'(', 'SELECT', 'FROM...\u2026\u2026\u2026...\r\n```\r\nMay I know the issue\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1411", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1411/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1411/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1411/events", "html_url": "https://github.com/databricks/koalas/issues/1411", "id": 596999476, "node_id": "MDU6SXNzdWU1OTY5OTk0NzY=", "number": 1411, "title": "Incompatible behavior with pandas for Series.div() when divide by zero.", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-09T03:45:16Z", "updated_at": "2020-04-16T23:48:43Z", "closed_at": "2020-04-16T23:48:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "The existing implementation of `Series.div()` doesn't same as pandas.\r\n\r\n```python\r\n>>> pser\r\n0    100.0\r\n1      NaN\r\n2   -300.0\r\n3      NaN\r\n4    500.0\r\n5   -700.0\r\nName: Koalas, dtype: float64\r\n\r\n>>> pser.div(0)\r\n0    inf\r\n1    NaN\r\n2   -inf\r\n3    NaN\r\n4    inf\r\n5   -inf\r\nName: Koalas, dtype: float64\r\n\r\n>>> ks.from_pandas(pser).div(0)\r\n0    None\r\n1    None\r\n2    None\r\n3    None\r\n4    None\r\n5    None\r\nName: Koalas, dtype: object\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1403", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1403/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1403/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1403/events", "html_url": "https://github.com/databricks/koalas/issues/1403", "id": 596175589, "node_id": "MDU6SXNzdWU1OTYxNzU1ODk=", "number": 1403, "title": "Column assignment doesn't support type list", "user": {"login": "brookewenig", "id": 6416014, "node_id": "MDQ6VXNlcjY0MTYwMTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/6416014?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brookewenig", "html_url": "https://github.com/brookewenig", "followers_url": "https://api.github.com/users/brookewenig/followers", "following_url": "https://api.github.com/users/brookewenig/following{/other_user}", "gists_url": "https://api.github.com/users/brookewenig/gists{/gist_id}", "starred_url": "https://api.github.com/users/brookewenig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brookewenig/subscriptions", "organizations_url": "https://api.github.com/users/brookewenig/orgs", "repos_url": "https://api.github.com/users/brookewenig/repos", "events_url": "https://api.github.com/users/brookewenig/events{/privacy}", "received_events_url": "https://api.github.com/users/brookewenig/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-07T21:56:36Z", "updated_at": "2020-07-10T02:04:36Z", "closed_at": "2020-07-10T02:04:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "In Pandas, I can assign a column of type list (code works below with df). But in Koalas, I get `TypeError: Column assignment doesn't support type list`. Could this please be supported?\r\n\r\n```\r\nimport pandas as pd\r\nimport databricks.koalas as ks\r\n\r\nd = {'col1': [1], 'col2': [2]}\r\ndf = pd.DataFrame(data=d)\r\nks_df = ks.DataFrame(df)\r\n\r\ndf['Masked_Verbatim'] = [\"hello\"]\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1398", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1398/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1398/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1398/events", "html_url": "https://github.com/databricks/koalas/issues/1398", "id": 595185374, "node_id": "MDU6SXNzdWU1OTUxODUzNzQ=", "number": 1398, "title": "Different behaviour for mod between pandas and Koalas", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-06T14:54:59Z", "updated_at": "2020-04-08T21:30:25Z", "closed_at": "2020-04-08T21:30:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "pandas and Koalas are using different way for calculating the `mod` for negative values like the below.\r\n\r\n```python\r\n>>> pser\r\n0    10\r\ndtype: int64\r\n\r\n>>> kser\r\n0    10\r\nName: 0, dtype: int64\r\n\r\n>>> pser.mod(-3)\r\n0   -2\r\ndtype: int64\r\n\r\n>>> kser.mod(-3)\r\n0    1\r\nName: 0, dtype: int64\r\n```\r\n\r\nI think It would be better to match this behaviour to the pandas.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1378", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1378/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1378/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1378/events", "html_url": "https://github.com/databricks/koalas/issues/1378", "id": 590170726, "node_id": "MDU6SXNzdWU1OTAxNzA3MjY=", "number": 1378, "title": "datarfame.loc question", "user": {"login": "zacqed", "id": 1675591, "node_id": "MDQ6VXNlcjE2NzU1OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1675591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zacqed", "html_url": "https://github.com/zacqed", "followers_url": "https://api.github.com/users/zacqed/followers", "following_url": "https://api.github.com/users/zacqed/following{/other_user}", "gists_url": "https://api.github.com/users/zacqed/gists{/gist_id}", "starred_url": "https://api.github.com/users/zacqed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zacqed/subscriptions", "organizations_url": "https://api.github.com/users/zacqed/orgs", "repos_url": "https://api.github.com/users/zacqed/repos", "events_url": "https://api.github.com/users/zacqed/events{/privacy}", "received_events_url": "https://api.github.com/users/zacqed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2020-03-30T10:18:46Z", "updated_at": "2020-06-19T07:31:31Z", "closed_at": "2020-06-19T07:31:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\nimport databricks.koalas as kd\r\nkdf = kd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],index=[4, 5, 5], columns=['A', 'B', 'C'])\r\n```\r\n```\r\nkdf\r\n    A   B   C\r\n4   0   2   3\r\n5   0   4   1\r\n5  10  20  30\r\n```\r\nOperation \r\n```\r\nkdf['C'].loc[(kdf[\"A\"] <= kdf[\"B\"])] = 10\r\nkdf\r\n    A   B   C\r\n4   0   2   3\r\n5   0   4   1\r\n5  10  20  30\r\n```\r\nThe output is incorrect \"C\" should have become 10.\r\n\r\nor Operation \r\n\r\n`kdf.loc[kdf[\"A\"] <= kdf[\"B\"],['C']] = 10\r\n`\r\n```\r\nkdf\r\nSparkPandasNotImplementedError: Can only assign value to the whole dataframe, the row index\r\n                    has to be `slice(None)` or `:` You are trying to use pandas function .loc[..., ...] = ..., use spark function withColumn, select\r\n```\r\n\r\nIs there anything that is getting missed?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1376", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1376/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1376/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1376/events", "html_url": "https://github.com/databricks/koalas/issues/1376", "id": 589906881, "node_id": "MDU6SXNzdWU1ODk5MDY4ODE=", "number": 1376, "title": "to_delta and to_parquet are dropping the index", "user": {"login": "Hoeze", "id": 1200058, "node_id": "MDQ6VXNlcjEyMDAwNTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1200058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hoeze", "html_url": "https://github.com/Hoeze", "followers_url": "https://api.github.com/users/Hoeze/followers", "following_url": "https://api.github.com/users/Hoeze/following{/other_user}", "gists_url": "https://api.github.com/users/Hoeze/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hoeze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hoeze/subscriptions", "organizations_url": "https://api.github.com/users/Hoeze/orgs", "repos_url": "https://api.github.com/users/Hoeze/repos", "events_url": "https://api.github.com/users/Hoeze/events{/privacy}", "received_events_url": "https://api.github.com/users/Hoeze/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-29T23:06:45Z", "updated_at": "2020-03-31T20:23:39Z", "closed_at": "2020-03-31T20:23:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I don't know if I made a mistake but `ks.DataFrame.to_delta` and `ks.read_delta` are dropping my index columns:\r\n![grafik](https://user-images.githubusercontent.com/1200058/77863412-7e33cd00-7222-11ea-8980-304631d4fa18.png)\r\n\r\nIs there a way to keep the index with the dataset?\r\nAlso, does this index actually give a performance boost compared to default Spark joins when I merge two large (terabyte range) datasets?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1373", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1373/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1373/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1373/events", "html_url": "https://github.com/databricks/koalas/issues/1373", "id": 588279875, "node_id": "MDU6SXNzdWU1ODgyNzk4NzU=", "number": 1373, "title": "df.cache() question", "user": {"login": "zacqed", "id": 1675591, "node_id": "MDQ6VXNlcjE2NzU1OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1675591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zacqed", "html_url": "https://github.com/zacqed", "followers_url": "https://api.github.com/users/zacqed/followers", "following_url": "https://api.github.com/users/zacqed/following{/other_user}", "gists_url": "https://api.github.com/users/zacqed/gists{/gist_id}", "starred_url": "https://api.github.com/users/zacqed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zacqed/subscriptions", "organizations_url": "https://api.github.com/users/zacqed/orgs", "repos_url": "https://api.github.com/users/zacqed/repos", "events_url": "https://api.github.com/users/zacqed/events{/privacy}", "received_events_url": "https://api.github.com/users/zacqed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1355475110, "node_id": "MDU6TGFiZWwxMzU1NDc1MTEw", "url": "https://api.github.com/repos/databricks/koalas/labels/discussions", "name": "discussions", "color": "f29721", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-03-26T09:36:35Z", "updated_at": "2020-03-31T18:13:53Z", "closed_at": "2020-03-31T18:13:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "PySpark allows for giving an argument for caching type. How can we pass this in Koalas?\r\n\r\nSpark \r\n```\r\n>>> df.persist(pyspark.StorageLevel.MEMORY_ONLY)\r\nDataFrame[id: bigint, name: string]\r\n\r\n>>> df.persist(pyspark.StorageLevel.DISK_ONLY)\r\nDataFrame[id: bigint, name: string]\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1372", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1372/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1372/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1372/events", "html_url": "https://github.com/databricks/koalas/issues/1372", "id": 588188596, "node_id": "MDU6SXNzdWU1ODgxODg1OTY=", "number": 1372, "title": "Documentation: read_spark_io examples", "user": {"login": "zacqed", "id": 1675591, "node_id": "MDQ6VXNlcjE2NzU1OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1675591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zacqed", "html_url": "https://github.com/zacqed", "followers_url": "https://api.github.com/users/zacqed/followers", "following_url": "https://api.github.com/users/zacqed/following{/other_user}", "gists_url": "https://api.github.com/users/zacqed/gists{/gist_id}", "starred_url": "https://api.github.com/users/zacqed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zacqed/subscriptions", "organizations_url": "https://api.github.com/users/zacqed/orgs", "repos_url": "https://api.github.com/users/zacqed/repos", "events_url": "https://api.github.com/users/zacqed/events{/privacy}", "received_events_url": "https://api.github.com/users/zacqed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-26T06:32:19Z", "updated_at": "2020-03-26T06:33:47Z", "closed_at": "2020-03-26T06:33:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Read_spark_io currently has to_spark_io examples in the document. If by design, please close this.\r\n\r\n`https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.read_spark_io.html#databricks.koalas.read_spark_io`\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1365", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1365/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1365/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1365/events", "html_url": "https://github.com/databricks/koalas/issues/1365", "id": 587611013, "node_id": "MDU6SXNzdWU1ODc2MTEwMTM=", "number": 1365, "title": "read_sql_query / read_spark_io on mssql trigger toPandas", "user": {"login": "zacqed", "id": 1675591, "node_id": "MDQ6VXNlcjE2NzU1OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1675591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zacqed", "html_url": "https://github.com/zacqed", "followers_url": "https://api.github.com/users/zacqed/followers", "following_url": "https://api.github.com/users/zacqed/following{/other_user}", "gists_url": "https://api.github.com/users/zacqed/gists{/gist_id}", "starred_url": "https://api.github.com/users/zacqed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zacqed/subscriptions", "organizations_url": "https://api.github.com/users/zacqed/orgs", "repos_url": "https://api.github.com/users/zacqed/repos", "events_url": "https://api.github.com/users/zacqed/events{/privacy}", "received_events_url": "https://api.github.com/users/zacqed/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-25T10:56:39Z", "updated_at": "2020-06-08T14:47:29Z", "closed_at": "2020-03-26T06:44:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "While executing koalas, i am continuously getting triggers to toPandas usage in \r\n\r\n```\r\nexposure_data_original = ks.read_sql_query(exposure_query, conn)  \r\ndataframe = ks.read_spark_io(dbtable = 'Position_Facility_ExposureData', format='JDBC',url = conn, index_col='date', fetchsize = 10000)\r\n```\r\n![image](https://user-images.githubusercontent.com/1675591/77529247-40971300-6eb5-11ea-9a50-035256f1e4e0.png)\r\n\r\n![image](https://user-images.githubusercontent.com/1675591/77529279-50165c00-6eb5-11ea-8fd0-d18c0b278bb8.png)\r\n\r\nAny idea what's going wrong here", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1362", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1362/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1362/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1362/events", "html_url": "https://github.com/databricks/koalas/issues/1362", "id": 586210773, "node_id": "MDU6SXNzdWU1ODYyMTA3NzM=", "number": 1362, "title": "Job aborted due to stage failure when running in AWS EMR", "user": {"login": "rabejens", "id": 8402886, "node_id": "MDQ6VXNlcjg0MDI4ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/8402886?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rabejens", "html_url": "https://github.com/rabejens", "followers_url": "https://api.github.com/users/rabejens/followers", "following_url": "https://api.github.com/users/rabejens/following{/other_user}", "gists_url": "https://api.github.com/users/rabejens/gists{/gist_id}", "starred_url": "https://api.github.com/users/rabejens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rabejens/subscriptions", "organizations_url": "https://api.github.com/users/rabejens/orgs", "repos_url": "https://api.github.com/users/rabejens/repos", "events_url": "https://api.github.com/users/rabejens/events{/privacy}", "received_events_url": "https://api.github.com/users/rabejens/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1351987327, "node_id": "MDU6TGFiZWwxMzUxOTg3MzI3", "url": "https://api.github.com/repos/databricks/koalas/labels/not%20a%20koalas%20issue", "name": "not a koalas issue", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-03-23T13:31:28Z", "updated_at": "2020-08-17T06:49:26Z", "closed_at": "2020-03-23T14:10:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "I set up an EMR 6.0.0 Cluster on AWS. I then added an EMR notebook, and installed Koalas in it. When I try to run the [Getting Started](https://koalas.readthedocs.io/en/latest/getting_started/10min.html), at point 3, I get:\r\n\r\n```\r\nAn error occurred while calling o166.collectToPython.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, ip-172-31-6-32.eu-west-1.compute.internal, executor 5): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1584968958146_0001_0\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\r\n\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\r\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2041)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2029)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2028)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:966)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3263)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:84)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1584968958146_0001_0\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\r\n\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\r\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/1584969180422-0/lib/python3.7/site-packages/databricks/koalas/series.py\", line 4568, in __repr__\r\n    pser = self.head(max_display_count + 1)._to_internal_pandas()\r\n  File \"/tmp/1584969180422-0/lib/python3.7/site-packages/databricks/koalas/series.py\", line 4561, in _to_internal_pandas\r\n    return _col(self._internal.to_pandas_frame)\r\n  File \"/tmp/1584969180422-0/lib/python3.7/site-packages/databricks/koalas/utils.py\", line 387, in _lazy_property\r\n    setattr(self, attr_name, fn(self))\r\n  File \"/tmp/1584969180422-0/lib/python3.7/site-packages/databricks/koalas/internal.py\", line 798, in to_pandas_frame\r\n    pdf = sdf.toPandas()\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2143, in toPandas\r\n    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 534, in collect\r\n    sock_info = self._jdf.collectToPython()\r\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\r\n    return f(*a, **kw)\r\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\n    format(target_id, \".\", name), value)\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o166.collectToPython.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, ip-172-31-6-32.eu-west-1.compute.internal, executor 5): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1584968958146_0001_0\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\r\n\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\r\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2041)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2029)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2028)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:966)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:966)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3263)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:84)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --system-site-packages virtualenv_application_1584968958146_0001_0\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:120)\r\n\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:78)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:85)\r\n\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:118)\r\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:118)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:109)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n```\r\n\r\nSteps to reproduce:\r\n\r\n1. Set up an EMR Cluster with EMR 6.0.0, Hadoop and Spark active, deactivate all other services\r\n2. Let the cluster boot, takes about 10 minutes\r\n3. Create an EMR Notebook, attach it to the cluster\r\n4. Open the notebook in Jupyter, change the Kernel to PySpark\r\n5. Execute the following:\r\n  ```python\r\n  print(\"This will set up the Spark connection\")\r\n  ```\r\n6. Wait for `SparkSession available as 'spark'` message\r\n7. Execute:\r\n  ```python\r\n  sc.install_pypi_package(\"koalas\")\r\n  ```\r\n8. Execute:\r\n  ```python\r\n  import pandas as pd\r\n  import numpy as np\r\n  import databricks.koalas as ks\r\n  from pyspark.sql import SparkSession\r\n  s = ks.Series([1, 3, 5, np.nan, 6, 8])\r\n  s\r\n  ```\r\n\r\nExpected: The series is printed out. Actual: The error above happens.\r\n\r\n## Followup\r\n\r\nOn the second try, the error already happens at step 7", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1361", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1361/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1361/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1361/events", "html_url": "https://github.com/databricks/koalas/issues/1361", "id": 586163217, "node_id": "MDU6SXNzdWU1ODYxNjMyMTc=", "number": 1361, "title": "Duplicated key when groupby -> apply is used", "user": {"login": "syonekura", "id": 13004870, "node_id": "MDQ6VXNlcjEzMDA0ODcw", "avatar_url": "https://avatars3.githubusercontent.com/u/13004870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syonekura", "html_url": "https://github.com/syonekura", "followers_url": "https://api.github.com/users/syonekura/followers", "following_url": "https://api.github.com/users/syonekura/following{/other_user}", "gists_url": "https://api.github.com/users/syonekura/gists{/gist_id}", "starred_url": "https://api.github.com/users/syonekura/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syonekura/subscriptions", "organizations_url": "https://api.github.com/users/syonekura/orgs", "repos_url": "https://api.github.com/users/syonekura/repos", "events_url": "https://api.github.com/users/syonekura/events{/privacy}", "received_events_url": "https://api.github.com/users/syonekura/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-03-23T12:13:15Z", "updated_at": "2020-04-13T01:11:19Z", "closed_at": "2020-04-13T01:11:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I opened this thread in [SO](https://stackoverflow.com/questions/60790960/koalas-groupby-apply-returns-cannot-insert-key-already-exists), but viewing other issues reported here ([[1]](https://github.com/databricks/koalas/issues/1300), [[2]](https://github.com/databricks/koalas/issues/1007)), I'm suspecting that this could also be a bug.\r\n\r\nI'm not sure if I'm missing something about the groupby - apply when moving from pandas to koalas, but the only requirement that we have is that the function to be applied cannot be type annotated since it will be used in different contexts, with different dataframes, so we don't know all the columns in advance. We're aware that koalas must run the function to infer the schema before applying the function, and we're ok with that overhead", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1353", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1353/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1353/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1353/events", "html_url": "https://github.com/databricks/koalas/issues/1353", "id": 583622847, "node_id": "MDU6SXNzdWU1ODM2MjI4NDc=", "number": 1353, "title": "minmax scaler not wotking with koalas dataframe", "user": {"login": "akashkumar398", "id": 24894328, "node_id": "MDQ6VXNlcjI0ODk0MzI4", "avatar_url": "https://avatars3.githubusercontent.com/u/24894328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akashkumar398", "html_url": "https://github.com/akashkumar398", "followers_url": "https://api.github.com/users/akashkumar398/followers", "following_url": "https://api.github.com/users/akashkumar398/following{/other_user}", "gists_url": "https://api.github.com/users/akashkumar398/gists{/gist_id}", "starred_url": "https://api.github.com/users/akashkumar398/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akashkumar398/subscriptions", "organizations_url": "https://api.github.com/users/akashkumar398/orgs", "repos_url": "https://api.github.com/users/akashkumar398/repos", "events_url": "https://api.github.com/users/akashkumar398/events{/privacy}", "received_events_url": "https://api.github.com/users/akashkumar398/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552133, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMz", "url": "https://api.github.com/repos/databricks/koalas/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": "This will not be worked on"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-18T10:23:37Z", "updated_at": "2020-03-18T12:38:40Z", "closed_at": "2020-03-18T12:37:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "the below code resulting in error with koalas data frame but working fine with pandas data-frame \r\n# Scale the training feature\r\n\r\ndf = ks.DataFrame(df)\r\nscaler = MinMaxScaler(feature_range=(0, 1)).fit(df)\r\ntrain_scaled = scaler.transform(df)\r\n\r\nValueError: cannot copy sequence with size 1375219 to array axis with dimension 323", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1341", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1341/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1341/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1341/events", "html_url": "https://github.com/databricks/koalas/issues/1341", "id": 580030669, "node_id": "MDU6SXNzdWU1ODAwMzA2Njk=", "number": 1341, "title": "[Question] read_spark_io partition_col, numPartition values and head", "user": {"login": "zacqed", "id": 1675591, "node_id": "MDQ6VXNlcjE2NzU1OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1675591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zacqed", "html_url": "https://github.com/zacqed", "followers_url": "https://api.github.com/users/zacqed/followers", "following_url": "https://api.github.com/users/zacqed/following{/other_user}", "gists_url": "https://api.github.com/users/zacqed/gists{/gist_id}", "starred_url": "https://api.github.com/users/zacqed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zacqed/subscriptions", "organizations_url": "https://api.github.com/users/zacqed/orgs", "repos_url": "https://api.github.com/users/zacqed/repos", "events_url": "https://api.github.com/users/zacqed/events{/privacy}", "received_events_url": "https://api.github.com/users/zacqed/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-12T15:43:34Z", "updated_at": "2020-03-22T14:29:07Z", "closed_at": "2020-03-22T14:29:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "Code:\r\n```\r\nfrom pyspark import SparkContext, SparkConf, SQLContext\r\n\r\nappName = \"PySpark SQL Server Example - via JDBC\"\r\nmaster = \"local[*]\"\r\nconf = SparkConf() \\\r\n    .setAppName(appName) \\\r\n    .setMaster(master) \\\r\n    .set(\"spark.driver.extraClassPath\",\"c:/program files/sqljdbc/enu/mssql-jdbc-8.2.1.jre8.jar\")\\\r\n    .set('spark.executor.memory', '4g') \\\r\n    .set(\"spark.sql.execution.arrow.enabled\", True)\r\n\r\nsc = SparkContext.getOrCreate(conf=conf)\r\nsqlContext = SQLContext(sc)\r\nspark = sqlContext.sparkSession\r\n\r\nhostname = \"LAPTOP\"\r\ndatabase = \"DB\"\r\nport = \"1433\"\r\ntable = \"report_mart\"\r\nuser = \"google\"\r\npassword  = \"google\"\r\n\r\nquery=\"SELECT TOP (500000) * FROM report_mart\"\r\n\r\nimport databricks.koalas as ks\r\nks.set_option('compute.default_index_type', 'distributed')\r\n\r\nconn = f\"conn string;\"\r\ndataframe = ks.read_spark_io(query = query, format='JDBC',url = conn, index_col='date', partition_cols = 'date', numPartitions=10)\r\ndataframe.head(1000)\r\n```\r\nTwo questions here:\r\n1. Even after specifying numPartitions and partition_cols, the explain function shows numPartitions as 1. Is there something amiss here?\r\n`Relation[created_on#0,changed_on#1,id#2,instu_id#3,ecl_model_name#4,product_group#5,productgroup_desc#6,product_code#7,category#8,currency_code#9,counterparty_id#10,maturity_date#11,al_position#12,outstanding#13,outstanding_LCY#14,flag_revolving#15,limit_amount#16,notional_amount#17,deliquency_days#18,billing_account#19,start_date#20,facility#21,frr#22,account_status#23,... 50 more fields] JDBCRelation((SELECT TOP (500000) * FROM report_mart) SPARK_GEN_SUBQ_0) [numPartitions=1]`\r\n\r\n2. dataframe.head(1000) takes close to a minute to print results. I am using Sypder, should i try another IDE? Some more information when checking spark jobs, when executing head command, toPandas() gets called (refer image below);\r\n![image](https://user-images.githubusercontent.com/1675591/76539890-64b32700-64a7-11ea-98ae-cecc0aabf385.png)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1332", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1332/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1332/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1332/events", "html_url": "https://github.com/databricks/koalas/issues/1332", "id": 577780180, "node_id": "MDU6SXNzdWU1Nzc3ODAxODA=", "number": 1332, "title": "Return type consistency in docs", "user": {"login": "sorenmc", "id": 42963644, "node_id": "MDQ6VXNlcjQyOTYzNjQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/42963644?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sorenmc", "html_url": "https://github.com/sorenmc", "followers_url": "https://api.github.com/users/sorenmc/followers", "following_url": "https://api.github.com/users/sorenmc/following{/other_user}", "gists_url": "https://api.github.com/users/sorenmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/sorenmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sorenmc/subscriptions", "organizations_url": "https://api.github.com/users/sorenmc/orgs", "repos_url": "https://api.github.com/users/sorenmc/repos", "events_url": "https://api.github.com/users/sorenmc/events{/privacy}", "received_events_url": "https://api.github.com/users/sorenmc/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-03-09T09:48:32Z", "updated_at": "2020-03-16T11:37:10Z", "closed_at": "2020-03-11T09:34:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "It is currently not transparent what the return type of a kolas dataframe / series is.\r\n\r\nTake for example\r\n[DataFrame.max()](https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.DataFrame.max.html)\r\n\r\n> Returns\r\nmaxscalar for a Series, and a Series for a DataFrame.\r\n\r\nReturned type i receive is a pandas.Series\r\n\r\n[Series.value_counts()](https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.Series.value_counts.html)\r\n\r\n> Returns\r\ncounts Series\r\n\r\nReturned type i receive is a databricks.koalas.series.Series\r\n\r\nI would like to suggest that it is made clear in the docs across all functions what the return type is in all cases\r\n\r\nIn both these cases it is unclear what type of series is returned", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1320", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1320/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1320/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1320/events", "html_url": "https://github.com/databricks/koalas/issues/1320", "id": 574373445, "node_id": "MDU6SXNzdWU1NzQzNzM0NDU=", "number": 1320, "title": "Implement SeriesGroupBy.unique", "user": {"login": "HyukjinKwon", "id": 6477701, "node_id": "MDQ6VXNlcjY0Nzc3MDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6477701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HyukjinKwon", "html_url": "https://github.com/HyukjinKwon", "followers_url": "https://api.github.com/users/HyukjinKwon/followers", "following_url": "https://api.github.com/users/HyukjinKwon/following{/other_user}", "gists_url": "https://api.github.com/users/HyukjinKwon/gists{/gist_id}", "starred_url": "https://api.github.com/users/HyukjinKwon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HyukjinKwon/subscriptions", "organizations_url": "https://api.github.com/users/HyukjinKwon/orgs", "repos_url": "https://api.github.com/users/HyukjinKwon/repos", "events_url": "https://api.github.com/users/HyukjinKwon/events{/privacy}", "received_events_url": "https://api.github.com/users/HyukjinKwon/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-03T02:27:19Z", "updated_at": "2020-03-12T06:55:48Z", "closed_at": "2020-03-12T06:55:47Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "```python\r\n>>> import pandas as pd\r\n>>> s = pd.Series([1,2,3])\r\n>>> s.groupby(s).count()\r\n1    1\r\n2    1\r\n3    1\r\ndtype: int64\r\n```\r\n\r\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.SeriesGroupBy.unique.html", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1312", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1312/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1312/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1312/events", "html_url": "https://github.com/databricks/koalas/issues/1312", "id": 571354548, "node_id": "MDU6SXNzdWU1NzEzNTQ1NDg=", "number": 1312, "title": "Databricks runtime not supported", "user": {"login": "sorenmc", "id": 42963644, "node_id": "MDQ6VXNlcjQyOTYzNjQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/42963644?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sorenmc", "html_url": "https://github.com/sorenmc", "followers_url": "https://api.github.com/users/sorenmc/followers", "following_url": "https://api.github.com/users/sorenmc/following{/other_user}", "gists_url": "https://api.github.com/users/sorenmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/sorenmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sorenmc/subscriptions", "organizations_url": "https://api.github.com/users/sorenmc/orgs", "repos_url": "https://api.github.com/users/sorenmc/repos", "events_url": "https://api.github.com/users/sorenmc/events{/privacy}", "received_events_url": "https://api.github.com/users/sorenmc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-26T13:17:36Z", "updated_at": "2020-02-26T15:04:25Z", "closed_at": "2020-02-26T13:37:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am currently trying to migrate a function utilizing pandas to koalas. I am using a databricks notebook to do this, but i receive an error when i try to import koalas, that seems like it is related to pandas. \r\n\r\nI run Databricks runtime 5.5 LTS (includes Apache Spark 2.4.3, Scala 2.11), and have tried to install koalas 3 different ways on the cluster including pip, dbutils and the inbuilt cluster package manager.\r\nI have also tried playing around with different pandas and koalas versions to see if that would fix the issue.\r\n\r\n```\r\ndbutils.library.installPyPI(\"koalas\")\r\ndbutils.library.restartPython()\r\nimport databricks.koalas as ks\r\n```\r\n\r\nreturns the traceback:\r\n\r\n> ImportError: No module named 'pandas.core.dtypes'\r\n> ---------------------------------------------------------------------------\r\n> ImportError                               Traceback (most recent call last)\r\n> <command-3721303037916941> in <module>()\r\n>       3 # import pandas.core.dtypes.common\r\n>       4 print(pd.core)\r\n> ----> 5 import databricks.koalas as ks\r\n>       6 \r\n>       7 raw_sa = os.getenv('RAW_STORAGEACCOUNT')\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/databricks/koalas/__init__.py in <module>()\r\n>      36 assert_pyspark_version()\r\n>      37 \r\n> ---> 38 from databricks.koalas.namespace import *\r\n>      39 from databricks.koalas.frame import DataFrame\r\n>      40 from databricks.koalas.indexes import Index, MultiIndex\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/databricks/koalas/namespace.py in <module>()\r\n>      32 from databricks import koalas as ks  # For running doctests and reference resolution in PyCharm.\r\n>      33 from databricks.koalas.utils import default_session\r\n> ---> 34 from databricks.koalas.frame import DataFrame, _reduce_spark_multi\r\n>      35 from databricks.koalas.typedef import Col, pandas_wraps\r\n>      36 from databricks.koalas.series import Series, _col\r\n> \r\n> /usr/local/lib/python3.5/dist-packages/databricks/koalas/frame.py in <module>()\r\n>      26 import pandas as pd\r\n>      27 from pandas.api.types import is_list_like, is_dict_like\r\n> ---> 28 from pandas.core.dtypes.inference import is_sequence\r\n>      29 from pyspark import sql as spark\r\n>      30 from pyspark.sql import functions as F, Column\r\n> \r\n> ImportError: No module named 'pandas.core.dtypes'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1307", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1307/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1307/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1307/events", "html_url": "https://github.com/databricks/koalas/issues/1307", "id": 570768788, "node_id": "MDU6SXNzdWU1NzA3Njg3ODg=", "number": 1307, "title": "DataFrame.apply(func, axis=1) returns Series with unmatched index", "user": {"login": "patryk-oleniuk", "id": 6854491, "node_id": "MDQ6VXNlcjY4NTQ0OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6854491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patryk-oleniuk", "html_url": "https://github.com/patryk-oleniuk", "followers_url": "https://api.github.com/users/patryk-oleniuk/followers", "following_url": "https://api.github.com/users/patryk-oleniuk/following{/other_user}", "gists_url": "https://api.github.com/users/patryk-oleniuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/patryk-oleniuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patryk-oleniuk/subscriptions", "organizations_url": "https://api.github.com/users/patryk-oleniuk/orgs", "repos_url": "https://api.github.com/users/patryk-oleniuk/repos", "events_url": "https://api.github.com/users/patryk-oleniuk/events{/privacy}", "received_events_url": "https://api.github.com/users/patryk-oleniuk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-02-25T19:03:51Z", "updated_at": "2020-06-19T03:06:39Z", "closed_at": "2020-06-19T03:06:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Env: koalas 0.27.0, Databricks DBR 6.2.\r\n\r\nConsider the following code for pandas:\r\n```python\r\ndf = pd.DataFrame({\"timestamp\":[0.0, 0.5, 1.0, 0.0, 0.5, 100.0, 101.0, 102.0],\r\n              \"car_id\": ['A','A','A','B','B', 'C', 'C', 'C'], \r\n              \"battery_charge\": [100, 90, 80,100,90, 50, 60, 70]\r\n              })\r\n\r\ndf2 = df.groupby(\"car_id\").sum()\r\n\r\ndef calc_somthing(row) -> float:\r\n  # doing some job per row\r\n  return row['battery_charge']/(row['timestamp'] +1)\r\n\r\nprint(df2.apply(calc_somthing, axis=1))\r\n```\r\nThat prints a series with the proper index: \r\n```bash\r\ncar_id\r\nA    108.000000\r\nB    126.666667\r\nC      0.592105\r\ndtype: float64\r\n```\r\nBut if we do the same for koalas, we get a wrong index (and also wrong name, BTW):\r\n```bash\r\n0    126.666664\r\n1    108.000000\r\n2      0.592105\r\nName: 0, dtype: float32\r\n```\r\nThat causes a lot of problems when merging that index with a new df, e.g. \r\n```python\r\ndf2[\"newcol\"] = df2.apply(calc_somthing, axis=1)\r\n``` \r\nThe workaround is to always reset_index before apply.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1305", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1305/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1305/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1305/events", "html_url": "https://github.com/databricks/koalas/issues/1305", "id": 570010666, "node_id": "MDU6SXNzdWU1NzAwMTA2NjY=", "number": 1305, "title": "Index.to_series() works not properly", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-24T17:35:58Z", "updated_at": "2020-03-02T18:50:31Z", "closed_at": "2020-03-02T18:50:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When converting an Index to Series for comparing operation like the below,\r\n\r\nthere is something problem.\r\n\r\n```python\r\n>>> pidx = pd.Index([1, 2, 3, 4, 5])\r\n>>> kidx1 = ks.Index([1, 2, 3, 4, 5])\r\n>>> kidx2 = ks.Index(pidx)\r\n>>> kidx3 = ks.from_pandas(pidx)\r\n>>> kidx1.to_series() == kidx2.to_series() == kidx3.to_series()\r\nTraceback (most recent call last):\r\n...\r\nAssertionError: (1, 0)\r\n```\r\n\r\nThe existing implementation seems only can convert from index to series properly when the index is came from DataFrame like the below.\r\n\r\n```python\r\n>>> df = ks.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\r\n...                   columns=['dogs', 'cats'],\r\n...                   index=list('abcd'))\r\n>>> df['dogs'].index.to_series() == df['cats'].index.to_series()\r\na    True\r\nb    True\r\nc    True\r\nd    True\r\nName: 0, dtype: bool\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1302", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1302/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1302/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1302/events", "html_url": "https://github.com/databricks/koalas/issues/1302", "id": 569025876, "node_id": "MDU6SXNzdWU1NjkwMjU4NzY=", "number": 1302, "title": "Implement 'keep' parameter for ``drop_duplicates``", "user": {"login": "deepyaman", "id": 14007150, "node_id": "MDQ6VXNlcjE0MDA3MTUw", "avatar_url": "https://avatars3.githubusercontent.com/u/14007150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepyaman", "html_url": "https://github.com/deepyaman", "followers_url": "https://api.github.com/users/deepyaman/followers", "following_url": "https://api.github.com/users/deepyaman/following{/other_user}", "gists_url": "https://api.github.com/users/deepyaman/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepyaman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepyaman/subscriptions", "organizations_url": "https://api.github.com/users/deepyaman/orgs", "repos_url": "https://api.github.com/users/deepyaman/repos", "events_url": "https://api.github.com/users/deepyaman/events{/privacy}", "received_events_url": "https://api.github.com/users/deepyaman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-02-21T15:34:13Z", "updated_at": "2020-02-25T23:11:13Z", "closed_at": "2020-02-25T23:11:13Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Possible implementation #1303 ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1300", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1300/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1300/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1300/events", "html_url": "https://github.com/databricks/koalas/issues/1300", "id": 568751162, "node_id": "MDU6SXNzdWU1Njg3NTExNjI=", "number": 1300, "title": "Unexpected Koalas groupby.apply groupby.agg behavior", "user": {"login": "matthewgson", "id": 46496637, "node_id": "MDQ6VXNlcjQ2NDk2NjM3", "avatar_url": "https://avatars2.githubusercontent.com/u/46496637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthewgson", "html_url": "https://github.com/matthewgson", "followers_url": "https://api.github.com/users/matthewgson/followers", "following_url": "https://api.github.com/users/matthewgson/following{/other_user}", "gists_url": "https://api.github.com/users/matthewgson/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthewgson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthewgson/subscriptions", "organizations_url": "https://api.github.com/users/matthewgson/orgs", "repos_url": "https://api.github.com/users/matthewgson/repos", "events_url": "https://api.github.com/users/matthewgson/events{/privacy}", "received_events_url": "https://api.github.com/users/matthewgson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-21T05:51:34Z", "updated_at": "2020-03-20T03:11:29Z", "closed_at": "2020-03-20T03:11:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I found some unexpected groupby.agg or groupby.apply behavior.\r\nHere's a reproducible example.\r\n\r\n```\r\ntest = pd.DataFrame({'secid': {0: 125047, 1: 125047, 2: 125047, 3: 125047, 4: 125047, 5: 125047},\r\n 'date': {0: pd.Timestamp('2018-03-05 00:00:00'),\r\n  1: pd.Timestamp('2018-03-05 00:00:00'),\r\n  2: pd.Timestamp('2018-03-05 00:00:00'),\r\n  3: pd.Timestamp('2018-03-05 00:00:00'),\r\n  4: pd.Timestamp('2018-03-05 00:00:00'),\r\n  5: pd.Timestamp('2018-03-05 00:00:00')},\r\n 'days': {0: 182, 1: 182, 2: 182, 3: 182, 4: 182, 5: 182},\r\n 'delta': {0: -20, 1: -15, 2: -10, 3: 10, 4: 15, 5: 20},\r\n 'impl_volatility': {0: 0.251335,\r\n  1: 0.321205,\r\n  2: 0.433905,\r\n  3: 0.327283,\r\n  4: 0.307678,\r\n  5: 0.28428},\r\n'abs_delta': {0: 20, 1: 15, 2: 10, 3: 10, 4: 15, 5: 20}})\r\n\r\n\r\ndef my_divider(x) -> float:\r\n    try:\r\n        return x.iloc[0]/x.iloc[1]\r\n    except IndexError: \r\n        pass\r\n\r\n\r\ntest\r\nsecid | date | days | delta | impl_volatility | abs_delta\r\n-- | -- | -- | -- | -- | --\r\n125047 | 2018-03-05 | 182 | -20 | 0.251335 | 20\r\n125047 | 2018-03-05 | 182 | -15 | 0.321205 | 15\r\n125047 | 2018-03-05 | 182 | -10 | 0.433905 | 10\r\n125047 | 2018-03-05 | 182 | 10 | 0.327283 | 10\r\n125047 | 2018-03-05 | 182 | 15 | 0.307678 | 15\r\n125047 | 2018-03-05 | 182 | 20 | 0.284280 | 20\r\n\r\n\r\n```\r\nI was trying to group by multiple columns and calculate the ratio using ```my_divider``` function. \r\n\r\nWhen I run below codes in pandas, it works as expected.\r\n```\r\ntest.groupby(['secid','date','days','abs_delta'])['impl_volatility'].apply(sum)\r\n>>>\r\nsecid   date        days  abs_delta\r\n125047  2018-03-05  182   10           0.761188\r\n                          15           0.628883\r\n                          20           0.535615\r\nName: impl_volatility, dtype: float64\r\n\r\ntest.groupby(['secid','date','days','abs_delta'])['impl_volatility'].apply(my_divider)\r\n>>>secid   date        days  abs_delta\r\n125047  2018-03-05  182   10           1.325779\r\n                          15           1.043965\r\n                          20           0.884111\r\nName: impl_volatility, dtype: float64\r\n\r\ntest.groupby(['secid','date','days','abs_delta']).agg(vol_ratio = ('impl_volatility',my_divider))\r\n>>>\r\nsecid\tdate\tdays\tabs_delta\tvol_ratio\r\n125047\t2018-03-05\t182\t10\t1.325779\r\n                                                15\t1.043965\r\n                                                20\t0.884111\r\n```\r\n\r\n\r\nHowever, in Koalas dataframe\r\n\r\n```\r\ntest2 = ks.from_pandas(test)\r\ntest2.groupby(['secid','date','days','abs_delta'])['impl_volatility'].apply(sum)\r\n>>>\r\n0    0.535615\r\n1    0.628883\r\n2    0.761188\r\n3    0.761188\r\n4    0.628883\r\n5    0.535615\r\n# six outputs (three * 2 duplicates)\r\n\r\n\r\ntest2.groupby(['secid','date','days','abs_delta']).agg(mycolumn = ('impl_volatility','sum'))\r\n>>>\r\nsecid\tdate\tdays\tabs_delta\tmycolumn\r\n125047\t2018-03-05\t182\t15\t0.628883\r\n                                                20\t0.535615\r\n                                                10\t0.761188\r\n\r\n```\r\n.agg works with 'sum' function as expected but .agg does not understand function defined above.\r\n```\r\ntest2.groupby(['secid','date','days','abs_delta']).agg(mycolumn = ('impl_volatility','my_divider'))\r\n>>>\r\nPy4JJavaError: An error occurred while calling o3520.agg.\r\n: org.apache.spark.sql.AnalysisException: Undefined function: 'my_divider'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'\r\nAnalysisException: \"Undefined function: 'my_divider'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 0\"\r\n```\r\nSo I tried registering function, but\r\n```\r\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\r\n@pandas_udf('double') \r\ndef my_divider_udf(x):\r\n    try:\r\n        return x.iloc[0]/x.iloc[1]\r\n    except:\r\n        pass\r\n\r\nspark.udf.register('my_udf',my_divider_udf)\r\n\r\n```\r\nI get \r\n```\r\n\r\ntest2.groupby(['secid','date','days','abs_delta']).agg(pc_ratio=('impl_volatility','my_udf'))\r\n\r\nPy4JJavaError: An error occurred while calling o2588.agg.\r\n: org.apache.spark.sql.AnalysisException: expression '`impl_volatility`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;\r\n```\r\n\r\n\r\n\r\n\r\nWhere .apply throws different type of error.\r\n```\r\ntest2.groupby(['secid','date','days','abs_delta'])['impl_volatility'].apply(my_divider)\r\n>>>\r\nPy4JJavaError: An error occurred while calling o1084.collectToPython.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 94 in stage 22.0 failed 1 times, most recent failure: Lost task 94.0 in stage 22.0 (TID 167, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\r\n    for series in iterator:\r\n  File \"<string>\", line 1, in <lambda>\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 113, in wrapped\r\n    result = f(pd.concat(value_series, axis=1))\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/databricks/koalas/groupby.py\", line 978, in rename_output\r\n    pdf = func(pdf)\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/databricks/koalas/groupby.py\", line 1628, in pandas_transform\r\n    return pdf.transform(func)\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 6735, in transform\r\n    return super().transform(func, *args, **kwargs)\r\n  File \"/Users/matthewson/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py\", line 10814, in transform\r\n    raise ValueError(\"transforms cannot produce \" \"aggregated results\")\r\nValueError: transforms cannot produce aggregated results\r\n\r\n```\r\n\r\nI'm not sure why I'm seeing last two errors in Koalas with my limited experience. I feel I might be doing something wrong here.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1297", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1297/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1297/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1297/events", "html_url": "https://github.com/databricks/koalas/issues/1297", "id": 568397227, "node_id": "MDU6SXNzdWU1NjgzOTcyMjc=", "number": 1297, "title": "Can't writeStream back to kafka", "user": {"login": "DaveOuds", "id": 24455900, "node_id": "MDQ6VXNlcjI0NDU1OTAw", "avatar_url": "https://avatars3.githubusercontent.com/u/24455900?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DaveOuds", "html_url": "https://github.com/DaveOuds", "followers_url": "https://api.github.com/users/DaveOuds/followers", "following_url": "https://api.github.com/users/DaveOuds/following{/other_user}", "gists_url": "https://api.github.com/users/DaveOuds/gists{/gist_id}", "starred_url": "https://api.github.com/users/DaveOuds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DaveOuds/subscriptions", "organizations_url": "https://api.github.com/users/DaveOuds/orgs", "repos_url": "https://api.github.com/users/DaveOuds/repos", "events_url": "https://api.github.com/users/DaveOuds/events{/privacy}", "received_events_url": "https://api.github.com/users/DaveOuds/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-20T15:52:55Z", "updated_at": "2020-02-20T21:52:51Z", "closed_at": "2020-02-20T21:52:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi all!\r\nI'm consuming data from kafka, which I want to parse using koalas and afterwards produce it back to kafka. \r\nHowever, whenever I try to write a stream it wont work. Depending on the output mode of the stream I get one of two errors:\r\n\r\n- Update/Append:\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o90.start.\r\n: org.apache.spark.sql.AnalysisException: Expression(s): monotonically_increasing_id() is not supported with streaming DataFrames/Datasets;\r\n\r\n- Complete:\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o90.start.\r\n: org.apache.spark.sql.AnalysisException: Complete output mode not supported when there are no streaming aggregations on streaming DataFrames/Datasets\r\nThis is what my code looks like:\r\n```python\r\nimport os\r\nimport databricks.koalas as ks\r\n\r\nfrom pyspark.sql import SparkSession\r\nfrom parse import parseData\r\nfrom analyze import analyze\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    bootstrapServers = os.environ.get(\"KAFKA_BOOTSTRAP_HOST\", \"127.0.0.1\") + \":\" + \\\r\n                       os.environ.get(\"KAFKA_BOOTSTRAP_PORT\", \"9092\")\r\n\r\n    # Create Spark Session \r\n    spark = SparkSession \\\r\n        .builder \\\r\n        .appName(\"PythonStructuredKafkaWordCount\") \\\r\n        .getOrCreate()\r\n\r\n    spark.sparkContext.setLogLevel(\"ERROR\")\r\n\r\n    # Consume data from Kafka of topic ra-ingest\r\n    sdf = spark \\\r\n        .readStream \\\r\n        .format(\"kafka\") \\\r\n        .option(\"kafka.bootstrap.servers\", bootstrapServers) \\\r\n        .option(\"subscribe\", \"input-topic\") \\\r\n        .option(\"failOnDataLoss\", \"false\") \\\r\n        .option(\"truncate\", \"false\") \\\r\n        .load() \\\r\n        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\r\n\r\n    # Convert S(park)DF to K(oalas)DF\r\n    kdf = ks.DataFrame(sdf)\r\n\r\n    # Convert K(oalas)DF back to S(park)DF\r\n    new_sdf = kdf.to_spark()\r\n  \r\n    # Produce the data to kafka on topic ra-result\r\n    query = new_sdf \\\r\n        .selectExpr(\"to_json(struct(*)) AS value\") \\\r\n        .writeStream \\\r\n        .outputMode(\"complete\") \\\r\n        .format(\"kafka\") \\\r\n        .option(\"kafka.bootstrap.servers\", bootstrapServers) \\\r\n        .option(\"topic\", \"output-topic\") \\\r\n        .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\\r\n        .start()\r\n\r\n    query.awaitTermination()\r\n```\r\n\r\nAm I doing something wrong or is Koalas not compatible with Spark Structured Streaming?\r\n\r\nBest,\r\n\r\nDave", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1282", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1282/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1282/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1282/events", "html_url": "https://github.com/databricks/koalas/issues/1282", "id": 565132456, "node_id": "MDU6SXNzdWU1NjUxMzI0NTY=", "number": 1282, "title": "ops_on_diff_frames  will cause a big performance overhead?", "user": {"login": "GODLMM", "id": 39004221, "node_id": "MDQ6VXNlcjM5MDA0MjIx", "avatar_url": "https://avatars1.githubusercontent.com/u/39004221?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GODLMM", "html_url": "https://github.com/GODLMM", "followers_url": "https://api.github.com/users/GODLMM/followers", "following_url": "https://api.github.com/users/GODLMM/following{/other_user}", "gists_url": "https://api.github.com/users/GODLMM/gists{/gist_id}", "starred_url": "https://api.github.com/users/GODLMM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GODLMM/subscriptions", "organizations_url": "https://api.github.com/users/GODLMM/orgs", "repos_url": "https://api.github.com/users/GODLMM/repos", "events_url": "https://api.github.com/users/GODLMM/events{/privacy}", "received_events_url": "https://api.github.com/users/GODLMM/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-02-14T06:29:26Z", "updated_at": "2020-03-12T09:38:41Z", "closed_at": "2020-03-12T09:38:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, when i want to do some operations on the slice of original df , it will make a big performance overhead.\r\n```\r\nimport gc\r\nimport time\r\nimport numpy as np\r\nimport databricks.koalas as ks\r\nks.set_option(\"compute.ops_on_diff_frames\", True)\r\n\r\n# first loop\r\nraw = np.random.randint(0, 9, (10000, ))\r\nkdf = ks.DataFrame(raw, columns=['c_01'])\r\nprint('first:')\r\nfor i in range(3):\r\n    begin = time.time()\r\n    for i in range(2):\r\n        feed_dict = kdf['c_01'].value_counts().to_dict()\r\n        kdf['x_{}'.format(i)] = kdf['c_01'].map(feed_dict)\r\n    print(time.time() - begin)\r\n\r\n# second loop\r\nprint('second:')\r\nraw = np.random.randint(0, 9, (10000, ))\r\nkdf_2 = ks.DataFrame(raw, columns=['c_01'])\r\nfor i in range(3):\r\n    begin = time.time()\r\n    for i in range(2):\r\n        # something different here\r\n        x = kdf_2[['c_01']]\r\n        feed_dict = x['c_01'].value_counts().to_dict()\r\n        kdf_2['x_{}'.format(i)] = x['c_01'].map(feed_dict)\r\n    print(time.time() - begin)\r\n```\r\nthe output:\r\n```\r\nfirst:\r\n2.279035806655884\r\n0.7010071277618408\r\n0.5962021350860596\r\nsecond:\r\n1.5885398387908936\r\n2.4906740188598633\r\n4.307945966720581\r\n```\r\nWhat can i do to avoid it? thx", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1280", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1280/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1280/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1280/events", "html_url": "https://github.com/databricks/koalas/issues/1280", "id": 565057385, "node_id": "MDU6SXNzdWU1NjUwNTczODU=", "number": 1280, "title": "df.apply(func_without_typehint, axis=1) is not running in parallel", "user": {"login": "patryk-oleniuk", "id": 6854491, "node_id": "MDQ6VXNlcjY4NTQ0OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6854491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patryk-oleniuk", "html_url": "https://github.com/patryk-oleniuk", "followers_url": "https://api.github.com/users/patryk-oleniuk/followers", "following_url": "https://api.github.com/users/patryk-oleniuk/following{/other_user}", "gists_url": "https://api.github.com/users/patryk-oleniuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/patryk-oleniuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patryk-oleniuk/subscriptions", "organizations_url": "https://api.github.com/users/patryk-oleniuk/orgs", "repos_url": "https://api.github.com/users/patryk-oleniuk/repos", "events_url": "https://api.github.com/users/patryk-oleniuk/events{/privacy}", "received_events_url": "https://api.github.com/users/patryk-oleniuk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-02-14T02:02:13Z", "updated_at": "2020-05-07T00:42:30Z", "closed_at": "2020-05-07T00:42:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I was hoping that each operation, per row is gonna be executed in parallel. \r\nHowever this code (and many other experiments I did, including logging to MLflow) shows that it just executes it sequentially for every row. \r\n```python\r\nimport time\r\nimport databricks.koalas as ks\r\n\r\nkdf = ks.DataFrame({\"col\":[i for i in range(12)]})\r\n\r\ndef do_job( value ) -> float:\r\n  time.sleep(5)\r\n  return value\r\n\r\nkdf.apply(lambda x: do_job(x[\"col\"]) , axis=1)\r\n```\r\nThis is taking 60 sec with > 12 Spark workers. \r\nI was expecting every row to be executed in parallel, that would result in around 5 seconds. \r\n\r\nAm I doing something wrong?\r\n\r\nMy workaround:\r\n```python\r\nkdf.to_spark().rdd.map(lambda x: do_job(x[\"col\"])).collect()\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1268", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1268/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1268/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1268/events", "html_url": "https://github.com/databricks/koalas/issues/1268", "id": 562137987, "node_id": "MDU6SXNzdWU1NjIxMzc5ODc=", "number": 1268, "title": "ImportError when importing koalas on azure databricks runtime 6.3 ML.", "user": {"login": "ttelfer", "id": 4094016, "node_id": "MDQ6VXNlcjQwOTQwMTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/4094016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ttelfer", "html_url": "https://github.com/ttelfer", "followers_url": "https://api.github.com/users/ttelfer/followers", "following_url": "https://api.github.com/users/ttelfer/following{/other_user}", "gists_url": "https://api.github.com/users/ttelfer/gists{/gist_id}", "starred_url": "https://api.github.com/users/ttelfer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ttelfer/subscriptions", "organizations_url": "https://api.github.com/users/ttelfer/orgs", "repos_url": "https://api.github.com/users/ttelfer/repos", "events_url": "https://api.github.com/users/ttelfer/events{/privacy}", "received_events_url": "https://api.github.com/users/ttelfer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1351987327, "node_id": "MDU6TGFiZWwxMzUxOTg3MzI3", "url": "https://api.github.com/repos/databricks/koalas/labels/not%20a%20koalas%20issue", "name": "not a koalas issue", "color": "1d76db", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-09T07:45:48Z", "updated_at": "2020-02-10T00:59:08Z", "closed_at": "2020-02-10T00:59:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "ImportError: cannot import name 'Timedelta' from 'pandas._libs.tslibs' (/databricks/python/lib/python3.7/site-packages/pandas/_libs/tslibs/__init__.py)\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<command-3047841609493545> in <module>\r\n      1 from pyspark.sql import SparkSession\r\n      2 from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\r\n----> 3 import databricks.koalas as ks\r\n      4 import pyspark.sql.functions as F\r\n      5 import json\r\n\r\n/databricks/python/lib/python3.7/site-packages/databricks/koalas/__init__.py in <module>\r\n     47     os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\r\n     48 \r\n---> 49 from databricks.koalas.frame import DataFrame\r\n     50 from databricks.koalas.indexes import Index, MultiIndex\r\n     51 from databricks.koalas.series import Series\r\n\r\n/databricks/python/lib/python3.7/site-packages/databricks/koalas/frame.py in <module>\r\n     30 \r\n     31 import numpy as np\r\n---> 32 import pandas as pd\r\n     33 from pandas.api.types import is_list_like, is_dict_like\r\n     34 if LooseVersion(pd.__version__) >= LooseVersion('0.24'):\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/__init__.py in <module>\r\n     40 import pandas.core.config_init\r\n     41 \r\n---> 42 from pandas.core.api import *\r\n     43 from pandas.core.sparse.api import *\r\n     44 from pandas.tseries.api import *\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/core/api.py in <module>\r\n      8 from pandas.core.dtypes.missing import isna, isnull, notna, notnull\r\n      9 from pandas.core.arrays import Categorical\r\n---> 10 from pandas.core.groupby.groupby import Grouper\r\n     11 from pandas.io.formats.format import set_eng_float_format\r\n     12 from pandas.core.index import (Index, CategoricalIndex, Int64Index,\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/core/groupby/__init__.py in <module>\r\n      1 # flake8: noqa\r\n----> 2 from pandas.core.groupby.groupby import (\r\n      3     Grouper, GroupBy, SeriesGroupBy, DataFrameGroupBy\r\n      4 )\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/core/groupby/groupby.py in <module>\r\n     47                                CategoricalIndex, _ensure_index)\r\n     48 from pandas.core.arrays import ExtensionArray, Categorical\r\n---> 49 from pandas.core.frame import DataFrame\r\n     50 from pandas.core.generic import NDFrame, _shared_docs\r\n     51 from pandas.core.internals import BlockManager, make_block\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/core/frame.py in <module>\r\n     64 \r\n     65 \r\n---> 66 from pandas.core.generic import NDFrame, _shared_docs\r\n     67 from pandas.core.index import (Index, MultiIndex, _ensure_index,\r\n     68                                _ensure_index_from_sequences)\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/core/generic.py in <module>\r\n     41 from pandas.core.indexes.datetimes import DatetimeIndex\r\n     42 from pandas.core.indexes.period import PeriodIndex, Period\r\n---> 43 from pandas.core.internals import BlockManager\r\n     44 import pandas.core.algorithms as algos\r\n     45 import pandas.core.common as com\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/core/internals/__init__.py in <module>\r\n      1 # -*- coding: utf-8 -*-\r\n----> 2 from .blocks import (  # noqa:F401\r\n      3     _block2d_to_blocknd, _factor_indexer, _block_shape,  # io.pytables\r\n      4     _safe_reshape,  # io.packers\r\n      5     make_block,     # io.pytables, io.packers\r\n\r\n/databricks/python/lib/python3.7/site-packages/pandas/core/internals/blocks.py in <module>\r\n      9 \r\n     10 from pandas._libs import internals as libinternals, lib, tslib, tslibs\r\n---> 11 from pandas._libs.tslibs import Timedelta, conversion, is_null_datetimelike\r\n     12 import pandas.compat as compat\r\n     13 from pandas.compat import range, zip\r\n\r\nImportError: cannot import name 'Timedelta' from 'pandas._libs.tslibs' (/databricks/python/lib/python3.7/site-packages/pandas/_libs/tslibs/__init__.py)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1260", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1260/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1260/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1260/events", "html_url": "https://github.com/databricks/koalas/issues/1260", "id": 561560318, "node_id": "MDU6SXNzdWU1NjE1NjAzMTg=", "number": 1260, "title": "When using ks.readcsv, Koalas type inference differs from pandas due to the use of Spark's inferSchema logic", "user": {"login": "Callum027", "id": 2997843, "node_id": "MDQ6VXNlcjI5OTc4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2997843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Callum027", "html_url": "https://github.com/Callum027", "followers_url": "https://api.github.com/users/Callum027/followers", "following_url": "https://api.github.com/users/Callum027/following{/other_user}", "gists_url": "https://api.github.com/users/Callum027/gists{/gist_id}", "starred_url": "https://api.github.com/users/Callum027/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Callum027/subscriptions", "organizations_url": "https://api.github.com/users/Callum027/orgs", "repos_url": "https://api.github.com/users/Callum027/repos", "events_url": "https://api.github.com/users/Callum027/events{/privacy}", "received_events_url": "https://api.github.com/users/Callum027/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1355475110, "node_id": "MDU6TGFiZWwxMzU1NDc1MTEw", "url": "https://api.github.com/repos/databricks/koalas/labels/discussions", "name": "discussions", "color": "f29721", "default": false, "description": ""}, {"id": 1179552127, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI3", "url": "https://api.github.com/repos/databricks/koalas/labels/duplicate", "name": "duplicate", "color": "cfd3d7", "default": true, "description": "This issue or pull request already exists"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-02-07T10:45:16Z", "updated_at": "2020-03-12T09:40:07Z", "closed_at": "2020-03-12T09:40:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi guys,\r\n\r\nI'm using Koalas to combine two sets of similar data, and there is a step where we block rows using a direct comparison on a specific column. As part of this, we remove all `NaN` values from that column using `DataFrame.dropna`, but in our tests, we found that for `float` columns, Koalas would convert that row to an `int` if the values did not have decimal components. This is not exactly undesirable per se, but it IS different from the behaviour in pandas, so I thought I'd at least double check if this is intended behaviour.\r\n\r\nTest data:\r\n```\r\nID,name,birth_year,hourly_wage,address,zipcode\r\nb1,Mark Levene,1987,29.5,\"108 Clement St, San Francisco\",94107\r\nb2,Bill Bridge,1986,32,\"3131 Webster St, San Francisco\",\r\nb3,Mike Franklin,1988,27.5,\"1652 Stockton St, San Francisco\",94122\r\nb4,,1982,26,\"108 South Park, San Francisco\",\r\nb5,Alfons Kemper,1984,35,\"170 Post St, Apt 4,  San Francisco\",94122\r\nb6,Michael Brodie,1987,32.5,,94107\r\n```\r\n\r\nTest script:\r\n```python\r\nimport pandas as pd\r\nfrom databricks import koalas as ks\r\n\r\ncsv_file = 'test.csv'\r\n\r\npandas_A = pd.read_csv(csv_file)\r\nkoalas_A = ks.read_csv(csv_file)\r\n\r\nprint('pandas_A:\\n{}\\n'.format(pandas_A.to_string()))\r\nprint('koalas_A:\\n{}\\n'.format(koalas_A.to_string()))\r\n\r\npandas_A_dropna = pandas_A.dropna(subset=['zipcode'])\r\nkoalas_A_dropna = koalas_A.dropna(subset=['zipcode'])\r\n\r\nprint('pandas_A_dropna:\\n{}\\n'.format(pandas_A_dropna.to_string()))\r\nprint('koalas_A_dropna:\\n{}\\n'.format(koalas_A_dropna.to_string()))\r\n\r\nprint('pandas_A_dropna[zipcode].dtype = {}'.format(pandas_A_dropna['zipcode'].dtype))\r\nprint('koalas_A_dropna[zipcode].dtype = {}'.format(koalas_A_dropna['zipcode'].dtype))\r\n```\r\n\r\nOutput:\r\n```\r\npandas_A:\r\n   ID            name  birth_year  hourly_wage                             address  zipcode\r\n0  b1     Mark Levene        1987         29.5       108 Clement St, San Francisco  94107.0\r\n1  b2     Bill Bridge        1986         32.0      3131 Webster St, San Francisco      NaN\r\n2  b3   Mike Franklin        1988         27.5     1652 Stockton St, San Francisco  94122.0\r\n3  b4             NaN        1982         26.0       108 South Park, San Francisco      NaN\r\n4  b5   Alfons Kemper        1984         35.0  170 Post St, Apt 4,  San Francisco  94122.0\r\n5  b6  Michael Brodie        1987         32.5                                 NaN  94107.0\r\n\r\nkoalas_A:\r\n   ID            name  birth_year  hourly_wage                             address  zipcode\r\n0  b1     Mark Levene        1987         29.5       108 Clement St, San Francisco  94107.0\r\n1  b2     Bill Bridge        1986         32.0      3131 Webster St, San Francisco      NaN\r\n2  b3   Mike Franklin        1988         27.5     1652 Stockton St, San Francisco  94122.0\r\n3  b4            None        1982         26.0       108 South Park, San Francisco      NaN\r\n4  b5   Alfons Kemper        1984         35.0  170 Post St, Apt 4,  San Francisco  94122.0\r\n5  b6  Michael Brodie        1987         32.5                                None  94107.0\r\n\r\npandas_A_dropna:\r\n   ID            name  birth_year  hourly_wage                             address  zipcode\r\n0  b1     Mark Levene        1987         29.5       108 Clement St, San Francisco  94107.0\r\n2  b3   Mike Franklin        1988         27.5     1652 Stockton St, San Francisco  94122.0\r\n4  b5   Alfons Kemper        1984         35.0  170 Post St, Apt 4,  San Francisco  94122.0\r\n5  b6  Michael Brodie        1987         32.5                                 NaN  94107.0\r\n\r\nkoalas_A_dropna:\r\n   ID            name  birth_year  hourly_wage                             address  zipcode\r\n0  b1     Mark Levene        1987         29.5       108 Clement St, San Francisco    94107\r\n2  b3   Mike Franklin        1988         27.5     1652 Stockton St, San Francisco    94122\r\n4  b5   Alfons Kemper        1984         35.0  170 Post St, Apt 4,  San Francisco    94122\r\n5  b6  Michael Brodie        1987         32.5                                None    94107\r\n\r\npandas_A_dropna[zipcode].dtype = float64\r\nkoalas_A_dropna[zipcode].dtype = int32\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1255", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1255/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1255/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1255/events", "html_url": "https://github.com/databricks/koalas/issues/1255", "id": 558893245, "node_id": "MDU6SXNzdWU1NTg4OTMyNDU=", "number": 1255, "title": "ValueError when creating koalas Dataframe with duplicate column name", "user": {"login": "beobest2", "id": 7010554, "node_id": "MDQ6VXNlcjcwMTA1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7010554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beobest2", "html_url": "https://github.com/beobest2", "followers_url": "https://api.github.com/users/beobest2/followers", "following_url": "https://api.github.com/users/beobest2/following{/other_user}", "gists_url": "https://api.github.com/users/beobest2/gists{/gist_id}", "starred_url": "https://api.github.com/users/beobest2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beobest2/subscriptions", "organizations_url": "https://api.github.com/users/beobest2/orgs", "repos_url": "https://api.github.com/users/beobest2/repos", "events_url": "https://api.github.com/users/beobest2/events{/privacy}", "received_events_url": "https://api.github.com/users/beobest2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-03T07:26:03Z", "updated_at": "2020-02-03T07:55:29Z", "closed_at": "2020-02-03T07:55:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "There is ValueError when creating koalas Dataframe with duplicate column name.\r\n```python\r\n>>> pdf = pd.DataFrame(np.random.randn(5, 5),\r\n...                   columns=['a', 'c', 'b', 'b', 'a'],\r\n...                   index=['Joe', 'Joe', 'Travis', 'Travis', 'Travis'])\r\n>>> pdf\r\n               a         c         b         b         a\r\nJoe    -0.865145 -0.006487 -1.410585  0.996534  0.369394\r\nJoe    -1.017601  0.808152 -0.412281 -0.004824 -0.419562\r\nTravis -0.405356  0.540787 -0.863082  0.891709  1.839520\r\nTravis -1.415441 -1.337224 -0.785092 -1.018160  1.336162\r\nTravis -2.050251  1.389728  0.036075 -1.583323 -3.696555\r\n>>> kdf = ks.from_pandas(pdf)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/hw/Desktop/dev/git_koalas/koalas/databricks/koalas/namespace.py\", line 70, in from_pandas\r\n    return DataFrame(pobj)\r\n  File \"/Users/hw/Desktop/dev/git_koalas/koalas/databricks/koalas/frame.py\", line 380, in __init__\r\n    super(DataFrame, self).__init__(_InternalFrame.from_pandas(pdf))\r\n  File \"/Users/hw/Desktop/dev/git_koalas/koalas/databricks/koalas/internal.py\", line 921, in from_pandas\r\n    sdf = default_session().createDataFrame(reset_index, schema=schema)\r\n  File \"/usr/local/lib/python3.7/site-packages/pyspark/sql/session.py\", line 724, in createDataFrame\r\n    data = self._convert_from_pandas(data, schema, timezone)\r\n  File \"/usr/local/lib/python3.7/site-packages/pyspark/sql/session.py\", line 487, in _convert_from_pandas\r\n    np_records = pdf.to_records(index=False)\r\n  File \"/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\", line 1824, in to_records\r\n    return np.rec.fromarrays(arrays, dtype={\"names\": names, \"formats\": formats})\r\n  File \"/usr/local/lib/python3.7/site-packages/numpy/core/records.py\", line 618, in fromarrays\r\n    descr = sb.dtype(dtype)\r\nValueError: name already used as a name or title\r\n```\r\n\r\nIt works fine without duplicate column name.\r\n```python\r\n>>> pdf = pd.DataFrame(np.random.randn(5, 5),\r\n...                   columns=['a', 'c', 'b', 'b1', 'a1'],\r\n...                   index=['Joe', 'Joe', 'Travis', 'Travis', 'Travis'])\r\n>>> pdf\r\n               a         c         b        b1        a1\r\nJoe     0.753069 -1.014067  0.154648 -0.731109 -0.302279\r\nJoe    -1.678150 -0.252196  0.939639 -0.907443  0.440821\r\nTravis  0.432489 -0.962521  0.486859 -1.701143 -0.017991\r\nTravis  0.650316 -1.125226 -0.403401 -0.349617  0.983986\r\nTravis  0.637044 -0.412164  1.424705 -0.459249  3.323804\r\n>>> kdf = ks.from_pandas(pdf)\r\n>>> kdf\r\n               a         c         b        b1        a1\r\nJoe     0.753069 -1.014067  0.154648 -0.731109 -0.302279\r\nJoe    -1.678150 -0.252196  0.939639 -0.907443  0.440821\r\nTravis  0.432489 -0.962521  0.486859 -1.701143 -0.017991\r\nTravis  0.650316 -1.125226 -0.403401 -0.349617  0.983986\r\nTravis  0.637044 -0.412164  1.424705 -0.459249  3.323804\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1252", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1252/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1252/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1252/events", "html_url": "https://github.com/databricks/koalas/issues/1252", "id": 558493194, "node_id": "MDU6SXNzdWU1NTg0OTMxOTQ=", "number": 1252, "title": " Check parameters when grouping in multiple columns", "user": {"login": "beobest2", "id": 7010554, "node_id": "MDQ6VXNlcjcwMTA1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7010554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beobest2", "html_url": "https://github.com/beobest2", "followers_url": "https://api.github.com/users/beobest2/followers", "following_url": "https://api.github.com/users/beobest2/following{/other_user}", "gists_url": "https://api.github.com/users/beobest2/gists{/gist_id}", "starred_url": "https://api.github.com/users/beobest2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beobest2/subscriptions", "organizations_url": "https://api.github.com/users/beobest2/orgs", "repos_url": "https://api.github.com/users/beobest2/repos", "events_url": "https://api.github.com/users/beobest2/events{/privacy}", "received_events_url": "https://api.github.com/users/beobest2/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-02-01T06:12:56Z", "updated_at": "2020-02-04T03:34:00Z", "closed_at": "2020-02-04T03:34:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```python\r\n>>> pdf = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\r\n...                     'foo', 'bar', 'foo', 'foo'],\r\n...                     'B': ['one', 'one', 'two', 'three',\r\n...                     'two', 'two', 'one', 'three'],\r\n...                     'C': np.random.randn(8),\r\n...                     'D': np.random.randn(8)})\r\n>>> kdf = ks.from_pandas(pdf)\r\n```\r\n\r\nChecking grouping by multiple columns forms a hierarchical index. I works fine.\r\n```python\r\n>>> kdf.groupby(['B', 'A']).sum().sort_index()\r\n                  C         D\r\nB     A\r\none   bar -0.193929  0.193662\r\n      foo  1.134526 -0.062965\r\nthree bar  1.075517 -0.621216\r\n      foo  1.297346 -0.462547\r\ntwo   bar  0.509226  1.057209\r\n      foo  0.911734  0.284388\r\n>>> pdf.groupby(['B', 'A']).sum()\r\n                  C         D\r\nB     A\r\none   bar -0.193929  0.193662\r\n      foo  1.134526 -0.062965\r\nthree bar  1.075517 -0.621216\r\n      foo  1.297346 -0.462547\r\ntwo   bar  0.509226  1.057209\r\n      foo  0.911734  0.284388\r\n```\r\n\r\n\r\nI missed square brackets around the parameter to __groupby__.\r\nkoalas couldn't catch a mistake and did __groupby__ job with the first parameter ('B' column).\r\n\r\n```python\r\n>>> kdf.groupby('B', 'A').sum().sort_index()\r\n              C         D\r\nB\r\none    0.940597  0.130697\r\nthree  2.372863 -1.083763\r\ntwo    1.420961  1.341598\r\n>>> pdf.groupby('B', 'A').sum()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\", line 7883, in groupby\r\n    axis = self._get_axis_number(axis)\r\n  File \"/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\", line 411, in _get_axis_number\r\n    raise ValueError(\"No axis named {0} for object type {1}\".format(axis, cls))\r\nValueError: No axis named A for object type <class 'pandas.core.frame.DataFrame'>\r\n>>>\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1248", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1248/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1248/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1248/events", "html_url": "https://github.com/databricks/koalas/issues/1248", "id": 558286494, "node_id": "MDU6SXNzdWU1NTgyODY0OTQ=", "number": 1248, "title": "Output of print ks.Series", "user": {"login": "beobest2", "id": 7010554, "node_id": "MDQ6VXNlcjcwMTA1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7010554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beobest2", "html_url": "https://github.com/beobest2", "followers_url": "https://api.github.com/users/beobest2/followers", "following_url": "https://api.github.com/users/beobest2/following{/other_user}", "gists_url": "https://api.github.com/users/beobest2/gists{/gist_id}", "starred_url": "https://api.github.com/users/beobest2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beobest2/subscriptions", "organizations_url": "https://api.github.com/users/beobest2/orgs", "repos_url": "https://api.github.com/users/beobest2/repos", "events_url": "https://api.github.com/users/beobest2/events{/privacy}", "received_events_url": "https://api.github.com/users/beobest2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-31T18:12:07Z", "updated_at": "2020-02-02T00:55:17Z", "closed_at": "2020-02-02T00:55:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I tried to check the __print()__ function output of koalas Series\r\nI noticed that there is a difference in output from pandas Series\r\n\r\n```python\r\n>>> ks = ks.Series([1, 3, 5, np.nan, 6, 8])\r\n>>> print(ks)\r\nColumn<b'0'>\r\n>>> print(str(ks))\r\nColumn<b'0'>\r\n>>> print(repr(ks))\r\n0    1.0\r\n1    3.0\r\n2    5.0\r\n3    NaN\r\n4    6.0\r\n5    8.0\r\nName: 0, dtype: float64\r\n>>>\r\n>>> ps = pd.Series([1, 3, 5, np.nan, 6, 8])\r\n>>> print(ps)\r\n0    1.0\r\n1    3.0\r\n2    5.0\r\n3    NaN\r\n4    6.0\r\n5    8.0\r\ndtype: float64\r\n>>> print(str(ps))\r\n0    1.0\r\n1    3.0\r\n2    5.0\r\n3    NaN\r\n4    6.0\r\n5    8.0\r\ndtype: float64\r\n>>> print(repr(ps))\r\n0    1.0\r\n1    3.0\r\n2    5.0\r\n3    NaN\r\n4    6.0\r\n5    8.0\r\ndtype: float64\r\n```\r\nOutput of print(ks) is  \"Column<b'0'>\" \r\nThis does not represent data inside the Series.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1240", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1240/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1240/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1240/events", "html_url": "https://github.com/databricks/koalas/issues/1240", "id": 556994515, "node_id": "MDU6SXNzdWU1NTY5OTQ1MTU=", "number": 1240, "title": "[Support pandas 1.0.0] list-up Koalas error with existing tests", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-29T17:01:19Z", "updated_at": "2020-03-01T03:55:19Z", "closed_at": "2020-03-01T03:54:39Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Related with #1194 \r\n\r\nThere are several errors detected in `Koalas` with `pandas 1.0.0` with the existing tests.\r\n\r\n(NOTE: Those errors are not raised in `Koalas` with `pandas < 1.0.0`)\r\n\r\n(Below list would continuously be updated. if solved, let's check the box)\r\n\r\n## DataFrame\r\n\r\n- [x] `DataFrame.max` with `axis=1`\r\n```python\r\n>>> kdf\r\n     A    B     C      D\r\n0    1  1.0  -6.0   True\r\n...\r\n999  5  5.0  10.0  False\r\n\r\n[Showing only the first 1000 rows x 4 columns]\r\n\r\n>>> kdf.max(axis=1)\r\nTraceback (most recent call last):\r\n...\r\nValueError: Invalid returnType: returnType can not be None\r\n```\r\n\r\n- [x] `DataFrame.info`\r\n```python\r\n>>> kdf\r\n   int_col text_col  float_col\r\n0        1    alpha       0.00\r\n1        2     beta       0.25\r\n2        3    gamma       0.50\r\n3        4    delta       0.75\r\n4        5  epsilon       1.00\r\n\r\n>>> kdf.info()\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 'get_dtype_counts'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n...\r\nAttributeError: 'DataFrame' object has no attribute 'get_dtype_counts'\r\n```\r\nI think this because of the below https://github.com/databricks/koalas/blob/894f8132d75b64c42ed93e9dbfd7b0918f668eba/databricks/koalas/frame.py#L8135-L8138\r\n\r\n## MultiIndex\r\n\r\n- [x] `MultiIndex.monotonic_increasing` with `None` values (seems like pandas' behavior has changed)\r\n```python\r\n>>> kmidx = ks.MultiIndex.from_tuples([(None, 100), (2, 200), (3, 300), (4, 400), (5, 500)])\r\n>>> pmidx = kmidx.to_pandas()\r\n>>> kmidx.is_monotonic_increasing\r\nFalse\r\n>>> pmidx.is_monotonic_increasing\r\nTrue\r\n```\r\n- the error data list in existing test\r\n```python\r\ndata = []\r\ndatas.append([(None, 100), (2, 200), (3, 300), (4, 400), (5, 500)])\r\ndatas.append([(1, 100), (2, 200), (3, 300), (4, 400), (None, 500)])\r\ndatas.append([(None, None), (2, 200), (3, 300), (4, 400), (5, 500)])\r\ndatas.append([(1, 100), (2, 200), (3, 300), (4, 400), (None, None)])\r\ndatas.append([('x', 'd'), ('y', None), ('y', 'c'), ('z', 'a')])\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1239", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1239/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1239/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1239/events", "html_url": "https://github.com/databricks/koalas/issues/1239", "id": 556994460, "node_id": "MDU6SXNzdWU1NTY5OTQ0NjA=", "number": 1239, "title": "[Support pandas 1.0.0] List of changes to move to pandas 1.0.0", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-29T17:01:14Z", "updated_at": "2020-03-01T03:55:31Z", "closed_at": "2020-03-01T03:54:54Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Related with #1194 \r\n\r\nThere are several errors detected in `pandas 1.0.0` with the existing tests.\r\n\r\n(Below list would continuously be updated. if solved, let's check the box)\r\n\r\n## DataFrameGroupBy\r\n\r\n- [x] `DataFrameGroupBy.idxmin` doesn't work anymore with non-monotonically increasing/decreasing index \r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/4d68cf667f74373477e5ad2b51e8bd68be497b66)**\r\n```python\r\n>>> pdf\r\n          a  b   c\r\n0.823938  1  1   1\r\n0.375564  1  1   4\r\n0.780599  2  2   9\r\n0.719376  2  2  16\r\n0.561594  3  3  25\r\n0.700282  3  4  36\r\n0.191650  1  1   1\r\n0.776248  1  1   4\r\n0.646595  2  2   9\r\n0.151985  2  2  16\r\n0.043341  3  3  25\r\n0.860290  3  4  36\r\n0.768670  1  1   1\r\n0.248475  1  1   4\r\n0.984787  2  2   9\r\n0.400554  2  2  16\r\n0.298807  3  3  25\r\n0.245567  3  4  36\r\n\r\n>>> pdf.groupby('a').idxmin()\r\nTraceback (most recent call last):\r\n...\r\nValueError: index must be monotonic increasing or decreasing\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 0.0\r\n```\r\n\r\n- [x] `DataFrameGroupBy.idxmax` doesn't work anymore with non-monotonically increasing/decreasing index \r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/4d68cf667f74373477e5ad2b51e8bd68be497b66)**\r\n```python\r\n>>> pdf.groupby('a').idxmax()\r\nTraceback (most recent call last):\r\n...\r\nValueError: index must be monotonic increasing or decreasing\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 0.0\r\n```\r\n\r\n- [x] `DataFrameGroupBy.fillna` doesn't work anymore with non-monotonically increasing/decreasing index \r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/4d68cf667f74373477e5ad2b51e8bd68be497b66)**\r\n```python\r\n>>> pdf.groupby('a').fillna(0)\r\nTraceback (most recent call last):\r\n...\r\nValueError: index must be monotonic increasing or decreasing\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 0.0\r\n```\r\n\r\n- [x] `DataFrameGroupBy.diff` doesn't work anymore with non-monotonically increasing/decreasing index \r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/4d68cf667f74373477e5ad2b51e8bd68be497b66)**\r\n```python\r\n>>> pdf.groupby('a').diff()\r\nTraceback (most recent call last):\r\n...\r\nValueError: index must be monotonic increasing or decreasing\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 0.0\r\n```\r\n\r\n- [x] parameter `fill_value` for `DataFrameGroupBy.shift`\r\n**(Commented for passing tests at https://github.com/databricks/koalas/blob/4d68cf667f74373477e5ad2b51e8bd68be497b66/databricks/koalas/tests/test_groupby.py#L722-L755**\r\n\r\n```python\r\n>>> pdf.groupby('a').shift(fill_value=0)\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 0.0\r\n```\r\n\r\n- [x] some cases for groupby with MultiIndex raise `ValueError`\r\n```python\r\n>>> pdf\r\n   x       y  z\r\n   a  b    c  d\r\n0  1  4  4.0  a\r\n1  2  2  2.0  b\r\n3  6  7  7.0  c\r\n5  4  3  3.0  d\r\n6  4  3  NaN  e\r\n8  6  1  1.0  f\r\n9  4  1  1.0  g\r\n9  3  1  1.0  h\r\n9  7  2  2.0  t\r\n\r\n>>> pdf['x', 'a'].groupby(pdf[('x', 'b')])\r\nTraceback (most recent call last):\r\n...\r\nValueError: Can only tuple-index with a MultiIndex\r\n```\r\n\r\n## Series\r\n\r\n- [x] parameter `encoding` for `Series.to_latex`\r\n**(Commented for passing tests at https://github.com/databricks/koalas/pull/1197/commits/1bc76b39a309d0a51aabf9def38cd9d5f7d50f03)**\r\n```python\r\n>>> pser\r\n0    1\r\n1    2\r\n2    3\r\n3    4\r\n4    5\r\n5    6\r\n6    7\r\nName: x, dtype: int64\r\n\r\n>>> pser.to_latex(encoding='ascii')\r\nTraceback (most recent call last):\r\n...\r\nValueError: buf is not a file name and encoding is specified.\r\n```\r\n\r\n## Expanding\r\n\r\n- [x] behavior of `Expanding.count` has changed\r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/079721d594d63c7ce92433c88b780f3f02a6959e)**\r\n\r\n**FROM**\r\n\r\n```python\r\n>>> pdf\r\n          a    b\r\n0.761107  1  4.0\r\n0.846601  2  2.0\r\n0.072477  3  3.0\r\n0.793417  2  1.0\r\n\r\n>>> pser.expanding(2).count()\r\na  x    1.0\r\n   y    2.0\r\nb  z    3.0\r\nName: 0, dtype: float64\r\n```\r\n\r\n**TO**\r\n\r\n```python\r\n>>> pdf\r\n          a    b\r\n0.761107  1  4.0\r\n0.846601  2  2.0\r\n0.072477  3  3.0\r\n0.793417  2  1.0\r\n\r\n>>> pser.expanding(2).count()\r\na  x    NaN\r\n   y    2.0\r\nb  z    3.0\r\nName: 0, dtype: float64\r\n```\r\n\r\n## RollingGroupBy\r\n\r\n- [x] functions for `RollingGroupBy` for `DataFrameGroupBy` don't work anymore with non-monotonically increasing/decreasing index \r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/6be375b8c114402b3486189b0fa7f49898b39c26)**\r\n```python\r\n>>> pdf\r\n          a    b\r\n0.006197  1  4.0\r\n0.331083  2  2.0\r\n0.886719  3  3.0\r\n0.040370  2  1.0\r\n\r\n>>> pdf.groupby(pdf.a).rolling(2).std()\r\nTraceback (most recent call last):\r\n...\r\nValueError: index must be monotonic increasing or decreasing\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 0.0\r\n```\r\n\r\n## ExpandingGroupby\r\n\r\n- [x] functions for `ExpandingGroupby` for `DataFrameGroupBy` don't work anymore with non-monotonically increasing/decreasing index \r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/71b78d7600f4eae318941d2a70d9729e30cee050)**\r\n```python\r\n>>> pdf\r\n          a    b\r\n0.006197  1  4.0\r\n0.331083  2  2.0\r\n0.886719  3  3.0\r\n0.040370  2  1.0\r\n\r\n>>> pdf.groupby(pdf.a).expanding(2).std()\r\nTraceback (most recent call last):\r\n...\r\nValueError: index must be monotonic increasing or decreasing\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n...\r\nKeyError: 0.0\r\n```\r\n\r\n- [x] behavior of `ExpandingGroupby.count` has changed\r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/957a4f94bf398dabc10f939ee79a81444c29e086)**\r\n\r\n**FROM**\r\n\r\n```python\r\n>>> pdf\r\n   a    b\r\n0  1  4.0\r\n1  2  2.0\r\n2  3  3.0\r\n3  2  1.0\r\n>>> pdf.groupby('a').expanding(2).count()\r\n       a    b\r\na\r\n1 0  1.0  1.0\r\n3 2  1.0  1.0\r\n2 1  1.0  1.0\r\n  3  2.0  2.0\r\n```\r\n\r\n**TO**\r\n\r\n```python\r\n>>> pdf\r\n   a    b\r\n0  1  4.0\r\n1  2  2.0\r\n2  3  3.0\r\n3  2  1.0\r\n>>> pdf.groupby('a').expanding(2).count()\r\n       a    b\r\na\r\n1 0  NaN  NaN\r\n2 1  NaN  NaN\r\n  3  2.0  2.0\r\n3 2  NaN  NaN\r\n```\r\n\r\n## Common\r\n\r\n- [x] `pd.get_dummies` no more allows non-list-like for `columns` parameter \r\n**(Fixed for passing tests at https://github.com/databricks/koalas/pull/1197/commits/75351c59fffbd6211acaa9632824d1a0f897d84e)**\r\n```python\r\n>>> pdf\r\n   x     y\r\n   a  b  c\r\n   1  2  3\r\n0  1  a  a\r\n1  2  b  b\r\n2  3  c  c\r\n3  4  d  d\r\n4  4  a  a\r\n5  3  b  b\r\n6  2  c  c\r\n7  1  d  d\r\n\r\n>>> pd.get_dummies(pdf, columns='x')\r\nTraceback (most recent call last):\r\n...\r\nTypeError: Input must be a list-like for parameter `columns`\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1230", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1230/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1230/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1230/events", "html_url": "https://github.com/databricks/koalas/issues/1230", "id": 555860875, "node_id": "MDU6SXNzdWU1NTU4NjA4NzU=", "number": 1230, "title": "Support slice as `rows_sel` for `iloc` indexer.", "user": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-01-27T22:03:24Z", "updated_at": "2020-03-12T01:27:28Z", "closed_at": "2020-03-12T01:27:28Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Now that we have an infrastructure to support slice as `rows_sel` for `iloc` indexer.\r\nE.g.,\r\n\r\n```py\r\n>>> pdf = pd.DataFrame({'a':list('abcdefghij')})\r\n>>> pdf\r\n   a\r\n0  a\r\n1  b\r\n2  c\r\n3  d\r\n4  e\r\n5  f\r\n6  g\r\n7  h\r\n8  i\r\n9  j\r\n>>> pdf.iloc[2:5]\r\n   a\r\n2  c\r\n3  d\r\n4  e\r\n>>> pdf.iloc[2:-3]\r\n   a\r\n2  c\r\n3  d\r\n4  e\r\n5  f\r\n6  g\r\n>>> pdf.iloc[-8:-3]\r\n   a\r\n2  c\r\n3  d\r\n4  e\r\n5  f\r\n6  g\r\n>>> pdf.iloc[2:-3:2]\r\n   a\r\n2  c\r\n4  e\r\n6  g\r\n>>> pdf.iloc[5:]\r\n   a\r\n5  f\r\n6  g\r\n7  h\r\n8  i\r\n9  j\r\n>>> pdf.iloc[5:2]\r\nEmpty DataFrame\r\nColumns: [a]\r\nIndex: []\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1228", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1228/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1228/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1228/events", "html_url": "https://github.com/databricks/koalas/issues/1228", "id": 555817912, "node_id": "MDU6SXNzdWU1NTU4MTc5MTI=", "number": 1228, "title": "DataFrame.apply(func, axis=1)", "user": {"login": "patryk-oleniuk", "id": 6854491, "node_id": "MDQ6VXNlcjY4NTQ0OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6854491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patryk-oleniuk", "html_url": "https://github.com/patryk-oleniuk", "followers_url": "https://api.github.com/users/patryk-oleniuk/followers", "following_url": "https://api.github.com/users/patryk-oleniuk/following{/other_user}", "gists_url": "https://api.github.com/users/patryk-oleniuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/patryk-oleniuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patryk-oleniuk/subscriptions", "organizations_url": "https://api.github.com/users/patryk-oleniuk/orgs", "repos_url": "https://api.github.com/users/patryk-oleniuk/repos", "events_url": "https://api.github.com/users/patryk-oleniuk/events{/privacy}", "received_events_url": "https://api.github.com/users/patryk-oleniuk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-27T20:36:15Z", "updated_at": "2020-02-11T00:27:11Z", "closed_at": "2020-02-11T00:27:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm missing the access of the whole row in the DataFrame.apply function, like\r\n\r\n```python\r\ndf = pd.DataFrame({\"date\":pd.to_datetime(['2012-01-01','2012-01-02']),\r\n              \"hour\": [5, 5], \r\n              })\r\n\r\nprint(df)\r\n\r\ndf['datetime'] = df.apply(lambda x: x['date'].replace(hour=x['hour']), axis=1)\r\n\r\nprint(df)\r\n```\r\n\r\nPrints out:\r\n```bash\r\n        date  hour\r\n0 2012-01-01     5\r\n1 2012-01-02     5\r\n\r\n        date  hour            datetime\r\n0 2012-01-01     5 2012-01-01 05:00:00\r\n1 2012-01-02     5 2012-01-02 05:00:00\r\n```\r\n\r\nWe should implement `DataFrame.apply(row_func, axis=1)` using the UDFs.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1226", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1226/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1226/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1226/events", "html_url": "https://github.com/databricks/koalas/issues/1226", "id": 554998127, "node_id": "MDU6SXNzdWU1NTQ5OTgxMjc=", "number": 1226, "title": "ks.read_csv() - compression parameter ( *.csv.gz )", "user": {"login": "patryk-oleniuk", "id": 6854491, "node_id": "MDQ6VXNlcjY4NTQ0OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/6854491?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patryk-oleniuk", "html_url": "https://github.com/patryk-oleniuk", "followers_url": "https://api.github.com/users/patryk-oleniuk/followers", "following_url": "https://api.github.com/users/patryk-oleniuk/following{/other_user}", "gists_url": "https://api.github.com/users/patryk-oleniuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/patryk-oleniuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patryk-oleniuk/subscriptions", "organizations_url": "https://api.github.com/users/patryk-oleniuk/orgs", "repos_url": "https://api.github.com/users/patryk-oleniuk/repos", "events_url": "https://api.github.com/users/patryk-oleniuk/events{/privacy}", "received_events_url": "https://api.github.com/users/patryk-oleniuk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-24T23:14:41Z", "updated_at": "2020-01-28T02:30:26Z", "closed_at": "2020-01-28T02:30:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I'm missing the CSV compression behavior from pandas:\r\n```python\r\npd.read_csv('dataset.csv.gz', compression='gzip')\r\n# is same as \r\n# (pandas is recognizing the compression automatically by the file extension)\r\npd.read_csv('dataset.csv.gz')\r\n```\r\nWe can use one of the Spark `codec` implementations explained here:\r\nhttps://stackoverflow.com/questions/40163996/how-to-save-a-dataframe-as-compressed-gzipped-csv ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1213", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1213/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1213/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1213/events", "html_url": "https://github.com/databricks/koalas/issues/1213", "id": 553643997, "node_id": "MDU6SXNzdWU1NTM2NDM5OTc=", "number": 1213, "title": "Faster calculation of moving averages in Koalas.  ", "user": {"login": "sushmit86", "id": 5600498, "node_id": "MDQ6VXNlcjU2MDA0OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5600498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sushmit86", "html_url": "https://github.com/sushmit86", "followers_url": "https://api.github.com/users/sushmit86/followers", "following_url": "https://api.github.com/users/sushmit86/following{/other_user}", "gists_url": "https://api.github.com/users/sushmit86/gists{/gist_id}", "starred_url": "https://api.github.com/users/sushmit86/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sushmit86/subscriptions", "organizations_url": "https://api.github.com/users/sushmit86/orgs", "repos_url": "https://api.github.com/users/sushmit86/repos", "events_url": "https://api.github.com/users/sushmit86/events{/privacy}", "received_events_url": "https://api.github.com/users/sushmit86/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-01-22T16:12:52Z", "updated_at": "2020-03-12T09:42:14Z", "closed_at": "2020-03-12T09:42:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to calculate the exponential moving average to a koalas dataframe. I am able to achieve this as below\r\n\r\n```\r\nimport pandas as pd\r\nimport databricks.koalas as ks\r\nfrom databricks.koalas import pandas_wraps\r\n\r\ndf = ks.DataFrame({'cust_id':['a', 'a', 'a', 'b', 'b'],\r\n                   'sales': [100, 200, 300, 400, 500]})\r\ndef fun(col1) -> ks.Series[np.float64]:\r\n    return col1.apply(lambda x: x.ewm(alpha=0.5, adjust=False).mean())  # Arbitrary pandas code.\r\ndf['moving_average'] = fun(df.groupby('cust_id').sales)\r\ndf.head()\r\n```\r\n\r\nHowever, when I try to implement the above in a dataset that has 30M records it takes 4 hrs to complete. Is there any way to speed this up", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1210", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1210/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1210/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1210/events", "html_url": "https://github.com/databricks/koalas/issues/1210", "id": 553284285, "node_id": "MDU6SXNzdWU1NTMyODQyODU=", "number": 1210, "title": "Update index_names of _InternalFrame when Index name changed", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-22T03:22:42Z", "updated_at": "2020-01-24T07:09:02Z", "closed_at": "2020-01-24T07:09:02Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When doing something with `_InternalFrame`,\r\n\r\nsometimes i feel a little confusing when such like below case.\r\n\r\n```python\r\n>>> idx = ks.Index([1, 2, 3])\r\n>>> idx\r\nInt64Index([1, 2, 3], dtype='int64')\r\n>>> idx.name = 'koalas'\r\n>>> idx\r\nInt64Index([1, 2, 3], dtype='int64', name='koalas')\r\n>>> idx.name\r\n'koalas'\r\n```\r\nit is okay when i just use `idx` itself with new name 'koalas' like above,\r\n\r\nbut the problem is occurred when i want to work with `_InternalFrame`,\r\n\r\nsince the internal `index_names` has not changed like the below.\r\n(to be honest, the `index_map` and other internal factors also have same behavior, but let's try to talk about `index_names` for now)\r\n\r\n```python\r\n>>> idx\r\nInt64Index([1, 2, 3], dtype='int64', name='koalas')\r\n>>> idx._internal.index_names\r\n[None]  # i expect this should be [('koalas',)]\r\n```\r\n\r\nand it's also the reason for several bugs like #1206 ,\r\n\r\nso i think it might be better we check about this behavior that we update internal information also (or keep the existing behavior) when related factors are changed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1206", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1206/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1206/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1206/events", "html_url": "https://github.com/databricks/koalas/issues/1206", "id": 552647299, "node_id": "MDU6SXNzdWU1NTI2NDcyOTk=", "number": 1206, "title": "Bug in Index.to_frame() when name is assigned after Index created", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-21T06:17:02Z", "updated_at": "2020-01-24T07:09:02Z", "closed_at": "2020-01-24T07:09:02Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```python\r\n>>> idx = ks.Index(['Ant', 'Bear', 'Cow'])\r\n>>> idx.name = 'animal'\r\n>>> idx.to_frame()  # 'animal' is not shown in created frame.\r\n         0\r\nAnt    Ant\r\nBear  Bear\r\nCow    Cow\r\n\r\n>>> idx.to_pandas().to_frame()  # we have to fix like this\r\n       animal\r\nanimal\r\nAnt       Ant\r\nBear     Bear\r\nCow       Cow\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1204", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1204/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1204/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1204/events", "html_url": "https://github.com/databricks/koalas/issues/1204", "id": 552601754, "node_id": "MDU6SXNzdWU1NTI2MDE3NTQ=", "number": 1204, "title": "CSV error when using quotation mark escape character(\"\") with commas", "user": {"login": "shchoichoi", "id": 31274680, "node_id": "MDQ6VXNlcjMxMjc0Njgw", "avatar_url": "https://avatars1.githubusercontent.com/u/31274680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shchoichoi", "html_url": "https://github.com/shchoichoi", "followers_url": "https://api.github.com/users/shchoichoi/followers", "following_url": "https://api.github.com/users/shchoichoi/following{/other_user}", "gists_url": "https://api.github.com/users/shchoichoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/shchoichoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shchoichoi/subscriptions", "organizations_url": "https://api.github.com/users/shchoichoi/orgs", "repos_url": "https://api.github.com/users/shchoichoi/repos", "events_url": "https://api.github.com/users/shchoichoi/events{/privacy}", "received_events_url": "https://api.github.com/users/shchoichoi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552132, "node_id": "MDU6TGFiZWwxMTc5NTUyMTMy", "url": "https://api.github.com/repos/databricks/koalas/labels/question", "name": "question", "color": "d876e3", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-21T03:19:59Z", "updated_at": "2020-01-22T20:59:04Z", "closed_at": "2020-01-22T20:59:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "When using the following raw csv text, pandas correctly parses the quotation mark correctly as string followed by a comma, while koalas fails to recognize the escape character (\"\") using the `df.read_csv` function\r\n```\r\nheader1,header2,header3\r\n85,\"some text with \"\"escape quotation\"\", \",Some text here\r\n86,\"comma with \"\",escape quotation\"\"\",Some text here\r\n```\r\n\r\nThe code: \r\n```python\r\nimport pandas as pd\r\nimport databricks.koalas as ks\r\npd.set_option('display.max_columns', None)\r\n\r\npdf = pd.read_csv(\"input.csv\")\r\nkdf = ks.read_csv(\"input.csv\")\r\n\r\nprint(\"pandas output:\")\r\nprint(pdf.head())\r\n\r\nprint(\"koalas output:\")\r\nprint(kdf.head())\r\n```\r\n\r\nThe output:\r\n```\r\npandas output:\r\n   header1                              header2         header3\r\n0       85  some text with \"escape quotation\",   Some text here\r\n1       86       comma with \",escape quotation\"  Some text here\r\nkoalas output:\r\n   header1                               header2              header3\r\n0       85  \"some text with \"\"escape quotation\"\"                    \"\r\n1       86                          comma with \"  escape quotation\"\"\"\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1195", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1195/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1195/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1195/events", "html_url": "https://github.com/databricks/koalas/issues/1195", "id": 550376498, "node_id": "MDU6SXNzdWU1NTAzNzY0OTg=", "number": 1195, "title": "Exponential moving average in Koalas", "user": {"login": "sushmit86", "id": 5600498, "node_id": "MDQ6VXNlcjU2MDA0OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5600498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sushmit86", "html_url": "https://github.com/sushmit86", "followers_url": "https://api.github.com/users/sushmit86/followers", "following_url": "https://api.github.com/users/sushmit86/following{/other_user}", "gists_url": "https://api.github.com/users/sushmit86/gists{/gist_id}", "starred_url": "https://api.github.com/users/sushmit86/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sushmit86/subscriptions", "organizations_url": "https://api.github.com/users/sushmit86/orgs", "repos_url": "https://api.github.com/users/sushmit86/repos", "events_url": "https://api.github.com/users/sushmit86/events{/privacy}", "received_events_url": "https://api.github.com/users/sushmit86/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-01-15T19:13:17Z", "updated_at": "2020-01-15T19:25:55Z", "closed_at": "2020-01-15T19:25:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to calculate the exponential moving average by groups\r\n\r\n```\r\n\r\nimport pandas as pd\r\nimport databricks.koalas as ks\r\nkdf = ks.DataFrame({\"cust_id\": ['A','A','A','B','B','B'],\r\n                    \"sales\":[1, 2, 3, 4, 5,6],\r\n              })\r\n\r\nfrom databricks.koalas import pandas_wraps\r\n@pandas_wraps\r\ndef fun(col1) -> ks.Series[np.float64]:\r\n    return col1.apply(lambda x: x.ewm(alpha=0.5, adjust=False))  # Arbitrary pandas code.\r\n\r\nkdf['test']= fun(kdf.groupby('cust_id').sales)\r\n```\r\n\r\nThis gives me an error \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n<command-495740549758385> in <module>\r\n----> 1 kdf['test']= fun(kdf.groupby('cust_id').sales)\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/typedef.py in wrapper(*args, **kwargs)\r\n    371                                  \" but found type {}\".format(sig_return))\r\n    372             spark_return_type = sig_return.tpe\r\n--> 373             return _make_fun(f, spark_return_type, *args, **kwargs)\r\n    374         return wrapper\r\n    375     if callable(function):\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/typedef.py in _make_fun(f, return_type, *args, **kwargs)\r\n    231         # No argument is related to spark\r\n    232         # The function is just called through without other considerations.\r\n--> 233         return f(*args, **kwargs)\r\n    234 \r\n    235     # We detected some columns. They need to be wrapped in a UDF to spark.\r\n\r\n<command-4386191212792342> in fun(col1)\r\n      5 @pandas_wraps\r\n      6 def fun(col1) -> ks.Series[np.float64]:\r\n----> 7     return col1.apply(lambda x: x.ewm(alpha=0.5, adjust=False))  # Arbitrary pandas code.\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/usage_logging/__init__.py in wrapper(*args, **kwargs)\r\n    138             start = time.perf_counter()\r\n    139             try:\r\n--> 140                 res = func(*args, **kwargs)\r\n    141                 logger.log_success(\r\n    142                     class_name, function_name, time.perf_counter() - start, signature)\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/groupby.py in apply(self, func)\r\n   1995 \r\n   1996     def apply(self, func):\r\n-> 1997         return _col(super(SeriesGroupBy, self).transform(func))\r\n   1998 \r\n   1999     apply.__doc__ = GroupBy.transform.__doc__\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/groupby.py in transform(self, func)\r\n   1644             pdf = self._kdf.head(limit + 1)._to_internal_pandas()\r\n   1645             pdf = pdf.groupby(input_groupnames).transform(func)\r\n-> 1646             kdf = DataFrame(pdf)\r\n   1647             return_schema = kdf._sdf.drop(*HIDDEN_COLUMNS).schema\r\n   1648             if len(pdf) <= limit:\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/usage_logging/__init__.py in wrapper(*args, **kwargs)\r\n    133         if hasattr(_local, 'logging') and _local.logging:\r\n    134             # no need to log since this should be internal call.\r\n--> 135             return func(*args, **kwargs)\r\n    136         _local.logging = True\r\n    137         try:\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/frame.py in __init__(self, data, index, columns, dtype, copy)\r\n    378             else:\r\n    379                 pdf = pd.DataFrame(data=data, index=index, columns=columns, dtype=dtype, copy=copy)\r\n--> 380             super(DataFrame, self).__init__(_InternalFrame.from_pandas(pdf))\r\n    381 \r\n    382     @property\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/internal.py in from_pandas(pdf)\r\n    755         schema = StructType([StructField(name_like_string(name), infer_pd_series_spark_type(col),\r\n    756                                          nullable=bool(col.isnull().any()))\r\n--> 757                              for name, col in reset_index.iteritems()])\r\n    758         for name, col in reset_index.iteritems():\r\n    759             dt = col.dtype\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/internal.py in <listcomp>(.0)\r\n    755         schema = StructType([StructField(name_like_string(name), infer_pd_series_spark_type(col),\r\n    756                                          nullable=bool(col.isnull().any()))\r\n--> 757                              for name, col in reset_index.iteritems()])\r\n    758         for name, col in reset_index.iteritems():\r\n    759             dt = col.dtype\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/databricks/koalas/typedef.py in infer_pd_series_spark_type(s)\r\n    168         if len(s) == 0 or s.isnull().all():\r\n    169             raise ValueError(\"can not infer schema from empty or null dataset\")\r\n--> 170         return types.from_arrow_type(pa.Array.from_pandas(s).type)\r\n    171     elif is_datetime64_dtype(dt) or is_datetime64tz_dtype(dt):\r\n    172         return types.TimestampType()\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.Array.from_pandas()\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()\r\n\r\n/local_disk0/pythonVirtualEnvDirs/virtualEnv-e3817b00-d29a-47b6-9c4f-e6a3544ceb72/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Could not convert EWM [com=1.0,min_periods=0,adjust=False,ignore_na=False,axis=0] with type EWM: did not recognize Python value type when inferring an Arrow data type\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1194", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1194/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1194/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1194/events", "html_url": "https://github.com/databricks/koalas/issues/1194", "id": 549928107, "node_id": "MDU6SXNzdWU1NDk5MjgxMDc=", "number": 1194, "title": "Support pandas>=1.0.0", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1355475110, "node_id": "MDU6TGFiZWwxMzU1NDc1MTEw", "url": "https://api.github.com/repos/databricks/koalas/labels/discussions", "name": "discussions", "color": "f29721", "default": false, "description": ""}, {"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2020-01-15T02:29:29Z", "updated_at": "2020-03-01T03:56:29Z", "closed_at": "2020-03-01T03:56:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "~~As pandas 1.0.0 will be released soon, we need prepare for it.~~\r\n\r\nAs pandas 1.0.0 has released, we need support it.\r\n\r\nso I have created some to-do list below.\r\n\r\n- [x] update the list of APIs that are newly added .\r\n- [x] update the list of APIs that are deprecated.\r\n- [x] update the list of APIs that are removed.\r\n- [x] update the name of APIs that are renamed.\r\n- [x] list up & update the implementation of existing Koalas APIs that have incompatible with pandas>=1.0.0.\r\n- [x] update `requirements-dev.txt` for pandas>=1.0.0\r\n- [x] update `.travis.yml` for pandas>=1.0.0\r\n- [x] update GitHub Actions for pandas>=1.0.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1190", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1190/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1190/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1190/events", "html_url": "https://github.com/databricks/koalas/issues/1190", "id": 548321238, "node_id": "MDU6SXNzdWU1NDgzMjEyMzg=", "number": 1190, "title": "Fix the index of `Index.to_series()`.", "user": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-01-10T22:27:37Z", "updated_at": "2020-03-12T00:35:59Z", "closed_at": "2020-03-12T00:35:59Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The index of `Index.to_series()` doesn't address the change of the index itself.\r\n\r\nE.g.,\r\n\r\n```py\r\n>>> pidx\r\nInt64Index([1, 2, 3], dtype='int64')\r\n>>> (pidx + 1).to_series()\r\n2    2\r\n3    3\r\n4    4\r\ndtype: int64\r\n```\r\n\r\nwhereas\r\n\r\n```py\r\n>>> kidx\r\nInt64Index([1, 2, 3], dtype='int64')\r\n>>> (kidx + 1).to_series()\r\n1    2\r\n2    3\r\n3    4\r\nName: 0, dtype: int64\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1178", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1178/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1178/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1178/events", "html_url": "https://github.com/databricks/koalas/issues/1178", "id": 547186874, "node_id": "MDU6SXNzdWU1NDcxODY4NzQ=", "number": 1178, "title": "DataFrame.transform operates per-element when transform function is type-annotated", "user": {"login": "chunyang", "id": 454684, "node_id": "MDQ6VXNlcjQ1NDY4NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/454684?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chunyang", "html_url": "https://github.com/chunyang", "followers_url": "https://api.github.com/users/chunyang/followers", "following_url": "https://api.github.com/users/chunyang/following{/other_user}", "gists_url": "https://api.github.com/users/chunyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/chunyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chunyang/subscriptions", "organizations_url": "https://api.github.com/users/chunyang/orgs", "repos_url": "https://api.github.com/users/chunyang/repos", "events_url": "https://api.github.com/users/chunyang/events{/privacy}", "received_events_url": "https://api.github.com/users/chunyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-01-09T00:56:46Z", "updated_at": "2020-01-09T22:44:51Z", "closed_at": "2020-01-09T02:10:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "[`DataFrame.transform`](https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.DataFrame.transform.html#databricks.koalas.DataFrame.transform) is behaving differently than its pandas equivalent when its return type is annotated. Consider the example below.\r\n\r\n```python\r\n>>> import databricks.koalas as ks\r\n>>> import numpy as np\r\n>>> import pandas as pd\r\n>>>\r\n>>> pdf = pd.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5]], columns=(\"a\", \"b\", \"c\", \"d\"))\r\n>>> kdf = ks.DataFrame([[1, 2, 3, 4], [2, 3, 4, 5]], columns=(\"a\", \"b\", \"c\", \"d\"))\r\n>>>\r\n>>> def normalize(v):\r\n...     return v / sum(v)\r\n...\r\n>>> pdf.transform(normalize)\r\n          a    b         c         d\r\n0  0.333333  0.4  0.428571  0.444444\r\n1  0.666667  0.6  0.571429  0.555556\r\n>>> kdf.transform(normalize)\r\n          a    b         c         d                                            \r\n0  0.333333  0.4  0.428571  0.444444\r\n1  0.666667  0.6  0.571429  0.555556\r\n>>>\r\n>>> def typed_normalize(v) -> ks.Series[np.float64]:\r\n...     return v / sum(v)\r\n...\r\n>>> pdf.transform(typed_normalize)\r\n          a    b         c         d\r\n0  0.333333  0.4  0.428571  0.444444\r\n1  0.666667  0.6  0.571429  0.555556\r\n>>> kdf.transform(typed_normalize)\r\n     a    b    c    d                                                           \r\n0  1.0  1.0  1.0  1.0\r\n1  1.0  1.0  1.0  1.0\r\n```\r\n\r\nKoalas version: 0.24.0\r\nNumpy version: 1.18.0\r\nPandas version: 0.25.3", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1176", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1176/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1176/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1176/events", "html_url": "https://github.com/databricks/koalas/issues/1176", "id": 547105210, "node_id": "MDU6SXNzdWU1NDcxMDUyMTA=", "number": 1176, "title": "Support slice as `col_sel` for `loc` indexer.", "user": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}, {"id": 1179552129, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI5", "url": "https://api.github.com/repos/databricks/koalas/labels/help%20wanted", "name": "help wanted", "color": "008672", "default": true, "description": "Extra attention is needed"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-08T21:14:17Z", "updated_at": "2020-03-18T01:32:37Z", "closed_at": "2020-03-18T01:32:37Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "We should support slice as `col_sel` for `loc` indexer.\r\n\r\n```py\r\n>>> pdf\r\n   x     y     z\r\n   a  b  c  d  e\r\n0  1  2  3  4  5\r\n>>> pdf.loc[:, 'y':]\r\n   y     z\r\n   c  d  e\r\n0  3  4  5\r\n>>> pdf.loc[:, :'y']\r\n   x     y\r\n   a  b  c  d\r\n0  1  2  3  4\r\n>>> pdf.loc[:, ('x','b'):]\r\n   x  y     z\r\n   b  c  d  e\r\n0  2  3  4  5\r\n>>> pdf.loc[:, :('y','c')]\r\n   x     y\r\n   a  b  c\r\n0  1  2  3\r\n>>> pdf.loc[:, ('x','b'):('y','c')]\r\n   x  y\r\n   b  c\r\n0  2  3\r\n>>> pdf.loc[:, 'x':('y','c')]\r\n   x     y\r\n   a  b  c\r\n0  1  2  3\r\n>>> pdf.loc[:, ('x','b'):'y']\r\n   x  y\r\n   b  c  d\r\n0  2  3  4\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1175", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1175/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1175/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1175/events", "html_url": "https://github.com/databricks/koalas/issues/1175", "id": 547103504, "node_id": "MDU6SXNzdWU1NDcxMDM1MDQ=", "number": 1175, "title": "Support MultiIndex for `loc` indexer with slice as `row_sel`.", "user": {"login": "ueshin", "id": 506656, "node_id": "MDQ6VXNlcjUwNjY1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/506656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ueshin", "html_url": "https://github.com/ueshin", "followers_url": "https://api.github.com/users/ueshin/followers", "following_url": "https://api.github.com/users/ueshin/following{/other_user}", "gists_url": "https://api.github.com/users/ueshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/ueshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ueshin/subscriptions", "organizations_url": "https://api.github.com/users/ueshin/orgs", "repos_url": "https://api.github.com/users/ueshin/repos", "events_url": "https://api.github.com/users/ueshin/events{/privacy}", "received_events_url": "https://api.github.com/users/ueshin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}, {"id": 1179552129, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI5", "url": "https://api.github.com/repos/databricks/koalas/labels/help%20wanted", "name": "help wanted", "color": "008672", "default": true, "description": "Extra attention is needed"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-01-08T21:10:29Z", "updated_at": "2020-03-17T04:40:15Z", "closed_at": "2020-03-17T04:40:15Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "We should support MultiIndex for `loc` indexer with slice as `row_sel`.\r\n\r\n```py\r\n>>> pser\r\nx  a    1\r\n   b    2\r\ny  c    3\r\n   d    4\r\nz  e    5\r\ndtype: int64\r\n>>> pser.loc['y':]\r\ny  c    3\r\n   d    4\r\nz  e    5\r\ndtype: int64\r\n>>> pser.loc[:'y']\r\nx  a    1\r\n   b    2\r\ny  c    3\r\n   d    4\r\ndtype: int64\r\n>>> pser.loc[('x','b'):]\r\nx  b    2\r\ny  c    3\r\n   d    4\r\nz  e    5\r\ndtype: int64\r\n>>> pser.loc[:('y','c')]\r\nx  a    1\r\n   b    2\r\ny  c    3\r\ndtype: int64\r\n>>> pser.loc[('x','b'):('y','c')]\r\nx  b    2\r\ny  c    3\r\ndtype: int64\r\n>>> pser.loc['x':('y','c')]\r\nx  a    1\r\n   b    2\r\ny  c    3\r\ndtype: int64\r\n>>> pser.loc[('x','b'):'y']\r\nx  b    2\r\ny  c    3\r\n   d    4\r\ndtype: int64\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1173", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1173/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1173/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1173/events", "html_url": "https://github.com/databricks/koalas/issues/1173", "id": 546610279, "node_id": "MDU6SXNzdWU1NDY2MTAyNzk=", "number": 1173, "title": "loc works not properly for some cases.", "user": {"login": "itholic", "id": 44108233, "node_id": "MDQ6VXNlcjQ0MTA4MjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/44108233?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itholic", "html_url": "https://github.com/itholic", "followers_url": "https://api.github.com/users/itholic/followers", "following_url": "https://api.github.com/users/itholic/following{/other_user}", "gists_url": "https://api.github.com/users/itholic/gists{/gist_id}", "starred_url": "https://api.github.com/users/itholic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itholic/subscriptions", "organizations_url": "https://api.github.com/users/itholic/orgs", "repos_url": "https://api.github.com/users/itholic/repos", "events_url": "https://api.github.com/users/itholic/events{/privacy}", "received_events_url": "https://api.github.com/users/itholic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-08T02:31:24Z", "updated_at": "2020-01-08T07:41:26Z", "closed_at": "2020-01-08T07:41:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "assuming that we have `Series` like the below\r\n\r\n```python\r\n>>> kser\r\n7    10\r\n6    20\r\n5    30\r\n4    40\r\n3    50\r\n2    60\r\n1    70\r\nName: 0, dtype: int64\r\n```\r\n\r\nfor given `kser`, below examples are working differently from pandas.\r\n(`pser` is pandas' `Series` that has exactly same shape with `kser`)\r\n\r\n```python\r\n>>> pser.loc[6:4]\r\nSeries([], dtype: int64)\r\n\r\n>>> kser.loc[6:4]\r\n6    20\r\n5    30\r\n4    40\r\nName: 0, dtype: int64\r\n```\r\n```python\r\n>>> pser.to_frame().loc[6:4]\r\n    0\r\n6  20\r\n5  30\r\n4  40\r\n\r\n>>> kser.to_frame().loc[6:4]\r\nEmpty DataFrame\r\nColumns: [0]\r\nIndex: []\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1167", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1167/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1167/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1167/events", "html_url": "https://github.com/databricks/koalas/issues/1167", "id": 545269724, "node_id": "MDU6SXNzdWU1NDUyNjk3MjQ=", "number": 1167, "title": "Convert from / to Dask types ", "user": {"login": "Hoeze", "id": 1200058, "node_id": "MDQ6VXNlcjEyMDAwNTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1200058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hoeze", "html_url": "https://github.com/Hoeze", "followers_url": "https://api.github.com/users/Hoeze/followers", "following_url": "https://api.github.com/users/Hoeze/following{/other_user}", "gists_url": "https://api.github.com/users/Hoeze/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hoeze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hoeze/subscriptions", "organizations_url": "https://api.github.com/users/Hoeze/orgs", "repos_url": "https://api.github.com/users/Hoeze/repos", "events_url": "https://api.github.com/users/Hoeze/events{/privacy}", "received_events_url": "https://api.github.com/users/Hoeze/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-01-04T10:38:50Z", "updated_at": "2020-01-06T05:01:55Z", "closed_at": "2020-01-06T05:01:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I'm searching for a good way to convert from PySpark dataframes to lazy Dask dataframes.\r\nDoes Koalas support this type of conversion?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1166", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1166/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1166/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1166/events", "html_url": "https://github.com/databricks/koalas/issues/1166", "id": 544716986, "node_id": "MDU6SXNzdWU1NDQ3MTY5ODY=", "number": 1166, "title": "DataFrameGroupBy.describe", "user": {"login": "deepyaman", "id": 14007150, "node_id": "MDQ6VXNlcjE0MDA3MTUw", "avatar_url": "https://avatars3.githubusercontent.com/u/14007150?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepyaman", "html_url": "https://github.com/deepyaman", "followers_url": "https://api.github.com/users/deepyaman/followers", "following_url": "https://api.github.com/users/deepyaman/following{/other_user}", "gists_url": "https://api.github.com/users/deepyaman/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepyaman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepyaman/subscriptions", "organizations_url": "https://api.github.com/users/deepyaman/orgs", "repos_url": "https://api.github.com/users/deepyaman/repos", "events_url": "https://api.github.com/users/deepyaman/events{/privacy}", "received_events_url": "https://api.github.com/users/deepyaman/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552128, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI4", "url": "https://api.github.com/repos/databricks/koalas/labels/enhancement", "name": "enhancement", "color": "a2eeef", "default": true, "description": "New feature or request"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-01-02T19:41:08Z", "updated_at": "2020-01-16T04:49:09Z", "closed_at": "2020-01-16T04:49:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1158", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1158/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1158/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1158/events", "html_url": "https://github.com/databricks/koalas/issues/1158", "id": 543422507, "node_id": "MDU6SXNzdWU1NDM0MjI1MDc=", "number": 1158, "title": ".loc behavior when using 'slice'", "user": {"login": "beobest2", "id": 7010554, "node_id": "MDQ6VXNlcjcwMTA1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7010554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beobest2", "html_url": "https://github.com/beobest2", "followers_url": "https://api.github.com/users/beobest2/followers", "following_url": "https://api.github.com/users/beobest2/following{/other_user}", "gists_url": "https://api.github.com/users/beobest2/gists{/gist_id}", "starred_url": "https://api.github.com/users/beobest2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beobest2/subscriptions", "organizations_url": "https://api.github.com/users/beobest2/orgs", "repos_url": "https://api.github.com/users/beobest2/repos", "events_url": "https://api.github.com/users/beobest2/events{/privacy}", "received_events_url": "https://api.github.com/users/beobest2/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1179552126, "node_id": "MDU6TGFiZWwxMTc5NTUyMTI2", "url": "https://api.github.com/repos/databricks/koalas/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-29T14:33:13Z", "updated_at": "2020-01-08T07:37:31Z", "closed_at": "2020-01-08T07:37:31Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I test as shown in the documentation below. \r\nI found something strange.\r\n\r\nhttps://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.Series.loc.html\r\n![image](https://user-images.githubusercontent.com/7010554/71557962-02927a80-2a91-11ea-94e1-c448125d5a37.png)\r\n\r\n\r\npandas :\r\n'sidewinder' is at the end of the list, so it prints normally.\r\n\r\n```python\r\n>>> pdf = pd.DataFrame([[1, 2], [4, 5], [7, 8]], \r\n...             index=['cobra', 'viper', 'sidewinder'],  \r\n...             columns=['max_speed', 'shield'])\r\n>>> pdf.loc['cobra':'viper', 'max_speed']\r\ncobra    1\r\nviper    4\r\nName: max_speed, dtype: int64\r\n\r\n```\r\n\r\nbut koalas prints 'sidewinder'\r\n\r\n```python\r\n>>> kdf = ks.DataFrame([[1, 2], [4, 5], [7, 8]],\r\n...              index=['cobra', 'viper', 'sidewinder'],\r\n...              columns=['max_speed', 'shield'])\r\n>>> kdf.loc['cobra':'viper', 'max_speed']\r\ncobra         1\r\nviper         4\r\nsidewinder    7\r\nName: max_speed, dtype: int64\r\n\r\n>>> kdf.loc['cobra':, 'max_speed']\r\ncobra         1\r\nviper         4\r\nsidewinder    7\r\nName: max_speed, dtype: int64\r\n\r\n>>> kdf.loc['sidewinder':, 'max_speed']\r\nviper         4\r\nsidewinder    7\r\nName: max_speed, dtype: int64\r\n\r\n>>> kdf.loc['viper':, 'max_speed']\r\nviper    4\r\nName: max_speed, dtype: int64\r\n```\r\n\r\nInput is done in the order ['cobra', 'viper', 'sidewinder'], \r\nbut koals appears to be recognized as ['cobra', 'sidewinder', 'viper'].\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/databricks/koalas/issues/1157", "repository_url": "https://api.github.com/repos/databricks/koalas", "labels_url": "https://api.github.com/repos/databricks/koalas/issues/1157/labels{/name}", "comments_url": "https://api.github.com/repos/databricks/koalas/issues/1157/comments", "events_url": "https://api.github.com/repos/databricks/koalas/issues/1157/events", "html_url": "https://github.com/databricks/koalas/issues/1157", "id": 543119913, "node_id": "MDU6SXNzdWU1NDMxMTk5MTM=", "number": 1157, "title": "Fix some typos in install.rst & best_practices.rst", "user": {"login": "beobest2", "id": 7010554, "node_id": "MDQ6VXNlcjcwMTA1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/7010554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/beobest2", "html_url": "https://github.com/beobest2", "followers_url": "https://api.github.com/users/beobest2/followers", "following_url": "https://api.github.com/users/beobest2/following{/other_user}", "gists_url": "https://api.github.com/users/beobest2/gists{/gist_id}", "starred_url": "https://api.github.com/users/beobest2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/beobest2/subscriptions", "organizations_url": "https://api.github.com/users/beobest2/orgs", "repos_url": "https://api.github.com/users/beobest2/repos", "events_url": "https://api.github.com/users/beobest2/events{/privacy}", "received_events_url": "https://api.github.com/users/beobest2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-28T10:49:29Z", "updated_at": "2019-12-28T18:12:23Z", "closed_at": "2019-12-28T18:12:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "It's nothing special. A few typos have been found.\r\nplease check https://github.com/databricks/koalas/pull/1156 when someone available.\r\nThank you.", "performed_via_github_app": null, "score": 1.0}]}