{"total_count": 141, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/314", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/314/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/314/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/314/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/314", "id": 671768151, "node_id": "MDU6SXNzdWU2NzE3NjgxNTE=", "number": 314, "title": "concate() failed with NoRetryPolicy()", "user": {"login": "Juliehzl", "id": 49508232, "node_id": "MDQ6VXNlcjQ5NTA4MjMy", "avatar_url": "https://avatars0.githubusercontent.com/u/49508232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Juliehzl", "html_url": "https://github.com/Juliehzl", "followers_url": "https://api.github.com/users/Juliehzl/followers", "following_url": "https://api.github.com/users/Juliehzl/following{/other_user}", "gists_url": "https://api.github.com/users/Juliehzl/gists{/gist_id}", "starred_url": "https://api.github.com/users/Juliehzl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Juliehzl/subscriptions", "organizations_url": "https://api.github.com/users/Juliehzl/orgs", "repos_url": "https://api.github.com/users/Juliehzl/repos", "events_url": "https://api.github.com/users/Juliehzl/events{/privacy}", "received_events_url": "https://api.github.com/users/Juliehzl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-03T03:30:09Z", "updated_at": "2020-08-06T00:09:01Z", "closed_at": "2020-08-06T00:09:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\n---\r\nSee more in https://github.com/Azure/azure-cli/issues/14545\r\n\r\n```\r\n  File \"e:\\microsoft\\azurecli\\azure-cli\\src\\azure-cli\\azure\\cli\\command_modules\\dls\\custom.py\", line 244, in join_adls_items\r\n    client.concat(destination_path, source_paths)\r\n  File \"e:\\microsoft\\azurecli\\azure-cli\\env\\lib\\site-packages\\azure\\datalake\\store\\core.py\", line 816, in concat\r\n    retry_policy=NoRetryPolicy())\r\n  File \"e:\\microsoft\\azurecli\\azure-cli\\env\\lib\\site-packages\\azure\\datalake\\store\\lib.py\", line 429, in call\r\n    if request_successful or not retry_policy.should_retry(response, last_exception, retry_count):\r\nTypeError: should_retry() takes 1 positional argument but 4 were given\r\n```\r\nCustomer failed in linux and I failed in Windows.\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n0.0.48\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n3.7.7 and 3.6.5\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\nWindows and Linux\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/311", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/311/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/311/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/311/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/311", "id": 568667090, "node_id": "MDU6SXNzdWU1Njg2NjcwOTA=", "number": 311, "title": "What is the relationship between this library and azure-storage-file-datalake", "user": {"login": "markusweimer", "id": 502371, "node_id": "MDQ6VXNlcjUwMjM3MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/502371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markusweimer", "html_url": "https://github.com/markusweimer", "followers_url": "https://api.github.com/users/markusweimer/followers", "following_url": "https://api.github.com/users/markusweimer/following{/other_user}", "gists_url": "https://api.github.com/users/markusweimer/gists{/gist_id}", "starred_url": "https://api.github.com/users/markusweimer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markusweimer/subscriptions", "organizations_url": "https://api.github.com/users/markusweimer/orgs", "repos_url": "https://api.github.com/users/markusweimer/repos", "events_url": "https://api.github.com/users/markusweimer/events{/privacy}", "received_events_url": "https://api.github.com/users/markusweimer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-02-21T00:41:18Z", "updated_at": "2020-02-21T01:21:31Z", "closed_at": "2020-02-21T01:16:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I am looking for the right API to use to connect to Azure Data Lake Storage. Upon my search, I came across this library and the `azure-storage-file-datalake` which is used by the documentation found [here](https://docs.microsoft.com/en-us/samples/azure/azure-sdk-for-python/storage-datalake-samples/). \r\n\r\nWhat is not clear to me is which one of those to use? Is one going to replace the other? Or do they solve different aspects of the access to data lake?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/308", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/308/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/308/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/308/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/308", "id": 549837857, "node_id": "MDU6SXNzdWU1NDk4Mzc4NTc=", "number": 308, "title": "Logging doesn't work on unicode characters", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-01-14T21:47:56Z", "updated_at": "2020-08-11T23:04:25Z", "closed_at": "2020-08-11T23:04:25Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n\r\nSee https://travis-ci.org/Azure/azure-data-lake-store-python/jobs/637094760?utm_medium=notification&utm_source=github_status\r\n\r\n\r\n### Reproduction Steps\r\n\r\nCreate a folder with Unicode in file paths. Run list status on it with logging enabled and it will fail. \r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n0.48\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\nAll\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\nWindows 10\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\nbash", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/306", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/306/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/306/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/306/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/306", "id": 521991538, "node_id": "MDU6SXNzdWU1MjE5OTE1Mzg=", "number": 306, "title": "[Question]is this ADLS Gen1 package?", "user": {"login": "xiafu-msft", "id": 49707495, "node_id": "MDQ6VXNlcjQ5NzA3NDk1", "avatar_url": "https://avatars1.githubusercontent.com/u/49707495?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiafu-msft", "html_url": "https://github.com/xiafu-msft", "followers_url": "https://api.github.com/users/xiafu-msft/followers", "following_url": "https://api.github.com/users/xiafu-msft/following{/other_user}", "gists_url": "https://api.github.com/users/xiafu-msft/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiafu-msft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiafu-msft/subscriptions", "organizations_url": "https://api.github.com/users/xiafu-msft/orgs", "repos_url": "https://api.github.com/users/xiafu-msft/repos", "events_url": "https://api.github.com/users/xiafu-msft/events{/privacy}", "received_events_url": "https://api.github.com/users/xiafu-msft/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-11-13T06:36:55Z", "updated_at": "2020-01-14T20:45:43Z", "closed_at": "2020-01-14T20:45:43Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Hi azure-data-lake-storage-python team!\r\nI just want to know if this is ADLS Gen1 package... Due to the broken links across this page https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-python#next-steps I don't know where to find ADLS Gen1 doc for python sdk.\r\nYour confirmation will be really appreciated!\r\n\r\nIt could be very nice that you could also comment on the existing issue reported by another customer https://github.com/MicrosoftDocs/azure-docs/issues/39409 so that to benefit the future clients!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/305", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/305/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/305/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/305/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/305", "id": 516096838, "node_id": "MDU6SXNzdWU1MTYwOTY4Mzg=", "number": 305, "title": "Readme.md contains an unresolved merge conflict", "user": {"login": "jamesclarke", "id": 415771, "node_id": "MDQ6VXNlcjQxNTc3MQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/415771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesclarke", "html_url": "https://github.com/jamesclarke", "followers_url": "https://api.github.com/users/jamesclarke/followers", "following_url": "https://api.github.com/users/jamesclarke/following{/other_user}", "gists_url": "https://api.github.com/users/jamesclarke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesclarke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesclarke/subscriptions", "organizations_url": "https://api.github.com/users/jamesclarke/orgs", "repos_url": "https://api.github.com/users/jamesclarke/repos", "events_url": "https://api.github.com/users/jamesclarke/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesclarke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-01T12:58:18Z", "updated_at": "2020-01-14T21:49:29Z", "closed_at": "2020-01-14T21:49:29Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "[Readme.md](https://github.com/Azure/azure-data-lake-store-python/blob/ef82296ba2c00b3d702d4ed33309d3813eec2516/Readme.md#api) contains an unresolved merge conflict introduced in ef82296ba2c00b3d702d4ed33309d3813eec2516.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/302", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/302/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/302/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/302/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/302", "id": 502354810, "node_id": "MDU6SXNzdWU1MDIzNTQ4MTA=", "number": 302, "title": "Not able to set the ACL - set_acl throws stacktrace", "user": {"login": "Gaurang033", "id": 2620261, "node_id": "MDQ6VXNlcjI2MjAyNjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2620261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gaurang033", "html_url": "https://github.com/Gaurang033", "followers_url": "https://api.github.com/users/Gaurang033/followers", "following_url": "https://api.github.com/users/Gaurang033/following{/other_user}", "gists_url": "https://api.github.com/users/Gaurang033/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gaurang033/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gaurang033/subscriptions", "organizations_url": "https://api.github.com/users/Gaurang033/orgs", "repos_url": "https://api.github.com/users/Gaurang033/repos", "events_url": "https://api.github.com/users/Gaurang033/events{/privacy}", "received_events_url": "https://api.github.com/users/Gaurang033/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-03T23:56:12Z", "updated_at": "2020-03-31T17:42:05Z", "closed_at": "2020-03-31T17:42:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nI am able to list the ACL on given folder without any issue. However when I am trying to set the permission on any folder. it shows me error. If I try to set `recursive=True` it just hangs. \r\nI am not sure if I am providing the `acl_spec` correct or not. \r\nI have tried so far following specs. \r\n\r\n- `acl_spec = \"user:Gaurang.Shah@mosaic.com:rwx\"`\r\n- `acl_spec = \"user:Gaurang.Shah@mosaic.com:r-w-x\"`\r\n\r\nIn both the cases I am getting different errors. \r\n\r\n**Error** for  `acl_spec = \"user:Gaurang.Shah@mosaic.com:r-w-x\"` :\r\n```Traceback (most recent call last):\r\n  File \"/Users/gaurang.shah/PycharmProjects/demo/acl.py\", line 21, in <module>\r\n    adl.set_acl(path=\"tmp/\", acl_spec=acl_spec)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/core.py\", line 485, in set_acl\r\n    self._acl_call('SETACL', path, acl_spec, invalidate_cache=True)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/core.py\", line 459, in _acl_call\r\n    to_return = self.azure.call(action, posix_path, **parms)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/lib.py\", line 441, in call\r\n    self.log_response_and_raise(response, PermissionError(path), level=exception_log_level)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/lib.py\", line 349, in log_response_and_raise\r\n    raise exception\r\nazure.datalake.store.exceptions.PermissionError: tmp\r\n``` \r\n\r\n\r\n**Error** for `acl_spec = \"user:Gaurang.Shah@mosaic.com:r-w-x\"` \r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/gaurang.shah/PycharmProjects/demo/acl.py\", line 21, in <module>\r\n    adl.set_acl(path=\"tmp/\", acl_spec=acl_spec)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/core.py\", line 485, in set_acl\r\n    self._acl_call('SETACL', path, acl_spec, invalidate_cache=True)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/core.py\", line 459, in _acl_call\r\n    to_return = self.azure.call(action, posix_path, **parms)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/lib.py\", line 454, in call\r\n    self.log_response_and_raise(response, err, level=exception_log_level)\r\n  File \"/Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages/azure/datalake/store/lib.py\", line 349, in log_response_and_raise\r\n    raise exception\r\nazure.datalake.store.exceptions.DatalakeRESTException: Data-lake REST exception: SETACL, tmp\r\n```\r\n\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n```python\r\nacl_spec = \"user:Gaurang.Shah@mosaic.com:rwx\"\r\nadl.set_acl(path=\"tmp/\", acl_spec=acl_spec)\r\n```\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n\r\n```\r\nName: azure-datalake-store\r\nVersion: 0.0.47\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: /Users/gaurang.shah/Downloads/.venv/lib/python2.7/site-packages\r\nRequires: futures, adal, cffi, azure-nspkg, requests, pathlib2\r\nRequired-by: azure\r\n```\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 64 bit Python 2.7.10\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: MacOS 10.14.6\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/301", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/301/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/301/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/301/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/301", "id": 502126021, "node_id": "MDU6SXNzdWU1MDIxMjYwMjE=", "number": 301, "title": "Not able to get the ADLS File System Object - Throws exception ", "user": {"login": "Gaurang033", "id": 2620261, "node_id": "MDQ6VXNlcjI2MjAyNjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2620261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gaurang033", "html_url": "https://github.com/Gaurang033", "followers_url": "https://api.github.com/users/Gaurang033/followers", "following_url": "https://api.github.com/users/Gaurang033/following{/other_user}", "gists_url": "https://api.github.com/users/Gaurang033/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gaurang033/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gaurang033/subscriptions", "organizations_url": "https://api.github.com/users/Gaurang033/orgs", "repos_url": "https://api.github.com/users/Gaurang033/repos", "events_url": "https://api.github.com/users/Gaurang033/events{/privacy}", "received_events_url": "https://api.github.com/users/Gaurang033/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-10-03T15:08:41Z", "updated_at": "2019-10-03T16:12:30Z", "closed_at": "2019-10-03T16:12:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nWhen I try to create the `core.AzureDLFileSystem` Object I am getting following exception. \r\n\r\n```\r\nFile \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/azure/datalake/store/core.py\", line 72, in __init__\r\n    self.connect()\r\n  File \"/usr/local/lib/python3.7/site-packages/azure/datalake/store/core.py\", line 90, in connect\r\n    self.azure = DatalakeRESTInterface(token=self.token, req_timeout_s=self.per_call_timeout_seconds, **self.kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/azure/datalake/store/lib.py\", line 273, in __init__\r\n    self._check_token()  # Retryable method. Will ensure that signed_session token is current when we set it on next line\r\n  File \"/usr/local/lib/python3.7/site-packages/azure/datalake/store/lib.py\", line 307, in _check_token\r\n    check_token_internal()\r\n  File \"/usr/local/lib/python3.7/site-packages/azure/datalake/store/retry.py\", line 93, in f_retry\r\n    out = func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/azure/datalake/store/lib.py\", line 303, in check_token_internal\r\n    cur_session = self.token.signed_session()\r\nAttributeError: 'str' object has no attribute 'signed_session'\r\n```\r\n\r\n\r\n### Reproduction Steps\r\n\r\ntannent_id, username and password has changed.\r\n\r\n\r\n```python\r\nfrom azure.datalake.store import lib,core\r\ntenant_id=\"12b02d9b-d7f2-204e-b345-0dbdcf0330f8\"\r\ntoken = lib.auth(tenant_id, \"gaurang.shah@abc.com\", \"MyPassword\")\r\nadl = core.AzureDLFileSystem('myadlsfilesystem', token)\r\n```\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\nName: azure-datalake-store\r\nVersion: 0.0.47\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: /usr/local/lib/python3.7/site-packages\r\nRequires: adal, cffi, requests\r\nRequired-by:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n64 bit Python 3.7.4 for mac\r\n\r\nPython 3.7.4 (default, Jul  9 2019, 18:13:23)\r\n[Clang 10.0.1 (clang-1001.0.46.4)] on darwin\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Mac 10.14.6\r\n\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Bash", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/300", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/300/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/300/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/300/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/300", "id": 501218606, "node_id": "MDU6SXNzdWU1MDEyMTg2MDY=", "number": 300, "title": "Can files be filtered based on Last Modified time?", "user": {"login": "karthiksubramanians", "id": 42067835, "node_id": "MDQ6VXNlcjQyMDY3ODM1", "avatar_url": "https://avatars0.githubusercontent.com/u/42067835?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karthiksubramanians", "html_url": "https://github.com/karthiksubramanians", "followers_url": "https://api.github.com/users/karthiksubramanians/followers", "following_url": "https://api.github.com/users/karthiksubramanians/following{/other_user}", "gists_url": "https://api.github.com/users/karthiksubramanians/gists{/gist_id}", "starred_url": "https://api.github.com/users/karthiksubramanians/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karthiksubramanians/subscriptions", "organizations_url": "https://api.github.com/users/karthiksubramanians/orgs", "repos_url": "https://api.github.com/users/karthiksubramanians/repos", "events_url": "https://api.github.com/users/karthiksubramanians/events{/privacy}", "received_events_url": "https://api.github.com/users/karthiksubramanians/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-10-02T00:59:22Z", "updated_at": "2019-10-18T23:47:05Z", "closed_at": "2019-10-18T23:47:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\nI am trying to perform in-memory operations on files stored in azure datalake. I am unable to find documentation regarding using a matching pattern without using the ADL Downloader.\r\n\r\nFor a single file, this is the code I use\r\n```\r\n\r\nfilename = '/<folder/<filename>.json'\r\nwith adlsFileSystemClient.open(filename) as f:\r\n    for line in f:\r\n         <file-operations>\r\n```\r\n\r\nBut how do we filter based on filename (string matching) or based on last modified date.\r\n\r\nWhen I used U-SQL , I had the option to filter the fileset based on the last modified option. \r\n\r\n```\r\nDECLARE EXTERNAL @TodaysTime = DateTime.UtcNow.AddDays(-1);\r\n\r\n@rawInput=\r\n    EXTRACT jsonString string,\r\n            uri = FILE.URI()\r\n            ,modified_date = FILE.MODIFIED()\r\n    FROM @in\r\n    USING Extractors.Tsv(quoting : true);\r\n\r\n\r\n\r\n@parsedInput=\r\n    SELECT *\r\n    FROM @rawInput\r\n    WHERE modified_date > @TodaysTime;\r\n```\r\n\r\nIs there any similar options to filter the files modified during a specified period when using adlsFileSystemClient?\r\n\r\nAny help is appreciated.\r\n\r\n\r\n---\r\n\r\n\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.47\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 3.6.5 - 64bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windows 10\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Jupyter Notebook (Anaconda Navigator)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/299", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/299/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/299/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/299/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/299", "id": 494130604, "node_id": "MDU6SXNzdWU0OTQxMzA2MDQ=", "number": 299, "title": "ValueError: Token cannot be auto-refreshed.", "user": {"login": "AkshayTharval", "id": 16597923, "node_id": "MDQ6VXNlcjE2NTk3OTIz", "avatar_url": "https://avatars2.githubusercontent.com/u/16597923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AkshayTharval", "html_url": "https://github.com/AkshayTharval", "followers_url": "https://api.github.com/users/AkshayTharval/followers", "following_url": "https://api.github.com/users/AkshayTharval/following{/other_user}", "gists_url": "https://api.github.com/users/AkshayTharval/gists{/gist_id}", "starred_url": "https://api.github.com/users/AkshayTharval/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AkshayTharval/subscriptions", "organizations_url": "https://api.github.com/users/AkshayTharval/orgs", "repos_url": "https://api.github.com/users/AkshayTharval/repos", "events_url": "https://api.github.com/users/AkshayTharval/events{/privacy}", "received_events_url": "https://api.github.com/users/AkshayTharval/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-09-16T15:43:21Z", "updated_at": "2020-01-22T08:55:39Z", "closed_at": "2020-01-14T20:47:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nI am running a script which gets data from ADL and performs some calculations on local. I am able to run the script for 2 hours before it fails with the following error:\r\nValueError: Token cannot be auto-refreshed.\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\nfrom azure.datalake.store import core, lib, multithread\r\n\r\ntoken = lib.auth()\r\nadlsFileSystemClient = core.AzureDLFileSystem(token, store_name='yourADL')\r\n\r\n# Below line is where issue is observed\r\nwith adlsFileSystemClient.open(your_path_to_file) as f:\r\n    # Do something\r\n\r\n### Environment summary\r\nPython: 3.6.8\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nazure.datalake.store __version__ = \"0.0.44\"\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 64\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windows 10 Enterprise\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Using Jupyter and anaconda ( I dont know that answers your question)\r\n\r\nLet me know what more information you need.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/296", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/296/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/296/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/296/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/296", "id": 479391115, "node_id": "MDU6SXNzdWU0NzkzOTExMTU=", "number": 296, "title": "[Question] How to get \"pythonic\" access / last modified timestamps ?", "user": {"login": "glenfant", "id": 703389, "node_id": "MDQ6VXNlcjcwMzM4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/703389?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glenfant", "html_url": "https://github.com/glenfant", "followers_url": "https://api.github.com/users/glenfant/followers", "following_url": "https://api.github.com/users/glenfant/following{/other_user}", "gists_url": "https://api.github.com/users/glenfant/gists{/gist_id}", "starred_url": "https://api.github.com/users/glenfant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glenfant/subscriptions", "organizations_url": "https://api.github.com/users/glenfant/orgs", "repos_url": "https://api.github.com/users/glenfant/repos", "events_url": "https://api.github.com/users/glenfant/events{/privacy}", "received_events_url": "https://api.github.com/users/glenfant/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-11T17:18:40Z", "updated_at": "2019-08-12T15:08:36Z", "closed_at": "2019-08-12T15:08:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nI can't find how I can make pythonic values, suitable to the \"time\" or \"datetime\"  packages, with the \"accessTime\" and \"modificationTime\" keys of the object returned by the \"info(...)\" method of a \"AzureDLFileSystem\" instance.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n\r\n```python\r\ndlfs = AzureDLFileSystem(**your_connection_params)\r\ninfo = dlfs.info(\"/some/existing/file\")\r\nprint(info[\"accessTime\"])\r\n# -> 1538409759284\r\nprint(info[\"modificationTime\"])\r\n# -> 1538410080090\r\n```\r\nIn Python, we can play with \"time.time()\" that give the number of seconds since epoch (float) that can be converted to anything including \"datetime.datetime\" objects.\r\n\r\n### Environment summary\r\n\r\nPython 3.6.8 On MacOS 10.13 (High Sierra) 64 bits, azure-datalake-store 0.0.46, bash\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/295", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/295/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/295/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/295/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/295", "id": 469943574, "node_id": "MDU6SXNzdWU0Njk5NDM1NzQ=", "number": 295, "title": "Scrub logs", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-18T19:16:04Z", "updated_at": "2019-08-21T17:25:17Z", "closed_at": "2019-08-21T17:25:17Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/294", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/294/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/294/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/294/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/294", "id": 469943437, "node_id": "MDU6SXNzdWU0Njk5NDM0Mzc=", "number": 294, "title": "Documentation needs to be in markdown format", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-18T19:15:43Z", "updated_at": "2019-08-21T17:25:28Z", "closed_at": "2019-08-21T17:25:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/293", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/293/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/293/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/293/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/293", "id": 463428746, "node_id": "MDU6SXNzdWU0NjM0Mjg3NDY=", "number": 293, "title": "Low information error given when passed wrong password", "user": {"login": "hpbrewton", "id": 22136621, "node_id": "MDQ6VXNlcjIyMTM2NjIx", "avatar_url": "https://avatars3.githubusercontent.com/u/22136621?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hpbrewton", "html_url": "https://github.com/hpbrewton", "followers_url": "https://api.github.com/users/hpbrewton/followers", "following_url": "https://api.github.com/users/hpbrewton/following{/other_user}", "gists_url": "https://api.github.com/users/hpbrewton/gists{/gist_id}", "starred_url": "https://api.github.com/users/hpbrewton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hpbrewton/subscriptions", "organizations_url": "https://api.github.com/users/hpbrewton/orgs", "repos_url": "https://api.github.com/users/hpbrewton/repos", "events_url": "https://api.github.com/users/hpbrewton/events{/privacy}", "received_events_url": "https://api.github.com/users/hpbrewton/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-02T20:48:22Z", "updated_at": "2019-10-18T23:48:12Z", "closed_at": "2019-10-18T23:48:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nWhen trying to access a datalake I got an attribute error though the problem turned about to be an incorrect password.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n```python\r\nfrom azure.datalake.store import core, lib, multithread\r\n\r\ntenant_id = # <correct tenant>\r\nusername = # <valid username>\r\npassword = # <incorrect password>\r\n\r\ntoken = lib.auth(tenant_id = tenant_id, username = username, password = password)\r\n```\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.46\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 64\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windows 10\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Powershell\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/290", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/290/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/290/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/290/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/290", "id": 454388250, "node_id": "MDU6SXNzdWU0NTQzODgyNTA=", "number": 290, "title": "cffi library '_openssl' has no function, constant or global variable named 'CRYPTOGRAPHY_PACKAGE_VERSION'", "user": {"login": "prashanthmadi", "id": 1486808, "node_id": "MDQ6VXNlcjE0ODY4MDg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1486808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prashanthmadi", "html_url": "https://github.com/prashanthmadi", "followers_url": "https://api.github.com/users/prashanthmadi/followers", "following_url": "https://api.github.com/users/prashanthmadi/following{/other_user}", "gists_url": "https://api.github.com/users/prashanthmadi/gists{/gist_id}", "starred_url": "https://api.github.com/users/prashanthmadi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prashanthmadi/subscriptions", "organizations_url": "https://api.github.com/users/prashanthmadi/orgs", "repos_url": "https://api.github.com/users/prashanthmadi/repos", "events_url": "https://api.github.com/users/prashanthmadi/events{/privacy}", "received_events_url": "https://api.github.com/users/prashanthmadi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-06-10T21:46:50Z", "updated_at": "2019-06-17T20:50:28Z", "closed_at": "2019-06-17T20:50:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nfrom azure.datalake.store import core, lib\r\n\r\nreturns below error\r\n\r\nAttributeError: cffi library '_openssl' has no function, constant or global variable named 'CRYPTOGRAPHY_PACKAGE_VERSION'\r\n\r\n---\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.45\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:3.5.2\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<command-3229090550960880> in <module>()\r\n      1 #Libraries used\r\n      2 from pyspark.sql.functions import col, regexp_replace\r\n----> 3 from azure.datalake.store import core, lib\r\n\r\n/databricks/python/lib/python3.5/site-packages/azure/datalake/store/__init__.py in <module>()\r\n      9 __version__ = \"0.0.45\"\r\n     10 \r\n---> 11 from .core import AzureDLFileSystem\r\n     12 from .multithread import ADLDownloader\r\n     13 \r\n\r\n/databricks/python/lib/python3.5/site-packages/azure/datalake/store/core.py in <module>()\r\n     26 from .exceptions import DatalakeBadOffsetException, DatalakeIncompleteTransferException\r\n     27 from .exceptions import FileNotFoundError, PermissionError\r\n---> 28 from .lib import DatalakeRESTInterface\r\n     29 from .utils import ensure_writable, read_block\r\n     30 from .enums import ExpiryOptionType\r\n\r\n/databricks/python/lib/python3.5/site-packages/azure/datalake/store/lib.py in <module>()\r\n     30 \r\n     31 # 3rd party imports\r\n---> 32 import adal\r\n     33 import requests\r\n     34 import requests.exceptions\r\n\r\n/databricks/python/lib/python3.5/site-packages/adal/__init__.py in <module>()\r\n     32 import logging\r\n     33 \r\n---> 34 from .authentication_context import AuthenticationContext\r\n     35 from .token_cache import TokenCache\r\n     36 from .log import (set_logging_options, \r\n\r\n/databricks/python/lib/python3.5/site-packages/adal/authentication_context.py in <module>()\r\n     29 import warnings\r\n     30 \r\n---> 31 from .authority import Authority\r\n     32 from . import argument\r\n     33 from .code_request import CodeRequest\r\n\r\n/databricks/python/lib/python3.5/site-packages/adal/authority.py in <module>()\r\n     32     from urlparse import urlparse # pylint: disable=import-error,ungrouped-imports\r\n     33 \r\n---> 34 import requests\r\n     35 \r\n     36 from .constants import AADConstants\r\n\r\n/databricks/python/lib/python3.5/site-packages/requests/__init__.py in <module>()\r\n     93 # Attempt to enable urllib3's SNI support, if possible\r\n     94 try:\r\n---> 95     from urllib3.contrib import pyopenssl\r\n     96     pyopenssl.inject_into_urllib3()\r\n     97 \r\n\r\n/databricks/python/lib/python3.5/site-packages/urllib3/contrib/pyopenssl.py in <module>()\r\n     44 from __future__ import absolute_import\r\n     45 \r\n---> 46 import OpenSSL.SSL\r\n     47 from cryptography import x509\r\n     48 from cryptography.hazmat.backends.openssl import backend as openssl_backend\r\n\r\n/databricks/python/lib/python3.5/site-packages/OpenSSL/__init__.py in <module>()\r\n      6 \"\"\"\r\n      7 \r\n----> 8 from OpenSSL import rand, crypto, SSL\r\n      9 from OpenSSL.version import (\r\n     10     __author__, __copyright__, __email__, __license__, __summary__, __title__,\r\n\r\n/databricks/python/lib/python3.5/site-packages/OpenSSL/rand.py in <module>()\r\n     10 from six import integer_types as _integer_types\r\n     11 \r\n---> 12 from OpenSSL._util import (\r\n     13     ffi as _ffi,\r\n     14     lib as _lib,\r\n\r\n/databricks/python/lib/python3.5/site-packages/OpenSSL/_util.py in <module>()\r\n      4 from six import PY3, binary_type, text_type\r\n      5 \r\n----> 6 from cryptography.hazmat.bindings.openssl.binding import Binding\r\n      7 \r\n      8 \r\n\r\n/databricks/python/lib/python3.5/site-packages/cryptography/hazmat/bindings/openssl/binding.py in <module>()\r\n    186 \r\n    187 \r\n--> 188 _verify_package_version(cryptography.__version__)\r\n    189 \r\n    190 # OpenSSL is not thread safe until the locks are initialized. We call this\r\n\r\n/databricks/python/lib/python3.5/site-packages/cryptography/hazmat/bindings/openssl/binding.py in _verify_package_version(version)\r\n    173     # shared object that were loaded have the same version and raise an\r\n    174     # ImportError if they do not\r\n--> 175     so_package_version = ffi.string(lib.CRYPTOGRAPHY_PACKAGE_VERSION)\r\n    176     if version.encode(\"ascii\") != so_package_version:\r\n    177         raise ImportError(\r\n\r\nAttributeError: cffi library '_openssl' has no function, constant or global variable named 'CRYPTOGRAPHY_PACKAGE_VERSION'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/289", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/289/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/289/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/289/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/289", "id": 454316677, "node_id": "MDU6SXNzdWU0NTQzMTY2Nzc=", "number": 289, "title": "AzureDLFileSystem does not implement cp(path1, path2) method", "user": {"login": "pragatishah", "id": 51676989, "node_id": "MDQ6VXNlcjUxNjc2OTg5", "avatar_url": "https://avatars2.githubusercontent.com/u/51676989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pragatishah", "html_url": "https://github.com/pragatishah", "followers_url": "https://api.github.com/users/pragatishah/followers", "following_url": "https://api.github.com/users/pragatishah/following{/other_user}", "gists_url": "https://api.github.com/users/pragatishah/gists{/gist_id}", "starred_url": "https://api.github.com/users/pragatishah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pragatishah/subscriptions", "organizations_url": "https://api.github.com/users/pragatishah/orgs", "repos_url": "https://api.github.com/users/pragatishah/repos", "events_url": "https://api.github.com/users/pragatishah/events{/privacy}", "received_events_url": "https://api.github.com/users/pragatishah/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-10T18:52:35Z", "updated_at": "2019-06-28T03:24:24Z", "closed_at": "2019-06-28T03:24:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nThe cp method for core.AzureDLFileSystem  is not implemented. The source code for the method reveals the function is empty and raises an exception. \r\n\r\ncp is mentioned in the documentation here: https://docs.microsoft.com/en-us/python/api/azure-datalake-store/azure.datalake.store.core.azuredlfilesystem?view=azure-python\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n Step 1: Create core.AzureDLFileSystem object. \r\n\r\nazure_client = core.AzureDLFileSystem(lib.auth(**azure_config),\r\n                                store_name=account_name)\r\nazure_client.cp(source, destination) #raises NotImplementedError exception\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:  0.0.43\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 3.6.7, 64-bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:  Windows 10\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Bash on Windows\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/288", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/288/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/288/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/288/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/288", "id": 453692137, "node_id": "MDU6SXNzdWU0NTM2OTIxMzc=", "number": 288, "title": "None type has no status code", "user": {"login": "A9NAHZZ", "id": 51123774, "node_id": "MDQ6VXNlcjUxMTIzNzc0", "avatar_url": "https://avatars1.githubusercontent.com/u/51123774?v=4", "gravatar_id": "", "url": "https://api.github.com/users/A9NAHZZ", "html_url": "https://github.com/A9NAHZZ", "followers_url": "https://api.github.com/users/A9NAHZZ/followers", "following_url": "https://api.github.com/users/A9NAHZZ/following{/other_user}", "gists_url": "https://api.github.com/users/A9NAHZZ/gists{/gist_id}", "starred_url": "https://api.github.com/users/A9NAHZZ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/A9NAHZZ/subscriptions", "organizations_url": "https://api.github.com/users/A9NAHZZ/orgs", "repos_url": "https://api.github.com/users/A9NAHZZ/repos", "events_url": "https://api.github.com/users/A9NAHZZ/events{/privacy}", "received_events_url": "https://api.github.com/users/A9NAHZZ/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-07T21:00:46Z", "updated_at": "2019-06-28T03:24:30Z", "closed_at": "2019-06-28T03:24:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nTrying to acquire a token, instead got the below error\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\A9NAHZZ\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\graphql\\execution\\executor.py\", line 450, in resolve_or_error\r\n    return executor.execute(resolve_fn, source, info, **args)\r\n  File \"C:\\Users\\A9NAHZZ\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\graphql\\execution\\executors\\sync.py\", line 16, in execute\r\n    return fn(*args, **kwargs)\r\n  File \"main.py\", line 12, in resolve_size\r\n    token = lib.auth(settings.tenant, settings.username, settings.password)\r\n  File \"C:\\Users\\A9NAHZZ\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\azure\\datalake\\store\\lib.py\", line 150, in auth\r\n    out = get_token_internal()\r\n  File \"C:\\Users\\A9NAHZZ\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\azure\\datalake\\store\\retry.py\", line 104, in f_retry\r\n    request_successful = last_exception is None or response.status_code == 401  # 401 = Invalid credentials\r\nAttributeError: 'NoneType' object has no attribute 'status_code'\r\n```\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n`tok = lib.auth(t, u, p)`\r\nwhere t, u, p are just strings\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\nVersion: 0.0.45\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\n64 3.6\r\n\r\n**OS Version:** What OS and version are you using?  \r\nWindows\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nPowershell\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/287", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/287/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/287/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/287/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/287", "id": 453373142, "node_id": "MDU6SXNzdWU0NTMzNzMxNDI=", "number": 287, "title": "Specify Read/Connect Timeout in AzureDLFileSystem", "user": {"login": "skadyan", "id": 624821, "node_id": "MDQ6VXNlcjYyNDgyMQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/624821?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skadyan", "html_url": "https://github.com/skadyan", "followers_url": "https://api.github.com/users/skadyan/followers", "following_url": "https://api.github.com/users/skadyan/following{/other_user}", "gists_url": "https://api.github.com/users/skadyan/gists{/gist_id}", "starred_url": "https://api.github.com/users/skadyan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skadyan/subscriptions", "organizations_url": "https://api.github.com/users/skadyan/orgs", "repos_url": "https://api.github.com/users/skadyan/repos", "events_url": "https://api.github.com/users/skadyan/events{/privacy}", "received_events_url": "https://api.github.com/users/skadyan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-06-07T07:20:20Z", "updated_at": "2019-08-21T16:14:57Z", "closed_at": "2019-06-28T03:26:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nIt appears that there is no option of specifying the read/write timeout . Due to some network glitches, call to API like AzureDLFileSystem#open() or AzureDLFileSystem#ls() never return. Also infinite timeout (which seems default underlying in requests library), retry logic also not getting kicked in.  \r\nAs a result , our application script get hangs. \r\n\r\nDo we have any way to specify read/connect timeout per request or per ?\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\nPython 3.6.1\r\nrequests==2.19.1\r\nrequests-kerberos==0.12.0\r\nazure-datalake-store==0.0.22\r\n\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Linux\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/286", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/286/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/286/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/286/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/286", "id": 448861025, "node_id": "MDU6SXNzdWU0NDg4NjEwMjU=", "number": 286, "title": "azure-datalake-store auth.lib() with proxy not working", "user": {"login": "robertfjones", "id": 19250301, "node_id": "MDQ6VXNlcjE5MjUwMzAx", "avatar_url": "https://avatars1.githubusercontent.com/u/19250301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robertfjones", "html_url": "https://github.com/robertfjones", "followers_url": "https://api.github.com/users/robertfjones/followers", "following_url": "https://api.github.com/users/robertfjones/following{/other_user}", "gists_url": "https://api.github.com/users/robertfjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/robertfjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robertfjones/subscriptions", "organizations_url": "https://api.github.com/users/robertfjones/orgs", "repos_url": "https://api.github.com/users/robertfjones/repos", "events_url": "https://api.github.com/users/robertfjones/events{/privacy}", "received_events_url": "https://api.github.com/users/robertfjones/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-27T13:34:29Z", "updated_at": "2019-05-27T14:12:03Z", "closed_at": "2019-05-27T14:12:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nI am using the lib.auth for authenticating for a service to service connection, I can get this to work from my home connection but as soon as I am behind a corporate proxy it does not work. I have read this uses the Requests library but I can confirm the proxy settings don't seem to be used when applied.\r\n\r\n~~~~\r\n#Without proxy\r\nadlCreds = lib.auth(tenant_id = tenant,\r\n                client_secret = client_secret,\r\n                client_id = client_id,\r\n                resource = RESOURCE)\r\n\r\n#With proxy\r\nproxies = {'https': 'mysuser:mypassword@myproxy:1234'}\r\n\r\nadlCreds = lib.auth(tenant_id = tenant,\r\n                client_secret = client_secret,\r\n                client_id = client_id,\r\n                resource = RESOURCE,\r\n                proxies = proxies)\r\n~~~~\r\n\r\nSeparately, for completeness, when authenticating using the the adal library for account management the proxy works fine.\r\n~~~~\r\n#Azure management authentication\r\ncontext = adal.AuthenticationContext(authority_uri, api_version=None, proxies = proxies, verify_ssl=True)\r\nmgmt_token = context.acquire_token_with_client_credentials(RESOURCE, client_id, client_secret)\r\narmCreds = AADTokenCredentials(mgmt_token, client_id, resource=RESOURCE)\r\n~~~~\r\n\r\nPlease help!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/285", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/285/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/285/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/285/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/285", "id": 444000009, "node_id": "MDU6SXNzdWU0NDQwMDAwMDk=", "number": 285, "title": "ADLS Gen2 support", "user": {"login": "holger-tr", "id": 50631768, "node_id": "MDQ6VXNlcjUwNjMxNzY4", "avatar_url": "https://avatars2.githubusercontent.com/u/50631768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/holger-tr", "html_url": "https://github.com/holger-tr", "followers_url": "https://api.github.com/users/holger-tr/followers", "following_url": "https://api.github.com/users/holger-tr/following{/other_user}", "gists_url": "https://api.github.com/users/holger-tr/gists{/gist_id}", "starred_url": "https://api.github.com/users/holger-tr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/holger-tr/subscriptions", "organizations_url": "https://api.github.com/users/holger-tr/orgs", "repos_url": "https://api.github.com/users/holger-tr/repos", "events_url": "https://api.github.com/users/holger-tr/events{/privacy}", "received_events_url": "https://api.github.com/users/holger-tr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-14T16:03:08Z", "updated_at": "2019-06-25T19:45:11Z", "closed_at": "2019-06-25T19:45:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "We are using this library to interact with ADLS Gen1. We would like to upgrade to ADLS Gen2. Will this library support this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/274", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/274/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/274/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/274/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/274", "id": 414492788, "node_id": "MDU6SXNzdWU0MTQ0OTI3ODg=", "number": 274, "title": "Do not force the log level to DEBUG", "user": {"login": "nicolas-doby", "id": 10741033, "node_id": "MDQ6VXNlcjEwNzQxMDMz", "avatar_url": "https://avatars2.githubusercontent.com/u/10741033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nicolas-doby", "html_url": "https://github.com/nicolas-doby", "followers_url": "https://api.github.com/users/nicolas-doby/followers", "following_url": "https://api.github.com/users/nicolas-doby/following{/other_user}", "gists_url": "https://api.github.com/users/nicolas-doby/gists{/gist_id}", "starred_url": "https://api.github.com/users/nicolas-doby/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nicolas-doby/subscriptions", "organizations_url": "https://api.github.com/users/nicolas-doby/orgs", "repos_url": "https://api.github.com/users/nicolas-doby/repos", "events_url": "https://api.github.com/users/nicolas-doby/events{/privacy}", "received_events_url": "https://api.github.com/users/nicolas-doby/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-02-26T08:43:27Z", "updated_at": "2019-02-28T19:11:13Z", "closed_at": "2019-02-28T19:11:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\n\r\n### Description\r\n**Outline the issue here:**\r\nWhen modifying the ACL entries on a path the following logs are displayed:\r\n```\r\nSending exception sentinel\r\nException monitor thread finished\r\nSending logger sentinel\r\nLog queue closed\r\nLog thread finished\r\n...\r\n```\r\n\r\nDebug logs should only be displayed on demand; not forced.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n**Enumerate the steps to reproduce the issue here:**\r\nCall this method:\r\n`auth.adls_file_system_client.modify_acl_entries(path, acl_spec, recursive=True)`\r\n\r\nIn multuprocessor.py, the log level is forced to DEBUG:\r\nLine 24, 68 and 160: `logger.setLevel(logging.DEBUG)`.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nName: azure-datalake-store\r\nVersion: 0.0.41\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: c:\\users\\nicolas_doby\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\r\nRequires: adal, cffi, requests\r\nRequired-by: azure\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nPython 3.7.0 (64-bit)\r\n\r\n**OS Version:** What OS and version are you using?  \r\nWindows 10\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nUbuntu Bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/272", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/272/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/272/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/272/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/272", "id": 404686523, "node_id": "MDU6SXNzdWU0MDQ2ODY1MjM=", "number": 272, "title": "Missing azure/__init__.py in wheel causes import errors", "user": {"login": "macoun", "id": 1018886, "node_id": "MDQ6VXNlcjEwMTg4ODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1018886?v=4", "gravatar_id": "", "url": "https://api.github.com/users/macoun", "html_url": "https://github.com/macoun", "followers_url": "https://api.github.com/users/macoun/followers", "following_url": "https://api.github.com/users/macoun/following{/other_user}", "gists_url": "https://api.github.com/users/macoun/gists{/gist_id}", "starred_url": "https://api.github.com/users/macoun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/macoun/subscriptions", "organizations_url": "https://api.github.com/users/macoun/orgs", "repos_url": "https://api.github.com/users/macoun/repos", "events_url": "https://api.github.com/users/macoun/events{/privacy}", "received_events_url": "https://api.github.com/users/macoun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2019-01-30T09:50:57Z", "updated_at": "2019-03-11T23:56:51Z", "closed_at": "2019-03-11T23:56:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nmaybe I'm missing something here. So, my apologies if this issue makes no sense. The problem is that the umbrella package `azure` (I guess this is an umbrella package) is not recognised without an appropriate `__init__.py` file.\r\n\r\nThis is what I get\r\n\r\n```\r\n    from azure.datalake.store import core, lib, multithread\r\nImportError: No module named datalake.store\r\n``` \r\n\r\nI checked the release package and couldn't find `azure/__init__.py`. I'm not sure if this is an intended behaviour. \r\n\r\n```\r\nunzip -t ~/Downloads/azure_datalake_store-0.0.40-py2.py3-none-any.whl\r\nArchive:  /Users/ayaz/Downloads/azure_datalake_store-0.0.40-py2.py3-none-any.whl\r\n    testing: azure/datalake/__init__.py   OK\r\n    testing: azure/datalake/store/__init__.py   OK\r\n    testing: azure/datalake/store/core.py   OK\r\n    testing: azure/datalake/store/enums.py   OK\r\n    testing: azure/datalake/store/exceptions.py   OK\r\n    testing: azure/datalake/store/lib.py   OK\r\n    testing: azure/datalake/store/multiprocessor.py   OK\r\n    testing: azure/datalake/store/multithread.py   OK\r\n    testing: azure/datalake/store/retry.py   OK\r\n    testing: azure/datalake/store/transfer.py   OK\r\n    testing: azure/datalake/store/utils.py   OK\r\n    testing: samples/__init__.py      OK\r\n    testing: samples/benchmarks.py    OK\r\n    testing: samples/cli.py           OK\r\n    testing: azure_datalake_store-0.0.40.dist-info/METADATA   OK\r\n    testing: azure_datalake_store-0.0.40.dist-info/WHEEL   OK\r\n    testing: azure_datalake_store-0.0.40.dist-info/top_level.txt   OK\r\n    testing: azure_datalake_store-0.0.40.dist-info/RECORD   OK\r\nNo errors detected in compressed data of /Users/ayaz/Downloads/azure_datalake_store-0.0.40-py2.py3-none-any.whl.\r\n\r\n```\r\n---\r\n\r\n### Reproduction Steps\r\n```\r\nconda create --name env python=2.7\r\nsource activate env\r\npip install azure-datalake-store\r\npython -m azure.datalake.store\r\n/home/ayaz/.conda/envs/delme/bin/python: No module named datalake\r\n\r\n```\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n```\r\n$ pip show azure-datalake-store\r\nName: azure-datalake-store\r\nVersion: 0.0.40\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: /home/ayaz/.conda/envs/delme/lib/python2.7/site-packages/azure_datalake_store-0.0.40-py2.7.egg\r\nRequires: cffi, adal, requests, pathlib2, futures, azure-nspkg\r\n\r\n```\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\n\r\nELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), for GNU/Linux 2.6.32, not stripped\r\n\r\n**OS Version:** What OS and version are you using?  \r\n```\r\n$ cat /etc/redhat-release \r\nRed Hat Enterprise Linux Server release 7.6 (Maipo)\r\n```\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\n```\r\n$ echo $SHELL\r\n/bin/bash\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/271", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/271/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/271/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/271/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/271", "id": 401151578, "node_id": "MDU6SXNzdWU0MDExNTE1Nzg=", "number": 271, "title": "Save, load and run a transfer if a transfer is interrupted e.g., by user action, the transfer can be restarted at another time.", "user": {"login": "nandishjpatel", "id": 46875149, "node_id": "MDQ6VXNlcjQ2ODc1MTQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/46875149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nandishjpatel", "html_url": "https://github.com/nandishjpatel", "followers_url": "https://api.github.com/users/nandishjpatel/followers", "following_url": "https://api.github.com/users/nandishjpatel/following{/other_user}", "gists_url": "https://api.github.com/users/nandishjpatel/gists{/gist_id}", "starred_url": "https://api.github.com/users/nandishjpatel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nandishjpatel/subscriptions", "organizations_url": "https://api.github.com/users/nandishjpatel/orgs", "repos_url": "https://api.github.com/users/nandishjpatel/repos", "events_url": "https://api.github.com/users/nandishjpatel/events{/privacy}", "received_events_url": "https://api.github.com/users/nandishjpatel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-20T23:29:07Z", "updated_at": "2019-03-11T23:56:30Z", "closed_at": "2019-03-11T23:56:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "ADLUploader.save() or ADLDowloader.save() does not save a copy on a transfer process under ~/.config/azure-datalake-store\r\n---\r\n\r\n- A issue in azure.datalake.store.multithread.ADLUploader or ADLDownloader\r\nhttps://docs.microsoft.com/en-us/python/api/azurehttps://docs.microsoft.com/en-us/python/api/azure-datalake-store/azure.datalake.store.multithread.adluploader?view=azure-python-datalake-store/azure.datalake.store.multithread.adluploader?view=azure-python\r\n\r\n- Using save() to persist a upload/download transfer process in its current state to the disk does not generate any file under ~/.config/azure-datalake-store\r\nhttps://github.com/Azure/azure-data-lake-store-python/blob/d929cf3f07f94f3162fb041384c9d29476d0689e/azure/datalake/store/multithread.py#L36-L50\r\n\r\n- Using load() to list persisted transfer for possible resumption always returns {} an empty dictionary\r\n\r\n- Using run() without an argument as ADLUploader or ADLDownloader gives an error: [pylint] No value for argument 'self' in unbound method call [E1120]\r\n\r\n***Example:***\r\n \r\n```\r\n# ADLS standard python API usage to upload file(s)/directory from remote PC to ADLS\r\ndef adluploader(rpath, lpath):\r\n    try:\r\n        # multithread.ADLUploader.clear_saved()\r\n        multithread.ADLUploader(adlsFileSystemClient, rpath=rpath,\r\n                                lpath=lpath,\r\n                                progress_callback=_update_progress).save()\r\n    except exceptions.FileExistsError:\r\n        print 'The directory or the file(s) in directory already exist on {} ADLS'.format(data_lake_store_name)\r\n        overwrite_input = raw_input('Do you want to overwrite [Y/N]: ')\r\n        if overwrite_input.upper() == 'Y':\r\n            try:\r\n                # multithread.ADLUploader.clear_saved()\r\n                multithread.ADLUploader(adlsFileSystemClient, rpath=rpath,\r\n                                        lpath=lpath,\r\n                                        overwrite=True, progress_callback=_update_progress).save()\r\n            except exceptions.DatalakeIncompleteTransferException:\r\n                print 'Incomplete transfer process'\r\n                multithread.ADLUploader.load()\r\n                print multithread.ADLUploader.load()\r\n                print 'Retrying'\r\n                azure_upload = multithread.ADLUploader(adlsFileSystemClient, rpath=rpath,\r\n                                        lpath=lpath,\r\n                                        overwrite=True, progress_callback=_update_progress)\r\n                multithread.ADLUploader.run(azure_upload)\r\n        elif overwrite_input.upper() == 'N':\r\n            print 'Exiting'\r\n    except exceptions.PermissionError:\r\n        print 'You do not have permission to this directory'\r\n    except exceptions.DatalakeIncompleteTransferException:\r\n        print 'Incomplete transfer process'\r\n        multithread.ADLUploader.load()\r\n        print multithread.ADLUploader.load()\r\n        print 'Retrying'\r\n        azure_upload = multithread.ADLUploader(adlsFileSystemClient, rpath=rpath,\r\n                                lpath=lpath,\r\n                                progress_callback=_update_progress)\r\n        multithread.ADLUploader.run(azure_upload)\r\n```\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.39\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 2.7.12 64-bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Ubuntu 16.04 LTS\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/269", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/269/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/269/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/269/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/269", "id": 396606624, "node_id": "MDU6SXNzdWUzOTY2MDY2MjQ=", "number": 269, "title": "multithread.ADLUploader hangs indefinitely", "user": {"login": "djtrumbe", "id": 56872, "node_id": "MDQ6VXNlcjU2ODcy", "avatar_url": "https://avatars1.githubusercontent.com/u/56872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/djtrumbe", "html_url": "https://github.com/djtrumbe", "followers_url": "https://api.github.com/users/djtrumbe/followers", "following_url": "https://api.github.com/users/djtrumbe/following{/other_user}", "gists_url": "https://api.github.com/users/djtrumbe/gists{/gist_id}", "starred_url": "https://api.github.com/users/djtrumbe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/djtrumbe/subscriptions", "organizations_url": "https://api.github.com/users/djtrumbe/orgs", "repos_url": "https://api.github.com/users/djtrumbe/repos", "events_url": "https://api.github.com/users/djtrumbe/events{/privacy}", "received_events_url": "https://api.github.com/users/djtrumbe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-01-07T18:54:05Z", "updated_at": "2019-02-26T18:38:30Z", "closed_at": "2019-02-26T18:38:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nmultithread.ADLUploader hanging when uploading many (~40) files simultaneously.\r\n\r\n\r\nSome background:\r\n\r\n - We are processing many json files into tabular, parquet files.\r\n - This processing is done using ~40 worker threads, each thread processing a single file before grabbing another file off of the queue.\r\n - Each thread, pulls down the json file locally, iterates over the lines in the file, converts them to python dictionaries, flattens those dictionaries and adds them to a list.\r\n - Once this thread has processed all rows into a list of dictionaries, it creates a pandas dataframe with the data, then writes the data to a temporary, local parquet file, then uploads that file to an ADL.\r\n\r\nThe problem comes when there are thousands of very small files, so the processing of the files occurs quickly and we are quickly trying to upload many files simultaneously.  The process hangs indefinitely, producing no error nor returning at all.\r\n\r\nHere are also a few code snippets to give you some understanding of how this all works:\r\n\r\nimport multiprocessing as mp\r\n...\r\npool = mp.Pool(threads)\r\n...\r\nparse_results = pool.map(func=multi_process_file, iterable=process_info, chunksize=1)\r\n...\r\nto_adl_parquet(df, adl_file, adls, log_tag)\r\n...\r\ndef to_adl_parquet(df, adl_file, adl_connection, log_tag='', timestamp_fix=False):\r\n    log_tag = log_tag and '{} -'.format(log_tag) or log_tag\r\n    temp_file = '{}/{}_{}'.format(TEMP_DIRECTORY, uuid.uuid1(), os.path.basename(adl_file))\r\n    log('{}   writing temp file: {}'.format(log_tag, temp_file))\r\n    if timestamp_fix:\r\n        df.to_parquet(temp_file, coerce_timestamps='us', allow_truncated_timestamps=True)\r\n    else:\r\n        df.to_parquet(temp_file)\r\n    file_size = os.path.getsize(temp_file)\r\n    log('{}   uploading file:    {}  ({} bytes)'.format(log_tag, adl_file, file_size))\r\n    multithread.ADLUploader(adl_connection, lpath=temp_file, rpath=adl_file, nthreads=1, overwrite=True)  ## THIS IS THE LINE THAT HANGS INDEFINITELY\r\n    os.remove(temp_file)\r\n\r\n\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nThis problem is difficult to consistently reproduce.  However, the more small files we are processing, the more likely this is to happen.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n\r\nName: azure-datalake-store\r\nVersion: 0.0.39\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: /home/etar/virtualenv/lib/python2.7/site-packages\r\nRequires: futures, requests, cffi, azure-nspkg, adal, pathlib2\r\nRequired-by: azure\r\n\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\n\r\nPython 2.7.12\r\n\r\n\r\n**OS Version:** What OS and version are you using?  \r\n\r\n$ uname -a\r\nLinux etar03-ubuntu-prd 4.15.0-1035-azure #36~16.04.1-Ubuntu SMP Fri Nov 30 15:25:49 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\n\r\nbash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/267", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/267/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/267/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/267/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/267", "id": 393485572, "node_id": "MDU6SXNzdWUzOTM0ODU1NzI=", "number": 267, "title": "open gzip files", "user": {"login": "cognoscentai", "id": 26249332, "node_id": "MDQ6VXNlcjI2MjQ5MzMy", "avatar_url": "https://avatars1.githubusercontent.com/u/26249332?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cognoscentai", "html_url": "https://github.com/cognoscentai", "followers_url": "https://api.github.com/users/cognoscentai/followers", "following_url": "https://api.github.com/users/cognoscentai/following{/other_user}", "gists_url": "https://api.github.com/users/cognoscentai/gists{/gist_id}", "starred_url": "https://api.github.com/users/cognoscentai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cognoscentai/subscriptions", "organizations_url": "https://api.github.com/users/cognoscentai/orgs", "repos_url": "https://api.github.com/users/cognoscentai/repos", "events_url": "https://api.github.com/users/cognoscentai/events{/privacy}", "received_events_url": "https://api.github.com/users/cognoscentai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-12-21T14:53:04Z", "updated_at": "2018-12-21T15:06:19Z", "closed_at": "2018-12-21T15:06:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/266", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/266/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/266/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/266/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/266", "id": 393202158, "node_id": "MDU6SXNzdWUzOTMyMDIxNTg=", "number": 266, "title": "azure/__init__.py causing issues with importing other azure components ", "user": {"login": "ankitkumarr", "id": 16520682, "node_id": "MDQ6VXNlcjE2NTIwNjgy", "avatar_url": "https://avatars1.githubusercontent.com/u/16520682?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankitkumarr", "html_url": "https://github.com/ankitkumarr", "followers_url": "https://api.github.com/users/ankitkumarr/followers", "following_url": "https://api.github.com/users/ankitkumarr/following{/other_user}", "gists_url": "https://api.github.com/users/ankitkumarr/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankitkumarr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankitkumarr/subscriptions", "organizations_url": "https://api.github.com/users/ankitkumarr/orgs", "repos_url": "https://api.github.com/users/ankitkumarr/repos", "events_url": "https://api.github.com/users/ankitkumarr/events{/privacy}", "received_events_url": "https://api.github.com/users/ankitkumarr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-12-20T20:35:05Z", "updated_at": "2019-01-09T20:54:28Z", "closed_at": "2019-01-09T20:54:28Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n\r\nInstalling `azure-data-lake-store` includes `azure/__init__.py`  with these line\r\n```\r\n__import__('pkg_resources').declare_namespace(__name__)\r\n```\r\n`declare_namespace` does not seem compatible with PEP420 style implicit namespace. \r\nSo, if I have\r\n\r\nPathA/\r\n     ---azure/\r\n     ------\\_\\_init\\_\\_.py (with declare_namespace)\r\n     ------datalake/\r\n     -----------....\r\n\r\nPathB/\r\n     ---azure/\r\n     ------<some_azure_module>/\r\n     -----------....\r\n\r\nwhere both PathA and PathB are in my `PYTHONPATH`, importing `datalake` and `<some_azure_module>` will not work. \r\n\r\nAlthough Python 3.3+ (PEP 420) does not require having `__init__.py`, it might be more compatible to use-\r\n```\r\nfrom pkgutil import extend_path\r\n__path__ = extend_path(__path__, __name__)\r\n```\r\nBecause `extend_path` is PEP 420 and future compatible making the above scenario work. \r\n\r\n__Note:__\r\nThis could also be solved by holding a dependency on `azure-nspkg>=3.0.1` and not installing `__init__.py` directly, because that takes care of all scenarios.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/265", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/265/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/265/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/265/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/265", "id": 391735941, "node_id": "MDU6SXNzdWUzOTE3MzU5NDE=", "number": 265, "title": "Head (or similar) should support a timeout value", "user": {"login": "epa095", "id": 1576438, "node_id": "MDQ6VXNlcjE1NzY0Mzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1576438?v=4", "gravatar_id": "", "url": "https://api.github.com/users/epa095", "html_url": "https://github.com/epa095", "followers_url": "https://api.github.com/users/epa095/followers", "following_url": "https://api.github.com/users/epa095/following{/other_user}", "gists_url": "https://api.github.com/users/epa095/gists{/gist_id}", "starred_url": "https://api.github.com/users/epa095/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/epa095/subscriptions", "organizations_url": "https://api.github.com/users/epa095/orgs", "repos_url": "https://api.github.com/users/epa095/repos", "events_url": "https://api.github.com/users/epa095/events{/privacy}", "received_events_url": "https://api.github.com/users/epa095/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-12-17T14:26:38Z", "updated_at": "2019-01-30T19:25:55Z", "closed_at": "2019-01-30T19:25:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nWe have an issue where at least one file on our datalake is broken. In the portal it has a size, but preview does not work, and \"downloading\" it results in an empty file. \r\n\r\nThe problem is that when trying to download it (using `open`) with this library the result is that python just hangs with 100% CPU. Using info/ls/stat etc gives valid information, so there is no way of discovering its \"broken-ness\" from this. Using e.g. `head file_name 1` to read a single byte also results in python standing in an infinite loop with 100% CPU. It would be nice if `head`  accepted a timeout-parameter, then we could use it to detect when we were in this situation. Alternatively, do you have any other way of programmatically detect it?\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\nazure-datalake-store==0.0.39\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n3.6.7 64bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\nLinux\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\nbash", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/264", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/264/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/264/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/264/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/264", "id": 382470772, "node_id": "MDU6SXNzdWUzODI0NzA3NzI=", "number": 264, "title": "Number of threads shouldn't be tied to core count.", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-20T02:19:25Z", "updated_at": "2019-01-05T00:00:10Z", "closed_at": "2019-01-05T00:00:10Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nSince Python threads don't use separate cores i.e run within the same process, core count is not a good method of deciding number of threads for ADLDownloader and Uploader.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/263", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/263/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/263/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/263/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/263", "id": 382470252, "node_id": "MDU6SXNzdWUzODI0NzAyNTI=", "number": 263, "title": "Documentation needs update", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-20T02:16:52Z", "updated_at": "2019-02-28T00:00:50Z", "closed_at": "2019-02-28T00:00:50Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nDocstring documentation is not complete for all public functions.\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/255", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/255/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/255/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/255/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/255", "id": 376959288, "node_id": "MDU6SXNzdWUzNzY5NTkyODg=", "number": 255, "title": "Problems authenticating with token from get_azure_cli_credentials", "user": {"login": "isobit", "id": 3468064, "node_id": "MDQ6VXNlcjM0NjgwNjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/3468064?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isobit", "html_url": "https://github.com/isobit", "followers_url": "https://api.github.com/users/isobit/followers", "following_url": "https://api.github.com/users/isobit/following{/other_user}", "gists_url": "https://api.github.com/users/isobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/isobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isobit/subscriptions", "organizations_url": "https://api.github.com/users/isobit/orgs", "repos_url": "https://api.github.com/users/isobit/repos", "events_url": "https://api.github.com/users/isobit/events{/privacy}", "received_events_url": "https://api.github.com/users/isobit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 414640883, "node_id": "MDU6TGFiZWw0MTQ2NDA4ODM=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/Resolved", "name": "Resolved", "color": "FFA500", "default": false, "description": null}, {"id": 415132854, "node_id": "MDU6TGFiZWw0MTUxMzI4NTQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/in%20progress", "name": "in progress", "color": "ededed", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-11-02T20:36:46Z", "updated_at": "2018-11-06T22:57:30Z", "closed_at": "2018-11-06T00:35:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nPassing a token retrieved from `azure.common.credentials.get_azure_cli_credentials` to `AzureDLFileSystem` results in an exception:\r\n\r\n```\r\n...\r\n  File \"/home/josh/code/adls-fuse/_venv/lib/python2.7/site-packages/azure_datalake_store-0.0.36-py2.7.egg/azure/datalake/store/core.py\", line 70, in __init__\r\n    self.connect()\r\n  File \"/home/josh/code/adls-fuse/_venv/lib/python2.7/site-packages/azure_datalake_store-0.0.36-py2.7.egg/azure/datalake/store/core.py\", line 88, in connect\r\n    self.azure = DatalakeRESTInterface(token=self.token, **self.kwargs)\r\n  File \"/home/josh/code/adls-fuse/_venv/lib/python2.7/site-packages/azure_datalake_store-0.0.36-py2.7.egg/azure/datalake/store/lib.py\", line 274, in __init__\r\n    self.head = {'Authorization': token.signed_session(retry_policy=None).headers['Authorization']}\r\nTypeError: signed_session() got an unexpected keyword argument 'retry_policy'\r\n```\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n\r\nFirst, log in with `az login`.\r\n\r\n```\r\nfrom azure.common.credentials import get_cli_credentials\r\nfrom azure.datalake.store.core import AzureDLFileSystem\r\n\r\ntoken, subscription_id = get_cli_credentials()\r\nadlfs = AzureDLFileSystem(token=token, store_name='<store name here>')\r\n```\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** 0.0.36\r\n\r\n**Python Version:** 2.7\r\n\r\n**OS Version:** CentOS 7\r\n\r\n**Shell Type:** Bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/252", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/252/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/252/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/252/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/252", "id": 375259088, "node_id": "MDU6SXNzdWUzNzUyNTkwODg=", "number": 252, "title": "Concat should not retry", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-29T23:33:03Z", "updated_at": "2019-06-28T03:26:30Z", "closed_at": "2019-06-28T03:26:30Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\nConcat and append are not idempotent and should not be retried.\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.34\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 64 bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windows Server 2016\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: cmd.exe\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/245", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/245/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/245/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/245/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/245", "id": 369493711, "node_id": "MDU6SXNzdWUzNjk0OTM3MTE=", "number": 245, "title": "Merging files after chunked upload with + in the filename fails", "user": {"login": "solita-tvoipio", "id": 44016520, "node_id": "MDQ6VXNlcjQ0MDE2NTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/44016520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/solita-tvoipio", "html_url": "https://github.com/solita-tvoipio", "followers_url": "https://api.github.com/users/solita-tvoipio/followers", "following_url": "https://api.github.com/users/solita-tvoipio/following{/other_user}", "gists_url": "https://api.github.com/users/solita-tvoipio/gists{/gist_id}", "starred_url": "https://api.github.com/users/solita-tvoipio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/solita-tvoipio/subscriptions", "organizations_url": "https://api.github.com/users/solita-tvoipio/orgs", "repos_url": "https://api.github.com/users/solita-tvoipio/repos", "events_url": "https://api.github.com/users/solita-tvoipio/events{/privacy}", "received_events_url": "https://api.github.com/users/solita-tvoipio/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-10-12T10:30:48Z", "updated_at": "2018-10-16T23:03:05Z", "closed_at": "2018-10-16T23:03:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\nChunk upload to DLS fails if the chunk names contain the `+` character. Possibly (likely) other characters may be affected too.\r\n\r\nInitially the issue was discovered when azure CLI (running on Linux) failed to upload large files with `+` in their remote names, while smaller files with the same name succeeded.\r\n\r\nAccording to initial analysis, the issue occurs due to how the `concat()` function uses the `MSCONCAT` operation to concatenate file chunks after upload.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n1. Download the attached files\r\n2. fill in secrets to `secrets.json.tpl` and rename it to `secrets.json`\r\n3. fill in DLS account name to the `config.json.*` files\r\n4. copy some largeish file (larger than the default chunking limit, 256 MiB) to the same directory as `test_file` and `test+file`\r\n5. copy one of the `config.json.*` files as `config.json`\r\n6. run `python chunk_upload_example.py`\r\n\r\nWhen uploading a file such that the remote name contains a `+` character as passed to ADLUploader via the `rpath` argument, the upload fails, but the failure is not visibly indicated by e.g. an exception being raised (or passed to the calling Python function). When uploading a file such that the remote name does not contain a `+`, the upload succeeds, even if the local filename contained a `+`.\r\n\r\nWhen the upload fails, the chunks remain in the DLS. This suggests that the failure occurs somewhere in the chain multithread.ADLUploader() -> multithread.merge_chunks() -> core.AzureDLFileSystem.concat(), which in turn makes the REST API call with operation `MSCONCAT`.\r\n\r\nMerging the file chunks succeeds if the MSCONCAT WebHDFS operation (not documented in the public API documentation, but mentioned in the Swagger file [here](https://github.com/Azure/azure-rest-api-specs/blob/master/specification/datalake-store/data-plane/Microsoft.DataLakeStore/stable/2016-11-01/filesystem.json) ) is called so that the `+` sign in the chunk names is urlencoded as `%2B`.\r\n\r\nCalling the REST API endpoints directly (e.g. with [Postman](https://www.getpostman.com/)), assuming that the folder `upload_test` contains files `test+001` and `test+002`:\r\n\r\n- HTTP POST to `https://{{dlAccount}}.azuredatalakestore.net/webhdfs/v1/upload_test/test_plus?op=MSCONCAT` with body (type raw) `sources=/upload_test/test+001,/upload_test/test+002`: fails\r\n- HTTP POST to `https://{{dlAccount}}.azuredatalakestore.net/webhdfs/v1/upload_test/test_plus?op=MSCONCAT` with body (type raw) `sources=/upload_test/test%2B001,/upload_test/test%2B002`: success, file `test_plus` appears in the folder `upload_test` and the two files disappear\r\n\r\nSubmitting as an issue instead of pull request, as it is not really possible to say whether this issue should be fixed in the Python library (e.g. using `urllib.quote()` in [core.py](https://github.com/Azure/azure-data-lake-store-python/blob/master/azure/datalake/store/core.py#L543)) or in the DLS REST API implementation. In any case, it would be nice if all REST API operations invoked by the Azure CLI tools were documented sufficiently, including encoding issues.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n```\r\nName: azure-datalake-store\r\nVersion: 0.0.32\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: /Users/<redacted>/.pyenv/versions/3.6.6/envs/adls-3.6.6/lib/python3.6/site-packages\r\nRequires: azure-nspkg, adal, cffi\r\nRequired-by: \r\n```\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n```Python 3.6.6 (default, Oct 10 2018, 12:07:53) \r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin```\r\n\r\n64-bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\nmacOS Sierra, 10.12.6 (16G1510)\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n\r\nGNU bash, version 3.2.57(1)-release (x86_64-apple-darwin16)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/242", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/242/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/242/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/242/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/242", "id": 361034515, "node_id": "MDU6SXNzdWUzNjEwMzQ1MTU=", "number": 242, "title": "AzureDLFileSystem does not follow PythonFilesystem Interface", "user": {"login": "yshahin", "id": 207449, "node_id": "MDQ6VXNlcjIwNzQ0OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/207449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yshahin", "html_url": "https://github.com/yshahin", "followers_url": "https://api.github.com/users/yshahin/followers", "following_url": "https://api.github.com/users/yshahin/following{/other_user}", "gists_url": "https://api.github.com/users/yshahin/gists{/gist_id}", "starred_url": "https://api.github.com/users/yshahin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yshahin/subscriptions", "organizations_url": "https://api.github.com/users/yshahin/orgs", "repos_url": "https://api.github.com/users/yshahin/repos", "events_url": "https://api.github.com/users/yshahin/events{/privacy}", "received_events_url": "https://api.github.com/users/yshahin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-09-17T20:38:58Z", "updated_at": "2018-11-02T19:32:04Z", "closed_at": "2018-11-02T19:32:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nWorking with [Bonobo](http://docs.bonobo-project.org) lib I am unable to utilize the library cause it doesn't conform the PythonFilesystem Interface.\r\nCurrent problem facing is \"open() got an unexpected keyword argument 'encoding'\"\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\nUse AzureDLFileSystem instance as a bonobo file system service\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.31\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 64-bit v3.6.5\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windows\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Git Bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/239", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/239/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/239/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/239/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/239", "id": 358145169, "node_id": "MDU6SXNzdWUzNTgxNDUxNjk=", "number": 239, "title": "ls needs batching for folders with large number of files", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-07T17:10:03Z", "updated_at": "2018-09-11T20:39:02Z", "closed_at": "2018-09-11T20:39:02Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nls for very large folders fails due to timeout errors. \r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\nadl.ls('Folder With Large number of files')\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n0.0.30\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nPython 3.6, 64 bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\nWindows 10\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\ncmd.exe", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/238", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/238/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/238/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/238/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/238", "id": 356891548, "node_id": "MDU6SXNzdWUzNTY4OTE1NDg=", "number": 238, "title": "Support for PyArrow", "user": {"login": "dcieslak19973", "id": 3847063, "node_id": "MDQ6VXNlcjM4NDcwNjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3847063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dcieslak19973", "html_url": "https://github.com/dcieslak19973", "followers_url": "https://api.github.com/users/dcieslak19973/followers", "following_url": "https://api.github.com/users/dcieslak19973/following{/other_user}", "gists_url": "https://api.github.com/users/dcieslak19973/gists{/gist_id}", "starred_url": "https://api.github.com/users/dcieslak19973/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dcieslak19973/subscriptions", "organizations_url": "https://api.github.com/users/dcieslak19973/orgs", "repos_url": "https://api.github.com/users/dcieslak19973/repos", "events_url": "https://api.github.com/users/dcieslak19973/events{/privacy}", "received_events_url": "https://api.github.com/users/dcieslak19973/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106231, "node_id": "MDU6TGFiZWw0MDUxMDYyMzE=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/wontfix", "name": "wontfix", "color": "ffffff", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-04T16:42:37Z", "updated_at": "2018-11-06T00:35:56Z", "closed_at": "2018-11-06T00:35:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nPyArrow supports HDFS and S3. Would be nice if ADL (and ADL Gen 2) were supported as well.\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\nThe ADLFS class cannot be passed in to the PyArrow Parquest DataSet class.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.30\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: both\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Linux\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: Bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/236", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/236/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/236/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/236/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/236", "id": 354459320, "node_id": "MDU6SXNzdWUzNTQ0NTkzMjA=", "number": 236, "title": "Cannot import azure-datalake-store in python <3.3 if no other azure library is installed", "user": {"login": "VincentBLortie", "id": 1178934, "node_id": "MDQ6VXNlcjExNzg5MzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1178934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VincentBLortie", "html_url": "https://github.com/VincentBLortie", "followers_url": "https://api.github.com/users/VincentBLortie/followers", "following_url": "https://api.github.com/users/VincentBLortie/following{/other_user}", "gists_url": "https://api.github.com/users/VincentBLortie/gists{/gist_id}", "starred_url": "https://api.github.com/users/VincentBLortie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VincentBLortie/subscriptions", "organizations_url": "https://api.github.com/users/VincentBLortie/orgs", "repos_url": "https://api.github.com/users/VincentBLortie/repos", "events_url": "https://api.github.com/users/VincentBLortie/events{/privacy}", "received_events_url": "https://api.github.com/users/VincentBLortie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-08-27T20:08:05Z", "updated_at": "2018-08-29T15:33:31Z", "closed_at": "2018-08-29T15:33:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nWhen azure-datalake-store is the only azure package installed using pip in a python environment with version < 3.3, azure.datalake.store cannot be imported.\r\n\r\n(My guess is that unlike other azure packages, azure-datalake-store does not depend on azure-nspkg)\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n1. Create a new environment with a version of python earlier than 3.3 and activate that environment\r\n\r\nExample:\r\n```\r\nconda create --name p27 python=2.7\r\nsource activate p27\r\n```\r\n\r\n2. Install azure-datalake-store and nothing else: `pip install azure-datalake-store`\r\n\r\n3. Attempt to import azure.datalake.store: `python -c \"import azure.datalake.store\"`\r\n\r\nThe output is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named azure.datalake.store\r\n```\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n```\r\nName: azure-datalake-store\r\nVersion: 0.0.29\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: /Users/vincentlortie/miniconda2/envs/p27/lib/python2.7/site-packages\r\nRequires: futures, pathlib2, adal, cffi\r\nRequired-by:\r\n```\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\n2.7.15\r\n\r\n**OS Version:** What OS and version are you using?  \r\nmacOS High Sierra 10.13.6\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nzsh\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/235", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/235/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/235/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/235/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/235", "id": 353152180, "node_id": "MDU6SXNzdWUzNTMxNTIxODA=", "number": 235, "title": "test_read_delimited_bloc under test_core.py randomly fails.", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-22T22:40:11Z", "updated_at": "2018-10-05T18:46:38Z", "closed_at": "2018-10-05T18:46:38Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n\r\ntest_read_delimited_bloc under test_core.py randomly fails.\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\nIt is easily observed on seeing travis test logs where on any run, there is a  chance of the test failing under one or more Python versions, though not consistently against any particular version.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n0.0.28\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nVariable\r\n\r\n**OS Version:** What OS and version are you using?  \r\nLinux,Windows\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nbash, cmd.exe\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/233", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/233/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/233/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/233/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/233", "id": 352945171, "node_id": "MDU6SXNzdWUzNTI5NDUxNzE=", "number": 233, "title": "Downloader does not maintain same file structure", "user": {"login": "cassioiks", "id": 1523715, "node_id": "MDQ6VXNlcjE1MjM3MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1523715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cassioiks", "html_url": "https://github.com/cassioiks", "followers_url": "https://api.github.com/users/cassioiks/followers", "following_url": "https://api.github.com/users/cassioiks/following{/other_user}", "gists_url": "https://api.github.com/users/cassioiks/gists{/gist_id}", "starred_url": "https://api.github.com/users/cassioiks/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cassioiks/subscriptions", "organizations_url": "https://api.github.com/users/cassioiks/orgs", "repos_url": "https://api.github.com/users/cassioiks/repos", "events_url": "https://api.github.com/users/cassioiks/events{/privacy}", "received_events_url": "https://api.github.com/users/cassioiks/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-22T13:05:18Z", "updated_at": "2018-08-22T13:18:47Z", "closed_at": "2018-08-22T13:18:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\nWe're using the Downloader class and it's showing erratic behavior in different scenarios:\r\n\r\n- When downloading from a folder with only 1 file: Downloader class downloads only the file (e.g.: *sample_file.json*)\r\n- When downloading from a folder with multiple files: Downloader class downloads the entire path structure until the folder being downloaded (e.g.: */path/to/folder/sample_file.json* and  */path/to/folder/sample_file2.json*)\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nSimply instantiate the class and let it download:\r\n\r\n```python\r\nfrom azure.datalake.store import multithread\r\nmultithread.ADLDownloader(adlfs = adlsFileSystemClient,\r\n                                                 rpath = pathSrc,\r\n                                                 lpath = \".\",\r\n                                                 overwrite = True)\r\n```\r\n\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nVersion: 0.0.22\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nPython 2.7.12 64-bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nUbuntu 16.04.4 LTS", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/232", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/232/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/232/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/232/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/232", "id": 352593876, "node_id": "MDU6SXNzdWUzNTI1OTM4NzY=", "number": 232, "title": "AzureDLFile does not implement readinto() function", "user": {"login": "kasuteru", "id": 16296708, "node_id": "MDQ6VXNlcjE2Mjk2NzA4", "avatar_url": "https://avatars2.githubusercontent.com/u/16296708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kasuteru", "html_url": "https://github.com/kasuteru", "followers_url": "https://api.github.com/users/kasuteru/followers", "following_url": "https://api.github.com/users/kasuteru/following{/other_user}", "gists_url": "https://api.github.com/users/kasuteru/gists{/gist_id}", "starred_url": "https://api.github.com/users/kasuteru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kasuteru/subscriptions", "organizations_url": "https://api.github.com/users/kasuteru/orgs", "repos_url": "https://api.github.com/users/kasuteru/repos", "events_url": "https://api.github.com/users/kasuteru/events{/privacy}", "received_events_url": "https://api.github.com/users/kasuteru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-08-21T15:25:33Z", "updated_at": "2018-10-16T23:03:24Z", "closed_at": "2018-10-16T23:03:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "Some libraries expect file-like objects to have a readinto() method, see also [here](https://docs.python.org/3/library/io.html#module-io). This is currently missing for the AzureDLFile class, making using the ADLS impossible for any libraries that try to use this method.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/230", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/230/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/230/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/230/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/230", "id": 351129377, "node_id": "MDU6SXNzdWUzNTExMjkzNzc=", "number": 230, "title": "Maintain file structure when using ADLUploader even if directory is empty", "user": {"login": "masangster", "id": 39884422, "node_id": "MDQ6VXNlcjM5ODg0NDIy", "avatar_url": "https://avatars3.githubusercontent.com/u/39884422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/masangster", "html_url": "https://github.com/masangster", "followers_url": "https://api.github.com/users/masangster/followers", "following_url": "https://api.github.com/users/masangster/following{/other_user}", "gists_url": "https://api.github.com/users/masangster/gists{/gist_id}", "starred_url": "https://api.github.com/users/masangster/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/masangster/subscriptions", "organizations_url": "https://api.github.com/users/masangster/orgs", "repos_url": "https://api.github.com/users/masangster/repos", "events_url": "https://api.github.com/users/masangster/events{/privacy}", "received_events_url": "https://api.github.com/users/masangster/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 415132854, "node_id": "MDU6TGFiZWw0MTUxMzI4NTQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/in%20progress", "name": "in progress", "color": "ededed", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-08-16T09:39:00Z", "updated_at": "2018-10-05T18:46:49Z", "closed_at": "2018-10-05T18:46:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nSimilar to the previous issue with the downloader, the uploader does not replicate the file structure on the remote path if the directory tree does not contain any file but only folders. This is needed to give predictable behaviour regarding whether a folder should already exist or not.\r\n\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.27\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 3.6 64 bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windows 7 and 10\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: VS code\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/229", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/229/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/229/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/229/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/229", "id": 350701602, "node_id": "MDU6SXNzdWUzNTA3MDE2MDI=", "number": 229, "title": "Add \"selective overwrite based on newest date modified\" option for downloads and uploads", "user": {"login": "masangster", "id": 39884422, "node_id": "MDQ6VXNlcjM5ODg0NDIy", "avatar_url": "https://avatars3.githubusercontent.com/u/39884422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/masangster", "html_url": "https://github.com/masangster", "followers_url": "https://api.github.com/users/masangster/followers", "following_url": "https://api.github.com/users/masangster/following{/other_user}", "gists_url": "https://api.github.com/users/masangster/gists{/gist_id}", "starred_url": "https://api.github.com/users/masangster/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/masangster/subscriptions", "organizations_url": "https://api.github.com/users/masangster/orgs", "repos_url": "https://api.github.com/users/masangster/repos", "events_url": "https://api.github.com/users/masangster/events{/privacy}", "received_events_url": "https://api.github.com/users/masangster/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-15T06:36:29Z", "updated_at": "2018-09-24T16:46:39Z", "closed_at": "2018-09-24T16:46:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "This option would be useful in the case that, for example, I download some subdirectory of files, use some of them as inputs to calculations, and then re-upload the entire subdirectory with additional calculation outputs scattered within subfolders in the subdirectory. In the meantime however, someone has updated a new version of a file that I am using as an input. When I re-upload the subdirectory including outputs to the data lake, then my older, not updated version of the input file will overwrite the newer version uploaded by a colleague in the meantime. In this case, it would be good to have a selective overwrite based on only overwriting if the files are newer.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/227", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/227/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/227/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/227/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/227", "id": 348940799, "node_id": "MDU6SXNzdWUzNDg5NDA3OTk=", "number": 227, "title": "FileNotFoundError exception import missing in multithread.py", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 415132854, "node_id": "MDU6TGFiZWw0MTUxMzI4NTQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/in%20progress", "name": "in progress", "color": "ededed", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-09T00:51:28Z", "updated_at": "2018-08-09T01:29:44Z", "closed_at": "2018-08-09T01:29:44Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\nFileNotFoundError exception import missing in multithread.py\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nDownload a file that doesn't exist using ADDownloader, while using Python 2.7, instead of FileNotFoundError exception on line120:multithread.py, a NameError is raised due to undefined exception.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** 0.0.26\r\n\r\n**Python Version:**  2.7\r\n\r\n**OS Version:** Windows 10\r\n\r\n**Shell Type:** cmd.exe\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/225", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/225/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/225/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/225/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/225", "id": 343197099, "node_id": "MDU6SXNzdWUzNDMxOTcwOTk=", "number": 225, "title": "tests/recordings should not be in the repository", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-20T17:59:39Z", "updated_at": "2018-09-10T22:49:09Z", "closed_at": "2018-09-10T22:49:09Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\nI am not 100% sure, but I think tests/recordings folder is generated every time the test suite is run. Since it is always dynamically generated after running the tests, and travis build has the test results anyway, I don't think this folder should be maintained in the repo. \r\n\r\nRunning tests cause this folder to be changed and pollutes the git change tree. Fix would be to add it to .gitignore .\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nNA\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.24\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:NA\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:NA\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:NA\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/223", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/223/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/223/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/223/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/223", "id": 342110516, "node_id": "MDU6SXNzdWUzNDIxMTA1MTY=", "number": 223, "title": "README cleanup", "user": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 415132854, "node_id": "MDU6TGFiZWw0MTUxMzI4NTQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/in%20progress", "name": "in progress", "color": "ededed", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-07-17T22:23:16Z", "updated_at": "2018-08-09T00:42:13Z", "closed_at": "2018-08-09T00:42:13Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\n\r\nThere are some issues in the README files under project root and tests directory. \r\n\r\n1. It doesn't mention environment variables to use. Those are mentioned in tests/README.md. Should move them to the top of readme file.\r\n\"You will need to set the appropriate environment variables as described above to connect to the Azure Data Lake Store.\"\r\n\r\n2. The test folder link should be to organizational repo, and not https://github.com/ro-joowan/azure-data-lake-store-python/tree/master/tests\r\n\r\n3. Some of the environment variable names under tests/README.md are old and incorrect.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nNA\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.24\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: NA\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: NA\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: NA\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/220", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/220/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/220/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/220/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/220", "id": 332154188, "node_id": "MDU6SXNzdWUzMzIxNTQxODg=", "number": 220, "title": "msrest ImportError when importing azure.common.credentials", "user": {"login": "jmillxyz", "id": 531476, "node_id": "MDQ6VXNlcjUzMTQ3Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/531476?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmillxyz", "html_url": "https://github.com/jmillxyz", "followers_url": "https://api.github.com/users/jmillxyz/followers", "following_url": "https://api.github.com/users/jmillxyz/following{/other_user}", "gists_url": "https://api.github.com/users/jmillxyz/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmillxyz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmillxyz/subscriptions", "organizations_url": "https://api.github.com/users/jmillxyz/orgs", "repos_url": "https://api.github.com/users/jmillxyz/repos", "events_url": "https://api.github.com/users/jmillxyz/events{/privacy}", "received_events_url": "https://api.github.com/users/jmillxyz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-06-13T20:36:22Z", "updated_at": "2018-07-10T09:20:06Z", "closed_at": "2018-07-10T09:20:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nRegression in v0.0.22 -- we're importing it as a part of `azure==3.0.0`\r\n\r\n```\r\n    from azure.common import credentials as azcreds\r\n.tox/py27/lib/python2.7/site-packages/azure/common/credentials.py:58: in <module>\r\n    raise ImportError(\"You need to install 'msrest' to use this feature\")\r\nE   ImportError: You need to install 'msrest' to use this feature\r\n```\r\n\r\nThe same test passes in v0.0.19 (tried 0.0.20 and 0.0.21 but they're not released).\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n```\r\n$ pip install azure==3.0.0\r\n$ python\r\n> from azure.common import credentials\r\n```\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: `0.0.22`\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 2.7.12, 64-bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Ubuntu 14.04.05 LTS\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: bash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/219", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/219/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/219/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/219/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/219", "id": 331370910, "node_id": "MDU6SXNzdWUzMzEzNzA5MTA=", "number": 219, "title": "It does not seem that files are closed after writing", "user": {"login": "okhoma", "id": 212077, "node_id": "MDQ6VXNlcjIxMjA3Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/212077?v=4", "gravatar_id": "", "url": "https://api.github.com/users/okhoma", "html_url": "https://github.com/okhoma", "followers_url": "https://api.github.com/users/okhoma/followers", "following_url": "https://api.github.com/users/okhoma/following{/other_user}", "gists_url": "https://api.github.com/users/okhoma/gists{/gist_id}", "starred_url": "https://api.github.com/users/okhoma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/okhoma/subscriptions", "organizations_url": "https://api.github.com/users/okhoma/orgs", "repos_url": "https://api.github.com/users/okhoma/repos", "events_url": "https://api.github.com/users/okhoma/events{/privacy}", "received_events_url": "https://api.github.com/users/okhoma/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-11T22:20:18Z", "updated_at": "2018-06-22T07:25:46Z", "closed_at": "2018-06-22T07:25:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\n---\r\nThe write() method always flushes data using syncFlag='DATA', so when file is closed the flush(syncFlag='CLOSE') method does nothing. I propose that zero bytes are appended in this case to actually close the remote file.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/218", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/218/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/218/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/218/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/218", "id": 330918356, "node_id": "MDU6SXNzdWUzMzA5MTgzNTY=", "number": 218, "title": "In glob() details parameter is reverted", "user": {"login": "okhoma", "id": 212077, "node_id": "MDQ6VXNlcjIxMjA3Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/212077?v=4", "gravatar_id": "", "url": "https://api.github.com/users/okhoma", "html_url": "https://github.com/okhoma", "followers_url": "https://api.github.com/users/okhoma/followers", "following_url": "https://api.github.com/users/okhoma/following{/other_user}", "gists_url": "https://api.github.com/users/okhoma/gists{/gist_id}", "starred_url": "https://api.github.com/users/okhoma/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/okhoma/subscriptions", "organizations_url": "https://api.github.com/users/okhoma/orgs", "repos_url": "https://api.github.com/users/okhoma/repos", "events_url": "https://api.github.com/users/okhoma/events{/privacy}", "received_events_url": "https://api.github.com/users/okhoma/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-09T20:51:13Z", "updated_at": "2018-06-22T07:37:20Z", "closed_at": "2018-06-22T07:37:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nThere seems to be a typo in glob() function:\r\n\r\n```\r\ndef glob(self, path, details=False, invalidate_cache=True):\r\n        ...\r\n        return [f for f in allfiles if AzureDLPath(f['name'] if details else f).match(path_as_posix)]\r\n```\r\n\r\nNote that it says `f['name'] if details else f`. \r\nIt should, probably, be `f if details else f['name']`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/217", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/217/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/217/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/217/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/217", "id": 329761490, "node_id": "MDU6SXNzdWUzMjk3NjE0OTA=", "number": 217, "title": "Use proxy with azure.datalake.store.lib.auth()", "user": {"login": "kasuteru", "id": 16296708, "node_id": "MDQ6VXNlcjE2Mjk2NzA4", "avatar_url": "https://avatars2.githubusercontent.com/u/16296708?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kasuteru", "html_url": "https://github.com/kasuteru", "followers_url": "https://api.github.com/users/kasuteru/followers", "following_url": "https://api.github.com/users/kasuteru/following{/other_user}", "gists_url": "https://api.github.com/users/kasuteru/gists{/gist_id}", "starred_url": "https://api.github.com/users/kasuteru/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kasuteru/subscriptions", "organizations_url": "https://api.github.com/users/kasuteru/orgs", "repos_url": "https://api.github.com/users/kasuteru/repos", "events_url": "https://api.github.com/users/kasuteru/events{/privacy}", "received_events_url": "https://api.github.com/users/kasuteru/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-06T08:18:47Z", "updated_at": "2020-01-13T16:18:40Z", "closed_at": "2018-06-25T12:42:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to use this library behind a corporate proxy and there does not seem to be a (documented) way to configure the proxy to be used with azure.datalake.store.lib.auth()? \r\n\r\nHere is an example how it works with the requests library:\r\n\r\n```\r\nimport requests\r\n\r\nproxies = {\r\n    'http': 'http://user:pass@10.10.1.0:3128',\r\n    'https': 'http://user:pass@10.10.1.0:3128',\r\n}\r\n\r\n# Create the session and set the proxies.\r\ns = requests.Session()\r\ns.proxies = proxies\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/215", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/215/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/215/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/215/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/215", "id": 328859048, "node_id": "MDU6SXNzdWUzMjg4NTkwNDg=", "number": 215, "title": "'lpath' not correctly used when downloading", "user": {"login": "masangster", "id": 39884422, "node_id": "MDQ6VXNlcjM5ODg0NDIy", "avatar_url": "https://avatars3.githubusercontent.com/u/39884422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/masangster", "html_url": "https://github.com/masangster", "followers_url": "https://api.github.com/users/masangster/followers", "following_url": "https://api.github.com/users/masangster/following{/other_user}", "gists_url": "https://api.github.com/users/masangster/gists{/gist_id}", "starred_url": "https://api.github.com/users/masangster/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/masangster/subscriptions", "organizations_url": "https://api.github.com/users/masangster/orgs", "repos_url": "https://api.github.com/users/masangster/repos", "events_url": "https://api.github.com/users/masangster/events{/privacy}", "received_events_url": "https://api.github.com/users/masangster/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-06-03T20:24:03Z", "updated_at": "2018-07-17T07:34:03Z", "closed_at": "2018-07-16T10:30:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nThe 'lpath' argument passed to the downloader does not get used reliably when downloading. \r\n\r\n---\r\n\r\n### Reproduction Steps\r\nTry different forms of the 'lpath' argument when downloading a directory - sometimes the download location path is taken relative to where the python script that called it is located, sometimes it is taken relative to the local root eg. C:/, and the actual 'lpath' naming is not used reliably, i.e. if I am downloading /root/path/files to an 'lpath' of /testpath/test, I do not end up with testpath/test/files/... . Sometimes I get testpath/files/..., and sometimes the files/... download directly into the location of the python script that called the downloader. \r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n0.0.19\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\n3.6.2 64 bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nWindows 7 Enterprise 64-bit\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nPython via VS Code\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/214", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/214/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/214/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/214/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/214", "id": 328849337, "node_id": "MDU6SXNzdWUzMjg4NDkzMzc=", "number": 214, "title": "When downloading a directory that actually only has one file buried in it, the directory structure is not maintained", "user": {"login": "masangster", "id": 39884422, "node_id": "MDQ6VXNlcjM5ODg0NDIy", "avatar_url": "https://avatars3.githubusercontent.com/u/39884422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/masangster", "html_url": "https://github.com/masangster", "followers_url": "https://api.github.com/users/masangster/followers", "following_url": "https://api.github.com/users/masangster/following{/other_user}", "gists_url": "https://api.github.com/users/masangster/gists{/gist_id}", "starred_url": "https://api.github.com/users/masangster/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/masangster/subscriptions", "organizations_url": "https://api.github.com/users/masangster/orgs", "repos_url": "https://api.github.com/users/masangster/repos", "events_url": "https://api.github.com/users/masangster/events{/privacy}", "received_events_url": "https://api.github.com/users/masangster/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 415132854, "node_id": "MDU6TGFiZWw0MTUxMzI4NTQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/in%20progress", "name": "in progress", "color": "ededed", "default": false, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2018-06-03T18:09:39Z", "updated_at": "2018-08-15T20:37:50Z", "closed_at": "2018-08-15T20:37:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nUsing the ADLDownloader, when trying to download a directory with multiple subdirectories, if any of the lowest level folders have more than one file in them, the whole directory tree is downloaded as expected. However, if only one file actually exists in one of the lowest level subfolders, then the file is downloaded on its own as though it was only a single file specified for download, and not the whole directory. \r\n\r\nThis is problematic when trying to programmatically interface with the ADL, pull down files to a VM, use them for computations, and then re-upload them when finished, as the expected file structure is not there for referencing file paths etc.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n1. Connect to Azure Data lake Store using service to service application.\r\n2. Download a subset of the ADL directory structure which may have many nested levels of subfolders, but only one of the lowest level subfolders actually has a file in it. Do this using eg. \r\nADLDownloader(adlsFileSystemClient, rpath='RootFolder/Subfolder/Subfolder2', lpath='/localfolder', chunksize=1000000, buffersize=1000000, blocksize=1000000, client=None, run=True, overwrite=True, verbose=True)\r\n\r\nResult: Only the single file is downloaded, it doesn't come within the subset of the file directory requested.. \r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n0.0.19\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\n3.6.2 64 bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nWindows 7 Enterprise 64 bit\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nPython with VS Code\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/213", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/213/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/213/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/213/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/213", "id": 326313922, "node_id": "MDU6SXNzdWUzMjYzMTM5MjI=", "number": 213, "title": "On thread safety, asynchronous and deadlocks", "user": {"login": "npezolano", "id": 1518637, "node_id": "MDQ6VXNlcjE1MTg2Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1518637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/npezolano", "html_url": "https://github.com/npezolano", "followers_url": "https://api.github.com/users/npezolano/followers", "following_url": "https://api.github.com/users/npezolano/following{/other_user}", "gists_url": "https://api.github.com/users/npezolano/gists{/gist_id}", "starred_url": "https://api.github.com/users/npezolano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/npezolano/subscriptions", "organizations_url": "https://api.github.com/users/npezolano/orgs", "repos_url": "https://api.github.com/users/npezolano/repos", "events_url": "https://api.github.com/users/npezolano/events{/privacy}", "received_events_url": "https://api.github.com/users/npezolano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-24T22:08:33Z", "updated_at": "2018-07-05T12:56:02Z", "closed_at": "2018-07-05T12:56:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm experiencing deadlocks on a process that runs several multithreaded reads and writes.\r\n\r\nIs this library thread safe? \r\n\r\nIs the call below handled asynchronously ?\r\n```\r\n\r\nwith adl.open('anewfile', 'wb') as f:\r\n    f.write(b'important data')\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/210", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/210/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/210/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/210/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/210", "id": 306163174, "node_id": "MDU6SXNzdWUzMDYxNjMxNzQ=", "number": 210, "title": "python error importing data lake store: azure.mgmt.datalake.store  (Windows 8-64 bit, Python 3.6.3, Pip 9.0.2)", "user": {"login": "starhh", "id": 36446094, "node_id": "MDQ6VXNlcjM2NDQ2MDk0", "avatar_url": "https://avatars2.githubusercontent.com/u/36446094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/starhh", "html_url": "https://github.com/starhh", "followers_url": "https://api.github.com/users/starhh/followers", "following_url": "https://api.github.com/users/starhh/following{/other_user}", "gists_url": "https://api.github.com/users/starhh/gists{/gist_id}", "starred_url": "https://api.github.com/users/starhh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/starhh/subscriptions", "organizations_url": "https://api.github.com/users/starhh/orgs", "repos_url": "https://api.github.com/users/starhh/repos", "events_url": "https://api.github.com/users/starhh/events{/privacy}", "received_events_url": "https://api.github.com/users/starhh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-17T15:02:03Z", "updated_at": "2018-03-26T22:09:28Z", "closed_at": "2018-03-26T22:09:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n (Windows 8-64 bit, Python 3.6.3, Pip 9.0.2)\r\nFollowing Documentation to use azure data lake store using python (https://docs.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-manage-use-python-sdk), I was able to successfully install other libraries and the one below for ADLS:\r\npip install azure-datalake-store                                                                               #successfully install \r\npip install azure-mgmt-datalake-store                                                                    #successfully install \r\n\r\nBUT, when i import the library following as mentioned in the documentation, I encounter the Error\r\n(\r\n## Required for Azure Data Lake Store account management\r\nfrom azure.mgmt.datalake.store import DataLakeStoreAccountManagementClient     #Error Importing \r\nfrom azure.mgmt.datalake.store.models import DataLakeStoreAccount                       #Error Importing\r\n\r\n## Required for Azure Data Lake Store filesystem management\r\nfrom azure.datalake.store import core, lib, multithread                                                 #Error Importing\r\n)\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\nError Details:\r\nC:\\Windows\\system32>python\r\nPython 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 17:26:49) [MSC v.1900 32 bit (Intel)]\r\n on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\r\n>>> from azure.mgmt.datalake.store import DataLakeStoreAccountManagementClient\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'azure.mgmt.datalake.store'\r\n>>> from azure.mgmt.datalake.store.models import DataLakeStoreAccount\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'azure.mgmt.datalake.store'\r\n>>> from azure.datalake.store import core, lib, multithread\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'azure.datalake'\r\n>>>\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\nC:\\Windows\\system32>pip show azure-datalake-store\r\nName: azure-datalake-store\r\nVersion: 0.0.19\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: c:\\program files\\python36\\lib\\site-packages\r\nRequires: cffi, msrest, adal\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 32 bit\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windows 8, 64 bit\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: cmd.exe\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/208", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/208/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/208/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/208/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/208", "id": 304899746, "node_id": "MDU6SXNzdWUzMDQ4OTk3NDY=", "number": 208, "title": "Uploading a directory with single file was messing up with filename", "user": {"login": "milanchandna", "id": 1725750, "node_id": "MDQ6VXNlcjE3MjU3NTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1725750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/milanchandna", "html_url": "https://github.com/milanchandna", "followers_url": "https://api.github.com/users/milanchandna/followers", "following_url": "https://api.github.com/users/milanchandna/following{/other_user}", "gists_url": "https://api.github.com/users/milanchandna/gists{/gist_id}", "starred_url": "https://api.github.com/users/milanchandna/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/milanchandna/subscriptions", "organizations_url": "https://api.github.com/users/milanchandna/orgs", "repos_url": "https://api.github.com/users/milanchandna/repos", "events_url": "https://api.github.com/users/milanchandna/events{/privacy}", "received_events_url": "https://api.github.com/users/milanchandna/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-13T19:14:45Z", "updated_at": "2018-03-13T20:16:09Z", "closed_at": "2018-03-13T20:16:09Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "### Description\r\nUploading the files from directory with single file messes up the destination filename.\r\nIdeally destination filename should have similar filename as of source file and directory structure should be similar.\r\nBut rather destination filename is the name of source directory and source filename is not being used. \r\n\r\n---\r\n\r\n### Reproduction Steps\r\nSample Command: Command arguments ['dls', 'fs', 'upload', '--account', 'xxx', '--thread-count', '50', '--source-path', '/foo/bar', '--destination-path', '/foo/bar']\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n0.0.17\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/207", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/207/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/207/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/207/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/207", "id": 303073890, "node_id": "MDU6SXNzdWUzMDMwNzM4OTA=", "number": 207, "title": "Unable to apply acl loaded from other file", "user": {"login": "dgfmdrv", "id": 1451834, "node_id": "MDQ6VXNlcjE0NTE4MzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1451834?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dgfmdrv", "html_url": "https://github.com/dgfmdrv", "followers_url": "https://api.github.com/users/dgfmdrv/followers", "following_url": "https://api.github.com/users/dgfmdrv/following{/other_user}", "gists_url": "https://api.github.com/users/dgfmdrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/dgfmdrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dgfmdrv/subscriptions", "organizations_url": "https://api.github.com/users/dgfmdrv/orgs", "repos_url": "https://api.github.com/users/dgfmdrv/repos", "events_url": "https://api.github.com/users/dgfmdrv/events{/privacy}", "received_events_url": "https://api.github.com/users/dgfmdrv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-03-07T12:10:01Z", "updated_at": "2018-03-26T11:34:03Z", "closed_at": "2018-03-26T11:34:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nHello, I'm trying to copy a file from one datalake to other. I sucesfully copy a file using ADLDownloader and ADLUploader (temporary file on shm) \r\nIn this case I would like to preserve ACL from source datalake. I run xxx.get_acl_status(file) and get an acl dict:\r\n{'stickyBit': False, 'entries': ['user::rwx', 'group::rwx', 'group:0xxxxxx78a4751:--x', 'group:3e6xxxx94df8f9f50:r-x', 'group:bd3xxx97ecc3:rwx', 'mask::rwx', 'other::---'], 'group': 'f6bxxxxx9d2', 'permission': '770', 'owner': 'f6bb982bxxxxb985fd9d2'}\r\n\r\nIn HISTORY.rst I found this note:\r\n   * set_acl_entries: allows for \"patching\" an existing ACL on a file or folder\r\nBut I get:\r\n    AttributeError: 'AzureDLFileSystem' object has no attribute 'set_acl_entries'\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using?\r\nazure-common (1.1.8)\r\nazure-datalake-store (0.0.18)\r\nazure-mgmt-datalake-nspkg (2.0.0)\r\nazure-mgmt-datalake-store (0.4.0)\r\nazure-mgmt-nspkg (2.0.0)\r\nazure-mgmt-resource (1.2.2)\r\nazure-nspkg (2.0.0)\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nPython 3.5.2\r\n\r\n**OS Version:** What OS and version are you using?  \r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 16.04.3 LTS\r\nRelease:        16.04\r\nCodename:       xenial\r\n\r\n**Shell Type:** What shell are you using?\r\nbash\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/206", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/206/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/206/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/206/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/206", "id": 296866007, "node_id": "MDU6SXNzdWUyOTY4NjYwMDc=", "number": 206, "title": "pkg_resources.DistributionNotFound: The 'futures; python_version <= \"2.7\"' distribution was not found", "user": {"login": "ssvaddiparthy", "id": 15088530, "node_id": "MDQ6VXNlcjE1MDg4NTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/15088530?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ssvaddiparthy", "html_url": "https://github.com/ssvaddiparthy", "followers_url": "https://api.github.com/users/ssvaddiparthy/followers", "following_url": "https://api.github.com/users/ssvaddiparthy/following{/other_user}", "gists_url": "https://api.github.com/users/ssvaddiparthy/gists{/gist_id}", "starred_url": "https://api.github.com/users/ssvaddiparthy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ssvaddiparthy/subscriptions", "organizations_url": "https://api.github.com/users/ssvaddiparthy/orgs", "repos_url": "https://api.github.com/users/ssvaddiparthy/repos", "events_url": "https://api.github.com/users/ssvaddiparthy/events{/privacy}", "received_events_url": "https://api.github.com/users/ssvaddiparthy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-13T19:58:43Z", "updated_at": "2018-03-21T04:58:10Z", "closed_at": "2018-03-21T04:58:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nIncluding azure==2.0.0 as a dependency (in setup.py file install_requires section) fails for any library consuming azure due to the following error:\r\n\r\n`pkg_resources.DistributionNotFound: The 'futures; python_version <= \"2.7\"' distribution was not found and is required by azure-datalake-store\r\n`\r\nThis is a blocker as I am sure the world has moved from consuming any version of python less than 2.7.\r\nhttps://github.com/Azure/azure-data-lake-store-python/blob/master/setup.py#L55\r\nCan this be removed?\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n```\r\nName: azure-datalake-store\r\nVersion: 0.0.18\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: /Library/Python/2.7/site-packages\r\nRequires: futures, msrest, adal, cffi, pathlib2\r\n```\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n```\r\n$ python --version\r\nPython 2.7.10\r\n```\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n```\r\n$ uname -ar\r\nDarwin lab-mac-8a2dbf 17.4.0 Darwin Kernel Version 17.4.0: Sun Dec 17 09:19:54 PST 2017; root:xnu-4570.41.2~1/RELEASE_X86_64 x86_64\r\n```\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here: bash \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/204", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/204/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/204/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/204/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/204", "id": 295325534, "node_id": "MDU6SXNzdWUyOTUzMjU1MzQ=", "number": 204, "title": "Renaming and moving files not working", "user": {"login": "npezolano", "id": 1518637, "node_id": "MDQ6VXNlcjE1MTg2Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1518637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/npezolano", "html_url": "https://github.com/npezolano", "followers_url": "https://api.github.com/users/npezolano/followers", "following_url": "https://api.github.com/users/npezolano/following{/other_user}", "gists_url": "https://api.github.com/users/npezolano/gists{/gist_id}", "starred_url": "https://api.github.com/users/npezolano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/npezolano/subscriptions", "organizations_url": "https://api.github.com/users/npezolano/orgs", "repos_url": "https://api.github.com/users/npezolano/repos", "events_url": "https://api.github.com/users/npezolano/events{/privacy}", "received_events_url": "https://api.github.com/users/npezolano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-07T23:03:01Z", "updated_at": "2018-03-09T13:37:48Z", "closed_at": "2018-03-09T13:37:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "Both AzureDLFileSystem.mv and AzureDLFileSystem.rename throw the following error on linux:\r\n\r\n`DatalakeRESTException: Operation failed: RENAME, myfile.csv`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/201", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/201/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/201/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/201/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/201", "id": 282863314, "node_id": "MDU6SXNzdWUyODI4NjMzMTQ=", "number": 201, "title": "Certificate ", "user": {"login": "AMustapha", "id": 17221275, "node_id": "MDQ6VXNlcjE3MjIxMjc1", "avatar_url": "https://avatars0.githubusercontent.com/u/17221275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AMustapha", "html_url": "https://github.com/AMustapha", "followers_url": "https://api.github.com/users/AMustapha/followers", "following_url": "https://api.github.com/users/AMustapha/following{/other_user}", "gists_url": "https://api.github.com/users/AMustapha/gists{/gist_id}", "starred_url": "https://api.github.com/users/AMustapha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AMustapha/subscriptions", "organizations_url": "https://api.github.com/users/AMustapha/orgs", "repos_url": "https://api.github.com/users/AMustapha/repos", "events_url": "https://api.github.com/users/AMustapha/events{/privacy}", "received_events_url": "https://api.github.com/users/AMustapha/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-18T12:31:49Z", "updated_at": "2018-02-23T20:31:07Z", "closed_at": "2018-02-23T20:31:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nDoest the Current Version Support Authentification using Service principal and Certificate\r\n\r\nThank you", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/200", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/200/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/200/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/200/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/200", "id": 268146419, "node_id": "MDU6SXNzdWUyNjgxNDY0MTk=", "number": 200, "title": "Frequent Connection Reset errors with 0.0.17", "user": {"login": "argoneus", "id": 365334, "node_id": "MDQ6VXNlcjM2NTMzNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/365334?v=4", "gravatar_id": "", "url": "https://api.github.com/users/argoneus", "html_url": "https://github.com/argoneus", "followers_url": "https://api.github.com/users/argoneus/followers", "following_url": "https://api.github.com/users/argoneus/following{/other_user}", "gists_url": "https://api.github.com/users/argoneus/gists{/gist_id}", "starred_url": "https://api.github.com/users/argoneus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/argoneus/subscriptions", "organizations_url": "https://api.github.com/users/argoneus/orgs", "repos_url": "https://api.github.com/users/argoneus/repos", "events_url": "https://api.github.com/users/argoneus/events{/privacy}", "received_events_url": "https://api.github.com/users/argoneus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-10-24T18:53:34Z", "updated_at": "2018-04-12T12:47:11Z", "closed_at": "2018-04-12T12:47:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nWe recently upgraded to version 0.0.17 and have been experiencing a multitude of \"Connection Reset\" errors when interacting with ADLS.\r\n\r\nExample error:\r\n\r\n`DatalakeRESTException: HTTP error: ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')),)`\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n1. Authenticate to ADLS\r\n2. Get a list of files from a directory listing\r\n3. Iterate over file list\r\n4.  Open each file into a pandas dataframe, e.g.\r\n    `df = pd.read_csv(adl.open(file_))`\r\n\r\nThe errors are frequent but occur on separate files, i.e. it's not reproducible on an individual file basis.  We're seeing the error 60-70% of the time when reading files in a directory listing.\r\n\r\nAs a mitigation step, I downgraded azure-datalake-store to 0.0.16, and the issue improved dramatically.  Now seeing the Connection Reset error perhaps <5% of the time.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n0.0.17\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\nPython 3.6.1 :: Anaconda 4.4.0 (64-bit)\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\nUbuntu Linux 16.04\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\nJupyter notebook/bash on Ubuntu Linux", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/198", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/198/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/198/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/198/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/198", "id": 261358314, "node_id": "MDU6SXNzdWUyNjEzNTgzMTQ=", "number": 198, "title": "Unsuitable behaviour for reading parts of files", "user": {"login": "xhochy", "id": 70274, "node_id": "MDQ6VXNlcjcwMjc0", "avatar_url": "https://avatars2.githubusercontent.com/u/70274?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xhochy", "html_url": "https://github.com/xhochy", "followers_url": "https://api.github.com/users/xhochy/followers", "following_url": "https://api.github.com/users/xhochy/following{/other_user}", "gists_url": "https://api.github.com/users/xhochy/gists{/gist_id}", "starred_url": "https://api.github.com/users/xhochy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xhochy/subscriptions", "organizations_url": "https://api.github.com/users/xhochy/orgs", "repos_url": "https://api.github.com/users/xhochy/repos", "events_url": "https://api.github.com/users/xhochy/events{/privacy}", "received_events_url": "https://api.github.com/users/xhochy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-28T15:18:35Z", "updated_at": "2018-02-17T21:44:10Z", "closed_at": "2018-02-17T21:44:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n\r\nWhen using the package to access the Azure Data Lake Store to read Apache Parquet files with `pyarrow`, it can be seen that we retrieve significantly more data than needed on selective reads. Using columnar file formats like Apache Parquet are essentials for analytic access as the engine reading these files can already push parts of the query down into the storage layer. Data that would be pruned by the engine is not even retrieved from the storage.\r\n\r\nAs an example, we read for an example 50MB Parquet file the *last* 65536 bytes to load the metadata. This is then cached in RAM in an `AzureDLFile`  object. As a follow-up to this, `pyarrow` now requests the relevant columns, starting with the first in the file. These columns are in roughly 8MB chunks which would be suitable to be loaded in independent requests. But instead of only loading the relevant data, the code in https://github.com/Azure/azure-data-lake-store-python/blob/master/azure/datalake/store/core.py#L768-L773 will load the data from the beginning of the first relevant column until the start of the previous request. In most cases, this request spans 95% of the file and so subsequent requests are then done as the data already resides in memory.\r\n\r\nFor columnar reads, this situation is unfortunate as we have transferred nearly all data but for the successful read of the file, we may only need 10% of the file. In the usual case, such selective reads are not wanted as they increase the number of requests heavily but for formats like Parquet they are essential to take full benefit of the format. For comparison, the S3 implementation in the newest Hadoop version let's the user select different input policies: https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#S3A_Experimental_fadvise_input_policy_support In most of the cases, one would chose `sequential` here but for analytics reads/queries, `random` gives a much better performance.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n\r\n```\r\nfrom azure.datalake.store import lib\r\nfrom azure.datalake.store.core import AzureDLFileSystem\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\n\r\n# Add secrets and store_name \u2026\r\ntoken = lib.auth(tenant_id, client_id=client_id, client_secret=client_secret)\r\nadl = AzureDLFileSystem(token, store_name=store_name)\r\n\r\nsize = 1000000\r\n\r\ndf = pd.DataFrame({\"col1\": np.random.randint(0, 100, size=size),\r\n                   \"col2\": np.random.randint(0, 100, size=size),\r\n                   \"col3\": np.random.randn(size),\r\n                   \"col4\": np.random.randn(size),\r\n                   \"col5\": np.random.randn(size),\r\n                   \"col6\": np.random.randn(size),\r\n                   \"col7\": np.random.randn(size),\r\n                   \"col8\": np.random.randn(size),\r\n                   \"col9\": np.random.randn(size),\r\n                   \"col10\": np.random.randn(size)})\r\n\r\ntable = pa.Table.from_pandas(df)\r\nfilename = 'test.parquet'\r\nwith adl.open(filename, 'wb') as f:\r\n    pq.write_table(table, f)\r\n\r\n# This should only read a small selection of the file, currently it reads about 90% \r\nwith adl.open(filename, 'rb') as f:\r\n    table2 = pq.read_table(f, columns=['col2', 'col7'])\r\n```\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** 0.0.17\r\n\r\n**Python Version:** 64bit, Python 3.6\r\n\r\n**OS Version:** macOS Sierra\r\n\r\n**Shell Type:** zsh\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/194", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/194/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/194/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/194/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/194", "id": 258578286, "node_id": "MDU6SXNzdWUyNTg1NzgyODY=", "number": 194, "title": "Formatting is broken on PyPi landing page", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-18T18:34:21Z", "updated_at": "2018-02-20T07:45:23Z", "closed_at": "2018-02-20T07:45:22Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\nThe formatting of the README.rst and HISTORY.rst is broken, resulting in a plain text landing page on PyPi (see here: https://pypi.python.org/pypi/azure-datalake-store/0.0.16). You can see the difference by looking at this previous version of the package: https://pypi.python.org/pypi/azure-datalake-store/0.0.14\r\n\r\nBoth version 0.0.16 and 0.0.15 have this broken formatting.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/190", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/190/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/190/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/190/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/190", "id": 254112987, "node_id": "MDU6SXNzdWUyNTQxMTI5ODc=", "number": 190, "title": "azure/datalake/store/transfer.py missing _submit in master", "user": {"login": "spula", "id": 17090271, "node_id": "MDQ6VXNlcjE3MDkwMjcx", "avatar_url": "https://avatars2.githubusercontent.com/u/17090271?v=4", "gravatar_id": "", "url": "https://api.github.com/users/spula", "html_url": "https://github.com/spula", "followers_url": "https://api.github.com/users/spula/followers", "following_url": "https://api.github.com/users/spula/following{/other_user}", "gists_url": "https://api.github.com/users/spula/gists{/gist_id}", "starred_url": "https://api.github.com/users/spula/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/spula/subscriptions", "organizations_url": "https://api.github.com/users/spula/orgs", "repos_url": "https://api.github.com/users/spula/repos", "events_url": "https://api.github.com/users/spula/events{/privacy}", "received_events_url": "https://api.github.com/users/spula/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-30T19:42:05Z", "updated_at": "2017-09-20T21:18:10Z", "closed_at": "2017-09-20T21:18:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n  File \"C:\\Python\\lib\\concurrent\\futures\\_base.py\", line 297, in _invoke_callbac\r\nks\r\n    callback(self)\r\n  File \"C:\\Python\\lib\\site-packages\\azure\\datalake\\store\\transfer.py\", line 434,\r\n in _update\r\n    merge_future = self._submit(\r\nAttributeError: 'ADLTransferClient' object has no attribute '_submit'\r\nend upload\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nmultithread.ADLUploader(adlsFileSystemClient, lpath=inputpath, rpath=destpath, nthreads=64, overwrite=True)\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n$ pip show azure-datalake-store\r\nName: azure-datalake-store\r\nVersion: 0.0.15\r\nSummary: Azure Data Lake Store Filesystem Client Library for Python\r\nHome-page: https://github.com/Azure/azure-data-lake-store-python\r\nAuthor: Microsoft Corporation\r\nAuthor-email: ptvshelp@microsoft.com\r\nLicense: MIT License\r\nLocation: c:\\python\\lib\\site-packages\r\nRequires: adal, cffi, msrest\r\n\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 64\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:windows server 2012R2\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:Windows\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/188", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/188/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/188/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/188/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/188", "id": 249205286, "node_id": "MDU6SXNzdWUyNDkyMDUyODY=", "number": 188, "title": "Refactoring: remove transfer reference to parent ", "user": {"login": "clehene", "id": 101597, "node_id": "MDQ6VXNlcjEwMTU5Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/101597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clehene", "html_url": "https://github.com/clehene", "followers_url": "https://api.github.com/users/clehene/followers", "following_url": "https://api.github.com/users/clehene/following{/other_user}", "gists_url": "https://api.github.com/users/clehene/gists{/gist_id}", "starred_url": "https://api.github.com/users/clehene/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clehene/subscriptions", "organizations_url": "https://api.github.com/users/clehene/orgs", "repos_url": "https://api.github.com/users/clehene/repos", "events_url": "https://api.github.com/users/clehene/events{/privacy}", "received_events_url": "https://api.github.com/users/clehene/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "akharit", "id": 38331238, "node_id": "MDQ6VXNlcjM4MzMxMjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/38331238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akharit", "html_url": "https://github.com/akharit", "followers_url": "https://api.github.com/users/akharit/followers", "following_url": "https://api.github.com/users/akharit/following{/other_user}", "gists_url": "https://api.github.com/users/akharit/gists{/gist_id}", "starred_url": "https://api.github.com/users/akharit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akharit/subscriptions", "organizations_url": "https://api.github.com/users/akharit/orgs", "repos_url": "https://api.github.com/users/akharit/repos", "events_url": "https://api.github.com/users/akharit/events{/privacy}", "received_events_url": "https://api.github.com/users/akharit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-08-10T01:13:09Z", "updated_at": "2020-01-14T20:46:44Z", "closed_at": "2020-01-14T20:46:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\nIt's an unnecessary coupling used to access an otherwise private field.\r\nInstead `overwrite` should be a transfer parameter and `save()` should be passed as a reference like `merge` and `transfer`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/187", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/187/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/187/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/187/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/187", "id": 249196211, "node_id": "MDU6SXNzdWUyNDkxOTYyMTE=", "number": 187, "title": "Regression: Multi chunk transfer hangs as merging chunks fails", "user": {"login": "clehene", "id": 101597, "node_id": "MDQ6VXNlcjEwMTU5Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/101597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clehene", "html_url": "https://github.com/clehene", "followers_url": "https://api.github.com/users/clehene/followers", "following_url": "https://api.github.com/users/clehene/following{/other_user}", "gists_url": "https://api.github.com/users/clehene/gists{/gist_id}", "starred_url": "https://api.github.com/users/clehene/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clehene/subscriptions", "organizations_url": "https://api.github.com/users/clehene/orgs", "repos_url": "https://api.github.com/users/clehene/repos", "events_url": "https://api.github.com/users/clehene/events{/privacy}", "received_events_url": "https://api.github.com/users/clehene/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-10T00:03:43Z", "updated_at": "2017-09-08T00:33:49Z", "closed_at": "2017-09-08T00:33:49Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\n\r\nThis is caused by https://github.com/Azure/azure-data-lake-store-python/commit/4b3171d87fe0efeadcd688a561b67144d229c288 which removed `_submit` to achieve better control of done callback behavior and avoid a race condition. \r\n\r\nHowever `_submit` was called when merging chunks and now fails.\r\n\r\nThe behavior is that the transfer will hang after the error. I'm not clear why it hangs instead of actually bubbling the error up (that may be due to a separate issue).\r\n\r\nI'll submit a patch once I finish testing. Note that this affects latest release of the CLI.\r\n```\r\nazure.datalake.store.transfer : Chunks transferred\r\nazure.datalake.store.transfer : Merging file: transferring\r\nconcurrent.futures : exception calling callback for <Future at 0x10faa1610 state=finished returned tuple>\r\nTraceback (most recent call last):\r\n  File \"/Users/clehene/work/azure-cli/env/lib/python2.7/site-packages/concurrent/futures/_base.py\", line 301, in _invoke_callbacks\r\n    callback(self)\r\n  File \"/Users/clehene/work/azure-cli/env/lib/python2.7/site-packages/azure/datalake/store/transfer.py\", line 434, in _update\r\n    merge_future = self._submit(\r\nAttributeError: 'ADLTransferClient' object has no attribute '_submit'\r\n```\r\n\r\nInterrupting the process after the exception will \"wake it\":\r\n\r\n```\r\n^Cazure.datalake.store.transfer : <azure.datalake.store.transfer.ADLTransferClient object at 0x104fba290> suspended and persisted\r\nazure.datalake.store.transfer : Shutting down worker threads\r\nazure.datalake.store.transfer : Shutdown complete\r\nApplication event 'Application.TransformResults' with event data {'event_data': {'result': None}}\r\nApplication event 'Application.FilterResults' with event data {'event_data': {'result': None}}\r\n```\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n* Upload a file and make sure the chunk size is smaller than the file size \r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n```\r\nName: azure-datalake-store\r\nVersion: 0.0.15\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/185", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/185/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/185/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/185/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/185", "id": 247019533, "node_id": "MDU6SXNzdWUyNDcwMTk1MzM=", "number": 185, "title": "Token request returned http error 400", "user": {"login": "Mikeprod", "id": 9095880, "node_id": "MDQ6VXNlcjkwOTU4ODA=", "avatar_url": "https://avatars3.githubusercontent.com/u/9095880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mikeprod", "html_url": "https://github.com/Mikeprod", "followers_url": "https://api.github.com/users/Mikeprod/followers", "following_url": "https://api.github.com/users/Mikeprod/following{/other_user}", "gists_url": "https://api.github.com/users/Mikeprod/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mikeprod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mikeprod/subscriptions", "organizations_url": "https://api.github.com/users/Mikeprod/orgs", "repos_url": "https://api.github.com/users/Mikeprod/repos", "events_url": "https://api.github.com/users/Mikeprod/events{/privacy}", "received_events_url": "https://api.github.com/users/Mikeprod/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-01T10:08:46Z", "updated_at": "2018-04-14T15:21:22Z", "closed_at": "2018-04-14T15:21:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\nI cannot get a token since yesterday evening (CEST time) and therefore cannot access files stored in any datalake store.\r\n---\r\n\r\n### Reproduction Steps\r\nWith only the following code : \r\n```\r\nusername = 'myusername'\r\npassword = 'mypassword'\r\ntenant_id = 'mytenant'\r\nfrom azure.datalake.store import core, lib, multithread\r\ntoken = lib.auth(tenant_id, username, password)\r\n```\r\nI get the following error:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"c:\\users\\username\\downloads\\azure-data-lake-store-python\\azure\\datalake\\store\\lib.py\", line 130, in auth\r\n    password, client_id)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\authentication_context.py\", line 145, in acquire_token_with_username_password\r\n    return self._acquire_token(token_func)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\authentication_context.py\", line 109, in _acquire_token\r\n    return token_func(self)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\authentication_context.py\", line 143, in token_func\r\n    return token_request.get_token_with_username_password(username, password)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\token_request.py\", line 286, in get_token_with_username_password\r\n    token = self._get_token_username_password_federated(username, password)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\token_request.py\", line 252, in _get_token_username_password_federated\r\n    username, password)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\token_request.py\", line 212, in _perform_username_password_for_access_token_exchange\r\n    return self._perform_wstrust_assertion_oauth_exchange(wstrust_response)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\token_request.py\", line 192, in _perform_wstrust_assertion_oauth_exchange\r\n    return self._oauth_get_token(oauth_parameters)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\token_request.py\", line 113, in _oauth_get_token\r\n    return client.get_token(oauth_parameters)\r\n  File \"C:\\Users\\username\\Anaconda3\\lib\\site-packages\\adal\\oauth2_client.py\", line 281, in get_token\r\n    raise AdalError(return_error_string, error_response)\r\nadal.adal_error.AdalError: Get Token request returned http error: 400 and server response: {\"error\":\"invalid_grant\",\"error_description\":\"AADSTS70002: Error validating credentials. AADSTS50008: SAML token is invalid. AADSTS50006: The element with ID '_eeff9e30-4011-4389-bdeb-b787a7a3f896' was either unsigned or the signature was invalid.\\r\\nTrace ID: e8490f62-652e-4792-97b5-d2eb85bd1d00\\r\\nCorrelation ID: 5e7a1e38-db45-4bfb-880b-74d0859cba31\\r\\nTimestamp: 2017-08-01 09:56:34Z\",\"error_codes\":[70002,50008,50006],\"timestamp\":\"2017-08-01 09:56:34Z\",\"trace_id\":\"e8490f62-652e-4792-97b5-d2eb85bd1d00\",\"correlation_id\":\"5e7a1e38-db45-4bfb-880b-74d0859cba31\"}\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here: 0.0.14 and same with 0.0.15\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here: 64 bit python 3.6.1\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here: Windowns 10\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/182", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/182/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/182/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/182/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/182", "id": 245874777, "node_id": "MDU6SXNzdWUyNDU4NzQ3Nzc=", "number": 182, "title": "File state incorrectly marked as \"errored\" if contains chunks is \"pending\" state", "user": {"login": "clehene", "id": 101597, "node_id": "MDQ6VXNlcjEwMTU5Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/101597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clehene", "html_url": "https://github.com/clehene", "followers_url": "https://api.github.com/users/clehene/followers", "following_url": "https://api.github.com/users/clehene/following{/other_user}", "gists_url": "https://api.github.com/users/clehene/gists{/gist_id}", "starred_url": "https://api.github.com/users/clehene/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clehene/subscriptions", "organizations_url": "https://api.github.com/users/clehene/orgs", "repos_url": "https://api.github.com/users/clehene/repos", "events_url": "https://api.github.com/users/clehene/events{/privacy}", "received_events_url": "https://api.github.com/users/clehene/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-26T22:36:00Z", "updated_at": "2017-07-28T21:39:14Z", "closed_at": "2017-07-28T21:39:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\n\r\n\r\nin `_update` \r\n\r\n```\r\nif cstates.contains_all('finished'):\r\n...\r\nelif cstates.contains_none('running'):              \r\n    self._fstates[parent] = 'errored'\r\n```\r\nIf chunks are in `pending` state instead, this will mark the parent state as \"errored\" which will subsequently cause `active` ( `return not self._fstates.contains_none('pending', 'transferring', 'merging')`) to return `False` and `monitor` to continue with chunks in `pending` / `running` states.\r\n\r\nIf by the time `run` checks the chunks of a file the chunk has finished, this will continue, if chunks are in `running` this will error.\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nNot a deterministic way, as it depends on parallel task execution, but either having many chunks or repeatedly will cause this (i.e. we need to 1) have chunks in pending state **and** 2) have some in running state when we check them)\r\n\r\n```\r\ndef test_dummy(azure):\r\n    def transfer(adlfs, src, dst, offset, size, blocksize, buffersize, shutdown_event=None):\r\n        return size, None\r\n\r\n    client = ADLTransferClient(azure, transfer=transfer, chunksize=8,\r\n                               chunked=True)\r\n\r\n    client.submit('foo', AzureDLPath('bar'), 32*32)\r\n    client.run()\r\n```\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/180", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/180/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/180/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/180/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/180", "id": 245485099, "node_id": "MDU6SXNzdWUyNDU0ODUwOTk=", "number": 180, "title": "Readme issues", "user": {"login": "lmazuel", "id": 1050156, "node_id": "MDQ6VXNlcjEwNTAxNTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1050156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmazuel", "html_url": "https://github.com/lmazuel", "followers_url": "https://api.github.com/users/lmazuel/followers", "following_url": "https://api.github.com/users/lmazuel/following{/other_user}", "gists_url": "https://api.github.com/users/lmazuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmazuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmazuel/subscriptions", "organizations_url": "https://api.github.com/users/lmazuel/orgs", "repos_url": "https://api.github.com/users/lmazuel/repos", "events_url": "https://api.github.com/users/lmazuel/events{/privacy}", "received_events_url": "https://api.github.com/users/lmazuel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-25T18:02:32Z", "updated_at": "2018-07-16T20:48:33Z", "closed_at": "2018-07-16T20:48:33Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Every OSS project should have in the README (and are scored on this):\r\n\r\n- [ ] How to start tests\r\n- [ ] How to contribute text. Like:\r\nhttps://github.com/Azure/autorest/#code-of-conduct\r\n\r\nNon exhaustive list, but required by current scorecard.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/177", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/177/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/177/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/177/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/177", "id": 244222445, "node_id": "MDU6SXNzdWUyNDQyMjI0NDU=", "number": 177, "title": "Race condition due to `transfer` future `done_callback` that depends on state that may set after it's execution", "user": {"login": "clehene", "id": 101597, "node_id": "MDQ6VXNlcjEwMTU5Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/101597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clehene", "html_url": "https://github.com/clehene", "followers_url": "https://api.github.com/users/clehene/followers", "following_url": "https://api.github.com/users/clehene/following{/other_user}", "gists_url": "https://api.github.com/users/clehene/gists{/gist_id}", "starred_url": "https://api.github.com/users/clehene/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clehene/subscriptions", "organizations_url": "https://api.github.com/users/clehene/orgs", "repos_url": "https://api.github.com/users/clehene/repos", "events_url": "https://api.github.com/users/clehene/events{/privacy}", "received_events_url": "https://api.github.com/users/clehene/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-20T01:49:08Z", "updated_at": "2017-07-24T17:57:09Z", "closed_at": "2017-07-24T17:57:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\nI noticed this while trying to figure out why `client.run()` hangs in tests if the transfer function doesn't sleep a bit.\r\n\r\n#### Background\r\nUpon calling `submit` on a `ThreadPoolExecutor` a future is returned, but the task can be immediately executed (i.e. there's no `start()` on the pool).\r\nA done callback can be a added on the future, but it depends on actually submitting the future.\r\n`add_done_callback` has internal logic that will execute the callback immediately if the future has already finished (https://github.com/python/cpython/blob/master/Lib/concurrent/futures/_base.py#L376)\r\n\r\n#### Actual problem\r\nThe problem is that the callback for transfer (`_update`) is dependent on state that is added only after it executes.\r\nMainly it checks for the future reference in `cfutures`\r\n\r\n    if future in self._cfutures:\r\n    ...\r\n\r\nHowever the future is added to `cfutures` after submit and callback\r\n\r\n```\r\n    def _start(self, src, dst):\r\n            ...\r\n            future = self._submit(\r\n                self._transfer, self._adlfs, src, name, offset,\r\n                self._chunks[obj]['expected'], self._buffersize,\r\n                self._blocksize)\r\n            # THE CODE ABOVE DEPENDS ON THE FOLLOWING STATEMENT\r\n            self._cfutures[future] = obj\r\n\r\n...\r\n    def _submit(self, fn, *args, **kwargs):\r\n        kwargs['shutdown_event'] = self._shutdown_event\r\n        future = self._pool.submit(fn, *args, **kwargs)        \r\n        future.add_done_callback(self._update)\r\n        return future\r\n```\r\n\r\nLater, `monitor` will wait until `self.active` returns `False`, which never happens since the condition `not self._fstates.contains_none('pending', 'transferring', 'merging')` is dependent on the branch that never gets executed in `_update`\r\n\r\n---\r\n\r\n### Reproduction Steps\r\nRemove the `sleep` in one of the tests that calls `client.run()` (e.g. https://github.com/Azure/azure-data-lake-store-python/blob/master/tests/test_transfer.py#L36) and see how the test hangs.\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\n`0.0.14`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/176", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/176/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/176/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/176/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/176", "id": 244125380, "node_id": "MDU6SXNzdWUyNDQxMjUzODA=", "number": 176, "title": "n", "user": {"login": "adambain-vokal", "id": 8353248, "node_id": "MDQ6VXNlcjgzNTMyNDg=", "avatar_url": "https://avatars3.githubusercontent.com/u/8353248?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adambain-vokal", "html_url": "https://github.com/adambain-vokal", "followers_url": "https://api.github.com/users/adambain-vokal/followers", "following_url": "https://api.github.com/users/adambain-vokal/following{/other_user}", "gists_url": "https://api.github.com/users/adambain-vokal/gists{/gist_id}", "starred_url": "https://api.github.com/users/adambain-vokal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adambain-vokal/subscriptions", "organizations_url": "https://api.github.com/users/adambain-vokal/orgs", "repos_url": "https://api.github.com/users/adambain-vokal/repos", "events_url": "https://api.github.com/users/adambain-vokal/events{/privacy}", "received_events_url": "https://api.github.com/users/adambain-vokal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-07-19T18:03:35Z", "updated_at": "2017-07-19T18:04:13Z", "closed_at": "2017-07-19T18:04:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Description\r\n**Outline the issue here:**\r\n\r\n---\r\n\r\n### Reproduction Steps\r\n** Enumerate the steps to reproduce the issue here:**\r\n\r\n### Environment summary\r\n\r\n**SDK Version:** What version of the SDK  are you using? (pip show azure-datalake-store)\r\nAnswer here:\r\n\r\n**Python Version:** What Python version are you using? Is it 64-bit or 32-bit?  \r\nAnswer here:\r\n\r\n**OS Version:** What OS and version are you using?  \r\nAnswer here:\r\n\r\n**Shell Type:** What shell are you using? (e.g. bash, cmd.exe, Bash on Windows)  \r\nAnswer here:\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/174", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/174/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/174/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/174/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/174", "id": 242758442, "node_id": "MDU6SXNzdWUyNDI3NTg0NDI=", "number": 174, "title": "Enable Data Lake Store progress controller callback", "user": {"login": "clehene", "id": 101597, "node_id": "MDQ6VXNlcjEwMTU5Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/101597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/clehene", "html_url": "https://github.com/clehene", "followers_url": "https://api.github.com/users/clehene/followers", "following_url": "https://api.github.com/users/clehene/following{/other_user}", "gists_url": "https://api.github.com/users/clehene/gists{/gist_id}", "starred_url": "https://api.github.com/users/clehene/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/clehene/subscriptions", "organizations_url": "https://api.github.com/users/clehene/orgs", "repos_url": "https://api.github.com/users/clehene/repos", "events_url": "https://api.github.com/users/clehene/events{/privacy}", "received_events_url": "https://api.github.com/users/clehene/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106224, "node_id": "MDU6TGFiZWw0MDUxMDYyMjQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "matt1883", "id": 11821839, "node_id": "MDQ6VXNlcjExODIxODM5", "avatar_url": "https://avatars3.githubusercontent.com/u/11821839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matt1883", "html_url": "https://github.com/matt1883", "followers_url": "https://api.github.com/users/matt1883/followers", "following_url": "https://api.github.com/users/matt1883/following{/other_user}", "gists_url": "https://api.github.com/users/matt1883/gists{/gist_id}", "starred_url": "https://api.github.com/users/matt1883/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matt1883/subscriptions", "organizations_url": "https://api.github.com/users/matt1883/orgs", "repos_url": "https://api.github.com/users/matt1883/repos", "events_url": "https://api.github.com/users/matt1883/events{/privacy}", "received_events_url": "https://api.github.com/users/matt1883/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "matt1883", "id": 11821839, "node_id": "MDQ6VXNlcjExODIxODM5", "avatar_url": "https://avatars3.githubusercontent.com/u/11821839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matt1883", "html_url": "https://github.com/matt1883", "followers_url": "https://api.github.com/users/matt1883/followers", "following_url": "https://api.github.com/users/matt1883/following{/other_user}", "gists_url": "https://api.github.com/users/matt1883/gists{/gist_id}", "starred_url": "https://api.github.com/users/matt1883/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matt1883/subscriptions", "organizations_url": "https://api.github.com/users/matt1883/orgs", "repos_url": "https://api.github.com/users/matt1883/repos", "events_url": "https://api.github.com/users/matt1883/events{/privacy}", "received_events_url": "https://api.github.com/users/matt1883/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-07-13T16:25:40Z", "updated_at": "2017-07-24T17:44:04Z", "closed_at": "2017-07-24T17:44:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "### Description\r\nThis should report progress on transferred chunks (similar to blob store transfer)\r\n```\r\nAlive[#########################                                       ]  40.0881%\r\nFinished[#############################################################]  100.0000%\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/170", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/170/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/170/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/170/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/170", "id": 240360737, "node_id": "MDU6SXNzdWUyNDAzNjA3Mzc=", "number": 170, "title": "Could not find cli.py in the library", "user": {"login": "jerrysoft-x", "id": 25337098, "node_id": "MDQ6VXNlcjI1MzM3MDk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25337098?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jerrysoft-x", "html_url": "https://github.com/jerrysoft-x", "followers_url": "https://api.github.com/users/jerrysoft-x/followers", "following_url": "https://api.github.com/users/jerrysoft-x/following{/other_user}", "gists_url": "https://api.github.com/users/jerrysoft-x/gists{/gist_id}", "starred_url": "https://api.github.com/users/jerrysoft-x/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jerrysoft-x/subscriptions", "organizations_url": "https://api.github.com/users/jerrysoft-x/orgs", "repos_url": "https://api.github.com/users/jerrysoft-x/repos", "events_url": "https://api.github.com/users/jerrysoft-x/events{/privacy}", "received_events_url": "https://api.github.com/users/jerrysoft-x/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-04T09:16:22Z", "updated_at": "2017-07-06T21:01:35Z", "closed_at": "2017-07-06T21:01:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm reading the document of azure data lake store in http://azure-datalake-store.readthedocs.io/en/latest/, in the chapter \"Command Line Usage\", the code sample is running \"python cli.py ls -l\".\r\n\r\nHowever, I haven't found cli.py in azure.datalake.store library, so I wonder if it is not available anymore?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/169", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/169/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/169/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/169/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/169", "id": 240113224, "node_id": "MDU6SXNzdWUyNDAxMTMyMjQ=", "number": 169, "title": "'upload' should not remove common prefix when uploading folders", "user": {"login": "nickraptis", "id": 1311679, "node_id": "MDQ6VXNlcjEzMTE2Nzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1311679?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nickraptis", "html_url": "https://github.com/nickraptis", "followers_url": "https://api.github.com/users/nickraptis/followers", "following_url": "https://api.github.com/users/nickraptis/following{/other_user}", "gists_url": "https://api.github.com/users/nickraptis/gists{/gist_id}", "starred_url": "https://api.github.com/users/nickraptis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nickraptis/subscriptions", "organizations_url": "https://api.github.com/users/nickraptis/orgs", "repos_url": "https://api.github.com/users/nickraptis/repos", "events_url": "https://api.github.com/users/nickraptis/events{/privacy}", "received_events_url": "https://api.github.com/users/nickraptis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106220, "node_id": "MDU6TGFiZWw0MDUxMDYyMjA=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-07-03T09:18:24Z", "updated_at": "2017-07-11T20:48:44Z", "closed_at": "2017-07-11T20:48:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm trying to upload a tree of data using azure cli.\r\nThe folder structure looks something like \"json/$project/$year/$month/$day/part_*.json\"\r\nI only have a couple of days data so, project,year,month are constant.\r\n\r\nThe command I issue is\r\naz dls fs upload --source-path ./json --destination-path /json\r\n\r\nMy expectation was that the whole tree structure under json would be copied to DLS, as any other command like cp or rsync would do.\r\n\r\nInstead, the $project/$year/$month part of the tree was omitted and I got a folder structure of  \"json/$day/part_*.json\"\r\n\r\nIt seems that the code computes a common prefix of all the files to be copied, and omits it.\r\nFor folder copying, this breaks expectations and should not be the case.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/168", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/168/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/168/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/168/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/168", "id": 240086564, "node_id": "MDU6SXNzdWUyNDAwODY1NjQ=", "number": 168, "title": "AccessControlException when accessing the file uploaded by Python script from portal", "user": {"login": "jerrysoft-x", "id": 25337098, "node_id": "MDQ6VXNlcjI1MzM3MDk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25337098?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jerrysoft-x", "html_url": "https://github.com/jerrysoft-x", "followers_url": "https://api.github.com/users/jerrysoft-x/followers", "following_url": "https://api.github.com/users/jerrysoft-x/following{/other_user}", "gists_url": "https://api.github.com/users/jerrysoft-x/gists{/gist_id}", "starred_url": "https://api.github.com/users/jerrysoft-x/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jerrysoft-x/subscriptions", "organizations_url": "https://api.github.com/users/jerrysoft-x/orgs", "repos_url": "https://api.github.com/users/jerrysoft-x/repos", "events_url": "https://api.github.com/users/jerrysoft-x/events{/privacy}", "received_events_url": "https://api.github.com/users/jerrysoft-x/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-03T07:19:59Z", "updated_at": "2017-07-06T18:24:41Z", "closed_at": "2017-07-06T18:24:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "I uploaded a test file to Azure Data Lake Store through Python script with login as an Web App account, however I'm the owner of this ADL and I confirmed I have all the access to this uploaded file in the file properties, when I try to see the content of the uploaded file in File Preview in Azure Portal, I got #AccessControlException telling me I'm not authorized. I don't understand that, to me this is a obvious bug, isn't it?\r\n\r\nI have the same problem in portal when opening a folder created by python script, got the same AccessControlException.\r\n![20170703141311](https://user-images.githubusercontent.com/25337098/27781906-c8e55fba-6002-11e7-95e3-7a58b2630900.png)\r\n![20170703141328](https://user-images.githubusercontent.com/25337098/27781907-c8e60a96-6002-11e7-9c34-ccd4b043bfcf.png)\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/165", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/165/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/165/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/165/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/165", "id": 236282085, "node_id": "MDU6SXNzdWUyMzYyODIwODU=", "number": 165, "title": "Automatically refresh service principal access token", "user": {"login": "matt1883", "id": 11821839, "node_id": "MDQ6VXNlcjExODIxODM5", "avatar_url": "https://avatars3.githubusercontent.com/u/11821839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matt1883", "html_url": "https://github.com/matt1883", "followers_url": "https://api.github.com/users/matt1883/followers", "following_url": "https://api.github.com/users/matt1883/following{/other_user}", "gists_url": "https://api.github.com/users/matt1883/gists{/gist_id}", "starred_url": "https://api.github.com/users/matt1883/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matt1883/subscriptions", "organizations_url": "https://api.github.com/users/matt1883/orgs", "repos_url": "https://api.github.com/users/matt1883/repos", "events_url": "https://api.github.com/users/matt1883/events{/privacy}", "received_events_url": "https://api.github.com/users/matt1883/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106224, "node_id": "MDU6TGFiZWw0MDUxMDYyMjQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-15T18:51:53Z", "updated_at": "2017-07-06T22:43:29Z", "closed_at": "2017-07-06T22:43:29Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "The built-in authentication logic accepts service principal credentials to fetch an access token once. When this access token expires, the authentication logic does not refresh the access token.\r\n\r\nThe access token should be refreshed on-demand, if the existing token has expired.\r\n\r\nThis allows the user of the SDK to not worry about manually refreshing the token in the case of service principal auth, much like the existing support for refreshing user tokens.\r\n\r\nNote: Special consideration may be needed re: how the service principal's credentials are stored for re-use when fetching a new token. Other Azure SDKs for Python (e.g., Azure SDK's auth library) may provide insight here.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/163", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/163/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/163/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/163/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/163", "id": 232899519, "node_id": "MDU6SXNzdWUyMzI4OTk1MTk=", "number": 163, "title": "slow performance downloading folders", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106230, "node_id": "MDU6TGFiZWw0MDUxMDYyMzA=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/question", "name": "question", "color": "cc317c", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-01T14:32:43Z", "updated_at": "2018-03-21T20:12:17Z", "closed_at": "2018-03-21T20:12:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have probably 300,000 or so images from 40kb to 150kb each currently stored in ADLS I am attempting to download into a container.\r\n\r\nThe ADLS store is in Central US, the container is in East US.  We let the download job run overnight (14 hours or so) and the result is still nothing is downloaded.\r\n\r\nDetails on container: It is a Jupyter Notebook docker container running python 2.7.  It is mounted onto Azure Files and the drop location is Azure Files.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/158", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/158/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/158/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/158/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/158", "id": 227748128, "node_id": "MDU6SXNzdWUyMjc3NDgxMjg=", "number": 158, "title": "Token cannot be auto-refreshed", "user": {"login": "hlums", "id": 16907204, "node_id": "MDQ6VXNlcjE2OTA3MjA0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hlums", "html_url": "https://github.com/hlums", "followers_url": "https://api.github.com/users/hlums/followers", "following_url": "https://api.github.com/users/hlums/following{/other_user}", "gists_url": "https://api.github.com/users/hlums/gists{/gist_id}", "starred_url": "https://api.github.com/users/hlums/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hlums/subscriptions", "organizations_url": "https://api.github.com/users/hlums/orgs", "repos_url": "https://api.github.com/users/hlums/repos", "events_url": "https://api.github.com/users/hlums/events{/privacy}", "received_events_url": "https://api.github.com/users/hlums/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-10T17:04:08Z", "updated_at": "2017-05-30T17:48:43Z", "closed_at": "2017-05-30T17:48:43Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Hi, I got the following error when I submitted a job to ADLA and it got queued for a long time. I noticed that the token expires after some time, so I always create a new token when possible. But in this case, the job is already submitted and queued, so I couldn't create a new token. I checked the source code and didn't see where I can increase the token expiration time. Thanks!\r\n\r\n[05/09/2017 20:36:24 > 48435a: ERR ]  File \"D:\\home\\Python35\\lib\\site-packages\\azure\\mgmt\\datalake\\analytics\\job\\operations\\job_operations.py\", line 330, in get\r\n[05/09/2017 20:36:24 > 48435a: ERR ]     response = self._client.send(request, header_parameters, **operation_config)\r\n[05/09/2017 20:36:24 > 48435a: ERR ]   File \"D:\\home\\Python35\\lib\\site-packages\\msrest\\service_client.py\", line 167, in send\r\n[05/09/2017 20:36:24 > 48435a: ERR ]     session = self.creds.signed_session()\r\n[05/09/2017 20:36:24 > 48435a: ERR ]   File \"D:\\home\\Python35\\lib\\site-packages\\azure\\datalake\\store\\lib.py\", line 160, in signed_session\r\n[05/09/2017 20:36:24 > 48435a: ERR ]     self.token = refresh_token(self.token)\r\n[05/09/2017 20:36:24 > 48435a: INFO] Waiting for 3 seconds\r\n[05/09/2017 20:36:24 > 48435a: ERR ]   File \"D:\\home\\Python35\\lib\\site-packages\\azure\\datalake\\store\\lib.py\", line 60, in refresh_token\r\n[05/09/2017 20:36:24 > 48435a: ERR ]     raise ValueError(\"Token cannot be auto-refreshed.\")", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/153", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/153/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/153/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/153/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/153", "id": 218919589, "node_id": "MDU6SXNzdWUyMTg5MTk1ODk=", "number": 153, "title": "Feature Request: Create nested folder structure during upload if not already present.", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106224, "node_id": "MDU6TGFiZWw0MDUxMDYyMjQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-04-03T12:36:58Z", "updated_at": "2017-06-01T22:16:57Z", "closed_at": "2017-06-01T22:16:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Here is the output of the command:\r\n\r\nRemoteException\":{\"exception\":\"FileNotFoundException\",\"message\":\"Folder does not exist: /mnt/doc_suite/rvl_cdip_raw/labels [c0c0aeec-2c4d-4545-8f34-fc82bc26688d][2017-04-01T20:39:35.0374181-07:00]\",\"javaClassName\":\"java.io.FileNotFoundException\"}}'\r\n\r\nI currently have several engineers working simultaneously and may generate all sorts of folder structures.  These structures should be able to be managed by a simple upload/download.  Attempting to upload into a folder structure should create that structure if it doesn't exist.\r\n\r\nWork around available, however as our structures get more complex this will become increasingly painful.\r\n\r\nThanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/151", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/151/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/151/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/151/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/151", "id": 212766488, "node_id": "MDU6SXNzdWUyMTI3NjY0ODg=", "number": 151, "title": "Download Folder Api", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-03-08T15:29:27Z", "updated_at": "2017-03-24T18:18:25Z", "closed_at": "2017-03-24T18:18:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "This is an enhancement request that would be great to have.\r\n\r\nHere is what I tried to build:\r\n`def DownloadFolder(remote, local, threads = 64, overwrite = False, filemanager = None):\r\n    if(filemanager == None):\r\n        filemanager = GetADLFileManager()\r\n    ADLDownloader(filemanager, lpath=local, rpath=remote, nthreads=threads, overwrite=overwrite, buffersize=4194304, blocksize=4194304)\r\n    `\r\n\r\nI would like to use it via:\r\n\r\n`DownloadFolder('/doc_suite/', '/usr/local/db/doc_suite/')`\r\n    ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/150", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/150/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/150/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/150/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/150", "id": 212752946, "node_id": "MDU6SXNzdWUyMTI3NTI5NDY=", "number": 150, "title": "pip installs fail", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-08T14:42:51Z", "updated_at": "2017-03-08T15:17:50Z", "closed_at": "2017-03-08T15:17:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Here is the output.\r\n\r\nOn an Azure Ubuntu 16.04 NC12 machine running python 2.  Getting it set up for deep learning.\r\n\r\ndliuser@robosciencevm:~$ sudo pip install azure-datalake-store\r\nThe directory '/home/dliuser/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nThe directory '/home/dliuser/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nCollecting azure-datalake-store\r\n  Downloading azure-datalake-store-0.0.5.tar.gz\r\nRequirement already satisfied: cffi in /usr/local/lib/python2.7/dist-packages (from azure-datalake-store)\r\nCollecting adal>=0.4.2 (from azure-datalake-store)\r\n  Downloading adal-0.4.5-py2.py3-none-any.whl (49kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 4.9MB/s\r\nRequirement already satisfied: msrest~=0.4.5 in /usr/local/lib/python2.7/dist-packages (from azure-datalake-store)\r\nRequirement already satisfied: azure-nspkg in /usr/local/lib/python2.7/dist-packages (from azure-datalake-store)\r\nRequirement already satisfied: pathlib2 in /usr/local/lib/python2.7/dist-packages (from azure-datalake-store)\r\nCollecting futures (from azure-datalake-store)\r\n  Downloading futures-3.0.5-py2-none-any.whl\r\nRequirement already satisfied: pycparser in /usr/local/lib/python2.7/dist-packages (from cffi->azure-datalake-store)\r\nCollecting python-dateutil>=2.1.0 (from adal>=0.4.2->azure-datalake-store)\r\n  Downloading python_dateutil-2.6.0-py2.py3-none-any.whl (194kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 194kB 5.5MB/s\r\nCollecting cryptography>=1.1.0 (from adal>=0.4.2->azure-datalake-store)\r\n  Downloading cryptography-1.7.2.tar.gz (420kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 430kB 3.2MB/s\r\nCollecting PyJWT>=1.0.0 (from adal>=0.4.2->azure-datalake-store)\r\n  Downloading PyJWT-1.4.2-py2.py3-none-any.whl\r\nRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from adal>=0.4.2->azure-datalake-store)\r\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python2.7/dist-packages (from msrest~=0.4.5->azure-datalake-store)\r\nRequirement already satisfied: certifi>=2015.9.6.2 in /usr/local/lib/python2.7/dist-packages (from msrest~=0.4.5->azure-datalake-store)\r\nRequirement already satisfied: enum34>=1.0.4; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from msrest~=0.4.5->azure-datalake-store)\r\nRequirement already satisfied: isodate>=0.5.4 in /usr/local/lib/python2.7/dist-packages (from msrest~=0.4.5->azure-datalake-store)\r\nRequirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from pathlib2->azure-datalake-store)\r\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python2.7/dist-packages (from cryptography>=1.1.0->adal>=0.4.2->azure-datalake-store)\r\nRequirement already satisfied: pyasn1>=0.1.8 in /usr/local/lib/python2.7/dist-packages (from cryptography>=1.1.0->adal>=0.4.2->azure-datalake-store)\r\nRequirement already satisfied: setuptools>=11.3 in /usr/local/lib/python2.7/dist-packages (from cryptography>=1.1.0->adal>=0.4.2->azure-datalake-store)\r\nRequirement already satisfied: ipaddress in /usr/local/lib/python2.7/dist-packages (from cryptography>=1.1.0->adal>=0.4.2->azure-datalake-store)\r\nRequirement already satisfied: oauthlib>=0.6.2 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.5.0->msrest~=0.4.5->azure-datalake-store)\r\nInstalling collected packages: python-dateutil, cryptography, PyJWT, adal, futures, azure-datalake-store\r\n  Found existing installation: python-dateutil 1.5\r\n    Uninstalling python-dateutil-1.5:\r\n      Successfully uninstalled python-dateutil-1.5\r\n  Running setup.py install for cryptography ... error\r\n    Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-y5UGok/cryptography/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-u8bwuT-record/install-record.txt --single-version-externally-managed --compile:\r\n    running install\r\n    running build\r\n    running build_py\r\n    creating build\r\n    creating build/lib.linux-x86_64-2.7\r\n    creating build/lib.linux-x86_64-2.7/cryptography\r\n    copying src/cryptography/__about__.py -> build/lib.linux-x86_64-2.7/cryptography\r\n    copying src/cryptography/__init__.py -> build/lib.linux-x86_64-2.7/cryptography\r\n    copying src/cryptography/fernet.py -> build/lib.linux-x86_64-2.7/cryptography\r\n    copying src/cryptography/utils.py -> build/lib.linux-x86_64-2.7/cryptography\r\n    copying src/cryptography/exceptions.py -> build/lib.linux-x86_64-2.7/cryptography\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat\r\n    copying src/cryptography/hazmat/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat\r\n    creating build/lib.linux-x86_64-2.7/cryptography/x509\r\n    copying src/cryptography/x509/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/x509\r\n    copying src/cryptography/x509/oid.py -> build/lib.linux-x86_64-2.7/cryptography/x509\r\n    copying src/cryptography/x509/general_name.py -> build/lib.linux-x86_64-2.7/cryptography/x509\r\n    copying src/cryptography/x509/extensions.py -> build/lib.linux-x86_64-2.7/cryptography/x509\r\n    copying src/cryptography/x509/base.py -> build/lib.linux-x86_64-2.7/cryptography/x509\r\n    copying src/cryptography/x509/name.py -> build/lib.linux-x86_64-2.7/cryptography/x509\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/constant_time.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/hmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/keywrap.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/padding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/hashes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/serialization.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    copying src/cryptography/hazmat/primitives/cmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings\r\n    copying src/cryptography/hazmat/bindings/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\r\n    copying src/cryptography/hazmat/backends/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\r\n    copying src/cryptography/hazmat/backends/interfaces.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\r\n    copying src/cryptography/hazmat/backends/multibackend.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    copying src/cryptography/hazmat/primitives/asymmetric/rsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    copying src/cryptography/hazmat/primitives/asymmetric/dsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    copying src/cryptography/hazmat/primitives/asymmetric/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    copying src/cryptography/hazmat/primitives/asymmetric/ec.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    copying src/cryptography/hazmat/primitives/asymmetric/padding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    copying src/cryptography/hazmat/primitives/asymmetric/dh.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    copying src/cryptography/hazmat/primitives/asymmetric/utils.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/asymmetric\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/interfaces\r\n    copying src/cryptography/hazmat/primitives/interfaces/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/interfaces\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    copying src/cryptography/hazmat/primitives/kdf/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    copying src/cryptography/hazmat/primitives/kdf/concatkdf.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    copying src/cryptography/hazmat/primitives/kdf/hkdf.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    copying src/cryptography/hazmat/primitives/kdf/x963kdf.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    copying src/cryptography/hazmat/primitives/kdf/pbkdf2.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    copying src/cryptography/hazmat/primitives/kdf/kbkdf.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    copying src/cryptography/hazmat/primitives/kdf/scrypt.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/kdf\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\r\n    copying src/cryptography/hazmat/primitives/twofactor/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\r\n    copying src/cryptography/hazmat/primitives/twofactor/totp.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\r\n    copying src/cryptography/hazmat/primitives/twofactor/utils.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\r\n    copying src/cryptography/hazmat/primitives/twofactor/hotp.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/twofactor\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\r\n    copying src/cryptography/hazmat/primitives/ciphers/algorithms.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\r\n    copying src/cryptography/hazmat/primitives/ciphers/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\r\n    copying src/cryptography/hazmat/primitives/ciphers/base.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\r\n    copying src/cryptography/hazmat/primitives/ciphers/modes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/primitives/ciphers\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/commoncrypto\r\n    copying src/cryptography/hazmat/bindings/commoncrypto/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/commoncrypto\r\n    copying src/cryptography/hazmat/bindings/commoncrypto/binding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/commoncrypto\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\r\n    copying src/cryptography/hazmat/bindings/openssl/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\r\n    copying src/cryptography/hazmat/bindings/openssl/_conditional.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\r\n    copying src/cryptography/hazmat/bindings/openssl/binding.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/bindings/openssl\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\r\n    copying src/cryptography/hazmat/backends/commoncrypto/hmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\r\n    copying src/cryptography/hazmat/backends/commoncrypto/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\r\n    copying src/cryptography/hazmat/backends/commoncrypto/hashes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\r\n    copying src/cryptography/hazmat/backends/commoncrypto/backend.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\r\n    copying src/cryptography/hazmat/backends/commoncrypto/ciphers.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/commoncrypto\r\n    creating build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/rsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/dsa.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/hmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/__init__.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/ec.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/encode_asn1.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/dh.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/x509.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/hashes.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/backend.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/utils.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/cmac.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/decode_asn1.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    copying src/cryptography/hazmat/backends/openssl/ciphers.py -> build/lib.linux-x86_64-2.7/cryptography/hazmat/backends/openssl\r\n    running egg_info\r\n    writing requirements to src/cryptography.egg-info/requires.txt\r\n    writing src/cryptography.egg-info/PKG-INFO\r\n    writing top-level names to src/cryptography.egg-info/top_level.txt\r\n    writing dependency_links to src/cryptography.egg-info/dependency_links.txt\r\n    writing entry points to src/cryptography.egg-info/entry_points.txt\r\n    warning: manifest_maker: standard file '-c' not found\r\n\r\n    reading manifest file 'src/cryptography.egg-info/SOURCES.txt'\r\n    reading manifest template 'MANIFEST.in'\r\n    no previously-included directories found matching 'docs/_build'\r\n    warning: no previously-included files matching '*' found under directory 'vectors'\r\n    writing manifest file 'src/cryptography.egg-info/SOURCES.txt'\r\n    running build_ext\r\n    generating cffi module 'build/temp.linux-x86_64-2.7/_padding.c'\r\n    creating build/temp.linux-x86_64-2.7\r\n    generating cffi module 'build/temp.linux-x86_64-2.7/_constant_time.c'\r\n    generating cffi module 'build/temp.linux-x86_64-2.7/_openssl.c'\r\n    building '_openssl' extension\r\n    creating build/temp.linux-x86_64-2.7/build\r\n    creating build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7\r\n    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/_openssl.c -o build/temp.linux-x86_64-2.7/build/temp.linux-x86_64-2.7/_openssl.o\r\n    build/temp.linux-x86_64-2.7/_openssl.c:434:30: fatal error: openssl/opensslv.h: No such file or directory\r\n    compilation terminated.\r\n    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\r\n\r\n    ----------------------------------------\r\nCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-y5UGok/cryptography/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-u8bwuT-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-y5UGok/cryptography/", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/148", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/148/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/148/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/148/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/148", "id": 211543006, "node_id": "MDU6SXNzdWUyMTE1NDMwMDY=", "number": 148, "title": "Uploader fails for multiple files if DL path exists", "user": {"login": "argoneus", "id": 365334, "node_id": "MDQ6VXNlcjM2NTMzNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/365334?v=4", "gravatar_id": "", "url": "https://api.github.com/users/argoneus", "html_url": "https://github.com/argoneus", "followers_url": "https://api.github.com/users/argoneus/followers", "following_url": "https://api.github.com/users/argoneus/following{/other_user}", "gists_url": "https://api.github.com/users/argoneus/gists{/gist_id}", "starred_url": "https://api.github.com/users/argoneus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/argoneus/subscriptions", "organizations_url": "https://api.github.com/users/argoneus/orgs", "repos_url": "https://api.github.com/users/argoneus/repos", "events_url": "https://api.github.com/users/argoneus/events{/privacy}", "received_events_url": "https://api.github.com/users/argoneus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-02T22:51:26Z", "updated_at": "2017-03-10T19:24:23Z", "closed_at": "2017-03-10T19:24:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm attempting to upload multiple files to an existing DL path, and I'm receiving the following error:\r\n\r\n```In [201]: multithread.ADLUploader(adl, lpath='file*', rpath='/test/src/', nthreads=64, buffersize=4194304, blocksize=4194304)\r\n---------------------------------------------------------------------------\r\nFileExistsError                           Traceback (most recent call last)\r\n<ipython-input-201-0e276e5924b4> in <module>()\r\n----> 1 multithread.ADLUploader(adl, lpath='file*', rpath='/test/src/', nthreads=64, buffersize=4194304, blocksize=4194304)\r\n\r\n/opt/conda/lib/python3.5/site-packages/azure/datalake/store/multithread.py in __init__(self, adlfs, rpath, lpath, nthreads, chunksize, buffersize, blocksize, client, run, overwrite, verbose)\r\n    318                  overwrite=False, verbose=True):\r\n    319         if not overwrite and adlfs.exists(rpath):\r\n--> 320             raise FileExistsError(rpath)\r\n    321\r\n    322         # forcibly remove the target file before execution\r\n\r\nFileExistsError: /test/src/\r\n```\r\n\r\nMy input line is:\r\n\r\n```\r\nmultithread.ADLUploader(adl, lpath='file*', rpath='/test/src/', nthreads=64, buffersize=4194304, blocksize=4194304)\r\n```\r\n\r\nJust to test, I tried uploading a single file, specifying the path in DL, and it worked, i.e.\r\n\r\n```\r\nmultithread.ADLUploader(adl, lpath='file.py', rpath='/test/src/file.py', nthreads=64, buffersize=4194304, blocksize=4194304)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/146", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/146/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/146/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/146/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/146", "id": 208514620, "node_id": "MDU6SXNzdWUyMDg1MTQ2MjA=", "number": 146, "title": "Not all validation exceptions are raised prior to beginning upload/download", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106220, "node_id": "MDU6TGFiZWw0MDUxMDYyMjA=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2017-02-17T18:04:14Z", "updated_at": "2017-03-10T19:16:58Z", "closed_at": "2017-03-10T19:16:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "A customer discovered that, when they do not have permissions to download a file, unless error logging is enabled they do not get an exception. Additionally, the client preemptively creates the empty file prior to doing the validation that it can actually download the file.\r\n\r\nTo fix this we need to fix the following:\r\n1. Ensure that all terminating exceptions are ultimately raised to the caller (not just logged)\r\n2. Ensure that all preconditions are met and validated prior to making any state changes to the local or remote filesystems (don't create empty files before verifying that the actual files can be obtained). \r\n\r\nNote that for 2 this is best effort, since for a long running operation the user could lose permissions in the middle of the download/upload, at which point the client should fail with the right permissions error, but the state of the upload/download will be necessarily inconsistent.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/145", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/145/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/145/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/145/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/145", "id": 206020018, "node_id": "MDU6SXNzdWUyMDYwMjAwMTg=", "number": 145, "title": "Feature Request: Better logic for folder upload/download when overwrite=false", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106224, "node_id": "MDU6TGFiZWw0MDUxMDYyMjQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-02-07T21:25:50Z", "updated_at": "2017-06-01T22:15:59Z", "closed_at": "2017-06-01T22:15:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently it throws an exception.  This is painful.\r\n\r\nCheck this code, just to be sure, but I upload an entire folder.  Sometimes things go wrong and upload is interrupted.  So I move the already done files to another folder and keep uploading the source folder.\r\n\r\nGetting remote file list appears to not necessarily match the local file list.\r\n\r\nSomeFunctions\r\n`#%%\r\ndef GetADLSManager():\r\n    credentials = UserPassCredentials(user, password, tenant = tenant)\r\n    return DataLakeStoreAccountManagementClient(credentials, subscriptionId)\r\n\r\ndef GetADLFileManager():\r\n    token = lib.auth(tenant_id = tenant, username = user, password = password)\r\n    return core.AzureDLFileSystem(token, store_name=adlsAccountName)\r\n\r\n#%%\r\ndef Upload(source, destination, threads = 4, overwrite = False, filemanager = None):\r\n    if(filemanager == None):\r\n        filemanager = GetADLFileManager()\r\n    ADLUploader(filemanager, destination, source, nthreads = threads, overwrite = overwrite)`\r\n\r\n\r\n`source = 'STUFF'\r\ndest = 'STUFF'\r\nUpload(source, dest, threads = 1)`\r\n\r\n`import pandas as pd\r\nman = GetADLFileManager()\r\nrData = man.ls('REMOTE')\r\nrData = pd.Series(rData)\r\nremoteFiles = rData.str.rsplit('/', expand = True, n = 1)[1]\r\nlocalFiles = GetLocalFiles(source)\r\ndeltaFiles = localFiles[np.invert(localFiles.isin(remoteFiles))]\r\n#localFiles[[False if file in remoteFiles else True for file in localFiles]]\r\ndoneFiles = localFiles[localFiles.isin(remoteFiles)]\r\n\r\n#%%\r\nfor file in doneFiles:\r\n    filepath = os.path.join(source, file)\r\n    shutil.move(filepath, os.path.join('LOCAL_DONE', file))`\r\n\r\nAfter moving the folde,r try uploading again with the command: \r\n\r\n`source = 'STUFF'\r\ndest = 'STUFF'\r\nUpload(source, dest, threads = 1)`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/142", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/142/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/142/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/142/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/142", "id": 205991692, "node_id": "MDU6SXNzdWUyMDU5OTE2OTI=", "number": 142, "title": "Feature Request: Progress Tracking Performant for Large Folders", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106224, "node_id": "MDU6TGFiZWw0MDUxMDYyMjQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-07T19:36:43Z", "updated_at": "2017-07-24T17:43:42Z", "closed_at": "2017-07-24T17:41:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Currently progress tracking is part of \"verbose\" output which inherently implies that it may not be particularly performant. This is true for very large folders, as the way progress tracking is done ultimately results in thread locking and out of sync transfer reporting status.\r\n\r\nThis feature request is to rework progress tracking to be separate from verbose output and handle large folders of very small files efficiently.\r\n\r\nPossible solutions are:\r\n1. Update progress less frequently (once every N files completing)\r\n2. Track progress in a separate thread, if possible.\r\n3. Track progress as a raw number of bytes instead of on an individual file basis (unsure if this resolves the locking issue).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/139", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/139/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/139/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/139/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/139", "id": 205013331, "node_id": "MDU6SXNzdWUyMDUwMTMzMzE=", "number": 139, "title": "Pip install azure-datalake-store doesn't work", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lmazuel", "id": 1050156, "node_id": "MDQ6VXNlcjEwNTAxNTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1050156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmazuel", "html_url": "https://github.com/lmazuel", "followers_url": "https://api.github.com/users/lmazuel/followers", "following_url": "https://api.github.com/users/lmazuel/following{/other_user}", "gists_url": "https://api.github.com/users/lmazuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmazuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmazuel/subscriptions", "organizations_url": "https://api.github.com/users/lmazuel/orgs", "repos_url": "https://api.github.com/users/lmazuel/repos", "events_url": "https://api.github.com/users/lmazuel/events{/privacy}", "received_events_url": "https://api.github.com/users/lmazuel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lmazuel", "id": 1050156, "node_id": "MDQ6VXNlcjEwNTAxNTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1050156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmazuel", "html_url": "https://github.com/lmazuel", "followers_url": "https://api.github.com/users/lmazuel/followers", "following_url": "https://api.github.com/users/lmazuel/following{/other_user}", "gists_url": "https://api.github.com/users/lmazuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmazuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmazuel/subscriptions", "organizations_url": "https://api.github.com/users/lmazuel/orgs", "repos_url": "https://api.github.com/users/lmazuel/repos", "events_url": "https://api.github.com/users/lmazuel/events{/privacy}", "received_events_url": "https://api.github.com/users/lmazuel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2017-02-02T22:16:27Z", "updated_at": "2017-02-02T22:31:50Z", "closed_at": "2017-02-02T22:31:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "Here is the exact error codes.\r\n\r\nI'm doing it inside the CNTK v8 virtual environment using python 3.4.\r\n\r\n(C:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py34) c:\\local\\cntk\\Scripts>pip install azure-datalake-store --upgrade\r\nCollecting azure-datalake-store\r\n  Using cached azure-datalake-store-0.0.2.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"C:\\Users\\DAVIDC~1\\AppData\\Local\\Temp\\pip-build-r2ekxt4b\\azure-datalake-store\\setup.py\", line 10, in <module>\r\n        with open('HISTORY.rst', encoding='utf-8') as f:\r\n    FileNotFoundError: [Errno 2] No such file or directory: 'HISTORY.rst'\r\n\r\n    ----------------------------------------\r\nCommand \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\DAVIDC~1\\AppData\\Local\\Temp\\pip-build-r2ekxt4b\\azure-datalake-store\\\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/136", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/136/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/136/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/136/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/136", "id": 204697406, "node_id": "MDU6SXNzdWUyMDQ2OTc0MDY=", "number": 136, "title": "pip install broken", "user": {"login": "chasehere", "id": 14252610, "node_id": "MDQ6VXNlcjE0MjUyNjEw", "avatar_url": "https://avatars1.githubusercontent.com/u/14252610?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chasehere", "html_url": "https://github.com/chasehere", "followers_url": "https://api.github.com/users/chasehere/followers", "following_url": "https://api.github.com/users/chasehere/following{/other_user}", "gists_url": "https://api.github.com/users/chasehere/gists{/gist_id}", "starred_url": "https://api.github.com/users/chasehere/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chasehere/subscriptions", "organizations_url": "https://api.github.com/users/chasehere/orgs", "repos_url": "https://api.github.com/users/chasehere/repos", "events_url": "https://api.github.com/users/chasehere/events{/privacy}", "received_events_url": "https://api.github.com/users/chasehere/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-01T20:35:10Z", "updated_at": "2017-02-02T23:18:27Z", "closed_at": "2017-02-02T23:18:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\nroot@08e4d8f1cbb2:/# pip install azure-datalake-store\r\nCollecting azure-datalake-store\r\n  Downloading azure-datalake-store-0.0.2.tar.gz\r\n    Complete output from command python setup.py egg_info:\r\n    Traceback (most recent call last):\r\n      File \"<string>\", line 1, in <module>\r\n      File \"/tmp/pip-build-fwmi3z5k/azure-datalake-store/setup.py\", line 10, in <module>\r\n        with open('HISTORY.rst', encoding='utf-8') as f:\r\n    FileNotFoundError: [Errno 2] No such file or directory: 'HISTORY.rst'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/133", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/133/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/133/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/133/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/133", "id": 203890551, "node_id": "MDU6SXNzdWUyMDM4OTA1NTE=", "number": 133, "title": "Permission Denied", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-01-29T19:38:24Z", "updated_at": "2017-02-02T23:30:55Z", "closed_at": "2017-02-02T23:30:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nIt seems that no matter what I do, I can not get access to my store.  Below is the exact code I am using (minus the password etc.)\r\n\r\nThe user is the subscription global admin and as such is an owner over the resource.  I have also tried the service account I created with office 365 and have verified the user is a contributor to the resource group owning data lake as well as ensuring has read, write, execute access from the root folder in data lake.\r\n\r\nThis is customer blocking, any tips are welcome.\r\n\r\n```\r\n#%% imports\r\n## Use this only for Azure AD service-to-service authentication\r\nfrom azure.common.credentials import ServicePrincipalCredentials\r\n## Use this only for Azure AD end-user authentication\r\nfrom azure.common.credentials import UserPassCredentials\r\n## Use this only for Azure AD multi-factor authentication\r\nfrom msrestazure.azure_active_directory import AADTokenCredentials\r\n## Required for Azure Data Lake Store account management\r\nfrom azure.mgmt.datalake.store import DataLakeStoreAccountManagementClient\r\nfrom azure.mgmt.datalake.store.models import DataLakeStoreAccount\r\n## Required for Azure Data Lake Store filesystem management\r\nfrom azure.datalake.store import core, lib, multithread\r\n# Common Azure imports\r\nfrom azure.mgmt.resource.resources import ResourceManagementClient\r\nfrom azure.mgmt.resource.resources.models import ResourceGroup\r\n## Use these as needed for your application\r\nimport logging, getpass, pprint, uuid, time\r\n\r\nsubscriptionId = 'YOUR ID'\r\nadlsAccountName = 'YOUR NAME'\r\n#Best to use Office 365 or similiar and provision a non licensed user\r\n#who can be your batch management user\r\nuser = 'user@domain.com'\r\npassword = 'YOUR PASS'\r\n\r\ntoken = lib.auth(tenant_id = \"common\", username = user, password = password)\r\ncredentials = UserPassCredentials(user, password)\r\n\r\n#%%\r\nadlsAccountClient = DataLakeStoreAccountManagementClient(credentials, subscriptionId)\r\nadlsFileSystemClient = core.AzureDLFileSystem(token, store_name=adlsAccountName)\r\n\r\n#%%\r\nadlsFileSystemClient.ls('', detail=True)\r\n```\r\n\r\n\r\n\r\n------------------------\r\nERROR\r\n-----------------------\r\n---------------------------------------------------------------------------\r\nPermissionError                           Traceback (most recent call last)\r\n<ipython-input-31-7050d508e59f> in <module>()\r\n      1 #%%\r\n----> 2 adlsFileSystemClient.ls('', detail=True)\r\n\r\nC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\azure\\datalake\\store\\core.py in ls(self, path, detail)\r\n    117         \"\"\" List single directory with or without details \"\"\"\r\n    118         path = AzureDLPath(path)\r\n--> 119         files = self._ls(path)\r\n    120         if not files:\r\n    121             inf = self.info(path)\r\n\r\nC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\azure\\datalake\\store\\core.py in _ls(self, path)\r\n    108         key = path.as_posix()\r\n    109         if path not in self.dirs:\r\n--> 110             out = self.azure.call('LISTSTATUS', path.as_posix())\r\n    111             self.dirs[key] = out['FileStatuses']['FileStatus']\r\n    112             for f in self.dirs[key]:\r\n\r\nC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\azure\\datalake\\store\\lib.py in call(self, op, path, **kwargs)\r\n    303 \r\n    304         if r.status_code == 403:\r\n--> 305             self.log_response_and_raise(r, PermissionError(path))\r\n    306         elif r.status_code == 404:\r\n    307             self.log_response_and_raise(r, FileNotFoundError(path))\r\n\r\nC:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\azure\\datalake\\store\\lib.py in log_response_and_raise(self, response, exception, level)\r\n    256                 msg += \"\\n(Response body was truncated)\"\r\n    257         logger.log(level, msg)\r\n--> 258         raise exception\r\n    259 \r\n    260     def _is_json_response(self, response):\r\n\r\nPermissionError: .\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/132", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/132/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/132/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/132/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/132", "id": 203275909, "node_id": "MDU6SXNzdWUyMDMyNzU5MDk=", "number": 132, "title": "[Feature request]: Delimiter Support for file and folder upload", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106224, "node_id": "MDU6TGFiZWw0MDUxMDYyMjQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-01-26T02:02:41Z", "updated_at": "2018-08-14T21:31:37Z", "closed_at": "2018-08-14T21:31:37Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "This is a nice to have enhancement that allows for record boundaries to be honored during upload, so that no data is uploaded in the middle of a record.\r\n\r\nWork required for this:\r\n1. Add functionality to properly chunk large files on delimiter terminations, so that no records span chunks\r\n2. Add awareness of all chunks to the delimiter searching logic to allow for robust error handling and validation.\r\n3. Add E2E tests.\r\n\r\nCost:\r\n~40 hours.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/124", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/124/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/124/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/124/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/124", "id": 200467928, "node_id": "MDU6SXNzdWUyMDA0Njc5Mjg=", "number": 124, "title": "[Feature request]: Re-introduce state saving functionality", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106224, "node_id": "MDU6TGFiZWw0MDUxMDYyMjQ=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-01-12T20:13:01Z", "updated_at": "2018-08-14T21:31:25Z", "closed_at": "2018-08-14T21:31:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "State saving was removed due to intensive IO operations being performed in PR #114. This issue tracks the need to re-introduce state saving to support resumability of failed or interrupted uploads and downloads.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/123", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/123/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/123/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/123/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/123", "id": 199972458, "node_id": "MDU6SXNzdWUxOTk5NzI0NTg=", "number": 123, "title": "lib.auth() with no args should default to 2FA to the common tenant", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-01-11T00:30:44Z", "updated_at": "2017-01-25T18:04:23Z", "closed_at": "2017-01-25T18:04:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "It currently requires the tenant_id to be passed in. The 'common' tenant should be used by default.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/117", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/117/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/117/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/117/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/117", "id": 189257523, "node_id": "MDU6SXNzdWUxODkyNTc1MjM=", "number": 117, "title": "Upload has inconsistent hangs due to over zealous state tracking.", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-11-14T23:36:00Z", "updated_at": "2017-01-12T20:11:03Z", "closed_at": "2017-01-12T20:11:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "During upload, we are seeing throughput drop to zero on some attempts for periods up to 20 minutes. During this time it appears that all processing has stopped, as though the uploader is deadlocked or waiting for some exclusive lock to be released. This is drastically impacting overall performance of the uploader, especially when run for very large files.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/115", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/115/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/115/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/115/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/115", "id": 189210409, "node_id": "MDU6SXNzdWUxODkyMTA0MDk=", "number": 115, "title": "Cancelling executing results in typeError", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2016-11-14T19:59:35Z", "updated_at": "2016-11-15T01:04:45Z", "closed_at": "2016-11-15T01:04:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When cancelling out of an upload or download (ctrl+c) the following error message is returned, which indicates that future appears to be set as an int instead of an object.\r\n\r\ntransfer.py\", line 364, in _update\r\n    nbytes, exception = future.result()\r\n**TypeError: 'int' object is not iterable**\r\n2016-11-14 19:56:59,545 concurrent.futures ERROR    exception calling callback f\r\nor <Future at 0x7775f44a90 state=finished returned int>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\adlsperf\\AppData\\Local\\Programs\\Python\\Python35\\lib\\concurrent\\\r\nfutures\\_base.py\", line 297, in _invoke_callbacks\r\n    callback(self)\r\n  File \"c:\\tools\\begoldsm\\azure-data-lake-store-python-dev\\azure\\datalake\\store\\\r\ntransfer.py\", line 364, in _update\r\n    nbytes, exception = future.result()\r\nTypeError: 'int' object is not iterable", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/112", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/112/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/112/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/112/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/112", "id": 188369097, "node_id": "MDU6SXNzdWUxODgzNjkwOTc=", "number": 112, "title": "Parallel upload/download opening too many TCP/IP connections", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2016-11-09T22:28:03Z", "updated_at": "2016-11-14T18:03:28Z", "closed_at": "2016-11-14T18:03:23Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "The current implementation does not submit web requests through a session, which results in a new session being created and destroyed each time a request is sent. This results in a lot of time being spent in getaddrinfo calls when that information can be re-used. The solution to this is to execute requests through a session like in the below example:\r\n```\r\nself.global_session = requests.Session()\r\nself.global_session.put(<url>, params=params, headers=headers, data=data, stream=stream)\r\n```\r\n\r\nnote, though, that Session is not guaranteed to be thread safe, so session awareness will need to be passed down through each thread, resulting in one session per thread.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/111", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/111/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/111/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/111/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/111", "id": 187997857, "node_id": "MDU6SXNzdWUxODc5OTc4NTc=", "number": 111, "title": "Multi-factor authentication ", "user": {"login": "JonathanNiesel", "id": 20426132, "node_id": "MDQ6VXNlcjIwNDI2MTMy", "avatar_url": "https://avatars2.githubusercontent.com/u/20426132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JonathanNiesel", "html_url": "https://github.com/JonathanNiesel", "followers_url": "https://api.github.com/users/JonathanNiesel/followers", "following_url": "https://api.github.com/users/JonathanNiesel/following{/other_user}", "gists_url": "https://api.github.com/users/JonathanNiesel/gists{/gist_id}", "starred_url": "https://api.github.com/users/JonathanNiesel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JonathanNiesel/subscriptions", "organizations_url": "https://api.github.com/users/JonathanNiesel/orgs", "repos_url": "https://api.github.com/users/JonathanNiesel/repos", "events_url": "https://api.github.com/users/JonathanNiesel/events{/privacy}", "received_events_url": "https://api.github.com/users/JonathanNiesel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-11-08T13:56:27Z", "updated_at": "2016-11-21T18:07:26Z", "closed_at": "2016-11-21T18:07:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\nI try to connect with the following line:\r\n`token = lib.auth(tenant_id, username, pasr)`,\r\nbut it returns the following error:\r\n`AdalError: Get Token request returned http error: 400 and server response: {\"error\":\"interaction_required\",\"error_description\":\"AADSTS50076: Due to a configuration change made by your administrator, or because you moved to a new location, you must use multi-factor authentication to access '797f4846-ba00-4fd7-ba43-XXXXXXX'.\\r\\nTrace ID: 438223a7-da6d-473d-bdf5-XXXXXXX\\r\\nCorrelation ID: 03d712aa-d504-4afb-be04-XXXXXXX\\r\\nTimestamp: 2016-11-08 13:51:43Z\",\"error_codes\":[50076],\"timestamp\":\"2016-11-08 13:51:43Z\",\"trace_id\":\"438223a7-da6d-473d-bdf5-XXXXXX\",\"correlation_id\":\"03d712aa-d504-4afb-be04-XXXXXXX\"}`\r\n\r\nIs there a possibility to access while using two-factor authentication? \r\n`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/108", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/108/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/108/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/108/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/108", "id": 185548626, "node_id": "MDU6SXNzdWUxODU1NDg2MjY=", "number": 108, "title": "Download operations failing silently sometimes", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2016-10-27T01:00:13Z", "updated_at": "2016-10-30T22:08:09Z", "closed_at": "2016-10-30T22:08:06Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "We are seeing issues where, unless progress tracking is being printed out, download fails without reporting any error messages, resulting in a corrupt downloaded file and the perception of success.\n\nExample output:\nremote file     : /begoldsmtest/50_1GB_Files\nremote file size: 52425000000\n[errored] file begoldsmtest/50_1GB_Files/14 -> D:\\ingress\\50_1GB_Files.out\\14, chunk D:\\ingress\\50_1GB_Files.out\\14 0\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/14 -> D:\\ingress\\50_1GB_Files.out\\14\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/36 -> D:\\ingress\\50_1GB_Files.out\\36\n[errored] file begoldsmtest/50_1GB_Files/45 -> D:\\ingress\\50_1GB_Files.out\\45, chunk D:\\ingress\\50_1GB_Files.out\\45 805306368\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/45 -> D:\\ingress\\50_1GB_Files.out\\45\n[errored] file begoldsmtest/50_1GB_Files/30 -> D:\\ingress\\50_1GB_Files.out\\30, chunk D:\\ingress\\50_1GB_Files.out\\30 0\n[errored] file begoldsmtest/50_1GB_Files/30 -> D:\\ingress\\50_1GB_Files.out\\30, chunk D:\\ingress\\50_1GB_Files.out\\30 536870912\n[   2/   4 chunks] begoldsmtest/50_1GB_Files/30 -> D:\\ingress\\50_1GB_Files.out\\30\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/18 -> D:\\ingress\\50_1GB_Files.out\\18\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/31 -> D:\\ingress\\50_1GB_Files.out\\31\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/17 -> D:\\ingress\\50_1GB_Files.out\\17\n[errored] file begoldsmtest/50_1GB_Files/26 -> D:\\ingress\\50_1GB_Files.out\\26, chunk D:\\ingress\\50_1GB_Files.out\\26 805306368\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/26 -> D:\\ingress\\50_1GB_Files.out\\26\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/29 -> D:\\ingress\\50_1GB_Files.out\\29\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/11 -> D:\\ingress\\50_1GB_Files.out\\11\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/50 -> D:\\ingress\\50_1GB_Files.out\\50\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/39 -> D:\\ingress\\50_1GB_Files.out\\39\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/10 -> D:\\ingress\\50_1GB_Files.out\\10\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/7 -> D:\\ingress\\50_1GB_Files.out\\7\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/12 -> D:\\ingress\\50_1GB_Files.out\\12\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/3 -> D:\\ingress\\50_1GB_Files.out\\3\n[errored] file begoldsmtest/50_1GB_Files/9 -> D:\\ingress\\50_1GB_Files.out\\9, chunk D:\\ingress\\50_1GB_Files.out\\9 268435456\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/9 -> D:\\ingress\\50_1GB_Files.out\\9\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/20 -> D:\\ingress\\50_1GB_Files.out\\20\n[errored] file begoldsmtest/50_1GB_Files/37 -> D:\\ingress\\50_1GB_Files.out\\37, chunk D:\\ingress\\50_1GB_Files.out\\37 0\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/37 -> D:\\ingress\\50_1GB_Files.out\\37\n[errored] file begoldsmtest/50_1GB_Files/33 -> D:\\ingress\\50_1GB_Files.out\\33, chunk D:\\ingress\\50_1GB_Files.out\\33 0\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/33 -> D:\\ingress\\50_1GB_Files.out\\33\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/22 -> D:\\ingress\\50_1GB_Files.out\\22\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/19 -> D:\\ingress\\50_1GB_Files.out\\19\n[errored] file begoldsmtest/50_1GB_Files/23 -> D:\\ingress\\50_1GB_Files.out\\23, chunk D:\\ingress\\50_1GB_Files.out\\23 805306368\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/23 -> D:\\ingress\\50_1GB_Files.out\\23\n[errored] file begoldsmtest/50_1GB_Files/5 -> D:\\ingress\\50_1GB_Files.out\\5, chunk D:\\ingress\\50_1GB_Files.out\\5 0\n[   3/   4 chunks] begoldsmtest/50_1GB_Files/5 -> D:\\ingress\\50_1GB_Files.out\\5\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/6 -> D:\\ingress\\50_1GB_Files.out\\6\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/47 -> D:\\ingress\\50_1GB_Files.out\\47\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/16 -> D:\\ingress\\50_1GB_Files.out\\16\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/8 -> D:\\ingress\\50_1GB_Files.out\\8\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/25 -> D:\\ingress\\50_1GB_Files.out\\25\n[errored] file begoldsmtest/50_1GB_Files/28 -> D:\\ingress\\50_1GB_Files.out\\28, chunk D:\\ingress\\50_1GB_Files.out\\28 805306368\n[errored] file begoldsmtest/50_1GB_Files/28 -> D:\\ingress\\50_1GB_Files.out\\28, chunk D:\\ingress\\50_1GB_Files.out\\28 268435456\n[   2/   4 chunks] begoldsmtest/50_1GB_Files/28 -> D:\\ingress\\50_1GB_Files.out\\28\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/32 -> D:\\ingress\\50_1GB_Files.out\\32\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/40 -> D:\\ingress\\50_1GB_Files.out\\40\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/13 -> D:\\ingress\\50_1GB_Files.out\\13\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/49 -> D:\\ingress\\50_1GB_Files.out\\49\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/1 -> D:\\ingress\\50_1GB_Files.out\\1\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/43 -> D:\\ingress\\50_1GB_Files.out\\43\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/21 -> D:\\ingress\\50_1GB_Files.out\\21\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/34 -> D:\\ingress\\50_1GB_Files.out\\34\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/48 -> D:\\ingress\\50_1GB_Files.out\\48\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/42 -> D:\\ingress\\50_1GB_Files.out\\42\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/44 -> D:\\ingress\\50_1GB_Files.out\\44\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/46 -> D:\\ingress\\50_1GB_Files.out\\46\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/27 -> D:\\ingress\\50_1GB_Files.out\\27\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/41 -> D:\\ingress\\50_1GB_Files.out\\41\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/4 -> D:\\ingress\\50_1GB_Files.out\\4\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/2 -> D:\\ingress\\50_1GB_Files.out\\2\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/15 -> D:\\ingress\\50_1GB_Files.out\\15\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/35 -> D:\\ingress\\50_1GB_Files.out\\35\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/24 -> D:\\ingress\\50_1GB_Files.out\\24\n[   4/   4 chunks] begoldsmtest/50_1GB_Files/38 -> D:\\ingress\\50_1GB_Files.out\\38\n[bench_download_50_1gb] finished in 180.0904s\n16ca4ecebb7987748097be6a87a98c20 D:\\ingress\\50_1GB_Files\n3f38b1eccd3f79f2fde9b0e032c37449 D:\\ingress\\50_1GB_Files.out\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/96", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/96/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/96/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/96/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/96", "id": 181364165, "node_id": "MDU6SXNzdWUxODEzNjQxNjU=", "number": 96, "title": "Auth with user name and password does not work", "user": {"login": "avaranovich", "id": 963035, "node_id": "MDQ6VXNlcjk2MzAzNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/963035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avaranovich", "html_url": "https://github.com/avaranovich", "followers_url": "https://api.github.com/users/avaranovich/followers", "following_url": "https://api.github.com/users/avaranovich/following{/other_user}", "gists_url": "https://api.github.com/users/avaranovich/gists{/gist_id}", "starred_url": "https://api.github.com/users/avaranovich/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avaranovich/subscriptions", "organizations_url": "https://api.github.com/users/avaranovich/orgs", "repos_url": "https://api.github.com/users/avaranovich/repos", "events_url": "https://api.github.com/users/avaranovich/events{/privacy}", "received_events_url": "https://api.github.com/users/avaranovich/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-10-06T09:23:03Z", "updated_at": "2016-10-06T20:57:27Z", "closed_at": "2016-10-06T20:57:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "`token = lib.auth(tenant_id, username, password)`\n\ngives\n\n`adal.adal_error.AdalError: Get Token request returned http error: 400 and server response: {\"error\":\"invalid_request\",\"error_description\":\"AADSTS90014: The request body must contain the following parameter: 'client_id'.\\r\\nTrace ID: 899e028b-469f-4dba-ab97-518e5a3e42e5\\r\\nCorrelation ID: 24bd2ce3-b084-40ab-9e8f-46f5a54ac501\\r\\nTimestamp: 2016-10-06 09:21:19Z\",\"error_codes\":[90014],\"timestamp\":\"2016-10-06 09:21:19Z\",\"trace_id\":\"899e028b-469f-4dba-ab97-518e5a3e42e5\",\"correlation_id\":\"24bd2ce3-b084-40ab-9e8f-46f5a54ac501\"}`\n\nand according to this line https://github.com/Azure/azure-data-lake-store-python/blob/dev/azure/datalake/store/lib.py#L106\n\nit cannot work just with username and password.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/95", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/95/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/95/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/95/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/95", "id": 181346682, "node_id": "MDU6SXNzdWUxODEzNDY2ODI=", "number": 95, "title": "AccessControlException after file upload", "user": {"login": "avaranovich", "id": 963035, "node_id": "MDQ6VXNlcjk2MzAzNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/963035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avaranovich", "html_url": "https://github.com/avaranovich", "followers_url": "https://api.github.com/users/avaranovich/followers", "following_url": "https://api.github.com/users/avaranovich/following{/other_user}", "gists_url": "https://api.github.com/users/avaranovich/gists{/gist_id}", "starred_url": "https://api.github.com/users/avaranovich/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avaranovich/subscriptions", "organizations_url": "https://api.github.com/users/avaranovich/orgs", "repos_url": "https://api.github.com/users/avaranovich/repos", "events_url": "https://api.github.com/users/avaranovich/events{/privacy}", "received_events_url": "https://api.github.com/users/avaranovich/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2016-10-06T07:53:13Z", "updated_at": "2017-06-01T22:13:00Z", "closed_at": "2017-06-01T22:13:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to upload the content of the local file into a DataLake store, with the following simple snippet:\n\n```\ntoken = lib.auth(tenant_id='782633d2-40ee-4e13-a016-xxxx', client_id='6a706405-9ef2-440c-9585-xxxxx', client_secret='yyyy=')\nadl = AzureDLFileSystem(store_name='mystore', token=token)\n\nadl.put('updates/update.xml', update.xm)\n```\n\nI can see the file is a data explorer, with a non-zero size, however, once I click on the file, I get\n\n```\n Error\nAccessControlException\nMessage\nFsOpenStream failed with error 0x83090aa2 (Either the resource does not exist or the current user is not authorized to perform the requested operation).\n\n [58c2c59a-1a6d-42da-aaff-9f9bde8216be] failed with error 0x83090aa2 (Either the resource does not exist or the current user is not authorized to perform the requested operation).\n\n [58c2c59a-1a6d-42da-aaff-9f9bde8216be][2016-10-06T00:44:11.1559029-07:00]\n```\n\nI can oly delete the file from web browser. My user is the owner of the root datalake store folder.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/91", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/91/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/91/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/91/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/91", "id": 180752074, "node_id": "MDU6SXNzdWUxODA3NTIwNzQ=", "number": 91, "title": "Client Usability: Error messages must be actionable and include trace IDs from the server", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2016-10-03T21:29:27Z", "updated_at": "2016-10-06T16:54:01Z", "closed_at": "2016-10-06T16:53:57Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "When the user receives an error from the client it must be actionable. This means it either\n1. Tells the user what they did wrong and how they can correct (FileNotFound, AccessDenied, Conflict, etc).\n2. Gives the user enough information to effectively escalate to service engineers (Full error responses from the service, x-ms-client-request-id, traceId, etc.).\n\nRight now, in the parallel upload/download client we are returning a \"RuntimeError: Max number of ADL retries exceeded\" message. This should be updated to also include what the error was that kept resulting in retries so the user knows what the underlying failure was that caused us to abort the run.\n\nThis should also be true for all single calls, since this is a service and it could return a 500 error at some point (due to service unavailability for example) and its important that customers can give the service engineers as much information as possible to quickly and correctly identify and resolve their issue.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/78", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/78/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/78/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/78/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/78", "id": 178932174, "node_id": "MDU6SXNzdWUxNzg5MzIxNzQ=", "number": 78, "title": "Meta: Test scenarios", "user": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-09-23T17:32:33Z", "updated_at": "2019-08-14T23:03:16Z", "closed_at": "2019-08-14T23:03:16Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "**Cancel upload/download**\r\n- [ ] Validate that the final file does not exist\r\n- [ ] Resume the upload/download\r\n- [ ] Validate that it completes successfully\r\n\r\n**Validate retry logic when a basic REST call fails for uploader/downloader**\r\n- [x] Mock a failure in append/create. Validate the operation retries and eventually fails\r\n- [ ] Resume the upload/download with mocked failure remove, verify it succeeds\r\n\r\n**Verify overwrite logic**\r\n- [x] Files/folders should not be overwritten if the user does not indicate a force operation\r\n- [x] Files/folders should be overwritten if the user does indicate force (default should not be force)\r\n\r\n**Verify record boundary splitting is honored**\r\n- [x] If this is passed in, verify that it is honored.\r\n- [x] Verify that if the user passes in this boundary splitting and the data is >4MB when a boundary is found, that we throw (since we cannot support records that are longer than 4mb).\r\n\r\n**Progress tracking/state validation**\r\n- [x] Validate that metadata about the upload/download is correct upon completion/error termination\r\n- [x] This should also be done during the resume tests to ensure the metadata is accurate/useable.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/77", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/77/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/77/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/77/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/77", "id": 178479264, "node_id": "MDU6SXNzdWUxNzg0NzkyNjQ=", "number": 77, "title": "PRI0: Upload fails but reports success", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106220, "node_id": "MDU6TGFiZWw0MDUxMDYyMjA=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/milestones/4", "html_url": "https://github.com/Azure/azure-data-lake-store-python/milestone/4", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/milestones/4/labels", "id": 1905764, "node_id": "MDk6TWlsZXN0b25lMTkwNTc2NA==", "number": 4, "title": "M4: Stabilization, Integration and Documentation", "description": "- Stabilize the work of the previous three milestones and ensure all tests and CI jobs have robust coverage\r\n- Integrate this custom functionality with the existing Azure SDK for python. This includes proper packaging and naming, ensure inclusion of any common dependencies for things like error handling (ideally this is done in an ongoing basis during development in milestone one, but just in case anything is missed it is fixed here).\r\n- Get the functionality ready for package publishing, which includes ensuring our getting started documentation, samples and readthedocs code documentation is ready and has been reviewed.", "creator": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 22, "state": "open", "created_at": "2016-07-26T20:08:26Z", "updated_at": "2017-01-30T20:28:54Z", "due_on": "2016-09-30T07:00:00Z", "closed_at": null}, "comments": 1, "created_at": "2016-09-21T22:35:09Z", "updated_at": "2016-09-29T00:24:52Z", "closed_at": "2016-09-29T00:24:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Upload of a 50gb file is failing, even though no error is returned to the caller. This bug has two issues:\n1. There is not enough validation taking place in the upload to properly report success/failure to the caller with confidence\n2. Upload is failing consistently, which needs to be addressed and validation for it must be included in future runs.\n\nWith debug logs active I see that a lot of transfers are failing, most don't have any error at all associated with them, and some are indicating I/O errors due to accessing closed files (log attached).\n\n[uploadLog.txt](https://github.com/Azure/azure-data-lake-store-python/files/486147/uploadLog.txt)\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/73", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/73/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/73/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/73/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/73", "id": 178135936, "node_id": "MDU6SXNzdWUxNzgxMzU5MzY=", "number": 73, "title": "Place temporary files under destination path", "user": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2016-09-20T18:18:31Z", "updated_at": "2016-09-29T22:15:00Z", "closed_at": "2016-09-29T22:14:56Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "As mentioned by Ben, we should place uploaded chunks in a temporary directory under the destination path. The proposed temporary directory/files are in keeping with other SDK implementations.\n\nIf the destination file is `/user/deeperFolder/bar.txt`, then the temporary directory will be:\n\n```\n/user/deeperFolder/bar.txt.segments.{random string}/\n```\n\nWithin the temporary directory, the temporary chunk filenames will be:\n\n```\nbar.txt.{random string}.segment{int}\n```\n\nwhere `int` is the chunk number.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/72", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/72/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/72/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/72/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/72", "id": 178116568, "node_id": "MDU6SXNzdWUxNzgxMTY1Njg=", "number": 72, "title": "Can no longer pass the store name or url_suffix into AzureDLFileSystem. Only option is env variable", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106220, "node_id": "MDU6TGFiZWw0MDUxMDYyMjA=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "martindurant", "id": 6042212, "node_id": "MDQ6VXNlcjYwNDIyMTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6042212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martindurant", "html_url": "https://github.com/martindurant", "followers_url": "https://api.github.com/users/martindurant/followers", "following_url": "https://api.github.com/users/martindurant/following{/other_user}", "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}", "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions", "organizations_url": "https://api.github.com/users/martindurant/orgs", "repos_url": "https://api.github.com/users/martindurant/repos", "events_url": "https://api.github.com/users/martindurant/events{/privacy}", "received_events_url": "https://api.github.com/users/martindurant/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "martindurant", "id": 6042212, "node_id": "MDQ6VXNlcjYwNDIyMTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6042212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martindurant", "html_url": "https://github.com/martindurant", "followers_url": "https://api.github.com/users/martindurant/followers", "following_url": "https://api.github.com/users/martindurant/following{/other_user}", "gists_url": "https://api.github.com/users/martindurant/gists{/gist_id}", "starred_url": "https://api.github.com/users/martindurant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martindurant/subscriptions", "organizations_url": "https://api.github.com/users/martindurant/orgs", "repos_url": "https://api.github.com/users/martindurant/repos", "events_url": "https://api.github.com/users/martindurant/events{/privacy}", "received_events_url": "https://api.github.com/users/martindurant/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-09-20T16:57:04Z", "updated_at": "2016-09-21T02:40:42Z", "closed_at": "2016-09-21T02:40:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In the comments it indicates there is a param called \"store\". In reality, this is the constructor definition for AzureDLFileSystem (including the comments of what the args are claimed to be as well):\n\n**store : str (\"\")**\n        Store name to connect to\n    token : dict\n        When setting up a new connection, this contains the authorization\n        credentials (see `lib.auth()`).\n    **url_suffix: str (None)**\n        Domain to send REST requests to. The end-point URL is constructed\n        using this and the store_name. If None, use default.\n    kwargs: optional key/values\n        For auth, such as username, password. See `lib.auth()`\ndef **init**(self, token=None, **kwargs)\n\nNote that store and url_suffix are both  missing from the list.\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/71", "repository_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/71/labels{/name}", "comments_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/71/comments", "events_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/issues/71/events", "html_url": "https://github.com/Azure/azure-data-lake-store-python/issues/71", "id": 177940669, "node_id": "MDU6SXNzdWUxNzc5NDA2Njk=", "number": 71, "title": "Pri0: Upload and download of 100GB file fails with no error, debug logs do not indicate what the error is either.", "user": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 405106220, "node_id": "MDU6TGFiZWw0MDUxMDYyMjA=", "url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/labels/bug", "name": "bug", "color": "ee0701", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": {"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jbcrail", "id": 6038, "node_id": "MDQ6VXNlcjYwMzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6038?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jbcrail", "html_url": "https://github.com/jbcrail", "followers_url": "https://api.github.com/users/jbcrail/followers", "following_url": "https://api.github.com/users/jbcrail/following{/other_user}", "gists_url": "https://api.github.com/users/jbcrail/gists{/gist_id}", "starred_url": "https://api.github.com/users/jbcrail/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jbcrail/subscriptions", "organizations_url": "https://api.github.com/users/jbcrail/orgs", "repos_url": "https://api.github.com/users/jbcrail/repos", "events_url": "https://api.github.com/users/jbcrail/events{/privacy}", "received_events_url": "https://api.github.com/users/jbcrail/received_events", "type": "User", "site_admin": false}], "milestone": {"url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/milestones/4", "html_url": "https://github.com/Azure/azure-data-lake-store-python/milestone/4", "labels_url": "https://api.github.com/repos/Azure/azure-data-lake-store-python/milestones/4/labels", "id": 1905764, "node_id": "MDk6TWlsZXN0b25lMTkwNTc2NA==", "number": 4, "title": "M4: Stabilization, Integration and Documentation", "description": "- Stabilize the work of the previous three milestones and ensure all tests and CI jobs have robust coverage\r\n- Integrate this custom functionality with the existing Azure SDK for python. This includes proper packaging and naming, ensure inclusion of any common dependencies for things like error handling (ideally this is done in an ongoing basis during development in milestone one, but just in case anything is missed it is fixed here).\r\n- Get the functionality ready for package publishing, which includes ensuring our getting started documentation, samples and readthedocs code documentation is ready and has been reviewed.", "creator": {"login": "begoldsm", "id": 8781927, "node_id": "MDQ6VXNlcjg3ODE5Mjc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8781927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/begoldsm", "html_url": "https://github.com/begoldsm", "followers_url": "https://api.github.com/users/begoldsm/followers", "following_url": "https://api.github.com/users/begoldsm/following{/other_user}", "gists_url": "https://api.github.com/users/begoldsm/gists{/gist_id}", "starred_url": "https://api.github.com/users/begoldsm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/begoldsm/subscriptions", "organizations_url": "https://api.github.com/users/begoldsm/orgs", "repos_url": "https://api.github.com/users/begoldsm/repos", "events_url": "https://api.github.com/users/begoldsm/events{/privacy}", "received_events_url": "https://api.github.com/users/begoldsm/received_events", "type": "User", "site_admin": false}, "open_issues": 0, "closed_issues": 22, "state": "open", "created_at": "2016-07-26T20:08:26Z", "updated_at": "2017-01-30T20:28:54Z", "due_on": "2016-09-30T07:00:00Z", "closed_at": null}, "comments": 1, "created_at": "2016-09-20T01:32:10Z", "updated_at": "2016-09-21T22:22:51Z", "closed_at": "2016-09-21T22:22:46Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Attached is the debug output (below). This blocks my ability to run upload and download tests.\nAdditionally, it is not, by default, creating the tmp files closer to the target file. It is instead still creating them inside of /tmp, which makes it difficult for me to know which segments are associated with which file upload attempt.\n\n[outlogUpload.txt](https://github.com/Azure/azure-data-lake-store-python/files/481450/outlogUpload.txt)\n[outlogDownload.txt](https://github.com/Azure/azure-data-lake-store-python/files/481457/outlogDownload.txt) \n", "performed_via_github_app": null, "score": 1.0}]}