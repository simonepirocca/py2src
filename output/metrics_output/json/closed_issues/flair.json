{"total_count": 1130, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/flairNLP/flair/issues/1823", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1823/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1823/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1823/events", "html_url": "https://github.com/flairNLP/flair/issues/1823", "id": 681674251, "node_id": "MDU6SXNzdWU2ODE2NzQyNTE=", "number": 1823, "title": "Change batch_size but the usage of GPU remains the same.", "user": {"login": "ForLittleBeauty", "id": 34436078, "node_id": "MDQ6VXNlcjM0NDM2MDc4", "avatar_url": "https://avatars3.githubusercontent.com/u/34436078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ForLittleBeauty", "html_url": "https://github.com/ForLittleBeauty", "followers_url": "https://api.github.com/users/ForLittleBeauty/followers", "following_url": "https://api.github.com/users/ForLittleBeauty/following{/other_user}", "gists_url": "https://api.github.com/users/ForLittleBeauty/gists{/gist_id}", "starred_url": "https://api.github.com/users/ForLittleBeauty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ForLittleBeauty/subscriptions", "organizations_url": "https://api.github.com/users/ForLittleBeauty/orgs", "repos_url": "https://api.github.com/users/ForLittleBeauty/repos", "events_url": "https://api.github.com/users/ForLittleBeauty/events{/privacy}", "received_events_url": "https://api.github.com/users/ForLittleBeauty/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-08-19T09:05:10Z", "updated_at": "2020-08-21T07:38:09Z", "closed_at": "2020-08-21T07:38:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nWhen I use GPU to fine-tuning my BERT model,  I use nvidia-smi to check the usage of my GPU,  however, when I double the mini_batch_size, it seems that the usage of GPU remains the same as before smaller mini_batch_size, I am confused, is this a bug or something wrong to me?\r\n\r\n**To Reproduce**\r\n\r\n    trainer.train('resources/taggers/trec',\r\n                learning_rate=3e-5, # use very small learning rate\r\n                mini_batch_size=512,\r\n                mini_batch_chunk_size=32, # optionally set this if transformer is too much for your machine\r\n                max_epochs=50, # terminate after 5 epochs\r\n\t\tembeddings_storage_mode='gpu'\r\n                )\r\n\r\n\r\n\r\n**Expected behavior**\r\nI change mini_batch_size from 16 to 512, the usages of GPU are nearly the same. And I use V100 with 32GB so I do not think the batch_sizes of 16,32,64 will be too large.\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - Centos :\r\n - flair 0.6:\r\n\r\n**Additional context**\r\nBesides, I do not quite understand the usefulness of mini_batch_chunk_size\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1817", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1817/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1817/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1817/events", "html_url": "https://github.com/flairNLP/flair/issues/1817", "id": 680295261, "node_id": "MDU6SXNzdWU2ODAyOTUyNjE=", "number": 1817, "title": "HUNFLAIR paper reference is a placeholder", "user": {"login": "khituras", "id": 4648560, "node_id": "MDQ6VXNlcjQ2NDg1NjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/4648560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khituras", "html_url": "https://github.com/khituras", "followers_url": "https://api.github.com/users/khituras/followers", "following_url": "https://api.github.com/users/khituras/following{/other_user}", "gists_url": "https://api.github.com/users/khituras/gists{/gist_id}", "starred_url": "https://api.github.com/users/khituras/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khituras/subscriptions", "organizations_url": "https://api.github.com/users/khituras/orgs", "repos_url": "https://api.github.com/users/khituras/repos", "events_url": "https://api.github.com/users/khituras/events{/privacy}", "received_events_url": "https://api.github.com/users/khituras/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "mariosaenger", "id": 40803339, "node_id": "MDQ6VXNlcjQwODAzMzM5", "avatar_url": "https://avatars2.githubusercontent.com/u/40803339?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosaenger", "html_url": "https://github.com/mariosaenger", "followers_url": "https://api.github.com/users/mariosaenger/followers", "following_url": "https://api.github.com/users/mariosaenger/following{/other_user}", "gists_url": "https://api.github.com/users/mariosaenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosaenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosaenger/subscriptions", "organizations_url": "https://api.github.com/users/mariosaenger/orgs", "repos_url": "https://api.github.com/users/mariosaenger/repos", "events_url": "https://api.github.com/users/mariosaenger/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosaenger/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosaenger", "id": 40803339, "node_id": "MDQ6VXNlcjQwODAzMzM5", "avatar_url": "https://avatars2.githubusercontent.com/u/40803339?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosaenger", "html_url": "https://github.com/mariosaenger", "followers_url": "https://api.github.com/users/mariosaenger/followers", "following_url": "https://api.github.com/users/mariosaenger/following{/other_user}", "gists_url": "https://api.github.com/users/mariosaenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosaenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosaenger/subscriptions", "organizations_url": "https://api.github.com/users/mariosaenger/orgs", "repos_url": "https://api.github.com/users/mariosaenger/repos", "events_url": "https://api.github.com/users/mariosaenger/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosaenger/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-08-17T14:50:06Z", "updated_at": "2020-08-18T15:04:43Z", "closed_at": "2020-08-18T15:04:43Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "On the page https://github.com/flairNLP/flair/blob/master/resources/docs/HUNFLAIR.md the link to the paper is just a placeholder link at the moment.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1809", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1809/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1809/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1809/events", "html_url": "https://github.com/flairNLP/flair/issues/1809", "id": 678846322, "node_id": "MDU6SXNzdWU2Nzg4NDYzMjI=", "number": 1809, "title": "\"learning rate 0.0000\" in the first epoch, is it ok when training char LM", "user": {"login": "yuye2133", "id": 20791462, "node_id": "MDQ6VXNlcjIwNzkxNDYy", "avatar_url": "https://avatars1.githubusercontent.com/u/20791462?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuye2133", "html_url": "https://github.com/yuye2133", "followers_url": "https://api.github.com/users/yuye2133/followers", "following_url": "https://api.github.com/users/yuye2133/following{/other_user}", "gists_url": "https://api.github.com/users/yuye2133/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuye2133/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuye2133/subscriptions", "organizations_url": "https://api.github.com/users/yuye2133/orgs", "repos_url": "https://api.github.com/users/yuye2133/repos", "events_url": "https://api.github.com/users/yuye2133/events{/privacy}", "received_events_url": "https://api.github.com/users/yuye2133/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-08-14T02:12:51Z", "updated_at": "2020-08-14T03:33:17Z", "closed_at": "2020-08-14T03:33:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "**loss.txt** \r\n\r\n| end of split 262 /442 | epoch   1 | time: 57.05s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0012\r\n| end of split 263 /442 | epoch   1 | time: 57.12s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0012\r\n| end of split 264 /442 | epoch   1 | time: 56.98s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0012\r\n| end of split 265 /442 | epoch   1 | time: 57.03s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0012\r\n| end of split 266 /442 | epoch   1 | time: 57.04s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0012\r\n| end of split 267 /442 | epoch   1 | time: 57.09s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0012\r\n| end of split 268 /442 | epoch   1 | time: 57.02s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 269 /442 | epoch   1 | time: 57.14s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 270 /442 | epoch   1 | time: 57.02s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 271 /442 | epoch   1 | time: 57.01s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 272 /442 | epoch   1 | time: 57.16s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 273 /442 | epoch   1 | time: 57.06s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 274 /442 | epoch   1 | time: 57.17s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 275 /442 | epoch   1 | time: 57.13s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 276 /442 | epoch   1 | time: 57.29s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 277 /442 | epoch   1 | time: 57.04s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 278 /442 | epoch   1 | time: 57.23s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0003\r\n| end of split 279 /442 | epoch   1 | time: 57.02s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 280 /442 | epoch   1 | time: 57.24s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 281 /442 | epoch   1 | time: 57.25s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 282 /442 | epoch   1 | time: 57.00s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 283 /442 | epoch   1 | time: 57.17s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 284 /442 | epoch   1 | time: 57.09s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 285 /442 | epoch   1 | time: 56.99s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 286 /442 | epoch   1 | time: 57.17s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 287 /442 | epoch   1 | time: 57.19s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 288 /442 | epoch   1 | time: 57.06s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 289 /442 | epoch   1 | time: 57.05s | valid loss  2.94 | valid ppl    18.96 | learning rate 0.0001\r\n| end of split 290 /442 | epoch   1 | time: 57.10s | valid loss  2.94 | valid ppl    18.96 | **learning rate 0.0000**\r\n| end of split 291 /442 | epoch   1 | time: 57.25s | valid loss  2.94 | valid ppl    18.96 | **learning rate 0.0000**\r\n| end of split 292 /442 | epoch   1 | time: 57.23s | valid loss  2.94 | valid ppl    18.96 | **learning rate 0.0000**\r\n| end of split 293 /442 | epoch   1 | time: 57.11s | valid loss  2.94 | valid ppl    18.96 | **learning rate 0.0000**\r\n| end of split 294 /442 | epoch   1 | time: 57.46s | valid loss  2.94 | valid ppl    18.96 | **learning rate 0.0000**\r\n| end of split 295 /442 | epoch   1 | time: 57.22s | valid loss  2.94 | valid ppl    18.96 | **learning rate 0.0000**\r\n| end of split 296 /442 | epoch   1 | time: 57.04s | valid loss  2.94 | valid ppl    18.96 | **learning rate 0.0000**", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1799", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1799/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1799/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1799/events", "html_url": "https://github.com/flairNLP/flair/issues/1799", "id": 675974913, "node_id": "MDU6SXNzdWU2NzU5NzQ5MTM=", "number": 1799, "title": "Location of downloaded models and embeddings", "user": {"login": "codechannel", "id": 67317792, "node_id": "MDQ6VXNlcjY3MzE3Nzky", "avatar_url": "https://avatars0.githubusercontent.com/u/67317792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/codechannel", "html_url": "https://github.com/codechannel", "followers_url": "https://api.github.com/users/codechannel/followers", "following_url": "https://api.github.com/users/codechannel/following{/other_user}", "gists_url": "https://api.github.com/users/codechannel/gists{/gist_id}", "starred_url": "https://api.github.com/users/codechannel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/codechannel/subscriptions", "organizations_url": "https://api.github.com/users/codechannel/orgs", "repos_url": "https://api.github.com/users/codechannel/repos", "events_url": "https://api.github.com/users/codechannel/events{/privacy}", "received_events_url": "https://api.github.com/users/codechannel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-10T08:54:32Z", "updated_at": "2020-08-13T12:20:19Z", "closed_at": "2020-08-13T12:20:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "How to locate the path of the downloaded models/embeddings?\r\n\r\nWhile running the following code for the first time, flair downloads the models.\r\n````\r\nfrom flair.models import SequenceTagger\r\nfrom flair.embeddings import TransformerDocumentEmbeddings\r\n\r\ntagger_ner = SequenceTagger.load('de-ner') # location available\r\nembedding = TransformerDocumentEmbeddings('bert-base-uncased')\r\n````\r\n\r\nAm using Ubuntu 18.04. and can locate ner, pos models under ~/.flair/models, but ~/.flair/embeddings/ does not have bert.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1791", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1791/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1791/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1791/events", "html_url": "https://github.com/flairNLP/flair/issues/1791", "id": 672643836, "node_id": "MDU6SXNzdWU2NzI2NDM4MzY=", "number": 1791, "title": "MultiTagger for Multiple Sequence Tagging Models", "user": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-04T09:22:12Z", "updated_at": "2020-08-12T10:12:26Z", "closed_at": "2020-08-12T10:12:26Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "With the upcoming biomedical Flair release (\"hunflair\") comes a new object called `MultiTagger` with which multiple different `SequenceTagger` can be chained. \r\n\r\nThis tasks monitors the optimization of this model, namely: \r\n- If two models use the same static embedding, only one of them should be kept in memory\r\n- If two models use the same static embedding, only one of them should be applied during prediction\r\n\r\nThe idea is to enable us to load several models at once, like this: \r\n\r\n```python\r\n# load tagger for POS, chunking, NER and frame detection\r\ntagger = MultiTagger.load(['pos', 'upos', 'chunk', 'ner', 'frame'])\r\n\r\n# example sentence\r\nsentence = Sentence(\"George Washington was born in Washington\")\r\n\r\n# predict\r\ntagger.predict(sentence)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1788", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1788/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1788/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1788/events", "html_url": "https://github.com/flairNLP/flair/issues/1788", "id": 671858716, "node_id": "MDU6SXNzdWU2NzE4NTg3MTY=", "number": 1788, "title": "\"BertEncoder' object has no attribute 'output_hidden_states\"", "user": {"login": "thanish", "id": 4056145, "node_id": "MDQ6VXNlcjQwNTYxNDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4056145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thanish", "html_url": "https://github.com/thanish", "followers_url": "https://api.github.com/users/thanish/followers", "following_url": "https://api.github.com/users/thanish/following{/other_user}", "gists_url": "https://api.github.com/users/thanish/gists{/gist_id}", "starred_url": "https://api.github.com/users/thanish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thanish/subscriptions", "organizations_url": "https://api.github.com/users/thanish/orgs", "repos_url": "https://api.github.com/users/thanish/repos", "events_url": "https://api.github.com/users/thanish/events{/privacy}", "received_events_url": "https://api.github.com/users/thanish/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-03T07:19:36Z", "updated_at": "2020-08-14T01:52:15Z", "closed_at": "2020-08-14T01:52:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi I have trained a Bert token classification model for the Italian language using the \"dbmdz/bert-base-italian-uncased\". I have trained the model in a machine running Pytorch-1.4.0 and transformer 3.0.2,  when I installed it few days back as it's the latest version. \r\nI copied the saved best model to a server that runs Pytorch-1.4.0 & transformer version 2.3.0. I sent a request to the model to get the predictions, but I got the following warnings.\r\n\r\n## Inference code\r\n```\r\ntokenizer = transformers.BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-uncased\", do_lower_case=False)\r\n\r\n# Assuming I have tokenized the requested text into the variable \"tokens\"\r\n\r\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\r\nsegments_ids = [0] * len(tokens)\r\ntokens_tensor = torch.tensor([indexed_tokens]).to(device)\r\nsegments_tensors = torch.tensor([segments_ids]).to(device)\r\nlogit = model(tokens_tensor, token_type_ids=None, attention_mask=segments_tensors)\r\n\r\n```\r\n## Warnings\r\n```\r\nModel name 'dbmdz/bert-base-italian-uncased' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming 'dbmdz/bert-base-italian-uncased' is a path or url to a directory containing tokenizer files.\r\nDidn't find file dbmdz/bert-base-italian-uncased/added_tokens.json. We won't load it.\r\nDidn't find file dbmdz/bert-base-italian-uncased/special_tokens_map.json. We won't load it.\r\nDidn't find file dbmdz/bert-base-italian-uncased/tokenizer_config.json. We won't load it.\r\nloading file https://s3.amazonaws.com/models.huggingface.co/bert/dbmdz/bert-base-italian-uncased/vocab.txt from cache at /root/.cache/torch/transformers/02b5ab8ef6a3a1d4af18c318bb4c53155a59a3893dd557b922d2467b269cd405.5cbaac66fdfadbe363aad01956dac0be9bf700f2c8c87012dc078b87e2fa4181\r\nloading file None\r\nloading file None\r\nloading file None\r\n\r\n```\r\n```\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertForTokenClassification' has changed. Saved a reverse patch to BertForTokenClassification.patch. Run `patch -p0 < BertForTokenClassification.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertModel' has changed. Saved a reverse patch to BertModel.patch. Run `patch -p0 < BertModel.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertEmbeddings' has changed. Saved a reverse patch to BertEmbeddings.patch. Run `patch -p0 < BertEmbeddings.patch` to revert your changes.  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.normalization.LayerNorm' has changed. Saved a reverse patch to LayerNorm.patch. Run `patch -p0 < LayerNorm.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertEncoder' has changed. Saved a reverse patch to BertEncoder.patch. Run `patch -p0 < BertEncoder.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. Saved a reverse patch to ModuleList.patch. Run `patch -p0 < ModuleList.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertLayer' has changed. Saved a reverse patch to BertLayer.patch. Run `patch -p0 < BertLayer.patch` to revert your changes.  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertAttention' has changed. Saved a reverse patch to BertAttention.patch. Run `patch -p0 < BertAttention.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertSelfAttention' has changed. Saved a reverse patch to BertSelfAttention.patch. Run `patch -p0 < BertSelfAttention.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. Saved a reverse patch to Linear.patch. Run `patch -p0 < Linear.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertSelfOutput' has changed. Saved a reverse patch to BertSelfOutput.patch. Run `patch -p0 < BertSelfOutput.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertIntermediate' has changed. Saved a reverse patch to BertIntermediate.patch. Run `patch -p0 < BertIntermediate.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertOutput' has changed. Saved a reverse patch to BertOutput.patch. Run `patch -p0 < BertOutput.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'transformers.modeling_bert.BertPooler' has changed. Saved a reverse patch to BertPooler.patch. Run `patch -p0 < BertPooler.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n./torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.Tanh' has changed. Saved a reverse patch to Tanh.patch. Run `patch -p0 < Tanh.patch` to revert your changes.\r\n  warnings.warn(msg, SourceChangeWarning)\r\n```\r\n\r\nand finally it ended with the below error.\r\n\r\n```\r\n\"BertEncoder' object has no attribute 'output_hidden_states\".\r\n```\r\n\r\nCan someone help me understand Is it because of the Pytorch, transformer version mismatch between the trained model on a machine and the inference on the server? or if \"dbmdz/bert-base-italian-uncased\" is available in the 2.3.0 version or not? or is there any other way I can make this work instead of retraining the model at a lower version to match the version of the server? \r\nAssuming that changing the versions in the server is not quite possible as of now.\r\nAppreciate your help.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1782", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1782/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1782/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1782/events", "html_url": "https://github.com/flairNLP/flair/issues/1782", "id": 667716970, "node_id": "MDU6SXNzdWU2Njc3MTY5NzA=", "number": 1782, "title": "Fail to download DANE() dataset", "user": {"login": "maziyarpanahi", "id": 5762953, "node_id": "MDQ6VXNlcjU3NjI5NTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5762953?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maziyarpanahi", "html_url": "https://github.com/maziyarpanahi", "followers_url": "https://api.github.com/users/maziyarpanahi/followers", "following_url": "https://api.github.com/users/maziyarpanahi/following{/other_user}", "gists_url": "https://api.github.com/users/maziyarpanahi/gists{/gist_id}", "starred_url": "https://api.github.com/users/maziyarpanahi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maziyarpanahi/subscriptions", "organizations_url": "https://api.github.com/users/maziyarpanahi/orgs", "repos_url": "https://api.github.com/users/maziyarpanahi/repos", "events_url": "https://api.github.com/users/maziyarpanahi/events{/privacy}", "received_events_url": "https://api.github.com/users/maziyarpanahi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-29T10:03:47Z", "updated_at": "2020-08-12T10:37:04Z", "closed_at": "2020-08-12T10:37:03Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\nThe command `flair.datasets.DANE()` fails with 403 status code\r\n\r\n**To Reproduce**\r\n```python\r\nimport flair.datasets\r\n\r\nflair.datasets.DANE()\r\n```\r\n**Expected behavior**\r\nIt should download the Danish NER datasets in train, dev, and test.\r\n\r\n**Screenshots**\r\n![image](https://user-images.githubusercontent.com/5762953/88786970-774aa600-d193-11ea-8ba7-88b37b0347ff.png)\r\n\r\n**Environment (please complete the following information):**\r\n - OS [e.g. iOS, Linux]: macOS Catalina\r\n - Version [e.g. flair-0.3.2]: 0.5.1\r\n\r\n**Additional context**\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1774", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1774/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1774/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1774/events", "html_url": "https://github.com/flairNLP/flair/issues/1774", "id": 664993963, "node_id": "MDU6SXNzdWU2NjQ5OTM5NjM=", "number": 1774, "title": "Is TransformerDocumentEmbeddings sentence length is limited to 512 tokens? ", "user": {"login": "olanag1", "id": 27998731, "node_id": "MDQ6VXNlcjI3OTk4NzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/27998731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olanag1", "html_url": "https://github.com/olanag1", "followers_url": "https://api.github.com/users/olanag1/followers", "following_url": "https://api.github.com/users/olanag1/following{/other_user}", "gists_url": "https://api.github.com/users/olanag1/gists{/gist_id}", "starred_url": "https://api.github.com/users/olanag1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olanag1/subscriptions", "organizations_url": "https://api.github.com/users/olanag1/orgs", "repos_url": "https://api.github.com/users/olanag1/repos", "events_url": "https://api.github.com/users/olanag1/events{/privacy}", "received_events_url": "https://api.github.com/users/olanag1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-24T08:09:20Z", "updated_at": "2020-08-12T23:09:09Z", "closed_at": "2020-08-12T23:09:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, \r\n\r\nIn the class _TransformerDocumentEmbeddings__ in the method __add_embeddings_to_sentences_ there is an indication that the sentence is truncated up to 512 tokens (max_length=512). \r\n\r\n> \r\n             # subtokenize sentences\r\n\r\n            for sentence in sentences:\r\n                # tokenize and truncate to 512 subtokens (TODO: check better truncation strategies)\r\n                subtokenized_sentence = self.tokenizer.encode(sentence.to_tokenized_string(),\r\n                                                              add_special_tokens=True,\r\n                                                              max_length=512,\r\n                                                              truncation=True,\r\n                                                              )\r\n\r\nSo, is only 512 tokens are taken into account in order to calculate the sentence embeddings ? Or the whole document is divided on the sequences of 512 tokens and then all the document is parsed to get the final embedding? I mean, in the case if the document is longer than 512 tokens.\r\n\r\nAnother question, how the token is determined, is it a character, word, sub-word ? \r\n\r\nThank for your help :)\r\nAnd thanks for a developer team, very useful library :) ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1768", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1768/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1768/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1768/events", "html_url": "https://github.com/flairNLP/flair/issues/1768", "id": 659995626, "node_id": "MDU6SXNzdWU2NTk5OTU2MjY=", "number": 1768, "title": "Inconsistent tokenizer use cause bad predictions ...", "user": {"login": "sankaran45", "id": 8388863, "node_id": "MDQ6VXNlcjgzODg4NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/8388863?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sankaran45", "html_url": "https://github.com/sankaran45", "followers_url": "https://api.github.com/users/sankaran45/followers", "following_url": "https://api.github.com/users/sankaran45/following{/other_user}", "gists_url": "https://api.github.com/users/sankaran45/gists{/gist_id}", "starred_url": "https://api.github.com/users/sankaran45/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sankaran45/subscriptions", "organizations_url": "https://api.github.com/users/sankaran45/orgs", "repos_url": "https://api.github.com/users/sankaran45/repos", "events_url": "https://api.github.com/users/sankaran45/events{/privacy}", "received_events_url": "https://api.github.com/users/sankaran45/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-18T08:08:43Z", "updated_at": "2020-08-13T13:45:32Z", "closed_at": "2020-08-13T13:10:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI have a CSV training/test files that i use CSVClassificationCorpus to load and then train etc. The evaluate that runs after training works fine. Then i manually load the CSV file and for each line, i call Sentence(...) and then pass it to predict function. This time the results are arbitrary and poor. \r\n\r\nI looked at it a bit, and it turned out that by default Sentence uses SpaceTokenizer (if no use_tokenizer parameter) is passed.\r\n\r\nOTOH, CSVClassificationCorpus uses SegtokTokenizer by default ...\r\n\r\nLeading to completely different results in the default case of not specifying these parameters.\r\n\r\nSo i fixed it by passing use_tokenize=SegtokTokenizer to my Sentence call before invoking predict\r\n\r\nQuite counter-intutitive .. not necessarily a bug but posting in case some one else runs into same issue\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1767", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1767/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1767/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1767/events", "html_url": "https://github.com/flairNLP/flair/issues/1767", "id": 659932971, "node_id": "MDU6SXNzdWU2NTk5MzI5NzE=", "number": 1767, "title": "Loading checkpoint created with Flair0.4.5 fails in new Flair 0.5.1", "user": {"login": "sankaran45", "id": 8388863, "node_id": "MDQ6VXNlcjgzODg4NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/8388863?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sankaran45", "html_url": "https://github.com/sankaran45", "followers_url": "https://api.github.com/users/sankaran45/followers", "following_url": "https://api.github.com/users/sankaran45/following{/other_user}", "gists_url": "https://api.github.com/users/sankaran45/gists{/gist_id}", "starred_url": "https://api.github.com/users/sankaran45/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sankaran45/subscriptions", "organizations_url": "https://api.github.com/users/sankaran45/orgs", "repos_url": "https://api.github.com/users/sankaran45/repos", "events_url": "https://api.github.com/users/sankaran45/events{/privacy}", "received_events_url": "https://api.github.com/users/sankaran45/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-18T05:57:56Z", "updated_at": "2020-07-31T14:40:23Z", "closed_at": "2020-07-31T14:40:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI have an old checkpoint file created with Flair 0.4.5 that i tried to load with Flair 0.5.1 and it fails\r\n\r\nMainly its due to the issue here in text_classification_model .. if i comment out the last two lines it works fine .. i suspect they probably don't exist in the old checkpoint ???\r\n\r\n    def __str__(self):\r\n        return super(flair.nn.Model, self).__str__().rstrip(')') + \\\r\n               f'  (beta): {self.beta}\\n' + \\\r\n               f'  (weights): {self.weight_dict}\\n' + \\\r\n               f'  (weight_tensor) {self.loss_weights}\\n)'\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1765", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1765/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1765/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1765/events", "html_url": "https://github.com/flairNLP/flair/issues/1765", "id": 659721776, "node_id": "MDU6SXNzdWU2NTk3MjE3NzY=", "number": 1765, "title": "Wassa - UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 2280: character maps to <undefined>", "user": {"login": "Greg-Tarr", "id": 43277605, "node_id": "MDQ6VXNlcjQzMjc3NjA1", "avatar_url": "https://avatars1.githubusercontent.com/u/43277605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Greg-Tarr", "html_url": "https://github.com/Greg-Tarr", "followers_url": "https://api.github.com/users/Greg-Tarr/followers", "following_url": "https://api.github.com/users/Greg-Tarr/following{/other_user}", "gists_url": "https://api.github.com/users/Greg-Tarr/gists{/gist_id}", "starred_url": "https://api.github.com/users/Greg-Tarr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Greg-Tarr/subscriptions", "organizations_url": "https://api.github.com/users/Greg-Tarr/orgs", "repos_url": "https://api.github.com/users/Greg-Tarr/repos", "events_url": "https://api.github.com/users/Greg-Tarr/events{/privacy}", "received_events_url": "https://api.github.com/users/Greg-Tarr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-17T23:11:21Z", "updated_at": "2020-08-04T20:01:28Z", "closed_at": "2020-08-04T20:01:28Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\nIt is a UnicodeDecodeError that occurs when using the flair.datasets.WASSA datasets.\r\n\r\nCan be seen here:\r\n\r\n> \u03bb python wassa-sad.py\r\n2020-07-17 23:49:59,174 http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/sadness-ratings-0to1.train.txt not found in cache, downloading to C:\\Users\\me\\AppData\\Local\\Temp\\tmpqf9ay00c\r\n92977B [00:00, 211589.48B/s]\r\n2020-07-17 23:49:59,950 copying C:\\Users\\me\\AppData\\Local\\Temp\\tmpqf9ay00c to cache at C:\\Users\\me\\.flair\\datasets\\wassa_sadness\\sadness-ratings-0to1.train.txt\r\n2020-07-17 23:49:59,960 removing temp file C:\\Users\\me\\AppData\\Local\\Temp\\tmpqf9ay00c\r\nTraceback (most recent call last):\r\n  File \"wassa-sadness2-pooled.py\", line 15, in <module>\r\n    corpus = WASSA_SADNESS()\r\n  File \"D:\\Programs\\Python\\Python37\\lib\\site-packages\\flair\\datasets\\document_classification.py\", line 1607, in __init__\r\n    _download_wassa_if_not_there(\"sadness\", data_folder, dataset_name)\r\n  File \"D:\\Programs\\Python\\Python37\\lib\\site-packages\\flair\\datasets\\document_classification.py\", line 1466, in _download_wassa_if_not_there\r\n    next(f)\r\n  File \"D:\\Programs\\Python\\Python37\\lib\\encodings\\cp1252.py\", line 23, in decode\r\n    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\r\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 2280: character maps to <undefined>\r\n\r\n**To Reproduce**\r\n`from flair.datasets import WASSA_SADNESS`\r\n`corpus = WASSA_SADNESS()`\r\n\r\n**Expected behavior**\r\nThe above error will occur due to the encoding left unspecified on lines 1464, 1465 in flair.datasets.document_classification.py\r\n\r\n**Environment (please complete the following information):**\r\n - OS [e.g. iOS, Linux]: Windows 10 - Python 3.7.4\r\n - Version [e.g. flair-0.3.2]: 0.5.1 - but appears in non-master branches\r\n\r\n**Additional context**\r\nI changed the lines 1464, 1465 in flair.datasets.document_classification.py to include the encoding when opening files and it has solved the problem:\r\n\r\n```\r\n1464:   with open(path, \"r\", encoding=\"UTF-8\") as f:\r\n1465:       with open(data_file, \"w\", encoding=\"UTF-8\") as out:\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1757", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1757/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1757/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1757/events", "html_url": "https://github.com/flairNLP/flair/issues/1757", "id": 657094561, "node_id": "MDU6SXNzdWU2NTcwOTQ1NjE=", "number": 1757, "title": "reduce memory usage by sharing embeddings between multiple classifiers", "user": {"login": "mondor", "id": 841209, "node_id": "MDQ6VXNlcjg0MTIwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/841209?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mondor", "html_url": "https://github.com/mondor", "followers_url": "https://api.github.com/users/mondor/followers", "following_url": "https://api.github.com/users/mondor/following{/other_user}", "gists_url": "https://api.github.com/users/mondor/gists{/gist_id}", "starred_url": "https://api.github.com/users/mondor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mondor/subscriptions", "organizations_url": "https://api.github.com/users/mondor/orgs", "repos_url": "https://api.github.com/users/mondor/repos", "events_url": "https://api.github.com/users/mondor/events{/privacy}", "received_events_url": "https://api.github.com/users/mondor/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-15T06:24:14Z", "updated_at": "2020-08-12T14:04:27Z", "closed_at": "2020-08-12T14:04:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "A clear and concise description of what you want to know.\r\nHi there,\r\n\r\nThank you for sharing this wonderful library to the world. I have a question regarding using this library for inferencing. \r\n\r\nWe have multiple classifiers running on the same inferencing API server, all the classifiers were loaded into memory when the application starts, all classifiers were using the same embedding. \r\n\r\nIn order to minimise the memory usage on the server, is there anyway we can share the embeddings between multiple classifiers? \r\n\r\n\r\n```python\r\nword_embeddings = [WordEmbeddings('en'), BytePairEmbeddings('en')]\r\n\r\nclassifier1 =  TextClassifier(\r\n        DocumentRNNEmbeddings( \r\n                word_embeddings,\r\n                hidden_size=128,\r\n                reproject_words=True,\r\n                reproject_words_dimension=128\r\n        ), \r\n        label_dictionary=[a, b],\r\n        multi_label=True\r\n) \r\nclassifier1.load_state_dict(torch.load(...))\r\n\r\nclassifier2 =  TextClassifier(\r\n        DocumentRNNEmbeddings( \r\n                word_embeddings,\r\n                hidden_size=128,\r\n                reproject_words=True,\r\n                reproject_words_dimension=128\r\n        ), \r\n        label_dictionary=[c, d, e, f, g],\r\n        multi_label=True\r\n)\r\nclassifier2.load_state_dict(torch.load(...))\r\n\r\nclassifier3 =  TextClassifier(\r\n        DocumentRNNEmbeddings( \r\n                word_embeddings,\r\n                hidden_size=128,\r\n                reproject_words=True,\r\n                reproject_words_dimension=128\r\n        ), \r\n        label_dictionary=[h,i,j,k],\r\n        multi_label=True\r\n) \r\nclassifier3.load_state_dict(torch.load(...))\r\n \r\n```\r\n\r\nIf I am understanding it correctly, the embeddings weight is static, non-trainable, and they are not saved into the classifier's checkpoint file, whereas the RNN layer (DocumentRNNEmbeddings) and Fully connected layer are uniquely to each classifier. Would loading the classifiers this way reduce the memory foot print for this API server?\r\n\r\nAppreciate your helps.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1756", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1756/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1756/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1756/events", "html_url": "https://github.com/flairNLP/flair/issues/1756", "id": 655327990, "node_id": "MDU6SXNzdWU2NTUzMjc5OTA=", "number": 1756, "title": "Flair prediction speed and allowed maximum number of tags of predicted sentences", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-12T04:19:27Z", "updated_at": "2020-07-12T09:35:12Z", "closed_at": "2020-07-12T09:35:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'd like to ask a few questions about flair embedding. \r\n\r\n- First, I'd like to know is there any options about **increasing prediction speed** of the model that uses **flair embedding**. I've trained the POS model with stacked embedding of flair embedding(with hidden state of 2048) and word embedding and the result is pretty good. But the drawback is that it takes **2 or 3** seconds for **normal** sentence and takes **longer** when predicting **longer** sentences. I saw some answers like prediction with **batch processing** and **sentence breaking** but for the language I've trained, sentence breaking is not simple as Latin based languages and prediction with batch processing makes faster(between 15 and 25 sentences per second) but I need more speed. Are there any options about increasing prediction speed of the model that uses flair embedding besides retrained flair embedding with lower hidden states??\r\n- Second, before I use flair, I used to train the POS model with scratch training. At that time, I needed to set maximum allowed tags during training and during prediction, my scratch trained model can't predict the sentences with more than maximum allowed tags. When I started to use and train models in flair framework, what I found out that I don't need to care about the **maximum** allowed tags in prediction(besides transformer model. In the transformer model  it can only predict  maximum 512 tags of sentence). I mean mdoels trained using flair framework can predict the **unlimited** number of tags in sentence. What I'd like to know is that how the models from flair framework can handle unlimited number of tags??\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1754", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1754/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1754/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1754/events", "html_url": "https://github.com/flairNLP/flair/issues/1754", "id": 655172115, "node_id": "MDU6SXNzdWU2NTUxNzIxMTU=", "number": 1754, "title": "Choosing how to choose tag_type for custom tagging system", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-11T09:25:39Z", "updated_at": "2020-07-11T12:02:57Z", "closed_at": "2020-07-11T12:02:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I'd like to ask about how to choose parameter **tag_type** for custom sequence labeling training task . For my sequence labeling task, there are only two tags, B (begin tag) and I (inner tag). Currently I choose '**ner**' for tag_type in this  training task. But in my mind, are there any options about tag_type for this kind of task??", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1753", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1753/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1753/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1753/events", "html_url": "https://github.com/flairNLP/flair/issues/1753", "id": 654060707, "node_id": "MDU6SXNzdWU2NTQwNjA3MDc=", "number": 1753, "title": "classifier unable to make predictions when using elmo embeddings", "user": {"login": "ud2195", "id": 42403625, "node_id": "MDQ6VXNlcjQyNDAzNjI1", "avatar_url": "https://avatars2.githubusercontent.com/u/42403625?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ud2195", "html_url": "https://github.com/ud2195", "followers_url": "https://api.github.com/users/ud2195/followers", "following_url": "https://api.github.com/users/ud2195/following{/other_user}", "gists_url": "https://api.github.com/users/ud2195/gists{/gist_id}", "starred_url": "https://api.github.com/users/ud2195/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ud2195/subscriptions", "organizations_url": "https://api.github.com/users/ud2195/orgs", "repos_url": "https://api.github.com/users/ud2195/repos", "events_url": "https://api.github.com/users/ud2195/events{/privacy}", "received_events_url": "https://api.github.com/users/ud2195/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-09T13:27:45Z", "updated_at": "2020-08-12T21:15:08Z", "closed_at": "2020-08-12T21:15:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I trained my text classifier using `ELMO pubmed` embeddings after training the model i am unable to predict on a gpu instance. The error and the code are mentioned below:-\r\n\r\nVersions:-\r\n```\r\nallennlp== 0.9.0\r\nflair== 0.5.1\r\n\r\n```\r\ncode-\r\n\r\n```\r\nfrom flair.models import TextClassifier\r\nfrom flair.data import Sentence\r\nimport pandas as pd \r\nclassifier = TextClassifier.load('/content/drive/My Drive/best-model (2).pt')\r\ndata=pd.read_csv('/content/sample_prediction.csv')\r\ndata.head()\r\npred=[]\r\n\r\nfor index, sample in data.iterrows():\r\n    sentence=Sentence(sample['sent'])\r\n    classifier.predict(sentence)\r\n    print(sentence.labels[0].value) \r\n    pred.append(sentence.labels[0].value)    \r\n\r\n```\r\n\r\ntraceback:-\r\n```\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-21-83d7f894d041> in <module>()\r\n      \r\n---->  classifier.predict(sentence)\r\n\r\n12 frames\r\n/usr/local/lib/python3.6/dist-packages/flair/models/text_classification_model.py in predict(self, sentences, mini_batch_size, multi_class_prob, verbose, label_name, return_loss, embedding_storage_mode)\r\n    220                     continue\r\n    221 \r\n--> 222                 scores = self.forward(batch)\r\n    223 \r\n    224                 if return_loss:\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/models/text_classification_model.py in forward(self, sentences)\r\n     97     def forward(self, sentences):\r\n     98 \r\n---> 99         self.document_embeddings.embed(sentences)\r\n    100 \r\n    101         embedding_names = self.document_embeddings.get_names()\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/embeddings/legacy.py in embed(self, sentences)\r\n   1423         sentences.sort(key=lambda x: len(x), reverse=True)\r\n   1424 \r\n-> 1425         self.embeddings.embed(sentences)\r\n   1426 \r\n   1427         # first, sort sentences by number of tokens\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/embeddings/token.py in embed(self, sentences, static_embeddings)\r\n     69 \r\n     70         for embedding in self.embeddings:\r\n---> 71             embedding.embed(sentences)\r\n     72 \r\n     73     @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/embeddings/base.py in embed(self, sentences)\r\n     59 \r\n     60         if not everything_embedded or not self.static_embeddings:\r\n---> 61             self._add_embeddings_internal(sentences)\r\n     62 \r\n     63         return sentences\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/embeddings/token.py in _add_embeddings_internal(self, sentences)\r\n   1702             sentence_words.append([token.text for token in sentence])\r\n   1703 \r\n-> 1704         embeddings = self.ee.embed_batch(sentence_words)\r\n   1705 \r\n   1706         for i, sentence in enumerate(sentences):\r\n\r\n/usr/local/lib/python3.6/dist-packages/allennlp/commands/elmo.py in embed_batch(self, batch)\r\n    253         if batch == [[]]:\r\n    254             elmo_embeddings.append(empty_embedding())\r\n--> 255         else:\r\n    256             embeddings, mask = self.batch_to_embeddings(batch)\r\n    257             for i in range(len(batch)):\r\n\r\n/usr/local/lib/python3.6/dist-packages/allennlp/commands/elmo.py in batch_to_embeddings(self, batch)\r\n    195         if self.cuda_device >= 0:\r\n    196             character_ids = character_ids.cuda(device=self.cuda_device)\r\n--> 197 \r\n    198         bilm_output = self.elmo_bilm(character_ids)\r\n    199         layer_activations = bilm_output['activations']\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    548             result = self._slow_forward(*input, **kwargs)\r\n    549         else:\r\n--> 550             result = self.forward(*input, **kwargs)\r\n    551         for hook in self._forward_hooks.values():\r\n    552             hook_result = hook(self, input, result)\r\n\r\n/usr/local/lib/python3.6/dist-packages/allennlp/modules/elmo.py in forward(self, inputs, word_inputs)\r\n    605                 type_representation = token_embedding['token_embedding']\r\n    606         else:\r\n--> 607             token_embedding = self._token_embedder(inputs)\r\n    608             mask = token_embedding['mask']\r\n    609             type_representation = token_embedding['token_embedding']\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    548             result = self._slow_forward(*input, **kwargs)\r\n    549         else:\r\n--> 550             result = self.forward(*input, **kwargs)\r\n    551         for hook in self._forward_hooks.values():\r\n    552             hook_result = hook(self, input, result)\r\n\r\n/usr/local/lib/python3.6/dist-packages/allennlp/modules/elmo.py in forward(self, inputs)\r\n    357         character_embedding = torch.nn.functional.embedding(\r\n    358                 character_ids_with_bos_eos.view(-1, max_chars_per_token),\r\n--> 359                 self._char_embedding_weights\r\n    360         )\r\n    361 \r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\r\n   1722         # remove once script supports set_grad_enabled\r\n   1723         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\n-> 1724     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n   1725 \r\n   1726 \r\n\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select\r\n```\r\nwill really appreciate if someone can please help me out with this ? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1751", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1751/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1751/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1751/events", "html_url": "https://github.com/flairNLP/flair/issues/1751", "id": 654010700, "node_id": "MDU6SXNzdWU2NTQwMTA3MDA=", "number": 1751, "title": "Long sentences while training SequenceTagger with TransformerWordEmbeddings", "user": {"login": "djstrong", "id": 1849959, "node_id": "MDQ6VXNlcjE4NDk5NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1849959?v=4", "gravatar_id": "", "url": "https://api.github.com/users/djstrong", "html_url": "https://github.com/djstrong", "followers_url": "https://api.github.com/users/djstrong/followers", "following_url": "https://api.github.com/users/djstrong/following{/other_user}", "gists_url": "https://api.github.com/users/djstrong/gists{/gist_id}", "starred_url": "https://api.github.com/users/djstrong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/djstrong/subscriptions", "organizations_url": "https://api.github.com/users/djstrong/orgs", "repos_url": "https://api.github.com/users/djstrong/repos", "events_url": "https://api.github.com/users/djstrong/events{/privacy}", "received_events_url": "https://api.github.com/users/djstrong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-07-09T12:10:46Z", "updated_at": "2020-08-18T14:53:50Z", "closed_at": "2020-07-09T12:57:13Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Most of transformer models handle texts up to 512 subtokens. If we train SequenceTagger with a longer sentence then we get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_tagger.py\", line 240, in <module>\r\n    anneal_with_restarts=args.anneal_with_restarts\r\n  File \"venv/lib/python3.7/site-packages/flair/trainers/trainer.py\", line 349, in train\r\n    loss = self.model.forward_loss(batch_step)\r\n  File \"venv/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py\", line 599, in forward_loss\r\n    features = self.forward(data_points)\r\n  File \"venv/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py\", line 604, in forward\r\n    self.embeddings.embed(sentences)\r\n  File \"venv/lib/python3.7/site-packages/flair/embeddings/token.py\", line 71, in embed\r\n    embedding.embed(sentences)\r\n  File \"venv/lib/python3.7/site-packages/flair/embeddings/base.py\", line 61, in embed\r\n    self._add_embeddings_internal(sentences)\r\n  File \"venv/lib/python3.7/site-packages/flair/embeddings/token.py\", line 897, in _add_embeddings_internal\r\n    self._add_embeddings_to_sentences(batch)\r\n  File \"venv/lib/python3.7/site-packages/flair/embeddings/token.py\", line 1053, in _add_embeddings_to_sentences\r\n    final_embedding: torch.FloatTensor = current_embeddings[0]\r\nIndexError: index 0 is out of bounds for dimension 0 with size 0\r\n```\r\nI am using flair master.\r\n\r\nIs there any automatic way of truncating such sentences?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1750", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1750/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1750/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1750/events", "html_url": "https://github.com/flairNLP/flair/issues/1750", "id": 653928229, "node_id": "MDU6SXNzdWU2NTM5MjgyMjk=", "number": 1750, "title": "Incorrect corpus name for swedish", "user": {"login": "bohanzhou", "id": 46751976, "node_id": "MDQ6VXNlcjQ2NzUxOTc2", "avatar_url": "https://avatars3.githubusercontent.com/u/46751976?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bohanzhou", "html_url": "https://github.com/bohanzhou", "followers_url": "https://api.github.com/users/bohanzhou/followers", "following_url": "https://api.github.com/users/bohanzhou/following{/other_user}", "gists_url": "https://api.github.com/users/bohanzhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/bohanzhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bohanzhou/subscriptions", "organizations_url": "https://api.github.com/users/bohanzhou/orgs", "repos_url": "https://api.github.com/users/bohanzhou/repos", "events_url": "https://api.github.com/users/bohanzhou/events{/privacy}", "received_events_url": "https://api.github.com/users/bohanzhou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-09T09:54:06Z", "updated_at": "2020-07-09T10:07:45Z", "closed_at": "2020-07-09T10:07:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nFirst of all, thank you for this awesome library \ud83d\ude04 \r\n\r\nI want to inform you that the instructions for loading the swedish corpus (#1652) is incorrect in the release notes for version 0.5.1\r\nIn the release notes it is stated that you load the corpus with `corpus = SWEDISH_NER()`. However the correct way should be `corpus = NER_SWEDISH()`  \r\n\r\nHopefully this will save time for others trying to find the problem\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1748", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1748/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1748/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1748/events", "html_url": "https://github.com/flairNLP/flair/issues/1748", "id": 653853933, "node_id": "MDU6SXNzdWU2NTM4NTM5MzM=", "number": 1748, "title": "TextClassifier : reported metrics after training always report precision=1.0", "user": {"login": "GuillaumeDD", "id": 6253295, "node_id": "MDQ6VXNlcjYyNTMyOTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/6253295?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GuillaumeDD", "html_url": "https://github.com/GuillaumeDD", "followers_url": "https://api.github.com/users/GuillaumeDD/followers", "following_url": "https://api.github.com/users/GuillaumeDD/following{/other_user}", "gists_url": "https://api.github.com/users/GuillaumeDD/gists{/gist_id}", "starred_url": "https://api.github.com/users/GuillaumeDD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GuillaumeDD/subscriptions", "organizations_url": "https://api.github.com/users/GuillaumeDD/orgs", "repos_url": "https://api.github.com/users/GuillaumeDD/repos", "events_url": "https://api.github.com/users/GuillaumeDD/events{/privacy}", "received_events_url": "https://api.github.com/users/GuillaumeDD/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-09T08:04:27Z", "updated_at": "2020-08-19T02:47:03Z", "closed_at": "2020-07-09T10:29:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nThe reported metrics after training always report precision=1.0.\r\n\r\n**To Reproduce**\r\n\r\nTraining code:\r\n```python\r\nfrom torch.optim.adam import Adam\r\n\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import TREC_6\r\nfrom flair.embeddings import TransformerDocumentEmbeddings\r\nfrom flair.models import TextClassifier\r\nfrom flair.trainers import ModelTrainer\r\n\r\n# 1. get the corpus\r\ncorpus: Corpus = TREC_6()\r\n\r\n# 2. create the label dictionary\r\nlabel_dict = corpus.make_label_dictionary()\r\n\r\n# 3. initialize transformer document embeddings (many models are available)\r\ndocument_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\r\n\r\n# 4. create the text classifier\r\nclassifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\r\n\r\n# 5. initialize the text classifier trainer with Adam optimizer\r\ntrainer = ModelTrainer(classifier, corpus, optimizer=Adam)\r\n\r\n# 6. start the training\r\ntrainer.train('/tmp/taggers/trec',\r\n              learning_rate=3e-5, # use very small learning rate\r\n              mini_batch_size=16,\r\n              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\r\n              max_epochs=5, # terminate after 5 epochs\r\n              )\r\n ```\r\n\r\nExample of produced report:\r\n```text\r\n2020-07-09 09:50:21,395 Testing using best model ...\r\n2020-07-09 09:50:21,395 loading file /tmp/taggers/trec/best-model.pt\r\n2020-07-09 09:50:27,486         0.964\r\n2020-07-09 09:50:27,487 \r\nResults:\r\n- F-score (micro) 0.9823\r\n- F-score (macro) 0.9745\r\n- Accuracy 0.964\r\n\r\nBy class:\r\n              precision    recall  f1-score   support\r\n\r\n        DESC     1.0000    0.9931    0.9965       145\r\n        ENTY     1.0000    0.8750    0.9333        96\r\n        ABBR     1.0000    0.8889    0.9412         9\r\n         HUM     1.0000    0.9851    0.9925        67\r\n         NUM     1.0000    0.9915    0.9957       117\r\n         LOC     1.0000    0.9762    0.9880        84\r\n\r\n   micro avg     1.0000    0.9653    0.9823       518\r\n   macro avg     1.0000    0.9516    0.9745       518\r\nweighted avg     1.0000    0.9653    0.9818       518\r\n samples avg     1.0000    0.9820    0.9880       518\r\n\r\n2020-07-09 09:50:27,487 ----------------------------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n**Expected behavior**\r\nReports correct metrics.\r\n\r\n**Screenshots**\r\nN/A\r\n\r\n**Environment (please complete the following information):**\r\n - OS [e.g. iOS, Linux]: CentOS\r\n - Version: flair 0.5.1, scikit-learn==0.23.1\r\n\r\n**Additional context**\r\nSame problem with other datasets.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1746", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1746/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1746/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1746/events", "html_url": "https://github.com/flairNLP/flair/issues/1746", "id": 652794978, "node_id": "MDU6SXNzdWU2NTI3OTQ5Nzg=", "number": 1746, "title": "use_crf = False/True", "user": {"login": "hongjiedai", "id": 1285729, "node_id": "MDQ6VXNlcjEyODU3Mjk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1285729?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongjiedai", "html_url": "https://github.com/hongjiedai", "followers_url": "https://api.github.com/users/hongjiedai/followers", "following_url": "https://api.github.com/users/hongjiedai/following{/other_user}", "gists_url": "https://api.github.com/users/hongjiedai/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongjiedai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongjiedai/subscriptions", "organizations_url": "https://api.github.com/users/hongjiedai/orgs", "repos_url": "https://api.github.com/users/hongjiedai/repos", "events_url": "https://api.github.com/users/hongjiedai/events{/privacy}", "received_events_url": "https://api.github.com/users/hongjiedai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-08T01:02:13Z", "updated_at": "2020-07-08T01:06:26Z", "closed_at": "2020-07-08T01:06:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "A clear and concise description of what you want to know.\r\nI try to conduct an experiment to compare the performance of the model w/ and w/o the CRF layer.\r\nI used the following code to calculate the number of parameters.\r\n\r\n```\r\ndef count_parameters(model):\r\n    print(f'Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\r\n```\r\nBut I found that there is no difference between the two models.\r\nThe output of the model architecture is also the same:\r\n\r\n```\r\n2020-07-08 00:55:04,134 Model: \"SequenceTagger(\r\n  (embeddings): StackedEmbeddings(\r\n    (list_embedding_0): WordEmbeddings('glove')\r\n    (list_embedding_1): RoBERTaEmbeddings(\r\n      (model): RobertaModel(\r\n        (embeddings): RobertaEmbeddings(\r\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\r\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\r\n          (token_type_embeddings): Embedding(1, 768)\r\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n          (dropout): Dropout(p=0.1, inplace=False)\r\n        )\r\n        (encoder): BertEncoder(\r\n          (layer): ModuleList(\r\n            (0): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (1): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (2): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (3): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (4): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (5): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (6): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (7): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (8): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (9): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (10): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n            (11): BertLayer(\r\n              (attention): BertAttention(\r\n                (self): BertSelfAttention(\r\n                  (query): Linear(in_features=768, out_features=768, bias=True)\r\n                  (key): Linear(in_features=768, out_features=768, bias=True)\r\n                  (value): Linear(in_features=768, out_features=768, bias=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n                (output): BertSelfOutput(\r\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\r\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                  (dropout): Dropout(p=0.1, inplace=False)\r\n                )\r\n              )\r\n              (intermediate): BertIntermediate(\r\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n              )\r\n              (output): BertOutput(\r\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n                (dropout): Dropout(p=0.1, inplace=False)\r\n              )\r\n            )\r\n          )\r\n        )\r\n        (pooler): BertPooler(\r\n          (dense): Linear(in_features=768, out_features=768, bias=True)\r\n          (activation): Tanh()\r\n        )\r\n      )\r\n    )\r\n  )\r\n  (word_dropout): WordDropout(p=0.05)\r\n  (locked_dropout): LockedDropout(p=0.5)\r\n  (embedding2nn): Linear(in_features=868, out_features=868, bias=True)\r\n  (rnn): LSTM(868, 256, batch_first=True, bidirectional=True)\r\n  (linear): Linear(in_features=512, out_features=19, bias=True)\r\n  (beta): 1.0\r\n  (weights): None\r\n  (weight_tensor) None\r\n)\"\r\n```\r\n\r\nIs any thing wrong here?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1744", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1744/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1744/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1744/events", "html_url": "https://github.com/flairNLP/flair/issues/1744", "id": 651858296, "node_id": "MDU6SXNzdWU2NTE4NTgyOTY=", "number": 1744, "title": "FlairEmbeddings function gives ValueError in python 3.6 (latest Nvidia Pytorch Docker container)", "user": {"login": "tylerlekang", "id": 20463434, "node_id": "MDQ6VXNlcjIwNDYzNDM0", "avatar_url": "https://avatars1.githubusercontent.com/u/20463434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tylerlekang", "html_url": "https://github.com/tylerlekang", "followers_url": "https://api.github.com/users/tylerlekang/followers", "following_url": "https://api.github.com/users/tylerlekang/following{/other_user}", "gists_url": "https://api.github.com/users/tylerlekang/gists{/gist_id}", "starred_url": "https://api.github.com/users/tylerlekang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tylerlekang/subscriptions", "organizations_url": "https://api.github.com/users/tylerlekang/orgs", "repos_url": "https://api.github.com/users/tylerlekang/repos", "events_url": "https://api.github.com/users/tylerlekang/events{/privacy}", "received_events_url": "https://api.github.com/users/tylerlekang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-06T22:33:16Z", "updated_at": "2020-07-09T08:34:57Z", "closed_at": "2020-07-09T08:34:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nSimply running the code ```FlairEmbeddings('news-forward')``` gives a ```ValueError```, in Python 3.6.10 (Conda), which is the python environment included in the most recent PyTorch Docker container from Nvidia. (https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_20-06.html#rel_20-06)\r\n\r\nHere is the error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"fineTune_langModel.py\", line 10, in <module>\r\n    language_model = FlairEmbeddings('news-forward').lm\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py\", line 578, in __init__\r\n    self.lm: LanguageModel = LanguageModel.load_language_model(model)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py\", line 202, in load_language_model\r\n    dropout=state[\"dropout\"],\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py\", line 63, in __init__\r\n    self.to(flair.device)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 465, in to\r\n    return self._apply(convert)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/models/language_model.py\", line 404, in _apply\r\n    for info in torch.__version__.replace(\"+\",\".\").split('.') if info.isdigit())\r\nValueError: not enough values to unpack (expected at least 3, got 2)\r\n```\r\n\r\n\r\n**To Reproduce**\r\nRun this code in the container (after pip installing flair):\r\n```from flair.embeddings import FlairEmbeddings```\r\n```FlairEmbeddings('news-forward')```\r\n\r\n**Expected behavior**\r\nThe function should work with no problems (in particular, the .lm is intended to be given to the LanguageModelTrainer function).\r\n\r\n**Environment (please complete the following information):**\r\n- Docker container running Linux (all details of Ubuntu, Python, PyTorch, etc. versions is in the Nvidia link above)\r\n- Flair version is 0.5\r\n\r\n**Additional context**\r\nRunning the code in Python 3.7.7 (Conda) gives no problems.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1743", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1743/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1743/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1743/events", "html_url": "https://github.com/flairNLP/flair/issues/1743", "id": 651371160, "node_id": "MDU6SXNzdWU2NTEzNzExNjA=", "number": 1743, "title": "AttributeError: 'BertTokenizer' object has no attribute 'unique_no_split_tokens'", "user": {"login": "plc-dev", "id": 33401663, "node_id": "MDQ6VXNlcjMzNDAxNjYz", "avatar_url": "https://avatars2.githubusercontent.com/u/33401663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/plc-dev", "html_url": "https://github.com/plc-dev", "followers_url": "https://api.github.com/users/plc-dev/followers", "following_url": "https://api.github.com/users/plc-dev/following{/other_user}", "gists_url": "https://api.github.com/users/plc-dev/gists{/gist_id}", "starred_url": "https://api.github.com/users/plc-dev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/plc-dev/subscriptions", "organizations_url": "https://api.github.com/users/plc-dev/orgs", "repos_url": "https://api.github.com/users/plc-dev/repos", "events_url": "https://api.github.com/users/plc-dev/events{/privacy}", "received_events_url": "https://api.github.com/users/plc-dev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-07-06T09:02:27Z", "updated_at": "2020-07-06T14:24:55Z", "closed_at": "2020-07-06T14:24:54Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\nAfter successfully training a NER model on Colab, loading the model again for inference in the same environment results in the following error, when calling model.evaluate().\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-5-56e09b327a86> in <module>\r\n----> 1 result, main_score = model.evaluate(corpus.test, out_path = f\"output.txt\")\r\n      2 \r\n      3 print(main_score)\r\n      4 print(result.detailed_results)\r\n\r\n~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in evaluate(self, sentences, out_path, embedding_storage_mode, mini_batch_size, num_workers)\r\n    515         # if span F1 needs to be used, use separate eval method\r\n    516         if self._requires_span_F1_evaluation():\r\n--> 517             return self._evaluate_with_span_F1(data_loader, embedding_storage_mode, mini_batch_size, out_path)\r\n    518 \r\n    519         # else, use scikit-learn to evaluate\r\n\r\n~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in _evaluate_with_span_F1(self, data_loader, embedding_storage_mode, mini_batch_size, out_path)\r\n    417                                 mini_batch_size=mini_batch_size,\r\n    418                                 label_name='predicted',\r\n--> 419                                 return_loss=True)\r\n    420             eval_loss += loss\r\n    421             batch_no += 1\r\n\r\n~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in predict(self, sentences, mini_batch_size, all_tag_prob, verbose, label_name, return_loss, embedding_storage_mode)\r\n    364                     continue\r\n    365 \r\n--> 366                 feature = self.forward(batch)\r\n    367 \r\n    368                 if return_loss:\r\n\r\n~/.local/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py in forward(self, sentences)\r\n    602     def forward(self, sentences: List[Sentence]):\r\n    603 \r\n--> 604         self.embeddings.embed(sentences)\r\n    605 \r\n    606         names = self.embeddings.get_names()\r\n\r\n~/.local/lib/python3.6/site-packages/flair/embeddings/token.py in embed(self, sentences, static_embeddings)\r\n     69 \r\n     70         for embedding in self.embeddings:\r\n---> 71             embedding.embed(sentences)\r\n     72 \r\n     73     @property\r\n\r\n~/.local/lib/python3.6/site-packages/flair/embeddings/base.py in embed(self, sentences)\r\n     59 \r\n     60         if not everything_embedded or not self.static_embeddings:\r\n---> 61             self._add_embeddings_internal(sentences)\r\n     62 \r\n     63         return sentences\r\n\r\n~/.local/lib/python3.6/site-packages/flair/embeddings/legacy.py in _add_embeddings_internal(self, sentences)\r\n   1195                 [\r\n   1196                     self.tokenizer.tokenize(sentence.to_tokenized_string())\r\n-> 1197                     for sentence in sentences\r\n   1198                 ],\r\n   1199                 key=len,\r\n\r\n~/.local/lib/python3.6/site-packages/flair/embeddings/legacy.py in <listcomp>(.0)\r\n   1195                 [\r\n   1196                     self.tokenizer.tokenize(sentence.to_tokenized_string())\r\n-> 1197                     for sentence in sentences\r\n   1198                 ],\r\n   1199                 key=len,\r\n\r\n~/.local/lib/python3.6/site-packages/transformers/tokenization_utils.py in tokenize(self, text, **kwargs)\r\n    361             )\r\n    362 \r\n--> 363         no_split_token = self.unique_no_split_tokens\r\n    364         tokenized_text = split_on_tokens(no_split_token, text)\r\n    365         return tokenized_text\r\n\r\nAttributeError: 'BertTokenizer' object has no attribute 'unique_no_split_tokens'\r\n\r\n```\r\n\r\n**To Reproduce**\r\nModell: \"bert-base-multilingual-cased\" finetuned on a NER-task\r\n\r\nDataset: Data is not publicly disclosable\r\n\r\nMinimal script to reproduce:\r\n\r\n```\r\nfrom flair.data import Sentence\r\nfrom flair.data import Corpus\r\nfrom flair.embeddings import BertEmbeddings, StackedEmbeddings, TokenEmbeddings\r\nfrom flair.models import SequenceTagger\r\nfrom flair.trainers import ModelTrainer\r\nfrom flair.datasets import ColumnCorpus\r\nfrom flair.datasets import DataLoader\r\n\r\nembedding_types: List[TokenEmbeddings] = [\r\n    BertEmbeddings('bert-base-multilingual-cased')\r\n]\r\ncolumns = {0: 'text', 1: 'ner'}\r\ntag_type = columns[1]\r\ncorpus: Corpus = ColumnCorpus('data',  columns, train_file='train.txt', in_memory=False)\r\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\n\r\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\r\n\r\ntagger: SequenceTagger = SequenceTagger(hidden_size=512,\r\n                                        embeddings=embeddings,\r\n                                        tag_dictionary=tag_dictionary,\r\n                                        tag_type=tag_type,\r\n                                        use_crf=True)\r\ntrainer.train('model/',\r\n              learning_rate=0.001,\r\n              mini_batch_size=32,\r\n              patience=2,\r\n              anneal_factor=0.4,\r\n              max_epochs=30,\r\n              checkpoint=True,\r\n              embeddings_storage_mode='None')\r\n\r\nmodel = SequenceTagger.load('model/final-model.pt')\r\ncolumns = {0: 'text', 1: 'ner'}\r\ncorpus: Corpus = ColumnCorpus('data/', columns,\r\n                                train_file='train.txt',\r\n                                test_file='test.txt',\r\n                                dev_file='dev.txt')\r\n\r\nresult, main_score = model.evaluate(corpus.test, out_path = f\"output.txt\")\r\n```\r\n\r\n**Expected behavior**\r\nBeing able to use the trained model for inference.\r\n\r\n**Environment (please complete the following information):**\r\nEnvironment: Google Colab\r\nGPU: Tesla P100\r\nVersions:\r\n- Flair 0.5.1\r\n- torch 1.5.1+cu101\r\n- transformers 3.0.1\r\n\r\n**Additional context**\r\nA similar issue occured earlier, when resuming training from a checkpoint, which resulted in the same error when loading the model with the ModelTrainer.load() method. I was able to somewhat work around that error by resuming from an earlier checkpoint. \r\n\r\n**Edit**: Might correlate with #1741  \r\n**Edit 2**: Correct paths in example code to match", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1741", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1741/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1741/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1741/events", "html_url": "https://github.com/flairNLP/flair/issues/1741", "id": 651165246, "node_id": "MDU6SXNzdWU2NTExNjUyNDY=", "number": 1741, "title": "Prediction bug for NER models trained with TransformerWordEmbeddings", "user": {"login": "seyyaw", "id": 1056051, "node_id": "MDQ6VXNlcjEwNTYwNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/1056051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seyyaw", "html_url": "https://github.com/seyyaw", "followers_url": "https://api.github.com/users/seyyaw/followers", "following_url": "https://api.github.com/users/seyyaw/following{/other_user}", "gists_url": "https://api.github.com/users/seyyaw/gists{/gist_id}", "starred_url": "https://api.github.com/users/seyyaw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seyyaw/subscriptions", "organizations_url": "https://api.github.com/users/seyyaw/orgs", "repos_url": "https://api.github.com/users/seyyaw/repos", "events_url": "https://api.github.com/users/seyyaw/events{/privacy}", "received_events_url": "https://api.github.com/users/seyyaw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-05T23:18:14Z", "updated_at": "2020-08-05T06:20:19Z", "closed_at": "2020-07-10T08:00:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI have trained a `TransformerWordEmbeddings` NER model using a custom `RoBERTa` model. Training completed successfully. But when loading the NER model for prediction,\r\n\r\n```\r\nmodel = SequenceTagger.load('resources/taggers/roberta/final-model.pt')\r\n....\r\n\r\nmodel.predict(sentence)\r\n```\r\nthe following error occured:\r\n\r\n````\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-a31cc734771e> in <module>\r\n      8 \r\n      9 # predict tags and print\r\n---> 10 model.predict(sentence)\r\n     11 \r\n     12 print(sentence.to_tagged_string())\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py in predict(self, sentences, mini_batch_size, all_tag_prob, verbose, label_name, return_loss, embedding_storage_mode)\r\n    364                     continue\r\n    365 \r\n--> 366                 feature = self.forward(batch)\r\n    367 \r\n    368                 if return_loss:\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py in forward(self, sentences)\r\n    602     def forward(self, sentences: List[Sentence]):\r\n    603 \r\n--> 604         self.embeddings.embed(sentences)\r\n    605 \r\n    606         names = self.embeddings.get_names()\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/flair/embeddings/token.py in embed(self, sentences, static_embeddings)\r\n     69 \r\n     70         for embedding in self.embeddings:\r\n---> 71             embedding.embed(sentences)\r\n     72 \r\n     73     @property\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/flair/embeddings/base.py in embed(self, sentences)\r\n     59 \r\n     60         if not everything_embedded or not self.static_embeddings:\r\n---> 61             self._add_embeddings_internal(sentences)\r\n     62 \r\n     63         return sentences\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/flair/embeddings/token.py in _add_embeddings_internal(self, sentences)\r\n    895         # embed each micro-batch\r\n    896         for batch in sentence_batches:\r\n--> 897             self._add_embeddings_to_sentences(batch)\r\n    898 \r\n    899         return sentences\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/flair/embeddings/token.py in _add_embeddings_to_sentences(self, sentences)\r\n   1007 \r\n   1008         # put encoded batch through transformer model to get all hidden states of all encoder layers\r\n-> 1009         hidden_states = self.model(input_ids, attention_mask=mask)[-1]\r\n   1010         # make the tuple a tensor; makes working with it easier.\r\n   1011         hidden_states = torch.stack(hidden_states)\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    545             result = self._slow_forward(*input, **kwargs)\r\n    546         else:\r\n--> 547             result = self.forward(*input, **kwargs)\r\n    548         for hook in self._forward_hooks.values():\r\n    549             hook_result = hook(self, input, result)\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/transformers/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\r\n    760             encoder_attention_mask=encoder_extended_attention_mask,\r\n    761             output_attentions=output_attentions,\r\n--> 762             output_hidden_states=output_hidden_states,\r\n    763         )\r\n    764         sequence_output = encoder_outputs[0]\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    545             result = self._slow_forward(*input, **kwargs)\r\n    546         else:\r\n--> 547             result = self.forward(*input, **kwargs)\r\n    548         for hook in self._forward_hooks.values():\r\n    549             hook_result = hook(self, input, result)\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/transformers/modeling_bert.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\r\n    414                 all_hidden_states = all_hidden_states + (hidden_states,)\r\n    415 \r\n--> 416             if getattr(self.config, \"gradient_checkpointing\", False):\r\n    417 \r\n    418                 def create_custom_forward(module):\r\n\r\n~/miniconda3/envs/pytorch_p37/lib/python3.7/site-packages/torch/nn/modules/module.py in __getattr__(self, name)\r\n    589                 return modules[name]\r\n    590         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\r\n--> 591             type(self).__name__, name))\r\n    592 \r\n    593     def __setattr__(self, name, value):\r\n\r\nAttributeError: 'BertEncoder' object has no attribute 'config'\r\n````\r\n\r\n\r\n\r\n**Expected behavior**\r\nNER tagged sentence\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS Linux:\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1736", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1736/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1736/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1736/events", "html_url": "https://github.com/flairNLP/flair/issues/1736", "id": 651003686, "node_id": "MDU6SXNzdWU2NTEwMDM2ODY=", "number": 1736, "title": "Truncation", "user": {"login": "khttknoob", "id": 36195176, "node_id": "MDQ6VXNlcjM2MTk1MTc2", "avatar_url": "https://avatars2.githubusercontent.com/u/36195176?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khttknoob", "html_url": "https://github.com/khttknoob", "followers_url": "https://api.github.com/users/khttknoob/followers", "following_url": "https://api.github.com/users/khttknoob/following{/other_user}", "gists_url": "https://api.github.com/users/khttknoob/gists{/gist_id}", "starred_url": "https://api.github.com/users/khttknoob/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khttknoob/subscriptions", "organizations_url": "https://api.github.com/users/khttknoob/orgs", "repos_url": "https://api.github.com/users/khttknoob/repos", "events_url": "https://api.github.com/users/khttknoob/events{/privacy}", "received_events_url": "https://api.github.com/users/khttknoob/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-07-05T05:17:30Z", "updated_at": "2020-07-07T10:52:56Z", "closed_at": "2020-07-05T21:35:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nGetting this warining\r\nTruncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\r\n\r\n**To Reproduce**\r\nI ran the transformers model a month ago didn't got anything but today getting this warning\r\n\r\n**Environment (please complete the following information):**\r\n - OS [Using Google Colab]\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1735", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1735/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1735/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1735/events", "html_url": "https://github.com/flairNLP/flair/issues/1735", "id": 650896867, "node_id": "MDU6SXNzdWU2NTA4OTY4Njc=", "number": 1735, "title": "Good number for training dataset?", "user": {"login": "pascalhuszar", "id": 45284935, "node_id": "MDQ6VXNlcjQ1Mjg0OTM1", "avatar_url": "https://avatars3.githubusercontent.com/u/45284935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pascalhuszar", "html_url": "https://github.com/pascalhuszar", "followers_url": "https://api.github.com/users/pascalhuszar/followers", "following_url": "https://api.github.com/users/pascalhuszar/following{/other_user}", "gists_url": "https://api.github.com/users/pascalhuszar/gists{/gist_id}", "starred_url": "https://api.github.com/users/pascalhuszar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pascalhuszar/subscriptions", "organizations_url": "https://api.github.com/users/pascalhuszar/orgs", "repos_url": "https://api.github.com/users/pascalhuszar/repos", "events_url": "https://api.github.com/users/pascalhuszar/events{/privacy}", "received_events_url": "https://api.github.com/users/pascalhuszar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-07-04T14:04:54Z", "updated_at": "2020-07-31T13:20:38Z", "closed_at": "2020-07-31T13:20:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey everyone,\r\n\r\ncurrently i'm investigating the performance of different flair models on the GermEval, CoNLL and another German dataset.\r\nThe total number of NEs in the first two mentioned dataset is more than 20k, \r\nMy own dataset has around 4k NEs. \r\n\r\nSo, what is a good number of NEs in a dataset? Currently the F1-score on my dataste is ~70.\r\nSomeone know a paper about that topic?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1733", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1733/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1733/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1733/events", "html_url": "https://github.com/flairNLP/flair/issues/1733", "id": 650749228, "node_id": "MDU6SXNzdWU2NTA3NDkyMjg=", "number": 1733, "title": "Import Error", "user": {"login": "s3afroze", "id": 33985550, "node_id": "MDQ6VXNlcjMzOTg1NTUw", "avatar_url": "https://avatars3.githubusercontent.com/u/33985550?v=4", "gravatar_id": "", "url": "https://api.github.com/users/s3afroze", "html_url": "https://github.com/s3afroze", "followers_url": "https://api.github.com/users/s3afroze/followers", "following_url": "https://api.github.com/users/s3afroze/following{/other_user}", "gists_url": "https://api.github.com/users/s3afroze/gists{/gist_id}", "starred_url": "https://api.github.com/users/s3afroze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/s3afroze/subscriptions", "organizations_url": "https://api.github.com/users/s3afroze/orgs", "repos_url": "https://api.github.com/users/s3afroze/repos", "events_url": "https://api.github.com/users/s3afroze/events{/privacy}", "received_events_url": "https://api.github.com/users/s3afroze/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-03T20:01:32Z", "updated_at": "2020-07-03T20:48:37Z", "closed_at": "2020-07-03T20:33:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi guys,\r\nI recently upgrade flair to the newest version and I am getting the following error on both jupyter notebook and console. I tried virualenv aswell.\r\n\r\n```\r\nImportError: dlopen(/anaconda3/lib/python3.6/site-packages/tokenizers/tokenizers.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin\r\n\r\nReferenced from: /anaconda3/lib/python3.6/site-packages/tokenizers/tokenizers.cpython-36m-darwin.so (which was built for Mac OS X 10.15)\r\n\r\nExpected in: /usr/lib/libSystem.B.dylib\r\n in /anaconda3/lib/python3.6/site-packages/tokenizers/tokenizers.cpython-36m-darwin.so\r\n```\r\n\r\nDo you guys know how I should resolve this? \r\n\r\nThanks,\r\n\r\nShahzeb", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1730", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1730/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1730/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1730/events", "html_url": "https://github.com/flairNLP/flair/issues/1730", "id": 650343820, "node_id": "MDU6SXNzdWU2NTAzNDM4MjA=", "number": 1730, "title": "How large to make val/test files for embeddings training?", "user": {"login": "petulla", "id": 3466817, "node_id": "MDQ6VXNlcjM0NjY4MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/3466817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petulla", "html_url": "https://github.com/petulla", "followers_url": "https://api.github.com/users/petulla/followers", "following_url": "https://api.github.com/users/petulla/following{/other_user}", "gists_url": "https://api.github.com/users/petulla/gists{/gist_id}", "starred_url": "https://api.github.com/users/petulla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petulla/subscriptions", "organizations_url": "https://api.github.com/users/petulla/orgs", "repos_url": "https://api.github.com/users/petulla/repos", "events_url": "https://api.github.com/users/petulla/events{/privacy}", "received_events_url": "https://api.github.com/users/petulla/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-03T04:59:52Z", "updated_at": "2020-07-03T14:36:50Z", "closed_at": "2020-07-03T14:36:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi\r\n\r\nThe train folder seems like it will be much larger than the test/val files, if I understand the instructions? I'll be training on many gb of text. I'm wondering if there's a recommended test/val data size for LM fine-tuning/training.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1729", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1729/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1729/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1729/events", "html_url": "https://github.com/flairNLP/flair/issues/1729", "id": 650074857, "node_id": "MDU6SXNzdWU2NTAwNzQ4NTc=", "number": 1729, "title": "ELMoEmbeddings not working", "user": {"login": "Alea4jacta6est", "id": 26580860, "node_id": "MDQ6VXNlcjI2NTgwODYw", "avatar_url": "https://avatars2.githubusercontent.com/u/26580860?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Alea4jacta6est", "html_url": "https://github.com/Alea4jacta6est", "followers_url": "https://api.github.com/users/Alea4jacta6est/followers", "following_url": "https://api.github.com/users/Alea4jacta6est/following{/other_user}", "gists_url": "https://api.github.com/users/Alea4jacta6est/gists{/gist_id}", "starred_url": "https://api.github.com/users/Alea4jacta6est/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Alea4jacta6est/subscriptions", "organizations_url": "https://api.github.com/users/Alea4jacta6est/orgs", "repos_url": "https://api.github.com/users/Alea4jacta6est/repos", "events_url": "https://api.github.com/users/Alea4jacta6est/events{/privacy}", "received_events_url": "https://api.github.com/users/Alea4jacta6est/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-07-02T16:50:29Z", "updated_at": "2020-07-03T21:04:21Z", "closed_at": "2020-07-03T21:04:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello! I have the same bug trying to use ELMoEmbeddings, I guess the reason might be in the new release of allennlp library, am i right?\r\n```UnboundLocalError: local variable 'allennlp' referenced before assignment```\r\n\r\n_Originally posted by @Alea4jacta6est in https://github.com/flairNLP/flair/issues/597#issuecomment-652643892_", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1726", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1726/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1726/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1726/events", "html_url": "https://github.com/flairNLP/flair/issues/1726", "id": 649362702, "node_id": "MDU6SXNzdWU2NDkzNjI3MDI=", "number": 1726, "title": "Transformers: AttributeError: 'BertTokenizer' object has no attribute 'prepare_for_model'", "user": {"login": "stefan-it", "id": 20651387, "node_id": "MDQ6VXNlcjIwNjUxMzg3", "avatar_url": "https://avatars1.githubusercontent.com/u/20651387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-it", "html_url": "https://github.com/stefan-it", "followers_url": "https://api.github.com/users/stefan-it/followers", "following_url": "https://api.github.com/users/stefan-it/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-it/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-it/subscriptions", "organizations_url": "https://api.github.com/users/stefan-it/orgs", "repos_url": "https://api.github.com/users/stefan-it/repos", "events_url": "https://api.github.com/users/stefan-it/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-it/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "stefan-it", "id": 20651387, "node_id": "MDQ6VXNlcjIwNjUxMzg3", "avatar_url": "https://avatars1.githubusercontent.com/u/20651387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-it", "html_url": "https://github.com/stefan-it", "followers_url": "https://api.github.com/users/stefan-it/followers", "following_url": "https://api.github.com/users/stefan-it/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-it/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-it/subscriptions", "organizations_url": "https://api.github.com/users/stefan-it/orgs", "repos_url": "https://api.github.com/users/stefan-it/repos", "events_url": "https://api.github.com/users/stefan-it/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-it/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "stefan-it", "id": 20651387, "node_id": "MDQ6VXNlcjIwNjUxMzg3", "avatar_url": "https://avatars1.githubusercontent.com/u/20651387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-it", "html_url": "https://github.com/stefan-it", "followers_url": "https://api.github.com/users/stefan-it/followers", "following_url": "https://api.github.com/users/stefan-it/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-it/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-it/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-it/subscriptions", "organizations_url": "https://api.github.com/users/stefan-it/orgs", "repos_url": "https://api.github.com/users/stefan-it/repos", "events_url": "https://api.github.com/users/stefan-it/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-it/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2020-07-01T22:24:16Z", "updated_at": "2020-07-03T11:58:07Z", "closed_at": "2020-07-03T11:58:06Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Hi,\r\n\r\nlatest version of \ud83e\udd17 Transformers no longer supports the `prepare_for_model` method, that we're using in our `TransformerWordEmbeddings` class:\r\n\r\nhttps://github.com/flairNLP/flair/blob/8c09e62d9a5a3c227b9ca0fb9f214de9620d4ca0/flair/embeddings/token.py#L957-L960\r\n\r\nThe following error message is thrown:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"train_conll_en.py\", line 58, in <module>\r\n    shuffle=True\r\n  File \"/mnt/flair-paper/flair/trainers/trainer.py\", line 349, in train\r\n    loss = self.model.forward_loss(batch_step)\r\n  File \"/mnt/flair-paper/flair/models/sequence_tagger_model.py\", line 599, in forward_loss\r\n    features = self.forward(data_points)\r\n  File \"/mnt/flair-paper/flair/models/sequence_tagger_model.py\", line 604, in forward\r\n    self.embeddings.embed(sentences)\r\n  File \"/mnt/flair-paper/flair/embeddings/token.py\", line 71, in embed\r\n    embedding.embed(sentences)\r\n  File \"/mnt/flair-paper/flair/embeddings/base.py\", line 61, in embed\r\n    self._add_embeddings_internal(sentences)\r\n  File \"/mnt/flair-paper/flair/embeddings/token.py\", line 892, in _add_embeddings_internal\r\n    self._add_embeddings_to_sentences(batch)\r\n  File \"/mnt/flair-paper/flair/embeddings/token.py\", line 957, in _add_embeddings_to_sentences\r\n    encoded_inputs = self.tokenizer.prepare_for_model(subtoken_ids_sentence,\r\nAttributeError: 'BertTokenizer' object has no attribute 'prepare_for_model'\r\n```\r\n\r\nIt should be replaced with `encode_plus` method, see https://github.com/huggingface/transformers/issues/5447.\r\n\r\nI'll prepare a PR for that!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1725", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1725/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1725/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1725/events", "html_url": "https://github.com/flairNLP/flair/issues/1725", "id": 648057858, "node_id": "MDU6SXNzdWU2NDgwNTc4NTg=", "number": 1725, "title": "Error with TransformerWordEmbeddings", "user": {"login": "pascalhuszar", "id": 45284935, "node_id": "MDQ6VXNlcjQ1Mjg0OTM1", "avatar_url": "https://avatars3.githubusercontent.com/u/45284935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pascalhuszar", "html_url": "https://github.com/pascalhuszar", "followers_url": "https://api.github.com/users/pascalhuszar/followers", "following_url": "https://api.github.com/users/pascalhuszar/following{/other_user}", "gists_url": "https://api.github.com/users/pascalhuszar/gists{/gist_id}", "starred_url": "https://api.github.com/users/pascalhuszar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pascalhuszar/subscriptions", "organizations_url": "https://api.github.com/users/pascalhuszar/orgs", "repos_url": "https://api.github.com/users/pascalhuszar/repos", "events_url": "https://api.github.com/users/pascalhuszar/events{/privacy}", "received_events_url": "https://api.github.com/users/pascalhuszar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-30T10:23:43Z", "updated_at": "2020-07-28T20:31:25Z", "closed_at": "2020-07-03T18:27:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n```python\r\nImportErrorTraceback (most recent call last)\r\n<ipython-input-4-3bbdce17bb1a> in <module>\r\n      2 embedding_types: List[TokenEmbeddings] = [\r\n      3     WordEmbeddings('de'),\r\n----> 4     TransformerWordEmbeddings('bert-base-german-cased'),\r\n      5 ]\r\n      6 \r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/flair/embeddings/token.py in __init__(self, model, layers, pooling_operation, batch_size, use_scalar_mix, fine_tune)\r\n    819 \r\n    820         # load tokenizer and transformer model\r\n--> 821         self.tokenizer = AutoTokenizer.from_pretrained(model)\r\n    822         config = AutoConfig.from_pretrained(model, output_hidden_states=True)\r\n    823         self.model = AutoModel.from_pretrained(model, config=config)\r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/transformers/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)\r\n    204         config = kwargs.pop(\"config\", None)\r\n    205         if not isinstance(config, PretrainedConfig):\r\n--> 206             config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\r\n    207 \r\n    208         if \"bert-base-japanese\" in str(pretrained_model_name_or_path):\r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/transformers/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\r\n    201 \r\n    202         \"\"\"\r\n--> 203         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\r\n    204 \r\n    205         if \"model_type\" in config_dict:\r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\r\n    236                 proxies=proxies,\r\n    237                 resume_download=resume_download,\r\n--> 238                 local_files_only=local_files_only,\r\n    239             )\r\n    240             # Load config dict\r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/transformers/file_utils.py in cached_path(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\r\n    569             resume_download=resume_download,\r\n    570             user_agent=user_agent,\r\n--> 571             local_files_only=local_files_only,\r\n    572         )\r\n    573     elif os.path.exists(url_or_filename):\r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/transformers/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\r\n    748             logger.info(\"%s not found in cache or force_download set to True, downloading to %s\", url, temp_file.name)\r\n    749 \r\n--> 750             http_get(url, temp_file, proxies=proxies, resume_size=resume_size, user_agent=user_agent)\r\n    751 \r\n    752         logger.info(\"storing %s in cache at %s\", url, cache_path)\r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/transformers/file_utils.py in http_get(url, temp_file, proxies, resume_size, user_agent)\r\n    639         initial=resume_size,\r\n    640         desc=\"Downloading\",\r\n--> 641         disable=bool(logger.getEffectiveLevel() == logging.NOTSET),\r\n    642     )\r\n    643     for chunk in response.iter_content(chunk_size=1024):\r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs)\r\n    206         total = self.total * unit_scale if self.total else self.total\r\n    207         self.container = self.status_printer(\r\n--> 208             self.fp, total, self.desc, self.ncols)\r\n    209         self.sp = self.display\r\n    210 \r\n\r\n/opt/conda/envs/gpu/lib/python3.6/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols)\r\n     95         if IProgress is None:  # #187 #451 #558 #872\r\n     96             raise ImportError(\r\n---> 97                 \"IProgress not found. Please update jupyter and ipywidgets.\"\r\n     98                 \" See https://ipywidgets.readthedocs.io/en/stable\"\r\n     99                 \"/user_install.html\")\r\n\r\nImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\r\n```\r\n\r\n**To Reproduce**\r\n```python\r\n# imports\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import CONLL_03_GERMAN, ColumnCorpus \r\nfrom flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings, FlairEmbeddings, TransformerWordEmbeddings\r\nfrom flair.visual.training_curves import Plotter\r\nfrom flair.trainers import ModelTrainer\r\nfrom flair.models import SequenceTagger\r\nfrom typing import List\r\n\r\ncolumns = {0: 'text', 1: 'ner'}\r\ndata_folder = '/workspace'\r\n\r\ncorpus: Corpus = ColumnCorpus(data_folder, columns,\r\n                              train_file='train.train',\r\n                              dev_file='dev.dev',\r\n                              test_file='test.test')\r\n    \r\n# define task + tag dict.\r\ntag_type = 'ner'\r\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\nprint(tag_dictionary )\r\n\r\n# initialize embeddings\r\nembedding_types: List[TokenEmbeddings] = [\r\n    WordEmbeddings('de'),\r\n    TransformerWordEmbeddings('bert-base-german-cased'),\r\n]\r\n\r\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\r\n\r\n# initialize sequence tagger\r\nfrom flair.models import SequenceTagger\r\n\r\ntagger: SequenceTagger = SequenceTagger(hidden_size=256,\r\n                                        embeddings=embeddings,\r\n                                        tag_dictionary=tag_dictionary,\r\n                                        tag_type=tag_type)\r\n\r\n# initialize trainer\r\nfrom flair.trainers import ModelTrainer\r\n\r\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n\r\ntrainer.train('taggers/distil',\r\n            learning_rate=0.1,\r\n              mini_batch_size=32,\r\n              train_with_dev = True, \r\n              max_epochs=150)\r\n``` \r\n**Expected behavior**\r\nThe model gets trained\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS [e.g. iOS, Linux]: Linux\r\n - Version [e.g. flair-0.3.2]: Tested both: 0.5 and 0.4.5\r\n\r\n**Additional context**\r\nThis problem only occur with TransformerEmbeddings not with Flair\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1724", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1724/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1724/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1724/events", "html_url": "https://github.com/flairNLP/flair/issues/1724", "id": 647002668, "node_id": "MDU6SXNzdWU2NDcwMDI2Njg=", "number": 1724, "title": "TransformerWordEmbeddings  Flair Release 0.5 cannot process special characters of Twitter data", "user": {"login": "marinapollo", "id": 37367530, "node_id": "MDQ6VXNlcjM3MzY3NTMw", "avatar_url": "https://avatars2.githubusercontent.com/u/37367530?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marinapollo", "html_url": "https://github.com/marinapollo", "followers_url": "https://api.github.com/users/marinapollo/followers", "following_url": "https://api.github.com/users/marinapollo/following{/other_user}", "gists_url": "https://api.github.com/users/marinapollo/gists{/gist_id}", "starred_url": "https://api.github.com/users/marinapollo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marinapollo/subscriptions", "organizations_url": "https://api.github.com/users/marinapollo/orgs", "repos_url": "https://api.github.com/users/marinapollo/repos", "events_url": "https://api.github.com/users/marinapollo/events{/privacy}", "received_events_url": "https://api.github.com/users/marinapollo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-06-28T20:49:33Z", "updated_at": "2020-07-07T15:33:34Z", "closed_at": "2020-07-07T15:33:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nTraceback (most recent call last):\r\n  File \"flair-POS.py\", line 121, in <module>\r\n    max_epochs=150) \r\n  File \"/work/flair/flair/trainers/trainer.py\", line 357, in train\r\n    loss = self.model.forward_loss(batch_step)\r\n  File \"/work/flair/flair/models/sequence_tagger_model.py\", line 627, in forward_loss\r\n    features = self.forward(data_points)\r\n  File \"/work/flair/flair/models/sequence_tagger_model.py\", line 662, in forward\r\n    self.embeddings.embedding_length,\r\nRuntimeError: shape '[32, 33, 3072]' is invalid for input of size 3240960\r\n\r\n\r\n**To Reproduce**\r\n     \r\n    data_folder = '/work/Twitter_data/'\r\n    embedding_type = 'bert-base-uncased'\r\n    corpus: Corpus = ColumnCorpus(data_folder, columns,\r\n                                  train_file='train-flair'\r\n                                  test_file='test-flair',\r\n                                  dev_file='dev-flair')\r\n    tag_type = 'pos'\r\n\r\n\r\n  \r\n\r\n    tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\n    embeddings = TransformerWordEmbeddings(embedding_type,fine_tune=True)#layers=\"-1\"\r\n\r\n    # 5. initialize sequence tagger\r\n    tagger: SequenceTagger = SequenceTagger(hidden_size=256,\r\n                                        embeddings=embeddings,\r\n                                        tag_dictionary=tag_dictionary,\r\n                                        tag_type=tag_type,\r\n                                        use_crf=False\r\n                                        )\r\n    \r\n    # 6. initialize trainer\r\n\r\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus)#optimizer=Adam\r\n\r\n   \r\n    trainer.train(path,\r\n              learning_rate=0.1,\r\n              embeddings_storage_mode = 'gpu',\r\n              #mini_batch_chunk_size=2, \r\n              mini_batch_size=32,\r\n              max_epochs=150) \r\n    \r\n\r\n    \r\n   \r\n\r\n**Expected behavior**\r\nexpected training as usual with TransformerWordEmbeddings module but the error appears because TransformerWordEmbeddings module cannot process special characters of Twitter, not sure which exactly characters these are (kind of smilies and others). When used other data (not Twitter) without special characters, everything works fine.\r\n\r\n**Environment (please complete the following information):**\r\n - Version flair-0.5\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1712", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1712/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1712/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1712/events", "html_url": "https://github.com/flairNLP/flair/issues/1712", "id": 643820469, "node_id": "MDU6SXNzdWU2NDM4MjA0Njk=", "number": 1712, "title": "albert-base-v2 tokenization broken", "user": {"login": "mittalsuraj18", "id": 5629517, "node_id": "MDQ6VXNlcjU2Mjk1MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5629517?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mittalsuraj18", "html_url": "https://github.com/mittalsuraj18", "followers_url": "https://api.github.com/users/mittalsuraj18/followers", "following_url": "https://api.github.com/users/mittalsuraj18/following{/other_user}", "gists_url": "https://api.github.com/users/mittalsuraj18/gists{/gist_id}", "starred_url": "https://api.github.com/users/mittalsuraj18/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mittalsuraj18/subscriptions", "organizations_url": "https://api.github.com/users/mittalsuraj18/orgs", "repos_url": "https://api.github.com/users/mittalsuraj18/repos", "events_url": "https://api.github.com/users/mittalsuraj18/events{/privacy}", "received_events_url": "https://api.github.com/users/mittalsuraj18/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-06-23T12:58:04Z", "updated_at": "2020-06-24T10:38:48Z", "closed_at": "2020-06-24T10:38:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\nThe TextClassifier model loading crashes when model is trained on albert-base-v2\r\n\r\n**To Reproduce**\r\n- Train a text classifier using `albert-base-v2`. Save the model.\r\n- Now try to load this model in some other machine.\r\n- The loading crashes due to SentencePiece file not existing.\r\n\r\n**Expected behavior**\r\nModel should load successfully.\r\n\r\n**Environment (please complete the following information):**\r\n - OS [e.g. iOS, Linux]: Ubuntu-20-LTS\r\n - Version [e.g. flair-0.3.2]: flair-github-master\r\n\r\n**Additional context**\r\nThere is workaround that involves monkey patching a bit of code like this\r\n```\r\nfrom types import MethodType\r\nimport transformers\r\nvocab_file  = transformers.tokenization_albert.AlbertTokenizer.from_pretrained(\"albert-base-v2\").vocab_file\r\ndef _setstate(self, d):  # Method to patch with\r\n    self.__dict__ = d\r\n    try:\r\n        import sentencepiece as spm\r\n    except ImportError:\r\n        logger.warning(\r\n            \"You need to install SentencePiece to use AlbertTokenizer: https://github.com/google/sentencepiece\"\r\n            \"pip install sentencepiece\"\r\n        )\r\n        raise\r\n    self.sp_model = spm.SentencePieceProcessor()\r\n    self.sp_model.Load(vocab_file)\r\n\r\n# Actual Patching being done here\r\ntransformers.tokenization_albert.AlbertTokenizer.__setstate__ = MethodType(\r\n            _setstate, transformers.tokenization_albert.AlbertTokenizer(vocab_file )\r\n)\r\n```\r\nHaving to do this everytime is crazy. Maybe we can implement some better way of handling this issue", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1711", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1711/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1711/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1711/events", "html_url": "https://github.com/flairNLP/flair/issues/1711", "id": 643748105, "node_id": "MDU6SXNzdWU2NDM3NDgxMDU=", "number": 1711, "title": "Improve tokenization API", "user": {"login": "mariosaenger", "id": 40803339, "node_id": "MDQ6VXNlcjQwODAzMzM5", "avatar_url": "https://avatars2.githubusercontent.com/u/40803339?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosaenger", "html_url": "https://github.com/mariosaenger", "followers_url": "https://api.github.com/users/mariosaenger/followers", "following_url": "https://api.github.com/users/mariosaenger/following{/other_user}", "gists_url": "https://api.github.com/users/mariosaenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosaenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosaenger/subscriptions", "organizations_url": "https://api.github.com/users/mariosaenger/orgs", "repos_url": "https://api.github.com/users/mariosaenger/repos", "events_url": "https://api.github.com/users/mariosaenger/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosaenger/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mariosaenger", "id": 40803339, "node_id": "MDQ6VXNlcjQwODAzMzM5", "avatar_url": "https://avatars2.githubusercontent.com/u/40803339?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosaenger", "html_url": "https://github.com/mariosaenger", "followers_url": "https://api.github.com/users/mariosaenger/followers", "following_url": "https://api.github.com/users/mariosaenger/following{/other_user}", "gists_url": "https://api.github.com/users/mariosaenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosaenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosaenger/subscriptions", "organizations_url": "https://api.github.com/users/mariosaenger/orgs", "repos_url": "https://api.github.com/users/mariosaenger/repos", "events_url": "https://api.github.com/users/mariosaenger/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosaenger/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mariosaenger", "id": 40803339, "node_id": "MDQ6VXNlcjQwODAzMzM5", "avatar_url": "https://avatars2.githubusercontent.com/u/40803339?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mariosaenger", "html_url": "https://github.com/mariosaenger", "followers_url": "https://api.github.com/users/mariosaenger/followers", "following_url": "https://api.github.com/users/mariosaenger/following{/other_user}", "gists_url": "https://api.github.com/users/mariosaenger/gists{/gist_id}", "starred_url": "https://api.github.com/users/mariosaenger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mariosaenger/subscriptions", "organizations_url": "https://api.github.com/users/mariosaenger/orgs", "repos_url": "https://api.github.com/users/mariosaenger/repos", "events_url": "https://api.github.com/users/mariosaenger/events{/privacy}", "received_events_url": "https://api.github.com/users/mariosaenger/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2020-06-23T11:06:08Z", "updated_at": "2020-07-13T14:15:39Z", "closed_at": "2020-07-13T14:15:39Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Currently, flair supports tokenization only in a rather simple, non-standardised form, using different functions / callables. In course of #1513 the need for a more sophisticated implementation emerged\r\n\r\nThe goal of this issue is to provide a general tokenization programming interface based on a abstract superclass. The new implementation should enable users to define and use custom tokenizers easily.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1709", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1709/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1709/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1709/events", "html_url": "https://github.com/flairNLP/flair/issues/1709", "id": 643408604, "node_id": "MDU6SXNzdWU2NDM0MDg2MDQ=", "number": 1709, "title": "predict(sentence, multi_class_prob=True) in TextClassification no longer works ", "user": {"login": "Amyylam", "id": 54615207, "node_id": "MDQ6VXNlcjU0NjE1MjA3", "avatar_url": "https://avatars3.githubusercontent.com/u/54615207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Amyylam", "html_url": "https://github.com/Amyylam", "followers_url": "https://api.github.com/users/Amyylam/followers", "following_url": "https://api.github.com/users/Amyylam/following{/other_user}", "gists_url": "https://api.github.com/users/Amyylam/gists{/gist_id}", "starred_url": "https://api.github.com/users/Amyylam/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Amyylam/subscriptions", "organizations_url": "https://api.github.com/users/Amyylam/orgs", "repos_url": "https://api.github.com/users/Amyylam/repos", "events_url": "https://api.github.com/users/Amyylam/events{/privacy}", "received_events_url": "https://api.github.com/users/Amyylam/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-22T22:52:14Z", "updated_at": "2020-06-24T16:25:56Z", "closed_at": "2020-06-24T11:16:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\npredict(sentence, multi_class_prob=True) used to give probabilities for all labels, but now only give out one label and associated probability.  I have to revert to earlier commit to produce multiple class probabilities (pip install --upgrade git+https://github.com/flairNLP/flair.git@63aeabf9a18bdf53af3bcba5bd80f43ac717656e). \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior (e.g. which model did you train? what parameters did you use? etc.).\r\n\r\nsentence = Sentence(\"Growth weakens as investment drops, consumers fade\")\r\n\r\nfinetuned_classifier.predict(sentence,multi_class_prob=True)\r\n\r\nprint(sentence.labels) \r\n\r\n**Expected behavior**\r\nit should return: [1 (0.0), -1 (1.0), 0 (0.0)] \r\ngiven the training data has 3 classes. \r\n\r\n**Screenshots**\r\nnow above steps returns only: [0 (0.0)]\r\n\r\n**Environment (please complete the following information):**\r\n - OS [e.g. iOS, Linux]: Mac, Linux virtualenv on Google Colab\r\n - Version [e.g. flair-0.3.2]: tried both flair-0.5 installed via \"pip install flair\" and also \"pip install --upgrade git+https://github.com/flairNLP/flair.git\"\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1707", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1707/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1707/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1707/events", "html_url": "https://github.com/flairNLP/flair/issues/1707", "id": 642576256, "node_id": "MDU6SXNzdWU2NDI1NzYyNTY=", "number": 1707, "title": "Learning rate - pre-training flair", "user": {"login": "pascalhuszar", "id": 45284935, "node_id": "MDQ6VXNlcjQ1Mjg0OTM1", "avatar_url": "https://avatars3.githubusercontent.com/u/45284935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pascalhuszar", "html_url": "https://github.com/pascalhuszar", "followers_url": "https://api.github.com/users/pascalhuszar/followers", "following_url": "https://api.github.com/users/pascalhuszar/following{/other_user}", "gists_url": "https://api.github.com/users/pascalhuszar/gists{/gist_id}", "starred_url": "https://api.github.com/users/pascalhuszar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pascalhuszar/subscriptions", "organizations_url": "https://api.github.com/users/pascalhuszar/orgs", "repos_url": "https://api.github.com/users/pascalhuszar/repos", "events_url": "https://api.github.com/users/pascalhuszar/events{/privacy}", "received_events_url": "https://api.github.com/users/pascalhuszar/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-21T14:42:09Z", "updated_at": "2020-06-25T12:48:55Z", "closed_at": "2020-06-25T12:48:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey everybody,\r\n\r\ni have question about the learning rate in pre-training a LM: Why is the learning rate set to 20? \r\n20 is relatively high, compared to other approaches like BERT.  And for example in training a flair LM for NER the learning rate is smaller than 1 and annealing is utilized. \r\nIs their a concept or idea behind this which i'm not aware of?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1705", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1705/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1705/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1705/events", "html_url": "https://github.com/flairNLP/flair/issues/1705", "id": 642477644, "node_id": "MDU6SXNzdWU2NDI0Nzc2NDQ=", "number": 1705, "title": "Q. Can we get more than one prediction value in the text classification task?", "user": {"login": "Kyubyong", "id": 13579986, "node_id": "MDQ6VXNlcjEzNTc5OTg2", "avatar_url": "https://avatars2.githubusercontent.com/u/13579986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kyubyong", "html_url": "https://github.com/Kyubyong", "followers_url": "https://api.github.com/users/Kyubyong/followers", "following_url": "https://api.github.com/users/Kyubyong/following{/other_user}", "gists_url": "https://api.github.com/users/Kyubyong/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kyubyong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kyubyong/subscriptions", "organizations_url": "https://api.github.com/users/Kyubyong/orgs", "repos_url": "https://api.github.com/users/Kyubyong/repos", "events_url": "https://api.github.com/users/Kyubyong/events{/privacy}", "received_events_url": "https://api.github.com/users/Kyubyong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-21T01:49:18Z", "updated_at": "2020-06-25T00:34:04Z", "closed_at": "2020-06-25T00:34:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I guess by default, classifiers return the best prediction result and its score.\r\nI wonder if we can get more than that, that is, top n values. For example,\r\n\r\n>>> print(sentence.labels)\r\n[\"dog (0.5)\", \"cat (0.3)\", duck (0.2)]", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1704", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1704/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1704/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1704/events", "html_url": "https://github.com/flairNLP/flair/issues/1704", "id": 642465501, "node_id": "MDU6SXNzdWU2NDI0NjU1MDE=", "number": 1704, "title": "Changing embedding storage mode to GPU when using optimizer", "user": {"login": "lfdharo", "id": 3659963, "node_id": "MDQ6VXNlcjM2NTk5NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3659963?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lfdharo", "html_url": "https://github.com/lfdharo", "followers_url": "https://api.github.com/users/lfdharo/followers", "following_url": "https://api.github.com/users/lfdharo/following{/other_user}", "gists_url": "https://api.github.com/users/lfdharo/gists{/gist_id}", "starred_url": "https://api.github.com/users/lfdharo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lfdharo/subscriptions", "organizations_url": "https://api.github.com/users/lfdharo/orgs", "repos_url": "https://api.github.com/users/lfdharo/repos", "events_url": "https://api.github.com/users/lfdharo/events{/privacy}", "received_events_url": "https://api.github.com/users/lfdharo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-20T23:52:51Z", "updated_at": "2020-06-23T10:48:28Z", "closed_at": "2020-06-23T10:48:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "When looking for the hyper-parameter optimizations for a textclassifier, it is not possible to change the default behavior of using 'cpu' storage mode to 'gpu' storage mode which could speed up the process. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1703", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1703/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1703/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1703/events", "html_url": "https://github.com/flairNLP/flair/issues/1703", "id": 641995087, "node_id": "MDU6SXNzdWU2NDE5OTUwODc=", "number": 1703, "title": "Several errors with BERT embeddings in flair 0.5", "user": {"login": "lucaventurini", "id": 2151065, "node_id": "MDQ6VXNlcjIxNTEwNjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/2151065?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucaventurini", "html_url": "https://github.com/lucaventurini", "followers_url": "https://api.github.com/users/lucaventurini/followers", "following_url": "https://api.github.com/users/lucaventurini/following{/other_user}", "gists_url": "https://api.github.com/users/lucaventurini/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucaventurini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucaventurini/subscriptions", "organizations_url": "https://api.github.com/users/lucaventurini/orgs", "repos_url": "https://api.github.com/users/lucaventurini/repos", "events_url": "https://api.github.com/users/lucaventurini/events{/privacy}", "received_events_url": "https://api.github.com/users/lucaventurini/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 20, "created_at": "2020-06-19T14:06:03Z", "updated_at": "2020-08-21T13:15:47Z", "closed_at": "2020-06-24T10:17:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI've been trying to replicate some experiments I did in the past with BERT embeddings on the new 0.5 release, without success.\r\nThe only update I did to the code was to replace BertEmbeddings with the new TransformerWordEmbeddings, as as I understood this is the recommended way to proceed. The embeddings I specify are 'bert-base-multilingual-cased', but I've been tried also with distilbert and gpt2.\r\nIn commit 63aeabf9a18bdf53af3bcba5bd80f43ac717656e, I met the error specified in #1672 , invalid shape. So I tried a more recent commit to see if this was solved.\r\nNow I'm in 3bf14fd13c3833394c7283132148261f937c0a83, which fails due to the following error:\r\n```python\r\nTraceback (most recent call last):\r\n  File \"train_flair.py\", line 187, in <module>\r\n    sys.exit(main(sys.argv[1:]))\r\n  File \"train_flair.py\", line 182, in main\r\n    trainer.train(args.outfolder, checkpoint=True, **hp)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/trainers/trainer.py\", line 364, in train\r\n    loss = self.model.forward_loss(batch_step)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py\", line 146, in forward_loss\r\n    scores = self.forward(data_points)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/models/text_classification_model.py\", line 100, in forward\r\n    self.document_embeddings.embed(sentences)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/embeddings/base.py\", line 61, in embed\r\n    self._add_embeddings_internal(sentences)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/embeddings/document.py\", line 369, in _add_embeddings_internal\r\n    self.embeddings.embed(sentences)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py\", line 71, in embed\r\n    embedding.embed(sentences)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/embeddings/base.py\", line 61, in embed\r\n    self._add_embeddings_internal(sentences)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py\", line 892, in _add_embeddings_internal\r\n    self._add_embeddings_to_sentences(batch)\r\n  File \"/opt/conda/lib/python3.6/site-packages/flair/embeddings/token.py\", line 965, in _add_embeddings_to_sentences\r\n    longest_sequence_in_batch: int = len(max(subtokenized_sentences, key=len))\r\nValueError: max() arg is an empty sequence\r\n```\r\nI thought it could be because of some empty sentence, but I always call `corpus.filter_empty_sentences()` before training, to be safe.\r\nIf I switch to turian embeddings, everything goes well. Is this a known issue?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1699", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1699/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1699/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1699/events", "html_url": "https://github.com/flairNLP/flair/issues/1699", "id": 640392891, "node_id": "MDU6SXNzdWU2NDAzOTI4OTE=", "number": 1699, "title": "Tokenization MISMATCH and RuntimeError: shape '[...]' is invalid for input", "user": {"login": "AylaRT", "id": 20221378, "node_id": "MDQ6VXNlcjIwMjIxMzc4", "avatar_url": "https://avatars2.githubusercontent.com/u/20221378?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AylaRT", "html_url": "https://github.com/AylaRT", "followers_url": "https://api.github.com/users/AylaRT/followers", "following_url": "https://api.github.com/users/AylaRT/following{/other_user}", "gists_url": "https://api.github.com/users/AylaRT/gists{/gist_id}", "starred_url": "https://api.github.com/users/AylaRT/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AylaRT/subscriptions", "organizations_url": "https://api.github.com/users/AylaRT/orgs", "repos_url": "https://api.github.com/users/AylaRT/repos", "events_url": "https://api.github.com/users/AylaRT/events{/privacy}", "received_events_url": "https://api.github.com/users/AylaRT/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2020-06-17T12:13:11Z", "updated_at": "2020-07-06T07:06:01Z", "closed_at": "2020-06-23T10:33:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "HI, I am trying to use TransformerWordEmbeddings(\"bert-base-cased\") to train a sequential labeller with IOB labels (only \"I\", \"O\", \"B\" without subcategories). I started from the tutorial code (no optimisation yet) and my own train-test-dev data. My corpus seems to be created without any trouble and when I use flair embeddings I don't get any errors and good results. \r\n\r\nThis is a sample sentence from the corpus:\r\nTreatment of anemia \\<B> in patients \\<B> with heart \\<B> disease \\<I> : a clinical \\<B> practice \\<I> guideline \\<I> from the American \\<B> College \\<I> of \\<I> Physicians \\<I> .\r\n\r\nHowever, with \"bert-base-cased\" I get the following message:\r\nTokenization MISMATCH in sentence '2\u03bbr d \u03bbr = \uf8ee6(4a - 1)(1 - 2a ) 2 / ( 1 - 3a ) 2 \uf8f9 da \uf8f0 \uf8fb'\r\nLast matched: 'Token: 19 \uf8f9'\r\nLast sentence: 'Token: 22 \uf8fb'\r\nsubtokenized: '['[CLS]', '2', '##\u03bb', '##r', 'd', '\u03bb', '##r', '=', '6', '(', '4', '##a', '-', '1', ')', '(', '1', '-', '2', '##a', ')', '2', '/', '(', '1', '-', '3', '##a', ')', '2', 'da', '[SEP]']'\r\n\r\n(I see some of the special characters don't display correctly in this text, it concerns unicode symbols U+2308 up to U+230B)\r\n\r\nThe message is followed by this error:\r\nRuntimeError: shape '[32, 72, 3072]' is invalid for input of size 7065600\r\n\r\nIs there any way to solve this without changing the tokens in the corpus itself?\r\n\r\nPS: I run this code on a mac (Catalina), with PyCharm", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1691", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1691/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1691/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1691/events", "html_url": "https://github.com/flairNLP/flair/issues/1691", "id": 637997794, "node_id": "MDU6SXNzdWU2Mzc5OTc3OTQ=", "number": 1691, "title": "Allow for kwargs to be passed to transformers models [enhancement]", "user": {"login": "albalaka", "id": 25569766, "node_id": "MDQ6VXNlcjI1NTY5NzY2", "avatar_url": "https://avatars3.githubusercontent.com/u/25569766?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albalaka", "html_url": "https://github.com/albalaka", "followers_url": "https://api.github.com/users/albalaka/followers", "following_url": "https://api.github.com/users/albalaka/following{/other_user}", "gists_url": "https://api.github.com/users/albalaka/gists{/gist_id}", "starred_url": "https://api.github.com/users/albalaka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albalaka/subscriptions", "organizations_url": "https://api.github.com/users/albalaka/orgs", "repos_url": "https://api.github.com/users/albalaka/repos", "events_url": "https://api.github.com/users/albalaka/events{/privacy}", "received_events_url": "https://api.github.com/users/albalaka/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-12T20:52:36Z", "updated_at": "2020-06-16T14:23:44Z", "closed_at": "2020-06-16T14:23:44Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "It would be very nice if we could pass kwargs into transformers models when calling them from flair. I have done it myself on an outdated version of Flair for Distilbert. It was relatively straightforward to just add the option for kwargs into the model initialization and then pass those into the calls to both tokenizer.from_pretrained and model.from_pretrained. It looks like it would be similar now to go from:\r\n\r\n```class TransformerWordEmbeddings(TokenEmbeddings):\r\n    def __init__(\r\n        self,\r\n        model: str = \"bert-base-uncased\",\r\n        layers: str = \"-1,-2,-3,-4\",\r\n        pooling_operation: str = \"first\",\r\n        batch_size: int = 1,\r\n        use_scalar_mix: bool = False,\r\n        fine_tune: bool = False,\r\n        allow_long_sentences: bool = False\r\n    ):\r\n        \"\"\"\r\n        Bidirectional transformer embeddings of words from various transformer architectures.\r\n        :param model: name of transformer model (see https://huggingface.co/transformers/pretrained_models.html for\r\n        options)\r\n        :param layers: string indicating which layers to take for embedding (-1 is topmost layer)\r\n        :param pooling_operation: how to get from token piece embeddings to token embedding. Either take the first\r\n        subtoken ('first'), the last subtoken ('last'), both first and last ('first_last') or a mean over all ('mean')\r\n        :param batch_size: How many sentence to push through transformer at once. Set to 1 by default since transformer\r\n        models tend to be huge.\r\n        :param use_scalar_mix: If True, uses a scalar mix of layers as embedding\r\n        :param fine_tune: If True, allows transformers to be fine-tuned during training\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n\r\n        # load tokenizer and transformer model\r\n        self.tokenizer = AutoTokenizer.from_pretrained(model)\r\n        config = AutoConfig.from_pretrained(model, output_hidden_states=True)\r\n        self.model = AutoModel.from_pretrained(model, config=config)\r\n```\r\n\r\nto \r\n\r\n```class TransformerWordEmbeddings(TokenEmbeddings):\r\n    def __init__(\r\n        self,\r\n        model: str = \"bert-base-uncased\",\r\n        layers: str = \"-1,-2,-3,-4\",\r\n        pooling_operation: str = \"first\",\r\n        batch_size: int = 1,\r\n        use_scalar_mix: bool = False,\r\n        fine_tune: bool = False,\r\n        allow_long_sentences: bool = False,\r\n        **kwargs\r\n    ):\r\n        \"\"\"\r\n        Bidirectional transformer embeddings of words from various transformer architectures.\r\n        :param model: name of transformer model (see https://huggingface.co/transformers/pretrained_models.html for\r\n        options)\r\n        :param layers: string indicating which layers to take for embedding (-1 is topmost layer)\r\n        :param pooling_operation: how to get from token piece embeddings to token embedding. Either take the first\r\n        subtoken ('first'), the last subtoken ('last'), both first and last ('first_last') or a mean over all ('mean')\r\n        :param batch_size: How many sentence to push through transformer at once. Set to 1 by default since transformer\r\n        models tend to be huge.\r\n        :param use_scalar_mix: If True, uses a scalar mix of layers as embedding\r\n        :param fine_tune: If True, allows transformers to be fine-tuned during training\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n\r\n        # load tokenizer and transformer model\r\n        self.tokenizer = AutoTokenizer.from_pretrained(model, **kwargs)\r\n        config = AutoConfig.from_pretrained(model, output_hidden_states=True,**kwargs)\r\n        self.model = AutoModel.from_pretrained(model, config=config,**kwargs)\r\n```\r\n\r\nFor me this would allow me to use the native transformers flag: \"local_files_only\" which guarantees that my model will not try to download anything from huggingface's server.\r\n\r\nI can imagine there may be other cases where this may be useful as well.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1690", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1690/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1690/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1690/events", "html_url": "https://github.com/flairNLP/flair/issues/1690", "id": 637946114, "node_id": "MDU6SXNzdWU2Mzc5NDYxMTQ=", "number": 1690, "title": "Problem with loading dialogpt", "user": {"login": "niedakh", "id": 291663, "node_id": "MDQ6VXNlcjI5MTY2Mw==", "avatar_url": "https://avatars0.githubusercontent.com/u/291663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/niedakh", "html_url": "https://github.com/niedakh", "followers_url": "https://api.github.com/users/niedakh/followers", "following_url": "https://api.github.com/users/niedakh/following{/other_user}", "gists_url": "https://api.github.com/users/niedakh/gists{/gist_id}", "starred_url": "https://api.github.com/users/niedakh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/niedakh/subscriptions", "organizations_url": "https://api.github.com/users/niedakh/orgs", "repos_url": "https://api.github.com/users/niedakh/repos", "events_url": "https://api.github.com/users/niedakh/events{/privacy}", "received_events_url": "https://api.github.com/users/niedakh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-06-12T18:57:02Z", "updated_at": "2020-08-13T13:10:29Z", "closed_at": "2020-08-13T13:10:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n```\r\n----------------------------------------------------------------------------------------------------\r\n2020-06-12 18:44:49,152 Model training base path: \"dialogpt\"\r\n2020-06-12 18:44:49,152 ----------------------------------------------------------------------------------------------------\r\n\r\n2020-06-12 18:44:49,153 Device: cuda:0\r\n2020-06-12 18:44:49,154 ----------------------------------------------------------------------------------------------------\r\n2020-06-12 18:44:49,155 Embeddings storage mode: cpu\r\n2020-06-12 18:44:49,230 ----------------------------------------------------------------------------------------------------\r\n2020-06-12 18:49:00,669 epoch 1 - iter 76/766 - loss 1.11887558 - samples/sec: 9.67\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-e12f92b71d0b> in <module>\r\n      7         mini_batch_chunk_size=2, # set this if you get OOM errors\r\n      8         max_epochs=1, # very few epochs of fine-tuning\r\n----> 9         embeddings_storage_mode='cpu',\r\n     10         #checkpoint=True\r\n     11     )\r\n\r\n~/venv36/lib64/python3.6/site-packages/flair/trainers/trainer.py in train(self, base_path, learning_rate, mini_batch_size, mini_batch_chunk_size, max_epochs, scheduler, anneal_factor, patience, initial_extra_patience, min_learning_rate, train_with_dev, monitor_train, monitor_test, embeddings_storage_mode, checkpoint, save_final_model, anneal_with_restarts, anneal_with_prestarts, batch_growth_annealing, shuffle, param_selection_mode, num_workers, sampler, use_amp, amp_opt_level, eval_on_train_fraction, eval_on_train_shuffle, **kwargs)\r\n    376                         if not param_selection_mode:\r\n    377                             weight_extractor.extract_weights(\r\n--> 378                                 self.model.state_dict(), iteration\r\n    379                             )\r\n    380 \r\n\r\n~/venv36/lib64/python3.6/site-packages/flair/training_utils.py in extract_weights(self, state_dict, iteration)\r\n    262             vec = state_dict[key]\r\n    263             weights_to_watch = min(\r\n--> 264                 self.number_of_weights, reduce(lambda x, y: x * y, list(vec.size()))\r\n    265             )\r\n    266 \r\n\r\nTypeError: reduce() of empty sequence with no initial value\r\n\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior (e.g. which model did you train? what parameters did you use? etc.).\r\n\r\nTake a tagging corpus of choice and run:\r\n\r\n```\r\nembeddings = TransformerWordEmbeddings(\r\n        'microsoft/DialoGPT-small',\r\n        layers=\"-1\",\r\n        pooling_operation='first_last',\r\n        fine_tune=True,\r\n    )\r\n\r\ntagger: SequenceTagger = SequenceTagger(\r\n        hidden_size=256,\r\n        embeddings=embeddings,\r\n        tag_dictionary=tag_dictionary,\r\n        tag_type=TAG_NAME,\r\n        use_crf=False,\r\n        use_rnn=False,\r\n    )\r\n\r\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.Adam)\r\n\r\ntrainer.train('dialogpt',\r\n        learning_rate=3e-5,\r\n        mini_batch_chunk_size=2,\r\n        max_epochs=1,\r\n        embeddings_storage_mode='cpu'\r\n    )\r\n\r\n\r\n```\r\n\r\n**Expected behavior**\r\nThe models should train, instead I get the error.\r\n\r\n**Environment (please complete the following information):**\r\n - Centos7\r\n - Flair 0.5.0\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1685", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1685/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1685/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1685/events", "html_url": "https://github.com/flairNLP/flair/issues/1685", "id": 636880810, "node_id": "MDU6SXNzdWU2MzY4ODA4MTA=", "number": 1685, "title": "Deserialization of tokenizer gets loaded from huggingface", "user": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-06-11T09:52:43Z", "updated_at": "2020-06-11T15:29:19Z", "closed_at": "2020-06-11T15:29:19Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "**Describe the bug**\r\nAt the deserialization of a model, the tokenizer gets loaded from huggingface (we're using TransformerWordEmbeddings). This throws an error when for instance using SCIBERT, because it is not supported in huggingface.\r\n\r\n**To Reproduce**\r\nTrain a sequence tagger model using scibert. Then according to the docs: tagger = SequenceTagger(\"model_using_scibert\"), which throws an errors since it is looking the scibert tokenizer in the huggingface lib.\r\n\r\n**Expected behavior**\r\nThe tokenizer should not get loaded from huggingface if it is not available.\r\n\r\n**Environment (please complete the following information):**\r\n - macOS Catalina\r\n - Version flair 0.5\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1672", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1672/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1672/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1672/events", "html_url": "https://github.com/flairNLP/flair/issues/1672", "id": 633564395, "node_id": "MDU6SXNzdWU2MzM1NjQzOTU=", "number": 1672, "title": "Tokenization MISMATCH causes Runtime Error (keyphrase tagger model)", "user": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-06-07T16:41:58Z", "updated_at": "2020-06-23T06:37:44Z", "closed_at": "2020-06-08T15:32:16Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "While training a tagging model using the provided Keyphrase dataset SEMEVAL2017, following error comes up:\r\n\r\nTokenization MISMATCH in sentence '...'\r\nLast matched: 'Token: 272 \u0304'\r\nLast sentence: 'Token: 325 .'\r\n\r\nwhich causes directly afterwards a Runtime Error during forward method in sequence_tagger_model.py:\r\nFile \"xxx/PycharmProjects/flair/flair/models/sequence_tagger_model.py\", line 541, in forward\r\n    self.embeddings.embedding_length,\r\nRuntimeError: shape '[16, 326, 3072]' is invalid for input of size 15857664\r\n\r\nTo reproduce, use a tagger tutorial code from flair. I use TransformerWordEmbeddings() with scibert or bert-base-uncased as embedding_types. Error does not comes up for example when training the tagger with flair embeddings.\r\n\r\nExpected is that the trainer can complete training without stopping due to special characters in the sentences.\r\n\r\nEnvironment:\r\n - MacOS Catalina 10.15.4\r\n - flair version 0.5\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1666", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1666/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1666/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1666/events", "html_url": "https://github.com/flairNLP/flair/issues/1666", "id": 631261803, "node_id": "MDU6SXNzdWU2MzEyNjE4MDM=", "number": 1666, "title": "Warnings in Python 3.8", "user": {"login": "tirkarthi", "id": 3972343, "node_id": "MDQ6VXNlcjM5NzIzNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3972343?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tirkarthi", "html_url": "https://github.com/tirkarthi", "followers_url": "https://api.github.com/users/tirkarthi/followers", "following_url": "https://api.github.com/users/tirkarthi/following{/other_user}", "gists_url": "https://api.github.com/users/tirkarthi/gists{/gist_id}", "starred_url": "https://api.github.com/users/tirkarthi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tirkarthi/subscriptions", "organizations_url": "https://api.github.com/users/tirkarthi/orgs", "repos_url": "https://api.github.com/users/tirkarthi/repos", "events_url": "https://api.github.com/users/tirkarthi/events{/privacy}", "received_events_url": "https://api.github.com/users/tirkarthi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-05T02:20:06Z", "updated_at": "2020-06-08T16:21:12Z", "closed_at": "2020-06-08T16:21:12Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\n\r\n* Deprecation warnings are raised due to invalid escape sequences. This can be fixed by using raw strings or escaping the literals. pyupgrade also helps in automatic conversion : https://github.com/asottile/pyupgrade/\r\n* Syntax warning due to comparison of literals using is.\r\n\r\n**To Reproduce**\r\n\r\n```\r\nfind . -iname '*.py' | grep -v example | xargs -P4 -I{} python3.8 -Wall -m py_compile {}\r\n./flair/datasets/sequence_labeling.py:24: DeprecationWarning: invalid escape sequence \\s\r\n  column_delimiter: str = \"\\s+\",\r\n./flair/datasets/sequence_labeling.py:106: DeprecationWarning: invalid escape sequence \\s\r\n  column_delimiter: str = \"\\s+\",\r\n./flair/data_fetcher.py:573: DeprecationWarning: invalid escape sequence \\s\r\n  fields: List[str] = re.split(\"\\s+\", line)\r\n./flair/trainers/language_model_trainer.py:146: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n  if no is 0:\r\n./flair/trainers/language_model_trainer.py:148: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\r\n  if no is 1:\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n - OS [e.g. iOS, Linux]: Linux\r\n - Version [e.g. flair-0.3.2]: master branch", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1664", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1664/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1664/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1664/events", "html_url": "https://github.com/flairNLP/flair/issues/1664", "id": 631112905, "node_id": "MDU6SXNzdWU2MzExMTI5MDU=", "number": 1664, "title": "Not able to download the new datasets of flair ", "user": {"login": "DivyaGupta1989", "id": 63741756, "node_id": "MDQ6VXNlcjYzNzQxNzU2", "avatar_url": "https://avatars3.githubusercontent.com/u/63741756?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DivyaGupta1989", "html_url": "https://github.com/DivyaGupta1989", "followers_url": "https://api.github.com/users/DivyaGupta1989/followers", "following_url": "https://api.github.com/users/DivyaGupta1989/following{/other_user}", "gists_url": "https://api.github.com/users/DivyaGupta1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/DivyaGupta1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DivyaGupta1989/subscriptions", "organizations_url": "https://api.github.com/users/DivyaGupta1989/orgs", "repos_url": "https://api.github.com/users/DivyaGupta1989/repos", "events_url": "https://api.github.com/users/DivyaGupta1989/events{/privacy}", "received_events_url": "https://api.github.com/users/DivyaGupta1989/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-06-04T20:08:29Z", "updated_at": "2020-06-08T16:16:43Z", "closed_at": "2020-06-08T16:16:17Z", "author_association": "NONE", "active_lock_reason": null, "body": "While trying to download the dataset using the command mentioned below, I am not able to do so: sentiment_140 = flair.datasets.SENTIMENT_140()\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-23-5e8cfec20052> in <module>\r\n----> 1 sentiment_140 = flair.datasets.SENTIMENT_140()\r\n\r\nAttributeError: module 'flair.datasets' has no attribute 'SENTIMENT_140'", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1663", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1663/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1663/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1663/events", "html_url": "https://github.com/flairNLP/flair/issues/1663", "id": 630029297, "node_id": "MDU6SXNzdWU2MzAwMjkyOTc=", "number": 1663, "title": "Getting always same number of TN and TP for multilabel classification", "user": {"login": "bichomartiano", "id": 45066115, "node_id": "MDQ6VXNlcjQ1MDY2MTE1", "avatar_url": "https://avatars1.githubusercontent.com/u/45066115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bichomartiano", "html_url": "https://github.com/bichomartiano", "followers_url": "https://api.github.com/users/bichomartiano/followers", "following_url": "https://api.github.com/users/bichomartiano/following{/other_user}", "gists_url": "https://api.github.com/users/bichomartiano/gists{/gist_id}", "starred_url": "https://api.github.com/users/bichomartiano/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bichomartiano/subscriptions", "organizations_url": "https://api.github.com/users/bichomartiano/orgs", "repos_url": "https://api.github.com/users/bichomartiano/repos", "events_url": "https://api.github.com/users/bichomartiano/events{/privacy}", "received_events_url": "https://api.github.com/users/bichomartiano/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-03T14:10:19Z", "updated_at": "2020-06-08T10:43:42Z", "closed_at": "2020-06-08T09:03:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi! I would like to share a concern I'm having with the results of the models I'm training using FLAIR. I couldn't find an issue with a similar topic, forgive me if it's something already solved. It's as simple as: Im always getting same number of True positives cases for every tag in multilable text classification. This happens with CONLL02 corpus but also for every label in my own corpus (with 42 tags, so little chance that is simple casuality). Here is an example in the results of what I mean:\r\n\r\nMICRO_AVG: acc 0.6523 - f1-score 0.7895\r\nMACRO_AVG: acc 0.6155 - f1-score 0.7372000000000001\r\nLOC        **tp: 94** - fp: 25 - fn: 29 - **tn: 94 -** precision: 0.7899 - recall: 0.7642 - accuracy: 0.6351 - f1-score: 0.7768\r\nMISC       **tp: 13** - fp: 12 - fn: 20 - **tn: 13** - precision: 0.5200 - recall: 0.3939 - accuracy: 0.2889 - f1-score: 0.4483\r\nORG        **tp: 112** - fp: 41 - fn: 26 - **tn: 112** - precision: 0.7320 - recall: 0.8116 - accuracy: 0.6257 - f1-score: 0.7697\r\nPER        **tp: 83** - fp: 5 - fn: 3 - **tn: 83** - precision: 0.9432 - recall: 0.9651 - accuracy: 0.9121 - f1-score: 0.9540\r\n\r\nif these values are wrong for some reason, there fore the metrics will be wrong as well.\r\n\r\nI'd really appreciate some insight of what might be going on here. \r\n\r\nThanks a lot in advance!!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1660", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1660/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1660/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1660/events", "html_url": "https://github.com/flairNLP/flair/issues/1660", "id": 629748872, "node_id": "MDU6SXNzdWU2Mjk3NDg4NzI=", "number": 1660, "title": "Add support for Sentence Transformers", "user": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023068682, "node_id": "MDU6TGFiZWwxMDIzMDY4Njgy", "url": "https://api.github.com/repos/flairNLP/flair/labels/feature", "name": "feature", "color": "87b70e", "default": false, "description": "A new feature"}], "state": "closed", "locked": false, "assignee": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2020-06-03T07:07:44Z", "updated_at": "2020-07-03T13:42:52Z", "closed_at": "2020-06-18T10:32:57Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The [sentence transformers](https://github.com/UKPLab/sentence-transformers) library has great pre-trained models to produce embeddings for entire sentences. \r\n\r\nWe should add a new `DocumentEmbeddings` class to support these embeddings in Flair.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1657", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1657/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1657/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1657/events", "html_url": "https://github.com/flairNLP/flair/issues/1657", "id": 629544188, "node_id": "MDU6SXNzdWU2Mjk1NDQxODg=", "number": 1657, "title": "Cannot download \"torch\" when installing flair through pip", "user": {"login": "neonet1", "id": 38730172, "node_id": "MDQ6VXNlcjM4NzMwMTcy", "avatar_url": "https://avatars1.githubusercontent.com/u/38730172?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neonet1", "html_url": "https://github.com/neonet1", "followers_url": "https://api.github.com/users/neonet1/followers", "following_url": "https://api.github.com/users/neonet1/following{/other_user}", "gists_url": "https://api.github.com/users/neonet1/gists{/gist_id}", "starred_url": "https://api.github.com/users/neonet1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neonet1/subscriptions", "organizations_url": "https://api.github.com/users/neonet1/orgs", "repos_url": "https://api.github.com/users/neonet1/repos", "events_url": "https://api.github.com/users/neonet1/events{/privacy}", "received_events_url": "https://api.github.com/users/neonet1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-06-02T22:03:14Z", "updated_at": "2020-06-04T16:00:08Z", "closed_at": "2020-06-04T16:00:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nAs the title says, the installation halts when torch download it attempted. It gives the following error messages:\r\n\r\nERROR: Could not find a version that satisfies the requirement torch>=1.1.0 (from flair) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\r\nERROR: No matching distribution found for torch>=1.1.0 (from flair)\r\n\r\n**To Reproduce**\r\nattempt to use pip to install flair by typing in command line:\r\npip install flair\r\n\r\n**Expected behavior**\r\nAll other files just download, except \"torch\" which fails every time it is attempted.\r\n\r\n**Environment (please complete the following information):**\r\n - OS [Windows 10]:", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1651", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1651/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1651/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1651/events", "html_url": "https://github.com/flairNLP/flair/issues/1651", "id": 627653515, "node_id": "MDU6SXNzdWU2Mjc2NTM1MTU=", "number": 1651, "title": "UnboundLocalError: local variable 'token' referenced before assignment", "user": {"login": "nh111", "id": 18840257, "node_id": "MDQ6VXNlcjE4ODQwMjU3", "avatar_url": "https://avatars0.githubusercontent.com/u/18840257?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nh111", "html_url": "https://github.com/nh111", "followers_url": "https://api.github.com/users/nh111/followers", "following_url": "https://api.github.com/users/nh111/following{/other_user}", "gists_url": "https://api.github.com/users/nh111/gists{/gist_id}", "starred_url": "https://api.github.com/users/nh111/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nh111/subscriptions", "organizations_url": "https://api.github.com/users/nh111/orgs", "repos_url": "https://api.github.com/users/nh111/repos", "events_url": "https://api.github.com/users/nh111/events{/privacy}", "received_events_url": "https://api.github.com/users/nh111/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-30T05:00:54Z", "updated_at": "2020-06-10T06:57:25Z", "closed_at": "2020-06-03T06:25:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\nWhen I am trying to run the NER for some text, I am getting this error\r\n\r\nBelow is the complete error:\r\n    sentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(AText)]\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flair\\data.py\", line 490, in __init__\r\n    [self.add_token(token) for token in tokenizer(text)]\r\n\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flair\\data.py\", line 1367, in segtok_tokenizer\r\n    previous_token = token\r\n\r\nUnboundLocalError: local variable 'token' referenced before assignment\r\n\r\n\r\n**To Reproduce**\r\n```python\r\nAText = '''\r\nJPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n1\r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nJPMORGAN\r\n \r\n \r\nModerator\r\n:\r\n \r\nPaul Formanko\r\n \r\nMay 13\r\n, 2016\r\n \r\n14\r\n:\r\n00 GMT\r\n \r\n \r\n \r\nOperator\r\n:\r\n \r\nThis is conference # 958739.\r\n \r\n \r\nOperator\r\n:\r\n \r\nWelcome to the OTP Bank First Quarter 2016 Conference Call.  At this time, \r\nall participants are in a listen\r\n-\r\nonly\r\n-\r\nmode.  There will be a \r\npresentation \r\nfollowed by a question\r\n-\r\nand\r\n-\r\nanswer session.\r\n  \r\nAt which time, if you wish to ask \r\na question, you will need to press star and one on your telephone.  \r\n \r\n \r\n \r\nI must advise you that this conference is being recorded today, Friday, May \r\n13, 2016.\r\n \r\n \r\n \r\nI woul\r\nd now like to hand the conference over to your speaker today, Mr. Paul \r\nFormanko.  Please go ahead, sir.\r\n \r\n \r\nPaul Formanko\r\n:\r\n \r\nThank you, operator, and good afternoon all.  Thank you very much for \r\njoining us today.  This is Paul Formanko from JPMorgan and dialing\r\n \r\nin from \r\nBudapest\r\n \r\nw\r\ne have with us today Mr. Laszlo Bencsik, OTP Bank's Chief \r\nFinancial and Strategic Officer.  Laszlo, please, over to you.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nThank you, Paul.  Thank you for the introduction.  Good morning and good \r\nafternoon, depending where\r\n \r\nyou are, and welcome on OTP Group's 2016 first \r\nquarter conference call.  As usual, we have a presentation available on the \r\nwebsite.  So, hopefully, you've been able to access it and download it, and you \r\ncan follow me when I go through at least the most im\r\nportant pages.\r\n \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n2\r\n \r\n \r\n \r\n \r\nSo, starting on page 2, you can see the quarterly results.  The accounting profit \r\nwas HUF34.3 billion and the adjusted profit, HUF47.6 million.  We booked \r\none item as adjustment, that's the bank tax for this year.  The total annual bank \r\ntax was booked acco\r\nrding to IFRS in the first quarter.  We expect one more \r\nadjustment coming, likely to be in the second half of this year, and that is the \r\none\r\n-\r\noff revenues from the Visa shares sales.  We already communicated that \r\nthe originally expected \r\namount\r\n \r\nwas EUR34.2 m\r\nillion, coming in as an inflow \r\nin the second quarter.  Now, this \r\nhas \r\nchanged slightly.  First of all, the \r\nexpected time of this income has shifted forward somewhere to the second \r\nhalf of this year, and also the cash amount we are expected to receive should\r\n \r\nbe higher.  We don't know exactly how much higher, but the deferred part will \r\nbe less which we did not quantify previously, and the upfront cash amount is \r\ngoing to be higher.  We don't know exactly how much higher.\r\n \r\n \r\n \r\nNext page, the P&L for the Group and b\r\nreaking down different lines.  First of \r\nall, you can see \r\nthe corporate tax\r\n \r\nwas relatively high compared to the pre\r\n-\r\ntax \r\nprofit.  So, we have a higher effective tax rate, mostly coming from Ukraine, \r\nbut also Hungary and Romania was higher.  You already saw l\r\nast year that on \r\na quarterly basis, the effective tax rate fluctuated somewhat, and this is going \r\nto be also this year.  There is no reason\r\n, however\r\n \r\nto expect, on an annual \r\nbasis, different effective rate than roughly 20 percent.  So, these are more or \r\nles\r\ns one\r\n-\r\noffs or kind of expected to reversing items related to deferred tax \r\nassets.\r\n \r\n \r\n \r\nThere were no one\r\n-\r\noff items only one \r\nwith \r\nHUF0.2 billion.  And in the lower \r\nsection of this slide, you see the major P&L lines.  I'm going to talk in detail \r\nabout the vario\r\nus income lines and also about the costs.  Some of those \r\nchange because of seasonality, some of those change because of underlying \r\nbusiness developments, which I'm going to explain later on.  The very kind of \r\nstriking out number is the risk costs.  As you \r\ncan see, the quarterly risk cost \r\nwas HUF20.8 billion.  I think we have not seen these levels of quarterly risk \r\ncosts since 2008.  So, this is a level which is, I believe, somewhat \r\nrepresentative of the current environment and the future environment which \r\nw\r\ne expect in the next couple of years, namely a reasonably relatively low risk \r\nenvironment, at least compared to the previous years.  The risk cost rate in the \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n3\r\n \r\n \r\n \r\nfirst quarter was 1.3 percent on the Group level, which is indeed quite a \r\nnormalized and moderate\r\n \r\nlevel.\r\n \r\n \r\n \r\nNow going forward, o\r\nn page 4, you see this split of\r\n \r\nthe CEE countries and the \r\nRussia and Ukraine.  \r\nD\r\nuring the previous two years, there was a hu\r\nge gap \r\nbetween these two groups:\r\n  \r\nCEE did very well, improved profits and was \r\nincreasing, whereas Russ\r\nia and Ukraine generated large losses, especially \r\nUkraine, and these numbers on the slide do not include \r\nthe \r\none\r\n-\r\noffs \r\nwhich we \r\nbooked in the eastern part of the \r\ncountry\r\n \r\nand Crimea \r\nfor expected \r\nlosses.  So, \r\neven if we add those, that was even bigger\r\n,\r\n \r\nthe loss in 2014 and 2015.  And \r\nthe good news is that the first quarter of this year was positive in Ukraine and \r\nRussia.  This is in line with what we previously expected and communicated \r\nto you and this is why we are going to expect for the remaining par\r\nt of this \r\nyear, both of these countries providing positive results.\r\n \r\n \r\n \r\nOn page 5, you can see further details on the individual performance of the \r\ngroup members, Hungary being up quarter\r\n-\r\non\r\n-\r\nquarter by 4 percent and down \r\nyear\r\n-\r\non\r\n-\r\nyear by 2 percent.  Bulgaria \r\nup quarter\r\n-\r\non\r\n-\r\nquarter almost 30 percent \r\nand down 22 percent, but the first quarter last year was really exceptional \r\nbecause the risk cost was actually positive.  So, there was a write\r\n-\r\nback\r\n. Thus\r\n, \r\nthis huge difference year\r\n-\r\non\r\n-\r\nyear is more explained by \r\nthat\r\n.\r\n  \r\nAnd then we \r\nhave this middle section, the smaller banks in the portfolio which contributed \r\npositively to the Group, each of them.\r\n \r\n \r\n \r\nAnd then in the bottom, you see Russia and Ukraine altogether being positive \r\nand Russia in itself being positive, despite \r\nthe fact that Touch Bank, which is \r\nthis virtual unit which we report as such the online bank which remains \r\nnegative, and actually the expectation related to Touch Bank itself is that it's \r\ngoing to continue to be negative for the remaining part of this year\r\n \r\nbecause we \r\nare still in this mode of building up this business and spending a lot on \r\ncustomer acquisition and having relatively large cost base, at least as long as \r\nwe don't have the sufficient level of business volumes and revenues.\r\n \r\n \r\n \r\nOn page 6, we see a\r\n \r\nfew miscellaneous items.  The first one is the bank tax.  \r\nYou already saw in the first quarter results as compared to last year, there was \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n4\r\n \r\n \r\n \r\na considerable reduction in the bank tax.  Now, this we expect to continue \r\nfrom this year to next year, and on this \r\npage, we try to highlight the main \r\nexpectations.  So, basically the ratio of the bank tax is going to go down from \r\n24 basis points to 21 basis points, and also the base of the calculation moves \r\nahead.  Therefore, the expected bank tax next year is going to\r\n \r\nbe lower by \r\nHUF1.5 billion after tax.  So, instead of the HUF13.3 billion which we \r\nbooked in the first quarter this year, we expect HUF11.8 billion after\r\n-\r\ntax to be \r\nbooked next year, that's a HUF1.5 billion decline.\r\n \r\n \r\n \r\nThere's also one more element which is\r\n \r\nrevealed in the budget for next year, \r\nn\r\namely the elimination or cessation of the previous contribution tax.  This is a \r\ntax which,\r\n \r\nif I remember right, started\r\n \r\nsomew\r\nhere in 2006 and was related to \r\nthe subsidized\r\n \r\nHUF mortgages and this was introduced by the\r\n \r\nsocial\r\nist\r\n \r\ngovernment at this time and we have been paying this extra tax.  But we have \r\nreport\r\ned this tax as part of exp\r\ne\r\nnses\r\n \r\nas general OPEX\r\n.  We have not shown it \r\nas a one\r\n-\r\noff item and has been with us for 10 years now and it's going to phase \r\nout next ye\r\nar.  So, this year, we paid HUF1.7 billion after\r\n-\r\ntax, HUF2.1 billion \r\npre\r\n-\r\ntax, and this is an amount which will not appear next year.  So the impact \r\nwill be HUF1.7 billion after\r\n-\r\ntax positive impact on the bottom line and it's not \r\ngoing to come out of the on\r\ne\r\n-\r\noffs or the adjustments.  It's going to come out \r\nfrom the operational expenses.\r\n \r\n \r\n \r\nNow, the rest \r\nis \r\njust updates on the ongoing \r\nSwiss Fr\r\nank mortgage conversion \r\nprogram in Croatia and in Romania.  We are progressing with these and \r\nexpect to \r\ncomplete\r\n \r\nthem s\r\noon.  There's this extra legislation in Romania \r\nrelated to the walkaway rights of mortgage customers.  We have not seen so \r\nfar any changes in customer \r\nbehavior \r\nrelated to this and therefore, we don't \r\nsee any immediate impact or expected impact on future re\r\nvenues or profits.  \r\nAnd finally, the Visa transaction which I've already talked about.\r\n \r\n \r\n \r\nOn page 7, you see the total income development on a quarterly basis.  \r\nOverall, we declined by HUF5 billion, 3 percent, and here you see the \r\ncomposition of this.  Ther\r\ne was \r\na bigger step\r\n \r\nin Hungary and Bulgaria, and \r\nI'm going to explain it in detail, basically regarding the NII and the fee income \r\nand the other income which components resulted in this.  \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n5\r\n \r\n \r\n \r\n \r\nNow, this is minus HUF3 billion in Russia, and this is only coming\r\n \r\nfrom the \r\nFX rate.  In ruble terms, revenues increased by 2 percent.  So, the only reason \r\nwe have 10 percent lower revenues in our reporting currency, which is the \r\nHungarian forint, is that the Ruble was very weak, especially the beginning of \r\nthe year.  By\r\n \r\nnow, it's corrected back.  So, we have seen ruble rate \r\nstrengthening and actually they are back to end of last year levels.  So, this is \r\ngoing to be a kind of oppos\r\nite direction\r\n \r\nin the second quarter, if things \r\ncontinue as they are.  So, in HUF terms, in \r\nforint terms, we're going to see \r\nsimilar increase due to the exchange rate dynamics in Russia.\r\n \r\n \r\n \r\nIn Ukraine, there was an intrinsic improvement, both in Hungarian\r\n \r\nforint and \r\nin local currency \r\n-\r\n \r\nlater you will see the details \r\n-\r\n \r\nand also Romania, but this is\r\n \r\ndue to a base effect.  You might remember that last year in the fourth quarter, \r\nthere were HUF1.7 billion equivalent of negative one\r\n-\r\noffs\r\n, which reduced \r\nthe \r\nrevenues of our Romanian bank, and these one\r\n-\r\noffs are out and therefore, we \r\nhave a sizable increas\r\ne of the total revenues in Romania.  But again, this is \r\nrelated to this base impact.\r\n \r\n \r\n \r\nGoing to page 8, where you see details of the net interest income decline on a \r\nquarterly basis, which was HU\r\nF4 billion altogether in HUF of which\r\n \r\nHUF3 \r\nbillion came from Hungary.  Now, out of this HUF3 billion, basically around \r\n40 percent, close to 50 percent, H\r\nUF1.3\r\n-\r\n1.\r\n4 billion is related to the \r\nrevaluation of derivative instruments related to the changes in the yield curve \r\nand basically the re\r\n-\r\npri\r\ncing of the IRS swaps.  So, this was a kind of one\r\n-\r\noff \r\nand there's some correction we have already seen in April and May.  \r\nConsequently\r\n, this is not going to recur in the further quarters.\r\n \r\n \r\n \r\nRussia, again as you can see, in ruble terms actually, we had 6 p\r\nercent growth \r\nin net interest income as opposed to the minus 7 percent decline in HUF.  So, \r\nthis is just related to the FX rate.  And in Ukraine, there was a reasonably \r\nstrong growth, which is partially due to the fact that the intra\r\n-\r\nGroup \r\nsubordinated deb\r\nt\r\n \r\nwas converted into equity.  T\r\nherefore, the cost of funding of \r\nthe Ukrainian entity went lower, whereas at the same time, the corporate \r\ncenter revenues declined in the Group.  So, it's just an intra\r\n-\r\nGroup allocation \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n6\r\n \r\n \r\n \r\neffect, but also there was another thir\r\nd\r\n-\r\nparty sub debt which was repaid.  So, \r\nthat's another element which reduced the cost of funding of the Ukrainian \r\nentity.\r\n \r\n \r\n \r\nNow page 9, last time we had the \r\nconference call\r\n, we had a lengthy discussion \r\nabout the expected net interest marg\r\nin of the Group an\r\nd especially at OTP \r\nBank,\r\n \r\nHungary.  \r\nBoth\r\n \r\non the Group level and in OTP Hungary, we had \r\nexperienced a decline in the first quarter.  Hungary was 343 basis points and \r\non the group level, the first quarter was 484 basis points, which compared to \r\nthe annual \r\n20\r\n15 \r\nNIM for the Group, which was 511 basis points, you already \r\nhad 27 basis points decline.  And \r\nI said that we expect\r\n \r\non an annual basis \r\naround 20 basis points\r\n \r\nNIM decline\r\n.  It's clear that due to the current rate \r\nenvironment expectations in Hungary, this \r\ndecline seems to be higher than 20 \r\nbasis points on an annual basis.\r\n \r\n \r\n \r\nSo, on the Group level, our current best estimate of the annual decline of the \r\nnet interest income is around 25 basis points.  And this might surprise you that \r\nwe don't expect kind of la\r\nrger further decline of the NIM on the Group level, \r\nand the reason is that we expect still a step down in the second quarter.  But \r\nin\r\n \r\nthe second half of this year, we expect improve\r\nment\r\n \r\nprimarily driven by \r\nvolume dynamics, especially in Hungary, and also d\r\nriven by stronger ruble \r\nrate in Russia, which will have some impact already in the second quarter.\r\n \r\n \r\n \r\nIn other markets, Bulgaria, we had a margin decline which will not be so \r\nsevere in the following quarters, but will probably continue.  Russia was \r\nstable \r\nin HUF terms, but again, if you look at the net interest margin in ruble \r\nterms, actually the NIM improved by 180 basis points, and due to the stronger \r\nruble in the second quarter, this is going to translate back to the Group NIM as \r\nwell and hopefully, when\r\n \r\nvolume dynamics turn around in the second half as \r\nusually do in Russia due to seasonality, this might even improve in terms of \r\nthe share of its contribution t\r\no the overall Group NIM.  So\r\n \r\nmuch about the \r\nmargins and the expectations related to the NIM for t\r\nhe whole year as a \r\nguidance.\r\n \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n7\r\n \r\n \r\n \r\n \r\nNow, talking a bit about the volumes, first of all, the loan to deposit ratio, \r\nthat's another very positive event.  In the first quarter this year, the previous \r\nlong decline of the loan to deposit ratio turned around and the \r\ndecline stopped \r\nand we actually have 1 percentage point growth in the Group level net loan to \r\ndeposit ratio.  And most importantly, Hungary started to grow as well, which \r\nwe are quite happy about.\r\n \r\n \r\n \r\nNow on page 11, you see the reasons for this turnaround o\r\nf the previous trend \r\nof the net loan to deposits ratio, namely the volume developments in Hungary.  \r\nAs you can see, during the first quarter, Hungarian performing volumes \r\nincreased by 2 percent.  There was \r\na \r\n4 percent increase in consumer loans and \r\nthere w\r\nas \r\na \r\n7 percent increase in corporates, including small, large and \r\nmedium\r\n \r\nsize ones\r\n.  And if you only\r\n \r\nlook at the loans to Hungarian companies\r\n, \r\nthen it was 6 percent.  So, the only part which remains to decline is the \r\nmortgage group, but I will later on try\r\n \r\nto explain the very positive \r\ndevelopments of new mortgage demand and new mortgage sales.  And in \r\ngeneral, we are quite positive about expectations related to consumer loan \r\nvolumes in Hungary and to corporate loan volumes, and the new mortgage \r\nproduction a\r\nlso looks very good.\r\n \r\n \r\n \r\nIn Russia, we saw consumer volumes going down in the first quarter.  This is \r\ndue to two factors.  One is the usual seasonality.  Usually, the POS volumes \r\nare seasonally weak in the first quarter in terms of new sales and therefore, w\r\ne \r\nshould see a decline in volumes, but also we have not restarted with large \r\nvolumes credit card sales.  So, this year, we are still conservative a\r\nnd the \r\ncross\r\n-\r\nsales machine has not started \r\nat \r\nfull speed\r\n \r\nyet\r\n.  We are doing it kind of \r\nlow level, being very \r\ncautious and still waiting, but this is a potential business, \r\nso we can restart soon.\r\n \r\n \r\n \r\nAnd then, in Romania and Croatia, you see this quite large decline, especially \r\nmortgages.  This is related to the conversion in both of these countries.  In \r\nRomania and\r\n \r\nin Croatia, we have ongoing programs to convert the Swiss\r\n \r\nFrank\r\n \r\nmortgages into local currencies or euro, and this conversion is coupled \r\nwith discount on principle, and this is the result of these declines on volumes.\r\n \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n8\r\n \r\n \r\n \r\n \r\nOn page 12, you see the deposit dyna\r\nmics in the first quarter, minus 1 percent.  \r\nThis is what we are quite comfortable with.  Certainly, we don't want to \r\nincrease deposit volumes on the Group level, given level of \r\nexc\r\ness liquidity.  \r\nSo, this is not a strategic priority at the moment.  \r\n \r\n \r\nNow,\r\n \r\ngoing back to the other items of the revenues, namely the net fee and \r\ncommission income, this declined on a quarter\r\n-\r\non\r\n-\r\nquarter basis by 11 percent, \r\nwhich is equivalent to HUF4.4 billion.  Now, this de\r\ncline is usually seasonal.  \r\nEach year\r\n \r\nwe see this diffe\r\nrence and seasonal decline in the first quarter \r\nwhich comes from two sources; one is that usually spending and card usage \r\nactivity is much lower in January than in December and the other one is the \r\ncalendar effect.  \r\n \r\n \r\nT\r\nhis year is a longer year, but Februa\r\nry is a shorter month, plus there is a \r\npeculiar item in Hungary which appears every year in the first quarter and this \r\nis the financial transaction tax related to card transactions, which you have to \r\npay \r\nin \r\na lump sum in the first quarter\r\n; it was around\r\n \r\nHU\r\nF1.6 billion, and so \r\nthis is something which always appears as a negative item in the first quarter.  \r\nIt did last year as well and this year, and therefore again creates the difference \r\nbetween the fourth quarter and the first quarter, and it should actuall\r\ny create a \r\npositive difference in the second quarter as increase should be expected there.  \r\n \r\n \r\nTherefore on this page, \r\nwe\r\n're actually comparing net fee and commission \r\nincome to the first quarter last year.  So instead of quarter\r\n-\r\non\r\n-\r\nquarter, we \r\ncompare year\r\n-\r\non\r\n-\r\nyear numbers, and here you see a quite healthy trend, 4 \r\npercent growth, mostly driven by the Hungarian business, 7 percent which is \r\nactually fundamental\r\n,\r\n \r\nmo\r\nstly related to POS transaction revenues to card \r\nusage, card\r\n-\r\nrelated transaction revenues, which \r\nhave gone up.\r\n \r\n \r\n \r\nAlso when the new loan origination picks up eventually in Hungary and starts \r\nto grow, this should contribute positively further to this number, and to some \r\nextent it's already there.  We have good growth in Bulgaria, 9 percent and then \r\nin \r\nRussia in ruble terms, the growth was 7 percent.  Again, we have this \r\ntranslation FX conversion impact to local currency, which makes this number \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n9\r\n \r\n \r\n \r\nnegative; otherwise it was actually 7 percent positive.  In Ukraine, we have \r\ngrowth.  So this is a revenue lin\r\ne where we see quite positive trends.\r\n \r\n \r\n \r\nOther net non\r\n-\r\ninterest income improved quarter\r\n-\r\non\r\n-\r\nquarter.  In Hungary, it \r\nwas supported by FX gains and gains on other securities.  In Bulgaria, the \r\nnegative is related to this intragroup swap deal, which kind of us\r\nually \r\nintroduces a quarterly fluctuation.  And then Romania, I already referred to \r\nthis line, this HUF1.7 billion improvement, which is entirely related to the \r\nbase effect which appear in the fourth quarter last year due to one\r\n-\r\noff items \r\nwhich we described\r\n \r\non this page.\r\n \r\n \r\n \r\nThe cost dynamics, again usually, there is a seasonality in the first quarter.  \r\nSo, on a quarter\r\n-\r\non\r\n-\r\nquarter comparison, we improved by 13 percent, which is \r\nnominally HUF12.7 billion.  Nevertheless, this is not a very meaningful \r\ncomparison \r\nthere on this slide.  You see the year\r\n-\r\non\r\n-\r\nyear numbers and year\r\n-\r\non\r\n-\r\nyear developments, both in nominal terms and FX adjusted.  So, \r\nnominally, there was 1 percent decline and FX adjusted, there was 1 percent \r\ngrowth, which was partially driven by the Hungaria\r\nn increase.  There was 2 \r\npercent increase in Hungary, which is related to this contribution to different \r\nfunds and authorities.  We've talked about this before and we expect the \r\ncontributions to the deposits and \r\nsecurities\r\n \r\nprotection funds and to the \r\nresol\r\nution fund to increase\r\n \r\nand we book it quarter\r\n-\r\nby\r\n-\r\nquarter gradually.  So, \r\nthis is an increase, which appeared and also other taxes increased and went up \r\nin Hungary.  There was an increase in amortization in Bulgaria and there is \r\nmore of it.\r\n \r\n \r\n \r\nIn Russia, by \r\nthe way even FX adjusted, we've had 15 percent decline year\r\n-\r\non\r\n-\r\nyear.  This is the result of the cost reduction program, the project which we \r\nconcluded last year, which reduced our cost base in Russia considerably, and \r\nthis actually contributes to a large e\r\nxtent to our improvement in the \r\nprofitability of the Russian bank.\r\n \r\n \r\n \r\nPage 16, we are starting to dig deeper into some selected group members' \r\nsituation starting with Hungary.  S\r\no, on page 16, you see the OTP \r\nC\r\nore P&L \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n10\r\n \r\n \r\n \r\nitems.  I've already talked about basic\r\nally each of these \r\nslides\r\n.  So, I'm not \r\ngoing to repeat it.  \r\n \r\n \r\nOn page 17, you see a macro slide, which potentially needs some elaboration.  \r\nThe GDP growth number came out this morning at 9 o'clock.  \r\nT\r\nhat was \r\nactually after we produce\r\nd\r\n \r\nthis slide, so, this\r\n \r\n2.1 percent GDP growth \r\nexpectation might look ambitious given the 0.9 percent year\r\n-\r\non\r\n-\r\nyear first \r\nquarter growth, and I would agree that this is indeed ambitious and it's only \r\nkind of a higher end of the potential spectrum of GDP growth, which we \r\nmight exp\r\nerience this year.  Nevertheless, we expect improvement in the \r\nsecond half of this year.\r\n \r\n \r\n \r\nLet me just elaborate a bit mor\r\ne.  We have two opposing trends \r\nmanifesting \r\nthis year.  One is the decline in the EU transfers and funds to Hungary, which \r\nis quite se\r\nvere compared to last year, which was really a peek and boo and \r\nwhich is much lower than what we expect in 2017 and 2018.  So this is a kind \r\nof cyclical effect of the EU transfers utilization, and this then slides to lower \r\nlevel of investments and CapEx.  \r\nTherefore, you see a lower level of expected \r\ninvestments to GDP.  This has an impact on the construction sector.  There \r\nwas a quite sizable decline actually in the construction sector quarter\r\n-\r\non\r\n-\r\nquarter in the first quarter, and this is due to the \r\ndecline\r\n \r\nof the EU funds.\r\n \r\n \r\n \r\nOn the other part, it's related to export growth.  And as you can see, we expect \r\na moderation in the growth rate of exports, and that's due to the fact that the \r\nEuroz\r\none demand is somewhat lukewarm\r\n \r\npotential\r\nly driven by Chinese \r\ndemand. S\r\no\r\n, with a\r\n \r\nGerman demand in terms of production goods and exports \r\nfrom Hungary\r\n \r\nwas weaker\r\n, and this is effective while the other weak sector in \r\nthe first quarter was the production sector.  So these are the kind of negatives \r\nwithin the positive side.\r\n \r\n \r\n \r\nWe \r\nexpect quite agile growth in household consumption.  It can even be \r\nhigher than this 3.6 percent what we highlighted here given the quite strong \r\nreal wage growth.  We expect 4.\r\n8\r\n \r\npercent real wage growth and all the other \r\npositive incentives, which are give\r\nn to the retail sectors and also the speeding \r\nu\r\np of retail lending.  We see \r\nfirst quarter consumer loans were quite strong \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n11\r\n \r\n \r\n \r\nand \r\nyou see\r\n \r\n4 percent growth in just one quarter.  This somewhat positively \r\nsurprises us as \r\nwell.  So we are just modifying and\r\n \r\nincre\r\nasing our own \r\nexpectations and plans internally regarding how much we should sell, for \r\ninstance, in the consumer loans, but also in mortgages.  So there are very clear \r\nsigns of increasing consumption, which actually translate by the way to higher \r\nconsumer \r\nloan growth than we originally expected.\r\n \r\n \r\n \r\nAnd the other thing is the housing construction and the housing market, which \r\nis really booming.  So actually year\r\n-\r\non\r\n-\r\nyear Budapest was the highest\r\n-\r\ngrowing real estate price city in Europe.  So this has been a qui\r\nte good \r\ninvestment if you \r\nbought\r\n \r\nsomething a year ago in Budapest.  \r\n \r\n \r\nAnd as you can see on this page, the number of new housing construction \r\npermits started to steeply increase and if you annualize the first quarter \r\nnumbers, then we get to 19,000,\r\n \r\nwhich i\r\ns compared to last year figures of\r\n \r\n12,500.  There's a considerable increase.  And actually, we expect an \r\naccelerating trend quarter\r\n-\r\non\r\n-\r\nquarter.  So the annual number is probably going \r\nto be definitely higher than this 19,000.  \r\n \r\n \r\nSo these are the two factor\r\ns; internal consumption growing more than \r\nexpected, household sector growing, new construction growing, whereas on \r\nthe other side, EU funds are less and therefore, construction industry \r\nsomewhat slower or declining plus the EU demand is somewhat less.\r\n \r\n \r\n \r\nThere's another element here, the fiscal spending is increasing and there have \r\nbeen different programs announced and there seem to be definitely sizable \r\nroom in the budget to increase spending, and we expect the government to do \r\nso and this will have impac\r\nt already in the second half of this year, but most \r\nimportantly next year.  So the expectations for next year certainly don't \r\nchange.  We do expect at or above 3 percent growth for 2017 and also for \r\n2018, and growth might be somewhat less than 2 percent th\r\nis year.  So that's \r\nthe overall kind of macro, which might be for you as an insight.\r\n \r\n \r\n \r\nPage 18, you see the activities in different segments in Hungary.  Mortgage \r\napplications continued to grow.  Number of new applications year\r\n-\r\non\r\n-\r\nyear \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n12\r\n \r\n \r\n \r\nincreased by \r\nalmost \r\n70 percent, \r\nactually by \r\n69 percent.\r\n  \r\nOur share is still \r\naround 25 percent.  We target 26.5 percent this year.  \r\nSo, this is a prescribed \r\nstretched \r\ntarget we try to achieve throughout this year as well.  At corporate, \r\nwe are quite strong.  You've seen the numbers, so we grew by 7 percent \r\nperforming and actually 8 percent gross loa\r\nns, which increased our market \r\nshare to 14.5 percent.  We've never seen these levels before, and we are quite \r\nhappy with that.  And this is a strategy to continue to try to g\r\nrow\r\n \r\nin corporate, \r\nwhich seems to be again more active than previously expected.\r\n \r\n \r\n \r\nAn\r\nd these are just few highlights probably during the next session, the next \r\ncon\r\nf\r\n \r\ncall, we will elaborate more on these developments.  We already told last \r\nyear that we started a new program to enhance our services to our customers \r\nin Hungary through digital\r\nization and new digital services and also improve \r\nour internal efficiency on working through digitalization, and these are some \r\nof the new services, which came out already to the market.  \r\n \r\nWe launched just one\r\n-\r\nand\r\n-\r\na\r\n-\r\nhalf months ago a discount program, \r\nfor \r\ncustomers who are OTP clients\r\n \r\nusing bank cards \r\nand \r\neligible to that and we \r\nare recruiting retailers who are quite keen to join us.  We already had Tesco \r\njoining us and MOL, which is the oil company; DM, that's kind of \r\ndrugstore \r\nand white goods chain, and t\r\nhis is about discounts and write\r\n-\r\nbacks.\r\n \r\nAnd \r\nalready, we almost have 1\r\n00\r\n,000 clients registered and using this after \r\none\r\n \r\nmonth.  \r\n \r\nWe have this S\r\nimple application, which again, I think, next time I will give \r\nyou more information on this.  This is very exciti\r\nng and this is growing very \r\nfast.  This is kind of a metro application, through which different services are \r\navailable; for instance, you can buy cinema tickets in almost 100 percent of \r\nthe cinemas, you can buy tickets online by selecting the seat yourself\r\n \r\nand you \r\nactually have the ticket.  You can do parking by just clicking three times, and \r\nit finds your location and you can order cabs and you can order food and buy \r\ntickets to various concerts and all the festivals which are organized in \r\nHungary.  \r\n \r\nThere \r\nis a new online account opening for SMEs, which immensely simplify \r\nand fasten the process.  We have a new payment engine for the utility bills.  \r\nWe have mobile POS application to make card acceptance cheaper for our \r\ncorporate clients, and so on and so on.\r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n13\r\n \r\n \r\n \r\n \r\n \r\nSo, we're quite active in coming out to the market with new innovations in \r\nterms of services and we believe that the next couple of years in Hungary will \r\nbe about actually market development because volume growth started and we \r\nhave already launched a lo\r\nt of new type of services and applications for our \r\ncustomers.  And there's a lot in the pipeline, which will come out during the \r\nnext 1.5 years.  So, we believe that we can further improve our market \r\npositioning in the country.\r\n \r\n \r\n \r\nThe next slide is \r\nBulgaria\r\n, I think I mostly touched up this part.  There was \r\na \r\ndecline in the total income due to the lower\r\n-\r\nmargin environment and lower net \r\ninterest margin.  Risk cost was quite favorable, HUF1.5 billion; the risk cost \r\nrate 0.5 percent.  I think this is the level \r\nwhere we should be or this is a \r\npotential level where we could be in the future, I would rather say that, given \r\nthe risk environment in the country.  And as you can see, we have \r\nimprovement in corporate and SME loans especially both in mortgages and \r\nconsum\r\ner loans, new sales increased.\r\n \r\n \r\n \r\nAnd then a few words about Russia on page 21, Russia started to do well.  It \r\nmakes profits.  Again in HUF terms, it's not so spectacular\r\n, but\r\n \r\nthe ruble just \r\nweakened.  In ruble terms, the improvement is even stronger and a \r\nkey driver \r\nhere is obviously the risk cost.  On page 21 in the lower right corner, you see \r\nthe risk cost rates by the major products we have.  POS is down to 7 percent, \r\nthat's quite good.  This is hopefully the level, which we will see in the future.  \r\n \r\n \r\nCr\r\nedit cards, still high and we hope that this is going to continue to go down.  \r\nBut again in credit cards, we really do\r\nn't have new vintages.  So, we \r\ntechnically\r\n \r\nstopped credit card selling at the beginning of 2015, and we \r\nhaven't really restarted it in a l\r\narger scale.  So, this is all the problems of the \r\nold vintages and the old loans.  And once we restarted this business with the \r\nprevious magnitude, which given the seemingly improving environment in \r\nRussia especially starting from the second part of this y\r\near, it might come \r\nsooner than we originally expected.  Cash loans, I mean, below 10 percent risk \r\ncost rates.  So this is okay.  At this level, this is a reasonably profitable \r\nproduct.\r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n14\r\n \r\n \r\n \r\n \r\n \r\nPage 22, so you see the market dynamics.  POS sales were RUB11 billio\r\nn in \r\nthe first quarter, which is quite a spectacular growth compared to last year, \r\nwhich was RUB8 billion in the first quarter and getting close to the 2014 \r\nnumbers, which were RUB12 billion.  So it seems clear that last year was the \r\nbottom of the cycle an\r\nd this year at least in POS, we can do better.  \r\n \r\n \r\nAlso in this slide, you can see that we haven't really restarted the credit cards.  \r\nThey continue to decline the volumes and the cash loan is somewhat more \r\nactive than last year and very important factor fo\r\nr performance here is on this \r\nslide as well.  That's the cost of deposits, which went up to quite a some extent \r\nlast year and now started to normalize back.  \r\n \r\n \r\nSo now the total cost of deposits is down to 7.9 percent, so below 8 percent, \r\nand this is due to\r\n \r\nthe phasing out of the very expensive term deposits, which \r\nwe had to make last year, and these were typically 12 months or six months\r\n \r\nterm deposits\r\n.  And as they phase out from our books, we expect further \r\nimprovement in the cost of deposits and therefore\r\n \r\nin the margins in Russia.\r\n \r\n \r\n \r\nNow Ukraine, nothing spectacular happened actually, except that there were \r\nno negative one\r\n-\r\noffs.  And if there are no negative one\r\n-\r\noffs in Ukraine, then \r\nUkraine\r\n \r\nis doing reasonably okay and produces positive results as it did last \r\nyear in the second quarter.  If you remember last year's second quarter, \r\nUkraine was positive because tha\r\nt was a quarter where there was\r\n \r\nno FX \r\nde\r\nvaluation, and we didn't provision extra\r\n \r\nfor the old volumes and as long as \r\nthere is no further stronger \r\nde\r\nvaluation of the currency and no major \r\ndisruption on the market or in the prevailing conditions around us.  This is \r\nwhat we expect in Ukraine, a moderate, but growing profit.  And in fact o\r\nur \r\nvulnerability to exchange rate changes has been reduced considerably.  There \r\nis hardly anything left from the previous dollar\r\n-\r\nbased\r\n \r\nmortgage book and our \r\ncorporate lending is mostly in local currency\r\n,\r\n \r\nas well.  So, we are reducing \r\nfast our exp\r\nosure to \r\nFX risk in the country\r\n \r\nstemming from FX assets we have.\r\n \r\n \r\n \r\nPage 25, the summary about the risk indicators.  So the risk cost rate \r\non \r\ngroup \r\nlevel \r\nwas \r\ndown to 131 basis points, very good level.  We're quite happy with \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n15\r\n \r\n \r\n \r\nit whereas the portfolio, the NPL ratios \r\nstayed flat.  Coverage slightly \r\ndeclined, but this is \r\nagain \r\nmostly due to the\r\n \r\nweaker ruble and hryvna rates \r\nin \r\nthe first quarter and therefore \r\nthere\r\n \r\nis smaller contribution of the very high \r\ncoverage level of Ukrainian and Russian non\r\n-\r\nperforming portfolios \r\nfor the \r\noverall coverage level and the group level.\r\n \r\n \r\n \r\nNow, Page 26, you see the portfolio worsening dynamics.  The good thing is \r\nthat Russia was only HUF17 billion.  Usually, the first quarter is worse than \r\nthe other quarters;\r\n \r\n \r\nthat's the usual seasonality\r\n \r\nif nothing else happens because \r\nwe have a high level of new origination in the last quarter and then the high \r\nlevel of defaults in the first quarter, but now it was quite \r\na \r\nmild increase.  We \r\nhad some corporate defaults in Hungary and in Romania.  These w\r\nere the \r\nother two countries where there was material deterioration of the portfolio.\r\n \r\n \r\n \r\nPage 27 shows the risk cost rates by countries.  So, Hungary was down to \r\npretty much zero; Bulgaria 0.5 percent; Russia came down to 10 percent \r\noverall; and Ukraine is d\r\nown to 4.4 percent.  And this is pretty much the entire \r\npresentation in terms of guidance and forward\r\n-\r\nlooking statements.  I've \r\nalready said that we are modifying our guidance related to net interest margin.  \r\nWe expect around 25 basis points decline over t\r\nhe whole year compared to \r\nlast year, as opposed to the original guidance, which was around 20 basis \r\npoints.  And the other guidance we said was related to this return on equity \r\nexpectation for 2017.  And we continue to believe that we are on the right \r\ntrac\r\nk to achieve that given that the without bank tax return on equity during \r\nthe first quarter was 15.5 percent, already this quarter, so this actually makes \r\nus feel confident that we can potentially deliver on this target.\r\n \r\n \r\n \r\nAnd basically that was a formal p\r\nresentation, and I'm more happy to try to \r\nanswer your questions.  So operator, please open the floor for questions.\r\n \r\n \r\nOperator\r\n:\r\n \r\nCertainly.  As a reminder, if you wish to ask a question, please press star and \r\none on your telephone and wait for your name to b\r\ne announced.  If you wish \r\nto cancel your request, please press the hash key. \r\n \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n16\r\n \r\n \r\n \r\nYour first question comes from Pawel Dziedzic from Goldman Sachs.  Please \r\ngo ahead.\r\n \r\n \r\n \r\nPawel Dziedzic\r\n:\r\n \r\nGood afternoon and thank you for the presentation.  Just few quick question\r\ns \r\nfrom my side.  The first one is on loan growth.  So you stated clearly that \r\nyou're expecting inflection point in mortgage lending in Hungary, but could \r\nyou perhaps comment what sort of growth rates you are thinking about in one, \r\ntwo\r\n-\r\nyear's time?  And als\r\no separately, do you think that the strong growth in \r\ncommercial lending, so both SMEs and corporates this quarter is sustainable?  \r\nWere there any large ticket transaction that can perhaps distort a little bit of \r\ngrowth rates for the last three months?\r\n \r\n \r\n \r\nTh\r\nen I have a question on your operating expenses.  You mentioned that the \r\nperformance in Hungary, Russia and Romania as well were very good.  \r\nShould we look at those as a sustainable run\r\n-\r\nrate going forward or whether \r\nany seasonal effect that could perhaps b\r\ne taken into account?  And also where \r\ndo you see any cost saving potential across the group at this stage?  And I \r\nleave it with that.  Thank you.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nThank you fo\r\nr the questions.  So, mortgages:\r\n \r\nour original expectation for this \r\nyear was still\r\n \r\ndeclining mortgage volumes.  Given the higher\r\n-\r\nthan\r\n-\r\npreviously \r\nexpected demand what we see today in Hungary, it might happen that \r\nsomewhere potentially closer to the end of this year, we might be able to stop \r\nthe decline of the portfolio in mortgages, that\r\n \r\nwould be fantastic.\r\n \r\n \r\n \r\nConsumer\r\n \r\nbook:\r\n \r\nthis 4 percent\r\n \r\nq\r\n-\r\no\r\n-\r\nq\r\n \r\ngrowth\r\n, \r\nwell, \r\nI don't think that we are \r\ngoing to grow each quarter by 4 percent.  This is partially related to the base \r\neffect at the end of the year.  There was a drop in the consumer \r\nloan volumes\r\n \r\nd\r\nue to the fact that the bonuses were paid in December and that was kind of \r\none\r\n-\r\noff decline, but certainly what we see now is well above our expectations \r\nin terms of the potential new sales we can do and therefore, again we don't \r\nexpect quarterly 4 percent \r\ngrowth, but we expect quarterly growth in this \r\nportfolio.\r\n \r\n \r\n \r\nCorporate\r\n \r\nbook:\r\n \r\nyes, there were obviously bigger tickets and these bigger \r\ntickets are always kind of either they come or not, I mean you can't \r\ntake \r\nthem\r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n17\r\n \r\n \r\n \r\ngranted\r\n, but the pipeline is strong\r\n. W\r\ne don\r\n't expect quarterly 7 percent or 8 \r\npercent growth in corporate loans, but we expect further growth in corporate \r\nvolumes in Hungary,\r\n \r\nthat's clear.\r\n \r\n \r\n \r\nN\r\now your question related to OPEX\r\n, each year, we have this s\r\neasonality, \r\nright.  I\r\nf you look back to the last\r\n \r\ncouple of years, you always s\r\nee this \r\nseasonality and the OPEX\r\n \r\nin the first quarter is lower and then\r\n,\r\n \r\nthe subsequent \r\nquarters increased\r\n, whereas\r\n \r\nthe last qua\r\nrter\r\n \r\nis a kind of spike and a higher \r\nnumber.  Therefore, I suggested to compare the first quarter \r\nthis year \r\nrather to \r\nlast\r\n \r\nyear first quarter and the growth was 1 percent\r\n \r\nFX adjusted\r\n. \r\n \r\n \r\nSo, I think this is a number, meaning \r\nthe FX adjusted one\r\n, we have 1 percent \r\noverall growth in \r\nOPEX.\r\n \r\nI \r\nwould \r\nstick\r\n \r\nto \r\nour\r\n \r\nprevious guidance on this when \r\nwe said that we expected similar F\r\nX adjusted growth this year what\r\n \r\nwe had \r\nlast year, so potentiall\r\ny around 2\r\n-\r\n3\r\n \r\npercent\r\n.\r\n \r\n \r\n \r\nCost saving potentials, this is not what we are foc\r\nusing on at the moment.  \r\nThere're\r\n \r\nalways opportunit\r\nies here and there, and \r\neven \r\nwe do have\r\n \r\ncost \r\nreduction initiatives,\r\n \r\ncurrently \r\nthis is not the main target.  We see in Hungary\r\n \r\na reviving environment coupled with\r\n \r\nincreasing demand.  We are very hopeful \r\nthat Russian demand for loans will increase as the Rus\r\nsi\r\nan environment \r\nimproves.  Let me\r\n \r\nremind you that last year we made a huge cost reduction \r\nprogram in Russia.  Certainly, we would be actually quite happy to increase \r\nthe cost base in Russia, if it \r\nis coupled\r\n \r\nwith sizable volume growth.\r\n \r\n \r\n \r\nLikewise, Ukraine\r\n, we did a very drastic cost reduction program in Ukraine.  \r\nThis is not very evident, if you look at the numbers we have here, but given \r\nthe level of inflation during the last three years, which was close to 100 \r\npercent, then actually keeping the levels is\r\n \r\na big achievement in Ukraine.  So \r\nthis is not our main focus.  We do still expect realizing all the cost synergies \r\nfrom the acquisition in Romania, so that's ongoing throughout this year, and \r\nthere will be some late impact from this in terms of improvemen\r\nt, but other \r\nthan that no major changes in terms of cost efficiency improvements expected.  \r\nWe a\r\nre actually kind of switching our mindset\r\n \r\ninto a growth mode, which \r\nmakes us actually much happier than just cutting costs.\r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n18\r\n \r\n \r\n \r\n \r\nPawel Dziedzic\r\n:\r\n \r\nAnd that's very hel\r\npful.  So in other words, we should not look into 1 percent \r\ndecline\r\n, FX adjusted this quarter as a run rate, you would rather stick to 2 \r\npercent, 3\r\n \r\npercent\r\n \r\nincrease \r\ngoing forward as indicated previously?\r\n \r\n \r\nLaszlo Bencsik:\r\n \r\nYes.\r\n \r\n \r\nPawel Dziedzic\r\n:\r\n \r\nThank you.\r\n \r\n \r\nO\r\nperator\r\n:\r\n \r\nThank you.  Your next question comes from the line of \r\nSimon Nellis, \r\nDeutsche Bank.\r\n  \r\nPlease go ahead. \r\n \r\n \r\nSimon Nellis\r\n:\r\n \r\nHi, Mr. Bencsik.  It's actually Citibank, but that's okay.  Yes, my first question \r\nwould be just on the margin outlook in Hungary.\r\n  \r\nI think you did mention that \r\nyou're looking for a bit larger contraction, but could you give us some steer on \r\nwhere the margin in Hungary this year is expected to go and actually also in \r\nBulgaria, there was a pretty steep drop, so maybe a little more on \r\nthose two \r\nmarkets?  And then maybe I can ask my other questions afterwards?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nIn Hungary\r\n \r\nthere was indeed a steep drop.  It went down to 3.43 percent.  And \r\nthe second quarter will be \r\neven \r\nlower, so there we expect further drop and \r\nthen start\r\ning from the second half of this year, we expect the volume growth \r\nto counterbalance the pressure on the margin.  By volume growth, I mean the \r\nasset base remaining the same, but kind of converting our liquid reserves into \r\nloans,\r\n \r\nso basically increasing the\r\n \r\nmargin on the asset side by doing more \r\nle\r\nnding.  T\r\nhe expectation for Hungary is that the second quarter will be \r\nfurther down and the second half of the year should actually show some \r\nimprovement, at least that's the current expectation what we have.  And \r\nif this \r\nhappens then on a yearly basis, we might expect something similar to the first \r\nquarter level.\r\n \r\n \r\nSimon Nellis\r\n:\r\n \r\nFor the full year?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nYes.\r\n \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n19\r\n \r\n \r\n \r\nSimon Nellis\r\n:\r\n \r\nRight, in Hungary, okay.  And for the group, you're basically guiding for \r\naround 25\r\n \r\nbasis points?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nYes.\r\n \r\n \r\nSimon Nellis:\r\n \r\nOkay.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nOverall, year\r\n-\r\non\r\n-\r\nyear decline.  So, that's not compared to first quarter.  That's \r\ncompared to last year.\r\n \r\n \r\nSimon Nellis:\r\n \r\nRight.  And can you give us a little color on the yield on \r\nthe existing \r\nmortgages, kind of the back book and what the yield is on the new production \r\nthat you're doing?  And maybe also give us some color on the yields in \r\nHungary on corporate\r\n,\r\n \r\nconsumer and mortgages?  I'm trying to figure out as \r\nthe expected growth \r\nin mortgage, what's that actually going to do\r\n \r\nto your \r\nmargin and likewise, i\r\ns fast growth that you are seeing in corporate that really \r\nhelp or is that much lower margin business than what you have historically \r\ndone?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nYou \r\ndon't seem to belie\r\nve me, right?\r\n  \r\nI just told you \r\n--\r\n \r\n \r\nSimon Nellis:\r\n \r\nKicking with higher, right, so to speak.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nThe yield of the new production in mortgages is very similar to the yield on \r\nthe converted mortgages, right.\r\n \r\n \r\nSimon Nellis:\r\n \r\nOkay.  And what's that \r\nexactly?\r\n \r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nAround 4.5 percent, and this is the spread actually.  So, this is not the yield.  \r\nThis is the spread.  And then it's certainly lower than the very old subsidized \r\nHUF mortgages, which have been declining for the last 10 years now.\r\n  \r\nSo, this \r\nis something which is not a new development.  \r\nAs for t\r\nhe corporate spreads, \r\nlarge corporate spreads are around 2 percent.  SME spreads, well, they \r\nac\r\ntually \r\nshould increase because for the last two years, we mostly sold loans \r\nunder this lending f\r\nor growth program where the spread was capped at 2.5 \r\npercent.  \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n20\r\n \r\n \r\n \r\n \r\nNow, this is phasing out.  It's existing, but the new volumes are quite small.  \r\nSo, the actual market rates are somewhat higher than 2.5 percent spread.  So, \r\nthe micro, small, medium spreads \r\nof new production actually increased \r\ncompared to what we saw during the last two years, and large corporates \r\nbetween 1 percent to 2 percent spread.\r\n \r\n \r\nSimon Nellis\r\n:\r\n \r\nRight.  And on the consumer side, just finish it off.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nConsumer is really jui\r\ncy.  So, that's a very good business.\r\n \r\n \r\nSimon Nellis\r\n:\r\n \r\nOkay.  And then maybe one last question just on Touch Bank.  You \r\nmentioned in the past that you were thinking of rolling it out elsewhere.  Is \r\nthat happening or what's the \r\n\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nNo, not yet.\r\n  \r\nFirst, we want to see this to succeed, right.\r\n \r\n \r\nSimon Nellis\r\n:\r\n \r\nRight.\r\n \r\nThanks so much.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nThank you.\r\n \r\n \r\nOperator\r\n:\r\n \r\nThank you.  Your next question comes from the line of \r\nGabor Kemeny, \r\nAutonomous Research\r\n.  Please go ahead. \r\n \r\n \r\nGabor Kemeny\r\n:\r\n \r\nThank yo\r\nu.  \r\nM\r\ny first question on Romania, I think you mentioned that 20 \r\npercent of your clients decided not to participate in this FX conversion \r\nscheme.  I recall you assumed more like 100 percent participation when you \r\nset aside the provisions for this scheme.  S\r\no, do you have some room for \r\nprovision write\r\n-\r\nbacks here.  And secondly, maybe if you can follow up on the \r\nprofitability of the overall Hungarian business, how would you expect the \r\noverall profitability of OTP Core to evolve from here?  I mean, your \r\nprovisi\r\noning was not too far from zero in the first quarter.  I guess, you \r\nmentioned that loan growth could accelerate somewhat in the second half.  \r\nYou are not really targeting \r\ncost\r\n \r\ngrowth.  If we put these together, it could be \r\nhelpful if you could give us a se\r\nnse how the overall profitability of this \r\nbusiness could develop.\r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n21\r\n \r\n \r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nSo, regarding Romania, yes, when you do these voluntary programs, there is \r\nalways a certain share of customers who don't want to do it.  Now, there can \r\nbe two reasons why th\r\ney don't \r\nparticipate\r\n.  One is that they don't want to do it \r\nbecause they go to court and they try to have a larger discount by going \r\nthrough at least a legal process.  And then, you have another type of \r\ncustomers, \r\nwho don't care\r\n \r\neither because they don't c\r\nare or because the \r\nvolume of the loan is so small that it's not worth \r\ngoing \r\nto the branch and do \r\nthe paperworks.  \r\n \r\n \r\nNow obviously for those clients who proceed with the legal \r\naction\r\n, we do \r\nh\r\nave to have the provision there. T\r\nhe very reason we did this progr\r\nam was \r\nthat \r\nthere had been a very considerable threat that a very large proportion of \r\nour clients actually go through the legal process and we might lose actually \r\nmuch more than the discount we gave to these customers, right.\r\n \r\n \r\n \r\nSo I believe it's quite reas\r\nonable to keep the provisions for those clients who \r\ndecide to pursue legal actions against the bank.  And for those customers who \r\njust don't do anything, yes, we will release provisions probably, but we don't \r\nknow it yet.  So this is not going to happen in\r\n \r\nthe second quarter.  Maybe b\r\ny \r\nthe end of the third quarter \r\nwe are going to have a clear picture\r\n. W\r\nhen the \r\nprogram is over we will see exactly which clients are going to proceed with \r\nlegal actions, right.  So not in the second quarter and not in the third \r\nquarter, \r\nbut probably at the end of the third quarter, we might be in a position to \r\nreassess the situation.\r\n \r\n \r\n \r\nCore profitability \r\nI already said \r\nwhat we expected for the margin.  Volume \r\ngrowth should happen this year.  It already happened during the first q\r\nuarter \r\nand seeing the market demand, it should continue during the course of this \r\nyear.  It's going to be still low\r\n-\r\nsingle\r\n-\r\ndigits, lower than 5 \r\npercent overall \r\ngrowth\r\n \r\nin the volumes in Hungary, but it might be actually higher, so the \r\nsecond half might be h\r\nigher.  I really don't know because we were positively \r\nsurprised actually by the first quarter, especially the consumer loan \r\ndevelopments and the current demand for consumer loans, that clearly \r\nexceeded our previous expectations.  So\r\n,\r\n \r\nI don't know actually\r\n.  I\r\nt might \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n22\r\n \r\n \r\n \r\ncontinue to surprise \r\nus \r\npositively, bec\r\nause there's a strong real wage\r\n \r\ngrowth \r\nand the mood is relatively good.  Real estate price is going up and so on and \r\nso on.  So this might increase spending.\r\n \r\n \r\n \r\nRisk cost, I've said it before, as I expect s\r\ntructurally lower level of risk costs \r\nnot just for this year, but for the next period in Hungary, which might be next \r\ntwo or three years, even more.  It may not be zero, but it should certainly be \r\nlow \r\nand\r\n \r\nI don't see any reason why it should be high number\r\n.  \r\nT\r\nhe cost \r\nbase, \r\nwe have already touched upon\r\n, so these are more or less the \r\nmoving \r\nparts.  \r\nAs \r\nI said \r\nwe\r\n \r\nexpect further decrease in the margin in the second quarter.  So \r\nrevenues might decrease in sec\r\nond quarter and then hopefully \r\nand hopefully \r\nthe revenue trend bottoms out \r\nin the \r\nsecond quarter this year; that would be\r\n \r\nour\r\n \r\nkind of optimistic expectation.  And then, we should see revenue growth \r\ndriven by volume growth, despite the lower interest rate environment.\r\n \r\n \r\nGabor Kemeny\r\n:\r\n \r\nThat\r\n's very clear and I guess\r\n \r\nthat the risk environment is very benign and you \r\nhave a high NPL coverage.  Can you give us any sense where maybe a range \r\nwhere this \r\n--\r\n \r\nwhere would you see your provisioning in the, let's say, two \r\nyears?  I mean, in the domestic b\r\nusiness?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nSure.  I don't see why \r\nthe risk cost rate\r\n \r\nshould be higher than 50 basis points.  \r\nSo, it's somewhere between zero and 50 basis points.  We will see.  The other \r\nquestion is, how much we can recover?  Because seeing the strong \r\ndevel\r\nopment in the real estate market, we're certain \r\nabout positive \r\ndevelopments. F\r\nor many years, there was a moratorium on foreclosure, right?  \r\nSo, it was \r\nfairly\r\n \r\ndifficult to work out the non\r\n-\r\nperforming mortgage portfolio \r\nand the market was also quite sluggish\r\n.  Now, this has changed a lot, and now \r\nwe have a very dynamic real estate market where prices are going up and \r\nthere are no further restrictions on foreclosures \r\nor\r\n \r\ndoing proper work out in \r\nthis area.\r\n \r\n \r\n \r\nSo again, this can be a rather positive surprise if we manage to have higher \r\nrecoveries on the mortgage portfolio than the original provision.  And if that \r\nhappens, then we might be able to actually write back provisions and risk \r\ncosts can even be positive\r\n.  But what I can say officially about the level of \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n23\r\n \r\n \r\n \r\nprovisions is that it's adequate and proper and in line with our expectations to \r\neventual credit losses.  But \r\nthere \r\ncan be a positive surprise seeing the \r\ndevelopments in the environment.\r\n \r\n \r\nGabor Kemeny\r\n:\r\n \r\nAn\r\nd when you say up to 50 basis points, you don't assume any substantial \r\nwrite backs there?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nCertainly, no.  If you look at the portfolio, it's not deteriorating, technically.  \r\nAnd then there are possible write\r\n-\r\nbacks.  On the other hand, we f\r\nind it prudent \r\nto have some levels of provisioning.  And there's IFRS 9 coming in 2018.  So, \r\nthere can be other factors.  But, I think, what I can say for certain, I expect a \r\nlower risk environment in Hungary and\r\n \r\nI don't see any reason why it sh\r\nould \r\nchange\r\n \r\nfor the next two years or three years.  Elections come in 2018.  So, \r\nnext year will be quite strong in terms of GDP \r\nas well as 2018\r\n.  So, we are \r\ngoing to see, I believe, two very dynamic and prosperous years in the country \r\nin which case, risk costs should\r\n \r\nbe quite low.\r\n \r\n \r\nGabor Kemeny\r\n:\r\n \r\nI have one final question.  I think you accrued about HUF13 billion for \r\ndividend payment in the first quarter.  Could this be any kind of indication of \r\nhow management thinks about the dividend payout next year?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nYes.  If you multiply it by four, then you roughly get to the number which is \r\nour initial thinking about the potential levels of dividends.  But obviously, this \r\nis not final.  This is just an indication, but I think a good indication.\r\n \r\n \r\nGabor Kemeny\r\n:\r\n \r\nOkay\r\n.  Thanks very much.\r\n \r\n \r\nOperator\r\n:\r\n \r\nYour next question comes from the line of \r\nAlan Webborn, Societe Generale.\r\n  \r\nPlease go ahead. \r\n \r\n \r\nAlan Webborn\r\n:\r\n \r\nThanks for the call.  \r\nCould you put a little bit of color on where the sort of \r\ncorporate lending demand is coming fr\r\nom?  Is it more private, large \r\ncorporates?  Is it investment loans?  Is it states?  Just to give an idea as to \r\nwhere that's coming from.\r\n \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n24\r\n \r\n \r\n \r\n \r\nAnd you started the call by telling us that you're revising down your GDP \r\nbecause of this sort of maybe EU investment\r\n \r\nhiatus, and I just wondered, \r\nshould we be a little concerned that's being a sort of rush and there could be a \r\nlittle more caution as if \r\n--\r\n \r\nat least in the short term, the economy is weakening.  \r\nSo, that would be really helpful to understand what the corpo\r\nrate lending \r\ndynamics are looking like, in Hungary in particular.\r\n \r\n \r\n \r\nAnd secondly, perhaps a little bit of a view in terms of how the competitive \r\nenvironment is developing.  You paint a quite positive picture of the dynamics \r\nof the banking economy and you t\r\nalk about juicy consumer margins.  Then, \r\ncapital tends to move to where juicy margins are.  Do you feel there is more \r\ncompetition coming in in unsecured retail or anywhere else in your major \r\nmarkets, particularly again in Hungary?  That would be interestin\r\ng to hear \r\nyour view on that.\r\n \r\n \r\n \r\nAnd I guess finally, where are you sort of from an investment perspective in \r\nyour digitalization process?  The sort of back\r\n-\r\nend and front\r\n-\r\nend \r\n--\r\n \r\nyou put a \r\nlot of impressive applications on your slide, but I wonder where the G\r\nroup is \r\nin terms of where it want it to be and when it gets to a point where that \r\ninvestment can start paying dividends in terms of cost efficiencies?  Again, \r\nyour view on where you are in that process would be helpful.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nRelated to corporat\r\ne:\r\n \r\nthe micro and small growth was 2 percent in the first \r\nquarter.  So, that's just the very plain vanilla lending to micro and small \r\ncorporates, and there is no one\r\n-\r\noff there.  And so this is what we expect to \r\ncontinue pretty much, and as a quarterly growt\r\nh, 2 percent, this is already very \r\nstrong.  There were some large corporate deals.  There was one big \r\ncommercial factoring deal, for instance.  This is one of the \r\nparticular \r\nbusiness \r\nlines\r\n \r\nwhich we are trying to develop\r\n \r\nstrategically\r\n, I mean the\r\n \r\ncommercial\r\n \r\nfactoring.  So, it was a mix.\r\n \r\n \r\n \r\nThe\r\n \r\nkind of underlying growth was roughly 2 percent, and then it went up to 7 \r\npercent by some bigger tickets, but these bigger tickets, they don't come every \r\nquarter.  Therefore, I said that it's obvious that we are not goi\r\nng to grow by 7 \r\npercent, 8 percent quarterly because then the annual growth rate would be,\r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n25\r\n \r\n \r\n \r\nwell, I don't know\r\n.  So, this is not going to happen, clearly.  But a good 6 \r\npercent to 8 percent, overall 10 percent growth in corporate, that should be \r\npossible.  \r\n \r\n \r\nNow, the \r\ncorporate lending dynamics\r\n, around 6 percent to 8 percent growth in \r\nmicro, small, which is the kind of less volatile part, and then on top of that, it \r\ncan be pretty much around 10 percent, anything plus/minus, depending on the \r\nother corporate \r\nand kind of large corporate growth.  It can be actually double\r\n-\r\ndigit growth for the full year.\r\n \r\n \r\n \r\nA\r\ngain, the bigger tickets in the first quarter were partially commercial, \r\npartially related to kind of state enterprise funding.  This is as well\r\n, \r\na \r\ndeveloping\r\n \r\narea because quite a number of utilities, energy companies and so \r\non now belong to the state sector.  So, really big ticket investments and \r\ndevelopments are expected on that front.  By the way, a huge share of the \r\nfunding of the corporate sector is done c\r\nross\r\n-\r\nborder.  So, the structure of the \r\nlarge corporate lending in Hungary is such that a large share of corporate \r\nlending is not done through the Hungarian banking sector, but \r\nrather through\r\n \r\ngroup funding from the parent companies somewhere in Europe, typi\r\ncally.  \r\n \r\n \r\nAnd this is not going to be channeled through the Hungarian banking sector, \r\nthat's clear.  It will remain as cross\r\n-\r\nborder lines.  So, the kind of larger ticket \r\ngrowth potential is in the state\r\n-\r\nowned sector, which actually increased a lot \r\nduring t\r\nhe last couple of years.  I don't know how much you follow the \r\nprocess, but they basically re\r\n-\r\nprivatize\r\nd\r\n \r\nthe utility sector mostly, and there are \r\nlarge investment plans.\r\n \r\n \r\n \r\nBy the way, the fiscal room is huge for state development.  If you look at the \r\nbudget, it looks phenomenal, and certainly the next few years will provide \r\nopportunity to more spending and still \r\nkeeping fiscal deficit\r\n \r\nbelow 2.5 \r\npercent\r\n.\r\n \r\n \r\n \r\n \r\nNow, \r\non \r\ncompetitiv\r\ne dynamics, I would describe the environment as healthy \r\nin terms of competition.  Everyone is trying to grow and compete, but we \r\ndon't see crazy behavior, neither in pricing nor in risk taking.  How long it's \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n26\r\n \r\n \r\n \r\ngoing to last?  I do not know.  Certainly, we h\r\nave two type of players in \r\nHungary.  Some of them have already kind of doing okay returns or coming \r\nback to okay levels of profitability.  And then, we have some which are still \r\nnot doing very well.  Part of them are state\r\n-\r\nowned like MKB, part of them are \r\nnot state\r\n-\r\nowned, but there is clearly a big appetite for profits and returns as \r\nopposed to market share, I would say.  And given the losses during the last \r\ncouple of years, I think now certainly\r\n \r\nthere is\r\n \r\na \r\nstrong \r\ndesire to recover at \r\nleast part of those lo\r\nsses for our competitors.  This makes us somewhat \r\nhopeful that no one will start a crazy price competition or something like that, \r\nand I certainly don't expect new players, at least short\r\n-\r\nterm, to come to the \r\nmarket.  But this is obviously hard for us to j\r\nudge how the competitive \r\ndynamics is going to evolve in the future.\r\n \r\n \r\n \r\nDigitalization, again I think next time, I'll elaborate more on this topic because \r\nthis is something we are very excited about and devoting a lot of effort, \r\nenergy and money to develop v\r\narious digital services and to actually make an \r\ninternal digital transformation as well because we are working on two fronts.  \r\nObviously, one is the external part, the services we provide to our customers, \r\nand the other front is the internal processes, how\r\n \r\nwe manage and conduct \r\nbusiness internally.  And on both fronts, there are a number of initiatives.  We \r\nhave more than 100 different individual projects under this umbrella only in \r\nHungary, and some of the results are already out in the market, so you see \r\nthat.  \r\n \r\n \r\nAnd I think, I will try to be more specific on this and prepare a few slides \r\nabout this for the next presentation.  But clearly, this is something where we \r\nfocus, and we have a program which we laid down in the beginning of last \r\nyear and we are ex\r\necuting it.  So, we know where we're growing and we \r\nbelieve that this is going to elevate our service level to a different platform \r\nand our target is clearly to continue to stay ahead of the competition in \r\nHungary.\r\n \r\n \r\nAlan Webborn\r\n:\r\n \r\nOkay.  That was very inter\r\nesting.  Thank you very much. \r\n \r\n \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n27\r\n \r\n \r\n \r\nOperator\r\n:\r\n \r\nThank you.  Your next question comes from the line of \r\nStefan Maxian, RCB.\r\n  \r\nPlease go ahead. \r\n \r\n \r\nStefan Maxian\r\n:\r\n \r\nThank you.  Good afternoon.  \r\nJust a few questions from my side.  First, you \r\ntalked about this stronger co\r\nrporate loans in Hungary, and with regard to the \r\nlarger tickets as well.  Why has been the fee line actually \r\n--\r\n \r\nthe corporate fee \r\nline in Hungary a bit weaker, if you look at the year\r\n-\r\non\r\n-\r\nyear development?  \r\nHas there been a special reason for that?\r\n \r\n \r\n \r\nThe se\r\ncond on AXA actually, can you give us more clarity when you expect \r\nclosing of this?  And if there would be some \r\n--\r\n \r\nmaybe give a little bit of \r\nguidance, if there are some special costs related to that, or if we might even \r\nsee a positive consolidation effect\r\n \r\nas you brought that portfolio quite cheaply.\r\n \r\n \r\n \r\nAnd then just for clarification on the Visa Europe transaction, what is \r\n--\r\n \r\nagain, \r\nI don't know if I got it right, what's the amount that you will book in the P&L \r\nlater this year and how much of that has been \r\nalready accounted for in equity \r\nlast year via revaluation gains?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nSo, corporate fee, I see this number on page 13 of the interim report.  I have to \r\ncheck.  I mean, there is no obvious answer I can give you at the moment.  I \r\nhave to check th\r\nis.  I think, there is some one\r\n-\r\noff or irregular item behind this \r\nbecause there is no obvious reason.  We are going to check it and come back \r\nto this, okay?\r\n \r\n \r\nStefan Maxian\r\n:\r\n \r\nOkay.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nRegarding AXA, we are going to conclude the transaction by \r\nthe end of this \r\nyear.  And what we have said was that we expect the volumes will increased \r\nby 20 percent, the mortgage volumes.  Obviously, there are some repayments \r\nby that time\r\n, so\r\n \r\nthe performing volumes are a bit lower.  \r\nConsequently,\r\n \r\nit's \r\nsomewhat less\r\n \r\nthan 20 percent.  And what we expect is an increase from then \r\non in the revenues.  \r\nH\r\nonestly, we haven't communicated the structure, how \r\nthis is set up and how exactly the pricing is done.  So, if you excuse me, I'm \r\nnot going to elaborate on this.  But onc\r\ne we close the transaction, it's going to \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n28\r\n \r\n \r\n \r\nbe more or less obvious with what discount and what pricing parameters we \r\nacquired that portfolio.\r\n \r\n \r\n \r\nVisa Europe, the P&L \r\nimpact\r\n \r\nis going to come somewhere during the second \r\nhalf of \r\nthis\r\n \r\nyear.  It appeared in the c\r\nomprehensive income during the fourth \r\nquarter\r\n \r\nlast year\r\n, so it's part of the equity.  And the amount is this, what we \r\nsaid, \r\nEUR\r\n34.2 million.  This is wh\r\nat\r\n \r\nwe booked into the equity and it will \r\nappear in the P&L somewhere during the second half of this year\r\n.  This was \r\nthe initial amount which we were told we would get as an initial cash payment \r\nand this is the amount which is going to increase because it's structured in a \r\nway that there's an initial payment and then there is an earn\r\n-\r\nout component.  \r\nAnd the e\r\narn\r\n-\r\nout component is going to be less and the initial payments will \r\nbe more, but we don't know yet how much more and much less.  So, it should \r\nbe more than EUR34.2 million, but we don't know how much more.  It's \r\nalready in the equity\r\n \r\nsince\r\n \r\nthe fourth quart\r\ner last year and it's going to come \r\nthrough the P&L somewhere \r\nin the \r\nsecond half of this year.\r\n \r\n \r\nStefan Maxian\r\n:\r\n \r\nAnd maybe one final clarification, you talked about this sp\r\necial tax that is \r\nbooked in OPEX\r\n \r\nand that should disappear from next year on, right?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nYes.\r\n \r\n \r\nStefan Maxian\r\n:\r\n \r\nSo, that w\r\nill be HUF1.7 billion lower OPEX\r\n \r\nfrom the special tax?\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nNo, that's actually the after\r\n-\r\ntax impact.  So, there w\r\nill be HUF2.1 billion lower \r\nOPEX\r\n, and the bottom line after\r\n-\r\ntax impact will be HUF1\r\n.7 billion.\r\n \r\n \r\nStefan Maxian\r\n:\r\n \r\nAll right.  Thank you.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nThank you.\r\n \r\n \r\nOperator\r\n:\r\n \r\nThank you.  There are no further questions at this time, please continue.\r\n \r\n \r\nLaszlo Bencsik\r\n:\r\n \r\nOkay.  Well, thank you very much.  Thank you for joining us today.  Thank \r\ny\r\nou for your interest.  I wish you all the best, and I hope you'll join us during \r\nthe second quarter conference call, which will be on \r\n12 \r\nAugust.  I wish you all \r\n JPMORGAN\r\n \r\nModerator: \r\nPaul Formanko\r\n \r\n05\r\n-\r\n13\r\n-\r\n16/14:00 GMT\r\n \r\nConfirmation # \r\n958739\r\n \r\nPage \r\n29\r\n \r\n \r\n \r\nthe best and a good vacation if you go for the summer, and thank you again \r\nfor joining us.  Bye\r\n-\r\nbye.\r\n \r\n \r\nOperator\r\n:\r\n \r\nAnd that concludes our conference for today.  Thank you all for participating.  \r\nYou may all disconnect.\r\n \r\n \r\n \r\n \r\n \r\nEND\r\n'''\r\nsentences = [Sentence(sent, use_tokenizer=True) for sent in split_single(AText)]\r\n\r\n```\r\nat above line I am getting Error, let me know if you need complete Code\r\n\r\n**Expected behavior**\r\nIt should run and give Entities\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Environment (please complete the following information):**\r\n - OS [Windows]:\r\n - Version [flair - 0.5]:\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1649", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1649/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1649/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1649/events", "html_url": "https://github.com/flairNLP/flair/issues/1649", "id": 627326788, "node_id": "MDU6SXNzdWU2MjczMjY3ODg=", "number": 1649, "title": "Meaningfulness of generated text from flair models", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-05-29T14:50:38Z", "updated_at": "2020-06-03T13:14:51Z", "closed_at": "2020-06-03T13:14:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'd like to ask a few question about the text generated from the flair training model(forward). The training data includes 13.8 million lines of text which domain varies from news to keyboard conversation data. (Most of them are keyboard conversation data). The validation set and the testing set contains 140k lines of sentences each. I've already trained for nearly 1 week and the perplexity of the validation set reaches to nearly 3.Then I found out that most of the generated short sentences are meaningful. But for the case of  long sentences, the words are meaningful in phrase level but not meaningful (I mean the generated phrases are not meaningfully connected) for sentence levels. Is this normal or did my language model training need something? I'm using the hidden state of 2048, mini-batch size of 100 and patience of 50.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1648", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1648/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1648/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1648/events", "html_url": "https://github.com/flairNLP/flair/issues/1648", "id": 626992022, "node_id": "MDU6SXNzdWU2MjY5OTIwMjI=", "number": 1648, "title": "Is there a way to use get_spans() for POS as well?", "user": {"login": "pidugusundeep", "id": 10946649, "node_id": "MDQ6VXNlcjEwOTQ2NjQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/10946649?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pidugusundeep", "html_url": "https://github.com/pidugusundeep", "followers_url": "https://api.github.com/users/pidugusundeep/followers", "following_url": "https://api.github.com/users/pidugusundeep/following{/other_user}", "gists_url": "https://api.github.com/users/pidugusundeep/gists{/gist_id}", "starred_url": "https://api.github.com/users/pidugusundeep/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pidugusundeep/subscriptions", "organizations_url": "https://api.github.com/users/pidugusundeep/orgs", "repos_url": "https://api.github.com/users/pidugusundeep/repos", "events_url": "https://api.github.com/users/pidugusundeep/events{/privacy}", "received_events_url": "https://api.github.com/users/pidugusundeep/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-29T04:37:37Z", "updated_at": "2020-06-03T03:59:43Z", "closed_at": "2020-06-03T03:59:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I would like to get the exact position of each tag and the annotation.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1647", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1647/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1647/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1647/events", "html_url": "https://github.com/flairNLP/flair/issues/1647", "id": 626637559, "node_id": "MDU6SXNzdWU2MjY2Mzc1NTk=", "number": 1647, "title": "Pretrained Model for Keyphrase Extraction", "user": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023068682, "node_id": "MDU6TGFiZWwxMDIzMDY4Njgy", "url": "https://api.github.com/repos/flairNLP/flair/labels/feature", "name": "feature", "color": "87b70e", "default": false, "description": "A new feature"}], "state": "closed", "locked": false, "assignee": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2020-05-28T16:15:39Z", "updated_at": "2020-06-11T18:17:58Z", "closed_at": "2020-06-11T18:17:58Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Since we've integrated Keyphrase Detection Datasets it might a cool to have pretrained model for this. As a foundation we can either use Datasets from #1621 or #1646.\r\nI would train some models and post the results here to check if it makes sense to integrate it.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1645", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1645/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1645/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1645/events", "html_url": "https://github.com/flairNLP/flair/issues/1645", "id": 626579663, "node_id": "MDU6SXNzdWU2MjY1Nzk2NjM=", "number": 1645, "title": "Why does DocumentPoolEmbeddings(..., fine_tune_mode='nonlinear) add randomness?", "user": {"login": "RouvenG", "id": 14348785, "node_id": "MDQ6VXNlcjE0MzQ4Nzg1", "avatar_url": "https://avatars0.githubusercontent.com/u/14348785?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RouvenG", "html_url": "https://github.com/RouvenG", "followers_url": "https://api.github.com/users/RouvenG/followers", "following_url": "https://api.github.com/users/RouvenG/following{/other_user}", "gists_url": "https://api.github.com/users/RouvenG/gists{/gist_id}", "starred_url": "https://api.github.com/users/RouvenG/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RouvenG/subscriptions", "organizations_url": "https://api.github.com/users/RouvenG/orgs", "repos_url": "https://api.github.com/users/RouvenG/repos", "events_url": "https://api.github.com/users/RouvenG/events{/privacy}", "received_events_url": "https://api.github.com/users/RouvenG/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-28T14:56:09Z", "updated_at": "2020-06-02T14:36:33Z", "closed_at": "2020-06-02T14:36:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Why is a _default initialised (thus random)_ `torch.nn.Linear` layer added when I use `fine_tune_mode='nonlinear'`?\r\n\r\n### Further Details:\r\nWhen I use the parameter `fine_tune_mode='nonlinear'` in `DocumentPoolEmbeddings` a Non-linearity (ReLU) is added as well as a `torch.nn.Linear` layer.\r\nIf I am not wrong, the weights of the linear layer of torch are initialised randomly with a scaled uniform distribution. This means that the embeddings become partly random for each newly initiated `DocumentPoolEmbeddings`.\r\n\r\nCan somebody please explain me why is layer is introduced.\r\n- Does this randomness improve the embedding in any way?\r\n- Is this layer supposed to be trained?\r\n- If yes and I don't train is it harmful to the embedding?\r\n\r\n\r\nI used this option because in the documentation of flair, it said:\r\n> The default operation is 'linear' transformation, but if you only use simple word embeddings that are not task-trained you should probably use a 'nonlinear' transformation instead:\r\n\r\n\r\n\r\n### Code Reference:\r\nhttps://github.com/flairNLP/flair/blob/63aeabf9a18bdf53af3bcba5bd80f43ac717656e/flair/embeddings/document.py#L198", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1644", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1644/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1644/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1644/events", "html_url": "https://github.com/flairNLP/flair/issues/1644", "id": 626466332, "node_id": "MDU6SXNzdWU2MjY0NjYzMzI=", "number": 1644, "title": "ClassificationCorpus not using all data provided", "user": {"login": "NielsRogge", "id": 48327001, "node_id": "MDQ6VXNlcjQ4MzI3MDAx", "avatar_url": "https://avatars2.githubusercontent.com/u/48327001?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NielsRogge", "html_url": "https://github.com/NielsRogge", "followers_url": "https://api.github.com/users/NielsRogge/followers", "following_url": "https://api.github.com/users/NielsRogge/following{/other_user}", "gists_url": "https://api.github.com/users/NielsRogge/gists{/gist_id}", "starred_url": "https://api.github.com/users/NielsRogge/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NielsRogge/subscriptions", "organizations_url": "https://api.github.com/users/NielsRogge/orgs", "repos_url": "https://api.github.com/users/NielsRogge/repos", "events_url": "https://api.github.com/users/NielsRogge/events{/privacy}", "received_events_url": "https://api.github.com/users/NielsRogge/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-28T12:23:18Z", "updated_at": "2020-06-03T06:59:27Z", "closed_at": "2020-06-03T06:59:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nClassificationCorpus is not using all the data provided in 3 txt files (train.txt, dev.txt and test.txt). I suspect this to be an encoding issue, however Pandas is able to read in all the data with the same encoding type (utf-16) provided as parameter. \r\n\r\n**To Reproduce**\r\nThis [colab notebook](https://colab.research.google.com/drive/14tnyFZhnQHppb8fim8ZLgEwT5hLZx2TB).\r\n\r\n**Expected behavior**\r\nI expect the ClassificationCorpus object to use all data provided. The weird thing is, when I read the txt data as Pandas dataframes, all data is read. But when I read the txt data as ClassificationCorpus objects, some data is not used, as can be seen by the difference in total length (see notebook). \r\n\r\n**Environment (please complete the following information):**\r\n - Google Colab\r\n - Flair: installed from source", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1643", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1643/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1643/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1643/events", "html_url": "https://github.com/flairNLP/flair/issues/1643", "id": 626187455, "node_id": "MDU6SXNzdWU2MjYxODc0NTU=", "number": 1643, "title": "Getting file not found error while using bytepair embedding", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-28T03:36:24Z", "updated_at": "2020-08-12T14:06:06Z", "closed_at": "2020-08-12T14:06:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\n@alanakbik , I got FileNotFoundError when I predicted with the model with character and  bytepair embedding. What I did was that I trained on a gpu and predicted on local cpu. Then it is showing file not found error with the gpu path. \r\n\r\n**To Reproduce**\r\n```\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import ColumnCorpus\r\nfrom flair.embeddings import TokenEmbeddings, StackedEmbeddings, BertEmbeddings, CharacterEmbeddings, BytePairEmbeddings, WordEmbeddings\r\nfrom flair.visual.training_curves import Plotter\r\nfrom flair.models import SequenceTagger\r\nfrom flair.trainers import ModelTrainer\r\nfrom typing import List\r\nfrom torch.optim.adam import Adam\r\nimport sys\r\nimport flair, torch \r\n\r\nflair.device = torch.device('cuda:0')\r\n\r\ncolumns = {0: 'text', 1: 'ner'}\r\ndata_folder =  './data/Model_Trainer_Data'\r\ncorpus: Corpus = ColumnCorpus(data_folder, columns,train_file= sys.argv[1] , dev_file=sys.argv[3], test_file= sys.argv[4], in_memory=False)\r\n\r\ntag_type = 'ner'\r\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\n# bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\r\n\r\nembedding_types: List[TokenEmbeddings] = [\r\n    CharacterEmbeddings(),\r\n    BytePairEmbeddings('my', 300),\r\n    # bert_embedding\r\n    ]\r\n\r\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\r\n\r\ntagger: SequenceTagger = SequenceTagger(hidden_size=256,\r\n                                        embeddings=embeddings,\r\n                                        tag_dictionary=tag_dictionary,\r\n                                        tag_type=tag_type,\r\n                                        rnn_layers = 1,\r\n                                        use_crf=True)\r\n\r\n# trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=Adam)\r\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n\r\ntrainer.train('./results/'+ sys.argv[2],\r\n              learning_rate=0.1,\r\n              embeddings_storage_mode = 'gpu',\r\n              mini_batch_size=64,\r\n              patience=5,\r\n              max_epochs=150,\r\n              train_with_dev=False,\r\n              checkpoint=True)\r\n\r\n# from pathlib import Path \r\n\r\n# checkpoint = 'results/model_chunk_6/checkpoint.pt'\r\n# checkpoint = tagger.load_checkpoint(Path(checkpoint))\r\n# trainer = ModelTrainer.load_from_checkpoint(checkpoint, corpus)\r\n\r\n# trainer.train('./results/'+ sys.argv[2],\r\n#               learning_rate=0.15,\r\n#               embeddings_storage_mode = 'gpu',\r\n#               mini_batch_size=16,\r\n#               patience=3,\r\n#               max_epochs=150,\r\n#               train_with_dev=False,\r\n#               checkpoint=True)\r\n\r\n\r\nplotter = Plotter()\r\n# plotter.plot_training_curves('./results/'+ sys.argv[2]+'/loss.tsv')\r\nplotter.plot_weights('./results/'+ sys.argv[2]+'/weights.txt')\r\n\r\n\r\n```\r\n\r\n**Expected behavior**\r\n\r\nI'd like to predict on cpu machines.\r\n\r\n**Screenshots**\r\n\r\nThis is where I got error\r\n\r\n![ss_1](https://user-images.githubusercontent.com/64765786/83096039-b47ab500-a0ca-11ea-91b5-bbdea6e9549c.png)\r\n\r\nShowing to gpu path\r\n\r\n![ss_2](https://user-images.githubusercontent.com/64765786/83096069-c9574880-a0ca-11ea-8715-9bbaa756837c.png)\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS [Linux-mint:19.3]:\r\n - Version [flair: 0.5 ]:\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1642", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1642/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1642/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1642/events", "html_url": "https://github.com/flairNLP/flair/issues/1642", "id": 625583681, "node_id": "MDU6SXNzdWU2MjU1ODM2ODE=", "number": 1642, "title": "NER model training too slow", "user": {"login": "nguyenvanhieuvn", "id": 12376486, "node_id": "MDQ6VXNlcjEyMzc2NDg2", "avatar_url": "https://avatars1.githubusercontent.com/u/12376486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nguyenvanhieuvn", "html_url": "https://github.com/nguyenvanhieuvn", "followers_url": "https://api.github.com/users/nguyenvanhieuvn/followers", "following_url": "https://api.github.com/users/nguyenvanhieuvn/following{/other_user}", "gists_url": "https://api.github.com/users/nguyenvanhieuvn/gists{/gist_id}", "starred_url": "https://api.github.com/users/nguyenvanhieuvn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nguyenvanhieuvn/subscriptions", "organizations_url": "https://api.github.com/users/nguyenvanhieuvn/orgs", "repos_url": "https://api.github.com/users/nguyenvanhieuvn/repos", "events_url": "https://api.github.com/users/nguyenvanhieuvn/events{/privacy}", "received_events_url": "https://api.github.com/users/nguyenvanhieuvn/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-05-27T10:42:34Z", "updated_at": "2020-05-29T14:14:27Z", "closed_at": "2020-05-29T14:14:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Thanks for a great framework, \r\n- I'm train a ner model but it seem training speed to very slow. It about ~3 hours to done 1 epoch with mini_batch_size = 64. \r\n- During model training, it use only 695MiB GPU RAM and < 6% GPU Compute and about 5 - 6GB CPU RAM.\r\n![Memory Use during model training](https://user-images.githubusercontent.com/12376486/83009611-37efc400-a041-11ea-9c82-4966b1906336.png)\r\n\r\n## Training device info\r\n- OS: Ubuntu 18.04\r\n- GPU: GTX 1080 Ti\r\n## Training configuration\r\n- Fasttext embedding, dim = 100\r\n- Corpus: 2500000 train + 750000 dev + 750000 test\r\n- Corpus mode: in_memory=False\r\n- Embeddings storage mode: None\r\n\r\n## Training log\r\n```\r\n2020-05-27 15:39:00,529 Model: \"SequenceTagger(\r\n  (embeddings): FastTextEmbeddings('embedding/news.prep.100.bin')\r\n  (word_dropout): WordDropout(p=0.05)\r\n  (locked_dropout): LockedDropout(p=0.5)\r\n  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\r\n  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\r\n  (linear): Linear(in_features=512, out_features=6, bias=True)\r\n  (beta): 1.0\r\n  (weights): None\r\n  (weight_tensor) None\r\n)\"\r\n2020-05-27 15:39:00,530 ----------------------------------------------------------------------------------------------------\r\n2020-05-27 15:39:00,530 Corpus: \"Corpus: 2500000 train + 750000 dev + 750000 test sentences\"\r\n2020-05-27 15:39:00,530 ----------------------------------------------------------------------------------------------------\r\n2020-05-27 15:39:00,530 Parameters:\r\n2020-05-27 15:39:00,530  - learning_rate: \"0.1\"\r\n2020-05-27 15:39:00,530  - mini_batch_size: \"64\"\r\n2020-05-27 15:39:00,530  - patience: \"3\"\r\n2020-05-27 15:39:00,530  - anneal_factor: \"0.5\"\r\n2020-05-27 15:39:00,530  - max_epochs: \"150\"\r\n2020-05-27 15:39:00,530  - shuffle: \"True\"\r\n2020-05-27 15:39:00,531  - train_with_dev: \"False\"\r\n2020-05-27 15:39:00,531  - batch_growth_annealing: \"False\"\r\n2020-05-27 15:39:00,531 ----------------------------------------------------------------------------------------------------\r\n2020-05-27 15:39:00,531 Model training base path: \"resources/punctuator-trainer_b64\"\r\n2020-05-27 15:39:00,531 ----------------------------------------------------------------------------------------------------\r\n2020-05-27 15:39:00,531 Device: cuda:0\r\n2020-05-27 15:39:00,531 ----------------------------------------------------------------------------------------------------\r\n2020-05-27 15:39:00,531 Embeddings storage mode: None\r\n2020-05-27 15:39:00,532 ----------------------------------------------------------------------------------------------------\r\n/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\r\n2020-05-27 15:59:45,014 epoch 1 - iter 3906/39063 - loss 7.44133509 - samples/sec: 228.62\r\n2020-05-27 16:20:18,614 epoch 1 - iter 7812/39063 - loss 7.22013033 - samples/sec: 230.63\r\n2020-05-27 16:41:12,607 epoch 1 - iter 11718/39063 - loss 7.05320136 - samples/sec: 226.84\r\n2020-05-27 17:02:01,065 epoch 1 - iter 15624/39063 - loss 6.91849714 - samples/sec: 227.89\r\n2020-05-27 17:22:56,335 epoch 1 - iter 19530/39063 - loss 6.80771093 - samples/sec: 226.83\r\n```\r\n## Question\r\n- Why my model training with low speed?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1638", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1638/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1638/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1638/events", "html_url": "https://github.com/flairNLP/flair/issues/1638", "id": 625027787, "node_id": "MDU6SXNzdWU2MjUwMjc3ODc=", "number": 1638, "title": "bert embeddings is running out of memory.", "user": {"login": "Lalit-001", "id": 64452286, "node_id": "MDQ6VXNlcjY0NDUyMjg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64452286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lalit-001", "html_url": "https://github.com/Lalit-001", "followers_url": "https://api.github.com/users/Lalit-001/followers", "following_url": "https://api.github.com/users/Lalit-001/following{/other_user}", "gists_url": "https://api.github.com/users/Lalit-001/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lalit-001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lalit-001/subscriptions", "organizations_url": "https://api.github.com/users/Lalit-001/orgs", "repos_url": "https://api.github.com/users/Lalit-001/repos", "events_url": "https://api.github.com/users/Lalit-001/events{/privacy}", "received_events_url": "https://api.github.com/users/Lalit-001/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-26T16:43:29Z", "updated_at": "2020-05-28T10:41:56Z", "closed_at": "2020-05-28T04:57:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "i am running this code on google colab to get the embedding of docs in \"long_desc\".\r\nlen(long_desc) is 20k\r\n\r\nembeddings = TransformerDocumentEmbeddings('bert-base-uncased')\r\n\r\n\r\nX = torch.empty(size=(len(long_desc), 768)).cuda()\r\ni=0\r\nfor text in tqdm(long_desc):\r\n    if len(text.split(' ')) < 300 :\r\n      print(text)\r\n      sentence = Sentence(text)\r\n      embeddings.embed(sentence)\r\n      X[i] = sentence.get_embedding()\r\n      sentence.clear_embeddings()\r\n      i += 1\r\n\r\nbut it gives RuntimeError: CUDA out of memory.  after 10-20 iterations in for loop.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1634", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1634/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1634/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1634/events", "html_url": "https://github.com/flairNLP/flair/issues/1634", "id": 624476595, "node_id": "MDU6SXNzdWU2MjQ0NzY1OTU=", "number": 1634, "title": "AttributeError: 'ELMoEmbeddings' object has no attribute 'embedding_mode_fn'", "user": {"login": "yzhaoinuw", "id": 22312388, "node_id": "MDQ6VXNlcjIyMzEyMzg4", "avatar_url": "https://avatars2.githubusercontent.com/u/22312388?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhaoinuw", "html_url": "https://github.com/yzhaoinuw", "followers_url": "https://api.github.com/users/yzhaoinuw/followers", "following_url": "https://api.github.com/users/yzhaoinuw/following{/other_user}", "gists_url": "https://api.github.com/users/yzhaoinuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhaoinuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhaoinuw/subscriptions", "organizations_url": "https://api.github.com/users/yzhaoinuw/orgs", "repos_url": "https://api.github.com/users/yzhaoinuw/repos", "events_url": "https://api.github.com/users/yzhaoinuw/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhaoinuw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-25T20:39:14Z", "updated_at": "2020-06-13T10:25:41Z", "closed_at": "2020-06-13T10:25:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi Flair developers, thank you for releasing version 0.5 of flair. I am very excited and wasted no time trying it. I upgraded it today to release 0.5 from release 0.45. Then I went ahead to run an NER module I made afew days ago to see if I need to make changes to my module because of the new flair release. My NER module uses the flair sequence tagger and the ELMo embeddings. But now I encounter the following error message:\r\n\r\n```\r\n  File \"/home/Desktop/name_parser.py\", line 44, in parse_name\r\n    self.model.predict(name,all_tag_prob=True)\r\n\r\n  File \"/home/anaconda3/envs/nlp/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py\", line 372, in predict\r\n    feature: torch.Tensor = self.forward(batch)\r\n\r\n  File \"/home/anaconda3/envs/nlp/lib/python3.7/site-packages/flair/models/sequence_tagger_model.py\", line 513, in forward\r\n    self.embeddings.embed(sentences)\r\n\r\n  File \"/home/anaconda3/envs/nlp/lib/python3.7/site-packages/flair/embeddings/base.py\", line 59, in embed\r\n    self._add_embeddings_internal(sentences)\r\n\r\n  File \"/home/anaconda3/envs/nlp/lib/python3.7/site-packages/flair/embeddings/token.py\", line 1594, in _add_embeddings_internal\r\n    word_embedding = self.embedding_mode_fn(elmo_embedding_layers)\r\n\r\n  File \"/home/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 585, in __getattr__\r\n    type(self).__name__, name))\r\n\r\nAttributeError: 'ELMoEmbeddings' object has no attribute 'embedding_mode_fn'\r\n```\r\n\r\nOne code in my module loads a NER model I trained, which used the ELMo emneddings. Then, I use the loaded model to predict NER tags. My module was working well with flair 0.45. All that changed was was that I upgraded flair to 0.5. I tried to look in to ELMoEmbeddings class in [token.py](https://github.com/flairNLP/flair/blob/083eb114505ede0ea3070317a5a08d13fc71891b/flair/embeddings/token.py) and found from line 1543 to 1548 where it is defined as follows\r\n\r\n```\r\n if embedding_mode == \"all\":\r\n        self.embedding_mode_fn = lambda x: torch.cat(x, 0)\r\nelif embedding_mode == \"top\":\r\n        self.embedding_mode_fn = lambda x: x[-1]\r\nelif embedding_mode == \"average\":\r\n        self.embedding_mode_fn = lambda x: torch.mean(torch.stack(x), 0)\r\n```\r\n\r\nI don't remember specifying the mode for ELMo when I trained the NER model. This is new feature in this release, right? Do I have need to retrain the model with ELMo and specify the mode? Is there a quicker fix?\r\n\r\nThank you!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1630", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1630/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1630/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1630/events", "html_url": "https://github.com/flairNLP/flair/issues/1630", "id": 624028465, "node_id": "MDU6SXNzdWU2MjQwMjg0NjU=", "number": 1630, "title": "Batch at get_representation is evaluated in a wrong order?", "user": {"login": "bratao", "id": 1090152, "node_id": "MDQ6VXNlcjEwOTAxNTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/1090152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bratao", "html_url": "https://github.com/bratao", "followers_url": "https://api.github.com/users/bratao/followers", "following_url": "https://api.github.com/users/bratao/following{/other_user}", "gists_url": "https://api.github.com/users/bratao/gists{/gist_id}", "starred_url": "https://api.github.com/users/bratao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bratao/subscriptions", "organizations_url": "https://api.github.com/users/bratao/orgs", "repos_url": "https://api.github.com/users/bratao/repos", "events_url": "https://api.github.com/users/bratao/events{/privacy}", "received_events_url": "https://api.github.com/users/bratao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-25T03:54:42Z", "updated_at": "2020-05-25T07:07:14Z", "closed_at": "2020-05-25T07:07:14Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hello, I\u00b4m trying to understand the Flair embedding. During my debug, I found something unexpected to me. \r\n\r\nAt the function `get_representation()` on `language_model.py`, it chunks every sentence in a batch every `chars_per_chunk`\r\nConsider a batch with two sentences of length 6\r\n[\"THIS IS A FIRST LONG STRING\",\r\n\"NOW THAT IS A DIFFERENT ONE\"]\r\n\r\nConsidering chars_per_chunk to be 3. The output is processed as:\r\nbatch 1 = [\"THIS IS A\", \"NOW THAT IS\"]\r\nbatch 2 = [\"FIRST LONG STRING\", \"A DIFFERENT ONE\"]\r\n\r\nHowever in this way, every sub-sequence is using a hidden state from a unrelated sentence. \r\nI would expect this instead:\r\n\r\nbatch 1 = [\"THIS IS A\", \"FIRST LONG STRING\"]\r\nbatch 2 = [\"NOW THAT IS\", \"A DIFFERENT ONE\"]\r\n\r\nThere is a motivation that the batching is done this way? @alanakbik \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1628", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1628/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1628/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1628/events", "html_url": "https://github.com/flairNLP/flair/issues/1628", "id": 623852023, "node_id": "MDU6SXNzdWU2MjM4NTIwMjM=", "number": 1628, "title": "use_dropout not getting saved in checkpoint ???", "user": {"login": "sankaran45", "id": 8388863, "node_id": "MDQ6VXNlcjgzODg4NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/8388863?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sankaran45", "html_url": "https://github.com/sankaran45", "followers_url": "https://api.github.com/users/sankaran45/followers", "following_url": "https://api.github.com/users/sankaran45/following{/other_user}", "gists_url": "https://api.github.com/users/sankaran45/gists{/gist_id}", "starred_url": "https://api.github.com/users/sankaran45/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sankaran45/subscriptions", "organizations_url": "https://api.github.com/users/sankaran45/orgs", "repos_url": "https://api.github.com/users/sankaran45/repos", "events_url": "https://api.github.com/users/sankaran45/events{/privacy}", "received_events_url": "https://api.github.com/users/sankaran45/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-24T12:10:42Z", "updated_at": "2020-06-03T17:21:26Z", "closed_at": "2020-06-03T17:21:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "I think the use_dropout parameter does not saved as part of state_dict due to bug in\r\n def _get_state_dict(self) \r\n\r\nas a result of which if we set a dropout layer in sequence tagger, and resume checkpoint training, the behavior changes.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1626", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1626/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1626/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1626/events", "html_url": "https://github.com/flairNLP/flair/issues/1626", "id": 623710402, "node_id": "MDU6SXNzdWU2MjM3MTA0MDI=", "number": 1626, "title": "Update documentation for 0.5 release", "user": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-23T17:57:49Z", "updated_at": "2020-05-24T12:34:53Z", "closed_at": "2020-05-24T12:34:53Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "This issue tracks the progress in updating our documentation for the upcoming release.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1624", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1624/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1624/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1624/events", "html_url": "https://github.com/flairNLP/flair/issues/1624", "id": 623380678, "node_id": "MDU6SXNzdWU2MjMzODA2Nzg=", "number": 1624, "title": "warning about numpy array is not writeable in fasttext word embedding", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-22T17:40:15Z", "updated_at": "2020-05-24T12:36:02Z", "closed_at": "2020-05-24T12:36:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm not sure about this is the bug or not. But when I trained the model with fast text embedding, the following warning occurs.\r\n\r\n> /pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\r\n\r\nTo reproduce the results, the following is my code .\r\n\r\n```\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import ColumnCorpus\r\nfrom flair.embeddings import FastTextEmbeddings,TokenEmbeddings, StackedEmbeddings, BertEmbeddings, CharacterEmbeddings, BytePairEmbeddings, WordEmbeddings\r\nfrom flair.visual.training_curves import Plotter\r\nfrom flair.models import SequenceTagger\r\nfrom flair.trainers import ModelTrainer\r\nfrom typing import List\r\nfrom torch.optim.adam import Adam\r\nimport sys\r\nimport flair, torch \r\n# import genism\r\n\r\nflair.device = torch.device('cuda:0')\r\n\r\ncolumns = {0: 'text', 1: 'ner'}\r\ndata_folder =  './data/Model_Trainer_Data'\r\ncorpus: Corpus = ColumnCorpus(data_folder, columns,train_file= sys.argv[1] , dev_file=sys.argv[3], test_file= sys.argv[4], in_memory=False)\r\n\r\ntag_type = 'ner'\r\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\n# bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\r\nft_embedding = FastTextEmbeddings('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.my.300.bin.gz', use_local=False)\r\n\r\nembedding_types: List[TokenEmbeddings] = [\r\n    CharacterEmbeddings(),\r\n    ft_embedding\r\n    # BytePairEmbeddings('my'),\r\n    \r\n    ]\r\n\r\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\r\n\r\ntagger: SequenceTagger = SequenceTagger(hidden_size=256,\r\n                                        embeddings=embeddings,\r\n                                        tag_dictionary=tag_dictionary,\r\n                                        tag_type=tag_type,\r\n                                        rnn_layers = 1,\r\n                                        use_crf=True)\r\n\r\n# trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=Adam)\r\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n\r\ntrainer.train('./results/'+ sys.argv[2],\r\n              learning_rate=0.1,\r\n              embeddings_storage_mode = 'gpu',\r\n              mini_batch_size=64,\r\n              patience=5,\r\n              max_epochs=150,\r\n              train_with_dev=False,\r\n              checkpoint=True)\r\n\r\nplotter = Plotter()\r\n# plotter.plot_training_curves('./results/4_class_test_2/loss.tsv')\r\nplotter.plot_weights('./results/'+ sys.argv[2]+'/weights.txt')\r\n\r\n```\r\nMy current system information is linux mint(19.3) and current lastest master branch.\r\n\r\nBtw, what I'd like to know is that this affects the model performance or not and if this is my code error, where can I solve it??\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1623", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1623/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1623/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1623/events", "html_url": "https://github.com/flairNLP/flair/issues/1623", "id": 623354786, "node_id": "MDU6SXNzdWU2MjMzNTQ3ODY=", "number": 1623, "title": "Fine-grained POS taggers", "user": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-22T16:54:50Z", "updated_at": "2020-05-22T20:13:46Z", "closed_at": "2020-05-22T20:13:46Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Our current POS models for English and German use universal dependency tags. But for some use cases you might prefer fine-grained POS tags (see #1349). \r\n\r\nThis issue tracks the addition of fine-grained models for English and German: \r\n\r\n- [ ] English POS model\r\n- [x] Fast English POS model\r\n- [x] German POS model ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1622", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1622/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1622/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1622/events", "html_url": "https://github.com/flairNLP/flair/issues/1622", "id": 623296854, "node_id": "MDU6SXNzdWU2MjMyOTY4NTQ=", "number": 1622, "title": "plot_weights figsize referenced before assignment", "user": {"login": "cod3licious", "id": 1093567, "node_id": "MDQ6VXNlcjEwOTM1Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1093567?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cod3licious", "html_url": "https://github.com/cod3licious", "followers_url": "https://api.github.com/users/cod3licious/followers", "following_url": "https://api.github.com/users/cod3licious/following{/other_user}", "gists_url": "https://api.github.com/users/cod3licious/gists{/gist_id}", "starred_url": "https://api.github.com/users/cod3licious/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cod3licious/subscriptions", "organizations_url": "https://api.github.com/users/cod3licious/orgs", "repos_url": "https://api.github.com/users/cod3licious/repos", "events_url": "https://api.github.com/users/cod3licious/events{/privacy}", "received_events_url": "https://api.github.com/users/cod3licious/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-22T15:39:32Z", "updated_at": "2020-05-28T22:46:26Z", "closed_at": "2020-05-28T22:46:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "`# figsize = (16, 16)` in [line 125 of Plotter.plot_weights](https://github.com/flairNLP/flair/blob/master/flair/visual/training_curves.py#L125) should be uncommented or the if statement below should be changed, otherwise this can throw `UnboundLocalError: local variable 'figsize' referenced before assignment`.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1621", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1621/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1621/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1621/events", "html_url": "https://github.com/flairNLP/flair/issues/1621", "id": 623170665, "node_id": "MDU6SXNzdWU2MjMxNzA2NjU=", "number": 1621, "title": "Keyphrase Extraction Dataset", "user": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023068682, "node_id": "MDU6TGFiZWwxMDIzMDY4Njgy", "url": "https://api.github.com/repos/flairNLP/flair/labels/feature", "name": "feature", "color": "87b70e", "default": false, "description": "A new feature"}], "state": "closed", "locked": false, "assignee": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2020-05-22T12:22:49Z", "updated_at": "2020-05-28T16:36:53Z", "closed_at": "2020-05-28T16:36:53Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "There are 3 datasets available for keyphrase extraction via sequence labeling from IIIT-Delhi:\r\n[Keyphrase Datasets](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data)\r\n\r\nI would like to add all three sets to the sequence labeling functionality of flair.\r\n\r\n- [x] [Inspec](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/tree/master/Inspec) \r\n\r\n- [x] [SemEval-2017](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/tree/master/SemEval-2017) \r\n\r\n- [x] [Processed SemEval-2010](https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/tree/master/processed_semeval-2010) \r\n\r\n<img width=\"1121\" alt=\"keyphrase-stats\" src=\"https://user-images.githubusercontent.com/38491117/82667075-65023800-9c37-11ea-9584-5a26420f064f.png\">", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1619", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1619/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1619/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1619/events", "html_url": "https://github.com/flairNLP/flair/issues/1619", "id": 622455558, "node_id": "MDU6SXNzdWU2MjI0NTU1NTg=", "number": 1619, "title": "Training NER for new language ", "user": {"login": "rockiram", "id": 20871228, "node_id": "MDQ6VXNlcjIwODcxMjI4", "avatar_url": "https://avatars3.githubusercontent.com/u/20871228?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rockiram", "html_url": "https://github.com/rockiram", "followers_url": "https://api.github.com/users/rockiram/followers", "following_url": "https://api.github.com/users/rockiram/following{/other_user}", "gists_url": "https://api.github.com/users/rockiram/gists{/gist_id}", "starred_url": "https://api.github.com/users/rockiram/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rockiram/subscriptions", "organizations_url": "https://api.github.com/users/rockiram/orgs", "repos_url": "https://api.github.com/users/rockiram/repos", "events_url": "https://api.github.com/users/rockiram/events{/privacy}", "received_events_url": "https://api.github.com/users/rockiram/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-21T12:07:05Z", "updated_at": "2020-06-01T04:23:25Z", "closed_at": "2020-06-01T04:23:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "\r\nI want to do NER for tamil language. I understood how to train the language model. \r\n1.How to extract the POS tag for tamil language? \r\n2.If I want to train POS tagger model seperately, please suggest me some reference?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1618", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1618/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1618/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1618/events", "html_url": "https://github.com/flairNLP/flair/issues/1618", "id": 621567586, "node_id": "MDU6SXNzdWU2MjE1Njc1ODY=", "number": 1618, "title": "Using ElmoEmbeddings crashes at the first save attempt. ", "user": {"login": "nstylia", "id": 3770033, "node_id": "MDQ6VXNlcjM3NzAwMzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/3770033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nstylia", "html_url": "https://github.com/nstylia", "followers_url": "https://api.github.com/users/nstylia/followers", "following_url": "https://api.github.com/users/nstylia/following{/other_user}", "gists_url": "https://api.github.com/users/nstylia/gists{/gist_id}", "starred_url": "https://api.github.com/users/nstylia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nstylia/subscriptions", "organizations_url": "https://api.github.com/users/nstylia/orgs", "repos_url": "https://api.github.com/users/nstylia/repos", "events_url": "https://api.github.com/users/nstylia/events{/privacy}", "received_events_url": "https://api.github.com/users/nstylia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-05-20T08:30:59Z", "updated_at": "2020-06-13T10:25:18Z", "closed_at": "2020-06-13T10:25:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Using stacked flair embeddings with elmo (or even standalone elmo I assume), results in an error while saving the model due to the lambda function in the elmo embeddings. As a result, the pickler in pytorch cannot save the model and crashes after the first epoch, while trying to save the model. The same code snippet works without the elmo embeddings (omitted parameters in SequenceTagger and train). \r\n\r\n`embedding_types: List[TokenEmbeddings] = [\r\n    PooledFlairEmbeddings('pubmed-forward'),\r\n    PooledFlairEmbeddings('pubmed-backward'),\r\n    ELMoEmbeddings('pubmed'),\r\n]`\r\n`embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)`\r\n`tagger: SequenceTagger = SequenceTagger(...)`\r\n`trainer: ModelTrainer = ModelTrainer(tagger, corpus)`\r\n`trainer.train(...)` \r\n\r\nThe error message: \r\n\r\n> Traceback (most recent call last):\r\n  File \"flair_main.py\", line 43, in <module>\r\n    max_epochs=100)\r\n  File \"/home/ns/envs/mldev/lib/python3.6/site-packages/flair/trainers/trainer.py\", line 552, in train\r\n    self.model.save(base_path / \"best-model.pt\")\r\n  File \"/home/ns/envs/mldev/lib/python3.6/site-packages/flair/nn.py\", line 70, in save\r\n    torch.save(model_state, str(model_file), pickle_protocol=4)\r\n  File \"/home/ns/envs/mldev/lib/python3.6/site-packages/torch/serialization.py\", line 328, in save\r\n    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\r\n  File \"/home/ns/envs/mldev/lib/python3.6/site-packages/torch/serialization.py\", line 401, in _legacy_save\r\n    pickler.dump(obj)\r\nAttributeError: Can't pickle local object 'ELMoEmbeddings.\\_\\_init\\_\\_.\\<locals\\>.\\<lambda\\> '\r\n\r\n\r\nEnvironment information: \r\nUbuntu 16.04\r\ntorch-1.4.0+cu101\r\nflair-0.4.5\r\n\r\nIs there any work around ? Am I doing something wrong? I don't think this is expected behavior. Also, I assume that pickle would be able to handle lambda functions.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1617", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1617/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1617/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1617/events", "html_url": "https://github.com/flairNLP/flair/issues/1617", "id": 621002197, "node_id": "MDU6SXNzdWU2MjEwMDIxOTc=", "number": 1617, "title": "Finnish NER", "user": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1023068682, "node_id": "MDU6TGFiZWwxMDIzMDY4Njgy", "url": "https://api.github.com/repos/flairNLP/flair/labels/feature", "name": "feature", "color": "87b70e", "default": false, "description": "A new feature"}], "state": "closed", "locked": false, "assignee": {"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "whoisjones", "id": 38491117, "node_id": "MDQ6VXNlcjM4NDkxMTE3", "avatar_url": "https://avatars3.githubusercontent.com/u/38491117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whoisjones", "html_url": "https://github.com/whoisjones", "followers_url": "https://api.github.com/users/whoisjones/followers", "following_url": "https://api.github.com/users/whoisjones/following{/other_user}", "gists_url": "https://api.github.com/users/whoisjones/gists{/gist_id}", "starred_url": "https://api.github.com/users/whoisjones/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whoisjones/subscriptions", "organizations_url": "https://api.github.com/users/whoisjones/orgs", "repos_url": "https://api.github.com/users/whoisjones/repos", "events_url": "https://api.github.com/users/whoisjones/events{/privacy}", "received_events_url": "https://api.github.com/users/whoisjones/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2020-05-19T14:02:32Z", "updated_at": "2020-05-22T20:13:11Z", "closed_at": "2020-05-22T20:13:11Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "There is a [Finnish NER](https://github.com/mpsilfve/finer-data) that provides NER training data. Data contains a corpus of Finnish technology related news articles (953 articles, 193,742 word tokens) with a manually prepared named entity annotation (six named entity classes (organization, location, person, product, event, and date)).\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1610", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1610/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1610/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1610/events", "html_url": "https://github.com/flairNLP/flair/issues/1610", "id": 617863198, "node_id": "MDU6SXNzdWU2MTc4NjMxOTg=", "number": 1610, "title": "AttributeError when finetuned bert model for downstream task is used", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-05-14T01:48:33Z", "updated_at": "2020-07-28T20:28:55Z", "closed_at": "2020-05-14T14:37:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI got the error called AttributeError:'BertTokenizer' object has no attribute 'model_max_length' when i used model.predict(sentence)\r\n\r\n\r\n**To Reproduce**\r\nI didn't get any errors when I trained. The script for training model is \r\n\r\n``\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import ColumnCorpus\r\nfrom flair.embeddings import TransformerWordEmbeddings, TokenEmbeddings, StackedEmbeddings, BertEmbeddings, CharacterEmbeddings, BytePairEmbeddings, WordEmbeddings\r\nfrom flair.visual.training_curves import Plotter\r\nfrom flair.models import SequenceTagger\r\nfrom flair.trainers import ModelTrainer\r\nfrom typing import List\r\nfrom torch.optim.adam import Adam\r\nimport sys\r\nimport flair, torch\r\n\r\n\r\nflair.device = torch.device('cuda:0')\r\ncolumns = {0: 'text', 1: 'ner'}\r\ndata_folder =  './data/Model_Trainer_Data'\r\ncorpus: Corpus = ColumnCorpus(data_folder, columns,train_file= sys.argv[1] , dev_file=sys.argv[3], test_file= sys.argv[4], in_memory=False)\r\n\r\ntag_type = 'ner'\r\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\r\n\r\nembeddings = TransformerWordEmbeddings(\r\n        'bert-base-multilingual-cased', # which transformer model\r\n        layers=\"-1\", # which layers (here: only last layer when fine-tuning)\r\n        pooling_operation='first_last', # how to pool over split tokens\r\n        fine_tune=True, # whether or not to fine-tune\r\n    )\r\n\r\ntagger: SequenceTagger = SequenceTagger(\r\n        hidden_size=256,\r\n        embeddings=embeddings,\r\n        tag_dictionary=tag_dictionary,\r\n        tag_type=tag_type,\r\n        use_crf=False,\r\n        use_rnn=False,\r\n    )\r\n\r\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.Adam)\r\n\r\ntrainer.train('./results/'+ sys.argv[2],\r\n              learning_rate=3e-5,\r\n              embeddings_storage_mode = 'gpu',\r\n              mini_batch_chunk_size=2, \r\n              max_epochs=4, \r\n              train_with_dev=False,\r\n              checkpoint=True)\r\n\r\nplotter = Plotter()\r\nplotter.plot_weights('./results/'+ sys.argv[2]+'/weights.txt')\r\n``\r\n\r\nThe prediction script is below.\r\n\r\n![to_send](https://user-images.githubusercontent.com/64765786/81883416-30e5a200-95bb-11ea-98ae-9ba1b43fed1d.png)\r\n\r\n\r\n\r\n\r\n**Expected behavior**\r\nTo get no errors in predicting with model\r\n\r\n**Screenshots**\r\n\r\n![to_send](https://user-images.githubusercontent.com/64765786/81883528-73a77a00-95bb-11ea-83ef-f0f05ecd63e7.png)\r\n\r\n\r\n\r\n**Environment (please complete the following information):**\r\n - OS [Linux mint 19.3]:\r\n - Version [flair current lastest branch]:\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1607", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1607/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1607/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1607/events", "html_url": "https://github.com/flairNLP/flair/issues/1607", "id": 617046919, "node_id": "MDU6SXNzdWU2MTcwNDY5MTk=", "number": 1607, "title": "NER embeddings returning zero for words with punctuation", "user": {"login": "petulla", "id": 3466817, "node_id": "MDQ6VXNlcjM0NjY4MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/3466817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petulla", "html_url": "https://github.com/petulla", "followers_url": "https://api.github.com/users/petulla/followers", "following_url": "https://api.github.com/users/petulla/following{/other_user}", "gists_url": "https://api.github.com/users/petulla/gists{/gist_id}", "starred_url": "https://api.github.com/users/petulla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petulla/subscriptions", "organizations_url": "https://api.github.com/users/petulla/orgs", "repos_url": "https://api.github.com/users/petulla/repos", "events_url": "https://api.github.com/users/petulla/events{/privacy}", "received_events_url": "https://api.github.com/users/petulla/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-13T00:16:36Z", "updated_at": "2020-05-13T14:08:12Z", "closed_at": "2020-05-13T14:08:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI'm back, struggling with a bug. When I try to gather word embeddings for the entities in a set of documents, all entities that have any punctuation associated with their recognition result in zeroed embeddings. See below for examples.\r\n\r\nI wasn't sure what a reasonable preprocessing step would be. I could strip out all punctuation, but it seems like I would need to keep the periods, which do seem to be getting pulled into the NER tokens. \r\n\r\nWhat's the best way to handle? Thanks.\r\n\r\n**Entity generation example script**\r\n\r\n```py\r\nwe= WordEmbeddings('glove.gensim').embed\r\n\r\nt = Sentence(\"This is the USA's best chance\")\r\ntagger.predict(t)\r\nwe(t)\r\n\r\n# iterate over entities\r\nent_extract = t.get_spans('ner')\r\nent_attrs = t.to_dict(tag_type='ner')['entities']\r\n\r\nfor ents in ent_extract:\r\n  print(ents.text)\r\n  for token in ents.tokens:\r\n\r\nz = [{'word': ents.text,\r\n                         'score': ent_attrs[i]['labels'][0].score,\r\n                         'entity': ent_attrs[i]['labels'][0].value,\r\n                         'embedding': torch.mean(torch.stack([token.embedding for token in ents.tokens]), dim=0).tolist()}\r\n                        for i, ents in enumerate(ent_extract)]\r\n```\r\n\r\nHere are some further examples:\r\n\r\n```js\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"Hesston, Kansas,\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"Halle, Germany,\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"Navarro's\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"Robert-Koch-Institute\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"McDonnell\u2014\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"USA's\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"\\\"Black Swan\\\"\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"Sound+Sleep\"}\r\n{\"embedding\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"word\":\"Prague.\"}\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1604", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1604/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1604/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1604/events", "html_url": "https://github.com/flairNLP/flair/issues/1604", "id": 616351455, "node_id": "MDU6SXNzdWU2MTYzNTE0NTU=", "number": 1604, "title": "is corpus object reusable across ModelTrainer instances ?", "user": {"login": "sankaran45", "id": 8388863, "node_id": "MDQ6VXNlcjgzODg4NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/8388863?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sankaran45", "html_url": "https://github.com/sankaran45", "followers_url": "https://api.github.com/users/sankaran45/followers", "following_url": "https://api.github.com/users/sankaran45/following{/other_user}", "gists_url": "https://api.github.com/users/sankaran45/gists{/gist_id}", "starred_url": "https://api.github.com/users/sankaran45/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sankaran45/subscriptions", "organizations_url": "https://api.github.com/users/sankaran45/orgs", "repos_url": "https://api.github.com/users/sankaran45/repos", "events_url": "https://api.github.com/users/sankaran45/events{/privacy}", "received_events_url": "https://api.github.com/users/sankaran45/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-12T04:35:41Z", "updated_at": "2020-06-10T07:14:08Z", "closed_at": "2020-06-10T07:14:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have three checkpoint files generated from a training run that uses PooledFlair embedding. Say chk10.pt, chk20.pt, chk30.pt.\r\n\r\nI finalize using the following code in a for loop to get the F1 predictions out:\r\n\r\ntrainer: ModelTrainer = ModelTrainer.load_checkpoint(chkfile, corpus)\r\ntrainer.train('.',  checkpoint = False, train_with_dev=True, max_epochs=epochs)\r\n\r\nI set the epochs to the value at which this checkpoint got generated. So 10, 20, 30 etc. So typically it goes straight to creating the final model and emitting the predictions.\r\n\r\nThis works perfectly fine for the first time in the loop, after which the predictions are quite wrong. Now instead of doing it in the loop, if i simply do just once by restarting the process i get the values i expect. This behavior happens only with PooledFlairEmbedding. Same program runs just fine with ElmoEmbedding, BertEmbedding.\r\n\r\nSo my question is why is this the case ? Is it because i create the corpus object outside the for loop and keep reusing it across different ModelTrainer instances ? \r\n\r\nIt happens quite regularly for me. If needed i can make a small program and share.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1603", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1603/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1603/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1603/events", "html_url": "https://github.com/flairNLP/flair/issues/1603", "id": 616056939, "node_id": "MDU6SXNzdWU2MTYwNTY5Mzk=", "number": 1603, "title": "Strange behavior with PooledFlairEmbeddings ???", "user": {"login": "sankaran45", "id": 8388863, "node_id": "MDQ6VXNlcjgzODg4NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/8388863?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sankaran45", "html_url": "https://github.com/sankaran45", "followers_url": "https://api.github.com/users/sankaran45/followers", "following_url": "https://api.github.com/users/sankaran45/following{/other_user}", "gists_url": "https://api.github.com/users/sankaran45/gists{/gist_id}", "starred_url": "https://api.github.com/users/sankaran45/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sankaran45/subscriptions", "organizations_url": "https://api.github.com/users/sankaran45/orgs", "repos_url": "https://api.github.com/users/sankaran45/repos", "events_url": "https://api.github.com/users/sankaran45/events{/privacy}", "received_events_url": "https://api.github.com/users/sankaran45/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-11T17:35:24Z", "updated_at": "2020-05-11T17:59:25Z", "closed_at": "2020-05-11T17:59:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am running a simple NER experiment with two labels with the default settings except for train_with_dev=True, max_epochs=150. I want to analyze the F1 scores obtained at 50, 100, 150 epochs respectively. So instead of stopping the training at these intervals and resuming i decided to run it for 150 epochs but just save the checkpoints at 50 and 100 by modifying trainer.py as its little easier this way in colab. Later when test is over, i finalize these checkpoint files using trainer.train which gives the predictions. \r\n\r\nThis works fine for Bert, Elmo, Flair embeddings as given below. For PooledFlair (pooling='mean' / pooling='min') the results are quite erratic although by 150 epochs it seems to catch up. Why is it the case ? Or what am i doing wrong ?\r\n\r\nIn the below, 2nd column is embedding, 3rd is epoch, and 4th is F1 score reported by trainer upon testing the best model at that time:\r\n\r\n4\tbert\t50\t86.61\r\n5\tbert\t100\t87.09666667\r\n6\tbert\t150\t87.32666667\r\n11\telmo\t50\t87.205\r\n12\telmo\t100\t87.295\r\n13\telmo\t150\t87.08\r\n14\tflair\t50\t87.58\r\n15\tflair\t100\t88.14\r\n16\tflair\t150\t87.45\r\n21\tpf\t50\t74.7\r\n22\tpf\t100\t78.94333333\r\n23\tpf\t150\t87.62333333\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1599", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1599/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1599/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1599/events", "html_url": "https://github.com/flairNLP/flair/issues/1599", "id": 615768171, "node_id": "MDU6SXNzdWU2MTU3NjgxNzE=", "number": 1599, "title": "Generate text for non-latin alphabets", "user": {"login": "Wickky", "id": 53470421, "node_id": "MDQ6VXNlcjUzNDcwNDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/53470421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wickky", "html_url": "https://github.com/Wickky", "followers_url": "https://api.github.com/users/Wickky/followers", "following_url": "https://api.github.com/users/Wickky/following{/other_user}", "gists_url": "https://api.github.com/users/Wickky/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wickky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wickky/subscriptions", "organizations_url": "https://api.github.com/users/Wickky/orgs", "repos_url": "https://api.github.com/users/Wickky/repos", "events_url": "https://api.github.com/users/Wickky/events{/privacy}", "received_events_url": "https://api.github.com/users/Wickky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-11T10:19:29Z", "updated_at": "2020-05-11T14:41:21Z", "closed_at": "2020-05-11T14:40:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to use generate_text() from language_model.py and I encountered the decoding issues.\r\nHow can I load idx2text dictionary to call generate_text() function?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1595", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1595/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1595/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1595/events", "html_url": "https://github.com/flairNLP/flair/issues/1595", "id": 615095701, "node_id": "MDU6SXNzdWU2MTUwOTU3MDE=", "number": 1595, "title": "CRF usage and hyper parameter tuning  for chunking task with only two tags", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-09T05:21:04Z", "updated_at": "2020-05-14T16:46:16Z", "closed_at": "2020-05-14T16:46:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "Currently, I am training the **chunking** (**word break**) task with only two **B and I** (beginning and intermediate) tags. The language I want to train is one of the sino-tibetan language and in that language there is **no proper word break**(only space is used for reading clearly) like Chinese and Japanese. So, begin and intermediate tags are used to predict for **each syllable** to become a word. What I would like to know is that\r\n1. Does **CRF** need for this task?? What I understood is that for sequence labeling, CRF mainly helps to prevent invalid label sequences theoretically. In my task, only one invalid label sequence is that predicting starting tag (first syllable in sentence) is intermediate tag.  When I trained the model both with and without CRF, training with CRF achieved more F1 scores(nearly 1%). In that training, the embedding I use was **stacked** embedding of **character** and **byte pair** embedding. I don't know why does training with CRF much better for this task?\r\n2. Secondly, to improve this chunking task, should I use more **layers** or more **hidden states**? Currently I used 256 hidden states and 2 RNN layers. \r\n3. Finally, for the case of sequence labeling with **syllables**, which kind of **embedding** might be better for this task?? In the future, I want to use flair embedding for this task. But currently, there is no pretrained language model for the language I want to train and currently I just start to collect the data for scratch training.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1594", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1594/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1594/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1594/events", "html_url": "https://github.com/flairNLP/flair/issues/1594", "id": 614802226, "node_id": "MDU6SXNzdWU2MTQ4MDIyMjY=", "number": 1594, "title": "Building a Spelling Corrector using a flair character level language model ", "user": {"login": "Wickky", "id": 53470421, "node_id": "MDQ6VXNlcjUzNDcwNDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/53470421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wickky", "html_url": "https://github.com/Wickky", "followers_url": "https://api.github.com/users/Wickky/followers", "following_url": "https://api.github.com/users/Wickky/following{/other_user}", "gists_url": "https://api.github.com/users/Wickky/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wickky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wickky/subscriptions", "organizations_url": "https://api.github.com/users/Wickky/orgs", "repos_url": "https://api.github.com/users/Wickky/repos", "events_url": "https://api.github.com/users/Wickky/events{/privacy}", "received_events_url": "https://api.github.com/users/Wickky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-08T15:08:33Z", "updated_at": "2020-05-11T16:54:38Z", "closed_at": "2020-05-11T16:54:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "I would like to develop a **spelling corrector** in my own language. I have tried the following methods and I am facing difficulties.\r\n1. Using **encoder decoder with attention** as in **Machine Translation Tasks**. But this produces **very bad outputs** even with the **correct** tokens and this method cannot handle **very long** sequences well although it works with **short** sequences.\r\n2. Using some **edit distance calculations** to get the candidates of the wrong tokens and using language model to find the appropriate correct token for that sequence. But this method does not cover as much as I expected.\r\nSo I would like to know if I can use a **character level language model** for the **Spelling Corrector** task. \r\nI saw a decent character level language model can be trained using Flair. \r\nIf there is **any ideas apart from this** would be appreciated. \r\nI hope someone can help me for this.\r\nThanks in advance! ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1592", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1592/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1592/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1592/events", "html_url": "https://github.com/flairNLP/flair/issues/1592", "id": 614680404, "node_id": "MDU6SXNzdWU2MTQ2ODA0MDQ=", "number": 1592, "title": "short conversation data for language model", "user": {"login": "Michael95-m", "id": 64765786, "node_id": "MDQ6VXNlcjY0NzY1Nzg2", "avatar_url": "https://avatars2.githubusercontent.com/u/64765786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Michael95-m", "html_url": "https://github.com/Michael95-m", "followers_url": "https://api.github.com/users/Michael95-m/followers", "following_url": "https://api.github.com/users/Michael95-m/following{/other_user}", "gists_url": "https://api.github.com/users/Michael95-m/gists{/gist_id}", "starred_url": "https://api.github.com/users/Michael95-m/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Michael95-m/subscriptions", "organizations_url": "https://api.github.com/users/Michael95-m/orgs", "repos_url": "https://api.github.com/users/Michael95-m/repos", "events_url": "https://api.github.com/users/Michael95-m/events{/privacy}", "received_events_url": "https://api.github.com/users/Michael95-m/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-05-08T11:20:31Z", "updated_at": "2020-05-14T16:49:08Z", "closed_at": "2020-05-14T16:49:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have got some questions for how to train flair character language model from scratch.\r\n1. I have got 10 million lines of short conversation data which are unlike formal data from wikipedia. I think many people starts training with large tons of formal data from both wikipedia and news and then continue fine tuning with informal data. My question is that is it OK to start training with conversation data which has got many spelling errors(not formal data)?? \r\n2. In my data, there are many english words like name (currently what I want to train is not english) and emoji. Should I remove these english words and emoji??\r\n3. Thirdly, Is 10 million lines of short conversation data (1.2GB) not enough for scratch training?? If not, how much data should I collect more??\r\n4. Finally, I read it that no tokenization is needed for character language model. If I don't tokenize the text data, are there any effects to the quality of the language model?? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1591", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1591/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1591/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1591/events", "html_url": "https://github.com/flairNLP/flair/issues/1591", "id": 614242355, "node_id": "MDU6SXNzdWU2MTQyNDIzNTU=", "number": 1591, "title": "Recommended recipe for entity embeddings", "user": {"login": "petulla", "id": 3466817, "node_id": "MDQ6VXNlcjM0NjY4MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/3466817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petulla", "html_url": "https://github.com/petulla", "followers_url": "https://api.github.com/users/petulla/followers", "following_url": "https://api.github.com/users/petulla/following{/other_user}", "gists_url": "https://api.github.com/users/petulla/gists{/gist_id}", "starred_url": "https://api.github.com/users/petulla/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petulla/subscriptions", "organizations_url": "https://api.github.com/users/petulla/orgs", "repos_url": "https://api.github.com/users/petulla/repos", "events_url": "https://api.github.com/users/petulla/events{/privacy}", "received_events_url": "https://api.github.com/users/petulla/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-05-07T18:01:26Z", "updated_at": "2020-05-11T17:03:26Z", "closed_at": "2020-05-11T17:03:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi\r\n\r\nI'm trying to design a recipe to get 'entity embeddings' after the NER step. I'd like to use your word embeddings API, but I'm wondering how to optimize it. At the moment, my code look a bit like this:\r\n\r\n```python\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\ntagger = SequenceTagger.load('ner')\r\nfrom flair.embeddings import PooledFlairEmbeddings\r\n# init embedding\r\npe = PooledFlairEmbeddings('news-forward', pooling='min')\r\n\r\nt = tagger.predict\r\ns = Sentence('This is a sentence about Bank of America. This is also a sentence about Apple computers. But it is also about the iPhone phone.')\r\nt(s)\r\n\r\nfor entity in s.to_dict(tag_type='ner')['entities']:\r\n  z = Sentence(entity['text'])\r\n  print(z)\r\n  pe.embed(z)\r\n  for token in z:\r\n    print(token)\r\n    print(token.embedding)\r\n  print('#~')\r\n```\r\n\r\nOutput:\r\n\r\n```python\r\nSentence: \"Bank of America.\" - 3 Tokens\r\nToken: 1 Bank\r\ntensor([-0.0098,  0.0003,  0.0088,  ..., -0.0029, -0.0990,  0.0098],\r\n       device='cuda:0')\r\nToken: 2 of\r\ntensor([ 1.1902e-03,  8.7077e-05,  2.1351e-02,  ..., -2.4563e-03,\r\n         5.8907e-03,  4.3574e-03], device='cuda:0')\r\nToken: 3 America.\r\ntensor([-0.0012,  0.0016, -0.4050,  ..., -0.0006,  0.0058,  0.0172],\r\n       device='cuda:0')\r\n#~\r\nSentence: \"Apple\" - 1 Tokens\r\nToken: 1 Apple\r\ntensor([-0.0016,  0.0008, -0.1110,  ..., -0.0004, -0.0042,  0.0097],\r\n       device='cuda:0')\r\n#~\r\nSentence: \"iPhone\" - 1 Tokens\r\nToken: 1 iPhone\r\ntensor([-2.1312e-04, -9.8402e-06,  8.7166e-02,  ..., -2.6429e-04,\r\n        -1.3806e-02,  2.8818e-02], device='cuda:0')\r\n#~\r\n```\r\n\r\nThe issue here is of course the nested loop and unpooled word embeddings. What I'd like instead is to get a single embedding back for each NER text span. I'm wondering if\r\n\r\na. You recommend an alternative to averaging/pooling this way\r\nb. There's a better pattern here. I can make a comprehension that averages the tensors of course.. but I wanted to check if there's a simpler pattern.\r\nc. Whether there's a specific way to fine-tune entity embeddings across the span, to get back a better word vector on an entity span by entity span basis rather than as word tokens.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1590", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1590/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1590/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1590/events", "html_url": "https://github.com/flairNLP/flair/issues/1590", "id": 614116709, "node_id": "MDU6SXNzdWU2MTQxMTY3MDk=", "number": 1590, "title": "Can we pass in a pre-trained word embeddings (i.e. Flair/FastText) in the TransformerDocumentEmbedding?", "user": {"login": "junhua", "id": 3516360, "node_id": "MDQ6VXNlcjM1MTYzNjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/3516360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junhua", "html_url": "https://github.com/junhua", "followers_url": "https://api.github.com/users/junhua/followers", "following_url": "https://api.github.com/users/junhua/following{/other_user}", "gists_url": "https://api.github.com/users/junhua/gists{/gist_id}", "starred_url": "https://api.github.com/users/junhua/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junhua/subscriptions", "organizations_url": "https://api.github.com/users/junhua/orgs", "repos_url": "https://api.github.com/users/junhua/repos", "events_url": "https://api.github.com/users/junhua/events{/privacy}", "received_events_url": "https://api.github.com/users/junhua/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-05-07T14:47:36Z", "updated_at": "2020-05-11T16:19:08Z", "closed_at": "2020-05-11T16:19:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "RT. Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1587", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1587/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1587/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1587/events", "html_url": "https://github.com/flairNLP/flair/issues/1587", "id": 613668938, "node_id": "MDU6SXNzdWU2MTM2Njg5Mzg=", "number": 1587, "title": "Refactor embeddings into submodules", "user": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-06T23:14:15Z", "updated_at": "2020-05-07T12:12:35Z", "closed_at": "2020-05-07T12:12:35Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "Currently, we have one gigantic file `embeddings.py` that keeps all of our embeddings classes. Since so many new embeddings have been added over time this file is grown way too large. \r\n\r\nThis issue tracks a refactoring of the `embeddings.py` file into a folder with submodules, each for one modality that can be embedded (words, documents, images, etc). We did a similar thing for datasets in #1510", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1582", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1582/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1582/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1582/events", "html_url": "https://github.com/flairNLP/flair/issues/1582", "id": 613052366, "node_id": "MDU6SXNzdWU2MTMwNTIzNjY=", "number": 1582, "title": "How can I generate next character given a few characters before using LM?", "user": {"login": "Wickky", "id": 53470421, "node_id": "MDQ6VXNlcjUzNDcwNDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/53470421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wickky", "html_url": "https://github.com/Wickky", "followers_url": "https://api.github.com/users/Wickky/followers", "following_url": "https://api.github.com/users/Wickky/following{/other_user}", "gists_url": "https://api.github.com/users/Wickky/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wickky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wickky/subscriptions", "organizations_url": "https://api.github.com/users/Wickky/orgs", "repos_url": "https://api.github.com/users/Wickky/repos", "events_url": "https://api.github.com/users/Wickky/events{/privacy}", "received_events_url": "https://api.github.com/users/Wickky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-05-06T05:04:08Z", "updated_at": "2020-05-11T15:20:56Z", "closed_at": "2020-05-11T15:20:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I saw to generate text using a character range to produce. But how to generate next prediction given a few characters.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1580", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1580/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1580/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1580/events", "html_url": "https://github.com/flairNLP/flair/issues/1580", "id": 612790505, "node_id": "MDU6SXNzdWU2MTI3OTA1MDU=", "number": 1580, "title": "Extend ColumnCorpus with option to define \"space after\"", "user": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-05T18:01:11Z", "updated_at": "2020-05-06T10:41:42Z", "closed_at": "2020-05-06T10:41:42Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The Universal Dependencies column format has a way of marking whether or not a whitespace follows a token (our `UniversalDependenciesCorpus` is able to recognize this). However, a similar mechanism does not currently exist for other column-formatted datasets which we read with the `ColumnCorpus` object. So far this was not necessary as all datasets we've encountered, such as CoNLL-03, do not include this information. \r\n\r\nIn the context of #1513, we've now come across a use case in which we need to know whether a whitespace follows a token in column format. This issue tracks the extension of `ColumnCorpus` to allow us to include this information.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1578", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1578/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1578/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1578/events", "html_url": "https://github.com/flairNLP/flair/issues/1578", "id": 612506631, "node_id": "MDU6SXNzdWU2MTI1MDY2MzE=", "number": 1578, "title": "Is the creation of embeddings not determinisctic?", "user": {"login": "Moiddes", "id": 1322289, "node_id": "MDQ6VXNlcjEzMjIyODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1322289?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Moiddes", "html_url": "https://github.com/Moiddes", "followers_url": "https://api.github.com/users/Moiddes/followers", "following_url": "https://api.github.com/users/Moiddes/following{/other_user}", "gists_url": "https://api.github.com/users/Moiddes/gists{/gist_id}", "starred_url": "https://api.github.com/users/Moiddes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Moiddes/subscriptions", "organizations_url": "https://api.github.com/users/Moiddes/orgs", "repos_url": "https://api.github.com/users/Moiddes/repos", "events_url": "https://api.github.com/users/Moiddes/events{/privacy}", "received_events_url": "https://api.github.com/users/Moiddes/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-05T10:44:20Z", "updated_at": "2020-05-05T12:34:30Z", "closed_at": "2020-05-05T12:34:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm working on a project about document/paragraph similarity and I wanted to incorporate flair embeddings in my research, however results have been worse than expected, so I started bug searching in my code. I'm using \r\n```\r\nembedding = BertEmbeddings(layers='-1')\r\ndocument_embeddings = DocumentPoolEmbeddings([embedding], fine_tune_mode='nonlinear')\r\n```\r\nas my model for creating document embeddings using flair right now. (Whether these are good or bad representations of documents for assessing similarity is another question, hence the research)\r\n\r\nI've written a generator function for generating embeddings from strings:\r\n\r\n```\r\ndef generate_embeddings(docs, batch_size, model=document_embeddings, offset=0):\r\n    rest = len(docs) % batch_size\r\n    for i in range(0, len(docs) - rest, batch_size):\r\n        sentences = [Sentence(sentence)for sentence in docs[i:i + batch_size]]\r\n        try:\r\n            model.embed(sentences)\r\n            print(f'successfully embedded sentences {offset + i} to {offset + i + batch_size-1}')\r\n            yield 1, [sentence.get_embedding().detach().cpu().numpy() for sentence in sentences]\r\n        except RuntimeError:\r\n            print(f'could not embed sentences with index {offset + i} '\r\n                  f'to {offset + i + batch_size-1}\\nstoring in failed index list')\r\n            yield 0, (offset + i, offset + i + batch_size-1)\r\n    if rest:\r\n        sentences = [Sentence(sentence)for sentence in docs[-rest:]]\r\n        try:\r\n            model.embed(sentences)\r\n            print(f'successfully embedded sentences from {len(docs) + offset - rest} to the end')\r\n            yield 2, [sentence.get_embedding().detach().cpu().numpy() for sentence in sentences]\r\n        except RuntimeError:\r\n            yield 0, (len(docs) - rest, 0)\r\n\r\n```\r\n\r\nNow, when I create embeddings of the same inputs (which are sentences, not long documents) more than once, I get different representations of my inputs. \r\n\r\nso:\r\n**Is the creation of embeddings not determinisctic?**\r\n\r\nHere are two pictures which show the first elements of vectors, which are different, but were generated with the same text as input:\r\n![Array Vectors from 0 to 3 before normalization](https://user-images.githubusercontent.com/1322289/81058057-f0589b00-8ecd-11ea-9ad1-f1f49b5ac0f1.png)\r\n![new Array Vectors (0 to 3) are different?](https://user-images.githubusercontent.com/1322289/81058061-f3ec2200-8ecd-11ea-8b00-a91a8e3c1d6e.png)\r\n\r\n\r\nI expected : same input -> same embeddings. However, that seems to not be the case. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1576", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1576/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1576/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1576/events", "html_url": "https://github.com/flairNLP/flair/issues/1576", "id": 612480325, "node_id": "MDU6SXNzdWU2MTI0ODAzMjU=", "number": 1576, "title": "Language Model producing <unk> tags", "user": {"login": "Wickky", "id": 53470421, "node_id": "MDQ6VXNlcjUzNDcwNDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/53470421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wickky", "html_url": "https://github.com/Wickky", "followers_url": "https://api.github.com/users/Wickky/followers", "following_url": "https://api.github.com/users/Wickky/following{/other_user}", "gists_url": "https://api.github.com/users/Wickky/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wickky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wickky/subscriptions", "organizations_url": "https://api.github.com/users/Wickky/orgs", "repos_url": "https://api.github.com/users/Wickky/repos", "events_url": "https://api.github.com/users/Wickky/events{/privacy}", "received_events_url": "https://api.github.com/users/Wickky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-05-05T09:56:40Z", "updated_at": "2020-05-08T15:34:49Z", "closed_at": "2020-05-05T17:07:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have trained a char-level language model and it produces \"unk\" while doing prediction.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1574", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1574/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1574/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1574/events", "html_url": "https://github.com/flairNLP/flair/issues/1574", "id": 611768548, "node_id": "MDU6SXNzdWU2MTE3Njg1NDg=", "number": 1574, "title": "Extreme memory usage on GPU server when using FLAIR NER model", "user": {"login": "igormis", "id": 6599037, "node_id": "MDQ6VXNlcjY1OTkwMzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/6599037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/igormis", "html_url": "https://github.com/igormis", "followers_url": "https://api.github.com/users/igormis/followers", "following_url": "https://api.github.com/users/igormis/following{/other_user}", "gists_url": "https://api.github.com/users/igormis/gists{/gist_id}", "starred_url": "https://api.github.com/users/igormis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/igormis/subscriptions", "organizations_url": "https://api.github.com/users/igormis/orgs", "repos_url": "https://api.github.com/users/igormis/repos", "events_url": "https://api.github.com/users/igormis/events{/privacy}", "received_events_url": "https://api.github.com/users/igormis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-05-04T10:31:21Z", "updated_at": "2020-05-05T13:15:59Z", "closed_at": "2020-05-05T13:15:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have trained a NER model with FLAIR, which occupies around 2GB of memory, when loaded. However, when I infer the RAM usage on the GPU server with CUDA 10.2 constantly grows and it reached 20GB after 18 hours of responses. This is strange, as it should clean the RAM memory after the response? Is there any command to do that?\r\nI am just loading the model using\r\n`model = SequenceTagger.load('./nlu_en/final-model.pt')` \r\nand predict using:\r\n```\r\nsentence = Sentence(text)\r\nmodel.predict(sentence)\r\npredict_dict=sentence.to_dict(tag_type='ner')\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1567", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1567/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1567/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1567/events", "html_url": "https://github.com/flairNLP/flair/issues/1567", "id": 611143475, "node_id": "MDU6SXNzdWU2MTExNDM0NzU=", "number": 1567, "title": "TransformerDocuementEmbedding does not take flair.device", "user": {"login": "junhua", "id": 3516360, "node_id": "MDQ6VXNlcjM1MTYzNjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/3516360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junhua", "html_url": "https://github.com/junhua", "followers_url": "https://api.github.com/users/junhua/followers", "following_url": "https://api.github.com/users/junhua/following{/other_user}", "gists_url": "https://api.github.com/users/junhua/gists{/gist_id}", "starred_url": "https://api.github.com/users/junhua/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junhua/subscriptions", "organizations_url": "https://api.github.com/users/junhua/orgs", "repos_url": "https://api.github.com/users/junhua/repos", "events_url": "https://api.github.com/users/junhua/events{/privacy}", "received_events_url": "https://api.github.com/users/junhua/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-05-02T09:12:30Z", "updated_at": "2020-05-06T22:22:46Z", "closed_at": "2020-05-06T22:22:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "By setting `flair.device = torch.device('cuda:0;)`, it throws an error saying `expected device cpu  but got device cuda:0`. I have to explicitly set `TextClassifier(...).to(torch.device('cuda:0')` to make it happen", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1564", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1564/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1564/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1564/events", "html_url": "https://github.com/flairNLP/flair/issues/1564", "id": 610684673, "node_id": "MDU6SXNzdWU2MTA2ODQ2NzM=", "number": 1564, "title": "Error using flair embeddings: ValueError: invalid literal for int() with base 10: '0+cu101'", "user": {"login": "ChessMateK", "id": 48825535, "node_id": "MDQ6VXNlcjQ4ODI1NTM1", "avatar_url": "https://avatars1.githubusercontent.com/u/48825535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChessMateK", "html_url": "https://github.com/ChessMateK", "followers_url": "https://api.github.com/users/ChessMateK/followers", "following_url": "https://api.github.com/users/ChessMateK/following{/other_user}", "gists_url": "https://api.github.com/users/ChessMateK/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChessMateK/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChessMateK/subscriptions", "organizations_url": "https://api.github.com/users/ChessMateK/orgs", "repos_url": "https://api.github.com/users/ChessMateK/repos", "events_url": "https://api.github.com/users/ChessMateK/events{/privacy}", "received_events_url": "https://api.github.com/users/ChessMateK/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-05-01T10:31:24Z", "updated_at": "2020-05-06T20:17:23Z", "closed_at": "2020-05-05T13:17:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nWhen I run this:\r\n\r\n```\r\nembedding_types: List[TokenEmbeddings] = [               \r\n    FlairEmbeddings('news-forward'),\r\n    FlairEmbeddings('news-backward')\r\n]\r\n```\r\n\r\nI obtain the following errors:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-177704bfc5f6> in <module>()\r\n---> 51     FlairEmbeddings('news-forward'),\r\n     52     FlairEmbeddings('news-backward')\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/flair/embeddings.py in __init__(self, model, fine_tune, chars_per_chunk)\r\n   1986             self.name = f\"Task-LSTM-{self.lm.hidden_size}-{self.lm.nlayers}-{self.lm.is_forward_lm}\"\r\n   1987         else:\r\n-> 1988             self.lm: LanguageModel = LanguageModel.load_language_model(model)\r\n   1989             self.name = str(model)\r\n   1990 \r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/models/language_model.py in load_language_model(cls, model_file)\r\n    195             state[\"embedding_size\"],\r\n    196             state[\"nout\"],\r\n--> 197             state[\"dropout\"],\r\n    198         )\r\n    199         model.load_state_dict(state[\"state_dict\"])\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/models/language_model.py in __init__(self, dictionary, is_forward_lm, hidden_size, nlayers, embedding_size, nout, dropout)\r\n     59 \r\n     60         # auto-spawn on GPU if available\r\n---> 61         self.to(flair.device)\r\n     62 \r\n     63     def init_weights(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\r\n    441             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\r\n    442 \r\n--> 443         return self._apply(convert)\r\n    444 \r\n    445     def register_backward_hook(self, hook):\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/models/language_model.py in _apply(self, fn)\r\n    392     def _apply(self, fn):\r\n    393         major, minor, build, *_ = (int(info)\r\n--> 394                                 for info in torch.__version__.split('.'))\r\n    395 \r\n    396         # fixed RNN change format for torch 1.4.0\r\n\r\n/usr/local/lib/python3.6/dist-packages/flair/models/language_model.py in <genexpr>(.0)\r\n    392     def _apply(self, fn):\r\n    393         major, minor, build, *_ = (int(info)\r\n--> 394                                 for info in torch.__version__.split('.'))\r\n    395 \r\n    396         # fixed RNN change format for torch 1.4.0\r\n\r\nValueError: invalid literal for int() with base 10: '0+cu101'\r\n```\r\n\r\n**To Reproduce**\r\nFollow [this flair guide](https://github.com/flairNLP/flair/blob/cd7f04352229284d2802cf7b5c701dca3ee28d6c/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md) for NER and use flair embeddings\r\n\r\n**Expected behavior**\r\nEmbeddings are stacked without errors.\r\n\r\n**Screenshots**\r\nNone\r\n\r\n**Environment (please complete the following information):**\r\n - Google Colab\r\n\r\n**Additional context**\r\nNone\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1563", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1563/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1563/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1563/events", "html_url": "https://github.com/flairNLP/flair/issues/1563", "id": 610564825, "node_id": "MDU6SXNzdWU2MTA1NjQ4MjU=", "number": 1563, "title": "Unable to load trained NER model using bytepair embedding", "user": {"login": "vinayakkailas", "id": 22210445, "node_id": "MDQ6VXNlcjIyMjEwNDQ1", "avatar_url": "https://avatars1.githubusercontent.com/u/22210445?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vinayakkailas", "html_url": "https://github.com/vinayakkailas", "followers_url": "https://api.github.com/users/vinayakkailas/followers", "following_url": "https://api.github.com/users/vinayakkailas/following{/other_user}", "gists_url": "https://api.github.com/users/vinayakkailas/gists{/gist_id}", "starred_url": "https://api.github.com/users/vinayakkailas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vinayakkailas/subscriptions", "organizations_url": "https://api.github.com/users/vinayakkailas/orgs", "repos_url": "https://api.github.com/users/vinayakkailas/repos", "events_url": "https://api.github.com/users/vinayakkailas/events{/privacy}", "received_events_url": "https://api.github.com/users/vinayakkailas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2020-05-01T05:03:09Z", "updated_at": "2020-08-12T14:02:24Z", "closed_at": "2020-08-12T14:02:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "i tried to load the best trained model outside of my training environment to do the prediction. But i am getting path error. It seems like the trained model also freeze the local path where embeddings are stored. The path showing in the error is the local path of training instance.\r\n\r\n<img width=\"1177\" alt=\"Screenshot 2020-05-01 at 10 29 16 AM\" src=\"https://user-images.githubusercontent.com/22210445/80783356-1c56e200-8b97-11ea-91b1-cf7d014eb426.png\">\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1562", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1562/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1562/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1562/events", "html_url": "https://github.com/flairNLP/flair/issues/1562", "id": 610310610, "node_id": "MDU6SXNzdWU2MTAzMTA2MTA=", "number": 1562, "title": "ModelTrainer's training on SequenceTagger raises an `AssertionError: `", "user": {"login": "iam-kevin", "id": 54111777, "node_id": "MDQ6VXNlcjU0MTExNzc3", "avatar_url": "https://avatars3.githubusercontent.com/u/54111777?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iam-kevin", "html_url": "https://github.com/iam-kevin", "followers_url": "https://api.github.com/users/iam-kevin/followers", "following_url": "https://api.github.com/users/iam-kevin/following{/other_user}", "gists_url": "https://api.github.com/users/iam-kevin/gists{/gist_id}", "starred_url": "https://api.github.com/users/iam-kevin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iam-kevin/subscriptions", "organizations_url": "https://api.github.com/users/iam-kevin/orgs", "repos_url": "https://api.github.com/users/iam-kevin/repos", "events_url": "https://api.github.com/users/iam-kevin/events{/privacy}", "received_events_url": "https://api.github.com/users/iam-kevin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-30T18:56:20Z", "updated_at": "2020-05-01T23:42:30Z", "closed_at": "2020-05-01T23:42:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Describe the bug**\r\nI am unable to Train a `SequenceTagger` model using the `ModelTrainer` API. This was upon my attempt to train a NER Tagger. The `AssertionError` is raised after the completion of the first epoch.\r\n\r\n**To Reproduce**\r\nThe format of my NER data is:\r\n```\r\njina O\r\nlangu O\r\nni O\r\nKevin B-PERSON\r\n...\r\n```\r\n\r\nSo I set up the Training sequence this way\r\n```\r\nfrom flair.data import Corpus\r\nfrom flair.datasets import ColumnCorpus\r\nfrom flair.embeddings import StackedEmbeddings, CharacterEmbeddings, FlairEmbeddings\r\n\r\nTAG_TYPE = 'ner'\r\ncorpus: Corpus = ColumnCorpus('./ner_data', {0: 'text', 1: 'ner'})\r\n\r\n# Setup the sequence tagger\r\nfrom flair.models import SequenceTagger\r\n\r\ntagger: SequenceTagger = SequenceTagger(hidden_size=256,\r\n                                        embeddings=embeddings,\r\n                                        tag_dictionary=corpus.make_label_dictionary(label_type=TAG_TYPE),# ... from corpus\r\n                                        tag_type=TAG_TYPE,\r\n                                        use_crf=True)\r\n\r\n\r\n# Setup the training process\r\nfrom flair.trainers import ModelTrainer\r\n\r\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\r\n\r\ntrainer.train('./resource/taggers/swahili-ner',\r\n              learning_rate=0.1,\r\n              mini_batch_size=32)\r\n\r\n# intializing the embeddings\r\nembeddings: StackedEmbeddings = StackedEmbeddings([CharacterEmbeddings(),\r\n                                                   FlairEmbeddings('./best-lm.pt')])\r\n```\r\n\r\nThis was the output from my training:\r\n```\r\n...\r\n2020-04-30 14:36:40,370 epoch 1 - iter 96/120 - loss 209973.64290365 - samples/sec: 33.04\r\n2020-04-30 14:36:52,323 epoch 1 - iter 108/120 - loss 208670.76244213 - samples/sec: 32.17\r\n2020-04-30 14:37:03,579 epoch 1 - iter 120/120 - loss 206854.72747396 - samples/sec: 34.17\r\n2020-04-30 14:37:03,594 ----------------------------------------------------------------------------------------------------\r\n2020-04-30 14:37:03,594 EPOCH 1 done: loss 206854.7275 - lr 0.1000000\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-18-67e2f7ae9c06> in <module>()\r\n     15 trainer.train('./resource/taggers/swahili-dumb-ner',\r\n     16               learning_rate=0.1,\r\n---> 17               mini_batch_size=32)\r\n     18 \r\n     19 # Plotting the performance graphs\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/flair/models/sequence_tagger_model.py in _viterbi_decode(self, feats, transitions, all_scores)\r\n    754 \r\n    755         start = best_path.pop()\r\n--> 756         assert start == id_start\r\n    757         best_path.reverse()\r\n    758 \r\n\r\nAssertionError: \r\n```\r\n\r\n**Expected behavior**\r\nTraining of the model to the default `100` epochs\r\n\r\n**Environment (please complete the following information):**\r\nI used this in colab for this\r\n\r\nHow i installed the packages:\r\n```\r\n!pip install torch==1.4.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\r\n!pip install --upgrade git+https://github.com/flairNLP/flair.git\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1553", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1553/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1553/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1553/events", "html_url": "https://github.com/flairNLP/flair/issues/1553", "id": 608241787, "node_id": "MDU6SXNzdWU2MDgyNDE3ODc=", "number": 1553, "title": "Got Error When Inference NER model", "user": {"login": "vmoudyp", "id": 9035591, "node_id": "MDQ6VXNlcjkwMzU1OTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/9035591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vmoudyp", "html_url": "https://github.com/vmoudyp", "followers_url": "https://api.github.com/users/vmoudyp/followers", "following_url": "https://api.github.com/users/vmoudyp/following{/other_user}", "gists_url": "https://api.github.com/users/vmoudyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/vmoudyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vmoudyp/subscriptions", "organizations_url": "https://api.github.com/users/vmoudyp/orgs", "repos_url": "https://api.github.com/users/vmoudyp/repos", "events_url": "https://api.github.com/users/vmoudyp/events{/privacy}", "received_events_url": "https://api.github.com/users/vmoudyp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2020-04-28T11:26:26Z", "updated_at": "2020-04-29T00:07:13Z", "closed_at": "2020-04-29T00:07:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "I already have a model, but when i try to do inference, i got these error :\r\n```\r\n\r\n2020-04-28 18:19:06,716 loading file D:/code/python/spasi/ner/modelling/final-model.pt\r\nTraceback (most recent call last):\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\flair\\nn.py\", line 86, in load\r\n    state = torch.load(f, map_location=flair.device)\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\torch\\serialization.py\", line 529, in load\r\n    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\torch\\serialization.py\", line 702, in _legacy_load\r\n    result = unpickler.load()\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\torch\\serialization.py\", line 665, in persistent_load\r\n    deserialized_objects[root_key] = restore_location(obj, location)\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\torch\\serialization.py\", line 740, in restore_location\r\n    return default_restore_location(storage, str(map_location))\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\torch\\serialization.py\", line 156, in default_restore_location\r\n    result = fn(storage, location)\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\torch\\serialization.py\", line 136, in _cuda_deserialize\r\n    return storage_type(obj.size())\r\n  File \"D:\\code\\python\\spasi\\envs\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 480, in _lazy_new\r\n    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\nRuntimeError: CUDA out of memory. Tried to allocate 94.00 MiB (GPU 0; 2.00 GiB total capacity; 1.44 GiB already allocated; 76.34 MiB free; 1.49 GiB reserved in total by PyTorch)\r\n```\r\n\r\n\r\nand this is the script i used :\r\n\r\n```\r\nfrom flair.data import Sentence\r\nfrom flair.models import SequenceTagger\r\n\r\nmodel = SequenceTagger.load('D:/code/python/spasi/ner/modelling/final-model.pt')\r\n\r\nsentence = Sentence('Saya tinggal di Jakarta')\r\n\r\n# predict tags and print\r\nmodel.predict(sentence)\r\n\r\nprint(sentence.to_tagged_string())\r\n\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1551", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1551/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1551/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1551/events", "html_url": "https://github.com/flairNLP/flair/issues/1551", "id": 607580891, "node_id": "MDU6SXNzdWU2MDc1ODA4OTE=", "number": 1551, "title": "Save checkpoint every N epochs", "user": {"login": "WiraDKP", "id": 33952122, "node_id": "MDQ6VXNlcjMzOTUyMTIy", "avatar_url": "https://avatars2.githubusercontent.com/u/33952122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WiraDKP", "html_url": "https://github.com/WiraDKP", "followers_url": "https://api.github.com/users/WiraDKP/followers", "following_url": "https://api.github.com/users/WiraDKP/following{/other_user}", "gists_url": "https://api.github.com/users/WiraDKP/gists{/gist_id}", "starred_url": "https://api.github.com/users/WiraDKP/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WiraDKP/subscriptions", "organizations_url": "https://api.github.com/users/WiraDKP/orgs", "repos_url": "https://api.github.com/users/WiraDKP/repos", "events_url": "https://api.github.com/users/WiraDKP/events{/privacy}", "received_events_url": "https://api.github.com/users/WiraDKP/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-27T14:08:19Z", "updated_at": "2020-05-11T09:42:47Z", "closed_at": "2020-05-11T09:42:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am fine-tuning a flair embedding. It is quite resourceful to save a model for each epoch (`epoch_1.pt, epoch_2.pt, ...`). Is there a way to turn this off? or maybe to save a model every N epochs?\r\n\r\nI have tried `checkpoint = False`, but it still saves a model for each epoch.\r\n\r\nHere is the sample code I am using\r\n```\r\nlm = FlairEmbeddings('xx-forward').lm\r\ncorpus = TextCorpus(\"corpus/\", lm.dictionary, lm.is_forward_lm, character_level=True)\r\ntrainer = LanguageModelTrainer(lm, corpus)\r\ntrainer.train(\"output\", 10, checkpoint=False)\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1550", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1550/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1550/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1550/events", "html_url": "https://github.com/flairNLP/flair/issues/1550", "id": 607456640, "node_id": "MDU6SXNzdWU2MDc0NTY2NDA=", "number": 1550, "title": "What averaging scheme does Flair use for final model?", "user": {"login": "junhua", "id": 3516360, "node_id": "MDQ6VXNlcjM1MTYzNjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/3516360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/junhua", "html_url": "https://github.com/junhua", "followers_url": "https://api.github.com/users/junhua/followers", "following_url": "https://api.github.com/users/junhua/following{/other_user}", "gists_url": "https://api.github.com/users/junhua/gists{/gist_id}", "starred_url": "https://api.github.com/users/junhua/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/junhua/subscriptions", "organizations_url": "https://api.github.com/users/junhua/orgs", "repos_url": "https://api.github.com/users/junhua/repos", "events_url": "https://api.github.com/users/junhua/events{/privacy}", "received_events_url": "https://api.github.com/users/junhua/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-04-27T11:08:42Z", "updated_at": "2020-04-27T11:21:59Z", "closed_at": "2020-04-27T11:21:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "What averaging scheme (micro / macro / weighted) does flair use for calculating average precision, recall and f1?\r\n\r\nAnother question: I trained several models and the resulting average precision/recall/f1 are the same. Does it make sense?\r\n\r\n```\r\n2020-04-27 17:26:49,708  **0.832\t0.832\t0.832**\r\n2020-04-27 17:26:49,708 \r\nMICRO_AVG: acc 0.7123 - f1-score 0.832\r\nMACRO_AVG: acc 0.6104 - f1-score 0.7195277777777779\r\n1          tp: 285 - fp: 52 - fn: 36 - tn: 2662 - precision: 0.8457 - recall: 0.8879 - accuracy: 0.7641 - f1-score: 0.8663\r\n10         tp: 8 - fp: 1 - fn: 7 - tn: 3019 - precision: 0.8889 - recall: 0.5333 - accuracy: 0.5000 - f1-score: 0.6666\r\n11         tp: 0 - fp: 1 - fn: 1 - tn: 3033 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\r\n...\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1548", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1548/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1548/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1548/events", "html_url": "https://github.com/flairNLP/flair/issues/1548", "id": 607144456, "node_id": "MDU6SXNzdWU2MDcxNDQ0NTY=", "number": 1548, "title": "Processing multiple sentences", "user": {"login": "rodriguesfas", "id": 5026592, "node_id": "MDQ6VXNlcjUwMjY1OTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/5026592?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rodriguesfas", "html_url": "https://github.com/rodriguesfas", "followers_url": "https://api.github.com/users/rodriguesfas/followers", "following_url": "https://api.github.com/users/rodriguesfas/following{/other_user}", "gists_url": "https://api.github.com/users/rodriguesfas/gists{/gist_id}", "starred_url": "https://api.github.com/users/rodriguesfas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rodriguesfas/subscriptions", "organizations_url": "https://api.github.com/users/rodriguesfas/orgs", "repos_url": "https://api.github.com/users/rodriguesfas/repos", "events_url": "https://api.github.com/users/rodriguesfas/events{/privacy}", "received_events_url": "https://api.github.com/users/rodriguesfas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-04-26T23:43:35Z", "updated_at": "2020-04-28T14:25:05Z", "closed_at": "2020-04-28T14:25:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello! I would like help on how to use multiple sentences.\r\nIn your example at: [tagging-a-list-of-sentences](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md#tagging-a-list-of-sentences)\r\nIt does not tell how to print the processed results.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1546", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1546/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1546/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1546/events", "html_url": "https://github.com/flairNLP/flair/issues/1546", "id": 607082700, "node_id": "MDU6SXNzdWU2MDcwODI3MDA=", "number": 1546, "title": "Pretrained POS tagger on PTB", "user": {"login": "ZhaofengWu", "id": 11954789, "node_id": "MDQ6VXNlcjExOTU0Nzg5", "avatar_url": "https://avatars2.githubusercontent.com/u/11954789?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhaofengWu", "html_url": "https://github.com/ZhaofengWu", "followers_url": "https://api.github.com/users/ZhaofengWu/followers", "following_url": "https://api.github.com/users/ZhaofengWu/following{/other_user}", "gists_url": "https://api.github.com/users/ZhaofengWu/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhaofengWu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhaofengWu/subscriptions", "organizations_url": "https://api.github.com/users/ZhaofengWu/orgs", "repos_url": "https://api.github.com/users/ZhaofengWu/repos", "events_url": "https://api.github.com/users/ZhaofengWu/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhaofengWu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-26T18:38:59Z", "updated_at": "2020-04-28T18:14:47Z", "closed_at": "2020-04-28T18:14:47Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, your README refers to a PTB POS model that gets 97.85 accuracy, but from [the tutorial page](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md) it seems that the only English model released is trained on Ontonotes. I wonder if the PTB model is released? Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1538", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1538/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1538/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1538/events", "html_url": "https://github.com/flairNLP/flair/issues/1538", "id": 604855445, "node_id": "MDU6SXNzdWU2MDQ4NTU0NDU=", "number": 1538, "title": "Train FlairEmbedding with document boundaries separator token", "user": {"login": "otahmasebi", "id": 13538735, "node_id": "MDQ6VXNlcjEzNTM4NzM1", "avatar_url": "https://avatars2.githubusercontent.com/u/13538735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/otahmasebi", "html_url": "https://github.com/otahmasebi", "followers_url": "https://api.github.com/users/otahmasebi/followers", "following_url": "https://api.github.com/users/otahmasebi/following{/other_user}", "gists_url": "https://api.github.com/users/otahmasebi/gists{/gist_id}", "starred_url": "https://api.github.com/users/otahmasebi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/otahmasebi/subscriptions", "organizations_url": "https://api.github.com/users/otahmasebi/orgs", "repos_url": "https://api.github.com/users/otahmasebi/repos", "events_url": "https://api.github.com/users/otahmasebi/events{/privacy}", "received_events_url": "https://api.github.com/users/otahmasebi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-22T15:36:36Z", "updated_at": "2020-08-12T14:29:17Z", "closed_at": "2020-04-26T12:18:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello\r\nI have tried to train new Flair Embeddings and as mentioned In https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md , it is possible to use a document separator token like [SEP]. I use this token in my corpus to separate document boundaries, how should we introduce this token to LanguageModelTrainer ?!\r\nIs there any docs to share more details to how sentences  feed to model in this way when shuffle_lines variabble is true?!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1537", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1537/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1537/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1537/events", "html_url": "https://github.com/flairNLP/flair/issues/1537", "id": 604622114, "node_id": "MDU6SXNzdWU2MDQ2MjIxMTQ=", "number": 1537, "title": "best-lm.pt is not updated", "user": {"login": "seyyaw", "id": 1056051, "node_id": "MDQ6VXNlcjEwNTYwNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/1056051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seyyaw", "html_url": "https://github.com/seyyaw", "followers_url": "https://api.github.com/users/seyyaw/followers", "following_url": "https://api.github.com/users/seyyaw/following{/other_user}", "gists_url": "https://api.github.com/users/seyyaw/gists{/gist_id}", "starred_url": "https://api.github.com/users/seyyaw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seyyaw/subscriptions", "organizations_url": "https://api.github.com/users/seyyaw/orgs", "repos_url": "https://api.github.com/users/seyyaw/repos", "events_url": "https://api.github.com/users/seyyaw/events{/privacy}", "received_events_url": "https://api.github.com/users/seyyaw/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2020-04-22T09:56:54Z", "updated_at": "2020-05-21T12:05:42Z", "closed_at": "2020-05-21T12:05:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "I have trained a new flair language model  for Amharic language. The `best-lm.pt` is created after the first epoch(`epoch_1.pt`) however it is not bieng updated afterwards. I run it for 10 epochs and now I have checkpoints from 1 to 9 (epoch_1.pt ....epoch_9.pt), but checkpoint for epoch 10 also is missed. Which one shall I use, `best-lm.pt` or any of the other?\r\n\r\n```\r\n2020-04-22 00:48:21,888 -----------------------------------------------------------------------------------------\r\n2020-04-22 00:48:21,888 | end of split  10 / 10 | epoch  10 | time: 5488.02s | valid loss  1.69 | valid ppl     5.42 | learning rate 0.0003\r\n2020-04-22 00:48:21,889 -----------------------------------------------------------------------------------------\r\n2020-04-22 00:48:22,850 Epoch time: 66796.78\r\n2020-04-22 00:48:28,027 TEST: valid loss  1.43 | valid ppl     4.16\r\n2020-04-22 00:48:28,027 -----------------------------------------------------------------------------------------\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1534", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1534/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1534/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1534/events", "html_url": "https://github.com/flairNLP/flair/issues/1534", "id": 603827651, "node_id": "MDU6SXNzdWU2MDM4Mjc2NTE=", "number": 1534, "title": "Sentiment Analysis very slow in mac", "user": {"login": "janeshdev", "id": 3325086, "node_id": "MDQ6VXNlcjMzMjUwODY=", "avatar_url": "https://avatars2.githubusercontent.com/u/3325086?v=4", "gravatar_id": "", "url": "https://api.github.com/users/janeshdev", "html_url": "https://github.com/janeshdev", "followers_url": "https://api.github.com/users/janeshdev/followers", "following_url": "https://api.github.com/users/janeshdev/following{/other_user}", "gists_url": "https://api.github.com/users/janeshdev/gists{/gist_id}", "starred_url": "https://api.github.com/users/janeshdev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/janeshdev/subscriptions", "organizations_url": "https://api.github.com/users/janeshdev/orgs", "repos_url": "https://api.github.com/users/janeshdev/repos", "events_url": "https://api.github.com/users/janeshdev/events{/privacy}", "received_events_url": "https://api.github.com/users/janeshdev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2020-04-21T08:54:42Z", "updated_at": "2020-05-22T19:49:53Z", "closed_at": "2020-05-22T19:49:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am using flair for sentiment analysis using `en-sentiment` text classifier and I am running for 2000 sentences and it's very slow. The code is running for more than 3 hours now and still not finished. The code I am using is as follows: \r\n\r\n```\r\nfrom flair.models import TextClassifier\r\nfrom flair.data import Sentence\r\nclassifier = TextClassifier.load('en-sentiment')\r\n\r\n# Create a function \r\ndef sentiment_analysis(text):\r\n    tt = Sentence(text)\r\n    classifier.predict(tt)\r\n    if tt.labels[0].value == 'POSITIVE':\r\n        multiplier = 1\r\n    else: \r\n        multiplier = -1  \r\n    sentiment = multiplier * tt.labels[0].score \r\n    return(';;'.join([tt.labels[0].value, repr(sentiment)]))\r\n\r\ndf['sentiment'] = df.customer_comments.apply(sentiment_analysis).str.split(\";;\")\r\n```\r\nAny tips or ideas to speed up this implementation? \r\n\r\nAlso can you please let me know how is the sentiment calculated in `en-sentiment` ? I am using this to find sentiment for customer reviews. Would it make sense to use on customer reviews? \r\n\r\nThanks for the wonderful package. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1531", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1531/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1531/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1531/events", "html_url": "https://github.com/flairNLP/flair/issues/1531", "id": 603356211, "node_id": "MDU6SXNzdWU2MDMzNTYyMTE=", "number": 1531, "title": "Integrate Taggers for Historical German by @redewiedergabe", "user": {"login": "alanakbik", "id": 18665324, "node_id": "MDQ6VXNlcjE4NjY1MzI0", "avatar_url": "https://avatars1.githubusercontent.com/u/18665324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanakbik", "html_url": "https://github.com/alanakbik", "followers_url": "https://api.github.com/users/alanakbik/followers", "following_url": "https://api.github.com/users/alanakbik/following{/other_user}", "gists_url": "https://api.github.com/users/alanakbik/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanakbik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanakbik/subscriptions", "organizations_url": "https://api.github.com/users/alanakbik/orgs", "repos_url": "https://api.github.com/users/alanakbik/repos", "events_url": "https://api.github.com/users/alanakbik/events{/privacy}", "received_events_url": "https://api.github.com/users/alanakbik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-20T16:10:34Z", "updated_at": "2020-04-20T23:15:56Z", "closed_at": "2020-04-20T23:15:56Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "The @redewiedergabe project trained taggers for historical German to detect different types of speech, though and writing representation. \r\n\r\nFull information on these taggers can be found here: https://github.com/redewiedergabe/tagger \r\n\r\nThis issue tracks the integration of the \"Redewiedergabe\" taggers into Flair.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1529", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1529/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1529/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1529/events", "html_url": "https://github.com/flairNLP/flair/issues/1529", "id": 602822418, "node_id": "MDU6SXNzdWU2MDI4MjI0MTg=", "number": 1529, "title": "Trapped in an infinite loop in the to_original_text method", "user": {"login": "psorianom", "id": 1085210, "node_id": "MDQ6VXNlcjEwODUyMTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1085210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/psorianom", "html_url": "https://github.com/psorianom", "followers_url": "https://api.github.com/users/psorianom/followers", "following_url": "https://api.github.com/users/psorianom/following{/other_user}", "gists_url": "https://api.github.com/users/psorianom/gists{/gist_id}", "starred_url": "https://api.github.com/users/psorianom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/psorianom/subscriptions", "organizations_url": "https://api.github.com/users/psorianom/orgs", "repos_url": "https://api.github.com/users/psorianom/repos", "events_url": "https://api.github.com/users/psorianom/events{/privacy}", "received_events_url": "https://api.github.com/users/psorianom/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-19T21:18:07Z", "updated_at": "2020-05-10T17:27:58Z", "closed_at": "2020-05-10T17:27:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "**Describe the bug**\r\nWhen using a faulty tokenizer, producing faulty span indices, the `Sentence` class method\r\n`to_original_text`  is bound to enter an infinite loop. While we should not have faulty tokenizers, \r\nit may happen, as it happened to me. This error is related to this closed [PR](https://github.com/flairNLP/flair/issues/226).\r\nhttps://github.com/flairNLP/flair/blob/dfa3fbfbc4d8ec237012a800415d1098abdb1aa4/flair/data.py#L406\r\n**To Reproduce**\r\nSteps to reproduce the behaviour (e.g. which model did you train? what parameters did you use? etc.).\r\n```python\r\n# Given a list of tokens\r\n# We simulate a faulty tokenizer by using wrong spans. (_execution_ `start_position` is wrong).\r\n# This faulty tokenizer may be used via a `build_faulty_tokenizer wrapper`,\r\n# such as the one for spacy.\r\nlist_tokens = [Token(\"de\", start_position=0), Token(\"pourvoir\", start_position=3), \r\nToken(\"\u00e0\", start_position=12), Token(\"l&apos;\", start_position=14), \r\nToken(\"execution\", start_position=16), Token(\"de\", start_position=32)]\r\ns = Sentence()\r\nfor token in list_tokens:\r\n    s.add_token(token)\r\n\r\n# We are now trapped in an infinite loop\r\nprint(s.to_original_text())\r\n```\r\n**Expected behaviour**\r\nPossibly an error/warning telling me that my tokenizer is faulty. Another solution would be to check if the `start_pos` is actually bigger, and not only different, than the `pos` counter, as below.\r\nThis may produce extra spaces not existent in the original phrase though.\r\n(`pytest tests/test_data.py::test_token_indices` still passes)\r\n```python\r\nfor t in self.tokens:\r\n            while t.start_pos > pos: \r\n                str += \" \"\r\n                pos += 1\r\n``` \r\n[Here](https://github.com/flairNLP/flair/compare/master...psorianom:original_text_infinite_loop) is a branch with this modif.\r\n\r\n**Environment (please complete the following information):**\r\n - OS Linux\r\n - Version flair-0.4.5\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1528", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1528/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1528/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1528/events", "html_url": "https://github.com/flairNLP/flair/issues/1528", "id": 602512351, "node_id": "MDU6SXNzdWU2MDI1MTIzNTE=", "number": 1528, "title": "Flair doesn't work correctly with CPU optimised torch for linux", "user": {"login": "kislerdm", "id": 13434797, "node_id": "MDQ6VXNlcjEzNDM0Nzk3", "avatar_url": "https://avatars1.githubusercontent.com/u/13434797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kislerdm", "html_url": "https://github.com/kislerdm", "followers_url": "https://api.github.com/users/kislerdm/followers", "following_url": "https://api.github.com/users/kislerdm/following{/other_user}", "gists_url": "https://api.github.com/users/kislerdm/gists{/gist_id}", "starred_url": "https://api.github.com/users/kislerdm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kislerdm/subscriptions", "organizations_url": "https://api.github.com/users/kislerdm/orgs", "repos_url": "https://api.github.com/users/kislerdm/repos", "events_url": "https://api.github.com/users/kislerdm/events{/privacy}", "received_events_url": "https://api.github.com/users/kislerdm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178841, "node_id": "MDU6TGFiZWw5NjExNzg4NDE=", "url": "https://api.github.com/repos/flairNLP/flair/labels/bug", "name": "bug", "color": "d73a4a", "default": true, "description": "Something isn't working"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-18T17:41:35Z", "updated_at": "2020-05-24T13:07:09Z", "closed_at": "2020-05-24T13:07:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Description**\r\n\r\nError when calling the method **_apply** of the class **flair.models.language_model.LanguageModel**:\r\n\r\n```bash\r\n/usr/local/lib/python3.8/site-packages/flair/models/language_model.py in <genexpr>(.0)\r\n    391 \r\n    392     def _apply(self, fn):\r\n--> 393         major, minor, build, *_ = (int(info)\r\n    394                                 for info in torch.__version__.split('.'))\r\n    395 \r\n\r\nValueError: invalid literal for int() with base 10: '0+cpu'\r\n```\r\n\r\nIt is blocking functionality of the library when it comes to *embeddings*, or *language_model* definition.\r\n\r\n**Error cause**\r\n\r\nError is caused by parsing torch version (see the line *394* of the module *flair.models.language_model.py*). It fails when a CPU specific torch version is being used. For example, the CPU optimised version of torch-1.4.0 for linux (available [here](https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp38-cp38-linux_x86_64.whl)) has `__versions__ = '1.4.0+cpu'`.\r\n\r\n**Solution proposal**\r\n\r\n*Status quo*:\r\n```python\r\ndef _apply(self, fn):\r\n        major, minor, build, *_ = (int(info)\r\n                                for info in torch.__version__.split('.'))\r\n```\r\n\r\n*Fix*:\r\n```python\r\ndef _apply(self, fn):\r\n        major, minor, *_ = (int(info) for info in torch.__version__.split('.')[:-1])\r\n```\r\n\r\nThe object `build` unpacked from the `torch.__version__` parsing result's tuple is not being used, ergo its removal would be a reasonable solution.\r\n\r\n**Environment:**\r\n - OS: linux debian 10 (base of the docker image `python:3.8.1-slim-buster`)\r\n - torch==1.4.0+cpu\r\n - flair==0.4.5\r\n\r\n**Instruction to reproduce**\r\n\r\nBuild the image, it will fail due described error.\r\n\r\n```Dockerfile\r\nFROM python:3.8.1-slim-buster AS base\r\n\r\nWORKDIR /\r\n\r\nRUN apt-get update -y \\\r\n  && pip install --upgrade pip \\\r\n  && pip install --no-cache-dir \"https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp38-cp38-linux_x86_64.whl\" flair==0.4.5\r\n\r\nRUN python -c \"import flair; from flair.embeddings import StackedEmbeddings, FlairEmbeddings;\\\r\nembeddings = StackedEmbeddings([FlairEmbeddings('mix-forward')]);\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1524", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1524/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1524/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1524/events", "html_url": "https://github.com/flairNLP/flair/issues/1524", "id": 600800131, "node_id": "MDU6SXNzdWU2MDA4MDAxMzE=", "number": 1524, "title": "LM Training: Structure of input files", "user": {"login": "leonweber", "id": 6436442, "node_id": "MDQ6VXNlcjY0MzY0NDI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6436442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leonweber", "html_url": "https://github.com/leonweber", "followers_url": "https://api.github.com/users/leonweber/followers", "following_url": "https://api.github.com/users/leonweber/following{/other_user}", "gists_url": "https://api.github.com/users/leonweber/gists{/gist_id}", "starred_url": "https://api.github.com/users/leonweber/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leonweber/subscriptions", "organizations_url": "https://api.github.com/users/leonweber/orgs", "repos_url": "https://api.github.com/users/leonweber/repos", "events_url": "https://api.github.com/users/leonweber/events{/privacy}", "received_events_url": "https://api.github.com/users/leonweber/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 961178847, "node_id": "MDU6TGFiZWw5NjExNzg4NDc=", "url": "https://api.github.com/repos/flairNLP/flair/labels/question", "name": "question", "color": "2936ad", "default": true, "description": "Further information is requested"}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-04-16T07:08:58Z", "updated_at": "2020-04-20T15:09:50Z", "closed_at": "2020-04-20T15:09:50Z", "author_association": "COLLABORATOR", "active_lock_reason": null, "body": "I would like to train a flair LM and I am a bit confused about the expected format of the input files. Does the LM training assume sentence segmented text? Is it one document or one sentence per line? How are documents separated? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/flairNLP/flair/issues/1521", "repository_url": "https://api.github.com/repos/flairNLP/flair", "labels_url": "https://api.github.com/repos/flairNLP/flair/issues/1521/labels{/name}", "comments_url": "https://api.github.com/repos/flairNLP/flair/issues/1521/comments", "events_url": "https://api.github.com/repos/flairNLP/flair/issues/1521/events", "html_url": "https://github.com/flairNLP/flair/issues/1521", "id": 597080687, "node_id": "MDU6SXNzdWU1OTcwODA2ODc=", "number": 1521, "title": "POS Tagger model for Malayalam Language", "user": {"login": "sabiqueqb", "id": 19944520, "node_id": "MDQ6VXNlcjE5OTQ0NTIw", "avatar_url": "https://avatars2.githubusercontent.com/u/19944520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sabiqueqb", "html_url": "https://github.com/sabiqueqb", "followers_url": "https://api.github.com/users/sabiqueqb/followers", "following_url": "https://api.github.com/users/sabiqueqb/following{/other_user}", "gists_url": "https://api.github.com/users/sabiqueqb/gists{/gist_id}", "starred_url": "https://api.github.com/users/sabiqueqb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sabiqueqb/subscriptions", "organizations_url": "https://api.github.com/users/sabiqueqb/orgs", "repos_url": "https://api.github.com/users/sabiqueqb/repos", "events_url": "https://api.github.com/users/sabiqueqb/events{/privacy}", "received_events_url": "https://api.github.com/users/sabiqueqb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2020-04-09T07:29:41Z", "updated_at": "2020-05-24T13:07:55Z", "closed_at": "2020-05-24T13:07:55Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I have found that there is no POS Tagger model available for the Malayalam language currently. I have trained a model for UPOS Tagging and XPOS Tagging  for Malayalam language and would like to add it to Flair repository. \r\n\r\nThe model has trained with more than 30000 sentences and have the average accuracy as follows:\r\n* UPOS Model - 87%\r\n* XPOS Model - 83%", "performed_via_github_app": null, "score": 1.0}]}