{"total_count": 332, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/joblib/joblib/issues/1089", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1089/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1089/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1089/events", "html_url": "https://github.com/joblib/joblib/issues/1089", "id": 664623901, "node_id": "MDU6SXNzdWU2NjQ2MjM5MDE=", "number": 1089, "title": "Updating Cloudpickle to 1.5.0", "user": {"login": "jakirkham", "id": 3019665, "node_id": "MDQ6VXNlcjMwMTk2NjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/3019665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakirkham", "html_url": "https://github.com/jakirkham", "followers_url": "https://api.github.com/users/jakirkham/followers", "following_url": "https://api.github.com/users/jakirkham/following{/other_user}", "gists_url": "https://api.github.com/users/jakirkham/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakirkham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakirkham/subscriptions", "organizations_url": "https://api.github.com/users/jakirkham/orgs", "repos_url": "https://api.github.com/users/jakirkham/repos", "events_url": "https://api.github.com/users/jakirkham/events{/privacy}", "received_events_url": "https://api.github.com/users/jakirkham/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-23T16:47:34Z", "updated_at": "2020-07-24T14:50:26Z", "closed_at": "2020-07-24T12:17:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Now that Cloudpickle 1.5.0 is out, it would be great to include that in Joblib as well so we can benefit from new features (like out-of-band pickle on older Python versions \ud83d\ude09).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1087", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1087/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1087/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1087/events", "html_url": "https://github.com/joblib/joblib/issues/1087", "id": 655646808, "node_id": "MDU6SXNzdWU2NTU2NDY4MDg=", "number": 1087, "title": "Recent versions are missing from the changelog in readthedocs", "user": {"login": "alegonz", "id": 26665243, "node_id": "MDQ6VXNlcjI2NjY1MjQz", "avatar_url": "https://avatars2.githubusercontent.com/u/26665243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alegonz", "html_url": "https://github.com/alegonz", "followers_url": "https://api.github.com/users/alegonz/followers", "following_url": "https://api.github.com/users/alegonz/following{/other_user}", "gists_url": "https://api.github.com/users/alegonz/gists{/gist_id}", "starred_url": "https://api.github.com/users/alegonz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alegonz/subscriptions", "organizations_url": "https://api.github.com/users/alegonz/orgs", "repos_url": "https://api.github.com/users/alegonz/repos", "events_url": "https://api.github.com/users/alegonz/events{/privacy}", "received_events_url": "https://api.github.com/users/alegonz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-07-13T08:25:36Z", "updated_at": "2020-07-15T14:02:40Z", "closed_at": "2020-07-15T14:02:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there :)\r\n\r\nWhen perusing the documentation I just noticed that the [changelog](https://joblib.readthedocs.io/en/latest/developing.html#latest-changes) as shown in readthedocs is missing the changes for versions 0.15.0 onward.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1081", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1081/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1081/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1081/events", "html_url": "https://github.com/joblib/joblib/issues/1081", "id": 649376699, "node_id": "MDU6SXNzdWU2NDkzNzY2OTk=", "number": 1081, "title": "two output value from joblib function", "user": {"login": "sassan72", "id": 22191884, "node_id": "MDQ6VXNlcjIyMTkxODg0", "avatar_url": "https://avatars3.githubusercontent.com/u/22191884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sassan72", "html_url": "https://github.com/sassan72", "followers_url": "https://api.github.com/users/sassan72/followers", "following_url": "https://api.github.com/users/sassan72/following{/other_user}", "gists_url": "https://api.github.com/users/sassan72/gists{/gist_id}", "starred_url": "https://api.github.com/users/sassan72/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sassan72/subscriptions", "organizations_url": "https://api.github.com/users/sassan72/orgs", "repos_url": "https://api.github.com/users/sassan72/repos", "events_url": "https://api.github.com/users/sassan72/events{/privacy}", "received_events_url": "https://api.github.com/users/sassan72/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-07-01T22:50:42Z", "updated_at": "2020-07-15T08:17:33Z", "closed_at": "2020-07-15T08:17:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\nwith parallel_backend(\"multiprocessing\"):\r\n        g, g_bias = Parallel(n_jobs=-2)(delayed(gradients)(var_corrected, feats_train_batch[j], Y_train_batch[j], bias) for j in range(batch_size) )\r\n        grads += g / batch_size\r\n        grad_bias +=g_bias / batch_size\r\n```\r\n\r\nI got this error:\r\n\r\n```python-traceback\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-9-147a7cd01c32> in <module>\r\n     29     with parallel_backend(\"multiprocessing\"):\r\n     30         g, g_bias = Parallel(n_jobs=-2)(delayed(gradients)(var_corrected, feats_train_batch[j], Y_train_batch[j], bias) for j in range(batch_size) )\r\n---> 31         grads += g / batch_size\r\n     32         grad_bias +=g_bias / batch_size\r\n     33 \r\n\r\nTypeError: unsupported operand type(s) for /: 'tuple' and 'int'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1076", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1076/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1076/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1076/events", "html_url": "https://github.com/joblib/joblib/issues/1076", "id": 647324157, "node_id": "MDU6SXNzdWU2NDczMjQxNTc=", "number": 1076, "title": "KeyError in line 287 of resource_tracker.py using loky backend, /dev/shm/folder not created?", "user": {"login": "matthewhang", "id": 24700304, "node_id": "MDQ6VXNlcjI0NzAwMzA0", "avatar_url": "https://avatars1.githubusercontent.com/u/24700304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthewhang", "html_url": "https://github.com/matthewhang", "followers_url": "https://api.github.com/users/matthewhang/followers", "following_url": "https://api.github.com/users/matthewhang/following{/other_user}", "gists_url": "https://api.github.com/users/matthewhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthewhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthewhang/subscriptions", "organizations_url": "https://api.github.com/users/matthewhang/orgs", "repos_url": "https://api.github.com/users/matthewhang/repos", "events_url": "https://api.github.com/users/matthewhang/events{/privacy}", "received_events_url": "https://api.github.com/users/matthewhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2020-06-29T12:17:21Z", "updated_at": "2020-07-21T10:17:26Z", "closed_at": "2020-07-21T03:40:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi all, \r\n\r\nI am using joblib 0.15.1 installed with conda and encountered the following problems while using the loky backend, there are multiple instances of error similar to the following, the only difference is the folder name (i guess) after /dev/shm.\r\n\r\n> KeyError: /dev/shm/joblib_memmapping_folder_71367_e761fc411a594e51bfec8e8002eef735_ea8c40a4eda54696a61ed4d8cb034363/71367-139949476506424-dcad585b155949789780aa7cb571abf7.pkl\r\n\r\nwhen the program end, there is another message:\r\n\r\n> /joblib/externals/loky/backend/resource_tracker.py:320: UserWarning: resource_tracker: There appear to be 289 leaked file objects to clean up at shutdown\r\n>   (len(rtype_registry), rtype))\r\n\r\nand lots of the following (many repeated warnings, the only difference is the folder name):\r\n\r\n> joblib/externals/loky/backend/resource_tracker.py:333: UserWarning: resource_tracker: /dev/shm/joblib_memmapping_folder_7         1367_e761fc411a594e51bfec8e8002eef735_aba7d846e2d7478fb3221a7435a5cbe1/71367-139949476506424-f948a4e70d864c3fb73b6b0dcad7d4b5.pkl: FileNotFoundError(2, 'No such file or directory'         )\r\n\r\nHowever, the program seems to keep running and return some results (not sure if the program is correctly run though)\r\n\r\nAny ideas on this? Thanks in advance.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1075", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1075/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1075/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1075/events", "html_url": "https://github.com/joblib/joblib/issues/1075", "id": 646996595, "node_id": "MDU6SXNzdWU2NDY5OTY1OTU=", "number": 1075, "title": "about using locky mode joblib. ", "user": {"login": "sassan72", "id": 22191884, "node_id": "MDQ6VXNlcjIyMTkxODg0", "avatar_url": "https://avatars3.githubusercontent.com/u/22191884?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sassan72", "html_url": "https://github.com/sassan72", "followers_url": "https://api.github.com/users/sassan72/followers", "following_url": "https://api.github.com/users/sassan72/following{/other_user}", "gists_url": "https://api.github.com/users/sassan72/gists{/gist_id}", "starred_url": "https://api.github.com/users/sassan72/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sassan72/subscriptions", "organizations_url": "https://api.github.com/users/sassan72/orgs", "repos_url": "https://api.github.com/users/sassan72/repos", "events_url": "https://api.github.com/users/sassan72/events{/privacy}", "received_events_url": "https://api.github.com/users/sassan72/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-06-28T20:11:07Z", "updated_at": "2020-06-30T14:31:21Z", "closed_at": "2020-06-29T15:52:29Z", "author_association": "NONE", "active_lock_reason": null, "body": "I could successfully run my code with multiprocessing mode joblib. here is the command line that I used to run the code in parallel mode:\r\n\r\nrr = Parallel(n_jobs=-1, backend=\"locky\")(delayed(bh_diag)(N, M, Mx, My, U, m[i], J, Vx, Vy) for i in range(0, len(m)) )\r\n\r\nhow can I run it with locky mode? which one is more powerful? multiprocessing or locky. I know that locky take advantage of both multiprocessing and multithreading. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1073", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1073/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1073/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1073/events", "html_url": "https://github.com/joblib/joblib/issues/1073", "id": 644439639, "node_id": "MDU6SXNzdWU2NDQ0Mzk2Mzk=", "number": 1073, "title": "ModuleNotFoundError: No module named 'utils'", "user": {"login": "Adblu", "id": 34426707, "node_id": "MDQ6VXNlcjM0NDI2NzA3", "avatar_url": "https://avatars3.githubusercontent.com/u/34426707?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Adblu", "html_url": "https://github.com/Adblu", "followers_url": "https://api.github.com/users/Adblu/followers", "following_url": "https://api.github.com/users/Adblu/following{/other_user}", "gists_url": "https://api.github.com/users/Adblu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Adblu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Adblu/subscriptions", "organizations_url": "https://api.github.com/users/Adblu/orgs", "repos_url": "https://api.github.com/users/Adblu/repos", "events_url": "https://api.github.com/users/Adblu/events{/privacy}", "received_events_url": "https://api.github.com/users/Adblu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-24T08:53:23Z", "updated_at": "2020-07-02T12:46:09Z", "closed_at": "2020-07-02T12:46:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "On fresh environment, python 3.6 I am trying to import the data that was previously saved:\r\n```\r\njoblib.load(file)\r\n```\r\nIm having following error:\r\n\r\n```\r\n>>> a = joblib.load('/home/n/Documents/production_inference/results/2016/16481755_Foto[10]')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/n/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py\", line 598, in load\r\n    obj = _unpickle(fobj, filename, mmap_mode)\r\n  File \"/home/n/anaconda3/lib/python3.7/site-packages/joblib/numpy_pickle.py\", line 526, in _unpickle\r\n    obj = unpickler.load()\r\n  File \"/home/n/anaconda3/lib/python3.7/pickle.py\", line 1088, in load\r\n    dispatch[key[0]](self)\r\n  File \"/home/n/anaconda3/lib/python3.7/pickle.py\", line 1376, in load_global\r\n    klass = self.find_class(module, name)\r\n  File \"/home/n/anaconda3/lib/python3.7/pickle.py\", line 1426, in find_class\r\n    __import__(module, level=0)\r\nModuleNotFoundError: No module named 'utils'\r\n\r\n```\r\n\r\nHow to fix it ?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1072", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1072/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1072/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1072/events", "html_url": "https://github.com/joblib/joblib/issues/1072", "id": 643619846, "node_id": "MDU6SXNzdWU2NDM2MTk4NDY=", "number": 1072, "title": "DeprecationWarning: tostring() is deprecated", "user": {"login": "mhooreman", "id": 7373718, "node_id": "MDQ6VXNlcjczNzM3MTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7373718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mhooreman", "html_url": "https://github.com/mhooreman", "followers_url": "https://api.github.com/users/mhooreman/followers", "following_url": "https://api.github.com/users/mhooreman/following{/other_user}", "gists_url": "https://api.github.com/users/mhooreman/gists{/gist_id}", "starred_url": "https://api.github.com/users/mhooreman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mhooreman/subscriptions", "organizations_url": "https://api.github.com/users/mhooreman/orgs", "repos_url": "https://api.github.com/users/mhooreman/repos", "events_url": "https://api.github.com/users/mhooreman/events{/privacy}", "received_events_url": "https://api.github.com/users/mhooreman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-23T07:59:29Z", "updated_at": "2020-07-02T14:00:33Z", "closed_at": "2020-07-02T14:00:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nThe method `joblib.numpy_pickle.NumpyArrayWrapper.write_array` is using `chunk.tostring`, which is deprecated and should be replaced by `chunk.tobytes`:\r\n\r\n    File \"_my_scrambled_path_/python3.7/site-packages/joblib/numpy_pickle.py\", line 103, in write_array\r\n\t    pickler.file_handle.write(chunk.tostring('C'))\r\n\tDeprecationWarning: tostring() is deprecated. Use tobytes() instead.\r\n\r\nVersions: python 3.7.4, joblib 0.15.1, numpy 1.19.0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1069", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1069/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1069/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1069/events", "html_url": "https://github.com/joblib/joblib/issues/1069", "id": 640138044, "node_id": "MDU6SXNzdWU2NDAxMzgwNDQ=", "number": 1069, "title": "Caching within parallel processing not working in Jupyter", "user": {"login": "dsevero", "id": 17755465, "node_id": "MDQ6VXNlcjE3NzU1NDY1", "avatar_url": "https://avatars0.githubusercontent.com/u/17755465?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dsevero", "html_url": "https://github.com/dsevero", "followers_url": "https://api.github.com/users/dsevero/followers", "following_url": "https://api.github.com/users/dsevero/following{/other_user}", "gists_url": "https://api.github.com/users/dsevero/gists{/gist_id}", "starred_url": "https://api.github.com/users/dsevero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dsevero/subscriptions", "organizations_url": "https://api.github.com/users/dsevero/orgs", "repos_url": "https://api.github.com/users/dsevero/repos", "events_url": "https://api.github.com/users/dsevero/events{/privacy}", "received_events_url": "https://api.github.com/users/dsevero/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-17T04:48:33Z", "updated_at": "2020-06-17T08:48:10Z", "closed_at": "2020-06-17T08:48:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "`Memory` and `Parallel` work fine independently in Jupyter, but once combined they do not. The simplest way to reproduce this is with the example given at: https://joblib.readthedocs.io/en/latest/auto_examples/nested_parallel_memory.html#sphx-glr-download-auto-examples-nested-parallel-memory-py\r\n\r\nRunning the `.py` works fine both in python and ipython, but the `.ipynb` running in Jupyter does not. This can be seen by noticing that the second round (that should take advantage of caching) takes the same time as the first round. Also, the following is printed multiple times:\r\n\r\n```\r\nJobLibCollisionWarning: Cannot detect name collisions for function 'costly_compute'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1065", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1065/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1065/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1065/events", "html_url": "https://github.com/joblib/joblib/issues/1065", "id": 636411683, "node_id": "MDU6SXNzdWU2MzY0MTE2ODM=", "number": 1065, "title": "[PyTorch] joblib.dump causes \"struct.error: 'I' format requires 0 <= number <= 4294967295\" ", "user": {"login": "S-W-Williams", "id": 13688953, "node_id": "MDQ6VXNlcjEzNjg4OTUz", "avatar_url": "https://avatars0.githubusercontent.com/u/13688953?v=4", "gravatar_id": "", "url": "https://api.github.com/users/S-W-Williams", "html_url": "https://github.com/S-W-Williams", "followers_url": "https://api.github.com/users/S-W-Williams/followers", "following_url": "https://api.github.com/users/S-W-Williams/following{/other_user}", "gists_url": "https://api.github.com/users/S-W-Williams/gists{/gist_id}", "starred_url": "https://api.github.com/users/S-W-Williams/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/S-W-Williams/subscriptions", "organizations_url": "https://api.github.com/users/S-W-Williams/orgs", "repos_url": "https://api.github.com/users/S-W-Williams/repos", "events_url": "https://api.github.com/users/S-W-Williams/events{/privacy}", "received_events_url": "https://api.github.com/users/S-W-Williams/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-06-10T17:06:37Z", "updated_at": "2020-06-15T11:43:12Z", "closed_at": "2020-06-15T11:43:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI'm trying to use `joblib.dump` on a fairly large Pytorch model. This is happening on joblib 0.14.1.\r\n\r\n```\r\njoblib.dump(o, buffer)\r\nFile \"/opt/amazon/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 507, in dump\r\n\r\nNumpyPickler(filename, protocol=protocol).dump(value)\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 409, in dump\r\n    self.save(obj)\r\n\r\nFile \"/opt/amazon/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 295, in save\r\nreturn Pickler.save(self, obj)\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 521, in save\r\nself.save_reduce(obj=obj, *rv)\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 634, in save_reduce\r\nsave(state)\r\n\r\nFile \"/opt/amazon/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 295, in save\r\nreturn Pickler.save(self, obj)\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 476, in save\r\nf(self, obj) # Call unbound method with explicit self\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 821, in save_dict\r\nself._batch_setitems(obj.items())\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\nsave(v)\r\n\r\nFile \"/opt/amazon/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 295, in save\r\nreturn Pickler.save(self, obj)\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 521, in save\r\nself.save_reduce(obj=obj, *rv)\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 634, in save_reduce\r\nsave(state)\r\n\r\nFile \"/opt/amazon/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 295, in save\r\nreturn Pickler.save(self, obj)\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 476, in save\r\nf(self, obj) # Call unbound method with explicit self\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 821, in save_dict\r\nself._batch_setitems(obj.items())\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\nsave(v)\r\n\r\nFile \"/opt/amazon/lib/python3.6/site-packages/joblib/numpy_pickle.py\", line 295, in save\r\nreturn Pickler.save(self, obj)\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 476, in save\r\nf(self, obj) # Call unbound method with explicit self\r\n\r\nFile \"/opt/amazon/python3.6/lib/python3.6/pickle.py\", line 714, in save_str\r\nself.write(BINUNICODE + pack(\"<I\", n) + encoded)\r\nstruct.error: 'I' format requires 0 <= number <= 4294967295\r\n```\r\n\r\nAny workarounds to this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1059", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1059/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1059/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1059/events", "html_url": "https://github.com/joblib/joblib/issues/1059", "id": 631760257, "node_id": "MDU6SXNzdWU2MzE3NjAyNTc=", "number": 1059, "title": "ImportError: cannot import name 'joblib'", "user": {"login": "Kmoorthi1989", "id": 66482598, "node_id": "MDQ6VXNlcjY2NDgyNTk4", "avatar_url": "https://avatars2.githubusercontent.com/u/66482598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kmoorthi1989", "html_url": "https://github.com/Kmoorthi1989", "followers_url": "https://api.github.com/users/Kmoorthi1989/followers", "following_url": "https://api.github.com/users/Kmoorthi1989/following{/other_user}", "gists_url": "https://api.github.com/users/Kmoorthi1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kmoorthi1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kmoorthi1989/subscriptions", "organizations_url": "https://api.github.com/users/Kmoorthi1989/orgs", "repos_url": "https://api.github.com/users/Kmoorthi1989/repos", "events_url": "https://api.github.com/users/Kmoorthi1989/events{/privacy}", "received_events_url": "https://api.github.com/users/Kmoorthi1989/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-06-05T17:02:29Z", "updated_at": "2020-06-05T18:57:14Z", "closed_at": "2020-06-05T18:57:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm getting **ImportError: cannot import name 'joblib'** error while run python app. App file contain **from sklearn.externals import joblib**,, sklearn and joblib library already installed.\r\n\r\nPlease help me.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1058", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1058/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1058/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1058/events", "html_url": "https://github.com/joblib/joblib/issues/1058", "id": 631444464, "node_id": "MDU6SXNzdWU2MzE0NDQ0NjQ=", "number": 1058, "title": "[Memory] UserWarning Persisting input arguments took 1.12s to run.", "user": {"login": "skjerns", "id": 14980558, "node_id": "MDQ6VXNlcjE0OTgwNTU4", "avatar_url": "https://avatars2.githubusercontent.com/u/14980558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skjerns", "html_url": "https://github.com/skjerns", "followers_url": "https://api.github.com/users/skjerns/followers", "following_url": "https://api.github.com/users/skjerns/following{/other_user}", "gists_url": "https://api.github.com/users/skjerns/gists{/gist_id}", "starred_url": "https://api.github.com/users/skjerns/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skjerns/subscriptions", "organizations_url": "https://api.github.com/users/skjerns/orgs", "repos_url": "https://api.github.com/users/skjerns/repos", "events_url": "https://api.github.com/users/skjerns/events{/privacy}", "received_events_url": "https://api.github.com/users/skjerns/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-06-05T09:19:19Z", "updated_at": "2020-06-15T17:15:48Z", "closed_at": "2020-06-15T17:15:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm wrapping data loading function from `pyedflib` that loads an EDF file (biosignal format).\r\n\r\nFiles are taken e.g. from here: https://physionet.org/content/sleep-edfx/1.0.0/\r\n\r\n```Python\r\nfrom pyedflib import highlevel\r\nread_edf = memory.cache(highlevel.read_edf)\r\n\r\nfiles = [... list of files from physionet]\r\n\r\nfor file in files:\r\n    signals, sheaders, header = read_edf(file)\r\n```\r\n\r\nThis produces the following error:\r\n\r\n```\r\n  signals, shead, header = read_edf(edf_file, digital=True, verbose=False)\r\nxxx.py:172: UserWarning: Persisting input arguments took 1.12s to run.\r\nIf this happens often in your code, it can cause performance problems\r\n(results will be correct in all cases).\r\nThe reason for this is probably some large input arguments for a wrapped\r\n function (e.g. large strings).\r\nTHIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\r\n example so that they can fix the problem.\r\n```\r\n\r\nAny idea how to fix this? I'm not actually supllying any large arguments to the function.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1050", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1050/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1050/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1050/events", "html_url": "https://github.com/joblib/joblib/issues/1050", "id": 619073289, "node_id": "MDU6SXNzdWU2MTkwNzMyODk=", "number": 1050, "title": "python 3.7.3 ModuleNotFoundError: No module named '_lzma' with joblib 0.15.0", "user": {"login": "xiaozhongtian", "id": 43748914, "node_id": "MDQ6VXNlcjQzNzQ4OTE0", "avatar_url": "https://avatars0.githubusercontent.com/u/43748914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiaozhongtian", "html_url": "https://github.com/xiaozhongtian", "followers_url": "https://api.github.com/users/xiaozhongtian/followers", "following_url": "https://api.github.com/users/xiaozhongtian/following{/other_user}", "gists_url": "https://api.github.com/users/xiaozhongtian/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiaozhongtian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiaozhongtian/subscriptions", "organizations_url": "https://api.github.com/users/xiaozhongtian/orgs", "repos_url": "https://api.github.com/users/xiaozhongtian/repos", "events_url": "https://api.github.com/users/xiaozhongtian/events{/privacy}", "received_events_url": "https://api.github.com/users/xiaozhongtian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}, {"id": 1718801259, "node_id": "MDU6TGFiZWwxNzE4ODAxMjU5", "url": "https://api.github.com/repos/joblib/joblib/labels/regression", "name": "regression", "color": "c600a5", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-05-15T15:50:04Z", "updated_at": "2020-05-17T21:17:02Z", "closed_at": "2020-05-16T12:37:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nNew version  joblib 0.15.0 has incompability with python 3.7.3 in centos.\r\n```\r\n>>> import joblib\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xiaozho/export_mlbox/mlbox_export/venv/lib/python3.7/site-packages/joblib/__init__.py\", line 113, in <module>\r\n    from .memory import Memory, MemorizedResult, register_store_backend\r\n  File \"/home/xiaozho/export_mlbox/mlbox_export/venv/lib/python3.7/site-packages/joblib/memory.py\", line 33, in <module>\r\n    from ._store_backends import StoreBackendBase, FileSystemStoreBackend\r\n  File \"/home/xiaozho/export_mlbox/mlbox_export/venv/lib/python3.7/site-packages/joblib/_store_backends.py\", line 17, in <module>\r\n    from . import numpy_pickle\r\n  File \"/home/xiaozho/export_mlbox/mlbox_export/venv/lib/python3.7/site-packages/joblib/numpy_pickle.py\", line 15, in <module>\r\n    from .compressor import lz4, LZ4_NOT_INSTALLED_ERROR\r\n  File \"/home/xiaozho/export_mlbox/mlbox_export/venv/lib/python3.7/site-packages/joblib/compressor.py\", line 24, in <module>\r\n    import lzma\r\n  File \"/usr/local/python3/lib/python3.7/lzma.py\", line 27, in <module>\r\n    from _lzma import *\r\nModuleNotFoundError: No module named '_lzma'\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1040", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1040/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1040/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1040/events", "html_url": "https://github.com/joblib/joblib/issues/1040", "id": 610623254, "node_id": "MDU6SXNzdWU2MTA2MjMyNTQ=", "number": 1040, "title": "Random timeout in test_pool_memmap_with_big_offset", "user": {"login": "pierreglaser", "id": 18555600, "node_id": "MDQ6VXNlcjE4NTU1NjAw", "avatar_url": "https://avatars3.githubusercontent.com/u/18555600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pierreglaser", "html_url": "https://github.com/pierreglaser", "followers_url": "https://api.github.com/users/pierreglaser/followers", "following_url": "https://api.github.com/users/pierreglaser/following{/other_user}", "gists_url": "https://api.github.com/users/pierreglaser/gists{/gist_id}", "starred_url": "https://api.github.com/users/pierreglaser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pierreglaser/subscriptions", "organizations_url": "https://api.github.com/users/pierreglaser/orgs", "repos_url": "https://api.github.com/users/pierreglaser/repos", "events_url": "https://api.github.com/users/pierreglaser/events{/privacy}", "received_events_url": "https://api.github.com/users/pierreglaser/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-05-01T07:59:33Z", "updated_at": "2020-05-02T17:48:00Z", "closed_at": "2020-05-02T17:48:00Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Appears frequently on `windows` + `Python 3.8`.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1035", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1035/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1035/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1035/events", "html_url": "https://github.com/joblib/joblib/issues/1035", "id": 608955182, "node_id": "MDU6SXNzdWU2MDg5NTUxODI=", "number": 1035, "title": "Using Memory and Parallel with cached function defined inside Jupyter notebook results in not using the cache", "user": {"login": "jontis", "id": 1307210, "node_id": "MDQ6VXNlcjEzMDcyMTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1307210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jontis", "html_url": "https://github.com/jontis", "followers_url": "https://api.github.com/users/jontis/followers", "following_url": "https://api.github.com/users/jontis/following{/other_user}", "gists_url": "https://api.github.com/users/jontis/gists{/gist_id}", "starred_url": "https://api.github.com/users/jontis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jontis/subscriptions", "organizations_url": "https://api.github.com/users/jontis/orgs", "repos_url": "https://api.github.com/users/jontis/repos", "events_url": "https://api.github.com/users/jontis/events{/privacy}", "received_events_url": "https://api.github.com/users/jontis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-04-29T10:45:09Z", "updated_at": "2020-08-04T16:59:16Z", "closed_at": "2020-08-04T16:59:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm running a Jupyter notebook on a machine with multiple cores. \r\nI'm trying to use both caching of intermediate results and parallelism for this.\r\nI get a lot of:\r\n\r\n```\r\npython3.7/site-packages/joblib/parallel.py:256: JobLibCollisionWarning: Cannot detect name collisions for function 'cached_two' for func, args, kwargs in self.items]\r\n```\r\n\r\nI believe these are not appearing when I just run single threade or not in a notebook.\r\nSample code below to run in jupyter notebook.\r\njoblib                    0.14.1\r\nnotebook                  6.0.3", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1029", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1029/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1029/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1029/events", "html_url": "https://github.com/joblib/joblib/issues/1029", "id": 590746397, "node_id": "MDU6SXNzdWU1OTA3NDYzOTc=", "number": 1029, "title": "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.", "user": {"login": "omarjamal1989", "id": 46420430, "node_id": "MDQ6VXNlcjQ2NDIwNDMw", "avatar_url": "https://avatars1.githubusercontent.com/u/46420430?v=4", "gravatar_id": "", "url": "https://api.github.com/users/omarjamal1989", "html_url": "https://github.com/omarjamal1989", "followers_url": "https://api.github.com/users/omarjamal1989/followers", "following_url": "https://api.github.com/users/omarjamal1989/following{/other_user}", "gists_url": "https://api.github.com/users/omarjamal1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/omarjamal1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/omarjamal1989/subscriptions", "organizations_url": "https://api.github.com/users/omarjamal1989/orgs", "repos_url": "https://api.github.com/users/omarjamal1989/repos", "events_url": "https://api.github.com/users/omarjamal1989/events{/privacy}", "received_events_url": "https://api.github.com/users/omarjamal1989/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-31T02:53:14Z", "updated_at": "2020-03-31T14:04:15Z", "closed_at": "2020-03-31T14:03:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "Please help solve the problem\r\n\r\n\r\n>>>from sklearn.model_selection import GridSearchCV\r\n>>>param_grid ={'C' :[0.1,1,10,100,1000], 'gamma':[1,0.1,0.01,0.001,0.0001]}\r\n>>>from sklearn.svm import SVC\r\n\r\n>>>grid = GridSearchCV(SVC(),param_grid,verbose=2)\r\nAfter carrying out \r\n>>>grid.fit(X_train,y_train)\r\n\r\nFitting 5 folds for each of 25 candidates, totalling 125 fits\r\n[CV] C=0.1, gamma=1 ..................................................\r\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1014", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1014/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1014/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1014/events", "html_url": "https://github.com/joblib/joblib/issues/1014", "id": 567702123, "node_id": "MDU6SXNzdWU1Njc3MDIxMjM=", "number": 1014, "title": "sklearn test failures on master", "user": {"login": "pierreglaser", "id": 18555600, "node_id": "MDQ6VXNlcjE4NTU1NjAw", "avatar_url": "https://avatars3.githubusercontent.com/u/18555600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pierreglaser", "html_url": "https://github.com/pierreglaser", "followers_url": "https://api.github.com/users/pierreglaser/followers", "following_url": "https://api.github.com/users/pierreglaser/following{/other_user}", "gists_url": "https://api.github.com/users/pierreglaser/gists{/gist_id}", "starred_url": "https://api.github.com/users/pierreglaser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pierreglaser/subscriptions", "organizations_url": "https://api.github.com/users/pierreglaser/orgs", "repos_url": "https://api.github.com/users/pierreglaser/repos", "events_url": "https://api.github.com/users/pierreglaser/events{/privacy}", "received_events_url": "https://api.github.com/users/pierreglaser/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-19T17:03:00Z", "updated_at": "2020-02-21T12:37:58Z", "closed_at": "2020-02-21T12:37:58Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Is there any way we can easily fix the CI errors happening on `master` when testing `sklearn`? cc @ogrisel ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/1007", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/1007/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/1007/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/1007/events", "html_url": "https://github.com/joblib/joblib/issues/1007", "id": 564849983, "node_id": "MDU6SXNzdWU1NjQ4NDk5ODM=", "number": 1007, "title": "RuntimeError: Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm", "user": {"login": "jctaillandier", "id": 31980034, "node_id": "MDQ6VXNlcjMxOTgwMDM0", "avatar_url": "https://avatars0.githubusercontent.com/u/31980034?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jctaillandier", "html_url": "https://github.com/jctaillandier", "followers_url": "https://api.github.com/users/jctaillandier/followers", "following_url": "https://api.github.com/users/jctaillandier/following{/other_user}", "gists_url": "https://api.github.com/users/jctaillandier/gists{/gist_id}", "starred_url": "https://api.github.com/users/jctaillandier/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jctaillandier/subscriptions", "organizations_url": "https://api.github.com/users/jctaillandier/orgs", "repos_url": "https://api.github.com/users/jctaillandier/repos", "events_url": "https://api.github.com/users/jctaillandier/events{/privacy}", "received_events_url": "https://api.github.com/users/jctaillandier/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-02-13T17:47:38Z", "updated_at": "2020-02-13T18:57:35Z", "closed_at": "2020-02-13T18:57:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "Error saying that specific matrix is not in right device, although that matrix is generated by an object I passed in upstream, that is in cuda.\r\n\r\nI am running on remote cluster, so might be a memory issue, but wanted to ask here in case.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"../gansan/Optimal.py\", line 793, in <module>\r\n    all_folds()\r\n  File \"../gansan/Optimal.py\", line 756, in all_folds\r\n    for fold_id in range(1, p.K_Folds + 1))\r\n  File \"/home/jct/.local/lib/python3.6/site-packages/joblib/parallel.py\", line 1017, in __call__\r\n    self.retrieve()\r\n  File \"/home/jct/.local/lib/python3.6/site-packages/joblib/parallel.py\", line 909, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"/home/jct/.local/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 562, in wrap_future_result\r\n    return future.result(timeout=timeout)\r\n  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/concurrent/futures/_base.py\", line 432, in result\r\n    return self.__get_result()\r\n  File \"/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/concurrent/futures/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #2 'mat1' in call to _th_addmm\r\n```\r\nWhere print statements confirmed to me that all that was passed in `Optimal.py` is pushed `.to(device)` and `device=cuda:0`.\r\njoblib `v0.14.1`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/991", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/991/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/991/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/991/events", "html_url": "https://github.com/joblib/joblib/issues/991", "id": 548525612, "node_id": "MDU6SXNzdWU1NDg1MjU2MTI=", "number": 991, "title": "joblib.load() causes I/O blocking", "user": {"login": "razibchamp", "id": 10421160, "node_id": "MDQ6VXNlcjEwNDIxMTYw", "avatar_url": "https://avatars0.githubusercontent.com/u/10421160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/razibchamp", "html_url": "https://github.com/razibchamp", "followers_url": "https://api.github.com/users/razibchamp/followers", "following_url": "https://api.github.com/users/razibchamp/following{/other_user}", "gists_url": "https://api.github.com/users/razibchamp/gists{/gist_id}", "starred_url": "https://api.github.com/users/razibchamp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/razibchamp/subscriptions", "organizations_url": "https://api.github.com/users/razibchamp/orgs", "repos_url": "https://api.github.com/users/razibchamp/repos", "events_url": "https://api.github.com/users/razibchamp/events{/privacy}", "received_events_url": "https://api.github.com/users/razibchamp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-12T05:53:39Z", "updated_at": "2020-01-13T03:19:16Z", "closed_at": "2020-01-13T03:19:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "Below code snippet is a part of API in production.\r\n\r\n```python\r\nfrom sklearn.externals import joblib\r\nimport numpy as np\r\nmejor =  joblib.load(os.path.join(settings.BASE_DIR +\r\n                                        path + file_name))  \r\np = mejor.predict_proba([text]).ravel()\r\npred = mejor.predict([text])\r\npredpro= np.amax(p)\r\n```\r\n\r\nWhen an API is called in production the model file is locked and it slows down subsequent api response. So it is unable to handle concurrent requests. Model file size is around 150 mb. I have searched through google, stack overflow and also issues in this repo but couldn't find a suitable solution. Could anyone please help me on that ? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/984", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/984/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/984/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/984/events", "html_url": "https://github.com/joblib/joblib/issues/984", "id": 544584241, "node_id": "MDU6SXNzdWU1NDQ1ODQyNDE=", "number": 984, "title": "Wrong number of workers reported by effective_n_jobs", "user": {"login": "glemaitre", "id": 7454015, "node_id": "MDQ6VXNlcjc0NTQwMTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7454015?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glemaitre", "html_url": "https://github.com/glemaitre", "followers_url": "https://api.github.com/users/glemaitre/followers", "following_url": "https://api.github.com/users/glemaitre/following{/other_user}", "gists_url": "https://api.github.com/users/glemaitre/gists{/gist_id}", "starred_url": "https://api.github.com/users/glemaitre/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glemaitre/subscriptions", "organizations_url": "https://api.github.com/users/glemaitre/orgs", "repos_url": "https://api.github.com/users/glemaitre/repos", "events_url": "https://api.github.com/users/glemaitre/events{/privacy}", "received_events_url": "https://api.github.com/users/glemaitre/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2020-01-02T13:44:30Z", "updated_at": "2020-01-06T15:25:10Z", "closed_at": "2020-01-06T15:25:10Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I want to highlight something which looks weird to me (not sure yet if this is a bug). I made a snippet which is a pattern that we use in scikit-learn.\r\n\r\n```python\r\nfrom joblib import Parallel, delayed, effective_n_jobs, parallel_backend\r\nfrom joblib.parallel import get_active_backend\r\n\r\n\r\ndef func(x):\r\n    return x ** 2\r\n\r\n\r\nclass Klass:\r\n    def __init__(self, n_jobs=None):\r\n        self.n_jobs = n_jobs\r\n\r\n    def func(self, x):\r\n        n_jobs = effective_n_jobs(self.n_jobs)\r\n        print(\r\n            f\"Number of effective jobs found: {effective_n_jobs(self.n_jobs)}\"\r\n        )\r\n        backend, backend_n_jobs = get_active_backend()\r\n        print(\r\n            f\"Current backend used: {backend}\"\r\n        )\r\n        print(\r\n            f\"Number of jobs in the backend: {backend_n_jobs}\"\r\n        )\r\n        return Parallel(n_jobs=n_jobs)(delayed(func)(i) for i in x)\r\n\r\n\r\nn_jobs = 4\r\nwith parallel_backend(\"threading\", n_jobs=n_jobs):\r\n    r = Klass().func(range(10))\r\n```\r\n\r\n```\r\nNumber of effective jobs found: 1\r\nCurrent backend used: <joblib._parallel_backends.ThreadingBackend object at 0x7f946468d810>\r\nNumber of jobs in the backend: 4\r\n```\r\n\r\nIf I am not wrong, one would expect to make the `Parallel` call with `n_jobs=4`. However, `effective_n_jobs` will report `n_jobs=1` due to:\r\n\r\nhttps://github.com/joblib/joblib/blob/1753c3bcb1d8430439ce48a1ef4e866166f34296/joblib/_parallel_backends.py#L232-L235\r\n\r\nI would have expected `parallel_backend` to overwrite the number of jobs if not specified in the estimator but I am not sure about it.\r\n\r\nNB: found during investigating https://github.com/scikit-learn/scikit-learn/issues/15978", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/982", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/982/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/982/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/982/events", "html_url": "https://github.com/joblib/joblib/issues/982", "id": 544382126, "node_id": "MDU6SXNzdWU1NDQzODIxMjY=", "number": 982, "title": "Lower prediction speed after Version 0.14.0", "user": {"login": "Zarkxxxxx", "id": 49779428, "node_id": "MDQ6VXNlcjQ5Nzc5NDI4", "avatar_url": "https://avatars0.githubusercontent.com/u/49779428?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zarkxxxxx", "html_url": "https://github.com/Zarkxxxxx", "followers_url": "https://api.github.com/users/Zarkxxxxx/followers", "following_url": "https://api.github.com/users/Zarkxxxxx/following{/other_user}", "gists_url": "https://api.github.com/users/Zarkxxxxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zarkxxxxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zarkxxxxx/subscriptions", "organizations_url": "https://api.github.com/users/Zarkxxxxx/orgs", "repos_url": "https://api.github.com/users/Zarkxxxxx/repos", "events_url": "https://api.github.com/users/Zarkxxxxx/events{/privacy}", "received_events_url": "https://api.github.com/users/Zarkxxxxx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-01-01T17:48:56Z", "updated_at": "2020-01-17T10:40:07Z", "closed_at": "2020-01-17T10:40:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi there.i found that the model predict speed will getting lower after i update joblib version from 0.13.0 to 0.14.0.i have tried several time between the version from 0.13.0 to 0.14.0,but get the same result. \r\n\r\nthe model used Avg 0.01s to predict the reult before version 0.14.0,but used Avg 0.10s  after version 0.14.0.(i have tried several different model and the same result.btw,the model all training in version 0.13)\r\n\r\nanother change is that, i will get a userwarning \r\n\" UserWarning: Loky-backed parallel loops cannot be nested below threads, setting n_jobs=1\r\n  n_jobs = effective_n_jobs(self.n_jobs)\"\r\nbefore version 0.14.0 when i ask model predicting.this warning seem be repaired after version 0.14.0.\r\nDoes this revision affect the prediction speed\uff1f: )\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/978", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/978/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/978/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/978/events", "html_url": "https://github.com/joblib/joblib/issues/978", "id": 538311583, "node_id": "MDU6SXNzdWU1MzgzMTE1ODM=", "number": 978, "title": "Loky-backed parallel loops cannot be nested below threads, setting n_jobs=1", "user": {"login": "jl-hk", "id": 52725685, "node_id": "MDQ6VXNlcjUyNzI1Njg1", "avatar_url": "https://avatars1.githubusercontent.com/u/52725685?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jl-hk", "html_url": "https://github.com/jl-hk", "followers_url": "https://api.github.com/users/jl-hk/followers", "following_url": "https://api.github.com/users/jl-hk/following{/other_user}", "gists_url": "https://api.github.com/users/jl-hk/gists{/gist_id}", "starred_url": "https://api.github.com/users/jl-hk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jl-hk/subscriptions", "organizations_url": "https://api.github.com/users/jl-hk/orgs", "repos_url": "https://api.github.com/users/jl-hk/repos", "events_url": "https://api.github.com/users/jl-hk/events{/privacy}", "received_events_url": "https://api.github.com/users/jl-hk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-12-16T10:11:18Z", "updated_at": "2020-02-10T18:42:40Z", "closed_at": "2019-12-24T04:52:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am getting 'Loky-backed parallel loops cannot be nested below threads, setting n_jobs=1\r\n  **self._backend_args)' when loky backend is placed inside a threading backend task. May I know if it is possible to do so?\r\n\r\nI am running joblib 0.13.2 in python3.7 with macOS.\r\n\r\nNot sure if this is the same as #180 . Since the error message is slightly different from #180's, I am creating a new issue here.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/977", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/977/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/977/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/977/events", "html_url": "https://github.com/joblib/joblib/issues/977", "id": 537319395, "node_id": "MDU6SXNzdWU1MzczMTkzOTU=", "number": 977, "title": "joblib.load with evil data will cause command execution", "user": {"login": "Cossack9989", "id": 32336251, "node_id": "MDQ6VXNlcjMyMzM2MjUx", "avatar_url": "https://avatars2.githubusercontent.com/u/32336251?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cossack9989", "html_url": "https://github.com/Cossack9989", "followers_url": "https://api.github.com/users/Cossack9989/followers", "following_url": "https://api.github.com/users/Cossack9989/following{/other_user}", "gists_url": "https://api.github.com/users/Cossack9989/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cossack9989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cossack9989/subscriptions", "organizations_url": "https://api.github.com/users/Cossack9989/orgs", "repos_url": "https://api.github.com/users/Cossack9989/repos", "events_url": "https://api.github.com/users/Cossack9989/events{/privacy}", "received_events_url": "https://api.github.com/users/Cossack9989/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-12-13T02:49:18Z", "updated_at": "2019-12-13T11:35:03Z", "closed_at": "2019-12-13T09:22:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "## Reproducing code example:\r\n- `joblib.load` function with evil data will cause command execution due to `pickle.load` in `joblib/numpy_pickle.py`, if attack share evil data on internet, when user load it , it will cause command execution.\r\n- for example, some online MachinLearning website using joblib may allow user to submit model. If evil model submmited, the website may suffer.\r\n```python\r\nimport joblib\r\nimport pickle\r\nclass A(object):\r\n    def __reduce__(self):\r\n        return eval,(\"__import__('os').system('calc.exe')\",)\r\nx = [3.14]\r\nx.append(A())\r\nres = pickle.dumps(x)\r\nopen(\"test.pkl\",\"wb\").write(res)\r\njoblib.load(\"test.pkl\")\r\n```\r\n## version\r\n<=0.14.1", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/967", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/967/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/967/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/967/events", "html_url": "https://github.com/joblib/joblib/issues/967", "id": 534034367, "node_id": "MDU6SXNzdWU1MzQwMzQzNjc=", "number": 967, "title": "Performance regression in joblib 0.14.0", "user": {"login": "ogrisel", "id": 89061, "node_id": "MDQ6VXNlcjg5MDYx", "avatar_url": "https://avatars0.githubusercontent.com/u/89061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ogrisel", "html_url": "https://github.com/ogrisel", "followers_url": "https://api.github.com/users/ogrisel/followers", "following_url": "https://api.github.com/users/ogrisel/following{/other_user}", "gists_url": "https://api.github.com/users/ogrisel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ogrisel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ogrisel/subscriptions", "organizations_url": "https://api.github.com/users/ogrisel/orgs", "repos_url": "https://api.github.com/users/ogrisel/repos", "events_url": "https://api.github.com/users/ogrisel/events{/privacy}", "received_events_url": "https://api.github.com/users/ogrisel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1718808083, "node_id": "MDU6TGFiZWwxNzE4ODA4MDgz", "url": "https://api.github.com/repos/joblib/joblib/labels/high%20priority", "name": "high priority", "color": "d93f0b", "default": false, "description": ""}, {"id": 1718801131, "node_id": "MDU6TGFiZWwxNzE4ODAxMTMx", "url": "https://api.github.com/repos/joblib/joblib/labels/performance", "name": "performance", "color": "0e8a16", "default": false, "description": ""}, {"id": 1718801259, "node_id": "MDU6TGFiZWwxNzE4ODAxMjU5", "url": "https://api.github.com/repos/joblib/joblib/labels/regression", "name": "regression", "color": "c600a5", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-12-06T14:36:26Z", "updated_at": "2019-12-09T20:38:48Z", "closed_at": "2019-12-09T20:38:48Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "As initially reported in scikit-learn/scikit-learn#15128\r\n\r\n```python\r\nfrom timeit import timeit \r\nfrom joblib import Parallel, delayed \r\nimport numpy as np \r\n \r\n \r\ndef test(n_jobs): \r\n    x = np.array(range(10)) \r\n    Parallel(n_jobs=n_jobs)(delayed(np.sum)(x) for i in range(10)) \r\n \r\n \r\nif __name__ == '__main__': \r\n    for n_jobs in [1, 2, 4, 8]: \r\n        test(n_jobs=n_jobs)  # pre-warm workers \r\n        print(n_jobs, '%.3f' % (timeit(lambda: test(n_jobs=n_jobs), number=10) / 10))\r\n```\r\n\r\n## joblib 0.13.2:\r\n```\r\n1 0.000\r\n2 0.004\r\n4 0.015\r\n8 0.027\r\n```\r\n\r\n## joblib 0.14.0\r\n``` \r\n1 0.001\r\n2 0.360\r\n4 0.477\r\n8 0.939\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/965", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/965/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/965/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/965/events", "html_url": "https://github.com/joblib/joblib/issues/965", "id": 532983938, "node_id": "MDU6SXNzdWU1MzI5ODM5Mzg=", "number": 965, "title": "Revert #847 ?", "user": {"login": "cgohlke", "id": 483428, "node_id": "MDQ6VXNlcjQ4MzQyOA==", "avatar_url": "https://avatars3.githubusercontent.com/u/483428?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cgohlke", "html_url": "https://github.com/cgohlke", "followers_url": "https://api.github.com/users/cgohlke/followers", "following_url": "https://api.github.com/users/cgohlke/following{/other_user}", "gists_url": "https://api.github.com/users/cgohlke/gists{/gist_id}", "starred_url": "https://api.github.com/users/cgohlke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cgohlke/subscriptions", "organizations_url": "https://api.github.com/users/cgohlke/orgs", "repos_url": "https://api.github.com/users/cgohlke/repos", "events_url": "https://api.github.com/users/cgohlke/events{/privacy}", "received_events_url": "https://api.github.com/users/cgohlke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1718808083, "node_id": "MDU6TGFiZWwxNzE4ODA4MDgz", "url": "https://api.github.com/repos/joblib/joblib/labels/high%20priority", "name": "high priority", "color": "d93f0b", "default": false, "description": ""}, {"id": 1718801131, "node_id": "MDU6TGFiZWwxNzE4ODAxMTMx", "url": "https://api.github.com/repos/joblib/joblib/labels/performance", "name": "performance", "color": "0e8a16", "default": false, "description": ""}, {"id": 1718801259, "node_id": "MDU6TGFiZWwxNzE4ODAxMjU5", "url": "https://api.github.com/repos/joblib/joblib/labels/regression", "name": "regression", "color": "c600a5", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-12-04T22:49:52Z", "updated_at": "2019-12-10T08:21:19Z", "closed_at": "2019-12-10T08:04:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "Please consider reverting #847  (FIX python-lz4 version comparison)\r\n\r\nImporting `pkg_resources` is known to be excruciatingly slow. See https://github.com/pypa/setuptools/issues/510\r\n\r\nOn my Windows workstation, `import pkg_resources` takes more than 3 s.\r\n\r\nAlso, I can't reproduce that `The python-lz4 package has its version string prefixed with \"v\"`:\r\n```\r\n>>> import lz4\r\n>>> lz4.__version__\r\n'2.2.1'\r\n>>> from distutils.version import LooseVersion\r\n>>> LooseVersion(lz4.__version__)\r\nLooseVersion ('2.2.1')\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/959", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/959/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/959/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/959/events", "html_url": "https://github.com/joblib/joblib/issues/959", "id": 519370234, "node_id": "MDU6SXNzdWU1MTkzNzAyMzQ=", "number": 959, "title": "Failures with nested joblib parallel and dask", "user": {"login": "mfeurer", "id": 5320498, "node_id": "MDQ6VXNlcjUzMjA0OTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/5320498?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfeurer", "html_url": "https://github.com/mfeurer", "followers_url": "https://api.github.com/users/mfeurer/followers", "following_url": "https://api.github.com/users/mfeurer/following{/other_user}", "gists_url": "https://api.github.com/users/mfeurer/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfeurer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfeurer/subscriptions", "organizations_url": "https://api.github.com/users/mfeurer/orgs", "repos_url": "https://api.github.com/users/mfeurer/repos", "events_url": "https://api.github.com/users/mfeurer/events{/privacy}", "received_events_url": "https://api.github.com/users/mfeurer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}, {"id": 1718506772, "node_id": "MDU6TGFiZWwxNzE4NTA2Nzcy", "url": "https://api.github.com/repos/joblib/joblib/labels/dask%20backend", "name": "dask backend", "color": "0e10af", "default": false, "description": ""}, {"id": 1718808083, "node_id": "MDU6TGFiZWwxNzE4ODA4MDgz", "url": "https://api.github.com/repos/joblib/joblib/labels/high%20priority", "name": "high priority", "color": "d93f0b", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-11-07T16:20:25Z", "updated_at": "2020-07-01T15:59:08Z", "closed_at": "2020-07-01T15:59:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm having the issue that something cancels scikit-learn jobs if parallel is nested together with the dask backend. (Simplified) code to reproduce:\r\n\r\n```python\r\nfrom dask.distributed import Client\r\nfrom joblib import parallel_backend\r\n\r\nfrom sklearn.datasets import fetch_openml\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\n\r\nmodel = GridSearchCV(\r\n    estimator=Pipeline(\r\n        steps=[\r\n            (\r\n                'Preprocessing', ColumnTransformer(transformers=[\r\n                    ('NumericalPreprocessing', StandardScaler(), [0, 1, 2, 3])\r\n                ]),\r\n            ),\r\n            ('Estimator', DecisionTreeClassifier())\r\n        ],\r\n    ),\r\n    param_grid={'Estimator__min_samples_split': list(range(2, 101))},\r\n)\r\n\r\nif __name__ == '__main__':\r\n    X, y = fetch_openml(return_X_y=True, data_id=61)\r\n    client = Client()\r\n    with parallel_backend('dask'):\r\n        model.fit(X, y)\r\n```\r\nOutput:\r\n```\r\n/home/feurerm/miniconda/3-4.5.4/envs/openml/bin/python3 /home/feurerm/sync_dir/projects/openml/Study-14/PythonV2/bug.py\r\n/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\r\n  warnings.warn(CV_WARNING, FutureWarning)\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f75a6bc62f0> of <Future: status: cancelled, key: _transform_one-batch-90ed01946f8a4adf8e253d7e6b47769e>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _transform_one-batch-90ed01946f8a4adf8e253d7e6b47769e\r\ndistributed.worker - WARNING -  Compute Failed\r\nFunction:  <joblib._dask.Batch object at 0x7f75a6bc0f60>\r\nargs:      (array([[5.1, 3.5, 1.4, 0.2],\r\n       [4.9, 3. , 1.4, 0.2],\r\n       [4.7, 3.2, 1.3, 0.2],\r\n       [4.6, 3.1, 1.5, 0.2],\r\n       [5. , 3.6, 1.4, 0.2],\r\n       [5.4, 3.9, 1.7, 0.4],\r\n       [4.6, 3.4, 1.4, 0.3],\r\n       [5. , 3.4, 1.5, 0.2],\r\n       [4.4, 2.9, 1.4, 0.2],\r\n       [4.9, 3.1, 1.5, 0.1],\r\n       [5.4, 3.7, 1.5, 0.2],\r\n       [4.8, 3.4, 1.6, 0.2],\r\n       [4.8, 3. , 1.4, 0.1],\r\n       [4.3, 3. , 1.1, 0.1],\r\n       [5.8, 4. , 1.2, 0.2],\r\n       [5.7, 4.4, 1.5, 0.4],\r\n       [5.4, 3.9, 1.3, 0.4],\r\n       [5.1, 3.5, 1.4, 0.3],\r\n       [5.7, 3.8, 1.7, 0.3],\r\n       [5.1, 3.8, 1.5, 0.3],\r\n       [5.4, 3.4, 1.7, 0.2],\r\n       [5.1, 3.7, 1.5, 0.4],\r\n       [4.6, 3.6, 1. , 0.2],\r\n       [5.1, 3.3, 1.7, 0.5],\r\n       [4.8, 3.4, 1.9, 0.2],\r\n       [5. , 3. , 1.6, 0.2],\r\n       [5. , 3.4, 1.6, 0.4],\r\n       [5.2, 3.5, 1.5, 0.2],\r\n       [5.2, 3.4, 1.4, 0.2],\r\n       [4.7, 3.2, 1.6, 0.2],\r\n       [4.8, 3.1, 1.6, 0.2],\r\n       [5.4, 3.4, 1.5, 0.4],\r\n       [5.2, 4.1, 1.5, 0.1],\r\n       [5.5, 4.2, 1.4, 0.2],\r\n       [4.9, \r\nkwargs:    {}\r\nException: CancelledError('_transform_one-batch-90ed01946f8a4adf8e253d7e6b47769e')\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/sync_dir/projects/openml/Study-14/PythonV2/bug.py\", line 30, in <module>\r\n    model.fit(X, y)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 688, in fit\r\n    self._run_search(evaluate_candidates)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 1149, in _run_search\r\n    evaluate_candidates(ParameterGrid(self.param_grid))\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/model_selection/_search.py\", line 667, in evaluate_candidates\r\n    cv.split(X, y, groups)))\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/parallel.py\", line 1016, in __call__\r\n    self.retrieve()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/parallel.py\", line 910, in retrieve\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec16f2bf8> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-6e3e3af2d5904839af62c37e3c2a1747>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nTypeError: exceptions must derive from BaseException\r\n    self._output.extend(job.get())\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 270, in get\r\n    return ref().result()\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec1940d90> of <Future: status: cancelled, key: _fit_and_score-batch-74bc4528b20d42cbb9cb8ddff63be4b8>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-74bc4528b20d42cbb9cb8ddff63be4b8\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 222, in result\r\n    raise exc.with_traceback(tb)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 106, in __call__\r\n    results.append(func(*args, **kwargs))\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 556, in _fit_and_score\r\n    test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 599, in _score\r\n    return _multimetric_score(estimator, X_test, y_test, scorer)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 629, in _multimetric_score\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec1709d90> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-d81e459278c6477c912644049c4b49c4>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-d81e459278c6477c912644049c4b49c4\r\n    score = scorer(estimator, X_test, y_test)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/metrics/scorer.py\", line 240, in _passthrough_scorer\r\n    return estimator.score(*args, **kwargs)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 116, in <lambda>\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec17d2048> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-a5b86ba5f9034ec7b1df639f505bde19>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-a5b86ba5f9034ec7b1df639f505bde19\r\n    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/pipeline.py\", line 598, in score\r\n    Xt = transform.transform(Xt)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\", line 539, in transform\r\n    Xs = self._fit_transform(X, None, _transform_one, fitted=True)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py\", line 420, in _fit_transform\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec1a12488> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-65544b059a8541b783c642e956f21d39>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-65544b059a8541b783c642e956f21d39\r\n    self._iter(fitted=fitted, replace_strings=True), 1))\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec16f2840> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-83f642a7b46a43bdb565ce09f961d6dd>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-83f642a7b46a43bdb565ce09f961d6dd\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/parallel.py\", line 1016, in __call__\r\n    self.retrieve()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/parallel.py\", line 910, in retrieve\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec1706510> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-67329437248541408258b10b4bdad1b7>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-67329437248541408258b10b4bdad1b7\r\n    self._output.extend(job.get())\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 270, in get\r\n    distributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec1633378> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-c98f8b79e32946e780f5bbdb204a2492>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-c98f8b79e32946e780f5bbdb204a2492\r\nreturn ref().result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _transform_one-batch-90ed01946f8a4adf8e253d7e6b47769e\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec1898840> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-b1e2e00cca37484489aa60c6605a757b>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-b1e2e00cca37484489aa60c6605a757b\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec18987b8> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-2a90e3e4814e4806b8b6929103c7fc08>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-2a90e3e4814e4806b8b6929103c7fc08\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec184c158> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-32640bac340943f5a94d1d0b4c68ed68>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-32640bac340943f5a94d1d0b4c68ed68\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec17f3ae8> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-9dab314edb5f4e6fa8e455a7ef21c347>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-9dab314edb5f4e6fa8e455a7ef21c347\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec17d2a60> of <Future: status: cancelled, type: list, key: _fit_and_score-batch-23de936dd0c340beb2be4a1ab3681ec3>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-23de936dd0c340beb2be4a1ab3681ec3\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec17f3268> of <Future: status: cancelled, key: _fit_and_score-batch-33c922e812ec4b7d8931ec7cc8c195e4>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-33c922e812ec4b7d8931ec7cc8c195e4\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec185f488> of <Future: status: cancelled, key: _fit_and_score-batch-20ba616a25ae498c8bfae09df8cdee90>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-20ba616a25ae498c8bfae09df8cdee90\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7f0ec1709488> of <Future: status: cancelled, key: _fit_and_score-batch-7e7e0d44916640fc95ed2d49c623f95d>:\r\nTraceback (most recent call last):\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/home/feurerm/miniconda/3-4.5.4/envs/openml/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: _fit_and_score-batch-7e7e0d44916640fc95ed2d49c623f95d\r\n\r\nProcess finished with exit code 1\r\n```\r\nSome more notes:\r\n\r\n* Unfortunately, I cannot reproduce the issue without scikit-learn\r\n* Obviously, the column transformer would include multiple parallel pipelines. I removed them for simplicity\r\n* I believe this issue is different to #957 as I use the default cluster (and not a SLURMCluster), and I use nested calls to joblib\r\n* The code works with the `loky`, `multiprocessing` and `threading` backend.\r\n* It's possible to fix this by monkey-patching the column transformer as shown below\r\n\r\n```python\r\n# Sequential column transformer which replaces the call to Parallel by a list comprehension\r\nclass SequentialColumnTransformer(ColumnTransformer):\r\n\r\n    def _fit_transform(self, X, y, func, fitted=False):\r\n\r\n        transformers = list(self._iter(fitted=fitted, replace_strings=True))\r\n        return [\r\n            func(\r\n                transformer=clone(trans) if not fitted else trans,\r\n                X=_get_column(X, column),\r\n                y=y,\r\n                weight=weight,\r\n                message_clsname='ColumnTransformer',\r\n                message=self._log_message(name, idx, len(transformers)))\r\n            for idx, (name, trans, column, weight)\r\n            in enumerate(self._iter(fitted=fitted, replace_strings=True), 1)\r\n        ]\r\n\r\n\r\n# Use a different backend in the inner call to parallel\r\nclass LokyColumnTransformer(ColumnTransformer):\r\n    def _fit_transform(self, X, y, func, fitted=False):\r\n        with parallel_backend('loky'):\r\n            return super()._fit_transform(X, y, func, fitted)\r\n```\r\n\r\nCC @amueller ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/957", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/957/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/957/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/957/events", "html_url": "https://github.com/joblib/joblib/issues/957", "id": 517899100, "node_id": "MDU6SXNzdWU1MTc4OTkxMDA=", "number": 957, "title": "With large number of short jobs joblib fails with CancelledError", "user": {"login": "hugorichard", "id": 9814422, "node_id": "MDQ6VXNlcjk4MTQ0MjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/9814422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hugorichard", "html_url": "https://github.com/hugorichard", "followers_url": "https://api.github.com/users/hugorichard/followers", "following_url": "https://api.github.com/users/hugorichard/following{/other_user}", "gists_url": "https://api.github.com/users/hugorichard/gists{/gist_id}", "starred_url": "https://api.github.com/users/hugorichard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hugorichard/subscriptions", "organizations_url": "https://api.github.com/users/hugorichard/orgs", "repos_url": "https://api.github.com/users/hugorichard/repos", "events_url": "https://api.github.com/users/hugorichard/events{/privacy}", "received_events_url": "https://api.github.com/users/hugorichard/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}, {"id": 1718506772, "node_id": "MDU6TGFiZWwxNzE4NTA2Nzcy", "url": "https://api.github.com/repos/joblib/joblib/labels/dask%20backend", "name": "dask backend", "color": "0e10af", "default": false, "description": ""}, {"id": 1718808083, "node_id": "MDU6TGFiZWwxNzE4ODA4MDgz", "url": "https://api.github.com/repos/joblib/joblib/labels/high%20priority", "name": "high priority", "color": "d93f0b", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-11-05T16:59:03Z", "updated_at": "2020-05-13T04:18:04Z", "closed_at": "2020-05-13T04:18:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "If I run this code\r\n\r\n```python\r\n    import time\r\n    from joblib import Parallel, delayed, load\r\n    from dask_jobqueue import SLURMCluster\r\n    from dask.distributed import Client\r\n    from joblib import parallel_backend\r\n    cluster = SLURMCluster(diagnostics_port=1243)\r\n    cluster.scale(4)\r\n    client = Client(cluster)  # registers as the default \"dask\" backend \r\n    parallel_backend('dask')\r\n    time.sleep(10)  # wait for the cluster to be up and running\r\n    print(\"cluster setup\")\r\n\r\n\r\n    def do_launch_classification():\r\n        def do_launch_classification_study():\r\n            time.sleep(0.1)\r\n\r\n        Parallel(n_jobs=-1,\r\n             verbose=1000)(delayed(do_launch_classification_study)()\r\n                           for study_path in range(34)\r\n                           for model, name in enumerate(range(24))\r\n                           for i_split, split in enumerate(range(5)))\r\n    do_launch_classification()\r\n```\r\n\r\nI get this error\r\n\r\n```python-traceback\r\ndistributed.client - ERROR - Error in callback <function DaskDistributedBackend.apply_async.<locals>.callback_wrapper at 0x7fc813d945f0> of <Future: status: cancelled, type: list, key: do_launch_classification_study-batch-234b27edb1e0493f90035dc2c1de8c61>:\r\nTraceback (most recent call last):\r\n  File \"/scratch/hrichard/venvs/miniconda3/lib/python3.7/site-packages/distributed/client.py\", line 287, in execute_callback\r\n    fn(fut)\r\n  File \"/home/parietal/hrichard/joblib/joblib/_dask.py\", line 260, in callback_wrapper\r\n    result = future.result()\r\n  File \"/scratch/hrichard/venvs/miniconda3/lib/python3.7/site-packages/distributed/client.py\", line 224, in result\r\n    raise result\r\nconcurrent.futures._base.CancelledError: do_launch_classification_study-batch-234b27edb1e0493f90035dc2c1de8c61\r\n```\r\n\r\nMy jobqueue.yaml:\r\n\r\n```yaml\r\n    jobqueue:\r\n\r\n        slurm:\r\n\r\n            name: dask-worker\r\n\r\n            cores: 40\r\n\r\n            memory: 250GB\r\n\r\n            processes: 2\r\n\r\n            local-directory: /scratch/hrichard/dask \r\n\r\n            log-directory: /scratch/hrichard/dask/log \r\n\r\n            death-timeout: 30\r\n```\r\n\r\nInterestingly if I set time.sleep(1) instead of time.sleep(0.1) everything works fine.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/953", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/953/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/953/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/953/events", "html_url": "https://github.com/joblib/joblib/issues/953", "id": 512503220, "node_id": "MDU6SXNzdWU1MTI1MDMyMjA=", "number": 953, "title": "test_weak_array_key_map failure on Windows CI", "user": {"login": "ogrisel", "id": 89061, "node_id": "MDQ6VXNlcjg5MDYx", "avatar_url": "https://avatars0.githubusercontent.com/u/89061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ogrisel", "html_url": "https://github.com/ogrisel", "followers_url": "https://api.github.com/users/ogrisel/followers", "following_url": "https://api.github.com/users/ogrisel/following{/other_user}", "gists_url": "https://api.github.com/users/ogrisel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ogrisel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ogrisel/subscriptions", "organizations_url": "https://api.github.com/users/ogrisel/orgs", "repos_url": "https://api.github.com/users/ogrisel/repos", "events_url": "https://api.github.com/users/ogrisel/events{/privacy}", "received_events_url": "https://api.github.com/users/ogrisel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-25T13:00:36Z", "updated_at": "2019-10-25T16:32:01Z", "closed_at": "2019-10-25T16:32:01Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```\r\n___________________________ test_weak_array_key_map ___________________________\r\n    @with_numpy\r\n    def test_weak_array_key_map():\r\n    \r\n        def assert_empty_after_gc_collect(container, retries=3):\r\n            for i in range(retries):\r\n                if len(container) == 0:\r\n                    return\r\n                gc.collect()\r\n            assert len(container) == 0\r\n    \r\n        a = np.ones(42)\r\n        m = _WeakArrayKeyMap()\r\n        m.set(a, 'a')\r\n        assert m.get(a) == 'a'\r\n    \r\n        b = a\r\n        assert m.get(b) == 'a'\r\n        m.set(b, 'b')\r\n        assert m.get(a) == 'b'\r\n    \r\n        del a\r\n        gc.collect()\r\n        assert len(m._data) == 1\r\n        assert m.get(b) == 'b'\r\n    \r\n        del b\r\n        assert_empty_after_gc_collect(m._data)\r\n    \r\n        c = np.ones(42)\r\n        m.set(c, 'c')\r\n        assert len(m._data) == 1\r\n        assert m.get(c) == 'c'\r\n    \r\n        with raises(KeyError):\r\n            m.get(np.ones(42))\r\n    \r\n        del c\r\n>       assert_empty_after_gc_collect(m._data)\r\nassert_empty_after_gc_collect = <function assert_empty_after_gc_collect at 0x0887CCF0>\r\nm          = <joblib._memmapping_reducer._WeakArrayKeyMap instance at 0x08A250F8>\r\njoblib\\test\\test_memmapping.py:610: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\ncontainer = {}, retries = 3\r\n    def assert_empty_after_gc_collect(container, retries=3):\r\n        for i in range(retries):\r\n            if len(container) == 0:\r\n                return\r\n            gc.collect()\r\n>       assert len(container) == 0\r\nE       assert 1 == 0\r\nE         -1\r\nE         +0\r\ncontainer  = {}\r\ni          = 2\r\nretries    = 3\r\njoblib\\test\\test_memmapping.py:581: AssertionError\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/947", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/947/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/947/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/947/events", "html_url": "https://github.com/joblib/joblib/issues/947", "id": 504790585, "node_id": "MDU6SXNzdWU1MDQ3OTA1ODU=", "number": 947, "title": "[Parallel(n_jobs=5)]: Using backend SequentialBackend with 1 concurrent workers.", "user": {"login": "omarcr", "id": 6231285, "node_id": "MDQ6VXNlcjYyMzEyODU=", "avatar_url": "https://avatars2.githubusercontent.com/u/6231285?v=4", "gravatar_id": "", "url": "https://api.github.com/users/omarcr", "html_url": "https://github.com/omarcr", "followers_url": "https://api.github.com/users/omarcr/followers", "following_url": "https://api.github.com/users/omarcr/following{/other_user}", "gists_url": "https://api.github.com/users/omarcr/gists{/gist_id}", "starred_url": "https://api.github.com/users/omarcr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/omarcr/subscriptions", "organizations_url": "https://api.github.com/users/omarcr/orgs", "repos_url": "https://api.github.com/users/omarcr/repos", "events_url": "https://api.github.com/users/omarcr/events{/privacy}", "received_events_url": "https://api.github.com/users/omarcr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-10-09T17:30:32Z", "updated_at": "2019-10-14T15:14:37Z", "closed_at": "2019-10-14T15:13:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I put nested parallel jobs I get the following message:\r\n`[Parallel(n_jobs=5)]: Using backend SequentialBackend with 1 concurrent workers.`\r\n\r\nWhich does not allow me to run some part of the code in parallel, how can I fix it?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/946", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/946/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/946/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/946/events", "html_url": "https://github.com/joblib/joblib/issues/946", "id": 503846935, "node_id": "MDU6SXNzdWU1MDM4NDY5MzU=", "number": 946, "title": "can't pickle DUFunc objects", "user": {"login": "isaac-you", "id": 7335320, "node_id": "MDQ6VXNlcjczMzUzMjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7335320?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isaac-you", "html_url": "https://github.com/isaac-you", "followers_url": "https://api.github.com/users/isaac-you/followers", "following_url": "https://api.github.com/users/isaac-you/following{/other_user}", "gists_url": "https://api.github.com/users/isaac-you/gists{/gist_id}", "starred_url": "https://api.github.com/users/isaac-you/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isaac-you/subscriptions", "organizations_url": "https://api.github.com/users/isaac-you/orgs", "repos_url": "https://api.github.com/users/isaac-you/repos", "events_url": "https://api.github.com/users/isaac-you/events{/privacy}", "received_events_url": "https://api.github.com/users/isaac-you/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-10-08T06:06:35Z", "updated_at": "2019-10-17T12:20:10Z", "closed_at": "2019-10-17T12:20:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "```python\r\n@vectorize([float64(float64, float64)])\r\ndef powerV(x1,x2):\r\n\t\r\n\treturn x1 ** x2\r\n\r\n@jit(nopython=True)\r\ndef power_mat(m1,m2):\r\n\r\n\tsize1 = m1.size\r\n\tsize2 = m2.size\r\n\r\n\tif size1 == size2:\t\t#m1 and m2 are both matrix with same shape or both constant \r\n\t\t\r\n\t\tretMat = powerV(m1,m2)\r\n\t\treturn retMat\r\n\r\n\telif size1 == 1 or size2 == 1:\t\t\r\n\r\n\t\tretMat = powerV(m1,m2)\r\n\t\treturn retMat\r\n\r\n\telse:\r\n\t\trowNum1 = m1.shape[0]\r\n\t\trowNum2 = m2.shape[0]\r\n\r\n\t\tif rowNum1 < rowNum2:\r\n\t\t\tm2 = m2[-rowNum1:]\r\n\t\telif rowNum1 > rowNum2:\r\n\t\t\tm1 = m1[-rowNum2:]\r\n\r\n\t\tretMat = powerV(m1,m2)\r\n\t\treturn retMat\r\n```\r\n\r\nthe function 'power_mat' cannot pickle DUFunc objects when using  the following to parallel \r\n```\r\nParallel(n_jobs=n_jobs, backend=\"multiprocessing\")(delayed(evalFitTensor)(gp,stockTensor2Eval_,targetMatrix,stockNum,sizeN,fitFunc,windowR,expr_i) for expr_i in gp.evalstr)\r\n```\r\nhow can I fix the problem ??? Thank you.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/932", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/932/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/932/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/932/events", "html_url": "https://github.com/joblib/joblib/issues/932", "id": 490314448, "node_id": "MDU6SXNzdWU0OTAzMTQ0NDg=", "number": 932, "title": "Ability to spawn parallel jobs from a child thread", "user": {"login": "kitchoi", "id": 3673984, "node_id": "MDQ6VXNlcjM2NzM5ODQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/3673984?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kitchoi", "html_url": "https://github.com/kitchoi", "followers_url": "https://api.github.com/users/kitchoi/followers", "following_url": "https://api.github.com/users/kitchoi/following{/other_user}", "gists_url": "https://api.github.com/users/kitchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/kitchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kitchoi/subscriptions", "organizations_url": "https://api.github.com/users/kitchoi/orgs", "repos_url": "https://api.github.com/users/kitchoi/repos", "events_url": "https://api.github.com/users/kitchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/kitchoi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-09-06T13:00:03Z", "updated_at": "2019-09-09T06:23:26Z", "closed_at": "2019-09-08T18:07:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "`loky` and `multiprocessing` backend have the following logic which prevent parallelizing from a child thread:\r\nhttps://github.com/joblib/joblib/blob/fd585ce36faba338fa1c71949cf29ed8e8f5ea19/joblib/_parallel_backends.py#L412-L419\r\n\r\nThat causes `SequentialBackend` to be used as a fallback. In use cases where the jobs to be parallelized are spawned from distributed framework such as `dask.distributed` (i have this use case), or just an application trying to do things asynchronously, `Parallel` could be called from a child thread. \r\n\r\nFrom offline discussion with @pierreglaser, this check is to prevent over subscriptions.\r\nOne possibility is to allow this restriction to be switched off at the user's discretion?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/931", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/931/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/931/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/931/events", "html_url": "https://github.com/joblib/joblib/issues/931", "id": 490168517, "node_id": "MDU6SXNzdWU0OTAxNjg1MTc=", "number": 931, "title": "DeprecationWarning: defusedxml.lxml is no longer supported ...", "user": {"login": "mhooreman", "id": 7373718, "node_id": "MDQ6VXNlcjczNzM3MTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7373718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mhooreman", "html_url": "https://github.com/mhooreman", "followers_url": "https://api.github.com/users/mhooreman/followers", "following_url": "https://api.github.com/users/mhooreman/following{/other_user}", "gists_url": "https://api.github.com/users/mhooreman/gists{/gist_id}", "starred_url": "https://api.github.com/users/mhooreman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mhooreman/subscriptions", "organizations_url": "https://api.github.com/users/mhooreman/orgs", "repos_url": "https://api.github.com/users/mhooreman/repos", "events_url": "https://api.github.com/users/mhooreman/events{/privacy}", "received_events_url": "https://api.github.com/users/mhooreman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-06T07:11:37Z", "updated_at": "2019-09-06T07:20:47Z", "closed_at": "2019-09-06T07:15:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI've just got the following deprecation warning while using joblib Parallel:\r\n\r\n```python\r\n\t  File \"{virtual_env_path}/lib/python3.7/site-packages/joblib/parallel.py\", line 934, in __call__\r\n\t    self.retrieve()\r\n\t  File \"{virtual_env_path}/lib/python3.7/site-packages/joblib/parallel.py\", line 833, in retrieve\r\n\t    self._output.extend(job.get(timeout=self.timeout))\r\n\t  File \"{virtual_env_path}/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 521, in wrap_future_result\r\n\t    return future.result(timeout=timeout)\r\n\t  File \"/opt/python/3.7.4/lib/python3.7/concurrent/futures/_base.py\", line 428, in result\r\n\t    return self.__get_result()\r\n\t  File \"/opt/python/3.7.4/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\r\n\t    raise self._exception\r\n\tDeprecationWarning: defusedxml.lxml is no longer supported and will be removed in a future release.\r\n```\r\n\r\nThanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/929", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/929/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/929/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/929/events", "html_url": "https://github.com/joblib/joblib/issues/929", "id": 487295826, "node_id": "MDU6SXNzdWU0ODcyOTU4MjY=", "number": 929, "title": "JOBLIB_MULTIPROCESSING does not exist in documentation", "user": {"login": "Arfey", "id": 10367100, "node_id": "MDQ6VXNlcjEwMzY3MTAw", "avatar_url": "https://avatars3.githubusercontent.com/u/10367100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Arfey", "html_url": "https://github.com/Arfey", "followers_url": "https://api.github.com/users/Arfey/followers", "following_url": "https://api.github.com/users/Arfey/following{/other_user}", "gists_url": "https://api.github.com/users/Arfey/gists{/gist_id}", "starred_url": "https://api.github.com/users/Arfey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Arfey/subscriptions", "organizations_url": "https://api.github.com/users/Arfey/orgs", "repos_url": "https://api.github.com/users/Arfey/repos", "events_url": "https://api.github.com/users/Arfey/events{/privacy}", "received_events_url": "https://api.github.com/users/Arfey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-30T05:38:05Z", "updated_at": "2019-08-30T06:05:03Z", "closed_at": "2019-08-30T06:05:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "I spent a lot of time to try understand why `celery` generate zombie processes. As result I find that `joblib` initialize semaphore and after set `JOBLIB_MULTIPROCESSING` to `0` all work correct.\r\n\r\nI think that add some description about `JOBLIB_MULTIPROCESSING `  to documentation is good idea.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/927", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/927/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/927/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/927/events", "html_url": "https://github.com/joblib/joblib/issues/927", "id": 484928568, "node_id": "MDU6SXNzdWU0ODQ5Mjg1Njg=", "number": 927, "title": "Tests errors on Python 3.8 (GIT master)", "user": {"login": "eclipseo", "id": 30413512, "node_id": "MDQ6VXNlcjMwNDEzNTEy", "avatar_url": "https://avatars0.githubusercontent.com/u/30413512?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eclipseo", "html_url": "https://github.com/eclipseo", "followers_url": "https://api.github.com/users/eclipseo/followers", "following_url": "https://api.github.com/users/eclipseo/following{/other_user}", "gists_url": "https://api.github.com/users/eclipseo/gists{/gist_id}", "starred_url": "https://api.github.com/users/eclipseo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eclipseo/subscriptions", "organizations_url": "https://api.github.com/users/eclipseo/orgs", "repos_url": "https://api.github.com/users/eclipseo/repos", "events_url": "https://api.github.com/users/eclipseo/events{/privacy}", "received_events_url": "https://api.github.com/users/eclipseo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-08-25T12:58:36Z", "updated_at": "2019-09-10T07:11:32Z", "closed_at": "2019-09-09T17:30:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Python 3.8beta3 with commit fd585ce36faba338fa1c71949cf29ed8e8f5ea19\r\n\r\nEnvironment (cloupickle is unbundled):\r\n```\r\nDEBUG util.py:587:   python3-cloudpickle         noarch       1.2.1-3.fc32\r\nDEBUG util.py:587:   python3-devel               armv7hl      3.8.0~b3-4.fc32\r\nDEBUG util.py:587:   python3-lz4                 armv7hl      2.1.2-5.fc32\r\nDEBUG util.py:587:   python3-numpy               armv7hl      1:1.17.0-3.fc32\r\nDEBUG util.py:587:   python3-psutil              armv7hl      5.6.3-3.fc32\r\nDEBUG util.py:587:   python3-pytest              noarch       4.6.5-3.fc32 \r\n```\r\n\r\nThere seems to be a hash issue:\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n________ test_hashes_stay_the_same[This is a string to hash-expected0] _________\r\nto_hash = 'This is a string to hash'\r\nexpected = {'py2': '80436ada343b0d79a99bfd8883a96e45', 'py3': '71b3f47df22cb19431d85d92d0b230b2'}\r\n    @parametrize('to_hash,expected',\r\n                 [('This is a string to hash',\r\n                     {'py2': '80436ada343b0d79a99bfd8883a96e45',\r\n                      'py3': '71b3f47df22cb19431d85d92d0b230b2'}),\r\n                  (u\"C'est l\\xe9t\\xe9\",\r\n                     {'py2': '2ff3a25200eb6219f468de2640913c2d',\r\n                      'py3': '2d8d189e9b2b0b2e384d93c868c0e576'}),\r\n                  ((123456, 54321, -98765),\r\n                     {'py2': '50d81c80af05061ac4dcdc2d5edee6d6',\r\n                      'py3': 'e205227dd82250871fa25aa0ec690aa3'}),\r\n                  ([random.Random(42).random() for _ in range(5)],\r\n                     {'py2': '1a36a691b2e2ba3a9df72de3dccf17ea',\r\n                      'py3': 'a11ffad81f9682a7d901e6edc3d16c84'}),\r\n                  ([3, 'abc', None, TransportableException('foo', ValueError)],\r\n                     {'py2': 'adb6ba84990ee5e462dc138383f11802',\r\n                      'py3': '994f663c64ba5e64b2a85ebe75287829'}),\r\n                  ({'abcde': 123, 'sadfas': [-9999, 2, 3]},\r\n                     {'py2': 'fc9314a39ff75b829498380850447047',\r\n                      'py3': 'aeda150553d4bb5c69f0e69d51b0e2ef'})])\r\n    def test_hashes_stay_the_same(to_hash, expected):\r\n        # We want to make sure that hashes don't change with joblib\r\n        # version. For end users, that would mean that they have to\r\n        # regenerate their cache from scratch, which potentially means\r\n        # lengthy recomputations.\r\n        # Expected results have been generated with joblib 0.9.2\r\n    \r\n        py_version_str = 'py3' if PY3_OR_LATER else 'py2'\r\n>       assert hash(to_hash) == expected[py_version_str]\r\nE       AssertionError: assert '115916c98f74...58a5c7184c94c' == '71b3f47df22cb...85d92d0b230b2'\r\nE         - 115916c98f747117d0458a5c7184c94c\r\nE         + 71b3f47df22cb19431d85d92d0b230b2\r\njoblib/test/test_hashing.py:372: AssertionError\r\n____________ test_hashes_stay_the_same[C'est l\\xe9t\\xe9-expected1] _____________\r\nto_hash = \"C'est l\u00e9t\u00e9\"\r\nexpected = {'py2': '2ff3a25200eb6219f468de2640913c2d', 'py3': '2d8d189e9b2b0b2e384d93c868c0e576'}\r\n    @parametrize('to_hash,expected',\r\n                 [('This is a string to hash',\r\n                     {'py2': '80436ada343b0d79a99bfd8883a96e45',\r\n                      'py3': '71b3f47df22cb19431d85d92d0b230b2'}),\r\n                  (u\"C'est l\\xe9t\\xe9\",\r\n                     {'py2': '2ff3a25200eb6219f468de2640913c2d',\r\n                      'py3': '2d8d189e9b2b0b2e384d93c868c0e576'}),\r\n                  ((123456, 54321, -98765),\r\n                     {'py2': '50d81c80af05061ac4dcdc2d5edee6d6',\r\n                      'py3': 'e205227dd82250871fa25aa0ec690aa3'}),\r\n                  ([random.Random(42).random() for _ in range(5)],\r\n                     {'py2': '1a36a691b2e2ba3a9df72de3dccf17ea',\r\n                      'py3': 'a11ffad81f9682a7d901e6edc3d16c84'}),\r\n                  ([3, 'abc', None, TransportableException('foo', ValueError)],\r\n                     {'py2': 'adb6ba84990ee5e462dc138383f11802',\r\n                      'py3': '994f663c64ba5e64b2a85ebe75287829'}),\r\n                  ({'abcde': 123, 'sadfas': [-9999, 2, 3]},\r\n                     {'py2': 'fc9314a39ff75b829498380850447047',\r\n                      'py3': 'aeda150553d4bb5c69f0e69d51b0e2ef'})])\r\n    def test_hashes_stay_the_same(to_hash, expected):\r\n        # We want to make sure that hashes don't change with joblib\r\n        # version. For end users, that would mean that they have to\r\n        # regenerate their cache from scratch, which potentially means\r\n        # lengthy recomputations.\r\n        # Expected results have been generated with joblib 0.9.2\r\n    \r\n        py_version_str = 'py3' if PY3_OR_LATER else 'py2'\r\n>       assert hash(to_hash) == expected[py_version_str]\r\nE       AssertionError: assert '6cfa6be6cf88...faa7bbaf4e5ee' == '2d8d189e9b2b0...d93c868c0e576'\r\nE         - 6cfa6be6cf88fc4835cfaa7bbaf4e5ee\r\nE         + 2d8d189e9b2b0b2e384d93c868c0e576\r\njoblib/test/test_hashing.py:372: AssertionError\r\n________________ test_hashes_stay_the_same[to_hash2-expected2] _________________\r\nto_hash = (123456, 54321, -98765)\r\nexpected = {'py2': '50d81c80af05061ac4dcdc2d5edee6d6', 'py3': 'e205227dd82250871fa25aa0ec690aa3'}\r\n    @parametrize('to_hash,expected',\r\n                 [('This is a string to hash',\r\n                     {'py2': '80436ada343b0d79a99bfd8883a96e45',\r\n                      'py3': '71b3f47df22cb19431d85d92d0b230b2'}),\r\n                  (u\"C'est l\\xe9t\\xe9\",\r\n                     {'py2': '2ff3a25200eb6219f468de2640913c2d',\r\n                      'py3': '2d8d189e9b2b0b2e384d93c868c0e576'}),\r\n                  ((123456, 54321, -98765),\r\n                     {'py2': '50d81c80af05061ac4dcdc2d5edee6d6',\r\n                      'py3': 'e205227dd82250871fa25aa0ec690aa3'}),\r\n                  ([random.Random(42).random() for _ in range(5)],\r\n                     {'py2': '1a36a691b2e2ba3a9df72de3dccf17ea',\r\n                      'py3': 'a11ffad81f9682a7d901e6edc3d16c84'}),\r\n                  ([3, 'abc', None, TransportableException('foo', ValueError)],\r\n                     {'py2': 'adb6ba84990ee5e462dc138383f11802',\r\n                      'py3': '994f663c64ba5e64b2a85ebe75287829'}),\r\n                  ({'abcde': 123, 'sadfas': [-9999, 2, 3]},\r\n                     {'py2': 'fc9314a39ff75b829498380850447047',\r\n                      'py3': 'aeda150553d4bb5c69f0e69d51b0e2ef'})])\r\n    def test_hashes_stay_the_same(to_hash, expected):\r\n        # We want to make sure that hashes don't change with joblib\r\n        # version. For end users, that would mean that they have to\r\n        # regenerate their cache from scratch, which potentially means\r\n        # lengthy recomputations.\r\n        # Expected results have been generated with joblib 0.9.2\r\n    \r\n        py_version_str = 'py3' if PY3_OR_LATER else 'py2'\r\n>       assert hash(to_hash) == expected[py_version_str]\r\nE       AssertionError: assert '6f7adce43153...a73430e3c79a3' == 'e205227dd8225...25aa0ec690aa3'\r\nE         - 6f7adce43153280d4c1a73430e3c79a3\r\nE         + e205227dd82250871fa25aa0ec690aa3\r\njoblib/test/test_hashing.py:372: AssertionError\r\n________________ test_hashes_stay_the_same[to_hash3-expected3] _________________\r\nto_hash = [0.6394267984578837, 0.6394267984578837, 0.6394267984578837, 0.6394267984578837, 0.6394267984578837]\r\nexpected = {'py2': '1a36a691b2e2ba3a9df72de3dccf17ea', 'py3': 'a11ffad81f9682a7d901e6edc3d16c84'}\r\n    @parametrize('to_hash,expected',\r\n                 [('This is a string to hash',\r\n                     {'py2': '80436ada343b0d79a99bfd8883a96e45',\r\n                      'py3': '71b3f47df22cb19431d85d92d0b230b2'}),\r\n                  (u\"C'est l\\xe9t\\xe9\",\r\n                     {'py2': '2ff3a25200eb6219f468de2640913c2d',\r\n                      'py3': '2d8d189e9b2b0b2e384d93c868c0e576'}),\r\n                  ((123456, 54321, -98765),\r\n                     {'py2': '50d81c80af05061ac4dcdc2d5edee6d6',\r\n                      'py3': 'e205227dd82250871fa25aa0ec690aa3'}),\r\n                  ([random.Random(42).random() for _ in range(5)],\r\n                     {'py2': '1a36a691b2e2ba3a9df72de3dccf17ea',\r\n                      'py3': 'a11ffad81f9682a7d901e6edc3d16c84'}),\r\n                  ([3, 'abc', None, TransportableException('foo', ValueError)],\r\n                     {'py2': 'adb6ba84990ee5e462dc138383f11802',\r\n                      'py3': '994f663c64ba5e64b2a85ebe75287829'}),\r\n                  ({'abcde': 123, 'sadfas': [-9999, 2, 3]},\r\n                     {'py2': 'fc9314a39ff75b829498380850447047',\r\n                      'py3': 'aeda150553d4bb5c69f0e69d51b0e2ef'})])\r\n    def test_hashes_stay_the_same(to_hash, expected):\r\n        # We want to make sure that hashes don't change with joblib\r\n        # version. For end users, that would mean that they have to\r\n        # regenerate their cache from scratch, which potentially means\r\n        # lengthy recomputations.\r\n        # Expected results have been generated with joblib 0.9.2\r\n    \r\n        py_version_str = 'py3' if PY3_OR_LATER else 'py2'\r\n>       assert hash(to_hash) == expected[py_version_str]\r\nE       AssertionError: assert '889a464e6239...e0306ad9b15e5' == 'a11ffad81f968...1e6edc3d16c84'\r\nE         - 889a464e6239c2e7a64e0306ad9b15e5\r\nE         + a11ffad81f9682a7d901e6edc3d16c84\r\njoblib/test/test_hashing.py:372: AssertionError\r\n________________ test_hashes_stay_the_same[to_hash4-expected4] _________________\r\nto_hash = [3, 'abc', None, TransportableException\r\n___________________________________________________________________________\r\nfoo\r\n___________________________________________________________________________]\r\nexpected = {'py2': 'adb6ba84990ee5e462dc138383f11802', 'py3': '994f663c64ba5e64b2a85ebe75287829'}\r\n    @parametrize('to_hash,expected',\r\n                 [('This is a string to hash',\r\n                     {'py2': '80436ada343b0d79a99bfd8883a96e45',\r\n                      'py3': '71b3f47df22cb19431d85d92d0b230b2'}),\r\n                  (u\"C'est l\\xe9t\\xe9\",\r\n                     {'py2': '2ff3a25200eb6219f468de2640913c2d',\r\n                      'py3': '2d8d189e9b2b0b2e384d93c868c0e576'}),\r\n                  ((123456, 54321, -98765),\r\n                     {'py2': '50d81c80af05061ac4dcdc2d5edee6d6',\r\n                      'py3': 'e205227dd82250871fa25aa0ec690aa3'}),\r\n                  ([random.Random(42).random() for _ in range(5)],\r\n                     {'py2': '1a36a691b2e2ba3a9df72de3dccf17ea',\r\n                      'py3': 'a11ffad81f9682a7d901e6edc3d16c84'}),\r\n                  ([3, 'abc', None, TransportableException('foo', ValueError)],\r\n                     {'py2': 'adb6ba84990ee5e462dc138383f11802',\r\n                      'py3': '994f663c64ba5e64b2a85ebe75287829'}),\r\n                  ({'abcde': 123, 'sadfas': [-9999, 2, 3]},\r\n                     {'py2': 'fc9314a39ff75b829498380850447047',\r\n                      'py3': 'aeda150553d4bb5c69f0e69d51b0e2ef'})])\r\n    def test_hashes_stay_the_same(to_hash, expected):\r\n        # We want to make sure that hashes don't change with joblib\r\n        # version. For end users, that would mean that they have to\r\n        # regenerate their cache from scratch, which potentially means\r\n        # lengthy recomputations.\r\n        # Expected results have been generated with joblib 0.9.2\r\n    \r\n        py_version_str = 'py3' if PY3_OR_LATER else 'py2'\r\n>       assert hash(to_hash) == expected[py_version_str]\r\nE       AssertionError: assert '71fef3696bd5...e7bbf050dbfda' == '994f663c64ba5...85ebe75287829'\r\nE         - 71fef3696bd507b1e36e7bbf050dbfda\r\nE         + 994f663c64ba5e64b2a85ebe75287829\r\njoblib/test/test_hashing.py:372: AssertionError\r\n________________ test_hashes_stay_the_same[to_hash5-expected5] _________________\r\nto_hash = {'abcde': 123, 'sadfas': [-9999, 2, 3]}\r\nexpected = {'py2': 'fc9314a39ff75b829498380850447047', 'py3': 'aeda150553d4bb5c69f0e69d51b0e2ef'}\r\n    @parametrize('to_hash,expected',\r\n                 [('This is a string to hash',\r\n                     {'py2': '80436ada343b0d79a99bfd8883a96e45',\r\n                      'py3': '71b3f47df22cb19431d85d92d0b230b2'}),\r\n                  (u\"C'est l\\xe9t\\xe9\",\r\n                     {'py2': '2ff3a25200eb6219f468de2640913c2d',\r\n                      'py3': '2d8d189e9b2b0b2e384d93c868c0e576'}),\r\n                  ((123456, 54321, -98765),\r\n                     {'py2': '50d81c80af05061ac4dcdc2d5edee6d6',\r\n                      'py3': 'e205227dd82250871fa25aa0ec690aa3'}),\r\n                  ([random.Random(42).random() for _ in range(5)],\r\n                     {'py2': '1a36a691b2e2ba3a9df72de3dccf17ea',\r\n                      'py3': 'a11ffad81f9682a7d901e6edc3d16c84'}),\r\n                  ([3, 'abc', None, TransportableException('foo', ValueError)],\r\n                     {'py2': 'adb6ba84990ee5e462dc138383f11802',\r\n                      'py3': '994f663c64ba5e64b2a85ebe75287829'}),\r\n                  ({'abcde': 123, 'sadfas': [-9999, 2, 3]},\r\n                     {'py2': 'fc9314a39ff75b829498380850447047',\r\n                      'py3': 'aeda150553d4bb5c69f0e69d51b0e2ef'})])\r\n    def test_hashes_stay_the_same(to_hash, expected):\r\n        # We want to make sure that hashes don't change with joblib\r\n        # version. For end users, that would mean that they have to\r\n        # regenerate their cache from scratch, which potentially means\r\n        # lengthy recomputations.\r\n        # Expected results have been generated with joblib 0.9.2\r\n    \r\n        py_version_str = 'py3' if PY3_OR_LATER else 'py2'\r\n>       assert hash(to_hash) == expected[py_version_str]\r\nE       AssertionError: assert 'b8d5ad181711...9dc9308b8ae05' == 'aeda150553d4b...0e69d51b0e2ef'\r\nE         - b8d5ad18171111167d69dc9308b8ae05\r\nE         + aeda150553d4bb5c69f0e69d51b0e2ef\r\njoblib/test/test_hashing.py:372: AssertionError\r\n_________________ test_hashes_stay_the_same_with_numpy_objects _________________\r\n    @with_numpy\r\n    def test_hashes_stay_the_same_with_numpy_objects():\r\n        # We want to make sure that hashes don't change with joblib\r\n        # version. For end users, that would mean that they have to\r\n        # regenerate their cache from scratch, which potentially means\r\n        # lengthy recomputations.\r\n        rng = np.random.RandomState(42)\r\n        # Being explicit about dtypes in order to avoid\r\n        # architecture-related differences. Also using 'f4' rather than\r\n        # 'f8' for float arrays because 'f8' arrays generated by\r\n        # rng.random.randn don't seem to be bit-identical on 32bit and\r\n        # 64bit machines.\r\n        to_hash_list = [\r\n            rng.randint(-1000, high=1000, size=50).astype('<i8'),\r\n            tuple(rng.randn(3).astype('<f4') for _ in range(5)),\r\n            [rng.randn(3).astype('<f4') for _ in range(5)],\r\n            {\r\n                -3333: rng.randn(3, 5).astype('<f4'),\r\n                0: [\r\n                    rng.randint(10, size=20).astype('<i8'),\r\n                    rng.randn(10).astype('<f4')\r\n                ]\r\n            },\r\n            # Non regression cases for https://github.com/joblib/joblib/issues/308.\r\n            # Generated with joblib 0.9.4.\r\n            np.arange(100, dtype='<i8').reshape((10, 10)),\r\n            # Fortran contiguous array\r\n            np.asfortranarray(np.arange(100, dtype='<i8').reshape((10, 10))),\r\n            # Non contiguous array\r\n            np.arange(100, dtype='<i8').reshape((10, 10))[:, :2],\r\n        ]\r\n    \r\n        # These expected results have been generated with joblib 0.9.0\r\n        expected_dict = {'py2': ['80f2387e7752abbda2658aafed49e086',\r\n                                 '0d700f7f25ea670fd305e4cd93b0e8cd',\r\n                                 '83a2bdf843e79e4b3e26521db73088b9',\r\n                                 '63e0efd43c0a9ad92a07e8ce04338dd3',\r\n                                 '03fef702946b602c852b8b4e60929914',\r\n                                 '07074691e90d7098a85956367045c81e',\r\n                                 'd264cf79f353aa7bbfa8349e3df72d8f'],\r\n                         'py3': ['10a6afc379ca2708acfbaef0ab676eab',\r\n                                 '988a7114f337f381393025911ebc823b',\r\n                                 'c6809f4b97e35f2fa0ee8d653cbd025c',\r\n                                 'b3ad17348e32728a7eb9cda1e7ede438',\r\n                                 '927b3e6b0b6a037e8e035bda134e0b05',\r\n                                 '108f6ee98e7db19ea2006ffd208f4bf1',\r\n                                 'bd48ccaaff28e16e6badee81041b7180']}\r\n    \r\n        py_version_str = 'py3' if PY3_OR_LATER else 'py2'\r\n        expected_list = expected_dict[py_version_str]\r\n    \r\n        for to_hash, expected in zip(to_hash_list, expected_list):\r\n>           assert hash(to_hash) == expected\r\nE           AssertionError: assert '445fe65329ff...e94ff3d8e673b' == '10a6afc379ca2...baef0ab676eab'\r\nE             - 445fe65329ff1c52ea9e94ff3d8e673b\r\nE             + 10a6afc379ca2708acfbaef0ab676eab\r\njoblib/test/test_hashing.py:447: AssertionError\r\n_____________________ test_no_semaphore_tracker_on_import ______________________\r\n    @pytest.mark.skipif(PY27, reason=\"Need python3.3+\")\r\n    def test_no_semaphore_tracker_on_import():\r\n        # check that importing joblib does not implicitly spawn a ressource tracker\r\n        # or a semaphore tracker\r\n        code = \"\"\"if True:\r\n            import joblib\r\n            from multiprocessing import semaphore_tracker\r\n            # The following line would raise RuntimeError if the\r\n            # start_method is already set.\r\n            msg = \"multiprocessing.semaphore_tracker has been spawned on import\"\r\n            assert semaphore_tracker._semaphore_tracker._fd is None, msg\"\"\"\r\n>       check_subprocess_call([sys.executable, '-c', code])\r\njoblib/test/test_module.py:38: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ncmd = ['/usr/bin/python3', '-c', 'if True:\\n        import joblib\\n        from multiprocessing import semaphore_tracker\\n  ...g.semaphore_tracker has been spawned on import\"\\n        assert semaphore_tracker._semaphore_tracker._fd is None, msg']\r\ntimeout = 5, stdout_regex = None, stderr_regex = None\r\n    def check_subprocess_call(cmd, timeout=5, stdout_regex=None,\r\n                              stderr_regex=None):\r\n        \"\"\"Runs a command in a subprocess with timeout in seconds.\r\n    \r\n        Also checks returncode is zero, stdout if stdout_regex is set, and\r\n        stderr if stderr_regex is set.\r\n        \"\"\"\r\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\r\n                                stderr=subprocess.PIPE)\r\n    \r\n        def kill_process():\r\n            warnings.warn(\"Timeout running {}\".format(cmd))\r\n            proc.kill()\r\n    \r\n        timer = threading.Timer(timeout, kill_process)\r\n        try:\r\n            timer.start()\r\n            stdout, stderr = proc.communicate()\r\n    \r\n            if PY3_OR_LATER:\r\n                stdout, stderr = stdout.decode(), stderr.decode()\r\n            if proc.returncode != 0:\r\n                message = (\r\n                    'Non-zero return code: {}.\\nStdout:\\n{}\\n'\r\n                    'Stderr:\\n{}').format(\r\n                        proc.returncode, stdout, stderr)\r\n>               raise ValueError(message)\r\nE               ValueError: Non-zero return code: 1.\r\nE               Stdout:\r\nE               \r\nE               Stderr:\r\nE               Traceback (most recent call last):\r\nE                 File \"<string>\", line 3, in <module>\r\nE               ImportError: cannot import name 'semaphore_tracker' from 'multiprocessing' (/usr/lib/python3.8/multiprocessing/__init__.py)\r\njoblib/testing.py:65: ValueError\r\n=============================== warnings summary ===============================\r\n/usr/lib/python3.8/site-packages/_pytest/mark/structures.py:330\r\n  /usr/lib/python3.8/site-packages/_pytest/mark/structures.py:330: PytestUnknownMarkWarning: Unknown pytest.mark.timeout - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n    warnings.warn(\r\njoblib/test/test_func_inspect.py::test_filter_args_2\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_func_inspect.py:132: UserWarning: Cannot inspect object functools.partial(<function f at 0xaf812928>, 1), ignore list will not work.\r\n    assert filter_args(ff, ['y'], (1, )) == {'*': [1], '**': {}}\r\njoblib/test/test_func_inspect.py::test_filter_args_python_3\r\njoblib/test/test_func_inspect.py::test_filter_args_error_msg[ValueError-Ignore list: argument \\\\'(.*)\\\\' is not defined-g-args1]\r\njoblib/test/test_func_inspect.py::test_filter_args_error_msg[ValueError-Wrong number of arguments-h-args2]\r\njoblib/test/test_memory.py::test_memory_func_with_kwonly_args\r\njoblib/test/test_memory.py::test_memory_func_with_kwonly_args\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/func_inspect.py:192: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\r\n    arg_spec_str = inspect.formatargspec(*arg_spec_for_format)\r\njoblib/test/test_hashing.py::test_bound_cached_methods_hash\r\njoblib/test/test_hashing.py::test_bound_cached_methods_hash\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_hashing.py:69: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\r\n  You provided \"cachedir='/tmp/pytest-of-mockbuild/pytest-0/test_bound_cached_methods_hash0'\", use \"location='/tmp/pytest-of-mockbuild/pytest-0/test_bound_cached_methods_hash0'\" instead.\r\n    mem = Memory(cachedir=cachedir)\r\njoblib/test/test_hashing.py::test_dict_hash\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_hashing.py:69: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\r\n  You provided \"cachedir='/tmp/pytest-of-mockbuild/pytest-0/test_dict_hash0'\", use \"location='/tmp/pytest-of-mockbuild/pytest-0/test_dict_hash0'\" instead.\r\n    mem = Memory(cachedir=cachedir)\r\njoblib/test/test_hashing.py::test_set_hash\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_hashing.py:69: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\r\n  You provided \"cachedir='/tmp/pytest-of-mockbuild/pytest-0/test_set_hash0'\", use \"location='/tmp/pytest-of-mockbuild/pytest-0/test_set_hash0'\" instead.\r\n    mem = Memory(cachedir=cachedir)\r\njoblib/test/test_memory.py::test_memory_integration\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_memory.py:103: UserWarning: Compressed results cannot be memmapped\r\n    memory = Memory(location=tmpdir.strpath, verbose=10,\r\njoblib/test/test_memory.py::test_memory_integration\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/memory.py:131: UserWarning: Compressed items cannot be memmapped in a filesystem store. Option will be ignored.\r\n    obj.configure(location, verbose=verbose,\r\njoblib/test/test_memory.py::test_memory_integration\r\n  /usr/lib/python3.8/contextlib.py:113: UserWarning: mmap_mode \"r\" is not compatible with compressed file /tmp/pytest-of-mockbuild/pytest-0/test_memory_integration0/joblib/joblib/test/test_memory/f/6946afe22f5c7e3ab08058d3ff5993cf/output.pkl. \"r\" flag will be ignored.\r\n    return next(self.gen)\r\njoblib/test/test_numpy_pickle.py::test_numpy_persistence[False]\r\njoblib/test/test_numpy_pickle.py::test_numpy_persistence[True]\r\njoblib/test/test_numpy_pickle.py::test_numpy_persistence[0]\r\njoblib/test/test_numpy_pickle.py::test_numpy_persistence[3]\r\njoblib/test/test_numpy_pickle.py::test_numpy_persistence[zlib]\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_numpy_pickle.py:169: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\r\n    for obj in (np.matrix(np.zeros(10)),\r\njoblib/test/test_numpy_pickle.py::test_compressed_pickle_dump_and_load\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_numpy_pickle.py:369: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\r\n    np.matrix([0, 1, 2], dtype=np.dtype('<i8')),\r\njoblib/test/test_numpy_pickle.py::test_compressed_pickle_dump_and_load\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_numpy_pickle.py:370: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\r\n    np.matrix([0, 1, 2], dtype=np.dtype('>i8')),\r\njoblib/test/test_numpy_pickle.py::test_file_handle_persistence\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_numpy_pickle.py:652: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\r\n    np.matrix([0, 1, 2])]\r\njoblib/test/test_numpy_pickle.py::test_in_memory_persistence\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/test/test_numpy_pickle.py:686: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\r\n    np.matrix([0, 1, 2])]\r\njoblib/test/test_parallel.py::test_nested_loop[threading-multiprocessing]\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/parallel.py:672: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1\r\n    n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\r\njoblib/test/test_parallel.py::test_nested_loop[threading-loky]\r\njoblib/test/test_parallel.py::test_nested_loop[threading-back_compat_backend]\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/parallel.py:672: UserWarning: Loky-backed parallel loops cannot be nested below threads, setting n_jobs=1\r\n    n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\r\njoblib/test/test_parallel.py::test_retrieval_context\r\njoblib/test/test_parallel.py::test_retrieval_context\r\njoblib/test/test_parallel.py::test_retrieval_context\r\njoblib/test/test_parallel.py::test_retrieval_context\r\njoblib/test/test_parallel.py::test_retrieval_context\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/parallel.py:267: DeprecationWarning: check_pickle is deprecated in joblib 0.12 and will be removed in 0.13\r\n    warnings.warn('check_pickle is deprecated in joblib 0.12 and will be'\r\njoblib/test/test_testing.py::test_check_subprocess_call_timeout\r\n  /builddir/build/BUILDROOT/python-joblib-0.13.2-5.20190824gitfd585ce.fc32.noarch/usr/lib/python3.8/site-packages/joblib/testing.py:50: UserWarning: Timeout running ['/usr/bin/python3', '-c', 'import time\\nimport sys\\nprint(\"before sleep on stdout\")\\nsys.stdout.flush()\\nsys.stderr.write(\"before sleep on stderr\")\\nsys.stderr.flush()\\ntime.sleep(1.1)\\nprint(\"process should have be killed before\")\\nsys.stdout.flush()']\r\n    warnings.warn(\"Timeout running {}\".format(cmd))\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n= 8 failed, 1122 passed, 18 skipped, 1 deselected, 32 warnings in 222.49 seconds =\r\n```\r\n\r\nAnd the latest failure comes from:\r\n\r\nbpo-36867: The multiprocessing.resource_tracker replaces the multiprocessing.semaphore_tracker module. Other than semaphores, resource_tracker also tracks shared_memory segments.\r\nhttps://bugs.python.org/issue36867", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/923", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/923/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/923/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/923/events", "html_url": "https://github.com/joblib/joblib/issues/923", "id": 477338578, "node_id": "MDU6SXNzdWU0NzczMzg1Nzg=", "number": 923, "title": "Parallel jobs do not see updated module variables", "user": {"login": "bersbersbers", "id": 12128514, "node_id": "MDQ6VXNlcjEyMTI4NTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/12128514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bersbersbers", "html_url": "https://github.com/bersbersbers", "followers_url": "https://api.github.com/users/bersbersbers/followers", "following_url": "https://api.github.com/users/bersbersbers/following{/other_user}", "gists_url": "https://api.github.com/users/bersbersbers/gists{/gist_id}", "starred_url": "https://api.github.com/users/bersbersbers/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bersbersbers/subscriptions", "organizations_url": "https://api.github.com/users/bersbersbers/orgs", "repos_url": "https://api.github.com/users/bersbersbers/repos", "events_url": "https://api.github.com/users/bersbersbers/events{/privacy}", "received_events_url": "https://api.github.com/users/bersbersbers/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-06T12:00:08Z", "updated_at": "2019-08-09T05:09:48Z", "closed_at": "2019-08-08T17:08:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Consider this example code:\r\n\r\n`MyConsts.py`\r\n```\r\nmyConst = 1\r\n```\r\n\r\n`Bug.py`\r\n```\r\nfrom joblib import Parallel, delayed\r\n\r\nimport MyConsts\r\n\r\nprint(f\"we have got myConst == {MyConsts.myConst}\")\r\nMyConsts.myConst = 2\r\nprint(f\"we have set myConst := {MyConsts.myConst}\")\r\n\r\ndef Job(name):\r\n    print(f\"{name} sees myConst == {MyConsts.myConst}\")\r\n\r\nParallel(n_jobs=1)(delayed(Job)(\"correct: serial\") for _ in range(1))\r\nParallel(n_jobs=2)(delayed(Job)(\"a bug? parallel\") for _ in range(1))\r\n```\r\n\r\nThis outputs:\r\n```\r\nwe have got myConst == 1\r\nwe have set myConst := 2\r\ncorrect: serial sees myConst == 2\r\na bug? parallel sees myConst == 1\r\n```\r\n\r\nIt seems that the update of `myConst` has not made it into the parallel environment that is used to run `Job`. Is this a bug or working as expected?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/917", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/917/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/917/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/917/events", "html_url": "https://github.com/joblib/joblib/issues/917", "id": 472932691, "node_id": "MDU6SXNzdWU0NzI5MzI2OTE=", "number": 917, "title": "joblib fails at import on Python3.8", "user": {"login": "pierreglaser", "id": 18555600, "node_id": "MDQ6VXNlcjE4NTU1NjAw", "avatar_url": "https://avatars3.githubusercontent.com/u/18555600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pierreglaser", "html_url": "https://github.com/pierreglaser", "followers_url": "https://api.github.com/users/pierreglaser/followers", "following_url": "https://api.github.com/users/pierreglaser/following{/other_user}", "gists_url": "https://api.github.com/users/pierreglaser/gists{/gist_id}", "starred_url": "https://api.github.com/users/pierreglaser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pierreglaser/subscriptions", "organizations_url": "https://api.github.com/users/pierreglaser/orgs", "repos_url": "https://api.github.com/users/pierreglaser/repos", "events_url": "https://api.github.com/users/pierreglaser/events{/privacy}", "received_events_url": "https://api.github.com/users/pierreglaser/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-25T15:30:37Z", "updated_at": "2019-12-19T19:57:15Z", "closed_at": "2019-10-24T14:19:09Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "```python\r\n>>> import joblib\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/__init__.py\", line 119, in <module>\r\n    from .parallel import Parallel\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/parallel.py\", line 28, in <module>\r\n    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 22, in <module>\r\n    from .executor import get_memmapping_executor\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/executor.py\", line 14, in <module>\r\n    from .externals.loky.reusable_executor import get_reusable_executor\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/externals/loky/__init__.py\", line 12, in <module>\r\n    from .backend.reduction import set_loky_pickler\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/externals/loky/backend/reduction.py\", line 125, in <module>\r\n    from joblib.externals import cloudpickle  # noqa: F401\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/externals/cloudpickle/__init__.py\", line 3, in <module>\r\n    from .cloudpickle import *\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 152, in <module>\r\n    _cell_set_template_code = _make_cell_set_template_code()\r\n  File \"/home/pierreglaser/.virtualenvs/py38/lib/python3.8/site-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 133, in _make_cell_set_template_code\r\n    return types.CodeType(\r\nTypeError: an integer is required (got type bytes)\r\n```\r\n\r\nThe last joblib pypi releases only has `cloudpickle0.8` which is not compatible with `python3.8`. We should release a new `joblib` soon. \r\n\r\nOther errors due to old cloudpickle: #909 ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/916", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/916/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/916/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/916/events", "html_url": "https://github.com/joblib/joblib/issues/916", "id": 472048799, "node_id": "MDU6SXNzdWU0NzIwNDg3OTk=", "number": 916, "title": "error using ipyparallel backend with sklearn.cluster.kmeans", "user": {"login": "nikhil003", "id": 5336184, "node_id": "MDQ6VXNlcjUzMzYxODQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5336184?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikhil003", "html_url": "https://github.com/nikhil003", "followers_url": "https://api.github.com/users/nikhil003/followers", "following_url": "https://api.github.com/users/nikhil003/following{/other_user}", "gists_url": "https://api.github.com/users/nikhil003/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikhil003/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikhil003/subscriptions", "organizations_url": "https://api.github.com/users/nikhil003/orgs", "repos_url": "https://api.github.com/users/nikhil003/repos", "events_url": "https://api.github.com/users/nikhil003/events{/privacy}", "received_events_url": "https://api.github.com/users/nikhil003/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-07-24T02:30:03Z", "updated_at": "2019-07-29T11:16:31Z", "closed_at": "2019-07-29T11:16:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "```\r\nfrom ipyparallel import Client\r\nfrom ipyparallel.joblib import IPythonParallelBackend\r\nfrom sklearn.externals.joblib import Parallel, parallel_backend, register_parallel_backend, delayed\r\n\r\nc1 = Client(profile=\"batch_slurm_python-3.6.1\")\r\nbview = c1.load_balanced_view()\r\nregister_parallel_backend('ipyparallel', lambda: IPythonParallelBackend(view=bview))\r\n```\r\n\r\nAfter doing the aforementioned steps, I tested the following code\r\n\r\n```\r\nwith parallel_backend('ipyparallel'):\r\n    cluster.KMeans(n_clusters=6, n_jobs=-1).fit(image_2d)\r\n```\r\n\r\nhowever, when i run the code, i get following error.\r\n\r\n**AttributeError: 'IPythonParallelBackend' object has no attribute '_effective_batch_size'**", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/906", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/906/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/906/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/906/events", "html_url": "https://github.com/joblib/joblib/issues/906", "id": 465268116, "node_id": "MDU6SXNzdWU0NjUyNjgxMTY=", "number": 906, "title": "BUG: Bug with loky backend and scipy.fft (pybind11?)", "user": {"login": "larsoner", "id": 2365790, "node_id": "MDQ6VXNlcjIzNjU3OTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2365790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/larsoner", "html_url": "https://github.com/larsoner", "followers_url": "https://api.github.com/users/larsoner/followers", "following_url": "https://api.github.com/users/larsoner/following{/other_user}", "gists_url": "https://api.github.com/users/larsoner/gists{/gist_id}", "starred_url": "https://api.github.com/users/larsoner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/larsoner/subscriptions", "organizations_url": "https://api.github.com/users/larsoner/orgs", "repos_url": "https://api.github.com/users/larsoner/repos", "events_url": "https://api.github.com/users/larsoner/events{/privacy}", "received_events_url": "https://api.github.com/users/larsoner/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-08T13:47:41Z", "updated_at": "2019-07-08T14:06:26Z", "closed_at": "2019-07-08T14:06:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Recently SciPy added pocketfft support via templates and pybind11 and C++ templating:\r\n\r\nhttps://github.com/scipy/scipy/pull/10238\r\n\r\nThis change passed all tests on SciPy's repo but seems to interact badly with `joblib`, specifically when using the \"processes\" method (on 0.13.2 and latest `master`):\r\n```Python\r\n>>> import numpy as np\r\n>>> from joblib import Parallel, delayed\r\n>>> from scipy.fft import rfft\r\n>>> [rfft(np.zeros(1)) for _ in range(2)]  # works\r\n>>> Parallel(n_jobs=2, prefer='threads')(delayed(rfft)(np.zeros(1)) for _ in range(2))  # works\r\n>>> Parallel(n_jobs=2, prefer='processes')(delayed(rfft)(np.zeros(1)) for _ in range(2))  # fails\r\njoblib.externals.loky.process_executor._RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/larsoner/python/joblib/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\r\n    r = call_item()\r\n  File \"/home/larsoner/python/joblib/joblib/externals/loky/process_executor.py\", line 272, in __call__\r\n    return self.fn(*self.args, **self.kwargs)\r\n  File \"/home/larsoner/python/joblib/joblib/_parallel_backends.py\", line 567, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"/home/larsoner/python/joblib/joblib/parallel.py\", line 225, in __call__\r\n    for func, args, kwargs in self.items]\r\n  File \"/home/larsoner/python/joblib/joblib/parallel.py\", line 225, in <listcomp>\r\n    for func, args, kwargs in self.items]\r\n  File \"/home/larsoner/python/scipy/scipy/fft/_basic.py\", line 298, in rfft\r\n    return _pocketfft.rfft(x, n, axis, norm, overwrite_x)\r\n  File \"/home/larsoner/python/scipy/scipy/fft/_pocketfft/basic.py\", line 134, in rfft\r\n    return pfft.r2c(tmp, (axis,), True, norm, None, _default_workers)\r\nRuntimeError: unsupported data type\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/larsoner/python/joblib/joblib/parallel.py\", line 934, in __call__\r\n    self.retrieve()\r\n  File \"/home/larsoner/python/joblib/joblib/parallel.py\", line 833, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"/home/larsoner/python/joblib/joblib/_parallel_backends.py\", line 521, in wrap_future_result\r\n    return future.result(timeout=timeout)\r\n  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 432, in result\r\n    return self.__get_result()\r\n  File \"/usr/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\r\n    raise self._exception\r\nRuntimeError: unsupported data type\r\n```\r\nModifying the SciPy code to print the dtype and `__class__` of at least `tmp` shows `np.float64` and `np.ndarray` as expected, so I'm not sure what unsupported datatype we are getting.\r\n\r\ncc @peterbell10 in case you have some idea why this would fail.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/905", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/905/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/905/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/905/events", "html_url": "https://github.com/joblib/joblib/issues/905", "id": 464687835, "node_id": "MDU6SXNzdWU0NjQ2ODc4MzU=", "number": 905, "title": "Parallel timeout arg", "user": {"login": "orihomie", "id": 29889683, "node_id": "MDQ6VXNlcjI5ODg5Njgz", "avatar_url": "https://avatars2.githubusercontent.com/u/29889683?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orihomie", "html_url": "https://github.com/orihomie", "followers_url": "https://api.github.com/users/orihomie/followers", "following_url": "https://api.github.com/users/orihomie/following{/other_user}", "gists_url": "https://api.github.com/users/orihomie/gists{/gist_id}", "starred_url": "https://api.github.com/users/orihomie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orihomie/subscriptions", "organizations_url": "https://api.github.com/users/orihomie/orgs", "repos_url": "https://api.github.com/users/orihomie/repos", "events_url": "https://api.github.com/users/orihomie/events{/privacy}", "received_events_url": "https://api.github.com/users/orihomie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-05T15:25:09Z", "updated_at": "2019-10-12T15:40:41Z", "closed_at": "2019-10-12T15:40:41Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, can you please tell me what is the measurement of timeout variable of Parallel class keyword arg? \r\nIs that seconds? Milliseconds? \r\n\r\nI've jumped deep into code and couldn't find anything about that.\r\nAnd can you please describe it in your docs / summaries?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/904", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/904/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/904/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/904/events", "html_url": "https://github.com/joblib/joblib/issues/904", "id": 462806795, "node_id": "MDU6SXNzdWU0NjI4MDY3OTU=", "number": 904, "title": "[Question] Role of `_to_func_args` and `maybe_to_future` in `DaskDistributedBackend`", "user": {"login": "jjerphan", "id": 13029839, "node_id": "MDQ6VXNlcjEzMDI5ODM5", "avatar_url": "https://avatars3.githubusercontent.com/u/13029839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjerphan", "html_url": "https://github.com/jjerphan", "followers_url": "https://api.github.com/users/jjerphan/followers", "following_url": "https://api.github.com/users/jjerphan/following{/other_user}", "gists_url": "https://api.github.com/users/jjerphan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjerphan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjerphan/subscriptions", "organizations_url": "https://api.github.com/users/jjerphan/orgs", "repos_url": "https://api.github.com/users/jjerphan/repos", "events_url": "https://api.github.com/users/jjerphan/events{/privacy}", "received_events_url": "https://api.github.com/users/jjerphan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-01T16:31:06Z", "updated_at": "2019-09-25T11:55:53Z", "closed_at": "2019-09-25T11:55:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\n_Note : this issue is more about understanding some internals of the `DaskDistributedBackend` connector._\r\n\r\nI am currently trying to troubleshoot `KeyError` and `AssertionError: yield from wasn't used with future` related to futures when using `dask` as a back-end with joblib (see #875).\r\n\r\nSimilar issues exist and relate to errors thrown in [`maybe_to_future`](https://github.com/joblib/joblib/blob/master/joblib/_dask.py#L213) in [`_to_func_args`](https://github.com/joblib/joblib/blob/master/joblib/_dask.py#L205) (see for instance #852 , [distributed#2149](https://github.com/dask/distributed/issues/2149), [distributed#2532](https://github.com/dask/distributed/issues/2532)).\r\n\r\nI would like to better understand the last changes that were made to those functions in [distributed#2020](https://github.com/dask/distributed/pull/2020/files).\r\n\r\nFor what I understand, `args` and `kwargs` are computed via `Futures` if needed before being returned in a `Batch` that is to be executed on `Workers` ; is that right or is it more subtil?\r\n\r\nThank you a lot!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/902", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/902/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/902/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/902/events", "html_url": "https://github.com/joblib/joblib/issues/902", "id": 462265564, "node_id": "MDU6SXNzdWU0NjIyNjU1NjQ=", "number": 902, "title": "module 'os' has no attribute 'statvfs'", "user": {"login": "tianxia3", "id": 25040585, "node_id": "MDQ6VXNlcjI1MDQwNTg1", "avatar_url": "https://avatars3.githubusercontent.com/u/25040585?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tianxia3", "html_url": "https://github.com/tianxia3", "followers_url": "https://api.github.com/users/tianxia3/followers", "following_url": "https://api.github.com/users/tianxia3/following{/other_user}", "gists_url": "https://api.github.com/users/tianxia3/gists{/gist_id}", "starred_url": "https://api.github.com/users/tianxia3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tianxia3/subscriptions", "organizations_url": "https://api.github.com/users/tianxia3/orgs", "repos_url": "https://api.github.com/users/tianxia3/repos", "events_url": "https://api.github.com/users/tianxia3/events{/privacy}", "received_events_url": "https://api.github.com/users/tianxia3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1718822484, "node_id": "MDU6TGFiZWwxNzE4ODIyNDg0", "url": "https://api.github.com/repos/joblib/joblib/labels/windows", "name": "windows", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-29T04:54:02Z", "updated_at": "2019-12-06T14:44:30Z", "closed_at": "2019-12-06T14:44:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "Crashed on Windows: joblib\\_memmapping_reducer.py.\r\nLooks like os.statvfs is not available on Windows. Is there a fix for this?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/901", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/901/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/901/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/901/events", "html_url": "https://github.com/joblib/joblib/issues/901", "id": 462015949, "node_id": "MDU6SXNzdWU0NjIwMTU5NDk=", "number": 901, "title": "Python 3.7, Windows: parallelisation does not work", "user": {"login": "madig", "id": 380829, "node_id": "MDQ6VXNlcjM4MDgyOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/380829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/madig", "html_url": "https://github.com/madig", "followers_url": "https://api.github.com/users/madig/followers", "following_url": "https://api.github.com/users/madig/following{/other_user}", "gists_url": "https://api.github.com/users/madig/gists{/gist_id}", "starred_url": "https://api.github.com/users/madig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/madig/subscriptions", "organizations_url": "https://api.github.com/users/madig/orgs", "repos_url": "https://api.github.com/users/madig/repos", "events_url": "https://api.github.com/users/madig/events{/privacy}", "received_events_url": "https://api.github.com/users/madig/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1718822484, "node_id": "MDU6TGFiZWwxNzE4ODIyNDg0", "url": "https://api.github.com/repos/joblib/joblib/labels/windows", "name": "windows", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-06-28T12:41:18Z", "updated_at": "2020-05-30T13:38:13Z", "closed_at": "2019-12-06T14:42:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "I'm currently tracking down why I can't get `n_jobs=-1` to work for something else, and found the following when trying the examples:\r\n\r\n```\r\n>>> from joblib import Parallel, delayed\r\n>>> from math import modf\r\n>>> Parallel(n_jobs=2)(delayed(modf)(i/2.) for i in range(10))\r\n\r\nexception calling callback for <Future at 0x2d4dd8b97b8 state=finished raised BrokenProcessPool>\r\njoblib.externals.loky.process_executor._RemoteTraceback:\r\n'''\r\nTraceback (most recent call last):\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 391, in _process_worker\r\n    call_item = call_queue.get(block=True, timeout=timeout)\r\n  File \"c:\\program files\\python37\\lib\\multiprocessing\\queues.py\", line 99, in get\r\n    if not self._rlock.acquire(block, timeout):\r\nPermissionError: [WinError 5] Access is denied\r\n'''\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\r\n    callback(self)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 309, in __call__\r\n    self.parallel.dispatch_next()\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 731, in dispatch_next\r\n    if not self.dispatch_one_batch(self._original_iterator):\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 510, in apply_async\r\n    future = self._workers.submit(SafeFunction(func))\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\", line 151, in submit\r\n    fn, *args, **kwargs)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 1022, in submit\r\n    raise self._flags.broken\r\njoblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. \r\nPlease ensure that the arguments of the function are all picklable.\r\nERROR: The process with PID 11044 (child process of PID 10084) could not be terminated.\r\nReason: There is no running instance of the task.\r\nERROR: The process with PID 10084 (child process of PID 10936) could not be terminated.\r\nReason: There is no running instance of the task.\r\nERROR: The process \"10020\" not found.\r\njoblib.externals.loky.process_executor._RemoteTraceback: \r\n'''\r\nTraceback (most recent call last):\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 391, in _process_worker\r\n    call_item = call_queue.get(block=True, timeout=timeout)\r\n  File \"c:\\program files\\python37\\lib\\multiprocessing\\queues.py\", line 99, in get\r\n    if not self._rlock.acquire(block, timeout):\r\nPermissionError: [WinError 5] Access is denied\r\n'''\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 934, in __call__\r\n    self.retrieve()\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 833, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 521, in wrap_future_result\r\n    return future.result(timeout=timeout)\r\n  File \"c:\\program files\\python37\\lib\\concurrent\\futures\\_base.py\", line 432, in result      \r\n    return self.__get_result()\r\n  File \"c:\\program files\\python37\\lib\\concurrent\\futures\\_base.py\", line 384, in __get_result\r\n    raise self._exception\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\r\n    callback(self)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 309, in __call__\r\n    self.parallel.dispatch_next()\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 731, in dispatch_next\r\n    if not self.dispatch_one_batch(self._original_iterator):\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 510, in apply_async\r\n    future = self._workers.submit(SafeFunction(func))\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\", line 151, in submit\r\n    fn, *args, **kwargs)\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 1022, in submit\r\n    raise self._flags.broken\r\njoblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. \r\nPlease ensure that the arguments of the function are all picklable.\r\n```\r\n\r\nThe errors may vary:\r\n\r\n```\r\n>>> Parallel(n_jobs=2, verbose=10)(delayed(modf)(i/2.) for i in range(10)) \r\n[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\r\n[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\r\n[Parallel(n_jobs=2)]: Done   2 tasks      | elapsed:    0.4s\r\n[Parallel(n_jobs=2)]: Done   3 tasks      | elapsed:    0.4s\r\n[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.4s\r\nERROR: The process \"6640\" not found.\r\njoblib.externals.loky.process_executor._RemoteTraceback: \r\n'''\r\nTraceback (most recent call last):\r\n  File \"c:\\program files\\python37\\lib\\multiprocessing\\queues.py\", line 109, in get\r\n    self._sem.release()\r\nOSError: [WinError 6] The handle is invalid\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 391, in _process_worker\r\n    call_item = call_queue.get(block=True, timeout=timeout)\r\n  File \"c:\\program files\\python37\\lib\\multiprocessing\\queues.py\", line 111, in get\r\n    self._rlock.release()\r\nOSError: [WinError 6] The handle is invalid\r\n'''\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 934, in __call__\r\n    self.retrieve()\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\parallel.py\", line 833, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"C:\\UsersLocal\\nikolaus.waxweiler\\Envs\\damakerngrouper-py3.7\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 521, in wrap_future_result\r\n    return future.result(timeout=timeout)\r\n  File \"c:\\program files\\python37\\lib\\concurrent\\futures\\_base.py\", line 432, in result      \r\n    return self.__get_result()\r\n  File \"c:\\program files\\python37\\lib\\concurrent\\futures\\_base.py\", line 384, in __get_result\r\n    raise self._exception\r\njoblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. \r\nPlease ensure that the arguments of the function are all picklable.\r\n```\r\n\r\nPython 3.7.3 x64 on Windows 10 1903, joblib 0.13.2.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/884", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/884/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/884/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/884/events", "html_url": "https://github.com/joblib/joblib/issues/884", "id": 451875754, "node_id": "MDU6SXNzdWU0NTE4NzU3NTQ=", "number": 884, "title": "Task queue usage?", "user": {"login": "gsakkis", "id": 291289, "node_id": "MDQ6VXNlcjI5MTI4OQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/291289?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gsakkis", "html_url": "https://github.com/gsakkis", "followers_url": "https://api.github.com/users/gsakkis/followers", "following_url": "https://api.github.com/users/gsakkis/following{/other_user}", "gists_url": "https://api.github.com/users/gsakkis/gists{/gist_id}", "starred_url": "https://api.github.com/users/gsakkis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gsakkis/subscriptions", "organizations_url": "https://api.github.com/users/gsakkis/orgs", "repos_url": "https://api.github.com/users/gsakkis/repos", "events_url": "https://api.github.com/users/gsakkis/events{/privacy}", "received_events_url": "https://api.github.com/users/gsakkis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-04T08:56:19Z", "updated_at": "2019-06-14T18:10:45Z", "closed_at": "2019-06-14T18:10:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it possible to use joblib, either directly or as a building block, to provide basic task queue functionality? What I mean is, instead of passing upfront a batch of tasks to be executed in parallel, have some sort of queue that tasks are added and executed dynamically. (I'm less interested in more advanced task queue features such as scheduling, retrying failed tasks, task prioritization, etc).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/880", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/880/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/880/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/880/events", "html_url": "https://github.com/joblib/joblib/issues/880", "id": 449835718, "node_id": "MDU6SXNzdWU0NDk4MzU3MTg=", "number": 880, "title": "Oversubscription protection done by setting environment variables in worker initializer is not effective", "user": {"login": "ogrisel", "id": 89061, "node_id": "MDQ6VXNlcjg5MDYx", "avatar_url": "https://avatars0.githubusercontent.com/u/89061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ogrisel", "html_url": "https://github.com/ogrisel", "followers_url": "https://api.github.com/users/ogrisel/followers", "following_url": "https://api.github.com/users/ogrisel/following{/other_user}", "gists_url": "https://api.github.com/users/ogrisel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ogrisel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ogrisel/subscriptions", "organizations_url": "https://api.github.com/users/ogrisel/orgs", "repos_url": "https://api.github.com/users/ogrisel/repos", "events_url": "https://api.github.com/users/ogrisel/events{/privacy}", "received_events_url": "https://api.github.com/users/ogrisel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-05-29T14:16:05Z", "updated_at": "2019-09-25T07:49:20Z", "closed_at": "2019-09-25T07:49:20Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "This was implemented in @tomMoral's #701 but it does not work. For instance with loky:\r\n\r\n- both the queue objects and the initializer are passed to worker as pickles\r\n- the worker program starts by unpickling this preparation data\r\n- then the initializer is called and sets the env variable\r\n\r\nUnfortunately this is too late: the queue objects have custom reducers some of which customize the handling of numpy arrays. Therefore numpy is imported well before calling the initializer.\r\n\r\nAn alternative would be to extend loky such that is `fork_exec` function use to spawn the worker process can receive a dict of custom environment variables to be passed to `os.execve` instead of just using `os.execv`.\r\n\r\nThis would make it possible to set the environment variable prior to starting the Python interpreter of the workers.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/877", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/877/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/877/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/877/events", "html_url": "https://github.com/joblib/joblib/issues/877", "id": 449197128, "node_id": "MDU6SXNzdWU0NDkxOTcxMjg=", "number": 877, "title": "Test fail on numpy-1.16", "user": {"login": "scarabeusiv", "id": 1055830, "node_id": "MDQ6VXNlcjEwNTU4MzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1055830?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scarabeusiv", "html_url": "https://github.com/scarabeusiv", "followers_url": "https://api.github.com/users/scarabeusiv/followers", "following_url": "https://api.github.com/users/scarabeusiv/following{/other_user}", "gists_url": "https://api.github.com/users/scarabeusiv/gists{/gist_id}", "starred_url": "https://api.github.com/users/scarabeusiv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scarabeusiv/subscriptions", "organizations_url": "https://api.github.com/users/scarabeusiv/orgs", "repos_url": "https://api.github.com/users/scarabeusiv/repos", "events_url": "https://api.github.com/users/scarabeusiv/events{/privacy}", "received_events_url": "https://api.github.com/users/scarabeusiv/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-05-28T10:44:11Z", "updated_at": "2019-05-29T14:09:21Z", "closed_at": "2019-05-29T14:09:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "As a sidenote first you guys don't test on travis against latest numpy :)\r\n\r\nWhen testing the pickle loading the default behaviour changed in numpy and breaks the test_joblib_pickle_across_python_versions test.\r\n\r\nThe most important change is in the numpy.load behaviour as inlined bellow:\r\n\r\n```[  379s]         allow_pickle : bool, optional\r\n[  379s]             Allow loading pickled object arrays stored in npy files. Reasons for\r\n[  379s]             disallowing pickles include security, as loading pickled data can\r\n[  379s]             execute arbitrary code. If pickles are disallowed, loading object\r\n[  379s]             arrays will fail. Default: False\r\n[  379s]     \r\n[  379s]             .. versionchanged:: 1.16.3\r\n[  379s]                 Made default False in response to CVE-2019-6446.\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/875", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/875/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/875/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/875/events", "html_url": "https://github.com/joblib/joblib/issues/875", "id": 445366205, "node_id": "MDU6SXNzdWU0NDUzNjYyMDU=", "number": 875, "title": "Parallel hangs when using Dask as a back-end", "user": {"login": "jjerphan", "id": 13029839, "node_id": "MDQ6VXNlcjEzMDI5ODM5", "avatar_url": "https://avatars3.githubusercontent.com/u/13029839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjerphan", "html_url": "https://github.com/jjerphan", "followers_url": "https://api.github.com/users/jjerphan/followers", "following_url": "https://api.github.com/users/jjerphan/following{/other_user}", "gists_url": "https://api.github.com/users/jjerphan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjerphan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjerphan/subscriptions", "organizations_url": "https://api.github.com/users/jjerphan/orgs", "repos_url": "https://api.github.com/users/jjerphan/repos", "events_url": "https://api.github.com/users/jjerphan/events{/privacy}", "received_events_url": "https://api.github.com/users/jjerphan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2019-05-17T10:07:19Z", "updated_at": "2019-09-26T15:46:38Z", "closed_at": "2019-09-26T15:46:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI have been using Dask through joblib and had somes problem of deadlocks for the execution of some tasks.\r\n\r\nI am using `Parallel` with Dask as a backend. `Parallel.__call__` does not returns.\r\nAfter having modified some of the code with logs in [`scikit-learn 0.20.x` distribution of joblib](https://github.com/jjerphan/scikit-learn/pull/1/files) and [`distributed master`](https://github.com/jjerphan/distributed/pull/1/files); what I observed is that Tasks are not submitted at one point. Apart from that it seems that tasks on workers are executed properly.\r\n\r\nI have inspected `Parallel` code, I think that I may have found a possible explanation of possible deadlocks. \r\n\r\n---\r\n\r\nThe dispatch of tasks in made as follows in `Parallel.__call__`:\r\n```python\r\nif self.dispatch_one_batch(iterator):\r\n    self._iterating = self._original_iterator is not None\r\n\r\nwhile self.dispatch_one_batch(iterator):\r\n    logging.info(\"Parallel.__call__ dispatched one batch\")\r\n    pass\r\n```\r\n\r\nin `Parallel.dispatch_one_batch` `Parallel._dispatch` is called.\r\nAs indicated in the docstring `Parallel._dispatch` \"_this method is not thread-safe: it should be only called indirectly via dispatch_one_batch_\". \r\n\r\n`_dispatch` does use `BatchCompletionCallBack`'s objects that are called by the backend`apply_async`. In the case of `SequentialBackend`'s, the execution is made in the same thread so there are no problems ; but in the case of Dask, `DaskDistributedBackend.apply_async` does register this callback on `tornado.IOLoop`:\r\n\r\n```python\r\nclass DaskDistributedBackend(ParallelBackendBase, AutoBatchingMixin):\r\n\r\n    # ...\r\n\r\n    def apply_async(self, func, callback=None):\r\n        key = '%s-batch-%s' % (_funcname(func), uuid4().hex)\r\n        func, args = self._to_func_args(func)\r\n\r\n        future = self.client.submit(func, *args, key=key, **self.submit_kwargs)\r\n        self.task_futures.add(future)\r\n\r\n        @gen.coroutine\r\n        def callback_wrapper():\r\n            result = yield _wait([future])\r\n            self.task_futures.remove(future)\r\n            if callback is not None:\r\n                callback(result)  # gets called in separate thread\r\n\r\n        self.client.loop.add_callback(callback_wrapper)\r\n        # ... \r\n\r\n```\r\n\r\nAnd as indicated, it \"gets called in another thread\" ; hence possibly producing a dead-lock if the lock is already taken in the main thread.\r\n\r\nI am not entirely sure this could be a valid explanation. Also, I do not see a way to do handle the callback differently in `DaskDistributedBackend`, nor do I understand all the moving parts of this connector.\r\n\r\nI am inspecting it currently. I'll be glad to have your thought on it, @ogrisel and @mrocklin.\r\nWhile I can hardly give a minimal complete reproducible example with my setup (sorry), I think that the problem goes hand in hand with specific workloads and can be present in a variety of setups and observed in a variety of errors that are hard to predict.\r\n\r\nHowever, you will find logs that present the problem ; I hope they are useful.\r\n\r\nPlease let me know if you need some more clues. \r\n\r\n---\r\n\r\n### Setup:\r\n\r\n```\r\nPython version: 2.7.5 (default, Apr  9 2019, 14:30:50)\r\nDask version: 1.2.2+10.gd53b560\r\nDistributed version: 1.28.1+26.gb876796.dirty\r\n```\r\n<details>\r\n<summary> Client-side logs</summary>\r\n<pre>\r\n[2019-05-17 08:18:35,615] [22/MainThread] [INFO] [root] Setting up Dask\r\n[2019-05-17 08:18:35,616] [22/MainThread] [INFO] [root] Python      version:\r\n[2019-05-17 08:18:35,616] [22/MainThread] [INFO] [root] 2.7.5 (default, Apr  9 2019, 14:30:50)\r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]\r\n[2019-05-17 08:18:35,616] [22/MainThread] [INFO] [root] Dask        version: 1.2.2+10.gd53b560\r\n[2019-05-17 08:18:35,616] [22/MainThread] [INFO] [root] Distributed version: 1.28.1+26.gb876796.dirty\r\n[2019-05-17 08:18:46,970] [22/MainThread] [INFO] [root] Dask Scheduler started on port 8786\r\n[2019-05-17 08:18:46,971] [22/MainThread] [INFO] [root] Done with Dask Scheduler Setup\r\n[2019-05-17 08:18:47,302] [22/Thread-1] [INFO] [root] Scheduler.__init__ called\r\n[2019-05-17 08:18:47,307] [22/Thread-1] [INFO] [root] Scheduler._setup_logging called\r\n[2019-05-17 08:18:48,938] [22/Thread-1] [INFO] [root] Scheduler.start called\r\ndistributed.scheduler - INFO - Clear task state\r\ndistributed.scheduler - INFO -   Scheduler at:     tcp://10.12.3.73:8786\r\ndistributed.scheduler - INFO -       bokeh at:                     :8787\r\n[2019-05-17 08:18:49,069] [22/Thread-1] [INFO] [root] Scheduler.finished called\r\n[2019-05-17 08:18:49,074] [22/Thread-1] [INFO] [root] Scheduler.add_worker called\r\n[2019-05-17 08:18:49,074] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\ndistributed.scheduler - INFO - Register tcp://10.12.3.74:33599\r\n[2019-05-17 08:18:49,077] [22/Thread-1] [INFO] [root] Scheduler.handle_worker called\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.12.3.74:33599\r\ndistributed.core - INFO - Starting established connection\r\n[2019-05-17 08:18:49,087] [22/Thread-1] [INFO] [root] Scheduler.add_worker called\r\n[2019-05-17 08:18:49,088] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\ndistributed.scheduler - INFO - Register tcp://10.12.3.75:42309\r\n[2019-05-17 08:18:49,091] [22/Thread-1] [INFO] [root] Scheduler.handle_worker called\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.12.3.75:42309\r\ndistributed.core - INFO - Starting established connection\r\n[2019-05-17 08:18:49,095] [22/Thread-1] [INFO] [root] Scheduler.add_worker called\r\n[2019-05-17 08:18:49,096] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\ndistributed.scheduler - INFO - Register tcp://10.12.3.76:36063\r\n[2019-05-17 08:18:49,099] [22/Thread-1] [INFO] [root] Scheduler.handle_worker called\r\n[2019-05-17 08:18:49,099] [22/MainThread] [INFO] [root] Checking series of type: float64 (isM8=False)\r\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.12.3.76:36063\r\ndistributed.core - INFO - Starting established connection\r\n[2019-05-17 08:18:49,212] [22/MainThread] [INFO] [root] Creating Dask client\r\n[2019-05-17 08:18:49,233] [22/Thread-1] [INFO] [root] Scheduler.add_client called\r\ndistributed.scheduler - INFO - Receive client connection: Client-65e26f35-787c-11e9-8016-0a580a0c0349\r\ndistributed.core - INFO - Starting established connection\r\n[2019-05-17 08:18:49,242] [22/MainThread] [INFO] [root] register_parallel_backend called\r\n[2019-05-17 08:18:49,242] [22/MainThread] [INFO] [root] Creating joblib.Parallel(n_jobs=4, verbose=100, pre_dispatch=n_jobs, backend=dask)\r\n[2019-05-17 08:18:49,242] [22/MainThread] [INFO] [root] Parallel.__init__ called\r\n[2019-05-17 08:18:49,242] [22/MainThread] [INFO] [root] DaskDistributedBackend.__init__ called\r\n[2019-05-17 08:18:49,242] [22/MainThread] [INFO] [root] Worker.get_client called\r\n[2019-05-17 08:18:49,242] [22/MainThread] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:49,242] [22/MainThread] [INFO] [root] Parallel.__call__ called\r\n[2019-05-17 08:18:49,243] [22/MainThread] [INFO] [root] Parallel._initialize_backend called\r\n[2019-05-17 08:18:49,243] [22/MainThread] [INFO] [root] DaskDistributedBackend.configure called\r\n[2019-05-17 08:18:49,243] [22/MainThread] [INFO] [root] DaskDistributedBackend.effective_n_jobs called\r\n[2019-05-17 08:18:49,251] [22/MainThread] [INFO] [root] Parallel._print called\r\n[2019-05-17 08:18:49,251] [22/MainThread] [INFO] [root] DaskDistributedBackend.start_call called\r\n[2019-05-17 08:18:49,251] [22/MainThread] [INFO] [root] Parallel.__init__: iterator created\r\n[2019-05-17 08:18:49,251] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch called\r\n[2019-05-17 08:18:49,251] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: took the lock\r\n[2019-05-17 08:18:49,251] [22/MainThread] [INFO] [root] DaskDistributedBackend.get_nested_backend called\r\n[2019-05-17 08:18:49,251] [22/MainThread] [INFO] [root] DaskDistributedBackend.__init__ called\r\n[2019-05-17 08:18:49,252] [22/MainThread] [INFO] [root] delayed called\r\n[2019-05-17 08:18:49,254] [22/MainThread] [INFO] [root] Parallel.dispatch: calling self._dispatch\r\n[2019-05-17 08:18:49,254] [22/MainThread] [INFO] [root] Parallel._dispatch called\r\n[2019-05-17 08:18:49,254] [22/MainThread] [INFO] [root] Parallel._dispatch: submitting job of id 0 to back-end\r\n[2019-05-17 08:18:49,254] [22/MainThread] [INFO] [root] DaskDistributedBackend.apply_async called\r\n[2019-05-17 08:18:49,254] [22/MainThread] [INFO] [root] DaskDistributedBackend._to_func_args called\r\n[2019-05-17 08:18:49,255] [22/MainThread] [INFO] [root] in '_dask._to_func_args'\r\n[2019-05-17 08:18:49,255] [22/MainThread] [INFO] [root] Looping on func.items\r\n[2019-05-17 08:18:49,255] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:49,262] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:49,270] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:49,280] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:49,280] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,280] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,280] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,294] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:49,303] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:49,308] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:49,312] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:49,312] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,312] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,313] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,322] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:49,328] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:49,331] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:49,334] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:49,334] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,334] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,334] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,344] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:49,351] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:49,354] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:49,356] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:49,357] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,357] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,357] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,369] [22/MainThread] [INFO] [root] Appending tasks done\r\n[2019-05-17 08:18:49,369] [22/MainThread] [INFO] [root] Batch.__init__ called\r\n[2019-05-17 08:18:49,369] [22/MainThread] [INFO] [root] Submitting to the client\r\n[2019-05-17 08:18:49,369] [22/MainThread] [INFO] [root] Batch.__reduce__ called\r\n[2019-05-17 08:18:49,370] [22/MainThread] [INFO] [root] Added future to tasks\r\n[2019-05-17 08:18:49,371] [22/MainThread] [INFO] [root] Parallel._dispatch: job of id 0 inserted to back-end\r\n[2019-05-17 08:18:49,371] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: leaving the lock after self._dispatch\r\n[2019-05-17 08:18:49,372] [22/Thread-1] [INFO] [root] Scheduler.update_graph called\r\n[2019-05-17 08:18:49,373] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch called\r\n[2019-05-17 08:18:49,373] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:49,373] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: took the lock\r\n[2019-05-17 08:18:49,374] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:49,374] [22/MainThread] [INFO] [root] DaskDistributedBackend.get_nested_backend called\r\n[2019-05-17 08:18:49,374] [22/Thread-1] [INFO] [root] Scheduler.valid_workers called\r\n[2019-05-17 08:18:49,374] [22/MainThread] [INFO] [root] DaskDistributedBackend.__init__ called\r\n[2019-05-17 08:18:49,374] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:49,375] [22/Thread-1] [INFO] [root] Scheduler.consumre_resources called\r\n[2019-05-17 08:18:49,375] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:49,375] [22/MainThread] [INFO] [root] delayed called\r\n[2019-05-17 08:18:49,375] [22/Thread-1] [INFO] [root] Scheduler.send_task_to_worker called\r\n[2019-05-17 08:18:49,376] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:49,377] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,377] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,378] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,378] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,379] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,379] [22/MainThread] [INFO] [root] Parallel.dispatch: calling self._dispatch\r\n[2019-05-17 08:18:49,379] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,380] [22/MainThread] [INFO] [root] Parallel._dispatch called\r\n[2019-05-17 08:18:49,380] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:49,380] [22/MainThread] [INFO] [root] Parallel._dispatch: submitting job of id 1 to back-end\r\n[2019-05-17 08:18:49,381] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:49,382] [22/MainThread] [INFO] [root] DaskDistributedBackend.apply_async called\r\n[2019-05-17 08:18:49,383] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:49,383] [22/MainThread] [INFO] [root] DaskDistributedBackend._to_func_args called\r\n[2019-05-17 08:18:49,383] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:49,383] [22/MainThread] [INFO] [root] in '_dask._to_func_args'\r\n[2019-05-17 08:18:49,384] [22/MainThread] [INFO] [root] Looping on func.items\r\n[2019-05-17 08:18:49,384] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:49,397] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:49,400] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:49,485] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:49,586] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:49,687] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:49,788] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:49,888] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:49,990] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,090] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,191] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,292] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,392] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,493] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,594] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,694] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,795] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,895] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:50,996] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,097] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,113] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:51,113] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,113] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,113] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,122] [22/Thread-1] [INFO] [root] Scheduler.add_client called\r\ndistributed.scheduler - INFO - Receive client connection: Client-worker-67033d1e-787c-11e9-8016-0a580a0c034a\r\ndistributed.core - INFO - Starting established connection\r\n[2019-05-17 08:18:51,165] [22/MainThread] [INFO] [root] maybe to future called\r\n=[2019-05-17 08:18:51,171] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:51,173] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:51,176] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:51,176] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,176] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,176] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,194] [22/MainThread] [INFO] [root] Appending tasks done\r\n[2019-05-17 08:18:51,194] [22/MainThread] [INFO] [root] Batch.__init__ called\r\n[2019-05-17 08:18:51,194] [22/MainThread] [INFO] [root] Submitting to the client\r\n[2019-05-17 08:18:51,194] [22/MainThread] [INFO] [root] Batch.__reduce__ called\r\n[2019-05-17 08:18:51,195] [22/MainThread] [INFO] [root] Added future to tasks\r\n[2019-05-17 08:18:51,196] [22/MainThread] [INFO] [root] Parallel._dispatch: job of id 1 inserted to back-end\r\n[2019-05-17 08:18:51,196] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: leaving the lock after self._dispatch\r\n[2019-05-17 08:18:51,196] [22/MainThread] [INFO] [root] Parallel.__call__ dispatched one batch\r\n[2019-05-17 08:18:51,196] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch called\r\n[2019-05-17 08:18:51,196] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: took the lock\r\n[2019-05-17 08:18:51,196] [22/MainThread] [INFO] [root] DaskDistributedBackend.get_nested_backend called\r\n[2019-05-17 08:18:51,197] [22/MainThread] [INFO] [root] DaskDistributedBackend.__init__ called\r\n[2019-05-17 08:18:51,198] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,198] [22/MainThread] [INFO] [root] delayed called\r\n[2019-05-17 08:18:51,199] [22/MainThread] [INFO] [root] Parallel.dispatch: calling self._dispatch\r\n[2019-05-17 08:18:51,200] [22/MainThread] [INFO] [root] Parallel._dispatch called\r\n[2019-05-17 08:18:51,200] [22/MainThread] [INFO] [root] Parallel._dispatch: submitting job of id 2 to back-end\r\n[2019-05-17 08:18:51,200] [22/MainThread] [INFO] [root] DaskDistributedBackend.apply_async called\r\n[2019-05-17 08:18:51,200] [22/MainThread] [INFO] [root] DaskDistributedBackend._to_func_args called\r\n[2019-05-17 08:18:51,200] [22/MainThread] [INFO] [root] in '_dask._to_func_args'\r\n[2019-05-17 08:18:51,200] [22/MainThread] [INFO] [root] Looping on func.items\r\n[2019-05-17 08:18:51,200] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:51,207] [22/Thread-1] [INFO] [root] Scheduler.update_graph called\r\n[2019-05-17 08:18:51,207] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,208] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:51,208] [22/Thread-1] [INFO] [root] Scheduler.valid_workers called\r\n[2019-05-17 08:18:51,208] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:51,208] [22/Thread-1] [INFO] [root] Scheduler.consumre_resources called\r\n[2019-05-17 08:18:51,208] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,209] [22/Thread-1] [INFO] [root] Scheduler.send_task_to_worker called\r\n[2019-05-17 08:18:51,209] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:51,209] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,208] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:51,209] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,210] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,211] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,211] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,211] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,211] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,211] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,213] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:51,216] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,217] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,219] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:51,219] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,219] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,219] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,224] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:51,230] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:51,232] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:51,237] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:51,238] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,238] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,238] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,247] [22/MainThread] [INFO] [root] Appending tasks done\r\n[2019-05-17 08:18:51,247] [22/MainThread] [INFO] [root] Batch.__init__ called\r\n[2019-05-17 08:18:51,247] [22/MainThread] [INFO] [root] Submitting to the client\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] Batch.__reduce__ called\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] Added future to tasks\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] Parallel._dispatch: job of id 2 inserted to back-end\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: leaving the lock after self._dispatch\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] Parallel.__call__ dispatched one batch\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch called\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: took the lock\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] DaskDistributedBackend.get_nested_backend called\r\n[2019-05-17 08:18:51,248] [22/MainThread] [INFO] [root] DaskDistributedBackend.__init__ called\r\n[2019-05-17 08:18:51,249] [22/MainThread] [INFO] [root] delayed called\r\n[2019-05-17 08:18:51,250] [22/MainThread] [INFO] [root] Parallel.dispatch: calling self._dispatch\r\n[2019-05-17 08:18:51,251] [22/MainThread] [INFO] [root] Parallel._dispatch called\r\n[2019-05-17 08:18:51,251] [22/MainThread] [INFO] [root] Parallel._dispatch: submitting job of id 3 to back-end\r\n[2019-05-17 08:18:51,251] [22/MainThread] [INFO] [root] DaskDistributedBackend.apply_async called\r\n[2019-05-17 08:18:51,251] [22/MainThread] [INFO] [root] DaskDistributedBackend._to_func_args called\r\n[2019-05-17 08:18:51,251] [22/MainThread] [INFO] [root] in '_dask._to_func_args'\r\n[2019-05-17 08:18:51,251] [22/MainThread] [INFO] [root] Looping on func.items\r\n[2019-05-17 08:18:51,251] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:51,253] [22/Thread-1] [INFO] [root] Scheduler.update_graph called\r\n[2019-05-17 08:18:51,254] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,254] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:51,254] [22/Thread-1] [INFO] [root] Scheduler.valid_workers called\r\n[2019-05-17 08:18:51,254] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:51,254] [22/Thread-1] [INFO] [root] Scheduler.consumre_resources called\r\n[2019-05-17 08:18:51,254] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.send_task_to_worker called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,255] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,264] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:51,265] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:51,277] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:51,278] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,278] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,278] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,282] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:51,290] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:51,292] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:51,295] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:51,295] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,295] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,296] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,301] [22/MainThread] [INFO] [root] Appending tasks done\r\n[2019-05-17 08:18:51,301] [22/MainThread] [INFO] [root] Batch.__init__ called\r\n[2019-05-17 08:18:51,301] [22/MainThread] [INFO] [root] Submitting to the client\r\n[2019-05-17 08:18:51,301] [22/MainThread] [INFO] [root] Batch.__reduce__ called\r\n[2019-05-17 08:18:51,302] [22/Thread-1] [INFO] [root] Scheduler.update_graph called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.valid_workers called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.decide_worker called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.worker_objective called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.worker_objective called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.consumre_resources called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.send_task_to_worker called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,303] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,304] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,304] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,304] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,304] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,304] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,305] [22/MainThread] [INFO] [root] Added future to tasks\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] Parallel._dispatch: job of id 3 inserted to back-end\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: leaving the lock after self._dispatch\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] Parallel.__call__ dispatched one batch\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch called\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] Parallel.dispatch_one_batch: took the lock\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] DaskDistributedBackend.get_nested_backend called\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] DaskDistributedBackend.__init__ called\r\n[2019-05-17 08:18:51,306] [22/MainThread] [INFO] [root] delayed called\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] Parallel.dispatch: calling self._dispatch\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] Parallel._dispatch called\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] Parallel._dispatch: submitting job of id 4 to back-end\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] DaskDistributedBackend.apply_async called\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] DaskDistributedBackend._to_func_args called\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] in '_dask._to_func_args'\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] Looping on func.items\r\n[2019-05-17 08:18:51,308] [22/MainThread] [INFO] [root] maybe to future called\r\n[2019-05-17 08:18:51,313] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,317] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,319] [22/IO loop] [INFO] [root] Worker.get_worker called\r\n[2019-05-17 08:18:51,321] [22/Thread-1] [INFO] [root] Scheduler.scatter called\r\n[2019-05-17 08:18:51,399] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,399] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,455] [22/Thread-1] [INFO] [root] Scheduler.handle_task_finished called\r\n[2019-05-17 08:18:51,455] [22/Thread-1] [INFO] [root] Scheduler._remove_from_processing called\r\n[2019-05-17 08:18:51,455] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,455] [22/Thread-1] [INFO] [root] Scheduler.release_resources called\r\n[2019-05-17 08:18:51,455] [22/Thread-1] [INFO] [root] Scheduler._add_to_memory called\r\n[2019-05-17 08:18:51,455] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,455] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,457] [22/IO loop] [INFO] [root] Parallel.print_progress called\r\n[2019-05-17 08:18:51,457] [22/IO loop] [INFO] [root] Parallel._print called\r\n[2019-05-17 08:18:51,500] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,500] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,595] [22/Thread-1] [INFO] [root] Scheduler.handle_task_finished called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler._remove_from_processing called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler.release_resources called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler._add_to_memory called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,596] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,597] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,597] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:51,597] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,597] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,597] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,600] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,601] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,695] [22/Thread-1] [INFO] [root] Scheduler.handle_task_finished called\r\n[2019-05-17 08:18:51,695] [22/Thread-1] [INFO] [root] Scheduler._remove_from_processing called\r\n[2019-05-17 08:18:51,696] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,696] [22/Thread-1] [INFO] [root] Scheduler.release_resources called\r\n[2019-05-17 08:18:51,696] [22/Thread-1] [INFO] [root] Scheduler._add_to_memory called\r\n[2019-05-17 08:18:51,696] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,696] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:51,696] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:51,697] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,697] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,697] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,697] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:51,697] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,697] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:51,697] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:51,701] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,802] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:51,902] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,003] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,104] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,204] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,305] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,407] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,508] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,608] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,709] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,810] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:52,910] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:53,011] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:53,061] [22/Thread-1] [INFO] [root] Scheduler.update_data called\r\n[2019-05-17 08:18:53,062] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:53,062] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:53,062] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:53,076] [22/Thread-1] [INFO] [root] Scheduler.add_keys called\r\n[2019-05-17 08:18:53,076] [22/Thread-1] [INFO] [root] Scheduler.add_keys called\r\n[2019-05-17 08:18:53,076] [22/Thread-1] [INFO] [root] Scheduler.add_keys called\r\n[2019-05-17 08:18:53,089] [22/Thread-1] [INFO] [root] Scheduler.add_client called\r\ndistributed.scheduler - INFO - Receive client connection: Client-worker-6830848a-787c-11e9-8016-0a580a0c034b\r\ndistributed.core - INFO - Starting established connection\r\n[2019-05-17 08:18:53,111] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:53,169] [22/Thread-1] [INFO] [root] Scheduler.handle_task_finished called\r\n[2019-05-17 08:18:53,169] [22/Thread-1] [INFO] [root] Scheduler._remove_from_processing called\r\n[2019-05-17 08:18:53,169] [22/Thread-1] [INFO] [root] Scheduler.check_idle_saturated called\r\n[2019-05-17 08:18:53,170] [22/Thread-1] [INFO] [root] Scheduler.release_resources called\r\n[2019-05-17 08:18:53,170] [22/Thread-1] [INFO] [root] Scheduler._add_to_memory called\r\n[2019-05-17 08:18:53,170] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:53,170] [22/Thread-1] [INFO] [root] Scheduler.transitions called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.worker_send called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.report_on_key called\r\n[2019-05-17 08:18:53,171] [22/Thread-1] [INFO] [root] Scheduler.report called\r\n# continuing indefinitely\r\n</pre>\r\n</details>\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/874", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/874/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/874/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/874/events", "html_url": "https://github.com/joblib/joblib/issues/874", "id": 444627280, "node_id": "MDU6SXNzdWU0NDQ2MjcyODA=", "number": 874, "title": "upgrade to cloudpickle-1.1.1 ?", "user": {"login": "stonebig", "id": 4312421, "node_id": "MDQ6VXNlcjQzMTI0MjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4312421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stonebig", "html_url": "https://github.com/stonebig", "followers_url": "https://api.github.com/users/stonebig/followers", "following_url": "https://api.github.com/users/stonebig/following{/other_user}", "gists_url": "https://api.github.com/users/stonebig/gists{/gist_id}", "starred_url": "https://api.github.com/users/stonebig/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stonebig/subscriptions", "organizations_url": "https://api.github.com/users/stonebig/orgs", "repos_url": "https://api.github.com/users/stonebig/repos", "events_url": "https://api.github.com/users/stonebig/events{/privacy}", "received_events_url": "https://api.github.com/users/stonebig/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-05-15T20:20:29Z", "updated_at": "2019-05-29T15:23:59Z", "closed_at": "2019-05-29T15:23:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/865", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/865/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/865/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/865/events", "html_url": "https://github.com/joblib/joblib/issues/865", "id": 429658570, "node_id": "MDU6SXNzdWU0Mjk2NTg1NzA=", "number": 865, "title": "not possible to use joblib as background thread", "user": {"login": "Krzysiaczek99", "id": 6003042, "node_id": "MDQ6VXNlcjYwMDMwNDI=", "avatar_url": "https://avatars1.githubusercontent.com/u/6003042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Krzysiaczek99", "html_url": "https://github.com/Krzysiaczek99", "followers_url": "https://api.github.com/users/Krzysiaczek99/followers", "following_url": "https://api.github.com/users/Krzysiaczek99/following{/other_user}", "gists_url": "https://api.github.com/users/Krzysiaczek99/gists{/gist_id}", "starred_url": "https://api.github.com/users/Krzysiaczek99/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Krzysiaczek99/subscriptions", "organizations_url": "https://api.github.com/users/Krzysiaczek99/orgs", "repos_url": "https://api.github.com/users/Krzysiaczek99/repos", "events_url": "https://api.github.com/users/Krzysiaczek99/events{/privacy}", "received_events_url": "https://api.github.com/users/Krzysiaczek99/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-05T09:16:48Z", "updated_at": "2019-09-10T10:20:04Z", "closed_at": "2019-09-10T10:20:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "when I try to start parallel loop as background thread I'm getting \r\n\r\n/usr/anaconda3/lib/python3.7/site-packages/joblib/parallel.py:673: UserWarning: Loky-backed parallel loops cannot be nested below threads, setting n_jobs=1\r\n  **self._backend_args)\r\n[Parallel(n_jobs=8)]: Using backend SequentialBackend with 1 concurrent workers.\r\n\r\nAttempt to rename thread to 'MainThread' don't help", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/859", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/859/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/859/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/859/events", "html_url": "https://github.com/joblib/joblib/issues/859", "id": 424557654, "node_id": "MDU6SXNzdWU0MjQ1NTc2NTQ=", "number": 859, "title": "Loading very large numpy arrays from file (in windows 10) fail with \"ValueError: negative dimensions are not allowed\"", "user": {"login": "AndreCAndersen", "id": 849449, "node_id": "MDQ6VXNlcjg0OTQ0OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/849449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreCAndersen", "html_url": "https://github.com/AndreCAndersen", "followers_url": "https://api.github.com/users/AndreCAndersen/followers", "following_url": "https://api.github.com/users/AndreCAndersen/following{/other_user}", "gists_url": "https://api.github.com/users/AndreCAndersen/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreCAndersen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreCAndersen/subscriptions", "organizations_url": "https://api.github.com/users/AndreCAndersen/orgs", "repos_url": "https://api.github.com/users/AndreCAndersen/repos", "events_url": "https://api.github.com/users/AndreCAndersen/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreCAndersen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-23T23:54:54Z", "updated_at": "2019-09-12T11:17:51Z", "closed_at": "2019-09-12T10:17:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "In windows 10 and joblib 0.13.2, when dumping and loading a large numpy array (16GB) joblib fails with:\r\n\r\n    ValueError: negative dimensions are not allowed\r\n\r\nThe code to do this is simple:\r\n\r\n    import joblib\r\n    joblib.dump(big_array, '/very/big/array.pkl')\r\n    loaded_big_array = joblib.load('/very/big/array.pkl')\r\n\r\nWhat is happening is that joblib calculates the size by multiplying the shape values of the array. These values are understood as `int32`, so you get a integer overflow when the shape components are large. The offending line is here: \r\n\r\nhttps://github.com/joblib/joblib/blob/master/joblib/numpy_pickle.py#L115\r\n\r\nMy workaround is to monkey patch that class with:\r\n\r\n    shape64 = np.asarray(self.shape, dtype='int64')\r\n    count = unpickler.np.multiply.reduce(shape64)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/857", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/857/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/857/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/857/events", "html_url": "https://github.com/joblib/joblib/issues/857", "id": 419418103, "node_id": "MDU6SXNzdWU0MTk0MTgxMDM=", "number": 857, "title": "Joblib is skipping one job and the last job just stalls. I see it in log it shows only one job is done", "user": {"login": "iqramali", "id": 35267920, "node_id": "MDQ6VXNlcjM1MjY3OTIw", "avatar_url": "https://avatars0.githubusercontent.com/u/35267920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iqramali", "html_url": "https://github.com/iqramali", "followers_url": "https://api.github.com/users/iqramali/followers", "following_url": "https://api.github.com/users/iqramali/following{/other_user}", "gists_url": "https://api.github.com/users/iqramali/gists{/gist_id}", "starred_url": "https://api.github.com/users/iqramali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iqramali/subscriptions", "organizations_url": "https://api.github.com/users/iqramali/orgs", "repos_url": "https://api.github.com/users/iqramali/repos", "events_url": "https://api.github.com/users/iqramali/events{/privacy}", "received_events_url": "https://api.github.com/users/iqramali/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-03-11T11:20:45Z", "updated_at": "2019-12-12T21:29:00Z", "closed_at": "2019-12-12T21:27:50Z", "author_association": "NONE", "active_lock_reason": null, "body": "The last job always stalls in windows don't know why, I removed pref='threads' I get the same problem. Anyone can tell me what is causing this issue?\r\n\r\nHere total_count is dyanmic right now its 2.\r\n\r\n```\r\n  def multi_thread(self, MainWindow, total_count):\r\n        parallel_io_list = [uart1_test, uart2_test, uart3_test, uart4_test, uart5_test,\r\n                            uart6_test, uart7_test, uart8_test, uart9_test, uart10_test]\r\n\r\n        with Parallel(n_jobs=total_count, prefer=\"threads\", verbose=80) as parallel:\r\n            test_results = parallel(delayed(parallel_io_list[i])()\r\n                                    for i in range(total_count))\r\n\r\n        print('IQ-uart %s' % test_results[0])\r\n        run_test_list = [self.run_iot_uart1, self.run_iot_uart2, self.run_iot_uart3, self.run_iot_uart4, self.run_iot_uart5,\r\n                         self.run_iot_uart6, self.run_iot_uart7, self.run_iot_uart8, self.run_iot_uart9, self.run_iot_uart10]\r\n\r\n        for i in range(total_count):\r\n            run_test_list[i](MainWindow, test_results[i])\r\n            self.progress_uart(100)\r\n\r\n        sys.stdout.flush()\r\n```\r\n\r\nLog\r\n\r\n```\r\nPlatform: win32\r\n['COM25', 'COM26']\r\nTotal UART connected 2\r\n[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\r\nUART1: COM25\r\nWriting bytearray(b'Z\\xa5\\x01 =\\xff\\x01\\x00\\xa1\\xfe')\r\nUART2: COM26\r\nWriting bytearray(b'Z\\xa5\\x01 =\\xff\\x01\\x00\\xa1\\xfe')\r\nTEST DONE *****\r\n[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   44.1s\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/855", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/855/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/855/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/855/events", "html_url": "https://github.com/joblib/joblib/issues/855", "id": 418701375, "node_id": "MDU6SXNzdWU0MTg3MDEzNzU=", "number": 855, "title": "Joblib is skipping one job and its not running, don't know why?", "user": {"login": "iqramali", "id": 35267920, "node_id": "MDQ6VXNlcjM1MjY3OTIw", "avatar_url": "https://avatars0.githubusercontent.com/u/35267920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iqramali", "html_url": "https://github.com/iqramali", "followers_url": "https://api.github.com/users/iqramali/followers", "following_url": "https://api.github.com/users/iqramali/following{/other_user}", "gists_url": "https://api.github.com/users/iqramali/gists{/gist_id}", "starred_url": "https://api.github.com/users/iqramali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iqramali/subscriptions", "organizations_url": "https://api.github.com/users/iqramali/orgs", "repos_url": "https://api.github.com/users/iqramali/repos", "events_url": "https://api.github.com/users/iqramali/events{/privacy}", "received_events_url": "https://api.github.com/users/iqramali/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-08T09:21:58Z", "updated_at": "2019-03-08T15:58:19Z", "closed_at": "2019-03-08T15:58:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "Total UART connected 10. I want to read the data from 10 uart parallely but the 9th one is missing anyone can help me, why joblib skips the last one?\r\n\r\nwhy it's repeating the last job?\r\n\r\nCode:\r\n\r\n```\r\n        parallel_io_list = [uart1_test, uart2_test, uart3_test, uart4_test, uart5_test,\r\n                            uart6_test, uart7_test, uart8_test, uart9_test, uart10_test]\r\n\r\n        with Parallel(n_jobs=total_count, verbose=80) as parallel:\r\n            test_results = parallel(delayed(parallel_io_list[i])()\r\n                                    for i in range(total_count))\r\n```\r\n\r\nWhy 9th job is not printing?\r\n\r\nLogs:\r\n\r\n```\r\n[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\r\n[Parallel(n_jobs=10)]: Done   1 tasks      | elapsed:    0.6s\r\n[Parallel(n_jobs=10)]: Done   2 out of  10 | elapsed:    0.6s remaining:    2.8s\r\n[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    0.6s remaining:    1.6s\r\n[Parallel(n_jobs=10)]: Done   4 out of  10 | elapsed:    0.6s remaining:    1.0s\r\n[Parallel(n_jobs=10)]: Done   5 out of  10 | elapsed:    0.6s remaining:    0.6s\r\n[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:    0.7s remaining:    0.4s\r\n[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:    0.7s remaining:    0.2s\r\n[Parallel(n_jobs=10)]: Done   8 out of  10 | elapsed:    0.7s remaining:    0.1s\r\n[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.7s remaining:    0.0s\r\n[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.7s finished\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/852", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/852/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/852/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/852/events", "html_url": "https://github.com/joblib/joblib/issues/852", "id": 415517189, "node_id": "MDU6SXNzdWU0MTU1MTcxODk=", "number": 852, "title": "KeyError with joblib and sklearn cross_validate", "user": {"login": "ogrisel", "id": 89061, "node_id": "MDQ6VXNlcjg5MDYx", "avatar_url": "https://avatars0.githubusercontent.com/u/89061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ogrisel", "html_url": "https://github.com/ogrisel", "followers_url": "https://api.github.com/users/ogrisel/followers", "following_url": "https://api.github.com/users/ogrisel/following{/other_user}", "gists_url": "https://api.github.com/users/ogrisel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ogrisel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ogrisel/subscriptions", "organizations_url": "https://api.github.com/users/ogrisel/orgs", "repos_url": "https://api.github.com/users/ogrisel/repos", "events_url": "https://api.github.com/users/ogrisel/events{/privacy}", "received_events_url": "https://api.github.com/users/ogrisel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 22, "created_at": "2019-02-28T09:08:32Z", "updated_at": "2019-09-10T12:13:11Z", "closed_at": "2019-09-10T12:13:11Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "This was originally reported as https://github.com/dask/distributed/issues/2532\r\n\r\nI tried be reproduce it on joblib master with scikit-learn 0.20.2 as follows.\r\n\r\n```python\r\nimport os\r\nos.environ['SKLEARN_SITE_JOBLIB'] = \"1\"\r\nfrom dask.distributed import Client\r\nfrom sklearn import datasets, linear_model\r\nfrom sklearn.model_selection import cross_validate\r\nimport joblib\r\n\r\n\r\nclient = Client(processes=False)\r\njoblib.parallel_backend('dask')\r\n\r\ndiabetes = datasets.load_diabetes()\r\nX = diabetes.data[:150]\r\ny = diabetes.target[:150]\r\nmodel = linear_model.LinearRegression()\r\n\r\ncv_results = cross_validate(model, X, y, cv=10, return_train_score=False,\r\n                            verbose=100)\r\n```\r\n\r\nHowever this seem to freeze without reporting the original error. Instead when I interrupt with ctrl-c I get:\r\n\r\n```\r\n[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 4 concurrent workers.\r\n[CV]  ................................................................\r\n[CV] .................................... , score=0.587, total=   0.0s\r\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.0s\r\n^CTraceback (most recent call last):\r\n  File \"/home/ogrisel/code/joblib/joblib/_dask.py\", line 223, in maybe_to_futures\r\n    f = call_data_futures[arg]\r\n  File \"/home/ogrisel/code/joblib/joblib/_dask.py\", line 56, in __getitem__\r\n    ref, val = self._data[id(obj)]\r\nKeyError: 140545475489024\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ogrisel/tmp/joblib_dask_freeze.py\", line 18, in <module>\r\n    verbose=100)\r\n  File \"/home/ogrisel/code/scikit-learn/sklearn/model_selection/_validation.py\", line 231, in cross_validate\r\n    for train, test in cv.split(X, y, groups))\r\n  File \"/home/ogrisel/code/joblib/joblib/parallel.py\", line 924, in __call__\r\n    while self.dispatch_one_batch(iterator):\r\n  File \"/home/ogrisel/code/joblib/joblib/parallel.py\", line 759, in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n  File \"/home/ogrisel/code/joblib/joblib/parallel.py\", line 716, in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n  File \"/home/ogrisel/code/joblib/joblib/_dask.py\", line 254, in apply_async\r\n    func, args = self._to_func_args(func)\r\n  File \"/home/ogrisel/code/joblib/joblib/_dask.py\", line 243, in _to_func_args\r\n    args = list(maybe_to_futures(args))\r\n  File \"/home/ogrisel/code/joblib/joblib/_dask.py\", line 231, in maybe_to_futures\r\n    [f] = self.client.scatter([arg])\r\n  File \"/home/ogrisel/code/distributed/distributed/client.py\", line 1875, in scatter\r\n    asynchronous=asynchronous, hash=hash)\r\n  File \"/home/ogrisel/code/distributed/distributed/client.py\", line 676, in sync\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"/home/ogrisel/code/distributed/distributed/utils.py\", line 275, in sync\r\n    e.wait(10)\r\n  File \"/opt/python3.7/lib/python3.7/threading.py\", line 552, in wait\r\n    signaled = self._cond.wait(timeout)\r\n  File \"/opt/python3.7/lib/python3.7/threading.py\", line 300, in wait\r\n    gotit = waiter.acquire(True, timeout)\r\nKeyboardInterrupt\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/848", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/848/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/848/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/848/events", "html_url": "https://github.com/joblib/joblib/issues/848", "id": 411162526, "node_id": "MDU6SXNzdWU0MTExNjI1MjY=", "number": 848, "title": "Not able to pickle QWindow  object when using joblib in my PyQT5 GUI. How to s", "user": {"login": "iqramali", "id": 35267920, "node_id": "MDQ6VXNlcjM1MjY3OTIw", "avatar_url": "https://avatars0.githubusercontent.com/u/35267920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iqramali", "html_url": "https://github.com/iqramali", "followers_url": "https://api.github.com/users/iqramali/followers", "following_url": "https://api.github.com/users/iqramali/following{/other_user}", "gists_url": "https://api.github.com/users/iqramali/gists{/gist_id}", "starred_url": "https://api.github.com/users/iqramali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iqramali/subscriptions", "organizations_url": "https://api.github.com/users/iqramali/orgs", "repos_url": "https://api.github.com/users/iqramali/repos", "events_url": "https://api.github.com/users/iqramali/events{/privacy}", "received_events_url": "https://api.github.com/users/iqramali/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-02-17T08:31:08Z", "updated_at": "2019-02-18T09:17:42Z", "closed_at": "2019-02-18T09:17:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "I get the following error, can somone tell me how can I solve this issue and want to know what is causing this issue?\r\n\r\nHow can I serialize the method into a pickleable class?\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport sys\r\nfrom PyQt5 import QtCore, QtWidgets\r\nfrom joblib import Parallel, delayed\r\nimport threading\r\n\r\nclass Window(QtWidgets.QMainWindow):\r\n\r\n    #uart_list = [1,2,3,4,5]\r\n\r\n    def __init__(self):\r\n        super(Window, self).__init__()\r\n\r\n        self.x_size = 800\r\n        self.y_size = 650\r\n        self.setFixedSize(self.x_size, self.y_size)\r\n        self.setWindowTitle(\"Test\")\r\n\r\n        buttons = list()\r\n\r\n        self.button1 = QtWidgets.QPushButton(\"ONE\", self)\r\n        self.button1.move(100,100)\r\n        buttons.append(self.button1)\r\n\r\n        self.button2 = QtWidgets.QPushButton(\"TWO\", self)\r\n        self.button2.move(100,150)\r\n        buttons.append(self.button2)\r\n\r\n        for button in buttons:\r\n            button.clicked.connect(self.button_click_parallel)\r\n\r\n        self.show()\r\n\r\n    def run_iot_uart1(self):\r\n        print(\"Job 1\")\r\n\r\n    def run_iot_uart2(self):\r\n        print(\"Job 2\")\r\n\r\n    def run_iot_uart3(self):\r\n        print(\"Job 3\")\r\n\r\n    def run_iot_uart4(self):\r\n        print(\"Job 4\")\r\n\r\n    def run_iot_uart5(self):\r\n        print(\"Job 5\")\r\n\r\n    def run_iot_uart6(self):\r\n        print(\"Job 6\")\r\n\r\n    def run_iot_uart7(self):\r\n        print(\"Job 7\")\r\n\r\n    def run_iot_uart8(self):\r\n        print(\"Job 8\")\r\n\r\n    def run_iot_uart9(self):\r\n        print(\"Job 9\")\r\n\r\n    def run_iot_uart10(self):\r\n        print(\"Job 10\")\r\n\r\n\r\n    def button_click_parallel(self): # Click any button, process all UART in parallel\r\n        command_list=[self.run_iot_uart1, self.run_iot_uart2, self.run_iot_uart3,\r\n                      self.run_iot_uart4, self.run_iot_uart5, self.run_iot_uart6,\r\n                      self.run_iot_uart7, self.run_iot_uart8, self.run_iot_uart9,\r\n                      self.run_iot_uart10]\r\n        #Method1:     raise self._exception_pickle.PicklingError: Could not pickle the task to send it to the workers.              \r\n        Parallel(n_jobs=10)(\r\n            delayed(command_list[i])()\r\n            for i in range(10)\r\n        )\r\n        #Method2 TypeError: cannot unpack non-iterable function object\r\n        #Parallel(n_jobs=8)(delayed(command_list[i])() for i in range(10))\r\n        #sys.stdout.flush()\r\n        \r\n\r\ndef run():\r\n    QtWidgets.QApplication.setAttribute(QtCore.Qt.AA_EnableHighDpiScaling, True)\r\n    app = QtWidgets.QApplication(sys.argv)\r\n    GUI = Window()\r\n    sys.exit(app.exec_())\r\n\r\nrun()\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/844", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/844/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/844/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/844/events", "html_url": "https://github.com/joblib/joblib/issues/844", "id": 409077788, "node_id": "MDU6SXNzdWU0MDkwNzc3ODg=", "number": 844, "title": "How to solve randomly hang ?", "user": {"login": "eromoe", "id": 3938751, "node_id": "MDQ6VXNlcjM5Mzg3NTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/3938751?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eromoe", "html_url": "https://github.com/eromoe", "followers_url": "https://api.github.com/users/eromoe/followers", "following_url": "https://api.github.com/users/eromoe/following{/other_user}", "gists_url": "https://api.github.com/users/eromoe/gists{/gist_id}", "starred_url": "https://api.github.com/users/eromoe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eromoe/subscriptions", "organizations_url": "https://api.github.com/users/eromoe/orgs", "repos_url": "https://api.github.com/users/eromoe/repos", "events_url": "https://api.github.com/users/eromoe/events{/privacy}", "received_events_url": "https://api.github.com/users/eromoe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-02-12T01:58:39Z", "updated_at": "2019-03-25T02:16:21Z", "closed_at": "2019-03-25T02:16:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI have a daily job to forecast product salecount every day. \r\nIt need 3 of 32 cores aws emr  instances to run 6 hours. \r\n\r\nThe problem is I found sometimes , one machine would randomly hang.\r\nIt happen several times. \r\nToday, one hang again, so I decide to work it out. \r\n\r\nI login in to that hang machine and found there is only one process with one core cpu 100%(other cores are empty):\r\n```\r\n   PID USER      PRI  NI  VIRT   RES   SHR S CPU% MEM%   TIME+  Command\r\n  15947 hadoop     20   0 2654M  915M 71660 R 100.  1.5  7h56:08 /home/hadoop/miniconda3/envs/py3/bin/python -m joblib.externals.loky.backend.popen_loky_posix --process-name LokyProcess-26 --pipe 44 --semaphore 11\r\n\r\n```\r\n\r\nBut I don't know how to debug the problem .\r\n\r\n\r\nThe last log is : \r\n\r\n```\r\n[INFO][/mnt/var/lib/hadoop/steps/s-1VEEG1LV4ZIGA/sales-forecast/forecast.py:195] [2019-02-12 08:23:52,423] Mission Complete !\r\n```\r\n\r\nBut now is 2019-02-12 10:22", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/842", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/842/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/842/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/842/events", "html_url": "https://github.com/joblib/joblib/issues/842", "id": 407473717, "node_id": "MDU6SXNzdWU0MDc0NzM3MTc=", "number": 842, "title": "Nested Parallel", "user": {"login": "KarenChen9999", "id": 36120522, "node_id": "MDQ6VXNlcjM2MTIwNTIy", "avatar_url": "https://avatars3.githubusercontent.com/u/36120522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KarenChen9999", "html_url": "https://github.com/KarenChen9999", "followers_url": "https://api.github.com/users/KarenChen9999/followers", "following_url": "https://api.github.com/users/KarenChen9999/following{/other_user}", "gists_url": "https://api.github.com/users/KarenChen9999/gists{/gist_id}", "starred_url": "https://api.github.com/users/KarenChen9999/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KarenChen9999/subscriptions", "organizations_url": "https://api.github.com/users/KarenChen9999/orgs", "repos_url": "https://api.github.com/users/KarenChen9999/repos", "events_url": "https://api.github.com/users/KarenChen9999/events{/privacy}", "received_events_url": "https://api.github.com/users/KarenChen9999/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-02-06T23:10:45Z", "updated_at": "2019-02-28T08:43:04Z", "closed_at": "2019-02-28T08:43:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to run nested parallel to reduce overhead in my functions. I have tested nested parallel in simple functions but it didn't work. It forces n_job to 1. some suggests to do \"threading.current_thread().name = 'MainThread\", but i am not sure how to do this. My 1st question: is it possible to run nested parallel using joblib? My 2nd question: if it is possible, how can i make it work?\r\n\r\nMy sample code:\r\n```python\r\ndef fun1(n):\r\n    storm_claim_b = pd.read_csv(csv_input_path+'STO_freq_b_up1.csv', nrows=100000)\r\n    fun2(n)\r\n    return\r\n\r\ndef fun2(n):\r\n    result = Parallel(n_jobs=6)(delayed(fun3)(i) for i in range(10))    \r\n    return\r\n\r\ndef fun3(n):\r\n    storm_claim_b = pd.read_csv(csv_input_path+'STO_freq_b_up1.csv', nrows=100000)\r\n    return\r\nresult = Parallel(n_jobs=6)(delayed(fun1)(i) for i in range(10))\r\n```\r\nwarning messages:\r\n/apps/anaconda3/envs/vectorized3/lib/python3.7/site-packages/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\r\n  **self._backend_args)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/841", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/841/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/841/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/841/events", "html_url": "https://github.com/joblib/joblib/issues/841", "id": 407274413, "node_id": "MDU6SXNzdWU0MDcyNzQ0MTM=", "number": 841, "title": "joblib creates a semaphore at import time, resulting in spawning semaphore tracker", "user": {"login": "frankier", "id": 299380, "node_id": "MDQ6VXNlcjI5OTM4MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/299380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frankier", "html_url": "https://github.com/frankier", "followers_url": "https://api.github.com/users/frankier/followers", "following_url": "https://api.github.com/users/frankier/following{/other_user}", "gists_url": "https://api.github.com/users/frankier/gists{/gist_id}", "starred_url": "https://api.github.com/users/frankier/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frankier/subscriptions", "organizations_url": "https://api.github.com/users/frankier/orgs", "repos_url": "https://api.github.com/users/frankier/repos", "events_url": "https://api.github.com/users/frankier/events{/privacy}", "received_events_url": "https://api.github.com/users/frankier/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-02-06T14:56:50Z", "updated_at": "2019-06-18T20:54:37Z", "closed_at": "2019-06-18T20:54:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "### Current behaviour:\r\n\r\nJoblib creates (and deletes) a multiprocessing semaphore at import time here: \r\n\r\nhttps://github.com/joblib/joblib/blob/d4ee44ada1abfe3ebc2a653796aecaebe48ab931/joblib/_multiprocessing_helpers.py#L34-L35\r\n\r\nOn Unix this will spawn the semaphore tracker. There are a couple of issues with this:\r\n\r\n 1. Processes embedding Python, notably uwsgi, will probably not spawn the semaphore tracker correctly. To make this work correctly, the embedder would have to set https://docs.python.org/3/library/multiprocessing.html#multiprocessing.set_executable -- but they may not be aware that multiprocessing is being used in the first place since it could be a transitive import. Uwsgi for example will print a message like this: https://github.com/joblib/joblib/issues/840\r\n 2. It might be taking more time than a simple import would be expected to take. Some packages (notably sklearn) import joblib unconditionally, whether it is used or not. The cumulative effect on startup time might be worth avoiding.\r\n\r\n### Desired behaviour:\r\n\r\nIdeally creation of the the semaphore tracker process would be delayed until joblib is actually used. The check for os semaphore support could either by performed:\r\n\r\n  1. Using an alternative check creation of a raw semaphore or feature sniffing rather than just trying it out OR\r\n  2. Lazily\r\n\r\n### Workaround:\r\n\r\nSet JOBLIB_MULTIPROCESSING=0", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/839", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/839/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/839/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/839/events", "html_url": "https://github.com/joblib/joblib/issues/839", "id": 403240324, "node_id": "MDU6SXNzdWU0MDMyNDAzMjQ=", "number": 839, "title": "'numpy.ndarray' object has no attribute 'offset' (memmap)", "user": {"login": "saimn", "id": 311639, "node_id": "MDQ6VXNlcjMxMTYzOQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/311639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saimn", "html_url": "https://github.com/saimn", "followers_url": "https://api.github.com/users/saimn/followers", "following_url": "https://api.github.com/users/saimn/following{/other_user}", "gists_url": "https://api.github.com/users/saimn/gists{/gist_id}", "starred_url": "https://api.github.com/users/saimn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saimn/subscriptions", "organizations_url": "https://api.github.com/users/saimn/orgs", "repos_url": "https://api.github.com/users/saimn/repos", "events_url": "https://api.github.com/users/saimn/events{/privacy}", "received_events_url": "https://api.github.com/users/saimn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-01-25T16:54:26Z", "updated_at": "2019-05-20T13:24:34Z", "closed_at": "2019-05-20T13:24:34Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Hi,\r\n\r\nI get an error, (similar to #161 but with Python 3.7, Numpy 1.16, Joblib 0.13.1), while serializing memmap arrays (traceback below).\r\n\r\nThis arrays is read with `astropy.io.fits`, which was using `np.memmap` but switch to using `mmap.mmap` directly (https://github.com/astropy/astropy/pull/7597, this may explain why I never encountered this error before). So basically the array is constructed like this:\r\n```\r\nself._mmap = mmap.mmap(self._file.fileno(), 0, access=access_mode, offset=0)\r\nnp.ndarray(shape=shape, dtype=dtype, offset=offset, buffer=self._mmap)\r\n```\r\n(https://github.com/astropy/astropy/blob/master/astropy/io/fits/file.py#L309-L334)\r\n\r\nAnd then the array does not have an `.offset` attribute, which is specific to `np.memmap`. \r\nSo I think that it should be safe to check this exception and use 0 as a default value for offset ?\r\n\r\n\r\n```\r\njoblib.externals.loky.process_executor._RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 344, in _sendback_result\r\n    exception=exception))\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/site-packages/joblib/externals/loky/backend/queues.py\", line 234, in put\r\n    obj = dumps(obj, reducers=self._reducers)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/site-packages/joblib/externals/loky/backend/reduction.py\", line 243, in dumps\r\n    dump(obj, buf, reducers=reducers, protocol=protocol)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/site-packages/joblib/externals/loky/backend/reduction.py\", line 236, in dump\r\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/site-packages/joblib/externals/cloudpickle/cloudpickle.py\", line 284, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 856, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 882, in _batch_setitems\r\n    save(v)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 816, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 856, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 882, in _batch_setitems\r\n    save(v)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 856, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 882, in _batch_setitems\r\n    save(v)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 856, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 882, in _batch_setitems\r\n    save(v)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 856, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 882, in _batch_setitems\r\n    save(v)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/pickle.py\", line 510, in save\r\n    rv = reduce(obj)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/site-packages/joblib/_memmapping_reducer.py\", line 304, in __call__\r\n    return _reduce_memmap_backed(a, m)\r\n  File \"/home/simon/.pyenv/versions/3.7.2/lib/python3.7/site-packages/joblib/_memmapping_reducer.py\", line 218, in _reduce_memmap_backed\r\n    offset += m.offset\r\nAttributeError: 'numpy.ndarray' object has no attribute 'offset'\r\n\"\"\"\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/836", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/836/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/836/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/836/events", "html_url": "https://github.com/joblib/joblib/issues/836", "id": 402147646, "node_id": "MDU6SXNzdWU0MDIxNDc2NDY=", "number": 836, "title": "Loky cache should be disabled by default", "user": {"login": "amogil", "id": 1420036, "node_id": "MDQ6VXNlcjE0MjAwMzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1420036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amogil", "html_url": "https://github.com/amogil", "followers_url": "https://api.github.com/users/amogil/followers", "following_url": "https://api.github.com/users/amogil/following{/other_user}", "gists_url": "https://api.github.com/users/amogil/gists{/gist_id}", "starred_url": "https://api.github.com/users/amogil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amogil/subscriptions", "organizations_url": "https://api.github.com/users/amogil/orgs", "repos_url": "https://api.github.com/users/amogil/repos", "events_url": "https://api.github.com/users/amogil/events{/privacy}", "received_events_url": "https://api.github.com/users/amogil/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-01-23T09:35:10Z", "updated_at": "2019-02-17T00:07:39Z", "closed_at": "2019-02-17T00:07:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "Joblib uses loky as a backend by default. Loky reuses previously forked processes during consequent runs. This may lead to unobvious hard to track bugs. For example:\r\n\r\n```\r\nimport joblib\r\nfrom joblib import Parallel, delayed\\\r\n\r\nprint(f\"Joblib version: {joblib.__version__}\")\r\n\r\ndef process(i):\r\n    return f\"{chunk} - {i}\"\r\n\r\nfor chunk in ['chunk1', 'chunk2', 'chunk3']:\r\n    print(f\"----------- {chunk} -----------\")\r\n    result = Parallel(n_jobs=-1)(delayed(process)(i) for i in range(5))\r\n    print(result)\r\n```\r\n\r\nproduces\r\n\r\n```\r\nJoblib version: 0.13.0\r\n----------- chunk1 -----------\r\n['chunk1 - 0', 'chunk1 - 1', 'chunk1 - 2', 'chunk1 - 3', 'chunk1 - 4']\r\n----------- chunk2 -----------\r\n['chunk1 - 0', 'chunk1 - 1', 'chunk1 - 2', 'chunk1 - 3', 'chunk2 - 4']\r\n----------- chunk3 -----------\r\n['chunk1 - 0', 'chunk1 - 1', 'chunk3 - 2', 'chunk1 - 3', 'chunk1 - 4']\r\n```\r\n\r\nPlease note that only the \"chunk1\" string was printed.\r\n\r\nIn contrast, when 'multiprocessing' backend is being used:\r\n\r\n```\r\nimport joblib\r\nfrom joblib import Parallel, delayed\\\r\n\r\nprint(f\"Joblib version: {joblib.__version__}\")\r\n\r\ndef process(i):\r\n    return f\"{chunk} - {i}\"\r\n\r\nfor chunk in ['chunk1', 'chunk2', 'chunk3']:\r\n    print(f\"----------- {chunk} -----------\")\r\n    result = Parallel(n_jobs=-1, backend='multiprocessing')(delayed(process)(i) for i in range(5))\r\n    print(result)\r\n```\r\n\r\ngives\r\n\r\n```\r\nJoblib version: 0.13.0\r\n----------- chunk1 -----------\r\n['chunk1 - 0', 'chunk1 - 1', 'chunk1 - 2', 'chunk1 - 3', 'chunk1 - 4']\r\n----------- chunk2 -----------\r\n['chunk2 - 0', 'chunk2 - 1', 'chunk2 - 2', 'chunk2 - 3', 'chunk2 - 4']\r\n----------- chunk3 -----------\r\n['chunk3 - 0', 'chunk3 - 1', 'chunk3 - 2', 'chunk3 - 3', 'chunk3 - 4']\r\n```\r\n\r\nIt works as expected.\r\n\r\nLoky doesn't spawn processes again and reuses them from the previous run. The `chunk` variable and `for cycle` here is just a synthetic example. It could be everything from any read-only data from the master process to libraries' imports. Consequent joblib's usages can happen in different parts of an app and can lead to really hard to track bugs.\r\n\r\nAs far as I understand, we can't control this behavior using joblib options, it's joblib implementation details which are not seen to joblib users.\r\n\r\nI think it would be better to disable loky process cache by default.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/833", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/833/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/833/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/833/events", "html_url": "https://github.com/joblib/joblib/issues/833", "id": 399586495, "node_id": "MDU6SXNzdWUzOTk1ODY0OTU=", "number": 833, "title": "joblib.Parallel overwrites args variable for n_jobs > 1", "user": {"login": "mgbckr", "id": 1241516, "node_id": "MDQ6VXNlcjEyNDE1MTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1241516?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mgbckr", "html_url": "https://github.com/mgbckr", "followers_url": "https://api.github.com/users/mgbckr/followers", "following_url": "https://api.github.com/users/mgbckr/following{/other_user}", "gists_url": "https://api.github.com/users/mgbckr/gists{/gist_id}", "starred_url": "https://api.github.com/users/mgbckr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mgbckr/subscriptions", "organizations_url": "https://api.github.com/users/mgbckr/orgs", "repos_url": "https://api.github.com/users/mgbckr/repos", "events_url": "https://api.github.com/users/mgbckr/events{/privacy}", "received_events_url": "https://api.github.com/users/mgbckr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-01-15T23:49:24Z", "updated_at": "2019-02-17T00:08:00Z", "closed_at": "2019-02-17T00:08:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "joblib version: 0.13.0\r\n\r\nIn the following example, when I set `n_jobs>1`, then the args from the outer scope will be overwritten by a Paralel namespace for  `n_jobs=1` this does not happen. Other variable names are not effected. This a) is not consistent and b) generally, I think Parallel should not overwrite _any_ variable. Thanks :)\r\n\r\n```python\r\nimport joblib\r\n\r\nargs=5\r\ndef func():\r\n    return args\r\n\r\njoblib.Parallel(n_jobs=2)(joblib.delayed(func)() for i in range(1))\r\n```\r\n\r\n**Update:**\r\n\r\nI am using Ubuntu:\r\n```bash\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.1 LTS (Bionic Beaver)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 18.04.1 LTS\"\r\nVERSION_ID=\"18.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=bionic\r\nUBUNTU_CODENAME=bionic\r\n```\r\n\r\nKernel:\r\n```\r\nLinux 4.15.0-43-generic x86_64\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/828", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/828/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/828/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/828/events", "html_url": "https://github.com/joblib/joblib/issues/828", "id": 396570448, "node_id": "MDU6SXNzdWUzOTY1NzA0NDg=", "number": 828, "title": "Dask backend returns early if no workers allocated", "user": {"login": "jcrist", "id": 2783717, "node_id": "MDQ6VXNlcjI3ODM3MTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2783717?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcrist", "html_url": "https://github.com/jcrist", "followers_url": "https://api.github.com/users/jcrist/followers", "following_url": "https://api.github.com/users/jcrist/following{/other_user}", "gists_url": "https://api.github.com/users/jcrist/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcrist/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcrist/subscriptions", "organizations_url": "https://api.github.com/users/jcrist/orgs", "repos_url": "https://api.github.com/users/jcrist/repos", "events_url": "https://api.github.com/users/jcrist/events{/privacy}", "received_events_url": "https://api.github.com/users/jcrist/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-01-07T17:05:02Z", "updated_at": "2019-01-14T13:53:50Z", "closed_at": "2019-01-11T14:37:04Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "If the cluster has no workers, the dask backend returns immediately.\r\n\r\n```python\r\nIn [1]: from dask.distributed import Client, LocalCluster\r\n\r\nIn [2]: cluster = LocalCluster(n_workers=0)\r\n\r\nIn [3]: client = Client(cluster)\r\n\r\nIn [4]: import joblib\r\n\r\nIn [5]: with joblib.parallel_backend('dask'):\r\n   ...:     results = joblib.Parallel(verbose=100)(joblib.delayed(lambda x: x**2)(x) for x in range(10))\r\n   ...:\r\n[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 0 concurrent workers.\r\n[Parallel(n_jobs=-1)]: Done   0 out of   0 | elapsed:    0.0s finished\r\n\r\nIn [6]: import dask\r\n\r\nIn [7]: joblib.__version__\r\nOut[7]: '0.13.0'\r\n\r\nIn [8]: dask.__version__\r\nOut[8]: '1.0.0'\r\n```\r\n\r\nIn this case the backend should probably hang until a worker is added, or error immediately. This is complicated since the number of workers could change dynamically during computation. I'm not sure what's best here.\r\n\r\nxref https://github.com/dask/dask-yarn/issues/46", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/823", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/823/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/823/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/823/events", "html_url": "https://github.com/joblib/joblib/issues/823", "id": 391769264, "node_id": "MDU6SXNzdWUzOTE3NjkyNjQ=", "number": 823, "title": "Cach memory for GPU objects", "user": {"login": "amirj", "id": 1645137, "node_id": "MDQ6VXNlcjE2NDUxMzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1645137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amirj", "html_url": "https://github.com/amirj", "followers_url": "https://api.github.com/users/amirj/followers", "following_url": "https://api.github.com/users/amirj/following{/other_user}", "gists_url": "https://api.github.com/users/amirj/gists{/gist_id}", "starred_url": "https://api.github.com/users/amirj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amirj/subscriptions", "organizations_url": "https://api.github.com/users/amirj/orgs", "repos_url": "https://api.github.com/users/amirj/repos", "events_url": "https://api.github.com/users/amirj/events{/privacy}", "received_events_url": "https://api.github.com/users/amirj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-12-17T15:41:50Z", "updated_at": "2018-12-19T15:01:07Z", "closed_at": "2018-12-19T15:01:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "If anybody have any experiment with caching GPU objects on disk?\r\n\r\nI have a function as follow:\r\n```\r\n@memory.cache\r\ndef predict(model, test_dataset):\r\n    return something\r\n```\r\nThe input object is a [pytorch](https://pytorch.org/) model which has been trained on **GPU**.\r\n\r\ncalling the above function leads to a weired error while normally run it (without @memory.cache) has no problem.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/819", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/819/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/819/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/819/events", "html_url": "https://github.com/joblib/joblib/issues/819", "id": 389685999, "node_id": "MDU6SXNzdWUzODk2ODU5OTk=", "number": 819, "title": "No handlers could be found for logger \"concurrent.futures\"", "user": {"login": "zihaozhu93", "id": 20263795, "node_id": "MDQ6VXNlcjIwMjYzNzk1", "avatar_url": "https://avatars1.githubusercontent.com/u/20263795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zihaozhu93", "html_url": "https://github.com/zihaozhu93", "followers_url": "https://api.github.com/users/zihaozhu93/followers", "following_url": "https://api.github.com/users/zihaozhu93/following{/other_user}", "gists_url": "https://api.github.com/users/zihaozhu93/gists{/gist_id}", "starred_url": "https://api.github.com/users/zihaozhu93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zihaozhu93/subscriptions", "organizations_url": "https://api.github.com/users/zihaozhu93/orgs", "repos_url": "https://api.github.com/users/zihaozhu93/repos", "events_url": "https://api.github.com/users/zihaozhu93/events{/privacy}", "received_events_url": "https://api.github.com/users/zihaozhu93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-12-11T10:11:16Z", "updated_at": "2020-02-26T10:13:36Z", "closed_at": "2018-12-11T11:04:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "i wrote a class, and using joblib.Parallel to call its method, like:\r\n\r\nclass A():\r\n  def fun1(xx):\r\n    ...\r\n  def fun2(xx):\r\n    ...\r\n\r\na = A()\r\nParallel(n_jobs=-1)(delayed(a.fun1)(a.fun2(xx)))\r\n\r\nand run several iterations, then terminated with errors:\r\n\r\nNo handlers could be found for logger \"concurrent.futures\",\r\nanyone has any ideas?\r\nthx", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/816", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/816/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/816/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/816/events", "html_url": "https://github.com/joblib/joblib/issues/816", "id": 385185219, "node_id": "MDU6SXNzdWUzODUxODUyMTk=", "number": 816, "title": "Silent failure if Path(\"...\") is passed - BUG", "user": {"login": "N-McA", "id": 8830910, "node_id": "MDQ6VXNlcjg4MzA5MTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/8830910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/N-McA", "html_url": "https://github.com/N-McA", "followers_url": "https://api.github.com/users/N-McA/followers", "following_url": "https://api.github.com/users/N-McA/following{/other_user}", "gists_url": "https://api.github.com/users/N-McA/gists{/gist_id}", "starred_url": "https://api.github.com/users/N-McA/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/N-McA/subscriptions", "organizations_url": "https://api.github.com/users/N-McA/orgs", "repos_url": "https://api.github.com/users/N-McA/repos", "events_url": "https://api.github.com/users/N-McA/events{/privacy}", "received_events_url": "https://api.github.com/users/N-McA/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-28T09:53:04Z", "updated_at": "2018-12-12T16:20:40Z", "closed_at": "2018-12-12T16:20:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following code shows a silent failure when a Path is passed.\r\n\r\n```\r\nfrom pathlib import Path\r\nfrom joblib import Memory\r\n\r\nm1 = Memory(Path('../joblib-cache'), verbose=True)\r\nm2 = Memory('../joblib-cache', verbose=True)\r\n\r\n@m1.cache\r\ndef foo1(x):\r\n    return x\r\n\r\n@m2.cache\r\ndef foo2(x):\r\n    return x\r\n\r\nfoo1(3)\r\nfoo2(3)\r\n```\r\n\r\nPrinting only the verbose joblib output for foo2, and not caching foo1.\r\n\r\n`python --version == 3.7 ; joblib.__version__ == 0.13.0`\r\n\r\nMay have time to produce a pull request tomorrow.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/807", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/807/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/807/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/807/events", "html_url": "https://github.com/joblib/joblib/issues/807", "id": 378833576, "node_id": "MDU6SXNzdWUzNzg4MzM1NzY=", "number": 807, "title": "Exception RuntimeError: RuntimeError('main thread is not in main loop',)", "user": {"login": "tvrbanec", "id": 5420280, "node_id": "MDQ6VXNlcjU0MjAyODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5420280?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tvrbanec", "html_url": "https://github.com/tvrbanec", "followers_url": "https://api.github.com/users/tvrbanec/followers", "following_url": "https://api.github.com/users/tvrbanec/following{/other_user}", "gists_url": "https://api.github.com/users/tvrbanec/gists{/gist_id}", "starred_url": "https://api.github.com/users/tvrbanec/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tvrbanec/subscriptions", "organizations_url": "https://api.github.com/users/tvrbanec/orgs", "repos_url": "https://api.github.com/users/tvrbanec/repos", "events_url": "https://api.github.com/users/tvrbanec/events{/privacy}", "received_events_url": "https://api.github.com/users/tvrbanec/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-11-08T17:13:26Z", "updated_at": "2018-11-20T14:41:12Z", "closed_at": "2018-11-12T20:40:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "In the main program I call modules (five times). In each module I use joblib.Parallel in the same way. If I call only one of these modules from the main program, everything is doing great. If in the main program I choose the option to execute the five modules one by one, in the third of them I get the following error:\r\n\r\n```python-traceback\r\nException RuntimeError: RuntimeError('main thread is not in main loop',) in <bound method PhotoImage.__del__ of <Tkinter.PhotoImage instance at 0x7f72ac673c68>> ignored\r\nTcl_AsyncDelete: async handler deleted by the wrong thread\r\nAborted\r\n```\r\n\r\nWhen only that third subprogram (module) is called, everything is executed without error.\r\njoblib-0.13.0, Debian", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/806", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/806/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/806/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/806/events", "html_url": "https://github.com/joblib/joblib/issues/806", "id": 378796329, "node_id": "MDU6SXNzdWUzNzg3OTYzMjk=", "number": 806, "title": "Windows permission error", "user": {"login": "albertcthomas", "id": 15966638, "node_id": "MDQ6VXNlcjE1OTY2NjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/15966638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertcthomas", "html_url": "https://github.com/albertcthomas", "followers_url": "https://api.github.com/users/albertcthomas/followers", "following_url": "https://api.github.com/users/albertcthomas/following{/other_user}", "gists_url": "https://api.github.com/users/albertcthomas/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertcthomas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertcthomas/subscriptions", "organizations_url": "https://api.github.com/users/albertcthomas/orgs", "repos_url": "https://api.github.com/users/albertcthomas/repos", "events_url": "https://api.github.com/users/albertcthomas/events{/privacy}", "received_events_url": "https://api.github.com/users/albertcthomas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 841761474, "node_id": "MDU6TGFiZWw4NDE3NjE0NzQ=", "url": "https://api.github.com/repos/joblib/joblib/labels/blocker", "name": "blocker", "color": "f91911", "default": false, "description": null}, {"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}, {"id": 1718808083, "node_id": "MDU6TGFiZWwxNzE4ODA4MDgz", "url": "https://api.github.com/repos/joblib/joblib/labels/high%20priority", "name": "high priority", "color": "d93f0b", "default": false, "description": ""}, {"id": 1718822484, "node_id": "MDU6TGFiZWwxNzE4ODIyNDg0", "url": "https://api.github.com/repos/joblib/joblib/labels/windows", "name": "windows", "color": "fef2c0", "default": false, "description": ""}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 44, "created_at": "2018-11-08T15:56:42Z", "updated_at": "2020-06-01T15:40:03Z", "closed_at": "2020-05-03T15:15:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I have a permission error while using Parallel(n_jobs=2) on windows:\r\n````python-traceback\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\site-packages\\joblib\\disk.py:122: UserWarning: Unable to delete folder xxxx\\AppData\\Local\\Temp\\joblib_memmapping_folder_15364_9553521538 after 5 tentatives.\r\n  .format(folder_path, RM_SUBDIRS_N_RETRY))\r\n---------------------------------------------------------------------------\r\nPermissionError                           Traceback (most recent call last)\r\n~\\Desktop\\all_repos\\gitlab_projects\\wireless_optim\\code\\complete_experiment.py in <module>()\r\n    422         simulator_data = np.concatenate([simulator_data, new_simulator_data],\r\n    423                                         axis=0)\r\n--> 424         n_admissible_actions.append(n_admissible_actions_d)\r\n    425\r\n    426 if 0:\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\site-packages\\joblib\\parallel.py in __exit__(self, exc_type, exc_value, traceback)\r\n    664\r\n    665     def __exit__(self, exc_type, exc_value, traceback):\r\n--> 666         self._terminate_backend()\r\n    667         self._managed_backend = False\r\n    668\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\site-packages\\joblib\\parallel.py in _terminate_backend(self)\r\n    694     def _terminate_backend(self):\r\n    695         if self._backend is not None:\r\n--> 696             self._backend.terminate()\r\n    697\r\n    698     def _dispatch(self, batch):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\site-packages\\joblib\\_parallel_backends.py in terminate(self)\r\n    528             # in latter calls but we free as much memory as we can by deleting\r\n    529             # the shared memory\r\n--> 530             delete_folder(self._workers._temp_folder)\r\n    531             self._workers = None\r\n    532\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\site-packages\\joblib\\disk.py in delete_folder(folder_path, onerror)\r\n    113             while True:\r\n    114                 try:\r\n--> 115                     shutil.rmtree(folder_path, False, None)\r\n    116                     break\r\n    117                 except (OSError, WindowsError):\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\shutil.py in rmtree(path, ignore_errors, onerror)\r\n    492             os.close(fd)\r\n    493     else:\r\n--> 494         return _rmtree_unsafe(path, onerror)\r\n    495\r\n    496 # Allow introspection of whether or not the hardening against symlink\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\r\n    387                 os.unlink(fullname)\r\n    388             except OSError:\r\n--> 389                 onerror(os.unlink, fullname, sys.exc_info())\r\n    390     try:\r\n    391         os.rmdir(path)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\wireless_optim\\lib\\shutil.py in _rmtree_unsafe(path, onerror)\r\n    385         else:\r\n    386             try:\r\n--> 387                 os.unlink(fullname)\r\n    388             except OSError:\r\n    389                 onerror(os.unlink, fullname, sys.exc_info())\r\n\r\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'xxxx\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_15364_9553521538\\\\15364-3031207306352-94b02d62d9b44d709a9a405235589ead.pkl'\r\n````\r\nIs this related to the fact that removing a memmap can fail on Windows as said in the doc?\r\nI can try to have a small reproducible example.\r\n\r\nI'm using joblib 0.13.\r\n(FWIW this code is working on Ubuntu 16.04.5 LTS)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/800", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/800/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/800/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/800/events", "html_url": "https://github.com/joblib/joblib/issues/800", "id": 375504907, "node_id": "MDU6SXNzdWUzNzU1MDQ5MDc=", "number": 800, "title": "performance drop with joblib>0.11 due to cloudpickle", "user": {"login": "ernstklrb", "id": 33454758, "node_id": "MDQ6VXNlcjMzNDU0NzU4", "avatar_url": "https://avatars2.githubusercontent.com/u/33454758?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ernstklrb", "html_url": "https://github.com/ernstklrb", "followers_url": "https://api.github.com/users/ernstklrb/followers", "following_url": "https://api.github.com/users/ernstklrb/following{/other_user}", "gists_url": "https://api.github.com/users/ernstklrb/gists{/gist_id}", "starred_url": "https://api.github.com/users/ernstklrb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ernstklrb/subscriptions", "organizations_url": "https://api.github.com/users/ernstklrb/orgs", "repos_url": "https://api.github.com/users/ernstklrb/repos", "events_url": "https://api.github.com/users/ernstklrb/events{/privacy}", "received_events_url": "https://api.github.com/users/ernstklrb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-10-30T14:01:27Z", "updated_at": "2018-11-10T12:29:02Z", "closed_at": "2018-11-10T12:28:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nwith our application we see a really large performance drop of joblib.Parallel when going from version 0.11 to 0.12.x. This is due to cloudpickle. If using 0.12.5 we choose backend=\"multiprocessing\" and edit joblib/parallel.py to use pickle instead of cloudpickle, the old performance is achieved again.\r\n(\"really large performance drop\" meaning a factor of over 10 for the pickling, so that parallelization becomes meaningless)\r\n\r\nSo obviously the data structures we use (own classes, networkx.DiGraph) are not handled well by cloudpickle. One option for us would be to dig into the details and find out exactly what causes cloudpickle to be so slow here.\r\n\r\nOr could you add an option to joblib.Parallel to set the pickling backend in addition to setting the parallelization backend?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/799", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/799/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/799/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/799/events", "html_url": "https://github.com/joblib/joblib/issues/799", "id": 374337502, "node_id": "MDU6SXNzdWUzNzQzMzc1MDI=", "number": 799, "title": "Add a paragraph about serialization/pickle/cloudpickle in doc?", "user": {"login": "albertcthomas", "id": 15966638, "node_id": "MDQ6VXNlcjE1OTY2NjM4", "avatar_url": "https://avatars3.githubusercontent.com/u/15966638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albertcthomas", "html_url": "https://github.com/albertcthomas", "followers_url": "https://api.github.com/users/albertcthomas/followers", "following_url": "https://api.github.com/users/albertcthomas/following{/other_user}", "gists_url": "https://api.github.com/users/albertcthomas/gists{/gist_id}", "starred_url": "https://api.github.com/users/albertcthomas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albertcthomas/subscriptions", "organizations_url": "https://api.github.com/users/albertcthomas/orgs", "repos_url": "https://api.github.com/users/albertcthomas/repos", "events_url": "https://api.github.com/users/albertcthomas/events{/privacy}", "received_events_url": "https://api.github.com/users/albertcthomas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-26T11:13:05Z", "updated_at": "2018-11-08T13:15:26Z", "closed_at": "2018-11-08T13:15:25Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "For process-based parallelism, the current version of the doc only says that \"data need to be serialized in a queue for communication with the worker processes\".\r\n\r\nI think it would be good to add a small paragraph/section about serialization in the doc, especially for the process-based parallelism part, briefly explaining what it is, how it is done in joblib (when pickle is used and when cloudpickle is used) and what this implies (e.g. why some objects might not be picklable).\r\n\r\nWDYT?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/796", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/796/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/796/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/796/events", "html_url": "https://github.com/joblib/joblib/issues/796", "id": 373519709, "node_id": "MDU6SXNzdWUzNzM1MTk3MDk=", "number": 796, "title": "Random failures on Appveyor", "user": {"login": "aabadie", "id": 1375137, "node_id": "MDQ6VXNlcjEzNzUxMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1375137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aabadie", "html_url": "https://github.com/aabadie", "followers_url": "https://api.github.com/users/aabadie/followers", "following_url": "https://api.github.com/users/aabadie/following{/other_user}", "gists_url": "https://api.github.com/users/aabadie/gists{/gist_id}", "starred_url": "https://api.github.com/users/aabadie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aabadie/subscriptions", "organizations_url": "https://api.github.com/users/aabadie/orgs", "repos_url": "https://api.github.com/users/aabadie/repos", "events_url": "https://api.github.com/users/aabadie/events{/privacy}", "received_events_url": "https://api.github.com/users/aabadie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-24T14:42:29Z", "updated_at": "2020-07-01T14:47:18Z", "closed_at": "2020-07-01T14:47:17Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In my last PRs, I got random failures on Appveyor: [here](https://ci.appveyor.com/project/joblib-ci/joblib/builds/19756534/job/grockg2349drc25v), [here](https://ci.appveyor.com/project/joblib-ci/joblib/builds/19757254/job/l4accx6cy843ro42) and [here](https://ci.appveyor.com/project/joblib-ci/joblib/builds/19732120/job/ummyb8xxrisq6y75).\r\n\r\nThe last 2 have happened in `est_no_blas_crash_or_freeze_with_subprocesses` and seem related to a remove attempt on a file which is opened by another process:\r\n```\r\n>                   os.remove(fullname)\r\nE                   WindowsError: [Error 32] The process cannot access the file because it is being used by another process: 'c:\\\\users\\\\appveyor\\\\appdata\\\\local\\\\temp\\\\1\\\\joblib_memmapping_folder_2596_5971308621\\\\2596-118618064-7c5742266e0843c3a08697d3ad392646.pkl'\r\n```\r\n\r\nThe first one is related to dask.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/793", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/793/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/793/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/793/events", "html_url": "https://github.com/joblib/joblib/issues/793", "id": 372646414, "node_id": "MDU6SXNzdWUzNzI2NDY0MTQ=", "number": 793, "title": "Parallel uses all cpus even when n_jobs is set to use fewer", "user": {"login": "hermidalc", "id": 461066, "node_id": "MDQ6VXNlcjQ2MTA2Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/461066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hermidalc", "html_url": "https://github.com/hermidalc", "followers_url": "https://api.github.com/users/hermidalc/followers", "following_url": "https://api.github.com/users/hermidalc/following{/other_user}", "gists_url": "https://api.github.com/users/hermidalc/gists{/gist_id}", "starred_url": "https://api.github.com/users/hermidalc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hermidalc/subscriptions", "organizations_url": "https://api.github.com/users/hermidalc/orgs", "repos_url": "https://api.github.com/users/hermidalc/repos", "events_url": "https://api.github.com/users/hermidalc/events{/privacy}", "received_events_url": "https://api.github.com/users/hermidalc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-22T18:42:08Z", "updated_at": "2018-10-24T19:51:45Z", "closed_at": "2018-10-24T19:51:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "When I use joblib.Parallel within scikit-learn 0.19.2, either directly or via e.g. GridSearchCV, the n_jobs parameter doesn't control the total number of cpus that can be used in parallel, only the number of distinct parallel worker processes and each process will utilize multiple cpus such that all cpus are always utilized no matter what n_jobs is set to.\r\n\r\nFor example, on a 4 core/8 logical cpu setup, if I set n_jobs=2 then Parallel will run two worker processes (which I see in top) but each will utilize approx 400% cpu (so each uses 4 logical cpus).  I cannot get it to just use two logical cpus.  Is this behavior normal and I misunderstanding usage, or is this a bug?\r\n\r\nFedora Linux 4.18.14-200.fc28.x86_64 \r\nAnaconda 5.2.0\r\nPython 3.6.5\r\nscikit-learn 0.19.2 (which I believe uses joblib 0.11)\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/786", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/786/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/786/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/786/events", "html_url": "https://github.com/joblib/joblib/issues/786", "id": 367472155, "node_id": "MDU6SXNzdWUzNjc0NzIxNTU=", "number": 786, "title": "Backporting n_jobs=None to joblib 0.11.1", "user": {"login": "rth", "id": 630936, "node_id": "MDQ6VXNlcjYzMDkzNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/630936?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rth", "html_url": "https://github.com/rth", "followers_url": "https://api.github.com/users/rth/followers", "following_url": "https://api.github.com/users/rth/following{/other_user}", "gists_url": "https://api.github.com/users/rth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rth/subscriptions", "organizations_url": "https://api.github.com/users/rth/orgs", "repos_url": "https://api.github.com/users/rth/repos", "events_url": "https://api.github.com/users/rth/events{/privacy}", "received_events_url": "https://api.github.com/users/rth/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-06T16:21:54Z", "updated_at": "2018-10-17T14:39:21Z", "closed_at": "2018-10-17T10:08:12Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "As mentioned by @ogrisel on Gitter (regarding using joblib <0.12 with latest scikit-learn),\r\n\r\n> The problem is that we cannot easily revert back to to 0.11 unfortunately: the use n_jobs=None change prevents that. We could issue a joblib 0.11.1 release for which n_jobs=None would be accepted an treated as n_jobs=1.\r\n\r\nIf it is possible to make such a release, I think it would be very useful.\r\n\r\nCurrently, scikit-learn 0.20, can only work with joblib 0.12 that also uses loky as the default back-end. For any issue, that affects loky, it means that that scikit-learn users need to wait for the issue to be fixed upstream. I don't doubt that most issues (if any are discovered) will be resolved, however I think it's also important to give users a quick workaround if needed, in any case. \r\n\r\nFor Linux distributions that package scikit-learn and joblib separately I think it also doesn't hurt to have a larger range of compatible versions.\r\n\r\nAnother use case for this is that I am currently working on packaging joblib for [Pyodide](https://github.com/iodide-project/pyodide/), and I wanted to start with joblib 0.11 as that the WebAssembly environment doesn't support creating threads/processes yet, so all the additions in the vendored loky in that context mean just more possibilities to fail by calling unsupported parts of the stdlib. And as mentioned earlier that version of joblib doesn't work with scikit-learn 0.20...\r\n\r\nThis would be very much appreciated. \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/784", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/784/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/784/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/784/events", "html_url": "https://github.com/joblib/joblib/issues/784", "id": 367134770, "node_id": "MDU6SXNzdWUzNjcxMzQ3NzA=", "number": 784, "title": "Nested backend handling set default n_jobs to -1", "user": {"login": "tomMoral", "id": 3321081, "node_id": "MDQ6VXNlcjMzMjEwODE=", "avatar_url": "https://avatars0.githubusercontent.com/u/3321081?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomMoral", "html_url": "https://github.com/tomMoral", "followers_url": "https://api.github.com/users/tomMoral/followers", "following_url": "https://api.github.com/users/tomMoral/following{/other_user}", "gists_url": "https://api.github.com/users/tomMoral/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomMoral/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomMoral/subscriptions", "organizations_url": "https://api.github.com/users/tomMoral/orgs", "repos_url": "https://api.github.com/users/tomMoral/repos", "events_url": "https://api.github.com/users/tomMoral/events{/privacy}", "received_events_url": "https://api.github.com/users/tomMoral/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-05T09:40:38Z", "updated_at": "2018-10-05T19:02:52Z", "closed_at": "2018-10-05T19:02:52Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "`parallel_backend` is used in `BatchedCalls` to set the default nested backend, and its default behavior is to set `n_jobs=-1`.\r\n\r\n```python\r\nfrom joblib import Parallel, delayed\r\n\r\ndef test():\r\n    parallel = Parallel()\r\n    print(parallel._backend)\r\n    print(parallel.n_jobs)\r\n\r\nParallel(n_jobs=2)(delayed(test)() for _ in range(1))\r\n```\r\n\r\n__Output__\r\n\r\n\r\n```\r\nbackend: <class 'joblib._parallel_backends.ThreadingBackend'>\r\nn_jobs: -1\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/781", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/781/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/781/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/781/events", "html_url": "https://github.com/joblib/joblib/issues/781", "id": 365578382, "node_id": "MDU6SXNzdWUzNjU1NzgzODI=", "number": 781, "title": "Large memory growth with parallel on numpy arrays.", "user": {"login": "jason-neal", "id": 10076879, "node_id": "MDQ6VXNlcjEwMDc2ODc5", "avatar_url": "https://avatars3.githubusercontent.com/u/10076879?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jason-neal", "html_url": "https://github.com/jason-neal", "followers_url": "https://api.github.com/users/jason-neal/followers", "following_url": "https://api.github.com/users/jason-neal/following{/other_user}", "gists_url": "https://api.github.com/users/jason-neal/gists{/gist_id}", "starred_url": "https://api.github.com/users/jason-neal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jason-neal/subscriptions", "organizations_url": "https://api.github.com/users/jason-neal/orgs", "repos_url": "https://api.github.com/users/jason-neal/repos", "events_url": "https://api.github.com/users/jason-neal/events{/privacy}", "received_events_url": "https://api.github.com/users/jason-neal/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-10-01T18:07:51Z", "updated_at": "2018-10-31T14:11:57Z", "closed_at": "2018-10-31T14:11:56Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "I do not know if this is similar to related to the other memory leak issues posted. \r\n\r\nI am trying to run a convolutions of several stellar which are 1d arrays of length ~100000.\r\nIn the parallel case I am getting a memory leak in which the used memory grows to full the system memory (24GB in about half a day).\r\n\r\nI have tried multiple experiments but I am no further ahead on the issue after several days. \r\n\r\nExample code in which I see the effect  is below. (although I have not tried running this out to 24GB)\r\n\r\n```\r\nfrom joblib import Parallel, delayed, load, dump\r\nimport gc\r\nimport os\r\nimport numpy as np\r\nimport shutil\r\n\r\ndef func(x,y, xi):\r\n        # complicated function\r\n        yy =  np.sum(y) + xi\r\n        return yy\r\n                                                                                                                         \r\nif __name__ == \"__main__\":\r\n      cores = 6\r\n      npix = 50000  # large 1d numpy array.\r\n      x_ = 2 + (1/npix) * np.arange(npix)\r\n      y_ = np.random.randn(npix-10)\r\n      x2_ = x_[10:-10]  # shorter than x/y    \r\n\r\n      folder = \"./memmap_dir/\"\r\n      for ii in range(2000):    \r\n          # x and y change in each iteration\r\n          # x, y   = x_ + ii, y_+ii\r\n          # trying memmapping\r\n          x_filename_memmap = os.path.join(folder, 'data_memmap_x')\r\n          dump(x_+ii, x_filename_memmap)\r\n          x = load(x_filename_memmap, mmap_mode='r')\r\n          y_filename_memmap = os.path.join(folder, 'data_memmap_y')\r\n          dump(y_+ii, y_filename_memmap)\r\n          y = load(y_filename_memmap, mmap_mode='r')\r\n        \r\n          with Parallel(n_jobs=cores) as parallel:\r\n              result =  parallel(delayed(func)(x, y, xi) for xi in x2_)\r\n          \r\n          gc.collect()\r\n\r\n\r\n      shutil.rmtree(folder, ignore_errors=True)\r\n\r\n```\r\nAs you can see I have tried adding memmapping, and gc.collect() but I still get memory growth up to GBs (before I kill it).\r\n\r\nVersions:\r\njoblib                    0.12.5\r\npython                  3.6.5\r\nnumpy                  1.15.0\r\n\r\nAm I missing something here?  It does not occur when num_procs=1\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/780", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/780/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/780/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/780/events", "html_url": "https://github.com/joblib/joblib/issues/780", "id": 365151949, "node_id": "MDU6SXNzdWUzNjUxNTE5NDk=", "number": 780, "title": "How to avoid multiple non-thread safe code execution on module re-imports inside Parallel", "user": {"login": "StrikerRUS", "id": 25141164, "node_id": "MDQ6VXNlcjI1MTQxMTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/25141164?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StrikerRUS", "html_url": "https://github.com/StrikerRUS", "followers_url": "https://api.github.com/users/StrikerRUS/followers", "following_url": "https://api.github.com/users/StrikerRUS/following{/other_user}", "gists_url": "https://api.github.com/users/StrikerRUS/gists{/gist_id}", "starred_url": "https://api.github.com/users/StrikerRUS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StrikerRUS/subscriptions", "organizations_url": "https://api.github.com/users/StrikerRUS/orgs", "repos_url": "https://api.github.com/users/StrikerRUS/repos", "events_url": "https://api.github.com/users/StrikerRUS/events{/privacy}", "received_events_url": "https://api.github.com/users/StrikerRUS/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 841761474, "node_id": "MDU6TGFiZWw4NDE3NjE0NzQ=", "url": "https://api.github.com/repos/joblib/joblib/labels/blocker", "name": "blocker", "color": "f91911", "default": false, "description": null}, {"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 20, "created_at": "2018-09-29T17:38:41Z", "updated_at": "2018-11-19T21:32:21Z", "closed_at": "2018-11-19T21:32:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "The following error(s) started to appear after updating `scikit-learn` to 0.20 and in consequence `joblib` to 0.12:\r\n```\r\n================================== FAILURES ===================================\r\n_________________________ TestExamples.test_examples __________________________\r\nsklearn.externals.joblib.externals.loky.process_executor._RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 420, in _process_worker\r\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 563, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 261, in __call__\r\n    for func, args, kwargs in self.items]\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 261, in <listcomp>\r\n    for func, args, kwargs in self.items]\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py\", line 242, in fit_ovr_binary\r\n    return binary_clf.fit(X, y, sample_weight)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py\", line 306, in fit\r\n    self._execute_command(cmd)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py\", line 267, in _execute_command\r\n    universal_newlines=True).communicate()\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\subprocess.py\", line 756, in __init__\r\n    restore_signals, start_new_session)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\subprocess.py\", line 1155, in _execute_child\r\n    startupinfo)\r\nFileNotFoundError: [WinError 2] The system cannot find the file specified\r\n\"\"\"\r\nThe above exception was the direct cause of the following exception:\r\nself = <test_examples.TestExamples testMethod=test_examples>\r\n    def test_examples(self):\r\n        for filename in find_files(os.path.join(os.path.abspath(os.path.dirname(__file__)), os.path.pardir, 'examples')):\r\n>           exec(open(filename).read(), globals())\r\ntests\\test_examples.py:17: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n<string>:16: in <module>\r\n    ???\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py:502: in fit\r\n    self._fit_multiclass_task(X, y, sample_weight, params)\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\rgf_model.py:377: in _fit_multiclass_task\r\n    for i in range(self._n_classes))\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py:996: in __call__\r\n    self.retrieve()\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py:899: in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py:517: in wrap_future_result\r\n    return future.result(timeout=timeout)\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\concurrent\\futures\\_base.py:432: in result\r\n    return self.__get_result()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nself = <Future at 0x5ae8519630 state=finished raised FileNotFoundError>\r\n    def __get_result(self):\r\n        if self._exception:\r\n>           raise self._exception\r\nE           FileNotFoundError: [WinError 2] The system cannot find the file specified\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\concurrent\\futures\\_base.py:384: FileNotFoundError\r\n______________________ TestRGFClassfier.test_attributes _______________________\r\nsklearn.externals.joblib.externals.loky.process_executor._RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 420, in _process_worker\r\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 563, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 261, in __call__\r\n    for func, args, kwargs in self.items]\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 261, in <listcomp>\r\n    for func, args, kwargs in self.items]\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py\", line 242, in fit_ovr_binary\r\n    return binary_clf.fit(X, y, sample_weight)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py\", line 306, in fit\r\n    self._execute_command(cmd)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py\", line 267, in _execute_command\r\n    universal_newlines=True).communicate()\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\subprocess.py\", line 756, in __init__\r\n    restore_signals, start_new_session)\r\n  File \"C:\\Miniconda36-x64\\envs\\test-environment\\lib\\subprocess.py\", line 1155, in _execute_child\r\n    startupinfo)\r\nFileNotFoundError: [WinError 2] The system cannot find the file specified\r\n\"\"\"\r\nThe above exception was the direct cause of the following exception:\r\nself = <test_rgf_python.TestRGFClassfier testMethod=test_attributes>\r\n    def test_attributes(self):\r\n        clf = self.classifier_class(**self.kwargs)\r\n        attributes = ('estimators_', 'classes_', 'n_classes_', 'n_features_', 'fitted_',\r\n                      'sl2_', 'min_samples_leaf_', 'n_iter_')\r\n    \r\n        for attr in attributes:\r\n            self.assertRaises(NotFittedError, getattr, clf, attr)\r\n>       clf.fit(self.X_train, self.y_train)\r\ntests\\test_rgf_python.py:256: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\utils.py:502: in fit\r\n    self._fit_multiclass_task(X, y, sample_weight, params)\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\rgf\\rgf_model.py:377: in _fit_multiclass_task\r\n    for i in range(self._n_classes))\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py:996: in __call__\r\n    self.retrieve()\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py:899: in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py:517: in wrap_future_result\r\n    return future.result(timeout=timeout)\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\concurrent\\futures\\_base.py:432: in result\r\n    return self.__get_result()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nself = <Future at 0x5ae86fe320 state=finished raised FileNotFoundError>\r\n    def __get_result(self):\r\n        if self._exception:\r\n>           raise self._exception\r\nE           FileNotFoundError: [WinError 2] The system cannot find the file specified\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\concurrent\\futures\\_base.py:384: FileNotFoundError\r\n---------------------------- Captured stderr call -----------------------------\r\nERROR: The process with PID 2448 (child process of PID 1612) could not be terminated.\r\nReason: The operation attempted is not supported.\r\n```\r\n\r\nI use Parallel to communicate with executable file which uses OpenMP:\r\n\r\nhttps://github.com/RGF-team/rgf/blob/a7c0a5b5b51d26eac689650194867a33ad640c47/python-package/rgf/rgf_model.py#L373-L377\r\nhttps://github.com/RGF-team/rgf/blob/a7c0a5b5b51d26eac689650194867a33ad640c47/python-package/rgf/utils.py#L263-L267\r\n\r\nFailures are random: roughly 1 failure for 3 runs. It can be illustrated by the following picture (it's 8 re-builds of the same commit):\r\n\r\n![image](https://user-images.githubusercontent.com/25141164/46248600-049c0280-c424-11e8-81a4-f2a104b9227d.png)\r\n\r\nError is presented **only** on x64 Windows with Python 3.7. x86 Windows, Linux, macOS are bug-free. The error doesn't appear at Python 2.7, 3.4-3.6 too.\r\n\r\nAfter facing this error I've tried to run scikit-learn with joblib from PyPI via `set SKLEARN_SITE_JOBLIB=true` (version 0.12.0 - 0.12.5) - no luck.\r\n\r\nOld stable joblib 0.11 cannot be used with the newest scikit-learn due to the following error in RandomForestRegressor:\r\n```\r\n            # Parallel loop: we prefer the threading backend as the Cython code\r\n            # for fitting the trees is internally releasing the Python GIL\r\n            # making threading more efficient than multiprocessing in\r\n            # that case. However, we respect any parallel_backend contexts set\r\n            # at a higher level, since correctness does not rely on using\r\n            # threads.\r\n            trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\r\n>                            prefer=\"threads\")(\r\n                delayed(_parallel_build_trees)(\r\n                    t, self, X, y, sample_weight, i, len(trees),\r\n                    verbose=self.verbose, class_weight=self.class_weight)\r\n                for i, t in enumerate(trees))\r\nE           TypeError: __init__() got an unexpected keyword argument 'prefer'\r\nC:\\Miniconda36-x64\\envs\\test-environment\\lib\\site-packages\\sklearn\\ensemble\\forest.py:331: TypeError\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/776", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/776/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/776/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/776/events", "html_url": "https://github.com/joblib/joblib/issues/776", "id": 359151593, "node_id": "MDU6SXNzdWUzNTkxNTE1OTM=", "number": 776, "title": "loky backend errant print statements", "user": {"login": "ibackus", "id": 6054037, "node_id": "MDQ6VXNlcjYwNTQwMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/6054037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ibackus", "html_url": "https://github.com/ibackus", "followers_url": "https://api.github.com/users/ibackus/followers", "following_url": "https://api.github.com/users/ibackus/following{/other_user}", "gists_url": "https://api.github.com/users/ibackus/gists{/gist_id}", "starred_url": "https://api.github.com/users/ibackus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ibackus/subscriptions", "organizations_url": "https://api.github.com/users/ibackus/orgs", "repos_url": "https://api.github.com/users/ibackus/repos", "events_url": "https://api.github.com/users/ibackus/events{/privacy}", "received_events_url": "https://api.github.com/users/ibackus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-11T17:45:19Z", "updated_at": "2018-09-13T12:13:28Z", "closed_at": "2018-09-13T12:13:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "I recently upgraded joblib v0.12.4 and started using the loky backend.  Unfortunately, when running a `Parallel` loop, I get a bunch of errant print statements that I cannot for the life of me track down.  The statements vanish when I switch back to using the multiprocessing backend.\r\n\r\nI run the code with something like:\r\n```python\r\nresults = Parallel(4, mmap_mode='r+', verbose=0)(\r\n    delayed(function)(a, b, c) for a, b, c in stuff)\r\n```\r\n\r\nThe print statements look like:\r\n```\r\n85827584\r\n81047552\r\n80310272\r\n86327296\r\n111181824\r\n88104960\r\n83111936\r\n86261760\r\n86261760\r\n88330240\r\n111181824\r\n83857408\r\n83857408\r\n111050752\r\n88903680\r\n86261760\r\n```\r\n\r\nAny ideas?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/774", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/774/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/774/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/774/events", "html_url": "https://github.com/joblib/joblib/issues/774", "id": 358132001, "node_id": "MDU6SXNzdWUzNTgxMzIwMDE=", "number": 774, "title": "Error when returning certain objects", "user": {"login": "khoopes", "id": 16563055, "node_id": "MDQ6VXNlcjE2NTYzMDU1", "avatar_url": "https://avatars2.githubusercontent.com/u/16563055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khoopes", "html_url": "https://github.com/khoopes", "followers_url": "https://api.github.com/users/khoopes/followers", "following_url": "https://api.github.com/users/khoopes/following{/other_user}", "gists_url": "https://api.github.com/users/khoopes/gists{/gist_id}", "starred_url": "https://api.github.com/users/khoopes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khoopes/subscriptions", "organizations_url": "https://api.github.com/users/khoopes/orgs", "repos_url": "https://api.github.com/users/khoopes/repos", "events_url": "https://api.github.com/users/khoopes/events{/privacy}", "received_events_url": "https://api.github.com/users/khoopes/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-09-07T16:23:43Z", "updated_at": "2018-09-12T08:12:36Z", "closed_at": "2018-09-12T08:12:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "Joblib is new to me so excuse my ignorance if there is a better way to do this that does not give an error. I have simplified my larger code down to this repeatable issue:\r\n\r\n\r\n```python\r\n\r\nfrom joblib import Parallel, delayed\r\nimport CoolProp\r\n\r\ndef run_fun(num):\r\n\tco2 = CoolProp.AbstractState('HEOS', 'CO2')\r\n\tco2.update(CoolProp.PT_INPUTS, 101325, 400+num)\r\n\treturn co2\r\n\r\nif __name__ == '__main__':\r\n\t\r\n\tnumber_range = [2,3,4,5,6,7,8]\r\n\t\r\n\t#################This works####################\r\n\tserial_output = []\r\n\tfor number in number_range:\r\n\t\tserial_output.append(run_fun(number))\r\n\t\r\n\tprint [k.rhomass() for k in serial_output]\r\n\t\r\n\t#################This doesn't##################\t\r\n\tparallel_output = Parallel(n_jobs=4, backend=\"multiprocessing\")(map(delayed(run_fun), number_range))\r\n\tprint [k.rhomass() for k in parallel_output]\r\n\r\n```\r\n\r\nI think the issue is what I return in my function, if I return k.rhomass() from the function the parallel works just fine. But it does not like getting that coolprop object. I get this error:\r\n\r\n```\r\n[1.336555986982784, 1.3332176198250427, 1.3298959970186623, 1.3265909917944818, 1.3233024786744818, 1.3200303334551526, 1.3167744331911175]\r\nException in thread Thread-3:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\khoopes\\AppData\\Local\\Continuum\\anaconda2\\lib\\threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"C:\\Users\\khoopes\\AppData\\Local\\Continuum\\anaconda2\\lib\\threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"C:\\Users\\khoopes\\AppData\\Local\\Continuum\\anaconda2\\lib\\multiprocessing\\pool.py\", line 392, in _handle_results\r\n    task = get()\r\n  File \"CoolProp\\AbstractState.pyx\", line 31, in CoolProp.CoolProp.AbstractState.__cinit__ (CoolProp\\CoolProp.cpp:11111)\r\nTypeError: __cinit__() takes exactly 2 positional arguments (0 given)\r\n\r\n```\r\n\r\nSorry to require CoolProp to recreate this issue, but that is what my other code uses and I wanted to get it as close as possible. I would really like to return the coolprop object.\r\n\r\nAny ideas?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/770", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/770/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/770/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/770/events", "html_url": "https://github.com/joblib/joblib/issues/770", "id": 356942185, "node_id": "MDU6SXNzdWUzNTY5NDIxODU=", "number": 770, "title": "Fonts in Joblib logo are not rendered correctly on non Linux systems", "user": {"login": "aabadie", "id": 1375137, "node_id": "MDQ6VXNlcjEzNzUxMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1375137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aabadie", "html_url": "https://github.com/aabadie", "followers_url": "https://api.github.com/users/aabadie/followers", "following_url": "https://api.github.com/users/aabadie/following{/other_user}", "gists_url": "https://api.github.com/users/aabadie/gists{/gist_id}", "starred_url": "https://api.github.com/users/aabadie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aabadie/subscriptions", "organizations_url": "https://api.github.com/users/aabadie/orgs", "repos_url": "https://api.github.com/users/aabadie/repos", "events_url": "https://api.github.com/users/aabadie/events{/privacy}", "received_events_url": "https://api.github.com/users/aabadie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-04T19:19:04Z", "updated_at": "2018-09-06T06:04:21Z", "closed_at": "2018-09-06T06:04:21Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "If one browse the documentation on readthedocs from a system other than Linux (and maybe other than Ubuntu), the fonts used for \"Joblib\" in the Logo is not correct. I noticed this problem on Windows and Mac OS.\r\n\r\nIn fact, the logo looks like this:\r\n\r\n![joblib_logo](https://user-images.githubusercontent.com/1375137/45052272-9fecc280-b086-11e8-9680-7cc38a34cc9f.png)\r\n\r\nJoblib is written using the default font and not the Ubuntu Mono that is defined in the SVG. There's also a spacing issue between `Job` and `lib`. This is because the Ubuntu Mono font is not available on non Linux/Ubuntu systems and the browser don't know how to correctly display the text.\r\nThis can be fixed by at least 2 ways:\r\n- embed the Ubuntu Mono font within the SVG, normally Inkscape is able to do that\r\n- instead of using a text element with a specific font, convert it to a vector path. I think this can also be done with Inkscape\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/766", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/766/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/766/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/766/events", "html_url": "https://github.com/joblib/joblib/issues/766", "id": 355559045, "node_id": "MDU6SXNzdWUzNTU1NTkwNDU=", "number": 766, "title": "Random freeze on travis at: test_concurrency_safe_write[multiprocessing]", "user": {"login": "ogrisel", "id": 89061, "node_id": "MDQ6VXNlcjg5MDYx", "avatar_url": "https://avatars0.githubusercontent.com/u/89061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ogrisel", "html_url": "https://github.com/ogrisel", "followers_url": "https://api.github.com/users/ogrisel/followers", "following_url": "https://api.github.com/users/ogrisel/following{/other_user}", "gists_url": "https://api.github.com/users/ogrisel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ogrisel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ogrisel/subscriptions", "organizations_url": "https://api.github.com/users/ogrisel/orgs", "repos_url": "https://api.github.com/users/ogrisel/repos", "events_url": "https://api.github.com/users/ogrisel/events{/privacy}", "received_events_url": "https://api.github.com/users/ogrisel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-30T12:17:19Z", "updated_at": "2019-01-11T18:14:16Z", "closed_at": "2018-09-13T12:14:50Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "We observed the following freeze once on travis:\r\n\r\n```\r\njoblib/test/test_store_backends.py::test_concurrency_safe_write[multiprocessing] \r\nNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\r\nCheck the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#Build-times-out-because-no-output-was-received\r\nThe build has been terminated\r\n```\r\n\r\nIf that occurs again, let's report it here as a comment to have an idea about the frequency of this issue.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/764", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/764/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/764/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/764/events", "html_url": "https://github.com/joblib/joblib/issues/764", "id": 354783009, "node_id": "MDU6SXNzdWUzNTQ3ODMwMDk=", "number": 764, "title": "FileSystemStoreBackend.__repr__ should not return a string", "user": {"login": "lesteve", "id": 1680079, "node_id": "MDQ6VXNlcjE2ODAwNzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1680079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lesteve", "html_url": "https://github.com/lesteve", "followers_url": "https://api.github.com/users/lesteve/followers", "following_url": "https://api.github.com/users/lesteve/following{/other_user}", "gists_url": "https://api.github.com/users/lesteve/gists{/gist_id}", "starred_url": "https://api.github.com/users/lesteve/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lesteve/subscriptions", "organizations_url": "https://api.github.com/users/lesteve/orgs", "repos_url": "https://api.github.com/users/lesteve/repos", "events_url": "https://api.github.com/users/lesteve/events{/privacy}", "received_events_url": "https://api.github.com/users/lesteve/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-28T15:25:40Z", "updated_at": "2018-08-30T12:26:39Z", "closed_at": "2018-08-30T12:26:39Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Something that tripped me up when working on https://github.com/joblib/joblib/pull/746, is that `FileSystemStoreBackend.__repr__` makes it look like a string. I could not find a good reason for this. Do you remember something about this @aabadie?\r\n\r\nHere is a snippet to show what I mean:\r\n```\r\nIn [1]: from joblib import Memory\r\n\r\nIn [2]: mem = Memory('/tmp/test')\r\n\r\nIn [3]: mem.store_backend\r\nOut[3]: /tmp/test/joblib\r\n```\r\n\r\nWhen I was debugging I lost at least one hour until I thought of doing `type(mem.store_backend)` ...\r\n\r\nAlso slightly less important but:\r\n```py\r\nfrom joblib._store_backends import FileSystemStoreBackend\r\nbackend = FileSystemStoreBackend()\r\nbackend  # Exception in __repr__ AttributeError: 'FileSystemStoreBackend' object has no attribute 'location'\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/758", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/758/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/758/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/758/events", "html_url": "https://github.com/joblib/joblib/issues/758", "id": 353987467, "node_id": "MDU6SXNzdWUzNTM5ODc0Njc=", "number": 758, "title": "test_nested_parallel_limit fails on 1-core-VM", "user": {"login": "bmwiedemann", "id": 637990, "node_id": "MDQ6VXNlcjYzNzk5MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/637990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bmwiedemann", "html_url": "https://github.com/bmwiedemann", "followers_url": "https://api.github.com/users/bmwiedemann/followers", "following_url": "https://api.github.com/users/bmwiedemann/following{/other_user}", "gists_url": "https://api.github.com/users/bmwiedemann/gists{/gist_id}", "starred_url": "https://api.github.com/users/bmwiedemann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bmwiedemann/subscriptions", "organizations_url": "https://api.github.com/users/bmwiedemann/orgs", "repos_url": "https://api.github.com/users/bmwiedemann/repos", "events_url": "https://api.github.com/users/bmwiedemann/events{/privacy}", "received_events_url": "https://api.github.com/users/bmwiedemann/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-25T06:12:20Z", "updated_at": "2018-08-27T12:13:53Z", "closed_at": "2018-08-27T09:34:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "While working on reproducible builds for openSUSE, I noticed\r\nthat 2 tests would fail on 1-core-VMs (using python-joblib 0.12.1)\r\n\r\n```\r\n_______________________ test_nested_parallel_limit[loky] _______________________\r\n\r\n>       assert backend_types_and_levels == expected_types_and_levels\r\nE       AssertionError: assert [('LokyBacken...lBackend', 2)] == [('LokyBackend...lBackend', 3)]\r\n\r\n____________________ test_nested_parallel_limit[threading] _____________________        \r\nbackend = 'threading'\r\n\r\n    @with_multiprocessing\r\n    @parametrize('backend', ['loky', 'threading'])\r\n    def test_nested_parallel_limit(backend):\r\n        with parallel_backend(backend, n_jobs=2):\r\n            backend_types_and_levels = _recursive_backend_info()\r\n\r\n        top_level_backend_type = backend.title() + 'Backend'\r\n        expected_types_and_levels = [\r\n            (top_level_backend_type, 0),\r\n            ('ThreadingBackend', 1),\r\n            ('SequentialBackend', 2),\r\n            ('SequentialBackend', 3)\r\n        ]\r\n>       assert backend_types_and_levels == expected_types_and_levels\r\nE       AssertionError: assert [('ThreadingB...lBackend', 2)] == [('ThreadingBa...lBackend', 3)]\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/755", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/755/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/755/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/755/events", "html_url": "https://github.com/joblib/joblib/issues/755", "id": 353472513, "node_id": "MDU6SXNzdWUzNTM0NzI1MTM=", "number": 755, "title": "test_backend_batch_statistics_reset failure in assert test_time / ... ", "user": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars3.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-08-23T17:06:15Z", "updated_at": "2018-08-28T06:32:41Z", "closed_at": "2018-08-28T06:32:41Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "joblib package keeps failing in debian reproducible builds with the following test failure:\r\nhttps://tests.reproducible-builds.org/debian/rbuild/unstable/i386/joblib_0.12.2-1.rbuild.log.gz\r\n([package page](https://tests.reproducible-builds.org/debian/rb-pkg/unstable/i386/joblib.html))\r\n```\r\n        # Tolerance in the timing comparison to avoid random failures on CIs\r\n>       assert test_time / ref_time <= 1 + relative_tolerance\r\nE       assert (2.2405660152435303 / 1.3164479732513428) <= (1 + 0.2)\r\n```\r\nAny hints on what potentially could have lead to it and how to mitigate are more than welcome!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/751", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/751/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/751/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/751/events", "html_url": "https://github.com/joblib/joblib/issues/751", "id": 352373397, "node_id": "MDU6SXNzdWUzNTIzNzMzOTc=", "number": 751, "title": "Caching breaks when new (uncached) args are treated as kwargs.", "user": {"login": "collijk", "id": 6313865, "node_id": "MDQ6VXNlcjYzMTM4NjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6313865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/collijk", "html_url": "https://github.com/collijk", "followers_url": "https://api.github.com/users/collijk/followers", "following_url": "https://api.github.com/users/collijk/following{/other_user}", "gists_url": "https://api.github.com/users/collijk/gists{/gist_id}", "starred_url": "https://api.github.com/users/collijk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/collijk/subscriptions", "organizations_url": "https://api.github.com/users/collijk/orgs", "repos_url": "https://api.github.com/users/collijk/repos", "events_url": "https://api.github.com/users/collijk/events{/privacy}", "received_events_url": "https://api.github.com/users/collijk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-21T03:35:54Z", "updated_at": "2018-08-23T09:03:56Z", "closed_at": "2018-08-23T09:03:56Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Simple fix addressed with PR: https://github.com/joblib/joblib/pull/750", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/747", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/747/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/747/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/747/events", "html_url": "https://github.com/joblib/joblib/issues/747", "id": 350541861, "node_id": "MDU6SXNzdWUzNTA1NDE4NjE=", "number": 747, "title": "pickling breaks between joblib version 0.11 and 0.12.2", "user": {"login": "mpsommer", "id": 9969903, "node_id": "MDQ6VXNlcjk5Njk5MDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/9969903?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpsommer", "html_url": "https://github.com/mpsommer", "followers_url": "https://api.github.com/users/mpsommer/followers", "following_url": "https://api.github.com/users/mpsommer/following{/other_user}", "gists_url": "https://api.github.com/users/mpsommer/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpsommer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpsommer/subscriptions", "organizations_url": "https://api.github.com/users/mpsommer/orgs", "repos_url": "https://api.github.com/users/mpsommer/repos", "events_url": "https://api.github.com/users/mpsommer/events{/privacy}", "received_events_url": "https://api.github.com/users/mpsommer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-08-14T18:20:05Z", "updated_at": "2018-08-22T11:38:35Z", "closed_at": "2018-08-22T11:38:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "For my script below, will someone please tell me why the result of call_and_shelve() can be pickled in version 0.11,  but not in version 0.12.2? Or tell me what I am doing wrong, or if this is a bug...\r\n\r\nThanks.\r\n\r\n## I have this little script\r\n```python\r\nfrom joblib import Memory\r\nimport numpy as np\r\nimport pickle\r\ncachedir = 'PATH_TO_CACHE'\r\nmemory = Memory(cachedir, verbose=0)\r\n\r\n@memory.cache\r\ndef g(x):\r\n\tprint('A long-running calculation, with parameter %s' % x)\r\n\treturn np.hamming(x)\r\n\r\nresult = g.call_and_shelve(4)\r\nprint('\\nMemorized result: {}'.format(result))\r\n\r\ncanned_pickle = pickle.dumps(result)\r\nopen_can = pickle.loads(canned_pickle)\r\n\r\nprint('Depickled: {}\\n'.format(open_can))\r\n```\r\n### With joblib:0.11 everything works and I get this output:\r\n```python\r\nMemorized result: MemorizedResult(cachedir=\"/Users/mpsommer/Documents/python/job_lib_cache/joblib\", func=\"__main__--Users-mpsommer-Documents-python-job_lib/g-alias\", argument_hash=\"f7c5defad10d1a7505df243840ad5209\")\r\nDepickled: MemorizedResult(cachedir=\"/Users/mpsommer/Documents/python/job_lib_cache/joblib\", func=\"__main__--Users-mpsommer-Documents-python-job_lib/g-alias\", argument_hash=\"f7c5defad10d1a7505df243840ad5209\")\r\n```\r\n\r\n### With joblib:0.12.2 the pickling breaks and I get this output:\r\n```python\r\nMemorized result: MemorizedResult(location=\"/Users/mpsommer/Documents/python/job_lib_cache/joblib\", func=\"<function g at 0x104448aa0>\", args_id=\"f7c5defad10d1a7505df243840ad5209\")\r\nTraceback (most recent call last):\r\n  File \"job_lib.py\", line 15, in <module>\r\n    canned_pickle = pickle.dumps(result)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 1374, in dumps\r\n    Pickler(file, protocol).dump(obj)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 224, in dump\r\n    self.save(obj)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 331, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 401, in save_reduce\r\n    save(args)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 562, in save_tuple\r\n    save(element)\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.py\", line 753, in save_global\r\n    (obj, module, name))\r\npickle.PicklingError: Can't pickle <function g at 0x104448aa0>: it's not the same object as __main__.g\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/742", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/742/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/742/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/742/events", "html_url": "https://github.com/joblib/joblib/issues/742", "id": 348954392, "node_id": "MDU6SXNzdWUzNDg5NTQzOTI=", "number": 742, "title": "Memory's backend_options parameter has a mutable default value", "user": {"login": "jnothman", "id": 78827, "node_id": "MDQ6VXNlcjc4ODI3", "avatar_url": "https://avatars2.githubusercontent.com/u/78827?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jnothman", "html_url": "https://github.com/jnothman", "followers_url": "https://api.github.com/users/jnothman/followers", "following_url": "https://api.github.com/users/jnothman/following{/other_user}", "gists_url": "https://api.github.com/users/jnothman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jnothman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jnothman/subscriptions", "organizations_url": "https://api.github.com/users/jnothman/orgs", "repos_url": "https://api.github.com/users/jnothman/repos", "events_url": "https://api.github.com/users/jnothman/events{/privacy}", "received_events_url": "https://api.github.com/users/jnothman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-09T02:34:27Z", "updated_at": "2018-08-17T11:50:18Z", "closed_at": "2018-08-17T11:50:18Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "It doesn't look dangerous in this context, but it is generally bad practice...", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/741", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/741/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/741/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/741/events", "html_url": "https://github.com/joblib/joblib/issues/741", "id": 348943800, "node_id": "MDU6SXNzdWUzNDg5NDM4MDA=", "number": 741, "title": "Memory.__reduce__ is broken since cachedir deprecation", "user": {"login": "jnothman", "id": 78827, "node_id": "MDQ6VXNlcjc4ODI3", "avatar_url": "https://avatars2.githubusercontent.com/u/78827?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jnothman", "html_url": "https://github.com/jnothman", "followers_url": "https://api.github.com/users/jnothman/followers", "following_url": "https://api.github.com/users/jnothman/following{/other_user}", "gists_url": "https://api.github.com/users/jnothman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jnothman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jnothman/subscriptions", "organizations_url": "https://api.github.com/users/jnothman/orgs", "repos_url": "https://api.github.com/users/jnothman/repos", "events_url": "https://api.github.com/users/jnothman/events{/privacy}", "received_events_url": "https://api.github.com/users/jnothman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-09T01:10:58Z", "updated_at": "2018-08-23T07:48:45Z", "closed_at": "2018-08-23T07:48:45Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "Memory can't be pickled/unpickled.\r\n\r\nIt defines `__reduce__` to exclude a timestamp from the pickle.\r\n\r\nBut `__reduce__` includes:\r\n\r\n```\r\n        return (self.__class__, (location, self.backend, self.mmap_mode,\r\n                                 compress, self._verbose))\r\n```\r\n\r\nwhere `__init__` is:\r\n\r\n```\r\n    def __init__(self, location=None, backend='local', cachedir=None,\r\n                 mmap_mode=None, compress=False, verbose=1, bytes_limit=None,\r\n                 backend_options={})\r\n```\r\n\r\n`cachedir`, not `mmap_mode` is now the third positional argument.  Perhaps it should be the last, and then `__reduce__` need not be changed. Or `__reduce__` can be fixed and changed again when the deprecation is completed.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/739", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/739/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/739/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/739/events", "html_url": "https://github.com/joblib/joblib/issues/739", "id": 348822825, "node_id": "MDU6SXNzdWUzNDg4MjI4MjU=", "number": 739, "title": "python 2.7: test_lz4_compression_without_IO  LZ4_NOT_INSTALLED_ERROR", "user": {"login": "costrouc", "id": 1740337, "node_id": "MDQ6VXNlcjE3NDAzMzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1740337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/costrouc", "html_url": "https://github.com/costrouc", "followers_url": "https://api.github.com/users/costrouc/followers", "following_url": "https://api.github.com/users/costrouc/following{/other_user}", "gists_url": "https://api.github.com/users/costrouc/gists{/gist_id}", "starred_url": "https://api.github.com/users/costrouc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/costrouc/subscriptions", "organizations_url": "https://api.github.com/users/costrouc/orgs", "repos_url": "https://api.github.com/users/costrouc/repos", "events_url": "https://api.github.com/users/costrouc/events{/privacy}", "received_events_url": "https://api.github.com/users/costrouc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-08T17:34:19Z", "updated_at": "2018-08-22T09:37:39Z", "closed_at": "2018-08-22T09:37:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "With python version 2.7 the `LZ4_NOT_INSTALLED_ERROR` is wrong when running the tests and lz4 is not present.\r\n\r\nInstalling `python-lz4` fixes the issue since that function is not run if lz4 is present.\r\n\r\n```\r\npy.test joblib\r\n````\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n_______________________ test_lz4_compression_without_lz4 _______________________\r\n\r\ntmpdir = local('/tmp/nix-build-python2.7-joblib-0.12.2.drv-0/pytest-of-costrouc/pytest-0/test_lz4_compression_without_l0')\r\n\r\n    @without_lz4\r\n    def test_lz4_compression_without_lz4(tmpdir):\r\n        # Check that lz4 cannot be used when dependency is not available.\r\n        fname = tmpdir.join('test.nolz4').strpath\r\n        data = 'test data'\r\n        with raises(ValueError) as excinfo:\r\n            numpy_pickle.dump(data, fname, compress='lz4')\r\n        excinfo.match(LZ4_NOT_INSTALLED_ERROR)\r\n    \r\n        with raises(ValueError) as excinfo:\r\n            numpy_pickle.dump(data, fname + '.lz4')\r\n>       excinfo.match(LZ4_NOT_INSTALLED_ERROR)\r\nE       AssertionError: Pattern 'LZ4 is not installed. Install it with pip: http://python-lz4.readthedocs.io/' not found in 'lz4 compression is only available with python3+.'\r\n\r\njoblib/test/test_numpy_pickle.py:1064: AssertionError\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/737", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/737/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/737/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/737/events", "html_url": "https://github.com/joblib/joblib/issues/737", "id": 348539119, "node_id": "MDU6SXNzdWUzNDg1MzkxMTk=", "number": 737, "title": "Forward URL from pythonhosted to readthedocs", "user": {"login": "GaelVaroquaux", "id": 208217, "node_id": "MDQ6VXNlcjIwODIxNw==", "avatar_url": "https://avatars3.githubusercontent.com/u/208217?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GaelVaroquaux", "html_url": "https://github.com/GaelVaroquaux", "followers_url": "https://api.github.com/users/GaelVaroquaux/followers", "following_url": "https://api.github.com/users/GaelVaroquaux/following{/other_user}", "gists_url": "https://api.github.com/users/GaelVaroquaux/gists{/gist_id}", "starred_url": "https://api.github.com/users/GaelVaroquaux/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GaelVaroquaux/subscriptions", "organizations_url": "https://api.github.com/users/GaelVaroquaux/orgs", "repos_url": "https://api.github.com/users/GaelVaroquaux/repos", "events_url": "https://api.github.com/users/GaelVaroquaux/events{/privacy}", "received_events_url": "https://api.github.com/users/GaelVaroquaux/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}, {"id": 60446705, "node_id": "MDU6TGFiZWw2MDQ0NjcwNQ==", "url": "https://api.github.com/repos/joblib/joblib/labels/documentation", "name": "documentation", "color": "009800", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-08T01:09:49Z", "updated_at": "2018-08-24T14:17:23Z", "closed_at": "2018-08-17T14:03:39Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "#724 is still not really fixed: Now that we have moved the docs from pythonhosted to readthedocs, we need a forward from the former to the latter: Google and old links will still land on the former.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/731", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/731/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/731/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/731/events", "html_url": "https://github.com/joblib/joblib/issues/731", "id": 346290906, "node_id": "MDU6SXNzdWUzNDYyOTA5MDY=", "number": 731, "title": "struct.error: 'i' format requires -2147483648 <= number <= 2147483647", "user": {"login": "enricopal", "id": 16625821, "node_id": "MDQ6VXNlcjE2NjI1ODIx", "avatar_url": "https://avatars1.githubusercontent.com/u/16625821?v=4", "gravatar_id": "", "url": "https://api.github.com/users/enricopal", "html_url": "https://github.com/enricopal", "followers_url": "https://api.github.com/users/enricopal/followers", "following_url": "https://api.github.com/users/enricopal/following{/other_user}", "gists_url": "https://api.github.com/users/enricopal/gists{/gist_id}", "starred_url": "https://api.github.com/users/enricopal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/enricopal/subscriptions", "organizations_url": "https://api.github.com/users/enricopal/orgs", "repos_url": "https://api.github.com/users/enricopal/repos", "events_url": "https://api.github.com/users/enricopal/events{/privacy}", "received_events_url": "https://api.github.com/users/enricopal/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-07-31T17:42:37Z", "updated_at": "2019-02-14T03:25:35Z", "closed_at": "2018-08-02T11:22:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI am using joblib to parallelize the computation of a feature matrix, a large numpy array of floats (~7k rows and ~10k columns, ~70M values).\r\n\r\nMy code breaks at this point: \r\n\r\n```python\r\nuser_item_features = Parallel(n_jobs=n_jobs)(\r\n    delayed(self._compute_features)(data, recommender, users_list)\r\n    for users_list in users_list_chunks\r\n)\r\n```\r\n\r\nwith this error:\r\n\r\n```python-traceback\r\nTraceback (most recent call last):\r\n  File \"entity2rec/node2vec_recommender.py\", line 138, in <module>\r\n    n_jobs=args.workers, supervised=False)\r\n  File \"/home/semantic/Repositories/entity2rec/entity2rec/evaluator.py\", line 255, in features\r\n    users_list_chunks, n_jobs)\r\n  File \"/home/semantic/Repositories/entity2rec/entity2rec/evaluator.py\", line 269, in _compute_features_parallel\r\n    for users_list in users_list_chunks)\r\n  File \"/home/semantic/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\", line 789, in __call__\r\n    self.retrieve()\r\n  File \"/home/semantic/anaconda3/lib/python3.6/site-packages/joblib/parallel.py\", line 699, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"/home/semantic/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 608, in get\r\n    raise self._value\r\n  File \"/home/semantic/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 385, in _handle_tasks\r\n    put(task)\r\n  File \"/home/semantic/anaconda3/lib/python3.6/site-packages/joblib/pool.py\", line 372, in send\r\n    self._writer.send_bytes(buffer.getvalue())\r\n  File \"/home/semantic/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\r\n    self._send_bytes(m[offset:offset + size])\r\n  File \"/home/semantic/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 393, in _send_bytes\r\n    header = struct.pack(\"!i\", n)\r\nstruct.error: 'i' format requires -2147483648 <= number <= 2147483647\r\n```\r\n\r\nI have obtained this error using Linux and different versions of Python:\r\n- python 3.6.6, 3.6.3, 3.6.0\r\n- joblib 0.11\r\n\r\nAny help would be appreciated.\r\nThank you for your work.\r\nEnrico", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/729", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/729/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/729/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/729/events", "html_url": "https://github.com/joblib/joblib/issues/729", "id": 346246435, "node_id": "MDU6SXNzdWUzNDYyNDY0MzU=", "number": 729, "title": "Error:Could not pickle the task to send it to the workers", "user": {"login": "EPinzuti", "id": 9115032, "node_id": "MDQ6VXNlcjkxMTUwMzI=", "avatar_url": "https://avatars1.githubusercontent.com/u/9115032?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EPinzuti", "html_url": "https://github.com/EPinzuti", "followers_url": "https://api.github.com/users/EPinzuti/followers", "following_url": "https://api.github.com/users/EPinzuti/following{/other_user}", "gists_url": "https://api.github.com/users/EPinzuti/gists{/gist_id}", "starred_url": "https://api.github.com/users/EPinzuti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EPinzuti/subscriptions", "organizations_url": "https://api.github.com/users/EPinzuti/orgs", "repos_url": "https://api.github.com/users/EPinzuti/repos", "events_url": "https://api.github.com/users/EPinzuti/events{/privacy}", "received_events_url": "https://api.github.com/users/EPinzuti/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-07-31T15:43:51Z", "updated_at": "2020-07-23T03:00:50Z", "closed_at": "2018-08-02T14:37:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\n\r\nI get the error 'Could not pickle the task to send it to the workers' when I call:\r\nreconstructed=Parallel(n_jobs=6,verbose=50)(delayed(self.spectral_surrogates)(data_slice)  for perm in range(0, self.settings['n_perm_spec']))\r\n\r\nspectral_surrogates has inside a wrapper to work with a C function. Is it possible to use joblib in this case?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/728", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/728/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/728/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/728/events", "html_url": "https://github.com/joblib/joblib/issues/728", "id": 345611553, "node_id": "MDU6SXNzdWUzNDU2MTE1NTM=", "number": 728, "title": "0.12 broke API a little", "user": {"login": "yarikoptic", "id": 39889, "node_id": "MDQ6VXNlcjM5ODg5", "avatar_url": "https://avatars3.githubusercontent.com/u/39889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yarikoptic", "html_url": "https://github.com/yarikoptic", "followers_url": "https://api.github.com/users/yarikoptic/followers", "following_url": "https://api.github.com/users/yarikoptic/following{/other_user}", "gists_url": "https://api.github.com/users/yarikoptic/gists{/gist_id}", "starred_url": "https://api.github.com/users/yarikoptic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yarikoptic/subscriptions", "organizations_url": "https://api.github.com/users/yarikoptic/orgs", "repos_url": "https://api.github.com/users/yarikoptic/repos", "events_url": "https://api.github.com/users/yarikoptic/events{/privacy}", "received_events_url": "https://api.github.com/users/yarikoptic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-07-30T04:58:58Z", "updated_at": "2018-08-01T13:38:23Z", "closed_at": "2018-08-01T11:37:26Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "In 9d521102a9fbfd5fb3e7b90c7df9f5e0047ee8e6\r\nBatchedCalls got 3rd required positional argument `pickle_cache`.  That apparently broke some dependent code in dask.distributed/1.21.8 . See http://bugs.debian.org/904944 for more detail\r\nDo you see an easy way to retain compatibility?  if not, at least an API-breakage warning then should be added to CHANGES.md\r\n\r\nCheers!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/724", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/724/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/724/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/724/events", "html_url": "https://github.com/joblib/joblib/issues/724", "id": 344380096, "node_id": "MDU6SXNzdWUzNDQzODAwOTY=", "number": 724, "title": "readthedocs still documents joblib 0.11", "user": {"login": "mhooreman", "id": 7373718, "node_id": "MDQ6VXNlcjczNzM3MTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7373718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mhooreman", "html_url": "https://github.com/mhooreman", "followers_url": "https://api.github.com/users/mhooreman/followers", "following_url": "https://api.github.com/users/mhooreman/following{/other_user}", "gists_url": "https://api.github.com/users/mhooreman/gists{/gist_id}", "starred_url": "https://api.github.com/users/mhooreman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mhooreman/subscriptions", "organizations_url": "https://api.github.com/users/mhooreman/orgs", "repos_url": "https://api.github.com/users/mhooreman/repos", "events_url": "https://api.github.com/users/mhooreman/events{/privacy}", "received_events_url": "https://api.github.com/users/mhooreman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-25T10:11:45Z", "updated_at": "2018-08-07T15:20:09Z", "closed_at": "2018-08-07T15:20:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nhttps://pythonhosted.org/joblib/index.html still documents joblib 0.11\r\n\r\nThis is disturbing, especially because there are some \"big\" changes on 0.12 (backend, etc.), and the 0.12 switch already happened (at least with anoconda).\r\n\r\nThanks.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/723", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/723/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/723/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/723/events", "html_url": "https://github.com/joblib/joblib/issues/723", "id": 344353261, "node_id": "MDU6SXNzdWUzNDQzNTMyNjE=", "number": 723, "title": "0.12 locky backend: can't inherit of a class who uses joblib and is defined on another module", "user": {"login": "mhooreman", "id": 7373718, "node_id": "MDQ6VXNlcjczNzM3MTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7373718?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mhooreman", "html_url": "https://github.com/mhooreman", "followers_url": "https://api.github.com/users/mhooreman/followers", "following_url": "https://api.github.com/users/mhooreman/following{/other_user}", "gists_url": "https://api.github.com/users/mhooreman/gists{/gist_id}", "starred_url": "https://api.github.com/users/mhooreman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mhooreman/subscriptions", "organizations_url": "https://api.github.com/users/mhooreman/orgs", "repos_url": "https://api.github.com/users/mhooreman/repos", "events_url": "https://api.github.com/users/mhooreman/events{/privacy}", "received_events_url": "https://api.github.com/users/mhooreman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2018-07-25T08:58:40Z", "updated_at": "2018-08-24T12:41:08Z", "closed_at": "2018-08-24T12:41:08Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nWith joblib 0.12 and the default locky backend, I can't inherit of a class who uses joblib and is defined on another module.\r\n\r\nIf I push that class in the same module, or if I go back to multiprocessing backend, it works.\r\n\r\nIt is possible, but I'm not sure, that this is a duplicate of https://github.com/joblib/joblib/issues/643. Sorry if that's the case.\r\n\r\nThe error is \"``A process in the executor was terminated abruptly while the future was running or pending.``\". Moreover, while showing stderr when I have the crash, I see (``TestJoblibLog`` is my child class):\r\n``AttributeError: Can't get attribute 'TestJoblibLog' on <module 'joblib.externals.loky.backend.popen_loky_posix' from '/home/tmp_mihoo/miniconda3/lib/python3.6/site-packages/joblib/externals/loky/backend/popen_loky_posix.py'>``\r\n\r\nSo, it seems that joblib locky imports the module where joblib is called, but not the calling module which uses the results.\r\n\r\nPlease see example codes below.\r\n\r\nThanks!\r\n\r\n\r\n# Example and code to reproduce\r\n\r\n## Execution results\r\n\r\n### ./in_modules.py 2>/dev/null\r\n\r\n```\r\nComputing chunks\r\nRunning in parallel\r\nHello, ['foo', 'bar', 'baz']\r\nExtraction done\r\nSuccess with default backend: ['foo', 'bar', 'baz']\r\nMapping foo start: 1 of 3: 33.33%\r\nHello, foo\r\nMapping foo done: 1 of 3: 33.33% in 0:00:00.180327, remaining 0:00:00.360654, end at 2018-07-25 10:52:15.640262\r\nMapping bar start: 2 of 3: 66.67%\r\nHello, bar\r\nMapping bar done: 2 of 3: 66.67% in 0:00:00.181148, remaining 0:00:00.090574, end at 2018-07-25 10:52:15.371003\r\nMapping baz start: 3 of 3: 100.00%\r\nHello, baz\r\nMapping baz done: 3 of 3: 100.00% in 0:00:00.181711, remaining 0:00:00, end at 2018-07-25 10:52:15.280992\r\n```\r\n\r\n### ./imported.py 2>/dev/null\r\n\r\n```\r\nComputing chunks\r\nRunning in parallel\r\nException with default backend: A process in the executor was terminated abruptly while the future was running or pending.\r\nComputing chunks\r\nRunning in parallel\r\nUsing multiprocessing joblib backend instead of the default one\r\nMapping foo start: 1 of 3: 33.33%\r\nHello, foo\r\nMapping foo done: 1 of 3: 33.33% in 0:00:00.013259, remaining 0:00:00.026518, end at 2018-07-25 10:56:25.080652\r\nMapping bar start: 2 of 3: 66.67%\r\nHello, bar\r\nMapping bar done: 2 of 3: 66.67% in 0:00:00.013529, remaining 0:00:00.006764, end at 2018-07-25 10:56:25.061168\r\nMapping baz start: 3 of 3: 100.00%\r\nHello, baz\r\nMapping baz done: 3 of 3: 100.00% in 0:00:00.013826, remaining 0:00:00, end at 2018-07-25 10:56:25.054701\r\nHello, ['foo', 'bar', 'baz']\r\nExtraction done\r\nSuccess with mutiprocessing backend: ['foo', 'bar', 'baz']\r\n```\r\n\r\n## Files\r\n\r\n### imported.py\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nfrom wrapper import MapReduceWrapper\r\n\r\n\r\nclass TestJoblibLog(MapReduceWrapper):\r\n    def __init__(self, *args, **kwargs):\r\n        MapReduceWrapper.__init__(self, *args, **kwargs)\r\n\r\n    def computeChunks(self):\r\n        print(\"Computing chunks\")\r\n        return ['foo', 'bar', 'baz']\r\n\r\n    def map(self, chunk):\r\n        print(f\"Hello, {chunk}\")\r\n        return chunk\r\n\r\n    def reduce(self, mapResults):\r\n        print(f\"Hello, {str(mapResults)}\")\r\n        return mapResults\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        print(\r\n            \"Success with default backend:\",\r\n            TestJoblibLog(backend=None).get()\r\n        )\r\n    except Exception as e:\r\n        print(f\"Exception with default backend: {e}\")\r\n        try:\r\n            print(\r\n                \"Success with mutiprocessing backend:\",\r\n                TestJoblibLog(backend='multiprocessing').get()\r\n            )\r\n        except Exception as e:\r\n            print(f\"Exception with multiprocessing backend: {e}\")\r\n            print(\"No result\")\r\n```\r\n\r\n### wrapper.py\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nimport abc\r\nimport datetime\r\nimport joblib as jl\r\n\r\n\r\nclass MapReduceWrapper(metaclass=abc.ABCMeta):\r\n    def __init__(self, nJobs=-1, verbose=2, backend='multiprocessing'):\r\n        \"\"\"Constructor\r\n        - nJobs: if 1 or 0, don't use parallel processing; else, use joblib\r\n                 parallel processing, and forward this value to the n_jobs\r\n                 argument of the joblib.Parallel's constructor\r\n        - verbose: 0: don't show anything, 1: show chunk name as info message,\r\n                   2: show chunk name and progres as info message\r\n        - backend: the joblib backend. Useful because, since jl 0.12, the\r\n                   locky backend is used by default, and it makes some issues;\r\n                   in this case, multiprocessing backend is better. If\r\n                   the provided value is None, it let joblib choose the\r\n                   backend.\r\n        Please note that, because of parallel processing, the estimated\r\n        progress and elapsed time are not accurate\r\n        \"\"\"\r\n        self._nJobs = nJobs\r\n        self._verbose = verbose\r\n        self._useParallel = (nJobs not in (0, 1))\r\n        self._mapStart = None\r\n        self._backend = backend\r\n        self._chunks = self.computeChunks()\r\n\r\n    @abc.abstractmethod\r\n    def computeChunks(self):\r\n        \"\"\"Returns the list of the chunks\r\n        Will typically give a list based on child's class constructor's\r\n        arguments\r\n        To be implemented in the child classes\r\n        \"\"\"\r\n\r\n    @abc.abstractmethod\r\n    def map(self, chunk):\r\n        \"\"\"Gets data from the provided chunk and returns the result\r\n        - chunk: the chunk identifier\r\n        To be implemented in the child classes\r\n        \"\"\"\r\n\r\n    @abc.abstractmethod\r\n    def reduce(self, mapResults):\r\n        \"\"\"Reduce the data retrieven from every map call and returns the result\r\n        - mapResults: list of map results\r\n        To be implemented in the child classes\r\n        \"\"\"\r\n\r\n    def _loggedMap(self, i, n, x):\r\n        \"\"\"Wrapper for map, logging the ongoing chunk + progress estimation\r\n        - i: id of the current chunk, from 1 to the number of chunks\r\n        - n: number of chunks\r\n        - x: chunk identifier\r\n        \"\"\"\r\n        if self._verbose == 2:\r\n            print(\"Mapping %s start: %d of %d: %.2f%%\" % (\r\n                x,\r\n                i,\r\n                n,\r\n                100 * i / n,\r\n            ))\r\n        elif self._verbose == 1:\r\n            print(\"Mapping %s\" % x)\r\n        ret = self.map(x)\r\n\r\n        if self._verbose == 2:\r\n            progress = 100 * i / n\r\n            now = datetime.datetime.now()\r\n            elapsed = now - self._mapStart\r\n            remaining = datetime.timedelta(\r\n                seconds=(\r\n                    elapsed.total_seconds() * (100 - progress) / progress\r\n                )\r\n            )\r\n            endAt = now + remaining\r\n            print(\r\n                (\r\n                    \"Mapping %s done: %d of %d: %.2f%% in %s, remaining %s, \"\r\n                    \"end at %s\"\r\n                ) % (\r\n                    x,\r\n                    i,\r\n                    n,\r\n                    100 * i / n,\r\n                    str(elapsed),\r\n                    str(remaining),\r\n                    str(endAt)\r\n                )\r\n            )\r\n        return ret\r\n\r\n    def get(self):\r\n        \"\"\"Computes and returns the selected data\"\"\"\r\n        n = len(self._chunks)\r\n        self._mapStart = datetime.datetime.now()\r\n        if self._useParallel:\r\n            print(\"Running in parallel\")\r\n            if self._nJobs != -1:\r\n                self.logWarning(\r\n                    \"Running %d jobs in parallel, which might be different \"\r\n                    \"than one per CPU\"\r\n                )\r\n            jlKwargs = dict(n_jobs=self._nJobs, verbose=0)\r\n            if self._backend is not None:\r\n                print(\r\n                    f\"Using {self._backend} joblib backend \"\r\n                    \"instead of the default one\"\r\n                )\r\n                jlKwargs['backend'] = self._backend\r\n            xs = jl.Parallel(**jlKwargs)(\r\n                jl.delayed(self._loggedMap)(\r\n                    i + 1, n, chunk\r\n                ) for i, chunk in list(enumerate(self._chunks))\r\n            )\r\n        else:\r\n            self.logWarning(\r\n                \"Parallel processing disabled; running in sequence\"\r\n            )\r\n            xs = [\r\n                self._loggedMap(\r\n                    i + 1, n, chunk\r\n                ) for (\r\n                    i, chunk\r\n                ) in enumerate(\r\n                    self._chunks\r\n                )\r\n            ]\r\n        self._mapStart = None\r\n        ret = self.reduce(xs)\r\n        try:\r\n            ret = ret.copy()\r\n        except AttributeError:\r\n            self.logWarning(\r\n                f\"Deep copy unavailable for return type {type(ret)}, \"\r\n                \"returning direct value instead.\"\r\n            )\r\n        print(\"Extraction done\")\r\n        return ret\r\n```\r\n\r\n\r\n### in_module.py\r\n\r\n```python\r\n#!/usr/bin/env python3\r\n\r\nimport abc\r\nimport datetime\r\nimport joblib as jl\r\n\r\nfrom vitriol.base import VitriolBase\r\nfrom vitriol.algorithms.parallel import MapReduceWrapper\r\n\r\nclass MapReduceWrapper(metaclass=abc.ABCMeta):\r\n    def __init__(self, nJobs=-1, verbose=2, backend='multiprocessing'):\r\n        \"\"\"Constructor\r\n        - nJobs: if 1 or 0, don't use parallel processing; else, use joblib\r\n                 parallel processing, and forward this value to the n_jobs\r\n                 argument of the joblib.Parallel's constructor\r\n        - verbose: 0: don't show anything, 1: show chunk name as info message,\r\n                   2: show chunk name and progres as info message\r\n        - backend: the joblib backend. Useful because, since jl 0.12, the\r\n                   locky backend is used by default, and it makes some issues;\r\n                   in this case, multiprocessing backend is better. If\r\n                   the provided value is None, it let joblib choose the\r\n                   backend.\r\n        Please note that, because of parallel processing, the estimated\r\n        progress and elapsed time are not accurate\r\n        \"\"\"\r\n        self._nJobs = nJobs\r\n        self._verbose = verbose\r\n        self._useParallel = (nJobs not in (0, 1))\r\n        self._mapStart = None\r\n        self._backend = backend\r\n        self._chunks = self.computeChunks()\r\n\r\n    @abc.abstractmethod\r\n    def computeChunks(self):\r\n        \"\"\"Returns the list of the chunks\r\n        Will typically give a list based on child's class constructor's\r\n        arguments\r\n        To be implemented in the child classes\r\n        \"\"\"\r\n\r\n    @abc.abstractmethod\r\n    def map(self, chunk):\r\n        \"\"\"Gets data from the provided chunk and returns the result\r\n        - chunk: the chunk identifier\r\n        To be implemented in the child classes\r\n        \"\"\"\r\n\r\n    @abc.abstractmethod\r\n    def reduce(self, mapResults):\r\n        \"\"\"Reduce the data retrieven from every map call and returns the result\r\n        - mapResults: list of map results\r\n        To be implemented in the child classes\r\n        \"\"\"\r\n\r\n    def _loggedMap(self, i, n, x):\r\n        \"\"\"Wrapper for map, logging the ongoing chunk + progress estimation\r\n        - i: id of the current chunk, from 1 to the number of chunks\r\n        - n: number of chunks\r\n        - x: chunk identifier\r\n        \"\"\"\r\n        if self._verbose == 2:\r\n            print(\"Mapping %s start: %d of %d: %.2f%%\" % (\r\n                x,\r\n                i,\r\n                n,\r\n                100 * i / n,\r\n            ))\r\n        elif self._verbose == 1:\r\n            print(\"Mapping %s\" % x)\r\n        ret = self.map(x)\r\n\r\n        if self._verbose == 2:\r\n            progress = 100 * i / n\r\n            now = datetime.datetime.now()\r\n            elapsed = now - self._mapStart\r\n            remaining = datetime.timedelta(\r\n                seconds=(\r\n                    elapsed.total_seconds() * (100 - progress) / progress\r\n                )\r\n            )\r\n            endAt = now + remaining\r\n            print(\r\n                (\r\n                    \"Mapping %s done: %d of %d: %.2f%% in %s, remaining %s, \"\r\n                    \"end at %s\"\r\n                ) % (\r\n                    x,\r\n                    i,\r\n                    n,\r\n                    100 * i / n,\r\n                    str(elapsed),\r\n                    str(remaining),\r\n                    str(endAt)\r\n                )\r\n            )\r\n        return ret\r\n\r\n    def get(self):\r\n        \"\"\"Computes and returns the selected data\"\"\"\r\n        n = len(self._chunks)\r\n        self._mapStart = datetime.datetime.now()\r\n        if self._useParallel:\r\n            print(\"Running in parallel\")\r\n            if self._nJobs != -1:\r\n                self.logWarning(\r\n                    \"Running %d jobs in parallel, which might be different \"\r\n                    \"than one per CPU\"\r\n                )\r\n            jlKwargs = dict(n_jobs=self._nJobs, verbose=0)\r\n            if self._backend is not None:\r\n                print(\r\n                    f\"Using {self._backend} joblib backend \"\r\n                    \"instead of the default one\"\r\n                )\r\n                jlKwargs['backend'] = self._backend\r\n            xs = jl.Parallel(**jlKwargs)(\r\n                jl.delayed(self._loggedMap)(\r\n                    i + 1, n, chunk\r\n                ) for i, chunk in list(enumerate(self._chunks))\r\n            )\r\n        else:\r\n            self.logWarning(\r\n                \"Parallel processing disabled; running in sequence\"\r\n            )\r\n            xs = [\r\n                self._loggedMap(\r\n                    i + 1, n, chunk\r\n                ) for (\r\n                    i, chunk\r\n                ) in enumerate(\r\n                    self._chunks\r\n                )\r\n            ]\r\n        self._mapStart = None\r\n        ret = self.reduce(xs)\r\n        try:\r\n            ret = ret.copy()\r\n        except AttributeError:\r\n            self.logWarning(\r\n                f\"Deep copy unavailable for return type {type(ret)}, \"\r\n                \"returning direct value instead.\"\r\n            )\r\n        print(\"Extraction done\")\r\n        return ret\r\n\r\n\r\nclass TestJoblibLog(MapReduceWrapper):\r\n    def __init__(self, *args, **kwargs):\r\n        MapReduceWrapper.__init__(self, *args, **kwargs)\r\n\r\n    def computeChunks(self):\r\n        print(\"Computing chunks\")\r\n        return ['foo', 'bar', 'baz']\r\n\r\n    def map(self, chunk):\r\n        print(f\"Hello, {chunk}\")\r\n        return chunk\r\n\r\n    def reduce(self, mapResults):\r\n        print(f\"Hello, {str(mapResults)}\")\r\n        return mapResults\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        print(\r\n            \"Success with default backend:\",\r\n            TestJoblibLog(backend=None).get()\r\n        )\r\n    except Exception as e:\r\n        print(f\"Exception with default backend: {e}\")\r\n        try:\r\n            print(\r\n                \"Success with mutiprocessing backend:\",\r\n                TestJoblibLog(backend='multiprocessing').get()\r\n            )\r\n        except Exception as e:\r\n            print(f\"Exception with multiprocessing backend: {e}\")\r\n            print(\"No result\")\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/721", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/721/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/721/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/721/events", "html_url": "https://github.com/joblib/joblib/issues/721", "id": 343200969, "node_id": "MDU6SXNzdWUzNDMyMDA5Njk=", "number": 721, "title": "Memory Leak in joblib.Parallel", "user": {"login": "umbernhard", "id": 2686765, "node_id": "MDQ6VXNlcjI2ODY3NjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/2686765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/umbernhard", "html_url": "https://github.com/umbernhard", "followers_url": "https://api.github.com/users/umbernhard/followers", "following_url": "https://api.github.com/users/umbernhard/following{/other_user}", "gists_url": "https://api.github.com/users/umbernhard/gists{/gist_id}", "starred_url": "https://api.github.com/users/umbernhard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/umbernhard/subscriptions", "organizations_url": "https://api.github.com/users/umbernhard/orgs", "repos_url": "https://api.github.com/users/umbernhard/repos", "events_url": "https://api.github.com/users/umbernhard/events{/privacy}", "received_events_url": "https://api.github.com/users/umbernhard/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 33, "created_at": "2018-07-20T18:13:01Z", "updated_at": "2019-06-03T18:43:51Z", "closed_at": "2018-09-19T08:28:27Z", "author_association": "NONE", "active_lock_reason": null, "body": "Apologies if this issue has been flagged before, I didn't see anything about it though. I've been running the following bit of code: \r\n\r\n```\r\ndef compute(margin, N_wl, pi, alpha, n): #start, end):\r\n\r\n    ret_val = 0\r\n    #for n in range(start, end):\r\n    p1 = 0.5 + margin/2\r\n    q_alpha = hypergeom.ppf(1-alpha, N_wl, N_wl/2, n) # upper alpha quantile of null distribution\r\n    prob_select_N = binom.pmf(n, N_wl, pi) # probability of selecting n out of N_wl at sampling rate pi\r\n    pvalue_nw = hypergeom.sf(q_alpha, N_wl, N_wl*p1, n) # probability of alternative distr falling above q_alpha\r\n    return prob_select_N*pvalue_nw\r\n\r\ndef compute_unconditional_power(margin, N_wl, pi, alpha):\r\n    '''\r\n    Compute unconditional power of the test.\r\n\r\n    margin = vote margin (votes for w / votes for w or l) in the population\r\n    N_wl = the total number of ballots for either the winner or loser in the population,\r\n    pop = total population size,\r\n    pi = the sampling probability,\r\n    alpha = the type I error rate\r\n    '''\r\n    unlikely_draw_lower = binom.ppf(0.005, N_wl, pi)\r\n    unlikely_draw_upper = binom.ppf(0.995, N_wl, pi)\r\n    power_sum = 0\r\n\r\n    powers = Parallel(n_jobs=num_cores)(delayed(compute)(margin, N_wl, pi, alpha, n) \\\r\n            for n in range(int(unlikely_draw_lower), int(unlikely_draw_upper)))\r\n\r\n    return sum(powers)\r\n```\r\n\r\nAnd i've noticed two things: the way that the parallel processes get spun up in 0.12.1 is different than in 0.11, and that this code, which works fine in 0.11, results in a memory leak in 0.12.1. Typically I get the following error, which as far as I can tell is just joblib's way of handling an OOM: \r\n\r\n``` \r\n/usr/local/lib/python2.7/dist-packages/joblib/externals/loky/process_executor.py:634:  UserWarning: A worker timeout while some jobs were given to the executor. You might want to use a longer timeout for the executor. \r\n  \"the executor.\", UserWarning\r\nTraceback (most recent call last):\r\n  File \"gen_plot_data.py\", line 214, in <module>\r\n    main()\r\n  File \"gen_plot_data.py\", line 165, in main\r\n    bbp_ss = get_bbp_sample_size(prop_winner, Ntot, alpha)\r\n  File \"gen_plot_data.py\", line 115, in get_bbp_sample_size\r\n    quants[quant] = get_sample_for_power(margin, Ntot, alpha, quant/100.0, 1/float(Ntot))\r\n  File \"gen_plot_data.py\", line 106, in get_sample_for_power\r\n    x = compute_unconditional_power(margin, Ntot, pi, alpha)\r\n  File \"gen_plot_data.py\", line 93, in compute_unconditional_power\r\n    for n in range(int(unlikely_draw_lower), int(unlikely_draw_upper)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/joblib/parallel.py\", line 962, in __call__\r\n    self.retrieve()\r\n  File \"/usr/local/lib/python2.7/dist-packages/joblib/parallel.py\", line 865, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"/usr/local/lib/python2.7/dist-packages/joblib/_parallel_backends.py\", line 515, in wrap_future_result\r\n    return future.result(timeout=timeout)\r\n  File \"/usr/local/lib/python2.7/dist-packages/joblib/externals/loky/_base.py\", line 431, in result\r\n    return self.__get_result()\r\n  File \"/usr/local/lib/python2.7/dist-packages/joblib/externals/loky/_base.py\", line 382, in __get_result\r\n    raise self._exception\r\njoblib.externals.loky.process_executor.BrokenProcessPool: A process in the executor was terminated abruptly while the future was running or pending.```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/719", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/719/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/719/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/719/events", "html_url": "https://github.com/joblib/joblib/issues/719", "id": 342603857, "node_id": "MDU6SXNzdWUzNDI2MDM4NTc=", "number": 719, "title": "ENH: test that we have no warnings at import", "user": {"login": "GaelVaroquaux", "id": 208217, "node_id": "MDQ6VXNlcjIwODIxNw==", "avatar_url": "https://avatars3.githubusercontent.com/u/208217?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GaelVaroquaux", "html_url": "https://github.com/GaelVaroquaux", "followers_url": "https://api.github.com/users/GaelVaroquaux/followers", "following_url": "https://api.github.com/users/GaelVaroquaux/following{/other_user}", "gists_url": "https://api.github.com/users/GaelVaroquaux/gists{/gist_id}", "starred_url": "https://api.github.com/users/GaelVaroquaux/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GaelVaroquaux/subscriptions", "organizations_url": "https://api.github.com/users/GaelVaroquaux/orgs", "repos_url": "https://api.github.com/users/GaelVaroquaux/repos", "events_url": "https://api.github.com/users/GaelVaroquaux/events{/privacy}", "received_events_url": "https://api.github.com/users/GaelVaroquaux/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446661, "node_id": "MDU6TGFiZWw2MDQ0NjY2MQ==", "url": "https://api.github.com/repos/joblib/joblib/labels/enhancement", "name": "enhancement", "color": "fbca04", "default": true, "description": null}, {"id": 725618864, "node_id": "MDU6TGFiZWw3MjU2MTg4NjQ=", "url": "https://api.github.com/repos/joblib/joblib/labels/help%20wanted", "name": "help wanted", "color": "bfd4f2", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-19T07:06:11Z", "updated_at": "2019-01-10T10:19:19Z", "closed_at": "2019-01-10T10:19:19Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "We should add a test like the following:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tests/test_init.py", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/717", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/717/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/717/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/717/events", "html_url": "https://github.com/joblib/joblib/issues/717", "id": 340930842, "node_id": "MDU6SXNzdWUzNDA5MzA4NDI=", "number": 717, "title": "joblib for shared list ", "user": {"login": "houssem1992", "id": 13033353, "node_id": "MDQ6VXNlcjEzMDMzMzUz", "avatar_url": "https://avatars2.githubusercontent.com/u/13033353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/houssem1992", "html_url": "https://github.com/houssem1992", "followers_url": "https://api.github.com/users/houssem1992/followers", "following_url": "https://api.github.com/users/houssem1992/following{/other_user}", "gists_url": "https://api.github.com/users/houssem1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/houssem1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/houssem1992/subscriptions", "organizations_url": "https://api.github.com/users/houssem1992/orgs", "repos_url": "https://api.github.com/users/houssem1992/repos", "events_url": "https://api.github.com/users/houssem1992/events{/privacy}", "received_events_url": "https://api.github.com/users/houssem1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-13T08:37:36Z", "updated_at": "2018-07-15T04:28:46Z", "closed_at": "2018-07-15T04:28:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "I Want to use joblib to parallelize a sliding windows task to search objects in images. the Problem is that the for loop of my code have a shared list (using append) which will be changed every time when the prediction of my classifier is true. (see the code below) It is possible to do this task with joblib?\r\n\r\n`    bboxes = []\r\n    for xb in range(nxsteps+1):\r\n        for yb in range(nysteps+1):\r\n            ypos = yb*cells_per_step\r\n            # some code there\r\n            test_prediction = svc.predict(test_features)\r\n\r\n            if test_prediction == 1:\r\n                xbox_left = np.int(xleft)\r\n                ytop_draw = np.int(ytop)\r\n                win_draw = np.int(window)\r\n                box = [(xbox_left+xstart, ytop_draw+ystart), (xbox_left+win_draw+xstart,ytop_draw+win_draw+ystart)]\r\n                bboxes.append(box)`\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/714", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/714/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/714/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/714/events", "html_url": "https://github.com/joblib/joblib/issues/714", "id": 338653782, "node_id": "MDU6SXNzdWUzMzg2NTM3ODI=", "number": 714, "title": "v0.12: 'MemorizedResult' object has no attribute 'argument_hash'", "user": {"login": "fmaussion", "id": 10050469, "node_id": "MDQ6VXNlcjEwMDUwNDY5", "avatar_url": "https://avatars3.githubusercontent.com/u/10050469?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmaussion", "html_url": "https://github.com/fmaussion", "followers_url": "https://api.github.com/users/fmaussion/followers", "following_url": "https://api.github.com/users/fmaussion/following{/other_user}", "gists_url": "https://api.github.com/users/fmaussion/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmaussion/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmaussion/subscriptions", "organizations_url": "https://api.github.com/users/fmaussion/orgs", "repos_url": "https://api.github.com/users/fmaussion/repos", "events_url": "https://api.github.com/users/fmaussion/events{/privacy}", "received_events_url": "https://api.github.com/users/fmaussion/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-05T16:57:11Z", "updated_at": "2018-07-23T09:50:52Z", "closed_at": "2018-07-23T09:48:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "I our test suite I was checking that caching occurred properly by checking that a ``MemorizedResult`` instance would have the same hash after two calls to ``call_and_shelve``.\r\n\r\nNow the ``argument_hash`` attr is gone, and I replaced ``argument_hash`` with ``timestamp`` in our tests.\r\n\r\nI was wondering if this is an oversight or if it was intended? I coudln't find anything in the changelog", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/712", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/712/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/712/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/712/events", "html_url": "https://github.com/joblib/joblib/issues/712", "id": 337876118, "node_id": "MDU6SXNzdWUzMzc4NzYxMTg=", "number": 712, "title": "OSError: [Errno 9] Bad file descriptor", "user": {"login": "JoeXinfa", "id": 24202719, "node_id": "MDQ6VXNlcjI0MjAyNzE5", "avatar_url": "https://avatars3.githubusercontent.com/u/24202719?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JoeXinfa", "html_url": "https://github.com/JoeXinfa", "followers_url": "https://api.github.com/users/JoeXinfa/followers", "following_url": "https://api.github.com/users/JoeXinfa/following{/other_user}", "gists_url": "https://api.github.com/users/JoeXinfa/gists{/gist_id}", "starred_url": "https://api.github.com/users/JoeXinfa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JoeXinfa/subscriptions", "organizations_url": "https://api.github.com/users/JoeXinfa/orgs", "repos_url": "https://api.github.com/users/JoeXinfa/repos", "events_url": "https://api.github.com/users/JoeXinfa/events{/privacy}", "received_events_url": "https://api.github.com/users/JoeXinfa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-03T12:24:07Z", "updated_at": "2018-08-10T17:34:58Z", "closed_at": "2018-08-10T17:34:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "Python 3.5, Joblib 0.12, running on Linux.\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"/home/zhuu/code/ezcad/ezcad/gocube/tool/prop_from_gsurf.py\", line 110, in apply\r\n    NXL, NDP, DP, depthArrays, propArrays) for i in nproc)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/parallel.py\", line 906, in __call__\r\n    n_jobs = self._initialize_backend()\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/parallel.py\", line 705, in _initialize_backend\r\n    **self._backend_args)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/_parallel_backends.py\", line 470, in configure\r\n    **memmappingexecutor_args)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/executor.py\", line 36, in get_memmapping_executor\r\n    initargs=initargs)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/reusable_executor.py\", line 108, in get_reusable_executor\r\n    executor_id=executor_id, **kwargs)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/reusable_executor.py\", line 144, in __init__\r\n    initializer=initializer, initargs=initargs)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/process_executor.py\", line 825, in __init__\r\n    self._processes_management_lock = self._context.Lock()\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/backend/context.py\", line 187, in Lock\r\n    return Lock()\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/backend/synchronize.py\", line 174, in __init__\r\n    super(Lock, self).__init__(SEMAPHORE, 1, 1)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/backend/synchronize.py\", line 90, in __init__\r\n    semaphore_tracker.register(self._semlock.name)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/backend/semaphore_tracker.py\", line 117, in register\r\n    self.ensure_running()\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/backend/semaphore_tracker.py\", line 96, in ensure_running\r\n    pid = spawnv_passfds(exe, args, fds_to_pass)\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/backend/semaphore_tracker.py\", line 230, in spawnv_passfds\r\n    _pass += [_mk_inheritable(fd)]\r\n  File \"/data/data323/devl/zhuu/lib/python3.5/site-packages/joblib/externals/loky/backend/_posix_reduction.py\", line 30, in _mk_inheritable\r\n    os.set_inheritable(fd, True)\r\nOSError: [Errno 9] Bad file descriptor\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/711", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/711/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/711/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/711/events", "html_url": "https://github.com/joblib/joblib/issues/711", "id": 337420084, "node_id": "MDU6SXNzdWUzMzc0MjAwODQ=", "number": 711, "title": "Undefined names: 'func_with_kwonly_args' and 'func_with_signature' in test_memory.py", "user": {"login": "cclauss", "id": 3709715, "node_id": "MDQ6VXNlcjM3MDk3MTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cclauss", "html_url": "https://github.com/cclauss", "followers_url": "https://api.github.com/users/cclauss/followers", "following_url": "https://api.github.com/users/cclauss/following{/other_user}", "gists_url": "https://api.github.com/users/cclauss/gists{/gist_id}", "starred_url": "https://api.github.com/users/cclauss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cclauss/subscriptions", "organizations_url": "https://api.github.com/users/cclauss/orgs", "repos_url": "https://api.github.com/users/cclauss/repos", "events_url": "https://api.github.com/users/cclauss/events{/privacy}", "received_events_url": "https://api.github.com/users/cclauss/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-07-02T08:39:01Z", "updated_at": "2018-07-03T14:20:30Z", "closed_at": "2018-07-03T14:20:30Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "flake8 testing of https://github.com/joblib/joblib on Python 3.6.3\r\n\r\n$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__\r\n```\r\n./joblib/test/test_memory.py:599:36: F821 undefined name 'func_with_kwonly_args'\r\n        func_cached = memory.cache(func_with_kwonly_args)\r\n                                   ^\r\n./joblib/test/test_memory.py:620:36: F821 undefined name 'func_with_kwonly_args'\r\n        func_cached = memory.cache(func_with_kwonly_args, ignore=['kw2'])\r\n                                   ^\r\n./joblib/test/test_memory.py:626:36: F821 undefined name 'func_with_signature'\r\n        func_cached = memory.cache(func_with_signature)\r\n                                   ^\r\n3     F821 undefined name 'func_with_kwonly_args'\r\n3\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/710", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/710/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/710/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/710/events", "html_url": "https://github.com/joblib/joblib/issues/710", "id": 337405113, "node_id": "MDU6SXNzdWUzMzc0MDUxMTM=", "number": 710, "title": " test_multiple_spawning is not testing what it's supposed to test", "user": {"login": "ogrisel", "id": 89061, "node_id": "MDQ6VXNlcjg5MDYx", "avatar_url": "https://avatars0.githubusercontent.com/u/89061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ogrisel", "html_url": "https://github.com/ogrisel", "followers_url": "https://api.github.com/users/ogrisel/followers", "following_url": "https://api.github.com/users/ogrisel/following{/other_user}", "gists_url": "https://api.github.com/users/ogrisel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ogrisel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ogrisel/subscriptions", "organizations_url": "https://api.github.com/users/ogrisel/orgs", "repos_url": "https://api.github.com/users/ogrisel/repos", "events_url": "https://api.github.com/users/ogrisel/events{/privacy}", "received_events_url": "https://api.github.com/users/ogrisel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446661, "node_id": "MDU6TGFiZWw2MDQ0NjY2MQ==", "url": "https://api.github.com/repos/joblib/joblib/labels/enhancement", "name": "enhancement", "color": "fbca04", "default": true, "description": null}, {"id": 725618864, "node_id": "MDU6TGFiZWw3MjU2MTg4NjQ=", "url": "https://api.github.com/repos/joblib/joblib/labels/help%20wanted", "name": "help wanted", "color": "bfd4f2", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-02T07:48:21Z", "updated_at": "2020-05-03T16:17:47Z", "closed_at": "2020-05-03T16:17:47Z", "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "See details in https://github.com/joblib/joblib/pull/675#issuecomment-399866164\r\n\r\nThis probably means that some code is now useless with the new backend behaviors.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/709", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/709/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/709/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/709/events", "html_url": "https://github.com/joblib/joblib/issues/709", "id": 337083014, "node_id": "MDU6SXNzdWUzMzcwODMwMTQ=", "number": 709, "title": "Error in Jupyter on Mac - Attempting to do parallel computing without protecting your import", "user": {"login": "simonhughes22", "id": 2167017, "node_id": "MDQ6VXNlcjIxNjcwMTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/2167017?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simonhughes22", "html_url": "https://github.com/simonhughes22", "followers_url": "https://api.github.com/users/simonhughes22/followers", "following_url": "https://api.github.com/users/simonhughes22/following{/other_user}", "gists_url": "https://api.github.com/users/simonhughes22/gists{/gist_id}", "starred_url": "https://api.github.com/users/simonhughes22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simonhughes22/subscriptions", "organizations_url": "https://api.github.com/users/simonhughes22/orgs", "repos_url": "https://api.github.com/users/simonhughes22/repos", "events_url": "https://api.github.com/users/simonhughes22/events{/privacy}", "received_events_url": "https://api.github.com/users/simonhughes22/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-06-29T17:46:41Z", "updated_at": "2018-07-15T04:17:17Z", "closed_at": "2018-07-13T17:16:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "On Mac High Sierra (10.13.4) and using joblib 0.11 and 0.12, in jupyter notebook and jupyter lab, I constantly run into the unprotected import error above. I cannot protect the import when in a notebook sessions AFAIK. Note that this does not happen straight away, it works fine for a while and then randomly fails. But this happens consistently, causing me to restart the kernel and lose a lot of work.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/707", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/707/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/707/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/707/events", "html_url": "https://github.com/joblib/joblib/issues/707", "id": 336120454, "node_id": "MDU6SXNzdWUzMzYxMjA0NTQ=", "number": 707, "title": "Regression in 0.12: different exception type than PicklingError when arguments of Parallel can not be pickled", "user": {"login": "lesteve", "id": 1680079, "node_id": "MDQ6VXNlcjE2ODAwNzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1680079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lesteve", "html_url": "https://github.com/lesteve", "followers_url": "https://api.github.com/users/lesteve/followers", "following_url": "https://api.github.com/users/lesteve/following{/other_user}", "gists_url": "https://api.github.com/users/lesteve/gists{/gist_id}", "starred_url": "https://api.github.com/users/lesteve/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lesteve/subscriptions", "organizations_url": "https://api.github.com/users/lesteve/orgs", "repos_url": "https://api.github.com/users/lesteve/repos", "events_url": "https://api.github.com/users/lesteve/events{/privacy}", "received_events_url": "https://api.github.com/users/lesteve/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 60446680, "node_id": "MDU6TGFiZWw2MDQ0NjY4MA==", "url": "https://api.github.com/repos/joblib/joblib/labels/bug", "name": "bug", "color": "e11d21", "default": true, "description": null}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-06-27T08:08:46Z", "updated_at": "2018-07-02T07:29:15Z", "closed_at": "2018-07-02T07:29:15Z", "author_association": "MEMBER", "active_lock_reason": null, "body": "Moved from https://github.com/joblib/joblib/issues/706#issuecomment-400189831. Probably because of the switch from pickle to cloudpickle.\r\n\r\n```py\r\nimport numpy as np\r\n\r\nimport pymc3 as pm\r\n\r\nrng = np.random.RandomState(0)\r\ncount_data = rng.randint(1, 70, 100).astype(dtype=np.float64)\r\nn_count_data = len(count_data)\r\n\r\nwith pm.Model() as model:\r\n    alpha = 1.0/count_data.mean()  # Recall count_data is the\r\n                                   # variable that holds our txt counts\r\n    lambda_1 = pm.Exponential(\"lambda_1\", alpha)\r\n    lambda_2 = pm.Exponential(\"lambda_2\", alpha)\r\n    \r\n    tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_count_data - 1)\r\n\r\nwith model:\r\n    step = pm.Metropolis()\r\n    trace = pm.sample(5, tune=10,step=step)\r\n```\r\n\r\nMore context from https://github.com/joblib/joblib/issues/706#issuecomment-400187390:\r\nIt seems like the problem comes from the fact that  pymc3 catches `PickleError` to fall-back to single threaded mode:\r\n\r\n```pytb\r\n~/miniconda3/lib/python3.6/site-packages/pymc3/sampling.py in sample(draws, step, init, n_init, start, trace, chain_idx, chains, cores, tune, nuts_kwargs, step_kwargs, progressbar, model, random_seed, live_plot, discard_tuned_samples, live_plot_kwargs, compute_convergence_checks, use_mmap, **kwargs)\r\n    440         _print_step_hierarchy(step)\r\n    441         try:\r\n--> 442             trace = _mp_sample(**sample_args)\r\n    443         except pickle.PickleError:\r\n    444             _log.warning(\"Could not pickle model, sampling singlethreaded.\")```\r\n```\r\n\r\nBut now that joblib uses cloudpickle the error you get is a `TypeError`. Not 100% sure what's the best way to fix that, maybe catch all errors produced by cloudpickle.dump and reraise a PickleError?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/joblib/joblib/issues/706", "repository_url": "https://api.github.com/repos/joblib/joblib", "labels_url": "https://api.github.com/repos/joblib/joblib/issues/706/labels{/name}", "comments_url": "https://api.github.com/repos/joblib/joblib/issues/706/comments", "events_url": "https://api.github.com/repos/joblib/joblib/issues/706/events", "html_url": "https://github.com/joblib/joblib/issues/706", "id": 335364136, "node_id": "MDU6SXNzdWUzMzUzNjQxMzY=", "number": 706, "title": "Pickle error in Parallel", "user": {"login": "dennisrpnw", "id": 4853346, "node_id": "MDQ6VXNlcjQ4NTMzNDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/4853346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dennisrpnw", "html_url": "https://github.com/dennisrpnw", "followers_url": "https://api.github.com/users/dennisrpnw/followers", "following_url": "https://api.github.com/users/dennisrpnw/following{/other_user}", "gists_url": "https://api.github.com/users/dennisrpnw/gists{/gist_id}", "starred_url": "https://api.github.com/users/dennisrpnw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dennisrpnw/subscriptions", "organizations_url": "https://api.github.com/users/dennisrpnw/orgs", "repos_url": "https://api.github.com/users/dennisrpnw/repos", "events_url": "https://api.github.com/users/dennisrpnw/events{/privacy}", "received_events_url": "https://api.github.com/users/dennisrpnw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-06-25T11:20:33Z", "updated_at": "2018-07-02T13:02:19Z", "closed_at": "2018-07-02T13:02:19Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hey,\r\n\r\nI get an error with 0.12 that didn't occur in 0.11\r\n\r\nsnippet of code:\r\n`num_cores = multiprocessing.cpu_count()`\r\n`chunks = _get_chunks(ids, 100)`\r\n`logger.info('cores: {0}, chunks: {1}'.format(num_cores, len(chunks)))`\r\n`Parallel(n_jobs=num_cores)(delayed(_clean_text)(c) for c in chunks)`\r\n\r\nThe resulting error is:\r\n`Cannot pickle files that are not opened for reading: a`", "performed_via_github_app": null, "score": 1.0}]}