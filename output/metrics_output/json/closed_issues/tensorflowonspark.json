{"total_count": 329, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/535", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/535/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/535/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/535/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/535", "id": 675991406, "node_id": "MDU6SXNzdWU2NzU5OTE0MDY=", "number": 535, "title": "requests.exceptions.ConnectionError: HTTPConnectionPool(host='storage.googleapis.com', port=80):", "user": {"login": "yolur", "id": 55946175, "node_id": "MDQ6VXNlcjU1OTQ2MTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/55946175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yolur", "html_url": "https://github.com/yolur", "followers_url": "https://api.github.com/users/yolur/followers", "following_url": "https://api.github.com/users/yolur/following{/other_user}", "gists_url": "https://api.github.com/users/yolur/gists{/gist_id}", "starred_url": "https://api.github.com/users/yolur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yolur/subscriptions", "organizations_url": "https://api.github.com/users/yolur/orgs", "repos_url": "https://api.github.com/users/yolur/repos", "events_url": "https://api.github.com/users/yolur/events{/privacy}", "received_events_url": "https://api.github.com/users/yolur/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-08-10T09:20:11Z", "updated_at": "2020-08-11T00:45:22Z", "closed_at": "2020-08-11T00:45:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "**hi, I got an error when I run the example of mnist. I followed the tutorial to convert the MNIST zip files into HDFS files, but it didn't work.**\r\n**The error message is as follows:**\r\nTraceback (most recent call last):\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\r\n    timeout=timeout\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 727, in urlopen\r\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/urllib3/util/retry.py\", line 439, in increment\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='storage.googleapis.com', port=80): Max retries exceeded with url: /tfds-data/?prefix=dataset_info/mnist/3.0.0/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd0d2b8a9d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mnist_data_setup.py\", line 26, in <module>\r\n    mnist, info = tfds.load('mnist', with_info=True)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py\", line 302, in load\r\n    dbuilder = builder(name, data_dir=data_dir, **builder_kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py\", line 172, in builder\r\n    return _DATASET_REGISTRY[name](**builder_kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py\", line 203, in __init__\r\n    self.info.initialize_from_bucket()\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_info.py\", line 428, in initialize_from_bucket\r\n    data_files = gcs_utils.gcs_dataset_info_files(self.full_name)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/utils/gcs_utils.py\", line 65, in gcs_dataset_info_files\r\n    filenames = [el for el in gcs_files(prefix_filter=prefix)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/utils/gcs_utils.py\", line 55, in gcs_files\r\n    top_level_xml_str = download_gcs_file(\"\", prefix_filter=prefix_filter)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/tensorflow_datasets/core/utils/gcs_utils.py\", line 41, in download_gcs_file\r\n    resp = requests.get(url, stream=stream)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/requests/api.py\", line 76, in get\r\n    return request('get', url, params=params, **kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/requests/api.py\", line 61, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/requests/sessions.py\", line 530, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/requests/sessions.py\", line 643, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/spark/anaconda3/envs/sparktest123/lib/python3.7/site-packages/requests/adapters.py\", line 516, in send\r\n    raise ConnectionError(e, request=request)\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='storage.googleapis.com', port=80): Max retries exceeded with url: /tfds-data/?prefix=dataset_info/mnist/3.0.0/ (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd0d2b8a9d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/533", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/533/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/533/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/533/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/533", "id": 673701476, "node_id": "MDU6SXNzdWU2NzM3MDE0NzY=", "number": 533, "title": "Parameter server number confusion", "user": {"login": "orwa-te", "id": 32763039, "node_id": "MDQ6VXNlcjMyNzYzMDM5", "avatar_url": "https://avatars2.githubusercontent.com/u/32763039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orwa-te", "html_url": "https://github.com/orwa-te", "followers_url": "https://api.github.com/users/orwa-te/followers", "following_url": "https://api.github.com/users/orwa-te/following{/other_user}", "gists_url": "https://api.github.com/users/orwa-te/gists{/gist_id}", "starred_url": "https://api.github.com/users/orwa-te/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orwa-te/subscriptions", "organizations_url": "https://api.github.com/users/orwa-te/orgs", "repos_url": "https://api.github.com/users/orwa-te/repos", "events_url": "https://api.github.com/users/orwa-te/events{/privacy}", "received_events_url": "https://api.github.com/users/orwa-te/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-05T16:54:40Z", "updated_at": "2020-08-09T09:44:04Z", "closed_at": "2020-08-09T09:44:04Z", "author_association": "NONE", "active_lock_reason": null, "body": "In [mnist_tf.py](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_tf.py) we can see that number of PS equals 0 in line \"  `cluster = TFCluster.run(sc, main_fun, args, args.cluster_size, num_ps=0, tensorboard=args.tensorboard, input_mode=TFCluster.InputMode.TENSORFLOW, log_dir=args.model_dir, master_node='chief', eval_node=True)`\r\nHow could the number of PS be equal to 0? What does that mean?\r\nIs there a way to choose a specific worker node to work as PS in the case of **MultiWorkerMirroredStrategy**? ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/532", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/532/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/532/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/532/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/532", "id": 673335289, "node_id": "MDU6SXNzdWU2NzMzMzUyODk=", "number": 532, "title": "Splitting data between workers confusion", "user": {"login": "orwa-te", "id": 32763039, "node_id": "MDQ6VXNlcjMyNzYzMDM5", "avatar_url": "https://avatars2.githubusercontent.com/u/32763039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orwa-te", "html_url": "https://github.com/orwa-te", "followers_url": "https://api.github.com/users/orwa-te/followers", "following_url": "https://api.github.com/users/orwa-te/following{/other_user}", "gists_url": "https://api.github.com/users/orwa-te/gists{/gist_id}", "starred_url": "https://api.github.com/users/orwa-te/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orwa-te/subscriptions", "organizations_url": "https://api.github.com/users/orwa-te/orgs", "repos_url": "https://api.github.com/users/orwa-te/repos", "events_url": "https://api.github.com/users/orwa-te/events{/privacy}", "received_events_url": "https://api.github.com/users/orwa-te/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-08-05T07:54:06Z", "updated_at": "2020-08-05T13:36:34Z", "closed_at": "2020-08-05T13:36:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to apply my **U-Net** model on TensorFlowonSpark in a way similar to the approach in [mnist_tf.py](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_tf.py), but following the guide in [here](https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/mnist/keras) it is mentioned under \"**Train via InputMode.TENSORFLOW**\" that \"each worker will load the **entire MNIST dataset into memory**\".\r\nMy question is that why each worker needs to load all the data while using \"AutoShardPolicy.DATA\"? Why not the driver node gives each worker its portion of data instead? \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/525", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/525/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/525/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/525/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/525", "id": 641912534, "node_id": "MDU6SXNzdWU2NDE5MTI1MzQ=", "number": 525, "title": "'NoneType' object has no attribute 'hadoopConfiguration'", "user": {"login": "orwa-te", "id": 32763039, "node_id": "MDQ6VXNlcjMyNzYzMDM5", "avatar_url": "https://avatars2.githubusercontent.com/u/32763039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/orwa-te", "html_url": "https://github.com/orwa-te", "followers_url": "https://api.github.com/users/orwa-te/followers", "following_url": "https://api.github.com/users/orwa-te/following{/other_user}", "gists_url": "https://api.github.com/users/orwa-te/gists{/gist_id}", "starred_url": "https://api.github.com/users/orwa-te/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/orwa-te/subscriptions", "organizations_url": "https://api.github.com/users/orwa-te/orgs", "repos_url": "https://api.github.com/users/orwa-te/repos", "events_url": "https://api.github.com/users/orwa-te/events{/privacy}", "received_events_url": "https://api.github.com/users/orwa-te/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2020-06-19T11:47:30Z", "updated_at": "2020-07-03T19:36:03Z", "closed_at": "2020-07-03T19:36:03Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.7.7]\r\n - Spark version [2.5.4]\r\n - TensorFlow version [2.1.0]\r\n - TensorFlowOnSpark version [2.2.1]\r\n - Cluster version [Standalone]\r\n\r\n**Describe the bug:**\r\nI am trying to execute [MNIST using Keras](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_tf.py) on my Jupyter notebook to avoid using the terminal. I replaced ArgumentParser with this object to pass the same params directly to `TFCluster` :\r\n**class Args():\r\n       batch_size = 64\r\n       buffer_size=10000\r\n       cluster_size=num_executors\r\n       epochs=3\r\n       model_dir=\"D:\\Master\\practical\\TFOS\\mnist_model\"\r\n       export_dir=\"D:\\Master\\practical\\TFOS\\mnist_export\"\r\n       steps_per_epoch=469\r\n       tensorboard=\"store_true\"**\r\n And then used `args = Args()` to pass params as `args.model_dir` (for example) but I get the following errors:\r\n**Logs:**\r\n\r\n\r\n~\\Anaconda3\\envs\\spark_env_3.7\\lib\\site-packages\\tensorflowonspark\\TFCluster.py in run(sc, map_fun, tf_args, num_executors, num_ps, tensorboard, input_mode, log_dir, driver_ps_nodes, master_node, reservation_timeout, queues, eval_node)\r\n    272 \r\n    273   # get default filesystem from spark\r\n- 274   defaultFS = sc._jsc.hadoopConfiguration().get(\"fs.defaultFS\")\r\n    275   # strip trailing \"root\" slash from \"file:///\" to be consistent w/ \"hdfs://...\"\r\n    276   if defaultFS.startswith(\"file://\") and len(defaultFS) > 7 and defaultFS.endswith(\"/\"):\r\n\r\nAttributeError: 'NoneType' object has no attribute 'hadoopConfiguration'\r\n\r\n\r\n\r\n**Spark Submit Command Line:**\r\n None. Using Jupyter notebook\r\n\r\nHow to solve this? the code works fine when using `spark-submit` command but not working from Notebook!!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/521", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/521/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/521/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/521/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/521", "id": 613870419, "node_id": "MDU6SXNzdWU2MTM4NzA0MTk=", "number": 521, "title": "fails to save model", "user": {"login": "OUCWIND", "id": 10203192, "node_id": "MDQ6VXNlcjEwMjAzMTky", "avatar_url": "https://avatars1.githubusercontent.com/u/10203192?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OUCWIND", "html_url": "https://github.com/OUCWIND", "followers_url": "https://api.github.com/users/OUCWIND/followers", "following_url": "https://api.github.com/users/OUCWIND/following{/other_user}", "gists_url": "https://api.github.com/users/OUCWIND/gists{/gist_id}", "starred_url": "https://api.github.com/users/OUCWIND/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OUCWIND/subscriptions", "organizations_url": "https://api.github.com/users/OUCWIND/orgs", "repos_url": "https://api.github.com/users/OUCWIND/repos", "events_url": "https://api.github.com/users/OUCWIND/events{/privacy}", "received_events_url": "https://api.github.com/users/OUCWIND/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2020-05-07T08:26:13Z", "updated_at": "2020-08-20T20:11:57Z", "closed_at": "2020-07-07T19:51:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "hi,\r\nwhen i do things below:\r\n```\r\n  compat.export_saved_model(multi_worker_model, export_dir, ctx.job_name == 'chief')\r\n```\r\n\r\nIn most cases, it will fail to save a model.  look like it's because ctx.job_name problem.\r\n\r\nwhen i test code like this:\r\n\r\n```\r\nwith open(ctx.job_name, \"w\") as f:\r\n    f.write(ctx.job_name)\r\n```\r\ni find that the file \"chief\" was created once when program run ten times.  it's the same on aws EMR.\r\n\r\nthis question has stuck me for a long time,  do you have any idea about what may be the cause?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/520", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/520/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/520/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/520/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/520", "id": 613492719, "node_id": "MDU6SXNzdWU2MTM0OTI3MTk=", "number": 520, "title": "How to keep extra column when transform?", "user": {"login": "OUCWIND", "id": 10203192, "node_id": "MDQ6VXNlcjEwMjAzMTky", "avatar_url": "https://avatars1.githubusercontent.com/u/10203192?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OUCWIND", "html_url": "https://github.com/OUCWIND", "followers_url": "https://api.github.com/users/OUCWIND/followers", "following_url": "https://api.github.com/users/OUCWIND/following{/other_user}", "gists_url": "https://api.github.com/users/OUCWIND/gists{/gist_id}", "starred_url": "https://api.github.com/users/OUCWIND/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OUCWIND/subscriptions", "organizations_url": "https://api.github.com/users/OUCWIND/orgs", "repos_url": "https://api.github.com/users/OUCWIND/repos", "events_url": "https://api.github.com/users/OUCWIND/events{/privacy}", "received_events_url": "https://api.github.com/users/OUCWIND/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-05-06T17:35:40Z", "updated_at": "2020-05-11T15:16:07Z", "closed_at": "2020-05-11T15:16:07Z", "author_association": "NONE", "active_lock_reason": null, "body": "hi, \r\nwhen i predict like this:\r\n```\r\n    model = TFModel(args)\\\r\n        .setInputMapping({\"x\": \"input_x\"})\\\r\n        .setOutputMapping({\"predict\": \"prediction\"}) \\\r\n        .setSignatureDefKey('serving_default') \\\r\n        .setExportDir(\"/model\")\\\r\n        .setTagSet(\"serve\")\r\n        # .setSignatureDefKey(\"serving_default\")\r\n\r\n    def argmax_fn(l):\r\n        return max(range(len(l)), key=lambda i: l[i])\r\n\r\n    argmax = udf(argmax_fn, IntegerType())\r\n    preds= model.transform(df).withColumn(\"argmax\", argmax(\"prediction\"))\r\n    preds.show()\r\n```\r\nhere is the situation:\r\n df is a dataframe with columns[uid, x].\r\nfinally i got  a dataframe **preds** as result, with columns [prediction, argmax].\r\n\r\nThen what i need to do when i want **preds**  to be a dataframe with columns [uid, prediction, argmax].\r\n\r\nis there any idea?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/519", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/519/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/519/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/519/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/519", "id": 602406872, "node_id": "MDU6SXNzdWU2MDI0MDY4NzI=", "number": 519, "title": "Stack on Yarn cluster mode", "user": {"login": "Alwaysproblem", "id": 31947147, "node_id": "MDQ6VXNlcjMxOTQ3MTQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/31947147?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Alwaysproblem", "html_url": "https://github.com/Alwaysproblem", "followers_url": "https://api.github.com/users/Alwaysproblem/followers", "following_url": "https://api.github.com/users/Alwaysproblem/following{/other_user}", "gists_url": "https://api.github.com/users/Alwaysproblem/gists{/gist_id}", "starred_url": "https://api.github.com/users/Alwaysproblem/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Alwaysproblem/subscriptions", "organizations_url": "https://api.github.com/users/Alwaysproblem/orgs", "repos_url": "https://api.github.com/users/Alwaysproblem/repos", "events_url": "https://api.github.com/users/Alwaysproblem/events{/privacy}", "received_events_url": "https://api.github.com/users/Alwaysproblem/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2020-04-18T08:19:42Z", "updated_at": "2020-07-07T19:51:22Z", "closed_at": "2020-07-07T19:51:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.7.6\r\n - Spark version 2.4.4\r\n - TensorFlow version 2.1.0\r\n - TensorFlowOnSpark version 2.2.1\r\n - Cluster version  yarn 2.7.3, hadoop 2.7.3\r\n\r\n**Describe the bug:**\r\nA clear and concise description of what the bug is.\r\n\r\n1. when I try to run mnist_keras.py on cluster with InputMode.SPARK mode, it will raise the timeout exception and stuck.\r\n\r\n2. I try to run the same code under the tf.distribution.MirroredStrategy. The Timeout exception is gone but there is nothing under the output hdfs directory. \r\n\r\n**Logs:**\r\nIf applicable, add logs to help explain your problem.  Note: errors may not be fully described in the driver/console logs.  Make sure to check the executor logs for possible root causes.\r\n\r\n```\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: Found binding in [jar:file:/data02/hadoop/yarn/log/usercache/profile/filecache/20655/__spark_libs__8712252141296904713.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/hdp/2.5.6.0-40/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\n20/04/18 06:21:09 INFO util.SignalUtils: Registered signal handler for TERM\r\n20/04/18 06:21:09 INFO util.SignalUtils: Registered signal handler for HUP\r\n20/04/18 06:21:09 INFO util.SignalUtils: Registered signal handler for INT\r\n20/04/18 06:21:09 INFO spark.SecurityManager: Changing view acls to: nobody,profile\r\n20/04/18 06:21:09 INFO spark.SecurityManager: Changing modify acls to: nobody,profile\r\n20/04/18 06:21:09 INFO spark.SecurityManager: Changing view acls groups to: \r\n20/04/18 06:21:09 INFO spark.SecurityManager: Changing modify acls groups to: \r\n20/04/18 06:21:09 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nobody, profile); groups with view permissions: Set(); users  with modify permissions: Set(nobody, profile); groups with modify permissions: Set()\r\n20/04/18 06:21:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n20/04/18 06:21:10 INFO yarn.ApplicationMaster: Preparing Local resources\r\n20/04/18 06:21:10 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\r\n20/04/18 06:21:11 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.data.dir;  Ignoring.\r\n20/04/18 06:21:11 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.failed.volumes.tolerated;  Ignoring.\r\n20/04/18 06:21:11 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1582757194275_91455_000001\r\n20/04/18 06:21:11 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread\r\n20/04/18 06:21:11 INFO yarn.ApplicationMaster: Waiting for spark context initialization...\r\n20/04/18 06:21:17 INFO spark.SparkContext: Running Spark version 2.4.4\r\n20/04/18 06:21:17 INFO spark.SparkContext: Submitted application: mnist_keras\r\n20/04/18 06:21:17 INFO spark.SecurityManager: Changing view acls to: nobody,profile\r\n20/04/18 06:21:17 INFO spark.SecurityManager: Changing modify acls to: nobody,profile\r\n20/04/18 06:21:17 INFO spark.SecurityManager: Changing view acls groups to: \r\n20/04/18 06:21:17 INFO spark.SecurityManager: Changing modify acls groups to: \r\n20/04/18 06:21:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nobody, profile); groups with view permissions: Set(); users  with modify permissions: Set(nobody, profile); groups with modify permissions: Set()\r\n20/04/18 06:21:17 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.data.dir;  Ignoring.\r\n20/04/18 06:21:17 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.failed.volumes.tolerated;  Ignoring.\r\n20/04/18 06:21:17 INFO util.Utils: Successfully started service 'sparkDriver' on port 36018.\r\n20/04/18 06:21:17 INFO spark.SparkEnv: Registering MapOutputTracker\r\n20/04/18 06:21:17 INFO spark.SparkEnv: Registering BlockManagerMaster\r\n20/04/18 06:21:17 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n20/04/18 06:21:17 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n20/04/18 06:21:17 INFO storage.DiskBlockManager: Created local directory at /data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/blockmgr-ec0c175a-e346-4e7f-bb53-0b255ed38d7a\r\n20/04/18 06:21:17 INFO storage.DiskBlockManager: Created local directory at /data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/blockmgr-ee29d966-43d4-4387-9377-9c74d637c9a7\r\n20/04/18 06:21:17 INFO storage.DiskBlockManager: Created local directory at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/blockmgr-5e29c72d-e676-492b-93db-afa95c40d346\r\n20/04/18 06:21:17 INFO memory.MemoryStore: MemoryStore started with capacity 7.8 GB\r\n20/04/18 06:21:17 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.data.dir;  Ignoring.\r\n20/04/18 06:21:17 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.failed.volumes.tolerated;  Ignoring.\r\n20/04/18 06:21:17 INFO spark.SparkEnv: Registering OutputCommitCoordinator\r\n20/04/18 06:21:17 INFO util.log: Logging initialized @8740ms\r\n20/04/18 06:21:17 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\r\n20/04/18 06:21:17 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\r\n20/04/18 06:21:17 INFO server.Server: Started @8820ms\r\n20/04/18 06:21:17 INFO server.AbstractConnector: Started ServerConnector@22ad7a87{HTTP/1.1,[http/1.1]}{0.0.0.0:33206}\r\n20/04/18 06:21:17 INFO util.Utils: Successfully started service 'SparkUI' on port 33206.\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6500b5fb{/jobs,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e026131{/jobs/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@323ae64{/jobs/job,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26b6c41a{/jobs/job/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b302214{/stages,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fee998{/stages/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@955ecff{/stages/stage,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@712953bd{/stages/stage/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57bd7b73{/stages/pool,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5592fbe8{/stages/pool/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2845addb{/storage,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13cab7d5{/storage/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f90203c{/storage/rdd,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d9fc39d{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68318080{/environment,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e83e64c{/environment/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ca260{/executors,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13ac1ad1{/executors/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@777dbda4{/executors/threadDump,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@467d8481{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2337118c{/static,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70c40a5{/,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5370faf7{/api,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cf6316{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e6b9481{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:17 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://n49-01.fn.ams.osa:33206\r\n20/04/18 06:21:18 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler\r\n20/04/18 06:21:18 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1582757194275_91455 and attemptId Some(appattempt_1582757194275_91455_000001)\r\n20/04/18 06:21:18 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43006.\r\n20/04/18 06:21:18 INFO netty.NettyBlockTransferService: Server created on n49-01.fn.ams.osa:43006\r\n20/04/18 06:21:18 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n20/04/18 06:21:18 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, n49-01.fn.ams.osa, 43006, None)\r\n20/04/18 06:21:18 INFO storage.BlockManagerMasterEndpoint: Registering block manager n49-01.fn.ams.osa:43006 with 7.8 GB RAM, BlockManagerId(driver, n49-01.fn.ams.osa, 43006, None)\r\n20/04/18 06:21:18 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, n49-01.fn.ams.osa, 43006, None)\r\n20/04/18 06:21:18 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, n49-01.fn.ams.osa, 43006, None)\r\n20/04/18 06:21:18 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\r\n20/04/18 06:21:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a2b76{/metrics/json,null,AVAILABLE,@Spark}\r\n20/04/18 06:21:18 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.data.dir;  Ignoring.\r\n20/04/18 06:21:18 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.failed.volumes.tolerated;  Ignoring.\r\n20/04/18 06:21:18 INFO client.RMProxy: Connecting to ResourceManager at n17-07-04/172.17.28.13:8030\r\n20/04/18 06:21:18 INFO yarn.YarnRMClient: Registering the ApplicationMaster\r\n20/04/18 06:21:18 INFO yarn.ApplicationMaster: \r\n===============================================================================\r\nYARN executor launch context:\r\n  env:\r\n    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>/usr/hdp/2.5.6.0-40/hadoop/lib/opera-s3o-2.8.2.jar:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.5.6.0-40/hadoop/lib/hadoop-lzo-0.6.0.2.5.6.0-40.jar:/etc/hadoop/conf/secure<CPS>/usr/hdp/2.5.6.0-40/hadoop/conf:/usr/hdp/2.5.6.0-40/hadoop/lib/*:/usr/hdp/2.5.6.0-40/hadoop/.//*:/usr/hdp/2.5.6.0-40/hadoop-hdfs/./:/usr/hdp/2.5.6.0-40/hadoop-hdfs/lib/*:/usr/hdp/2.5.6.0-40/hadoop-hdfs/.//*:/usr/hdp/2.5.6.0-40/hadoop-yarn/lib/*:/usr/hdp/2.5.6.0-40/hadoop-yarn/.//*:/usr/hdp/2.5.6.0-40/hadoop-mapreduce/lib/*:/usr/hdp/2.5.6.0-40/hadoop-mapreduce/.//*::jdbc-mysql.jar:mysql-connector-java-5.1.37-bin.jar:mysql-connector-java.jar:/usr/hdp/2.5.6.0-40/tez/*:/usr/hdp/2.5.6.0-40/tez/lib/*:/usr/hdp/2.5.6.0-40/tez/conf<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\r\n    SPARK_DIST_CLASSPATH -> /usr/hdp/2.5.6.0-40/hadoop/conf:/usr/hdp/2.5.6.0-40/hadoop/lib/*:/usr/hdp/2.5.6.0-40/hadoop/.//*:/usr/hdp/2.5.6.0-40/hadoop-hdfs/./:/usr/hdp/2.5.6.0-40/hadoop-hdfs/lib/*:/usr/hdp/2.5.6.0-40/hadoop-hdfs/.//*:/usr/hdp/2.5.6.0-40/hadoop-yarn/lib/*:/usr/hdp/2.5.6.0-40/hadoop-yarn/.//*:/usr/hdp/2.5.6.0-40/hadoop-mapreduce/lib/*:/usr/hdp/2.5.6.0-40/hadoop-mapreduce/.//*::jdbc-mysql.jar:mysql-connector-java-5.1.37-bin.jar:mysql-connector-java.jar:/usr/hdp/2.5.6.0-40/tez/*:/usr/hdp/2.5.6.0-40/tez/lib/*:/usr/hdp/2.5.6.0-40/tez/conf\r\n    SPARK_YARN_STAGING_DIR -> hdfs://opera/user/profile/.sparkStaging/application_1582757194275_91455\r\n    SPARK_USER -> profile\r\n    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\r\n    LD_LIBRARY_PATH -> /usr/jdk1.8.0_65/jre/lib/amd64/server:/usr/lib/ams-hbase/lib/hadoop-native\r\n\r\n  command:\r\n    {{JAVA_HOME}}/bin/java \\ \r\n      -server \\ \r\n      -Xmx2048m \\ \r\n      -Djava.io.tmpdir={{PWD}}/tmp \\ \r\n      '-Dspark.network.timeout=60000s' \\ \r\n      '-Dspark.ui.port=0' \\ \r\n      '-Dspark.driver.port=36018' \\ \r\n      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \r\n      -XX:OnOutOfMemoryError='kill %p' \\ \r\n      org.apache.spark.executor.CoarseGrainedExecutorBackend \\ \r\n      --driver-url \\ \r\n      spark://CoarseGrainedScheduler@n49-01.fn.ams.osa:36018 \\ \r\n      --executor-id \\ \r\n      <executorId> \\ \r\n      --hostname \\ \r\n      <hostname> \\ \r\n      --cores \\ \r\n      1 \\ \r\n      --app-id \\ \r\n      application_1582757194275_91455 \\ \r\n      --user-class-path \\ \r\n      file:$PWD/__app__.jar \\ \r\n      --user-class-path \\ \r\n      file:$PWD/spark-tensorflow-connector_2.11-1.15.0.jar \\ \r\n      --user-class-path \\ \r\n      file:$PWD/tensorflow-hadoop-1.15.0.jar \\ \r\n      1><LOG_DIR>/stdout \\ \r\n      2><LOG_DIR>/stderr\r\n\r\n  resources:\r\n    __spark_conf__ -> resource { scheme: \"hdfs\" host: \"opera\" port: -1 file: \"/user/profile/.sparkStaging/application_1582757194275_91455/__spark_conf__.zip\" } size: 260043 timestamp: 1587190837754 type: ARCHIVE visibility: PRIVATE\r\n    tensorflow-hadoop-1.15.0.jar -> resource { scheme: \"hdfs\" host: \"opera\" port: -1 file: \"/user-profile/yongxi/spark/jars/tensorflow-hadoop-1.15.0.jar\" } size: 20185 timestamp: 1586937285532 type: FILE visibility: PUBLIC\r\n    pyspark.zip -> resource { scheme: \"hdfs\" host: \"opera\" port: -1 file: \"/user/profile/.sparkStaging/application_1582757194275_91455/pyspark.zip\" } size: 591770 timestamp: 1587190836002 type: FILE visibility: PRIVATE\r\n    spark-tensorflow-connector_2.11-1.15.0.jar -> resource { scheme: \"hdfs\" host: \"opera\" port: -1 file: \"/user-profile/yongxi/spark/jars/spark-tensorflow-connector_2.11-1.15.0.jar\" } size: 1416499 timestamp: 1577962542220 type: FILE visibility: PUBLIC\r\n    __spark_libs__ -> resource { scheme: \"hdfs\" host: \"opera\" port: -1 file: \"/user/profile/.sparkStaging/application_1582757194275_91455/__spark_libs__8712252141296904713.zip\" } size: 240078342 timestamp: 1587190829097 type: ARCHIVE visibility: PRIVATE\r\n    py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"opera\" port: -1 file: \"/user/profile/.sparkStaging/application_1582757194275_91455/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1587190836463 type: FILE visibility: PRIVATE\r\n    tf2dis_zip -> resource { scheme: \"hdfs\" host: \"opera\" port: -1 file: \"/user/profile/.sparkStaging/application_1582757194275_91455/tf2dis.zip\" } size: 989937152 timestamp: 1587190835162 type: ARCHIVE visibility: PRIVATE\r\n\r\n===============================================================================\r\n20/04/18 06:21:18 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@n49-01.fn.ams.osa:36018)\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Will request 5 executor container(s), each with 1 core(s) and 2432 MB memory (including 384 MB of overhead)\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Submitted 5 unlocalized container requests.\r\n20/04/18 06:21:18 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals\r\n20/04/18 06:21:18 INFO impl.AMRMClientImpl: Received new token for : n45-15.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO impl.AMRMClientImpl: Received new token for : n05-19.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Launching container container_e41_1582757194275_91455_01_000002 on host n45-15.fn.ams.osa for executor with ID 1\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Launching container container_e41_1582757194275_91455_01_000003 on host n05-19.fn.ams.osa for executor with ID 2\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: Opening proxy : n05-19.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: Opening proxy : n45-15.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO impl.AMRMClientImpl: Received new token for : n45-16.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO impl.AMRMClientImpl: Received new token for : n07-13.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO impl.AMRMClientImpl: Received new token for : n36-02.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Launching container container_e41_1582757194275_91455_01_000004 on host n45-16.fn.ams.osa for executor with ID 3\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Launching container container_e41_1582757194275_91455_01_000005 on host n07-13.fn.ams.osa for executor with ID 4\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Launching container container_e41_1582757194275_91455_01_000006 on host n36-02.fn.ams.osa for executor with ID 5\r\n20/04/18 06:21:18 INFO yarn.YarnAllocator: Received 3 containers from YARN, launching executors on 3 of them.\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: Opening proxy : n36-02.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: Opening proxy : n45-16.fn.ams.osa:45454\r\n20/04/18 06:21:18 INFO impl.ContainerManagementProtocolProxy: Opening proxy : n07-13.fn.ams.osa:45454\r\n20/04/18 06:21:22 INFO impl.AMRMClientImpl: Received new token for : n35-02.fn.ams.osa:45454\r\n20/04/18 06:21:22 INFO impl.AMRMClientImpl: Received new token for : n45-13.fn.ams.osa:45454\r\n20/04/18 06:21:22 INFO impl.AMRMClientImpl: Received new token for : n36-01.fn.ams.osa:45454\r\n20/04/18 06:21:22 INFO yarn.YarnAllocator: Received 3 containers from YARN, launching executors on 0 of them.\r\n20/04/18 06:21:48 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\r\n20/04/18 06:21:48 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done\r\n20/04/18 06:21:48 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.data.dir;  Ignoring.\r\n20/04/18 06:21:48 WARN conf.Configuration: __spark_hadoop_conf__.xml:an attempt to override final parameter: dfs.datanode.failed.volumes.tolerated;  Ignoring.\r\n20/04/18 06:21:48 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 352.4 KB, free 7.8 GB)\r\n20/04/18 06:21:48 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KB, free 7.8 GB)\r\n20/04/18 06:21:48 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on n49-01.fn.ams.osa:43006 (size: 34.0 KB, free: 7.8 GB)\r\n20/04/18 06:21:48 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n20/04/18 06:21:48 INFO spark.SparkContext: Starting job: foreachPartition at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:325\r\n20/04/18 06:21:48 INFO scheduler.DAGScheduler: Got job 0 (foreachPartition at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:325) with 5 output partitions\r\n20/04/18 06:21:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:325)\r\n20/04/18 06:21:48 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n20/04/18 06:21:48 INFO scheduler.DAGScheduler: Missing parents: List()\r\n20/04/18 06:21:48 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[3] at foreachPartition at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:325), which has no missing parents\r\n20/04/18 06:21:48 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 21.2 KB, free 7.8 GB)\r\n20/04/18 06:21:48 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.9 KB, free 7.8 GB)\r\n20/04/18 06:21:48 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on n49-01.fn.ams.osa:43006 (size: 14.9 KB, free: 7.8 GB)\r\n20/04/18 06:21:48 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\r\n20/04/18 06:21:48 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 0 (PythonRDD[3] at foreachPartition at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:325) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\r\n20/04/18 06:21:48 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 5 tasks\r\n20/04/18 06:21:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.64.31:39808) with ID 3\r\n20/04/18 06:21:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, n45-16.fn.ams.osa, executor 3, partition 0, PROCESS_LOCAL, 7842 bytes)\r\n20/04/18 06:21:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.64.30:5885) with ID 1\r\n20/04/18 06:21:51 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, n45-15.fn.ams.osa, executor 1, partition 1, PROCESS_LOCAL, 7842 bytes)\r\n20/04/18 06:21:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.30.26:46424) with ID 5\r\n20/04/18 06:21:51 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, n36-02.fn.ams.osa, executor 5, partition 2, PROCESS_LOCAL, 7842 bytes)\r\n20/04/18 06:21:51 INFO storage.BlockManagerMasterEndpoint: Registering block manager n45-16.fn.ams.osa:39779 with 912.3 MB RAM, BlockManagerId(3, n45-16.fn.ams.osa, 39779, None)\r\n20/04/18 06:21:51 INFO storage.BlockManagerMasterEndpoint: Registering block manager n45-15.fn.ams.osa:12705 with 912.3 MB RAM, BlockManagerId(1, n45-15.fn.ams.osa, 12705, None)\r\n20/04/18 06:21:51 INFO storage.BlockManagerMasterEndpoint: Registering block manager n36-02.fn.ams.osa:33307 with 912.3 MB RAM, BlockManagerId(5, n36-02.fn.ams.osa, 33307, None)\r\n20/04/18 06:21:51 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on n45-16.fn.ams.osa:39779 (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:21:51 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on n45-15.fn.ams.osa:12705 (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:21:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.30.172:53310) with ID 4\r\n20/04/18 06:21:51 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, n07-13.fn.ams.osa, executor 4, partition 3, PROCESS_LOCAL, 7842 bytes)\r\n20/04/18 06:21:51 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on n36-02.fn.ams.osa:33307 (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:21:51 INFO storage.BlockManagerMasterEndpoint: Registering block manager n07-13.fn.ams.osa:37433 with 912.3 MB RAM, BlockManagerId(4, n07-13.fn.ams.osa, 37433, None)\r\n20/04/18 06:21:51 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on n07-13.fn.ams.osa:37433 (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:21:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.17.30.206:42536) with ID 2\r\n20/04/18 06:21:52 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, n05-19.fn.ams.osa, executor 2, partition 4, PROCESS_LOCAL, 7842 bytes)\r\n20/04/18 06:21:52 INFO storage.BlockManagerMasterEndpoint: Registering block manager n05-19.fn.ams.osa:41581 with 912.3 MB RAM, BlockManagerId(2, n05-19.fn.ams.osa, 41581, None)\r\n20/04/18 06:21:52 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on n05-19.fn.ams.osa:41581 (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:22:09 INFO mapred.FileInputFormat: Total input paths to process : 10\r\n20/04/18 06:22:09 INFO spark.SparkContext: Starting job: collect at PythonRDD.scala:166\r\n20/04/18 06:22:09 INFO scheduler.DAGScheduler: Got job 1 (collect at PythonRDD.scala:166) with 100 output partitions\r\n20/04/18 06:22:09 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at PythonRDD.scala:166)\r\n20/04/18 06:22:09 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n20/04/18 06:22:09 INFO scheduler.DAGScheduler: Missing parents: List()\r\n20/04/18 06:22:09 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\r\n20/04/18 06:22:09 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KB, free 7.8 GB)\r\n20/04/18 06:22:09 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 9.2 KB, free 7.8 GB)\r\n20/04/18 06:22:09 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on n49-01.fn.ams.osa:43006 (size: 9.2 KB, free: 7.8 GB)\r\n20/04/18 06:22:09 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161\r\n20/04/18 06:22:09 INFO scheduler.DAGScheduler: Submitting 100 missing tasks from ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\r\n20/04/18 06:22:09 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 100 tasks\r\n20/04/18 06:22:10 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 17471 ms on n05-19.fn.ams.osa (executor 2) (1/5)\r\n20/04/18 06:22:10 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 18400 ms on n07-13.fn.ams.osa (executor 4) (2/5)\r\n20/04/18 06:22:10 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 51638\r\n20/04/18 06:22:10 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 18950 ms on n45-16.fn.ams.osa (executor 3) (3/5)\r\n20/04/18 06:22:10 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 1.0 (TID 5, n45-15.fn.ams.osa, executor 1, partition 9, NODE_LOCAL, 8030 bytes)\r\n20/04/18 06:22:10 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 19080 ms on n45-15.fn.ams.osa (executor 1) (4/5)\r\n20/04/18 06:22:10 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on n45-15.fn.ams.osa:12705 (size: 9.2 KB, free: 912.3 MB)\r\n20/04/18 06:22:10 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on n45-15.fn.ams.osa:12705 (size: 34.0 KB, free: 912.2 MB)\r\n20/04/18 06:22:10 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 19236 ms on n36-02.fn.ams.osa (executor 5) (5/5)\r\n20/04/18 06:22:10 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \r\n20/04/18 06:22:10 INFO scheduler.DAGScheduler: ResultStage 0 (foreachPartition at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:325) finished in 22.026 s\r\n20/04/18 06:22:10 INFO scheduler.DAGScheduler: Job 0 finished: foreachPartition at /data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:325, took 22.073627 s\r\n20/04/18 06:22:14 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 6, n45-16.fn.ams.osa, executor 3, partition 7, RACK_LOCAL, 8030 bytes)\r\n20/04/18 06:22:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 7, n36-02.fn.ams.osa, executor 5, partition 0, RACK_LOCAL, 8030 bytes)\r\n20/04/18 06:22:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 8, n05-19.fn.ams.osa, executor 2, partition 3, RACK_LOCAL, 8030 bytes)\r\n20/04/18 06:22:14 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on n45-16.fn.ams.osa:39779 (size: 9.2 KB, free: 912.3 MB)\r\n20/04/18 06:22:14 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on n36-02.fn.ams.osa:33307 (size: 9.2 KB, free: 912.3 MB)\r\n20/04/18 06:22:14 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on n45-16.fn.ams.osa:39779 (size: 34.0 KB, free: 912.2 MB)\r\n20/04/18 06:22:14 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on n05-19.fn.ams.osa:41581 (size: 9.2 KB, free: 912.3 MB)\r\n20/04/18 06:22:14 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on n36-02.fn.ams.osa:33307 (size: 34.0 KB, free: 912.2 MB)\r\n20/04/18 06:22:14 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on n05-19.fn.ams.osa:41581 (size: 34.0 KB, free: 912.2 MB)\r\n20/04/18 06:22:17 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 1.0 (TID 9, n05-19.fn.ams.osa, executor 2, partition 13, RACK_LOCAL, 8030 bytes)\r\n20/04/18 06:22:17 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 1.0 (TID 8, n05-19.fn.ams.osa, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 491, in _train\r\nException: Exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 407, in wrapper_fn_background\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n  File \"mnist_spark.py\", line 58, in main_fun\r\n  File \"mnist_spark.py\", line 21, in build_and_compile_cnn_model\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 116, in __init__\r\n    self.add(layer)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\", line 185, in add\r\n    layer(x)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 748, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 2116, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/convolutional.py\", line 158, in build\r\n    dtype=self.dtype)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 446, in add_weight\r\n    caching_device=caching_device)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1330, in creator_with_resource_vars\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 548, in _create_variable\r\n    values.SyncOnReadVariable, *args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py\", line 1034, in create_mirrored_variable\r\n    value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 540, in _real_mirrored_creator\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\", line 2596, in default_variable_creator\r\n    shape=shape)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1411, in __init__\r\n    distribute_strategy=distribute_strategy)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 1542, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 383, in initial_value_fn\r\n    collective_instance_key)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/collective_ops.py\", line 176, in broadcast_recv\r\n    communication_hint=communication_hint.lower())\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_collective_ops.py\", line 57, in collective_bcast_recv\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: Connection refused\r\nAdditional GRPC error information:\r\n{\"created\":\"@1587190930.085819938\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Connection refused\",\"grpc_status\":14} [Op:CollectiveBcastRecv]\r\n\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n20/04/18 06:22:21 INFO scheduler.TaskSetManager: Starting task 3.1 in stage 1.0 (TID 10, n07-13.fn.ams.osa, executor 4, partition 3, ANY, 8030 bytes)\r\n20/04/18 06:22:21 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on n07-13.fn.ams.osa:37433 (size: 9.2 KB, free: 912.3 MB)\r\n20/04/18 06:22:21 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on n07-13.fn.ams.osa:37433 (size: 34.0 KB, free: 912.2 MB)\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 11\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 25\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 8\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 18\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 16\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 15\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 12\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 9\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 24\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 6\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 13\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 7\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 3\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 22\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 23\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 19\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 1\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 2\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 20\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 14\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 21\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 4\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 17\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 10\r\n20/04/18 06:24:18 INFO spark.ContextCleaner: Cleaned accumulator 5\r\n20/04/18 06:24:18 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on n49-01.fn.ams.osa:43006 in memory (size: 14.9 KB, free: 7.8 GB)\r\n20/04/18 06:24:18 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on n45-16.fn.ams.osa:39779 in memory (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:24:18 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on n05-19.fn.ams.osa:41581 in memory (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:24:18 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on n07-13.fn.ams.osa:37433 in memory (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:24:18 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on n45-15.fn.ams.osa:12705 in memory (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:24:18 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on n36-02.fn.ams.osa:33307 in memory (size: 14.9 KB, free: 912.3 MB)\r\n20/04/18 06:32:14 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 1.0 (TID 11, n45-15.fn.ams.osa, executor 1, partition 19, NODE_LOCAL, 8030 bytes)\r\n20/04/18 06:32:14 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 1.0 (TID 5, n45-15.fn.ams.osa, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000002/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000002/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 495, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n20/04/18 06:32:17 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 12, n36-02.fn.ams.osa, executor 5, partition 1, RACK_LOCAL, 8030 bytes)\r\n20/04/18 06:32:17 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 7, n36-02.fn.ams.osa, executor 5): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000006/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000006/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 495, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n20/04/18 06:32:17 INFO scheduler.TaskSetManager: Starting task 9.1 in stage 1.0 (TID 13, n45-16.fn.ams.osa, executor 3, partition 9, RACK_LOCAL, 8030 bytes)\r\n20/04/18 06:32:17 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 1.0 (TID 6, n45-16.fn.ams.osa, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000004/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000004/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 495, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n20/04/18 06:32:24 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 1.0 (TID 14, n05-19.fn.ams.osa, executor 2, partition 23, RACK_LOCAL, 8030 bytes)\r\n20/04/18 06:32:24 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 1.0 (TID 9, n05-19.fn.ams.osa, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000003/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 495, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n20/04/18 06:32:24 WARN scheduler.TaskSetManager: Lost task 3.1 in stage 1.0 (TID 10, n07-13.fn.ams.osa, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000005/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/data02/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/data01/hadoop/yarn/log/usercache/profile/appcache/application_1582757194275_91455/container_e41_1582757194275_91455_01_000001/tf2dis_zip/tf2dis/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 495, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n20/04/18 06:32:28 INFO scheduler.TaskSetManager: Starting task 3.2 in stage 1.0 (TID 15, n07-13.fn.ams.osa, executor 4, partition 3, ANY, 8030 bytes)\r\n```\r\n**Spark Submit Command Line:**\r\nIf applicable, add your spark-submit command line.\r\n\r\n```\r\nset -ex\r\n# set environment variables (if not already done)\r\nexport LD_LIBRARY_PATH=${PATH}\r\nexport PYSPARK_PYTHON=\"./${CONDAENV}_zip/${CONDAENV}/bin/python\"\r\nexport QUEUE=adx\r\nexport SPARK_HOME=/home/sdev/yongxi/spark-2.4.4-bin-hadoop2.7\r\n\r\n# set paths to libjvm.so, libhdfs.so, and libcuda*.so\r\n#export LIB_HDFS=/opt/cloudera/parcels/CDH/lib64                         # for CDH (per @wangyum)\r\nexport LIB_HDFS=/usr/lib/ams-hbase/lib/hadoop-native                     # path to libhdfs.so, for TF acccess to HDFS\r\nexport LIB_JVM=$JAVA_HOME/jre/lib/amd64/server                           # path to libjvm.so\r\n# export LIB_CUDA=/usr/local/cuda-7.5/lib64                              # for GPUs only\r\n\r\n# on the cluster the path for lihdfs.so and libjvm.so\r\n# /usr/hdp/2.5.6.0-40/usr/lib/libhdfs.so\r\n# /usr/lib/ams-hbase/lib/hadoop-native/libhdfs.so\r\nexport HADOOP_HDFS_HOME=/usr/hdp/2.5.6.0-40/hadoop-hdfs\r\n\r\n# jar Package on the air\r\nTFCONNECTOR=hdfs:///user-profile/yongxi/spark/jars/spark-tensorflow-connector_2.11-1.15.0.jar\r\nTFHADOOP=hdfs:///user-profile/yongxi/spark/jars/tensorflow-hadoop-1.15.0.jar\r\n\r\n# conda env\r\nCONDAENV=tf2dis\r\n\r\n# spark configuration\r\nSPARK_WORKER_INSTANCES=5\r\nEXECUTOR_MEMORY=2G\r\n\r\n# Train configuration\r\nEPOCHS=2\r\n\r\nINPUT_DATA=/user-profile/yongxi/spark/input/mnist/csv/train\r\nMODEL_DIR=/tmp/yongxi/tfoutput/mnist_model\r\nEXPORT_DIR=/tmp/yongxi/tfoutput/mnist_export\r\n\r\nsudo -u profile hadoop fs -rm -r -f -skipTrash ${MODEL_DIR}/*\r\nsudo -u profile hadoop fs -rm -r -f -skipTrash ${EXPORT_DIR}/*\r\n\r\n\r\nsudo -u profile ${SPARK_HOME}/bin/spark-submit \\\r\n                    --master yarn \\\r\n                    --deploy-mode cluster \\\r\n                    --queue ${QUEUE} \\\r\n                    --num-executors ${SPARK_WORKER_INSTANCES} \\\r\n                    --executor-memory ${EXECUTOR_MEMORY} \\\r\n                    --conf spark.dynamicAllocation.enabled=false \\\r\n                    --conf spark.yarn.maxAppAttempts=1 \\\r\n                    --conf \"spark.yarn.appMasterEnv.PYSPARK_PYTHON=./${CONDAENV}_zip/${CONDAENV}/bin/python\" \\\r\n                    --conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\n                    --conf spark.network.timeout=60000s \\\r\n                    --archives \"../${CONDAENV}.zip#${CONDAENV}_zip\" \\\r\n                    --jars ${TFCONNECTOR},${TFHADOOP} \\\r\n                    mnist_spark.py \\\r\n                        --cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n                        --epochs ${EPOCHS} \\\r\n                        --images_labels ${INPUT_DATA} \\\r\n                        --model_dir ${MODEL_DIR} \\\r\n                        --export_dir ${EXPORT_DIR}\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/518", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/518/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/518/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/518/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/518", "id": 596335074, "node_id": "MDU6SXNzdWU1OTYzMzUwNzQ=", "number": 518, "title": "1.4.4 exception: Timeout while feeding partition", "user": {"login": "MaQianheng", "id": 43992503, "node_id": "MDQ6VXNlcjQzOTkyNTAz", "avatar_url": "https://avatars1.githubusercontent.com/u/43992503?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MaQianheng", "html_url": "https://github.com/MaQianheng", "followers_url": "https://api.github.com/users/MaQianheng/followers", "following_url": "https://api.github.com/users/MaQianheng/following{/other_user}", "gists_url": "https://api.github.com/users/MaQianheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/MaQianheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MaQianheng/subscriptions", "organizations_url": "https://api.github.com/users/MaQianheng/orgs", "repos_url": "https://api.github.com/users/MaQianheng/repos", "events_url": "https://api.github.com/users/MaQianheng/events{/privacy}", "received_events_url": "https://api.github.com/users/MaQianheng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-04-08T06:15:36Z", "updated_at": "2020-04-08T15:40:54Z", "closed_at": "2020-04-08T07:01:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, I am running the mnist example with\r\n```\r\nspark-submit \\\r\n--master yarn \\\r\n--py-files ~/software/TensorFlowOnSpark-1.4.4/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=2 \\\r\n--conf spark.task.cpus=1 \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n~/software/TensorFlowOnSpark-1.4.4/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size 2 \\\r\n--images examples/mnist/csv/train/images \\\r\n--labels examples/mnist/csv/train/labels \\\r\n--format csv \\\r\n--mode train \\\r\n--model file:///home/pi/examples/mnist/mnist_model\r\n```\r\nIf I save the output in the local, it work well.\r\n\r\nWhen I try to save the output to the hdfs, it will stuck and show 'exception: Timeout while feeding partition'. How to solve it ?\ud83d\ude4f\r\n```\r\nspark-submit \\\r\n--master yarn \\\r\n--py-files ~/software/TensorFlowOnSpark-1.4.4/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=2 \\\r\n--conf spark.task.cpus=1 \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n~/software/TensorFlowOnSpark-1.4.4/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size 2 \\\r\n--images examples/mnist/csv/train/images \\\r\n--labels examples/mnist/csv/train/labels \\\r\n--format csv \\\r\n--mode train \\\r\n--model mnist_model\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/517", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/517/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/517/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/517/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/517", "id": 591047801, "node_id": "MDU6SXNzdWU1OTEwNDc4MDE=", "number": 517, "title": "Jupyter running error when reading a txt file and turning it into DF.", "user": {"login": "KayvT", "id": 40730865, "node_id": "MDQ6VXNlcjQwNzMwODY1", "avatar_url": "https://avatars1.githubusercontent.com/u/40730865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KayvT", "html_url": "https://github.com/KayvT", "followers_url": "https://api.github.com/users/KayvT/followers", "following_url": "https://api.github.com/users/KayvT/following{/other_user}", "gists_url": "https://api.github.com/users/KayvT/gists{/gist_id}", "starred_url": "https://api.github.com/users/KayvT/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KayvT/subscriptions", "organizations_url": "https://api.github.com/users/KayvT/orgs", "repos_url": "https://api.github.com/users/KayvT/repos", "events_url": "https://api.github.com/users/KayvT/events{/privacy}", "received_events_url": "https://api.github.com/users/KayvT/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2020-03-31T11:59:07Z", "updated_at": "2020-04-02T06:22:49Z", "closed_at": "2020-04-02T06:22:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.7.5\r\n - Spark version 3.0.0-preview2\r\n- Scala version 2.12.10 \r\n - Cluster version: hadoop 2.7\r\n\r\nI am trying to reading a simple text file that hsa 10 lines of text. When I read it normally without calling toDF(), it reads it and I can easily see it using `.collect()`. Can someone please tell me what this error mean? \r\n\r\nTHE CODE\r\n```\r\npyspark.sql import `SparkSession\r\nimport plotly.express as px  \r\nfrom pyspark.sql.types import FloatType \r\nimport fasttext as ft \r\nimport twint\r\nft.FastText.eprint = print\r\n\r\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\r\nsc = spark.sparkContext\r\n\r\nlid_model = ft.load_model(\"lid.176.ftz\") \r\n\r\negyptHandles = sc.textFile('egyptHandlesTemp.txt').toDF()\r\negyptHandles.show()\r\n\r\n```\r\n**THE ERROR:**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nPy4JJavaError                             Traceback (most recent call last)\r\n<ipython-input-42-ab96515126ec> in <module>\r\n      5 \r\n      6 # egyptHandles = readTxtToDF('egyptHandlesTemp.txt')\r\n----> 7 egyptHandles = sc.textFile('egyptHandlesTemp.txt').toDF()\r\n      8 # turkeyHandles = readTxtToDF('turkeyHandles.txt')\r\n      9 # usaHandles = readTxtToDF('usaHandles.txt')\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/session.py in toDF(self, schema, sampleRatio)\r\n     56         [Row(name=u'Alice', age=1)]\r\n     57         \"\"\"\r\n---> 58         return sparkSession.createDataFrame(self, schema, sampleRatio)\r\n     59 \r\n     60     RDD.toDF = toDF\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/session.py in createDataFrame(self, data, schema, samplingRatio, verifySchema)\r\n    783 \r\n    784         if isinstance(data, RDD):\r\n--> 785             rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\r\n    786         else:\r\n    787             rdd, schema = self._createFromLocal(map(prepare, data), schema)\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/session.py in _createFromRDD(self, rdd, schema, samplingRatio)\r\n    417         \"\"\"\r\n    418         if schema is None or isinstance(schema, (list, tuple)):\r\n--> 419             struct = self._inferSchema(rdd, samplingRatio, names=schema)\r\n    420             converter = _create_converter(struct)\r\n    421             rdd = rdd.map(converter)\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/session.py in _inferSchema(self, rdd, samplingRatio, names)\r\n    388         :return: :class:`pyspark.sql.types.StructType`\r\n    389         \"\"\"\r\n--> 390         first = rdd.first()\r\n    391         if not first:\r\n    392             raise ValueError(\"The first row in RDD is empty, \"\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py in first(self)\r\n   1449         ValueError: RDD is empty\r\n   1450         \"\"\"\r\n-> 1451         rs = self.take(1)\r\n   1452         if rs:\r\n   1453             return rs[0]\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/rdd.py in take(self, num)\r\n   1431 \r\n   1432             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))\r\n-> 1433             res = self.context.runJob(self, takeUpToNumLeft, p)\r\n   1434 \r\n   1435             items += res\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/context.py in runJob(self, rdd, partitionFunc, partitions, allowLocal)\r\n   1121         # SparkContext#runJob.\r\n   1122         mappedRDD = rdd.mapPartitions(partitionFunc)\r\n-> 1123         sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\r\n   1124         return list(_load_from_socket(sock_info, mappedRDD._jrdd_deserializer))\r\n   1125 \r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py in __call__(self, *args)\r\n   1284         answer = self.gateway_client.send_command(command)\r\n   1285         return_value = get_return_value(\r\n-> 1286             answer, self.gateway_client, self.target_id, self.name)\r\n   1287 \r\n   1288         for temp_arg in temp_args:\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw)\r\n     96     def deco(*a, **kw):\r\n     97         try:\r\n---> 98             return f(*a, **kw)\r\n     99         except py4j.protocol.Py4JJavaError as e:\r\n    100             converted = convert_exception(e.java_exception)\r\n\r\n~/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\r\n    326                 raise Py4JJavaError(\r\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\r\n--> 328                     format(target_id, \".\", name), value)\r\n    329             else:\r\n    330                 raise Py4JError(\r\n\r\nPy4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, pad, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/khaled/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 469, in main\r\n    (\"%d.%d\" % sys.version_info[:2], version))\r\nException: Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2156)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1989)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1977)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1976)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1976)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:956)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:956)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2155)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2144)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:758)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2137)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2156)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/khaled/CODE/spark/spark-3.0.0-preview2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 469, in main\r\n    (\"%d.%d\" % sys.version_info[:2], version))\r\nException: Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:484)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:619)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:602)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:437)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:154)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2156)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:441)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:444)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/516", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/516/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/516/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/516/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/516", "id": 586332005, "node_id": "MDU6SXNzdWU1ODYzMzIwMDU=", "number": 516, "title": "Handling big dataset with YARN and tfos", "user": {"login": "macro128", "id": 29261232, "node_id": "MDQ6VXNlcjI5MjYxMjMy", "avatar_url": "https://avatars0.githubusercontent.com/u/29261232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/macro128", "html_url": "https://github.com/macro128", "followers_url": "https://api.github.com/users/macro128/followers", "following_url": "https://api.github.com/users/macro128/following{/other_user}", "gists_url": "https://api.github.com/users/macro128/gists{/gist_id}", "starred_url": "https://api.github.com/users/macro128/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/macro128/subscriptions", "organizations_url": "https://api.github.com/users/macro128/orgs", "repos_url": "https://api.github.com/users/macro128/repos", "events_url": "https://api.github.com/users/macro128/events{/privacy}", "received_events_url": "https://api.github.com/users/macro128/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-23T16:17:01Z", "updated_at": "2020-05-11T15:17:25Z", "closed_at": "2020-05-11T15:17:25Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi,\r\nI'm trying to use this library on a project where we've got to train a network in a really big dataset (over 9,000,000,000 records), because of its size we cannot load the entire dataset in a single machine and train on it. After some research, I found tfos and tried it, but even though it works with relatively big portions of the data (12 million records) it always fails when we try to go further (no more memory issue).\r\n\r\nNow we're trying to understand what the library does to decide if it's the correct choice for us or not. \r\nUsing Spark input mode, it partitions the dataset, distributes it to the executors and each one reads each batch's records lazily until there's nothing left, right? If that's how it works, we think it should be able to handle the entire dataset no matter its size.  \r\nDoes tfos works that way or it loads the entire partition into an executor's memory before training?\r\n\r\nThe dataset is saved in the HDFS so we could use Tensorflow input mode, but in that case, every executor would load the entire dataset in memory before training, right?\r\n\r\nHope you can take the time to help us solve our doubts!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/515", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/515/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/515/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/515/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/515", "id": 586321403, "node_id": "MDU6SXNzdWU1ODYzMjE0MDM=", "number": 515, "title": "A question about num_executors and TFCluster's cluster_size", "user": {"login": "maqy1995", "id": 24785328, "node_id": "MDQ6VXNlcjI0Nzg1MzI4", "avatar_url": "https://avatars3.githubusercontent.com/u/24785328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maqy1995", "html_url": "https://github.com/maqy1995", "followers_url": "https://api.github.com/users/maqy1995/followers", "following_url": "https://api.github.com/users/maqy1995/following{/other_user}", "gists_url": "https://api.github.com/users/maqy1995/gists{/gist_id}", "starred_url": "https://api.github.com/users/maqy1995/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maqy1995/subscriptions", "organizations_url": "https://api.github.com/users/maqy1995/orgs", "repos_url": "https://api.github.com/users/maqy1995/repos", "events_url": "https://api.github.com/users/maqy1995/events{/privacy}", "received_events_url": "https://api.github.com/users/maqy1995/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-03-23T16:02:31Z", "updated_at": "2020-03-25T01:36:32Z", "closed_at": "2020-03-25T01:36:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.7.6\r\n - Spark version 2.4.5\r\n - TensorFlow version 2.1.0\r\n - TensorFlowOnSpark version 2.2.0\r\n - Hadoop 3.2.1\r\n\r\nI am learning to use TFOS recently, I run the code ${TFoS_HOME}/examples/keras/minist_spark.py successfully.\r\nAfter that, I noticed that there is a _cluster_size_ parameter in TFcluster.run (), which can be modified by _--cluster_size_. I try to set the _cluster_size_ smaller than _num_executors_, but it throws some errors and gives me a hint:\r\n\r\n`\r\n\"No executor_id file found on this node, please ensure that:\\n\" + \\\r\n          \"1. Spark num_executors matches TensorFlow cluster_size\\n\" + \\\r\n          \"2. Spark tasks per executor is 1\\n\" + \\\r\n          \"3. Spark dynamic allocation is disabled\\n\" + \\\r\n          \"4. There are no other root-cause exceptions on other nodes\\n\"\r\n`\r\n\r\nI want to know if TFCluster's cluster_size less than num_executors?\r\nIf the answer is no, what is the reason?\r\nthanks.\r\n\r\nps:\r\nThe reason I do this is that I have two machines, but only one has GPUs. I want to use both machines' CPUs for Spark calculations to get an RDD, then give this RDD to Tensorflow for deep learning(use GPUs). In other words, the initial calculation of a job is given to the CPUs of the two machines, and the TFCluster started afterward runs Tensorflow only on the machine with GPUs.\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/514", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/514/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/514/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/514/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/514", "id": 584335399, "node_id": "MDU6SXNzdWU1ODQzMzUzOTk=", "number": 514, "title": "TypeError: interleave() missing 1 required positional argument: 'cycle_length' while running mnist example", "user": {"login": "siyu1992", "id": 45645303, "node_id": "MDQ6VXNlcjQ1NjQ1MzAz", "avatar_url": "https://avatars3.githubusercontent.com/u/45645303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/siyu1992", "html_url": "https://github.com/siyu1992", "followers_url": "https://api.github.com/users/siyu1992/followers", "following_url": "https://api.github.com/users/siyu1992/following{/other_user}", "gists_url": "https://api.github.com/users/siyu1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/siyu1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/siyu1992/subscriptions", "organizations_url": "https://api.github.com/users/siyu1992/orgs", "repos_url": "https://api.github.com/users/siyu1992/repos", "events_url": "https://api.github.com/users/siyu1992/events{/privacy}", "received_events_url": "https://api.github.com/users/siyu1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2020-03-19T11:11:46Z", "updated_at": "2020-03-20T09:36:45Z", "closed_at": "2020-03-20T09:36:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [e.g. 2.3.2]\r\n - TensorFlow version [e.g. 2.1.0]\r\n - TensorFlowOnSpark version [e.g. 2.2.1]\r\n - Cluster version [e.g. Client, Hadoop 3.1]\r\n\r\n**Describe the bug:**\r\nI tried to run mnist example on tfos(version 2.2.1), I got an exception of TypeError(interleave() missing 1 required positional argument: 'cycle_length'), I'm not sure if there is something wrong with my environment, I have already installed tensorflowonspark and tensorflow on each node, please help me with this, thanks very much\r\n\r\n**Logs:**\r\n2020-03-20 06:35:31,771 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2020-03-20 14:35:33.301053: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/:/usr/hdp/3.1.0.0-78/usr/lib/\r\n2020-03-20 14:35:33.301152: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server/:/usr/hdp/3.1.0.0-78/usr/lib/\r\n2020-03-20 14:35:33.301164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-03-20 06:35:34,327 INFO spark.SparkContext: Running Spark version 2.3.2.3.1.0.0-78\r\n2020-03-20 06:35:34,347 INFO spark.SparkContext: Submitted application: mnist_keras\r\n2020-03-20 06:35:34,403 INFO spark.SecurityManager: Changing view acls to: root,hdfs\r\n2020-03-20 06:35:34,403 INFO spark.SecurityManager: Changing modify acls to: root,hdfs\r\n2020-03-20 06:35:34,404 INFO spark.SecurityManager: Changing view acls groups to: \r\n2020-03-20 06:35:34,404 INFO spark.SecurityManager: Changing modify acls groups to: \r\n2020-03-20 06:35:34,404 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, hdfs); groups with view permissions: Set(); users  with modify permissions: Set(root, hdfs); groups with modify permissions: Set()\r\n2020-03-20 06:35:34,619 WARN util.Utils: Service 'sparkDriver' could not bind on port 40001. Attempting port 40002.\r\n2020-03-20 06:35:34,622 WARN util.Utils: Service 'sparkDriver' could not bind on port 40002. Attempting port 40003.\r\n2020-03-20 06:35:34,627 INFO util.Utils: Successfully started service 'sparkDriver' on port 40003.\r\n2020-03-20 06:35:34,648 INFO spark.SparkEnv: Registering MapOutputTracker\r\n2020-03-20 06:35:34,663 INFO spark.SparkEnv: Registering BlockManagerMaster\r\n2020-03-20 06:35:34,666 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n2020-03-20 06:35:34,666 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n2020-03-20 06:35:34,673 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-fdad0ca1-ef47-4a1d-80c2-c2ba855caf6d\r\n2020-03-20 06:35:34,689 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\r\n2020-03-20 06:35:34,731 INFO spark.SparkEnv: Registering OutputCommitCoordinator\r\n2020-03-20 06:35:34,792 INFO util.log: Logging initialized @4002ms\r\n2020-03-20 06:35:34,844 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n2020-03-20 06:35:34,858 INFO server.Server: Started @4069ms\r\n2020-03-20 06:35:34,873 INFO server.AbstractConnector: Started ServerConnector@749791ba{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n2020-03-20 06:35:34,874 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\r\n2020-03-20 06:35:34,892 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@569935c5{/jobs,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,893 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6064e1f4{/jobs/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,893 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15d37f85{/jobs/job,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,895 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57931237{/jobs/job/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,895 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10bf3270{/stages,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,896 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e9e48b5{/stages/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,896 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@724f670a{/stages/stage,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,897 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38706971{/stages/stage/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,898 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@318e2e42{/stages/pool,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,898 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1aa9158c{/stages/pool/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,899 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74452c5d{/storage,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,899 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fc5f845{/storage/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,900 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ff923c1{/storage/rdd,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,900 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d2e52c7{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,901 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54aafb{/environment,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,901 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60b36d64{/environment/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,902 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2692ad98{/executors,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9856be7{/executors/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,903 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ca31b5e{/executors/threadDump,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,904 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57808be3{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,909 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@644d02e6{/static,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,909 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65255b5f{/,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,910 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54b1b93c{/api,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,911 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1eeff329{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,911 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56375c2b{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:34,912 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.53.47:4040\r\n2020-03-20 06:35:35,612 INFO client.RMProxy: Connecting to ResourceManager at /172.16.53.47:8050\r\n2020-03-20 06:35:35,789 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers\r\n2020-03-20 06:35:35,835 INFO conf.Configuration: resource-types.xml not found\r\n2020-03-20 06:35:35,836 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n2020-03-20 06:35:35,849 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2048 MB per container)\r\n2020-03-20 06:35:35,850 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\r\n2020-03-20 06:35:35,850 INFO yarn.Client: Setting up container launch context for our AM\r\n2020-03-20 06:35:35,853 INFO yarn.Client: Setting up the launch environment for our AM container\r\n2020-03-20 06:35:35,858 INFO yarn.Client: Preparing resources for our AM container\r\n2020-03-20 06:35:36,959 INFO yarn.Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz\r\n2020-03-20 06:35:36,961 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz\r\n2020-03-20 06:35:37,017 INFO yarn.Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz\r\n2020-03-20 06:35:37,018 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz\r\n2020-03-20 06:35:37,026 INFO yarn.Client: Uploading resource file:/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1582250849858_0772/pyspark.zip\r\n2020-03-20 06:35:37,201 INFO yarn.Client: Uploading resource file:/usr/hdp/3.1.0.0-78/spark2/python/lib/py4j-0.10.7-src.zip -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1582250849858_0772/py4j-0.10.7-src.zip\r\n2020-03-20 06:35:37,356 INFO yarn.Client: Uploading resource file:/tmp/spark-53ff808b-8019-420f-a266-466f5736ee01/__spark_conf__7131703926022795889.zip -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1582250849858_0772/__spark_conf__.zip\r\n2020-03-20 06:35:37,399 INFO spark.SecurityManager: Changing view acls to: root,hdfs\r\n2020-03-20 06:35:37,399 INFO spark.SecurityManager: Changing modify acls to: root,hdfs\r\n2020-03-20 06:35:37,399 INFO spark.SecurityManager: Changing view acls groups to: \r\n2020-03-20 06:35:37,399 INFO spark.SecurityManager: Changing modify acls groups to: \r\n2020-03-20 06:35:37,399 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, hdfs); groups with view permissions: Set(); users  with modify permissions: Set(root, hdfs); groups with modify permissions: Set()\r\n2020-03-20 06:35:37,419 INFO yarn.Client: Submitting application application_1582250849858_0772 to ResourceManager\r\n2020-03-20 06:35:37,653 INFO impl.YarnClientImpl: Submitted application application_1582250849858_0772\r\n2020-03-20 06:35:37,655 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1582250849858_0772 and attemptId None\r\n2020-03-20 06:35:38,660 INFO yarn.Client: Application report for application_1582250849858_0772 (state: ACCEPTED)\r\n2020-03-20 06:35:38,663 INFO yarn.Client: \r\n         client token: N/A\r\n         diagnostics: AM container is launched, waiting for AM container to Register with RM\r\n         ApplicationMaster host: N/A\r\n         ApplicationMaster RPC port: -1\r\n         queue: default\r\n         start time: 1584686137432\r\n         final status: UNDEFINED\r\n         tracking URL: http://hdfs.master1:8088/proxy/application_1582250849858_0772/\r\n         user: hdfs\r\n2020-03-20 06:35:39,664 INFO yarn.Client: Application report for application_1582250849858_0772 (state: ACCEPTED)\r\n2020-03-20 06:35:40,400 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hdfs.master1, PROXY_URI_BASES -> http://hdfs.master1:8088/proxy/application_1582250849858_0772), /proxy/application_1582250849858_0772\r\n2020-03-20 06:35:40,402 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\r\n2020-03-20 06:35:40,666 INFO yarn.Client: Application report for application_1582250849858_0772 (state: ACCEPTED)\r\n2020-03-20 06:35:40,975 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\r\n2020-03-20 06:35:41,667 INFO yarn.Client: Application report for application_1582250849858_0772 (state: RUNNING)\r\n2020-03-20 06:35:41,667 INFO yarn.Client: \r\n         client token: N/A\r\n         diagnostics: N/A\r\n         ApplicationMaster host: 172.16.53.49\r\n         ApplicationMaster RPC port: 0\r\n         queue: default\r\n         start time: 1584686137432\r\n         final status: UNDEFINED\r\n         tracking URL: http://hdfs.master1:8088/proxy/application_1582250849858_0772/\r\n         user: hdfs\r\n2020-03-20 06:35:41,668 INFO cluster.YarnClientSchedulerBackend: Application application_1582250849858_0772 has started running.\r\n2020-03-20 06:35:41,676 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40002. Attempting port 40003.\r\n2020-03-20 06:35:41,678 WARN util.Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40003. Attempting port 40004.\r\n2020-03-20 06:35:41,679 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40004.\r\n2020-03-20 06:35:41,680 INFO netty.NettyBlockTransferService: Server created on 172.16.53.47:40004\r\n2020-03-20 06:35:41,681 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n2020-03-20 06:35:41,703 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.53.47, 40004, None)\r\n2020-03-20 06:35:41,705 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.16.53.47:40004 with 366.3 MB RAM, BlockManagerId(driver, 172.16.53.47, 40004, None)\r\n2020-03-20 06:35:41,709 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.53.47, 40004, None)\r\n2020-03-20 06:35:41,709 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.53.47, 40004, None)\r\n2020-03-20 06:35:41,864 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\r\n2020-03-20 06:35:41,866 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57cf327c{/metrics/json,null,AVAILABLE,@Spark}\r\n2020-03-20 06:35:43,701 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.53.50:38346) with ID 2\r\n2020-03-20 06:35:43,771 INFO storage.BlockManagerMasterEndpoint: Registering block manager hdfs.slave1:40002 with 366.3 MB RAM, BlockManagerId(2, hdfs.slave1, 40002, None)\r\n2020-03-20 06:35:44,002 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.53.51:54442) with ID 1\r\n2020-03-20 06:35:44,076 INFO storage.BlockManagerMasterEndpoint: Registering block manager hdfs.slave2:40002 with 366.3 MB RAM, BlockManagerId(1, hdfs.slave2, 40002, None)\r\n2020-03-20 06:35:44,081 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\r\nargs: Namespace(batch_size=64, buffer_size=10000, cluster_size=2, data_format='tfos', epochs=3, export_dir='mnist_export', images_labels='hdfs://172.16.53.47:8020/user/hdfs/mnist/tfr/train', model_dir='mnist_model', tensorboard=False)\r\n2020-03-20 14:35:44,119 INFO (MainThread-16549) Reserving TFSparkNodes \r\n2020-03-20 14:35:44,119 INFO (MainThread-16549) cluster_template: {'chief': [0], 'worker': [1]}\r\n2020-03-20 14:35:44,122 INFO (MainThread-16549) Reservation server binding to port 40006\r\n2020-03-20 14:35:44,122 INFO (MainThread-16549) listening for reservations at ('172.16.53.47', 40006)\r\n2020-03-20 14:35:44,122 INFO (MainThread-16549) Starting TensorFlow on executors\r\n2020-03-20 14:35:44,409 INFO (MainThread-16549) Waiting for TFSparkNodes to start\r\n2020-03-20 14:35:44,409 INFO (MainThread-16549) waiting for 2 reservations\r\n2020-03-20 06:35:44,499 INFO spark.SparkContext: Starting job: foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:325\r\n2020-03-20 06:35:44,512 INFO scheduler.DAGScheduler: Got job 0 (foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:325) with 2 output partitions\r\n2020-03-20 06:35:44,513 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:325)\r\n2020-03-20 06:35:44,513 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n2020-03-20 06:35:44,514 INFO scheduler.DAGScheduler: Missing parents: List()\r\n2020-03-20 06:35:44,522 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:325), which has no missing parents\r\n2020-03-20 06:35:44,611 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 47.7 KB, free 366.3 MB)\r\n2020-03-20 06:35:44,639 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 366.2 MB)\r\n2020-03-20 06:35:44,641 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.53.47:40004 (size: 20.4 KB, free: 366.3 MB)\r\n2020-03-20 06:35:44,643 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1039\r\n2020-03-20 06:35:44,653 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:325) (first 15 tasks are for partitions Vector(0, 1))\r\n2020-03-20 06:35:44,653 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\r\n2020-03-20 06:35:44,681 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, hdfs.slave2, executor 1, partition 0, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:44,683 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, hdfs.slave1, executor 2, partition 1, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:44,895 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hdfs.slave1:40002 (size: 20.4 KB, free: 366.3 MB)\r\n2020-03-20 06:35:44,928 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hdfs.slave2:40002 (size: 20.4 KB, free: 366.3 MB)\r\n2020-03-20 14:35:45,410 INFO (MainThread-16549) waiting for 2 reservations\r\n2020-03-20 14:35:46,411 INFO (MainThread-16549) waiting for 2 reservations\r\n2020-03-20 14:35:47,412 INFO (MainThread-16549) all reservations completed\r\n2020-03-20 14:35:47,412 INFO (MainThread-16549) All TFSparkNodes started\r\n2020-03-20 14:35:47,412 INFO (MainThread-16549) {'executor_id': 1, 'host': '172.16.53.50', 'job_name': 'worker', 'task_index': 0, 'port': 40365, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-eds7_4xw/listener-p3hy97d6', 'authkey': b\"Cf\\xbc\\xf5jFI\\t\\x885\\xd2'\\x91J}\\\\\"}\r\n2020-03-20 14:35:47,412 INFO (MainThread-16549) {'executor_id': 0, 'host': '172.16.53.51', 'job_name': 'chief', 'task_index': 0, 'port': 42043, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-2d64a1fu/listener-kzzh7mfe', 'authkey': b'\\xeb\\xc5\\x1c~#\\xd5D\\xf4\\x92\\xdbF\\x99\\xf5\\xfb/\\xf0'}\r\n2020-03-20 14:35:47,412 INFO (MainThread-16549) Waiting for TensorFlow nodes to complete...\r\n2020-03-20 06:35:48,526 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, hdfs.slave2, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n\r\n2020-03-20 06:35:48,528 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 0.0 (TID 2, hdfs.slave2, executor 1, partition 0, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:49,255 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, hdfs.slave1, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000003/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000003/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n\r\n2020-03-20 06:35:49,256 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 0.0 (TID 3, hdfs.slave1, executor 2, partition 1, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:50,336 INFO scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 2) on hdfs.slave2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n) [duplicate 1]\r\n2020-03-20 06:35:50,338 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 0.0 (TID 4, hdfs.slave2, executor 1, partition 0, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:50,943 INFO scheduler.TaskSetManager: Lost task 1.1 in stage 0.0 (TID 3) on hdfs.slave1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000003/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000003/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n) [duplicate 1]\r\n2020-03-20 06:35:50,944 INFO scheduler.TaskSetManager: Starting task 1.2 in stage 0.0 (TID 5, hdfs.slave1, executor 2, partition 1, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:52,275 INFO scheduler.TaskSetManager: Lost task 0.2 in stage 0.0 (TID 4) on hdfs.slave2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n) [duplicate 2]\r\n2020-03-20 06:35:52,277 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 0.0 (TID 6, hdfs.slave2, executor 1, partition 0, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:52,657 INFO scheduler.TaskSetManager: Lost task 1.2 in stage 0.0 (TID 5) on hdfs.slave1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000003/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000003/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n) [duplicate 2]\r\n2020-03-20 06:35:52,659 INFO scheduler.TaskSetManager: Starting task 1.3 in stage 0.0 (TID 7, hdfs.slave1, executor 2, partition 1, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:54,100 INFO scheduler.TaskSetManager: Lost task 0.3 in stage 0.0 (TID 6) on hdfs.slave2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n) [duplicate 3]\r\n2020-03-20 06:35:54,101 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\r\n2020-03-20 06:35:54,105 INFO cluster.YarnScheduler: Cancelling stage 0\r\n2020-03-20 06:35:54,108 INFO cluster.YarnScheduler: Stage 0 was cancelled\r\n2020-03-20 06:35:54,109 INFO scheduler.DAGScheduler: ResultStage 0 (foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:325) failed in 9.574 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, hdfs.slave2, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/hdfs/appcache/application_1582250849858_0772/container_e27_1582250849858_0772_01_000002/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 442, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 401, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n\r\nDriver stacktrace:\r\n2020-03-20 06:35:54,112 INFO scheduler.DAGScheduler: Job 0 failed: foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:325, took 9.612931 s\r\n2020-03-20 14:35:54,114 ERROR (Thread-3-16549) Exception in TF background thread\r\n2020-03-20 06:35:54,353 WARN scheduler.TaskSetManager: Lost task 1.3 in stage 0.0 (TID 7, hdfs.slave1, executor 2): TaskKilled (Stage cancelled)\r\n2020-03-20 06:35:54,355 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \r\n2020-03-20 06:35:57,458 INFO spark.SparkContext: Starting job: foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:176\r\n2020-03-20 06:35:57,459 INFO scheduler.DAGScheduler: Got job 1 (foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:176) with 2 output partitions\r\n2020-03-20 06:35:57,459 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:176)\r\n2020-03-20 06:35:57,459 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n2020-03-20 06:35:57,459 INFO scheduler.DAGScheduler: Missing parents: List()\r\n2020-03-20 06:35:57,460 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:176), which has no missing parents\r\n2020-03-20 06:35:57,463 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 35.5 KB, free 366.2 MB)\r\n2020-03-20 06:35:57,465 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.7 KB, free 366.2 MB)\r\n2020-03-20 06:35:57,465 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.53.47:40004 (size: 11.7 KB, free: 366.3 MB)\r\n2020-03-20 06:35:57,466 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039\r\n2020-03-20 06:35:57,467 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[3] at foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:176) (first 15 tasks are for partitions Vector(0, 1))\r\n2020-03-20 06:35:57,467 INFO cluster.YarnScheduler: Adding task set 1.0 with 2 tasks\r\n2020-03-20 06:35:57,468 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, hdfs.slave2, executor 1, partition 0, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:57,468 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, hdfs.slave1, executor 2, partition 1, PROCESS_LOCAL, 7850 bytes)\r\n2020-03-20 06:35:57,482 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hdfs.slave1:40002 (size: 11.7 KB, free: 366.3 MB)\r\n2020-03-20 06:35:57,484 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hdfs.slave2:40002 (size: 11.7 KB, free: 366.3 MB)\r\n2020-03-20 06:35:59,108 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 9) in 1639 ms on hdfs.slave1 (executor 2) (1/2)\r\n2020-03-20 06:35:59,111 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 35427\r\n2020-03-20 06:35:59,220 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 1752 ms on hdfs.slave2 (executor 1) (2/2)\r\n2020-03-20 06:35:59,220 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \r\n2020-03-20 06:35:59,221 INFO scheduler.DAGScheduler: ResultStage 1 (foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:176) finished in 1.760 s\r\n2020-03-20 06:35:59,222 INFO scheduler.DAGScheduler: Job 1 finished: foreachPartition at /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py:176, took 1.764002 s\r\n2020-03-20 14:35:59,230 ERROR (MainThread-16549) Exiting Spark application with error status.\r\n2020-03-20 06:35:59,235 INFO server.AbstractConnector: Stopped Spark@749791ba{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n2020-03-20 06:35:59,237 INFO ui.SparkUI: Stopped Spark web UI at http://172.16.53.47:4040\r\n2020-03-20 06:35:59,241 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\r\n2020-03-20 06:35:59,257 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\r\n2020-03-20 06:35:59,257 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\r\n2020-03-20 06:35:59,261 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\r\n(serviceOption=None,\r\n services=List(),\r\n started=false)\r\n2020-03-20 06:35:59,262 INFO cluster.YarnClientSchedulerBackend: Stopped\r\n2020-03-20 06:35:59,265 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\r\n2020-03-20 06:35:59,273 INFO memory.MemoryStore: MemoryStore cleared\r\n2020-03-20 06:35:59,274 INFO storage.BlockManager: BlockManager stopped\r\n2020-03-20 06:35:59,274 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\r\n2020-03-20 06:35:59,276 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\r\n2020-03-20 06:35:59,279 INFO spark.SparkContext: Successfully stopped SparkContext\r\n2020-03-20 06:36:00,503 INFO util.ShutdownHookManager: Shutdown hook called\r\n2020-03-20 06:36:00,503 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-530c8478-dae1-471b-b754-242aa14d669b\r\n2020-03-20 06:36:00,504 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-53ff808b-8019-420f-a266-466f5736ee01\r\n2020-03-20 06:36:00,504 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-53ff808b-8019-420f-a266-466f5736ee01/pyspark-0c84e393-86f4-4c11-986e-ba4eeb6d822b\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--deploy-mode client \\\r\n--master yarn \\\r\n--num-executors 2 \\\r\n--executor-memory 1G \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\n--conf spark.executorEnv.CLASSPATH=$(hadoop classpath --glob) \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n/root/test_csy/TensorFlowOnSpark-2.2.1/examples/mnist/keras/mnist_tf_ds.py \\\r\n--images_labels hdfs://172.16.53.47:8020/user/hdfs/mnist/tfr/train \\\r\n--model mnist_model\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/513", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/513/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/513/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/513/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/513", "id": 583721648, "node_id": "MDU6SXNzdWU1ODM3MjE2NDg=", "number": 513, "title": "Running on AWS EMR gets failed", "user": {"login": "sydsim", "id": 9265107, "node_id": "MDQ6VXNlcjkyNjUxMDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/9265107?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sydsim", "html_url": "https://github.com/sydsim", "followers_url": "https://api.github.com/users/sydsim/followers", "following_url": "https://api.github.com/users/sydsim/following{/other_user}", "gists_url": "https://api.github.com/users/sydsim/gists{/gist_id}", "starred_url": "https://api.github.com/users/sydsim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sydsim/subscriptions", "organizations_url": "https://api.github.com/users/sydsim/orgs", "repos_url": "https://api.github.com/users/sydsim/repos", "events_url": "https://api.github.com/users/sydsim/events{/privacy}", "received_events_url": "https://api.github.com/users/sydsim/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-03-18T13:18:12Z", "updated_at": "2020-05-11T15:18:09Z", "closed_at": "2020-05-11T15:18:09Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environments**\r\n - Python version : 3.6\r\n - Spark version : 2.4.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark 1.4.4\r\n\r\n**Hardware Spec**\r\n- MASTER Instance Group | 1 Instance\r\n4 vCore, 30.5 GiB memory, EBS only storage\r\nEBS Storage:64 GiB\r\n- Core Instance Group | 40 Instances\r\n4 vCore, 30.5 GiB memory, EBS only storage\r\nEBS Storage:400 GiB\r\n\r\nI'm trying to implement distributed inferencing keras model with spark on AWS EMR, but keep getting errors.\r\n\r\nI tried with this configuration\r\n\r\n> spark | maximizeResourceAllocation | true \r\n> spark-defaults | spark.executor.cores | 4 \r\n> spark-defaults | spark.executor.instances | 40\r\n> spark-defaults | spark.dynamicAllocation.enabled | false\r\n> spark-defaults | spark.cores.max | 160\r\n> spark-defaults | spark.executor.memoryOverhead | 512 \r\n> spark-defaults | spark.driver.memoryOverhead | 512 \r\n> spark-defaults | spark.task.cpus | 4\r\n\r\nand got \r\n```\r\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1584431955469_0001/container_1584431955469_0001_01_000027/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1584431955469_0001/container_1584431955469_0001_01_000027/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 465, in _inference\r\n    queue_in.put(item, block=True)\r\nAttributeError: 'AutoProxy[get_queue]' object has no attribute 'put'\r\n```\r\n\r\n I looked up closed issues and saw that setting spark.executor.cores to 1 might be help.\r\nSo I changed spark configuration like this.\r\n\r\n> spark | maximizeResourceAllocation | true\r\n> spark-defaults | spark.executor.cores | 1\r\n> spark-defaults | spark.cores.max | 30\r\n> spark-defaults | spark.dynamicAllocation.enabled | false\r\n> spark-defaults | spark.executor.memoryOverhead | 512\r\n> spark-defaults | spark.driver.memoryOverhead | 512\r\n> spark-defaults | spark.task.cpus | 1\r\n\r\nbut I got \r\n```\r\n  File \"/mnt3/yarn/usercache/hadoop/appcache/application_1584420951260_0001/container_1584420951260_0001_01_000011/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/mnt3/yarn/usercache/hadoop/appcache/application_1584420951260_0001/container_1584420951260_0001_01_000011/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 516, in _shutdown\r\n    executor_id = util.read_executor_id()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/util.py\", line 74, in read_executor_id\r\n    with open(\"executor_id\", \"r\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'executor_id'\r\n```\r\n\r\nI looked up all container/step logs but there's nothing specific. (I can send you If you want to see)\r\nCan you give me any advice for this problem? Thanks.\r\n\r\nPS: Google User group(https://groups.google.com/forum/#!forum/TensorFlowOnSpark-users) on README page seems doesn't work for me. It keeps saying I have no privilege to access this contents. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/510", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/510/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/510/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/510/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/510", "id": 576043589, "node_id": "MDU6SXNzdWU1NzYwNDM1ODk=", "number": 510, "title": "Not able to run multiple tensorflowonspark tasks at the same time when TFOS_SERVER_PORT is configured", "user": {"login": "qzhong711", "id": 5584171, "node_id": "MDQ6VXNlcjU1ODQxNzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5584171?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qzhong711", "html_url": "https://github.com/qzhong711", "followers_url": "https://api.github.com/users/qzhong711/followers", "following_url": "https://api.github.com/users/qzhong711/following{/other_user}", "gists_url": "https://api.github.com/users/qzhong711/gists{/gist_id}", "starred_url": "https://api.github.com/users/qzhong711/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qzhong711/subscriptions", "organizations_url": "https://api.github.com/users/qzhong711/orgs", "repos_url": "https://api.github.com/users/qzhong711/repos", "events_url": "https://api.github.com/users/qzhong711/events{/privacy}", "received_events_url": "https://api.github.com/users/qzhong711/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2020-03-05T07:20:53Z", "updated_at": "2020-03-16T19:03:24Z", "closed_at": "2020-03-16T19:03:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.3.2\r\n - TensorFlow version 1.14.4\r\n - TensorFlowOnSpark version  1.4.4 \r\n - Cluster version Hadoop3.1.0\r\n\r\n**Describe the bug:**\r\nWe have an application installed in docker container, so we basically do spark-submit from container to the cluster, and a number of ports are mapping between the host and the docker container. In order to connect to the cluster, we had to set TFOS_SERVER_HOST = 'x.x.x.x' (IP address of host) and TFOS_SERVER_PORT = '40008' (a mapped port). But this does not work when multiple tfos tasks are running at the same time because the first task reserves the port and later tasks all failed with 40008 not available.\r\n\r\nWe located the code in reservation.py:\r\n  def start_listening_socket(self):\r\n    port_number = int(os.getenv(TFOS_SERVER_PORT)) if os.getenv(TFOS_SERVER_PORT) else 0\r\n    server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\n    server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\r\n    server_sock.bind(('', port_number))\r\n    server_sock.listen(10)\r\n    return server_sock\r\n\r\nSo it directly uses the configured TFOS_SERVER_PORT, would it possible to improve the code so that it accepts a list of ports or ports range e.g. 40001-40010? \r\n\r\n\r\n**Logs:**\r\n{\"name\":\"cloudcore-debug\",\"hostname\":\"a600513e1913\",\"pid\":25763,\"level\":20,\"msg\":\"[ExecuteTool] [STDOUT DATA]:     cluster = TFCluster.run( sc, main_fun, args, args.cluster_size, num_ps, args.tensorboard, TFCluster.InputMode.SPARK )\\n  File \\\"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFCluster.py\\\", line 279, in run\\n    server_addr = server.start()\\n  File \\\"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/reservation.py\\\", line 152, in start\\n    server_sock = self.start_listening_socket()\\n\",\"time\":\"2020-03-05T06:40:59.860Z\",\"v\":0}\r\n{\"name\":\"cloudcore-debug\",\"hostname\":\"a600513e1913\",\"pid\":25763,\"level\":20,\"msg\":\"[ExecuteTool] [STDOUT DATA]:   File \\\"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/reservation.py\\\", line 196, in start_listening_socket\\n    server_sock.bind(('', port_number))\\n\",\"time\":\"2020-03-05T06:40:59.860Z\",\"v\":0}\r\n{\"name\":\"cloudcore-debug\",\"hostname\":\"a600513e1913\",\"pid\":25763,\"level\":20,\"msg\":\"[ExecuteTool] [STDOUT DATA]: OSError: [Errno 98] Address already in use\\n\",\"time\":\"2020-03-05T06:40:59.860Z\",\"v\":0}\r\n**Spark Submit Command Line:**\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/508", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/508/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/508/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/508/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/508", "id": 573515110, "node_id": "MDU6SXNzdWU1NzM1MTUxMTA=", "number": 508, "title": "A problem while invoking cluster.inference(dataRDD) (the process is hanging up and cannot end)", "user": {"login": "guoyuhaoaaa", "id": 10019527, "node_id": "MDQ6VXNlcjEwMDE5NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/10019527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guoyuhaoaaa", "html_url": "https://github.com/guoyuhaoaaa", "followers_url": "https://api.github.com/users/guoyuhaoaaa/followers", "following_url": "https://api.github.com/users/guoyuhaoaaa/following{/other_user}", "gists_url": "https://api.github.com/users/guoyuhaoaaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/guoyuhaoaaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guoyuhaoaaa/subscriptions", "organizations_url": "https://api.github.com/users/guoyuhaoaaa/orgs", "repos_url": "https://api.github.com/users/guoyuhaoaaa/repos", "events_url": "https://api.github.com/users/guoyuhaoaaa/events{/privacy}", "received_events_url": "https://api.github.com/users/guoyuhaoaaa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2020-03-01T09:35:54Z", "updated_at": "2020-07-10T22:04:03Z", "closed_at": "2020-03-07T08:14:49Z", "author_association": "NONE", "active_lock_reason": null, "body": "TensorflowOnSpark version:1.4.4     Tensorflow version : 1.11.0    Spark version:2.3.1\r\ndataRDD is the RDD that I read from HDFS by spark.read.csv() API. After I get the data, I call spark repartition(5) API to partition the data into 5 parts , and write the result RDD to HDFS like:\r\n  labelRDD = cluster.inference(dataRDD.repartition(5))\r\n  labelRDD.saveAsTextFile(args.output)\r\n\r\nthen I submit the job through spark-submit command  applying for 5 exectuors resources as \r\n\r\nspark-submit --py-files piecewise_lr_dist.py --driver-cores 2 --driver-memory 4g --executor-cores 1 --executor-memory 3g --total-executor-cores 6 piecewise_lr_spark.py --cluster_size 6 --epochs 1 --model hdfs://hadoop-master:9000/data/gogo --steps 1000 --appName piecewise_app_infer  --partition_num 5 --output /data/piece_num_ans.csv --mode inference --time_out=600 \r\n\r\n But the job cannot end, when I see the spark worker logs , I find that \r\n![image](https://user-images.githubusercontent.com/10019527/75623235-fa66d080-5be2-11ea-9c02-38ace7d03fcf.png)\r\nThat is to say the input queue of data on that worker has been consumed. When I see the source TFSparknode.py , I find the place where the code is blocked:\r\n\r\n   logging.info(\"Processed {0} items in partition\".format(count)) **# This information has been printed in the spark worker logs**\r\n\r\n    **# This is where the code is blocked**\r\n    # read result queue\r\n    results = []\r\n    queue_out = mgr.get_queue('output')\r\n    while count > 0:\r\n      result = queue_out.get(block=True)\r\n      results.append(result)\r\n      count -= 1\r\n      queue_out.task_done()\r\n\r\n    logging.info(\"Finished processing partition\")\r\n    return results\r\n\r\nI can not understand why this happen. And not all workers are blocked in this place, some workers behavior normally, because in the last line, it prints logs (\"Finished processing partition\") demonstrating that this worker is not blocked in above place.The logs are as below:\r\n![image](https://user-images.githubusercontent.com/10019527/75623386-76154d00-5be4-11ea-81f1-11f3281443d3.png)\r\n\r\n\r\n\r\nHowever, when I repartition the dataRDD into 1 partition,   labelRDD = cluster.inference(dataRDD.repartition(1)) the program will end up normally.\r\n\r\nFollowing is spark drivers running logs when I repartition the dataRDD into 5 partitions:\r\n\r\n2020-03-01 16:48:24 INFO  SparkContext:54 - Running Spark version 2.3.1\r\n2020-03-01 16:48:24 INFO  SparkContext:54 - Submitted application: piecewise_app_infer\r\n2020-03-01 16:48:24 INFO  SecurityManager:54 - Changing view acls to: guoyuhao\r\n2020-03-01 16:48:24 INFO  SecurityManager:54 - Changing modify acls to: guoyuhao\r\n2020-03-01 16:48:24 INFO  SecurityManager:54 - Changing view acls groups to: \r\n2020-03-01 16:48:24 INFO  SecurityManager:54 - Changing modify acls groups to: \r\n2020-03-01 16:48:24 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(guoyuhao); groups with view permissions: Set(); users  with modify permissions: Set(guoyuhao); groups with modify permissions: Set()\r\n2020-03-01 16:48:25 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 36998.\r\n2020-03-01 16:48:25 INFO  SparkEnv:54 - Registering MapOutputTracker\r\n2020-03-01 16:48:25 INFO  SparkEnv:54 - Registering BlockManagerMaster\r\n2020-03-01 16:48:25 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n2020-03-01 16:48:25 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\r\n2020-03-01 16:48:25 INFO  DiskBlockManager:54 - Created local directory at /usr/local/spark/blockmgr-89e8187a-c2bc-4f54-aab5-07031223e014\r\n2020-03-01 16:48:25 INFO  MemoryStore:54 - MemoryStore started with capacity 2004.6 MB\r\n2020-03-01 16:48:25 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\r\n2020-03-01 16:48:26 INFO  log:192 - Logging initialized @12232ms\r\n2020-03-01 16:48:26 INFO  Server:346 - jetty-9.3.z-SNAPSHOT\r\n2020-03-01 16:48:26 INFO  Server:414 - Started @12568ms\r\n2020-03-01 16:48:26 INFO  AbstractConnector:278 - Started ServerConnector@387443f5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n2020-03-01 16:48:26 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@591cd160{/jobs,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@541efc9c{/jobs/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@16dcc77a{/jobs/job,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@220c48da{/jobs/job/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@155ba1b{/stages,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71424a5c{/stages/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@70d354e9{/stages/stage,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7f83c0bb{/stages/stage/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@151db7be{/stages/pool,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@697da9ac{/stages/pool/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@765c7b21{/storage,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@173bfb49{/storage/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f631e57{/storage/rdd,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6bd7fade{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e4701f6{/environment,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@366e5cc4{/environment/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41087df8{/executors,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@15dd7e0d{/executors/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@53d395f1{/executors/threadDump,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@179f7b33{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@157a2385{/static,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4255341e{/,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26c19a71{/api,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1e83f04f{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@36154a57{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:26 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://hadoop-master:4040\r\n2020-03-01 16:48:26 INFO  SparkContext:54 - Added file file:/home/guoyuhao/A_card/piecewise_lr_spark.py at spark://hadoop-master:36998/files/piecewise_lr_spark.py with timestamp 1583052506996\r\n2020-03-01 16:48:27 INFO  Utils:54 - Copying /home/guoyuhao/A_card/piecewise_lr_spark.py to /usr/local/spark/spark-454288f2-8cd3-44d0-9d25-17bb77bf1c79/userFiles-f39c2800-b908-4e25-92f5-7b99474c5296/piecewise_lr_spark.py\r\n2020-03-01 16:48:27 INFO  SparkContext:54 - Added file file:///home/guoyuhao/A_card/piecewise_lr_dist.py at spark://hadoop-master:36998/files/piecewise_lr_dist.py with timestamp 1583052507061\r\n2020-03-01 16:48:27 INFO  Utils:54 - Copying /home/guoyuhao/A_card/piecewise_lr_dist.py to /usr/local/spark/spark-454288f2-8cd3-44d0-9d25-17bb77bf1c79/userFiles-f39c2800-b908-4e25-92f5-7b99474c5296/piecewise_lr_dist.py\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Connecting to master spark://hadoop-master:7077...\r\n2020-03-01 16:48:27 INFO  TransportClientFactory:267 - Successfully created connection to hadoop-master/192.168.1.179:7077 after 111 ms (0 ms spent in bootstraps)\r\n2020-03-01 16:48:27 INFO  StandaloneSchedulerBackend:54 - Connected to Spark cluster with app ID app-20200301164827-0040\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20200301164827-0040/0 on worker-20200229113429-192.168.1.179-35579 (192.168.1.179:35579) with 1 core(s)\r\n2020-03-01 16:48:27 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20200301164827-0040/0 on hostPort 192.168.1.179:35579 with 1 core(s), 3.0 GB RAM\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20200301164827-0040/1 on worker-20200229113429-192.168.1.179-35579 (192.168.1.179:35579) with 1 core(s)\r\n2020-03-01 16:48:27 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20200301164827-0040/1 on hostPort 192.168.1.179:35579 with 1 core(s), 3.0 GB RAM\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20200301164827-0040/2 on worker-20200229113429-192.168.1.179-35579 (192.168.1.179:35579) with 1 core(s)\r\n2020-03-01 16:48:27 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20200301164827-0040/2 on hostPort 192.168.1.179:35579 with 1 core(s), 3.0 GB RAM\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20200301164827-0040/3 on worker-20200229113429-192.168.1.179-35579 (192.168.1.179:35579) with 1 core(s)\r\n2020-03-01 16:48:27 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20200301164827-0040/3 on hostPort 192.168.1.179:35579 with 1 core(s), 3.0 GB RAM\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20200301164827-0040/4 on worker-20200229113429-192.168.1.179-35579 (192.168.1.179:35579) with 1 core(s)\r\n2020-03-01 16:48:27 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20200301164827-0040/4 on hostPort 192.168.1.179:35579 with 1 core(s), 3.0 GB RAM\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20200301164827-0040/5 on worker-20200229113429-192.168.1.179-35579 (192.168.1.179:35579) with 1 core(s)\r\n2020-03-01 16:48:27 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20200301164827-0040/5 on hostPort 192.168.1.179:35579 with 1 core(s), 3.0 GB RAM\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20200301164827-0040/0 is now RUNNING\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20200301164827-0040/1 is now RUNNING\r\n2020-03-01 16:48:27 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33487.\r\n2020-03-01 16:48:27 INFO  NettyBlockTransferService:54 - Server created on hadoop-master:33487\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20200301164827-0040/2 is now RUNNING\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20200301164827-0040/3 is now RUNNING\r\n2020-03-01 16:48:27 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20200301164827-0040/4 is now RUNNING\r\n2020-03-01 16:48:27 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20200301164827-0040/5 is now RUNNING\r\n2020-03-01 16:48:27 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, hadoop-master, 33487, None)\r\n2020-03-01 16:48:27 INFO  BlockManagerMasterEndpoint:54 - Registering block manager hadoop-master:33487 with 2004.6 MB RAM, BlockManagerId(driver, hadoop-master, 33487, None)\r\n2020-03-01 16:48:27 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, hadoop-master, 33487, None)\r\n2020-03-01 16:48:27 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, hadoop-master, 33487, None)\r\n2020-03-01 16:48:28 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@47de24b0{/metrics/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:30 INFO  EventLoggingListener:54 - Logging events to hdfs://hadoop-master:9000/user/directory/app-20200301164827-0040\r\n2020-03-01 16:48:30 INFO  StandaloneSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\r\n2020-03-01 16:48:31 INFO  SharedState:54 - loading hive config file: file:/usr/local/spark/conf/hive-site.xml\r\n2020-03-01 16:48:31 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('/hive/warehouse') to the value of spark.sql.warehouse.dir ('hdfs:///hive/warehouse').\r\n2020-03-01 16:48:31 INFO  SharedState:54 - Warehouse path is 'hdfs:///hive/warehouse'.\r\n2020-03-01 16:48:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@41ca8ef7{/SQL,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@23f3da79{/SQL/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@66e1ef7f{/SQL/execution,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4c6c5dab{/SQL/execution/json,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:31 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@450d01af{/static/sql,null,AVAILABLE,@Spark}\r\n2020-03-01 16:48:32 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\r\n2020-03-01 16:48:33 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.179:42900) with ID 0\r\n2020-03-01 16:48:34 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.1.179:44755 with 1458.6 MB RAM, BlockManagerId(0, 192.168.1.179, 44755, None)\r\n2020-03-01 16:48:34 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.179:42906) with ID 1\r\n2020-03-01 16:48:34 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.179:42904) with ID 2\r\n2020-03-01 16:48:34 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.179:42908) with ID 5\r\n2020-03-01 16:48:34 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.1.179:45305 with 1458.6 MB RAM, BlockManagerId(2, 192.168.1.179, 45305, None)\r\n2020-03-01 16:48:34 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.1.179:33754 with 1458.6 MB RAM, BlockManagerId(1, 192.168.1.179, 33754, None)\r\n2020-03-01 16:48:34 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.179:42912) with ID 4\r\n2020-03-01 16:48:34 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.179:42910) with ID 3\r\n2020-03-01 16:48:34 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.1.179:45554 with 1458.6 MB RAM, BlockManagerId(5, 192.168.1.179, 45554, None)\r\n2020-03-01 16:48:34 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.1.179:33084 with 1458.6 MB RAM, BlockManagerId(4, 192.168.1.179, 33084, None)\r\n2020-03-01 16:48:34 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.1.179:37863 with 1458.6 MB RAM, BlockManagerId(3, 192.168.1.179, 37863, None)\r\n2020-03-01 16:48:39 INFO  FileSourceStrategy:54 - Pruning directories with: \r\n2020-03-01 16:48:39 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#0, None)) > 0)\r\n2020-03-01 16:48:39 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\r\n2020-03-01 16:48:39 INFO  FileSourceScanExec:54 - Pushed Filters: \r\n2020-03-01 16:48:40 INFO  CodeGenerator:54 - Code generated in 575.118764 ms\r\n2020-03-01 16:48:41 INFO  CodeGenerator:54 - Code generated in 82.194687 ms\r\n2020-03-01 16:48:41 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 293.3 KB, free 2004.3 MB)\r\n2020-03-01 16:48:42 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.3 KB, free 2004.3 MB)\r\n2020-03-01 16:48:42 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on hadoop-master:33487 (size: 25.3 KB, free: 2004.6 MB)\r\n2020-03-01 16:48:42 INFO  SparkContext:54 - Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\r\n2020-03-01 16:48:42 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\r\n2020-03-01 16:48:42 INFO  SparkContext:54 - Starting job: csv at NativeMethodAccessorImpl.java:0\r\n2020-03-01 16:48:42 INFO  DAGScheduler:54 - Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\r\n2020-03-01 16:48:42 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\r\n2020-03-01 16:48:42 INFO  DAGScheduler:54 - Parents of final stage: List()\r\n2020-03-01 16:48:42 INFO  DAGScheduler:54 - Missing parents: List()\r\n2020-03-01 16:48:42 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n2020-03-01 16:48:42 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 2004.3 MB)\r\n2020-03-01 16:48:42 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 2004.3 MB)\r\n2020-03-01 16:48:42 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on hadoop-master:33487 (size: 4.5 KB, free: 2004.6 MB)\r\n2020-03-01 16:48:42 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\r\n2020-03-01 16:48:42 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\r\n2020-03-01 16:48:42 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 1 tasks\r\n2020-03-01 16:48:42 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.179, executor 4, partition 0, ANY, 8311 bytes)\r\n2020-03-01 16:48:43 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 192.168.1.179:33084 (size: 4.5 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:46 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 192.168.1.179:33084 (size: 25.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:48 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 6090 ms on 192.168.1.179 (executor 4) (1/1)\r\n2020-03-01 16:48:48 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \r\n2020-03-01 16:48:48 INFO  DAGScheduler:54 - ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 6.333 s\r\n2020-03-01 16:48:48 INFO  DAGScheduler:54 - Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 6.484381 s\r\n2020-03-01 16:48:49 INFO  FileSourceStrategy:54 - Pruning directories with: \r\n2020-03-01 16:48:49 INFO  FileSourceStrategy:54 - Post-Scan Filters: \r\n2020-03-01 16:48:49 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\r\n2020-03-01 16:48:49 INFO  FileSourceScanExec:54 - Pushed Filters: \r\n2020-03-01 16:48:49 INFO  CodeGenerator:54 - Code generated in 22.919771 ms\r\n2020-03-01 16:48:49 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 293.3 KB, free 2004.0 MB)\r\n2020-03-01 16:48:49 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.3 KB, free 2004.0 MB)\r\n2020-03-01 16:48:49 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on hadoop-master:33487 (size: 25.3 KB, free: 2004.5 MB)\r\n2020-03-01 16:48:49 INFO  SparkContext:54 - Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\r\n2020-03-01 16:48:49 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\r\n2020-03-01 16:48:49 INFO  FileSourceStrategy:54 - Pruning directories with: \r\n2020-03-01 16:48:49 INFO  FileSourceStrategy:54 - Post-Scan Filters: \r\n2020-03-01 16:48:49 INFO  FileSourceStrategy:54 - Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 22 more fields>\r\n2020-03-01 16:48:49 INFO  FileSourceScanExec:54 - Pushed Filters: \r\n2020-03-01 16:48:49 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 293.3 KB, free 2003.7 MB)\r\n2020-03-01 16:48:49 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 25.3 KB, free 2003.7 MB)\r\n2020-03-01 16:48:49 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on hadoop-master:33487 (size: 25.3 KB, free: 2004.5 MB)\r\n2020-03-01 16:48:49 INFO  SparkContext:54 - Created broadcast 3 from javaToPython at NativeMethodAccessorImpl.java:0\r\n2020-03-01 16:48:49 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\r\n2020-03-01 16:48:49 INFO  SparkContext:54 - Starting job: runJob at PythonRDD.scala:149\r\n2020-03-01 16:48:49 INFO  DAGScheduler:54 - Registering RDD 14 (coalesce at NativeMethodAccessorImpl.java:0)\r\n2020-03-01 16:48:49 INFO  DAGScheduler:54 - Got job 1 (runJob at PythonRDD.scala:149) with 1 output partitions\r\n2020-03-01 16:48:49 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (runJob at PythonRDD.scala:149)\r\n2020-03-01 16:48:49 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 1)\r\n2020-03-01 16:48:49 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 1)\r\n2020-03-01 16:48:49 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 1 (MapPartitionsRDD[14] at coalesce at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n2020-03-01 16:48:49 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 16.0 KB, free 2003.6 MB)\r\n2020-03-01 16:48:49 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.3 KB, free 2003.6 MB)\r\n2020-03-01 16:48:49 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on hadoop-master:33487 (size: 8.3 KB, free: 2004.5 MB)\r\n2020-03-01 16:48:49 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1039\r\n2020-03-01 16:48:49 INFO  DAGScheduler:54 - Submitting 4 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[14] at coalesce at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\r\n2020-03-01 16:48:49 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 4 tasks\r\n2020-03-01 16:48:49 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, 192.168.1.179, executor 5, partition 0, ANY, 8300 bytes)\r\n2020-03-01 16:48:49 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 2, 192.168.1.179, executor 2, partition 1, ANY, 8300 bytes)\r\n2020-03-01 16:48:49 INFO  TaskSetManager:54 - Starting task 2.0 in stage 1.0 (TID 3, 192.168.1.179, executor 3, partition 2, ANY, 8300 bytes)\r\n2020-03-01 16:48:49 INFO  TaskSetManager:54 - Starting task 3.0 in stage 1.0 (TID 4, 192.168.1.179, executor 4, partition 3, ANY, 8300 bytes)\r\n2020-03-01 16:48:50 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 192.168.1.179:33084 (size: 8.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:50 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 192.168.1.179:45554 (size: 8.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:50 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 192.168.1.179:45305 (size: 8.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:50 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 192.168.1.179:37863 (size: 8.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:51 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 192.168.1.179:33084 (size: 25.3 KB, free: 1458.5 MB)\r\n2020-03-01 16:48:53 INFO  TaskSetManager:54 - Finished task 3.0 in stage 1.0 (TID 4) in 3294 ms on 192.168.1.179 (executor 4) (1/4)\r\n2020-03-01 16:48:55 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 192.168.1.179:37863 (size: 25.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:55 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 192.168.1.179:45554 (size: 25.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:55 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 192.168.1.179:45305 (size: 25.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:48:59 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 2) in 9856 ms on 192.168.1.179 (executor 2) (2/4)\r\n2020-03-01 16:48:59 INFO  TaskSetManager:54 - Finished task 2.0 in stage 1.0 (TID 3) in 9911 ms on 192.168.1.179 (executor 3) (3/4)\r\n2020-03-01 16:48:59 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 9922 ms on 192.168.1.179 (executor 5) (4/4)\r\n2020-03-01 16:48:59 INFO  DAGScheduler:54 - ShuffleMapStage 1 (coalesce at NativeMethodAccessorImpl.java:0) finished in 10.013 s\r\n2020-03-01 16:48:59 INFO  DAGScheduler:54 - looking for newly runnable stages\r\n2020-03-01 16:48:59 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \r\n2020-03-01 16:48:59 INFO  DAGScheduler:54 - running: Set()\r\n2020-03-01 16:48:59 INFO  DAGScheduler:54 - waiting: Set(ResultStage 2)\r\n2020-03-01 16:48:59 INFO  DAGScheduler:54 - failed: Set()\r\n2020-03-01 16:48:59 INFO  DAGScheduler:54 - Submitting ResultStage 2 (PythonRDD[18] at RDD at PythonRDD.scala:49), which has no missing parents\r\n2020-03-01 16:48:59 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 6.0 KB, free 2003.6 MB)\r\n2020-03-01 16:48:59 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.7 KB, free 2003.6 MB)\r\n2020-03-01 16:48:59 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on hadoop-master:33487 (size: 3.7 KB, free: 2004.5 MB)\r\n2020-03-01 16:48:59 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1039\r\n2020-03-01 16:48:59 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (PythonRDD[18] at RDD at PythonRDD.scala:49) (first 15 tasks are for partitions Vector(0))\r\n2020-03-01 16:48:59 INFO  ContextCleaner:54 - Cleaned accumulator 18\r\n2020-03-01 16:48:59 INFO  ContextCleaner:54 - Cleaned accumulator 26\r\n2020-03-01 16:48:59 INFO  ContextCleaner:54 - Cleaned accumulator 17\r\n2020-03-01 16:48:59 INFO  ContextCleaner:54 - Cleaned accumulator 22\r\n2020-03-01 16:48:59 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 1 tasks\r\n2020-03-01 16:49:00 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 5, 192.168.1.179, executor 1, partition 0, NODE_LOCAL, 7929 bytes)\r\n2020-03-01 16:49:00 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on hadoop-master:33487 in memory (size: 25.3 KB, free: 2004.5 MB)\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 15\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 25\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 13\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 4\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 30\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 34\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 14\r\n2020-03-01 16:49:00 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on hadoop-master:33487 in memory (size: 25.3 KB, free: 2004.6 MB)\r\n2020-03-01 16:49:00 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on 192.168.1.179:33084 in memory (size: 25.3 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 5\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 7\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 24\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 1\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 33\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 6\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 35\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 29\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 23\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 27\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 19\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 20\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 10\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 21\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 8\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 16\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 2\r\n2020-03-01 16:49:00 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on hadoop-master:33487 in memory (size: 4.5 KB, free: 2004.6 MB)\r\n2020-03-01 16:49:00 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 192.168.1.179:33084 in memory (size: 4.5 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 32\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 9\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 11\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 12\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 28\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 36\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 3\r\n2020-03-01 16:49:00 INFO  ContextCleaner:54 - Cleaned accumulator 31\r\n2020-03-01 16:49:00 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on 192.168.1.179:33754 (size: 3.7 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:03 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 192.168.1.179:42906\r\n2020-03-01 16:49:03 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 5) in 3361 ms on 192.168.1.179 (executor 1) (1/1)\r\n2020-03-01 16:49:03 INFO  TaskSchedulerImpl:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool \r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - ResultStage 2 (runJob at PythonRDD.scala:149) finished in 3.460 s\r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - Job 1 finished: runJob at PythonRDD.scala:149, took 13.516101 s\r\n2020-03-01 16:49:03,382 INFO (MainThread-214083) Reserving TFSparkNodes \r\n2020-03-01 16:49:03,382 INFO (MainThread-214083) cluster_template: {'ps': [0], 'worker': [1, 2, 3, 4, 5]}\r\n2020-03-01 16:49:03,385 INFO (MainThread-214083) listening for reservations at ('192.168.1.179', 45282)\r\n2020-03-01 16:49:03,391 INFO (MainThread-214083) Starting TensorFlow on executors\r\n2020-03-01 16:49:03,422 INFO (MainThread-214083) Waiting for TFSparkNodes to start\r\n2020-03-01 16:49:03,422 INFO (MainThread-214083) waiting for 6 reservations\r\n2020-03-01 16:49:03 INFO  SparkContext:54 - Starting job: foreachPartition at /home/xindun/anaconda3/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:320\r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - Got job 2 (foreachPartition at /home/xindun/anaconda3/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:320) with 6 output partitions\r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (foreachPartition at /home/xindun/anaconda3/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:320)\r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - Parents of final stage: List()\r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - Missing parents: List()\r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - Submitting ResultStage 3 (PythonRDD[20] at foreachPartition at /home/xindun/anaconda3/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:320), which has no missing parents\r\n2020-03-01 16:49:03 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 15.2 KB, free 2004.2 MB)\r\n2020-03-01 16:49:03 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.9 KB, free 2004.2 MB)\r\n2020-03-01 16:49:03 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on hadoop-master:33487 (size: 10.9 KB, free: 2004.6 MB)\r\n2020-03-01 16:49:03 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1039\r\n2020-03-01 16:49:03 INFO  DAGScheduler:54 - Submitting 6 missing tasks from ResultStage 3 (PythonRDD[20] at foreachPartition at /home/xindun/anaconda3/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:320) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\r\n2020-03-01 16:49:03 INFO  TaskSchedulerImpl:54 - Adding task set 3.0 with 6 tasks\r\n2020-03-01 16:49:03 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 6, 192.168.1.179, executor 1, partition 0, PROCESS_LOCAL, 7759 bytes)\r\n2020-03-01 16:49:03 INFO  TaskSetManager:54 - Starting task 1.0 in stage 3.0 (TID 7, 192.168.1.179, executor 4, partition 1, PROCESS_LOCAL, 7759 bytes)\r\n2020-03-01 16:49:03 INFO  TaskSetManager:54 - Starting task 2.0 in stage 3.0 (TID 8, 192.168.1.179, executor 5, partition 2, PROCESS_LOCAL, 7759 bytes)\r\n2020-03-01 16:49:03 INFO  TaskSetManager:54 - Starting task 3.0 in stage 3.0 (TID 9, 192.168.1.179, executor 3, partition 3, PROCESS_LOCAL, 7759 bytes)\r\n2020-03-01 16:49:03 INFO  TaskSetManager:54 - Starting task 4.0 in stage 3.0 (TID 10, 192.168.1.179, executor 0, partition 4, PROCESS_LOCAL, 7759 bytes)\r\n2020-03-01 16:49:03 INFO  TaskSetManager:54 - Starting task 5.0 in stage 3.0 (TID 11, 192.168.1.179, executor 2, partition 5, PROCESS_LOCAL, 7759 bytes)\r\n2020-03-01 16:49:03 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 192.168.1.179:33084 (size: 10.9 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:03 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 192.168.1.179:33754 (size: 10.9 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:03 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 192.168.1.179:45305 (size: 10.9 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:03 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 192.168.1.179:45554 (size: 10.9 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:03 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 192.168.1.179:37863 (size: 10.9 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:04,422 INFO (MainThread-214083) waiting for 6 reservations\r\n2020-03-01 16:49:04 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 192.168.1.179:44755 (size: 10.9 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:05,423 INFO (MainThread-214083) waiting for 6 reservations\r\n2020-03-01 16:49:06,424 INFO (MainThread-214083) waiting for 6 reservations\r\n2020-03-01 16:49:07,425 INFO (MainThread-214083) waiting for 6 reservations\r\n2020-03-01 16:49:08,426 INFO (MainThread-214083) waiting for 3 reservations\r\n2020-03-01 16:49:09,427 INFO (MainThread-214083) waiting for 1 reservations\r\n2020-03-01 16:49:10,428 INFO (MainThread-214083) waiting for 1 reservations\r\n2020-03-01 16:49:11,429 INFO (MainThread-214083) all reservations completed\r\n2020-03-01 16:49:11,429 INFO (MainThread-214083) All TFSparkNodes started\r\n2020-03-01 16:49:11,430 INFO (MainThread-214083) {'executor_id': 2, 'host': '192.168.1.179', 'job_name': 'worker', 'task_index': 1, 'port': 33804, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-yzwrwywi/listener-1b5tqqg6', 'authkey': b'8YbV\\x99iM\\xb9\\xb1Z{\\x19\\xcdo\\xc0;'}\r\n2020-03-01 16:49:11,430 INFO (MainThread-214083) {'executor_id': 1, 'host': '192.168.1.179', 'job_name': 'worker', 'task_index': 0, 'port': 34770, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-_2n6g56f/listener-atu325vy', 'authkey': b'\\xb7u\\xfcf\\xdb\\xbbL\\x91\\xa9\\xe3\\xe7]\\xda\\xf2\\xe8\\x87'}\r\n2020-03-01 16:49:11,430 INFO (MainThread-214083) {'executor_id': 5, 'host': '192.168.1.179', 'job_name': 'worker', 'task_index': 4, 'port': 45446, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-u9zg6pj_/listener-23hdc5qe', 'authkey': b\"'+\\xc2Qc\\xc7N\\xdc\\xbd\\xed4V\\x1b;St\"}\r\n2020-03-01 16:49:11,430 INFO (MainThread-214083) {'executor_id': 0, 'host': '192.168.1.179', 'job_name': 'ps', 'task_index': 0, 'port': 39696, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.1.179', 37200), 'authkey': b'\\xca}\\xf3\\x17\\xb0\\xbeFP\\xb9\\xc6\\xbf\\xc3c9Q\\x1f'}\r\n2020-03-01 16:49:11,430 INFO (MainThread-214083) {'executor_id': 3, 'host': '192.168.1.179', 'job_name': 'worker', 'task_index': 2, 'port': 35327, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-4523x4z4/listener-qw_dnrf9', 'authkey': b'\\xc5Oww\\xed\\x9aN\\xbc\\xa2\\xe1\\x1aw\\x85\\x89hG'}\r\n2020-03-01 16:49:11,430 INFO (MainThread-214083) {'executor_id': 4, 'host': '192.168.1.179', 'job_name': 'worker', 'task_index': 3, 'port': 41815, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-4ma9_jch/listener-o1l51v9q', 'authkey': b'R[d\\xa9\\xf2\\xd2J\\\\\\x91\\x86\\x0c\\x16\\xee\\x9c\\xfb\\xc9'}\r\n2020-03-01 16:49:11,430 INFO (MainThread-214083) Feeding inference data\r\n2020-03-01 16:49:11 INFO  deprecation:1173 - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\r\n2020-03-01 16:49:11 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1\r\n2020-03-01 16:49:11 INFO  SparkContext:54 - Starting job: runJob at SparkHadoopWriter.scala:78\r\n2020-03-01 16:49:11 INFO  DAGScheduler:54 - Got job 3 (runJob at SparkHadoopWriter.scala:78) with 4 output partitions\r\n2020-03-01 16:49:11 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)\r\n2020-03-01 16:49:11 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 4)\r\n2020-03-01 16:49:11 INFO  DAGScheduler:54 - Missing parents: List()\r\n2020-03-01 16:49:11 INFO  DAGScheduler:54 - Submitting ResultStage 5 (MapPartitionsRDD[23] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\r\n2020-03-01 16:49:11 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 81.2 KB, free 2004.2 MB)\r\n2020-03-01 16:49:11 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 32.2 KB, free 2004.1 MB)\r\n2020-03-01 16:49:11 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on hadoop-master:33487 (size: 32.2 KB, free: 2004.5 MB)\r\n2020-03-01 16:49:11 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1039\r\n2020-03-01 16:49:11 INFO  DAGScheduler:54 - Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\r\n2020-03-01 16:49:11 INFO  TaskSchedulerImpl:54 - Adding task set 5.0 with 4 tasks\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 12, 192.168.1.179, executor 0, partition 0, NODE_LOCAL, 7929 bytes)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Finished task 4.0 in stage 3.0 (TID 10) in 8753 ms on 192.168.1.179 (executor 0) (1/6)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Starting task 1.0 in stage 5.0 (TID 13, 192.168.1.179, executor 5, partition 1, NODE_LOCAL, 7929 bytes)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Finished task 2.0 in stage 3.0 (TID 8) in 8776 ms on 192.168.1.179 (executor 5) (2/6)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Starting task 2.0 in stage 5.0 (TID 14, 192.168.1.179, executor 4, partition 2, NODE_LOCAL, 7929 bytes)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Finished task 1.0 in stage 3.0 (TID 7) in 8827 ms on 192.168.1.179 (executor 4) (3/6)\r\n2020-03-01 16:49:12 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 192.168.1.179:45554 (size: 32.2 KB, free: 1458.5 MB)\r\n2020-03-01 16:49:12 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 192.168.1.179:44755 (size: 32.2 KB, free: 1458.6 MB)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Starting task 3.0 in stage 5.0 (TID 15, 192.168.1.179, executor 2, partition 3, NODE_LOCAL, 7929 bytes)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Finished task 5.0 in stage 3.0 (TID 11) in 8818 ms on 192.168.1.179 (executor 2) (4/6)\r\n2020-03-01 16:49:12 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 192.168.1.179:33084 (size: 32.2 KB, free: 1458.5 MB)\r\n2020-03-01 16:49:12 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 192.168.1.179:45305 (size: 32.2 KB, free: 1458.5 MB)\r\n2020-03-01 16:49:12 INFO  TaskSetManager:54 - Finished task 3.0 in stage 3.0 (TID 9) in 9000 ms on 192.168.1.179 (executor 3) (5/6)\r\n2020-03-01 16:49:12 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 192.168.1.179:42908\r\n2020-03-01 16:49:12 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 192.168.1.179:42912\r\n2020-03-01 16:49:12 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 192.168.1.179:42904\r\n2020-03-01 16:49:12 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 192.168.1.179:42900\r\n2020-03-01 16:57:55 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 12) in 522859 ms on 192.168.1.179 (executor 0) (1/4)\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 74\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 77\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 66\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 38\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 55\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 68\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 73\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 53\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 52\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 71\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 58\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 81\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 87\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 64\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 78\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 43\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 76\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 82\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 69\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 49\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 75\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 67\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 50\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 42\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 61\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 84\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 47\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 63\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 80\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 46\r\n2020-03-01 17:18:31 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on hadoop-master:33487 in memory (size: 3.7 KB, free: 2004.5 MB)\r\n2020-03-01 17:18:31 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on 192.168.1.179:33754 in memory (size: 3.7 KB, free: 1458.6 MB)\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 88\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 62\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 57\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 65\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 85\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 44\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 59\r\n2020-03-01 17:18:31 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on hadoop-master:33487 in memory (size: 8.3 KB, free: 2004.5 MB)\r\n2020-03-01 17:18:31 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 192.168.1.179:33084 in memory (size: 8.3 KB, free: 1458.5 MB)\r\n2020-03-01 17:18:31 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 192.168.1.179:45554 in memory (size: 8.3 KB, free: 1458.5 MB)\r\n2020-03-01 17:18:31 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 192.168.1.179:45305 in memory (size: 8.3 KB, free: 1458.5 MB)\r\n2020-03-01 17:18:31 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 192.168.1.179:37863 in memory (size: 8.3 KB, free: 1458.6 MB)\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 79\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 54\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 56\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 60\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 39\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 90\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 89\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 91\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 86\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 40\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 48\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 45\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 51\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 83\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 72\r\n2020-03-01 17:18:31 INFO  ContextCleaner:54 - Cleaned accumulator 70\r\n\r\nLooking forward to your response.\r\n@leewyang \r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/505", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/505/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/505/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/505/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/505", "id": 567291544, "node_id": "MDU6SXNzdWU1NjcyOTE1NDQ=", "number": 505, "title": "simple question about running  mechanism of TensorflowOnspark", "user": {"login": "guoyuhaoaaa", "id": 10019527, "node_id": "MDQ6VXNlcjEwMDE5NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/10019527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guoyuhaoaaa", "html_url": "https://github.com/guoyuhaoaaa", "followers_url": "https://api.github.com/users/guoyuhaoaaa/followers", "following_url": "https://api.github.com/users/guoyuhaoaaa/following{/other_user}", "gists_url": "https://api.github.com/users/guoyuhaoaaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/guoyuhaoaaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guoyuhaoaaa/subscriptions", "organizations_url": "https://api.github.com/users/guoyuhaoaaa/orgs", "repos_url": "https://api.github.com/users/guoyuhaoaaa/repos", "events_url": "https://api.github.com/users/guoyuhaoaaa/events{/privacy}", "received_events_url": "https://api.github.com/users/guoyuhaoaaa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-02-19T03:27:23Z", "updated_at": "2020-02-21T02:00:45Z", "closed_at": "2020-02-21T02:00:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "As I have read the core source code of TensorflowOnSpark, I know that we can build up distributed tensorflow (including ps node and worker node) among executors in spark cluster by invoking API TFCluster.run(). \r\n\r\nNext API  TFCluster.train() will be invoked to feed dataRDD into tensorflow worker node to train the model. The details content of TFCluster.train() are as follows:\r\ndataRDD.foreachRDD(lambda rdd: rdd.foreachPartition(TFSparkNode.train(self.cluster_info, self.cluster_meta, feed_timeout=feed_timeout, qname=qname)))\r\n\r\nNow I have a question , how can you avoid that any partition of dataRDD are fed into executor that running as PS node which cannot consume data but just hanging up and waiting for the gradients from worker node. Because Spark foreachRDD API will distribute the dataRDD to each executor including that running as PS node.\r\n\r\nLook forward to your response.\r\n@leewyang ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/495", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/495/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/495/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/495/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/495", "id": 552457912, "node_id": "MDU6SXNzdWU1NTI0NTc5MTI=", "number": 495, "title": "Failure to run keras examples on YARN", "user": {"login": "jerrygb", "id": 1516634, "node_id": "MDQ6VXNlcjE1MTY2MzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1516634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jerrygb", "html_url": "https://github.com/jerrygb", "followers_url": "https://api.github.com/users/jerrygb/followers", "following_url": "https://api.github.com/users/jerrygb/following{/other_user}", "gists_url": "https://api.github.com/users/jerrygb/gists{/gist_id}", "starred_url": "https://api.github.com/users/jerrygb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jerrygb/subscriptions", "organizations_url": "https://api.github.com/users/jerrygb/orgs", "repos_url": "https://api.github.com/users/jerrygb/repos", "events_url": "https://api.github.com/users/jerrygb/events{/privacy}", "received_events_url": "https://api.github.com/users/jerrygb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-20T18:45:02Z", "updated_at": "2020-04-28T17:17:37Z", "closed_at": "2020-04-28T17:17:37Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\nPython version [e.g. 2.7, 3.6] 3.6\r\nSpark version [e.g. 2.1, 2.3.1] 2.4.4\r\nTensorFlow version [e.g. 1.5, 1.9.0] 1.14\r\nTensorFlowOnSpark version [e.g. 1.1, 1.3.2] master\r\nCluster version [e.g. Standalone, Hadoop 2.8, CDH5] Hadoop 2.8.5\r\n\r\n\r\n**Describe the bug:**\r\nI am trying to follow the guide available at [keras example](https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/mnist/keras) to do dataset generation and training using keras. I am using YARN scheduler. However, I hit error during the training phase.\r\n\r\nWhen I am using tensorflow 1.14.\r\n\r\n**Logs:**\r\n\r\n```\r\n20/01/20 17:31:39 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, ip-10-0-1-161.ec2.internal, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/mnt2/yarn/usercache/hadoop/appcache/application_1579529222257_0022/container_1579529222257_0022_01_000002/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/mnt2/yarn/usercache/hadoop/appcache/application_1579529222257_0022/container_1579529222257_0022_01_000002/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/mnt3/yarn/usercache/hadoop/appcache/application_1579529222257_0022/container_1579529222257_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/mnt3/yarn/usercache/hadoop/appcache/application_1579529222257_0022/container_1579529222257_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/mnt3/yarn/usercache/hadoop/appcache/application_1579529222257_0022/container_1579529222257_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/mnt3/yarn/usercache/hadoop/appcache/application_1579529222257_0022/container_1579529222257_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/mnt3/yarn/usercache/hadoop/appcache/application_1579529222257_0022/container_1579529222257_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 375, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 334, in wrapper_fn\r\n    fn(args, context)\r\n  File \"mnist_tf_ds.py\", line 45, in main_fun\r\nTypeError: interleave() missing 1 required positional argument: 'cycle_length'\r\n```\r\n\r\n**Spark Submit Command Line:**\r\n\r\n```\r\n${SPARK_HOME}/bin/spark-submit  --deploy-mode cluster \\\r\n--queue default --num-executors 2 \\\r\n--conf spark.executorEnv.CLASSPATH=\"$(hadoop classpath --glob)\" \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/usr/lib/jvm/jre-1.8.0-openjdk.x86_64/lib/amd64/server/\" \\\r\n--executor-memory 4G --archives mnist/mnist.zip#mnist \\\r\n--jars hdfs:///user/${USER}/tensorflow-hadoop-1.10.0.jar,hdfs:///user/${USER}//spark-tensorflow-connector_2.11-1.10.0.jar \\\r\nTensorFlowOnSpark/examples/mnist/keras/mnist_tf_ds.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--images_labels hdfs:///user/hadoop/cluster/tfr/train \\\r\n--model_dir ${TFoS_HOME}/mnist_model \\\r\n--export_dir ${TFoS_HOME}/mnist_export \\\r\n--data_format tfos\r\n```\r\n\r\nIn addition to this I have also tried the training the getting started guide with when running YARN based scheduler [here](https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN).\r\n\r\nI noticed there are some dead links like `TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py`.\r\n\r\nI tried these examples without re-packaging the python source. since I already have PYTHONPATH (v3.6.9) pushed into the executor context task.\r\n\r\nAdditionally, though the issue is for YARN, I have also tried Standalone training with the following error. I am able to see data partitions feeding into the training function but it ends up with the same error,\r\n\r\n```\r\n20/01/20 19:01:32 INFO SparkContext: Running Spark version 2.4.4\r\n20/01/20 19:01:32 INFO SparkContext: Submitted application: mnist_keras\r\n20/01/20 19:01:32 INFO SecurityManager: Changing view acls to: hadoop\r\n20/01/20 19:01:32 INFO SecurityManager: Changing modify acls to: hadoop\r\n20/01/20 19:01:32 INFO SecurityManager: Changing view acls groups to: \r\n20/01/20 19:01:32 INFO SecurityManager: Changing modify acls groups to: \r\n20/01/20 19:01:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()\r\n20/01/20 19:01:32 INFO Utils: Successfully started service 'sparkDriver' on port 43231.\r\n20/01/20 19:01:32 INFO SparkEnv: Registering MapOutputTracker\r\n20/01/20 19:01:32 INFO SparkEnv: Registering BlockManagerMaster\r\n20/01/20 19:01:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n20/01/20 19:01:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n20/01/20 19:01:32 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-756b6c45-a2b9-4576-862c-7d36c8e45e2a\r\n20/01/20 19:01:32 INFO MemoryStore: MemoryStore started with capacity 1028.8 MB\r\n20/01/20 19:01:32 INFO SparkEnv: Registering OutputCommitCoordinator\r\n20/01/20 19:01:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.\r\n20/01/20 19:01:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-10-0-1-11.ec2.internal:4040\r\n20/01/20 19:01:32 INFO SparkContext: Added JAR hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/tensorflow-hadoop-1.10.0.jar at hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/tensorflow-hadoop-1.10.0.jar with timestamp 1579546892803\r\n20/01/20 19:01:32 INFO SparkContext: Added JAR hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/spark-tensorflow-connector_2.11-1.10.0.jar at hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/spark-tensorflow-connector_2.11-1.10.0.jar with timestamp 1579546892804\r\n20/01/20 19:01:32 INFO Executor: Starting executor ID driver on host localhost\r\n20/01/20 19:01:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46523.\r\n20/01/20 19:01:32 INFO NettyBlockTransferService: Server created on ip-10-0-1-11.ec2.internal:46523\r\n20/01/20 19:01:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n20/01/20 19:01:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-10-0-1-11.ec2.internal, 46523, None)\r\n20/01/20 19:01:33 INFO BlockManagerMasterEndpoint: Registering block manager ip-10-0-1-11.ec2.internal:46523 with 1028.8 MB RAM, BlockManagerId(driver, ip-10-0-1-11.ec2.internal, 46523, None)\r\n20/01/20 19:01:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-10-0-1-11.ec2.internal, 46523, None)\r\n20/01/20 19:01:33 INFO BlockManager: external shuffle service port = 7337\r\n20/01/20 19:01:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-10-0-1-11.ec2.internal, 46523, None)\r\n20/01/20 19:01:33 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/local-1579546892872\r\nargs: Namespace(batch_size=64, cluster_size=2, epochs=3, export_dir='/mnist_export', images_labels='hdfs:///user/hadoop/cluster/csv/train', model_dir='hdfs:///user/hadoop/mnist_model', tensorboard=False)\r\n20/01/20 19:01:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 237.1 KB, free 1028.6 MB)\r\n20/01/20 19:01:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 1028.6 MB)\r\n20/01/20 19:01:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-10-0-1-11.ec2.internal:46523 (size: 23.9 KB, free: 1028.8 MB)\r\n20/01/20 19:01:33 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n2020-01-20 19:01:33,868 INFO (MainThread-12644) Reserving TFSparkNodes \r\n2020-01-20 19:01:33,869 INFO (MainThread-12644) cluster_template: {'chief': [0], 'worker': [1]}\r\n2020-01-20 19:01:33,871 INFO (MainThread-12644) listening for reservations at ('10.0.1.11', 33739)\r\n2020-01-20 19:01:33,871 INFO (MainThread-12644) Starting TensorFlow on executors\r\n2020-01-20 19:01:33,884 INFO (MainThread-12644) Waiting for TFSparkNodes to start\r\n2020-01-20 19:01:33,884 INFO (MainThread-12644) waiting for 2 reservations\r\n20/01/20 19:01:33 INFO SparkContext: Starting job: foreachPartition at /usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:322\r\n20/01/20 19:01:33 INFO DAGScheduler: Got job 0 (foreachPartition at /usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:322) with 2 output partitions\r\n20/01/20 19:01:33 INFO DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:322)\r\n20/01/20 19:01:33 INFO DAGScheduler: Parents of final stage: List()\r\n20/01/20 19:01:33 INFO DAGScheduler: Missing parents: List()\r\n20/01/20 19:01:34 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[3] at foreachPartition at /usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:322), which has no missing parents\r\n20/01/20 19:01:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 50.3 KB, free 1028.5 MB)\r\n20/01/20 19:01:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.9 KB, free 1028.5 MB)\r\n20/01/20 19:01:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-10-0-1-11.ec2.internal:46523 (size: 20.9 KB, free: 1028.8 MB)\r\n20/01/20 19:01:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1201\r\n20/01/20 19:01:34 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[3] at foreachPartition at /usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:322) (first 15 tasks are for partitions Vector(0, 1))\r\n20/01/20 19:01:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\r\n20/01/20 19:01:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7852 bytes)\r\n20/01/20 19:01:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7852 bytes)\r\n20/01/20 19:01:34 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\r\n20/01/20 19:01:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\r\n20/01/20 19:01:34 INFO Executor: Fetching hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/tensorflow-hadoop-1.10.0.jar with timestamp 1579546892803\r\n20/01/20 19:01:34 INFO Utils: Fetching hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/tensorflow-hadoop-1.10.0.jar to /mnt/tmp/spark-a361b248-fc78-407a-b69a-6a73813264fe/userFiles-465eace1-bebd-43b9-9e74-70ec8e37f0ee/fetchFileTemp3622651641369623100.tmp\r\n20/01/20 19:01:34 INFO Executor: Adding file:/mnt/tmp/spark-a361b248-fc78-407a-b69a-6a73813264fe/userFiles-465eace1-bebd-43b9-9e74-70ec8e37f0ee/tensorflow-hadoop-1.10.0.jar to class loader\r\n20/01/20 19:01:34 INFO Executor: Fetching hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/spark-tensorflow-connector_2.11-1.10.0.jar with timestamp 1579546892804\r\n20/01/20 19:01:34 INFO Utils: Fetching hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/spark-tensorflow-connector_2.11-1.10.0.jar to /mnt/tmp/spark-a361b248-fc78-407a-b69a-6a73813264fe/userFiles-465eace1-bebd-43b9-9e74-70ec8e37f0ee/fetchFileTemp4678719661063671876.tmp\r\n20/01/20 19:01:34 INFO Executor: Adding file:/mnt/tmp/spark-a361b248-fc78-407a-b69a-6a73813264fe/userFiles-465eace1-bebd-43b9-9e74-70ec8e37f0ee/spark-tensorflow-connector_2.11-1.10.0.jar to class loader\r\n2020-01-20 19:01:34,885 INFO (MainThread-12644) waiting for 2 reservations\r\n2020-01-20 19:01:35,887 INFO (MainThread-12644) waiting for 2 reservations\r\n2020-01-20 19:01:36,888 INFO (MainThread-12644) waiting for 2 reservations\r\n2020-01-20 19:01:37,889 INFO (MainThread-12644) waiting for 2 reservations\r\n2020-01-20 19:01:38,138 INFO (MainThread-12724) connected to server at ('10.0.1.11', 33739)\r\n2020-01-20 19:01:38,139 INFO (MainThread-12724) TFSparkNode.reserve: {'executor_id': 0, 'host': '10.0.1.11', 'job_name': 'chief', 'task_index': 0, 'port': 37999, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-zx9jg5l_/listener-ao3t8a04', 'authkey': b'\\x169\\xa1\\xa6\\xd3\\xc1KD\\xbe\\x8c\\x89\\x7f\\x15\\x13Py'}\r\n2020-01-20 19:01:38,145 INFO (MainThread-12723) connected to server at ('10.0.1.11', 33739)\r\n2020-01-20 19:01:38,146 INFO (MainThread-12723) TFSparkNode.reserve: {'executor_id': 1, 'host': '10.0.1.11', 'job_name': 'worker', 'task_index': 0, 'port': 43677, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-2jgvjpys/listener-5_mxfl54', 'authkey': b'\\xa2\\xfb\\x90:\\x19\\x18Cs\\xbe\\x7f\\x88C\\xaf\\x9c\\x14<'}\r\n2020-01-20 19:01:38,890 INFO (MainThread-12644) all reservations completed\r\n2020-01-20 19:01:38,890 INFO (MainThread-12644) All TFSparkNodes started\r\n2020-01-20 19:01:38,890 INFO (MainThread-12644) {'executor_id': 0, 'host': '10.0.1.11', 'job_name': 'chief', 'task_index': 0, 'port': 37999, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-zx9jg5l_/listener-ao3t8a04', 'authkey': b'\\x169\\xa1\\xa6\\xd3\\xc1KD\\xbe\\x8c\\x89\\x7f\\x15\\x13Py'}\r\n2020-01-20 19:01:38,891 INFO (MainThread-12644) {'executor_id': 1, 'host': '10.0.1.11', 'job_name': 'worker', 'task_index': 0, 'port': 43677, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-2jgvjpys/listener-5_mxfl54', 'authkey': b'\\xa2\\xfb\\x90:\\x19\\x18Cs\\xbe\\x7f\\x88C\\xaf\\x9c\\x14<'}\r\n2020-01-20 19:01:38,891 INFO (MainThread-12644) Feeding training data\r\n20/01/20 19:01:38 INFO GPLNativeCodeLoader: Loaded native gpl library\r\n20/01/20 19:01:38 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev a3b61461af0d6b4d981c915b0a1f342464987aaa]\r\n20/01/20 19:01:38 INFO FileInputFormat: Total input files to process : 10\r\n20/01/20 19:01:39 INFO SparkContext: Starting job: collect at PythonRDD.scala:166\r\n20/01/20 19:01:39 INFO DAGScheduler: Got job 1 (collect at PythonRDD.scala:166) with 30 output partitions\r\n20/01/20 19:01:39 INFO DAGScheduler: Final stage: ResultStage 1 (collect at PythonRDD.scala:166)\r\n20/01/20 19:01:39 INFO DAGScheduler: Parents of final stage: List()\r\n20/01/20 19:01:39 INFO DAGScheduler: Missing parents: List()\r\n20/01/20 19:01:39 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53), which has no missing parents\r\n20/01/20 19:01:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 74.7 KB, free 1028.5 MB)\r\n20/01/20 19:01:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1028.4 MB)\r\n20/01/20 19:01:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-10-0-1-11.ec2.internal:46523 (size: 22.2 KB, free: 1028.8 MB)\r\n20/01/20 19:01:39 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1201\r\n20/01/20 19:01:39 INFO DAGScheduler: Submitting 30 missing tasks from ResultStage 1 (PythonRDD[6] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\r\n20/01/20 19:01:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 30 tasks\r\n20/01/20 19:01:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 8047 bytes)\r\n20/01/20 19:01:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 8047 bytes)\r\n20/01/20 19:01:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\r\n20/01/20 19:01:39 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\r\n20/01/20 19:01:39 INFO HadoopRDD: Input split: hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/cluster/csv/train/part-00001:0+11218547\r\n20/01/20 19:01:39 INFO HadoopRDD: Input split: hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/cluster/csv/train/part-00000:0+9350866\r\n2020-01-20 19:01:39,147 INFO (MainThread-12723) node: {'executor_id': 0, 'host': '10.0.1.11', 'job_name': 'chief', 'task_index': 0, 'port': 37999, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-zx9jg5l_/listener-ao3t8a04', 'authkey': b'\\x169\\xa1\\xa6\\xd3\\xc1KD\\xbe\\x8c\\x89\\x7f\\x15\\x13Py'}\r\n2020-01-20 19:01:39,147 INFO (MainThread-12723) node: {'executor_id': 1, 'host': '10.0.1.11', 'job_name': 'worker', 'task_index': 0, 'port': 43677, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-2jgvjpys/listener-5_mxfl54', 'authkey': b'\\xa2\\xfb\\x90:\\x19\\x18Cs\\xbe\\x7f\\x88C\\xaf\\x9c\\x14<'}\r\n2020-01-20 19:01:39,147 INFO (MainThread-12723) export TF_CONFIG: {\"cluster\": {\"chief\": [\"10.0.1.11:37999\"], \"worker\": [\"10.0.1.11:43677\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}, \"environment\": \"cloud\"}\r\n2020-01-20 19:01:39,148 INFO (MainThread-12723) Starting TensorFlow worker:0 as worker on cluster node 1 on background process\r\n20/01/20 19:01:39 INFO PythonRunner: Times: total = 4883, boot = 586, init = 3281, finish = 1016\r\n2020-01-20 19:01:39.199691: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-01-20 19:01:39.201209: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n20/01/20 19:01:39 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1461 bytes result sent to driver\r\n20/01/20 19:01:39 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4, localhost, executor driver, partition 2, ANY, 8047 bytes)\r\n20/01/20 19:01:39 INFO Executor: Running task 2.0 in stage 1.0 (TID 4)\r\n20/01/20 19:01:39 INFO HadoopRDD: Input split: hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/cluster/csv/train/part-00002:0+11218521\r\n2020-01-20 19:01:39,257 INFO (MainThread-12788) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:39,261 WARNING (MainThread-12788) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:39,262 INFO (MainThread-12788) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\n20/01/20 19:01:39 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5173 ms on localhost (executor driver) (1/2)\r\n2020-01-20 19:01:39,320 WARNING (MainThread-12788) From /usr/local/lib64/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n    \r\n20/01/20 19:01:39 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56767\r\n2020-01-20 19:01:40,142 INFO (MainThread-12724) node: {'executor_id': 0, 'host': '10.0.1.11', 'job_name': 'chief', 'task_index': 0, 'port': 37999, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-zx9jg5l_/listener-ao3t8a04', 'authkey': b'\\x169\\xa1\\xa6\\xd3\\xc1KD\\xbe\\x8c\\x89\\x7f\\x15\\x13Py'}\r\n2020-01-20 19:01:40,142 INFO (MainThread-12724) node: {'executor_id': 1, 'host': '10.0.1.11', 'job_name': 'worker', 'task_index': 0, 'port': 43677, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-2jgvjpys/listener-5_mxfl54', 'authkey': b'\\xa2\\xfb\\x90:\\x19\\x18Cs\\xbe\\x7f\\x88C\\xaf\\x9c\\x14<'}\r\n2020-01-20 19:01:40,143 INFO (MainThread-12724) export TF_CONFIG: {\"cluster\": {\"chief\": [\"10.0.1.11:37999\"], \"worker\": [\"10.0.1.11:43677\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}, \"environment\": \"cloud\"}\r\n2020-01-20 19:01:40,143 INFO (MainThread-12724) Starting TensorFlow chief:0 as chief on cluster node 0 on background process\r\n20/01/20 19:01:40 INFO PythonRunner: Times: total = 5878, boot = 590, init = 3270, finish = 2018\r\n20/01/20 19:01:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1461 bytes result sent to driver\r\n20/01/20 19:01:40 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 5, localhost, executor driver, partition 3, ANY, 8047 bytes)\r\n20/01/20 19:01:40 INFO Executor: Running task 3.0 in stage 1.0 (TID 5)\r\n20/01/20 19:01:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6053 ms on localhost (executor driver) (2/2)\r\n20/01/20 19:01:40 INFO DAGScheduler: ResultStage 0 (foreachPartition at /usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:322) finished in 6.130 s\r\n20/01/20 19:01:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \r\n20/01/20 19:01:40 INFO HadoopRDD: Input split: hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/cluster/csv/train/part-00003:0+11216962\r\n20/01/20 19:01:40 INFO DAGScheduler: Job 0 finished: foreachPartition at /usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:322, took 6.244327 s\r\n2020-01-20 19:01:40.238009: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX512F\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-01-20 19:01:40.239972: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2020-01-20 19:01:40,329 INFO (MainThread-12824) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:40,331 WARNING (MainThread-12824) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:40,331 INFO (MainThread-12824) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:40,396 WARNING (MainThread-12824) From /usr/local/lib64/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n    \r\n20/01/20 19:01:40 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ip-10-0-1-11.ec2.internal:46523 in memory (size: 20.9 KB, free: 1028.8 MB)\r\n2020-01-20 19:01:45,481 WARNING (MainThread-12788) From /usr/local/lib64/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2020-01-20 19:01:45,836 INFO (MainThread-12788) Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'worker', task_id = 0, environment = 'cloud', rpc_layer = 'grpc'\r\n2020-01-20 19:01:45,836 WARNING (MainThread-12788) `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n2020-01-20 19:01:45,836 WARNING (MainThread-12788) `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-01-20 19:01:45,838 INFO (MainThread-12788) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:45,838 WARNING (MainThread-12788) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:45,839 INFO (MainThread-12788) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:45,839 INFO (MainThread-12788) Starting standard TensorFlow server, target = 'grpc://10.0.1.11:43677', session_config= allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:chief/replica:0/task:0\"\r\n}\r\n\r\n2020-01-20 19:01:45.841960: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job chief -> {0 -> 10.0.1.11:37999}\r\n2020-01-20 19:01:45.841985: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:43677}\r\n2020-01-20 19:01:45.871215: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:43677\r\n2020-01-20 19:01:45.871268: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:43677)\r\n2020-01-20 19:01:45,873 INFO (MainThread-12788) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:45,873 WARNING (MainThread-12788) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:45,874 INFO (MainThread-12788) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:45,929 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:45,929 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:45,968 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:45,973 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:45,982 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:45,998 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,021 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,033 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,037 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,037 INFO (MainThread-12788) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,254 INFO (MainThread-12788) Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'worker', task_id = 0, environment = 'cloud', rpc_layer = 'grpc'\r\n2020-01-20 19:01:46,255 WARNING (MainThread-12788) `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n2020-01-20 19:01:46,255 WARNING (MainThread-12788) `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-01-20 19:01:46,262 INFO (MainThread-12788) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:46,263 WARNING (MainThread-12788) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:46,263 INFO (MainThread-12788) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:46,271 INFO (MainThread-12788) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:46,271 WARNING (MainThread-12788) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:46,271 INFO (MainThread-12788) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:46,443 WARNING (MainThread-12824) From /usr/local/lib64/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\n2020-01-20 19:01:46,707 INFO (MainThread-12824) Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'chief', task_id = 0, environment = 'cloud', rpc_layer = 'grpc'\r\n2020-01-20 19:01:46,708 WARNING (MainThread-12824) `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n2020-01-20 19:01:46,708 WARNING (MainThread-12824) `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-01-20 19:01:46,710 INFO (MainThread-12824) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:46,710 WARNING (MainThread-12824) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:46,710 INFO (MainThread-12824) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:46,710 INFO (MainThread-12824) Starting standard TensorFlow server, target = 'grpc://10.0.1.11:37999', session_config= allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    scoped_allocator_optimization: ON\r\n    scoped_allocator_opts {\r\n      enable_op: \"CollectiveReduce\"\r\n    }\r\n  }\r\n}\r\nexperimental {\r\n  collective_group_leader: \"/job:chief/replica:0/task:0\"\r\n}\r\n\r\n2020-01-20 19:01:46.713226: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job chief -> {0 -> localhost:37999}\r\n2020-01-20 19:01:46.713253: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.1.11:43677}\r\n2020-01-20 19:01:46.714236: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:37999\r\n2020-01-20 19:01:46.714264: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:37999)\r\n2020-01-20 19:01:46,715 INFO (MainThread-12824) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:46,716 WARNING (MainThread-12824) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:46,716 INFO (MainThread-12824) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:46,755 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,756 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,804 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,805 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,809 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,809 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,849 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,849 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,852 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:46,852 INFO (MainThread-12824) Collective batch_all_reduce: 1 all-reduces, num_workers = 2\r\n2020-01-20 19:01:47,000 INFO (MainThread-12824) Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'chief', task_id = 0, environment = 'cloud', rpc_layer = 'grpc'\r\n2020-01-20 19:01:47,001 WARNING (MainThread-12824) `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n2020-01-20 19:01:47,001 WARNING (MainThread-12824) `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-01-20 19:01:47,002 INFO (MainThread-12824) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:47,003 WARNING (MainThread-12824) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:47,003 INFO (MainThread-12824) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:47,005 INFO (MainThread-12824) Device is available but not used by distribute strategy: /device:CPU:0\r\n2020-01-20 19:01:47,005 WARNING (MainThread-12824) Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2020-01-20 19:01:47,005 INFO (MainThread-12824) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['10.0.1.11:37999'], 'worker': ['10.0.1.11:43677']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n2020-01-20 19:01:47.789722: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:335] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n2020-01-20 19:01:47.803315: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:335] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n2020-01-20 19:01:48,317 INFO (MainThread-12788) Collective batch_all_reduce: 6 all-reduces, num_workers = 2\r\n2020-01-20 19:01:48,317 INFO (MainThread-12788) Collective batch_all_reduce: 6 all-reduces, num_workers = 2\r\n2020-01-20 19:01:48,405 INFO (MainThread-12824) Collective batch_all_reduce: 6 all-reduces, num_workers = 2\r\n2020-01-20 19:01:48,405 INFO (MainThread-12824) Collective batch_all_reduce: 6 all-reduces, num_workers = 2\r\nEpoch 1/3\r\nEpoch 1/3\r\n2020-01-20 19:01:50,330 INFO (MainThread-12806) Connected to TFSparkNode.mgr on 10.0.1.11, executor=1, state='running'\r\n2020-01-20 19:01:50,340 INFO (MainThread-12806) mgr.state='running'\r\n2020-01-20 19:01:50,340 INFO (MainThread-12806) Feeding partition <itertools.chain object at 0x7fbb128a76d8> into input queue <multiprocessing.queues.JoinableQueue object at 0x7fbb128a7710>\r\n2020-01-20 19:01:50,491 INFO (MainThread-12785) Connected to TFSparkNode.mgr on 10.0.1.11, executor=1, state='running'\r\n2020-01-20 19:01:50,642 INFO (MainThread-12785) mgr.state='running'\r\n2020-01-20 19:01:50,643 INFO (MainThread-12785) Feeding partition <itertools.chain object at 0x7fbb128a76d8> into input queue <multiprocessing.queues.JoinableQueue object at 0x7fbb128a7710>\r\n20/01/20 19:01:50 INFO PythonRunner: Times: total = 11336, boot = 13, init = 24, finish = 11299\r\n20/01/20 19:01:50 INFO PythonRunner: Times: total = 5311, boot = 7, init = 17, finish = 5287\r\n2020-01-20 19:01:50,719 INFO (MainThread-12782) Connected to TFSparkNode.mgr on 10.0.1.11, executor=1, state='running'\r\n2020-01-20 19:01:50,742 INFO (MainThread-12782) mgr.state='running'\r\n2020-01-20 19:01:50,742 INFO (MainThread-12782) Feeding partition <itertools.chain object at 0x7fbb128a76d8> into input queue <multiprocessing.queues.JoinableQueue object at 0x7fbb128a7710>\r\n2020-01-20 19:01:50,773 INFO (MainThread-12842) Connected to TFSparkNode.mgr on 10.0.1.11, executor=1, state='running'\r\n2020-01-20 19:01:50,803 INFO (MainThread-12842) mgr.state='running'\r\n2020-01-20 19:01:50,803 INFO (MainThread-12842) Feeding partition <itertools.chain object at 0x7fbb128a76d8> into input queue <multiprocessing.queues.JoinableQueue object at 0x7fbb128a7710>\r\n20/01/20 19:01:50 INFO PythonRunner: Times: total = 11854, boot = 2, init = 21, finish = 11831\r\n2020-01-20 19:01:51.520744: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Upper bound check fail for input 1 from node SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1_17 input bounds = [0x7fbadc057b00, 0x7fbadc057f80] backing_tensor bounds = [0x7fba603a0040, 0x7fba604f3068]\r\n\t [[{{node scoped_allocator_concat_1_17}}]]\r\n2020-01-20 19:01:51.520977: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520815339\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521006: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520815339\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521062: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520893987\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521073: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520893987\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521106: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520928646\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521115: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520928646\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521234: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at collective_ops.cc:223 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520893987\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521240: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at collective_ops.cc:223 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520815339\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n2020-01-20 19:01:51.521397: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at collective_ops.cc:223 : Cancelled: [_Derived_]Cancelled\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579546911.520928646\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}\r\n20/01/20 19:01:51 INFO PythonRunner: Times: total = 11532, boot = 25, init = 25, finish = 11482\r\n20/01/20 19:01:52 ERROR Executor: Exception in task 2.0 in stage 1.0 (TID 4)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 424, in _train\r\n    raise Exception(\"Exception in worker:\\n\" + e_str)\r\nException: Exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 340, in wrapper_fn_background\r\n    wrapper_fn(args, context)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 334, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/hadoop/TensorFlowOnSpark/examples/mnist/keras/mnist_spark.py\", line 66, in main_fun\r\n    multi_worker_model.fit(x=ds, epochs=args.epochs, steps_per_epoch=max_steps_per_worker, callbacks=callbacks)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 631, in fit\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 625, in _worker_fn\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 274, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:\r\nUpper bound check fail for input 1 from node SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1_17 input bounds = [0x7fbadc057b00, 0x7fbadc057f80] backing_tensor bounds = [0x7fba603a0040, 0x7fba604f3068]\r\n\t [[{{node scoped_allocator_concat_1_17}}]]\r\n\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n20/01/20 19:01:52 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 6, localhost, executor driver, partition 4, ANY, 8047 bytes)\r\n20/01/20 19:01:52 INFO Executor: Running task 4.0 in stage 1.0 (TID 6)\r\n20/01/20 19:01:52 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 4, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 424, in _train\r\n    raise Exception(\"Exception in worker:\\n\" + e_str)\r\nException: Exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 340, in wrapper_fn_background\r\n    wrapper_fn(args, context)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 334, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/hadoop/TensorFlowOnSpark/examples/mnist/keras/mnist_spark.py\", line 66, in main_fun\r\n    multi_worker_model.fit(x=ds, epochs=args.epochs, steps_per_epoch=max_steps_per_worker, callbacks=callbacks)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 631, in fit\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 625, in _worker_fn\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 274, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:\r\nUpper bound check fail for input 1 from node SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1_17 input bounds = [0x7fbadc057b00, 0x7fbadc057f80] backing_tensor bounds = [0x7fba603a0040, 0x7fba604f3068]\r\n\t [[{{node scoped_allocator_concat_1_17}}]]\r\n\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n20/01/20 19:01:52 INFO HadoopRDD: Input split: hdfs://ip-10-0-1-11.ec2.internal:8020/user/hadoop/cluster/csv/train/part-00004:0+11225266\r\n20/01/20 19:01:52 ERROR TaskSetManager: Task 2 in stage 1.0 failed 1 times; aborting job\r\n20/01/20 19:01:52 INFO TaskSchedulerImpl: Cancelling stage 1\r\n20/01/20 19:01:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled\r\n20/01/20 19:01:52 INFO TaskSchedulerImpl: Stage 1 was cancelled\r\n20/01/20 19:01:52 INFO Executor: Executor is trying to kill task 3.0 in stage 1.0 (TID 5), reason: Stage cancelled\r\n20/01/20 19:01:52 INFO Executor: Executor is trying to kill task 0.0 in stage 1.0 (TID 2), reason: Stage cancelled\r\n20/01/20 19:01:52 INFO Executor: Executor is trying to kill task 4.0 in stage 1.0 (TID 6), reason: Stage cancelled\r\n20/01/20 19:01:52 INFO Executor: Executor is trying to kill task 1.0 in stage 1.0 (TID 3), reason: Stage cancelled\r\n20/01/20 19:01:52 INFO DAGScheduler: ResultStage 1 (collect at PythonRDD.scala:166) failed in 13.968 s due to Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 4, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 424, in _train\r\n    raise Exception(\"Exception in worker:\\n\" + e_str)\r\nException: Exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 340, in wrapper_fn_background\r\n    wrapper_fn(args, context)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 334, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/hadoop/TensorFlowOnSpark/examples/mnist/keras/mnist_spark.py\", line 66, in main_fun\r\n    multi_worker_model.fit(x=ds, epochs=args.epochs, steps_per_epoch=max_steps_per_worker, callbacks=callbacks)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 631, in fit\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 625, in _worker_fn\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 274, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:\r\nUpper bound check fail for input 1 from node SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1_17 input bounds = [0x7fbadc057b00, 0x7fbadc057f80] backing_tensor bounds = [0x7fba603a0040, 0x7fba604f3068]\r\n\t [[{{node scoped_allocator_concat_1_17}}]]\r\n\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n20/01/20 19:01:52 INFO DAGScheduler: Job 1 failed: collect at PythonRDD.scala:166, took 13.987017 s\r\nTraceback (most recent call last):\r\n  File \"/home/hadoop/TensorFlowOnSpark/examples/mnist/keras/mnist_spark.py\", line 108, in <module>\r\n    cluster.train(images_labels, args.epochs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py\", line 94, in train\r\n    unionRDD.foreachPartition(TFSparkNode.train(self.cluster_info, self.cluster_meta, feed_timeout=feed_timeout, qname=qname))\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 806, in foreachPartition\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1055, in count\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1046, in sum\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 917, in fold\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 816, in collect\r\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\r\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 1 times, most recent failure: Lost task 2.0 in stage 1.0 (TID 4, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 424, in _train\r\n    raise Exception(\"Exception in worker:\\n\" + e_str)\r\nException: Exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 340, in wrapper_fn_background\r\n    wrapper_fn(args, context)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 334, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/hadoop/TensorFlowOnSpark/examples/mnist/keras/mnist_spark.py\", line 66, in main_fun\r\n    multi_worker_model.fit(x=ds, epochs=args.epochs, steps_per_epoch=max_steps_per_worker, callbacks=callbacks)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 631, in fit\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 625, in _worker_fn\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 274, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:\r\nUpper bound check fail for input 1 from node SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1_17 input bounds = [0x7fbadc057b00, 0x7fbadc057f80] backing_tensor bounds = [0x7fba603a0040, 0x7fba604f3068]\r\n\t [[{{node scoped_allocator_concat_1_17}}]]\r\n\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 424, in _train\r\n    raise Exception(\"Exception in worker:\\n\" + e_str)\r\nException: Exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 340, in wrapper_fn_background\r\n    wrapper_fn(args, context)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 334, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/hadoop/TensorFlowOnSpark/examples/mnist/keras/mnist_spark.py\", line 66, in main_fun\r\n    multi_worker_model.fit(x=ds, epochs=args.epochs, steps_per_epoch=max_steps_per_worker, callbacks=callbacks)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 631, in fit\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 625, in _worker_fn\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 274, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/usr/local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:\r\nUpper bound check fail for input 1 from node SGD/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropFilter to node scoped_allocator_concat_1_17 input bounds = [0x7fbadc057b00, 0x7fbadc057f80] backing_tensor bounds = [0x7fba603a0040, 0x7fba604f3068]\r\n\t [[{{node scoped_allocator_concat_1_17}}]]\r\n\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n\r\n20/01/20 19:01:53 INFO Executor: Executor killed task 4.0 in stage 1.0 (TID 6), reason: Stage cancelled\r\n20/01/20 19:01:53 WARN TaskSetManager: Lost task 4.0 in stage 1.0 (TID 6, localhost, executor driver): TaskKilled (Stage cancelled)\r\n20/01/20 19:01:53 INFO SparkContext: Invoking stop() from shutdown hook\r\n20/01/20 19:01:53 INFO SparkUI: Stopped Spark web UI at http://ip-10-0-1-11.ec2.internal:4040\r\n20/01/20 19:01:53 INFO Executor: Executor killed task 1.0 in stage 1.0 (TID 3), reason: Stage cancelled\r\n20/01/20 19:01:53 INFO Executor: Executor killed task 0.0 in stage 1.0 (TID 2), reason: Stage cancelled\r\n20/01/20 19:01:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\r\n20/01/20 19:01:53 INFO Executor: Executor killed task 3.0 in stage 1.0 (TID 5), reason: Stage cancelled\r\n20/01/20 19:01:53 INFO MemoryStore: MemoryStore cleared\r\n20/01/20 19:01:53 INFO BlockManager: BlockManager stopped\r\n20/01/20 19:01:53 INFO BlockManagerMaster: BlockManagerMaster stopped\r\n20/01/20 19:01:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\r\n20/01/20 19:01:53 INFO SparkContext: Successfully stopped SparkContext\r\n20/01/20 19:01:53 INFO ShutdownHookManager: Shutdown hook called\r\n20/01/20 19:01:53 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-40999a81-0d50-4801-b854-57923fb786a9\r\n20/01/20 19:01:53 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-a361b248-fc78-407a-b69a-6a73813264fe\r\n20/01/20 19:01:53 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-a361b248-fc78-407a-b69a-6a73813264fe/pyspark-ee31ac37-9363-43ba-a66c-bb40af23a669\r\n\r\n\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/494", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/494/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/494/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/494/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/494", "id": 551468293, "node_id": "MDU6SXNzdWU1NTE0NjgyOTM=", "number": 494, "title": "hdfsBuilderConnect class not found when loading the datasets into HDFS", "user": {"login": "jerrygb", "id": 1516634, "node_id": "MDQ6VXNlcjE1MTY2MzQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1516634?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jerrygb", "html_url": "https://github.com/jerrygb", "followers_url": "https://api.github.com/users/jerrygb/followers", "following_url": "https://api.github.com/users/jerrygb/following{/other_user}", "gists_url": "https://api.github.com/users/jerrygb/gists{/gist_id}", "starred_url": "https://api.github.com/users/jerrygb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jerrygb/subscriptions", "organizations_url": "https://api.github.com/users/jerrygb/orgs", "repos_url": "https://api.github.com/users/jerrygb/repos", "events_url": "https://api.github.com/users/jerrygb/events{/privacy}", "received_events_url": "https://api.github.com/users/jerrygb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2020-01-17T15:09:14Z", "updated_at": "2020-01-20T16:29:10Z", "closed_at": "2020-01-20T16:29:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [e.g. 2.7, 3.6] 3.6\r\n - Spark version [e.g. 2.1, 2.3.1] 2.4.4\r\n - TensorFlow version [e.g. 1.5, 1.9.0] 1.14\r\n - TensorFlowOnSpark version [e.g. 1.1, 1.3.2] master\r\n - Cluster version [e.g. Standalone, Hadoop 2.8, CDH5] Hadoop 2.8.5\r\n\r\nI am running the hadoop/spark installation on AWS EMR at the moment.\r\n\r\n**Describe the bug:**\r\n\r\nI am trying to run mnist example and I having an issue when performing the data prep, using the tensorflow_datasets package. In my code, `mnist_data_setup.py` loads the data to HDFS as opposed to local file system as seen below,\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\nmnist, info = tfds.load('mnist', with_info=True, data_dir='hdfs://default/user/hadoop/tensorflow_datas')\r\n```\r\n\r\nPerhaps the exception (shown below) is not pertaining to TensorflowOnSpark directly, but I wanted to see @leewyang can provide some advise/assistance here. Appreciate your time.\r\n\r\n**Logs:**\r\n\r\nI am receiving the following when running the spark application.\r\n```\r\nloadFileSystems error:\r\n(unable to get stack trace for java.lang.NoClassDefFoundError exception: ExceptionUtils::getStackTrace error.)\r\nhdfsBuilderConnect(forceNewInstance=0, nn=default, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:\r\n```\r\n\r\n**Spark Submit Command Line:**\r\n\r\nI have tried various variations, including providing `LD_LIBRARY_PATH` to the executor env. \r\n\r\n```\r\n${SPARK_HOME}/bin/spark-submit  --deploy-mode cluster \\\r\n--queue default --num-executors 4 \\\r\n--conf spark.executorEnv.CLASSPATH=$(hadoop classpath --glob) \\\r\n--executor-memory 4G --archives mnist/mnist.zip#mnist \\\r\n--jars hdfs:///user/${USER}/tensorflow-hadoop-1.10.0.jar,hdfs:///user/${USER}//spark-tensorflow-connector_2.11-1.10.0.jar \\\r\nTensorFlowOnSpark/examples/mnist/mnist_data_setup.py \\\r\n--output cluster --format tfr\r\n```\r\n\r\nI have performed the `hadoop classpath --glob` and verified that the full list of jars are present on both master and slave nodes.\r\n\r\n\r\nWeird part is that when running the same python snippet on pyspark shell (after setting up `CLASSPATH`), it seems run perfectly fine.\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\nmnist, info = tfds.load('mnist', with_info=True, data_dir='hdfs://default/user/hadoop/tensorflow_datas')\r\n```\r\n\r\nIs there a known limitation around the length that can be passed via Spark Submit?\r\n\r\nAdditionally see a related issue [here](https://jira.apache.org/jira/browse/HDFS-4552).", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/486", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/486/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/486/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/486/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/486", "id": 546272325, "node_id": "MDU6SXNzdWU1NDYyNzIzMjU=", "number": 486, "title": "AttributeError: 'AutoProxy[get_queue]' object has no attribute 'put'", "user": {"login": "nonunu", "id": 58648914, "node_id": "MDQ6VXNlcjU4NjQ4OTE0", "avatar_url": "https://avatars1.githubusercontent.com/u/58648914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nonunu", "html_url": "https://github.com/nonunu", "followers_url": "https://api.github.com/users/nonunu/followers", "following_url": "https://api.github.com/users/nonunu/following{/other_user}", "gists_url": "https://api.github.com/users/nonunu/gists{/gist_id}", "starred_url": "https://api.github.com/users/nonunu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nonunu/subscriptions", "organizations_url": "https://api.github.com/users/nonunu/orgs", "repos_url": "https://api.github.com/users/nonunu/repos", "events_url": "https://api.github.com/users/nonunu/events{/privacy}", "received_events_url": "https://api.github.com/users/nonunu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2020-01-07T13:04:04Z", "updated_at": "2020-02-19T17:08:34Z", "closed_at": "2020-02-19T17:08:34Z", "author_association": "NONE", "active_lock_reason": null, "body": "when execute \r\n\r\n```\r\nexport PYTHON_ROOT=./Python\r\nexport LD_LIBRARY_PATH=${PATH}\r\nexport PYSPARK_PYTHON=${PYTHON_ROOT}/bin/python\r\nexport SPARK_YARN_USER_ENV=\"PYSPARK_PYTHON=Python/bin/python\"\r\nexport PATH=${PYTHON_ROOT}/bin/:$PATH\r\nexport QUEUE=queue_search\r\nexport SPARK_HOME=/data/soft/spark2.4\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue ${QUEUE} \\\r\n--num-executors 4 \\\r\n--executor-memory 27G \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/${USER}/Python36Centos6.zip#Python \\\r\nTensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images mnist/csv/train/images \\\r\n--labels mnist/csv/train/labels \\\r\n--mode train \\\r\n--model mnist_model\r\n```\r\n\r\n\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000003/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000003/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/data2/emr/yarn/local/usercache/zhaosl/appcache/application_1567671041846_91289/container_e02_1567671041846_91289_01_000001/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 404, in _train\r\nAttributeError: 'AutoProxy[get_queue]' object has no attribute 'put'\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/485", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/485/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/485/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/485/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/485", "id": 540173632, "node_id": "MDU6SXNzdWU1NDAxNzM2MzI=", "number": 485, "title": "Feeding partition", "user": {"login": "AlexKai1", "id": 32238962, "node_id": "MDQ6VXNlcjMyMjM4OTYy", "avatar_url": "https://avatars2.githubusercontent.com/u/32238962?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AlexKai1", "html_url": "https://github.com/AlexKai1", "followers_url": "https://api.github.com/users/AlexKai1/followers", "following_url": "https://api.github.com/users/AlexKai1/following{/other_user}", "gists_url": "https://api.github.com/users/AlexKai1/gists{/gist_id}", "starred_url": "https://api.github.com/users/AlexKai1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AlexKai1/subscriptions", "organizations_url": "https://api.github.com/users/AlexKai1/orgs", "repos_url": "https://api.github.com/users/AlexKai1/repos", "events_url": "https://api.github.com/users/AlexKai1/events{/privacy}", "received_events_url": "https://api.github.com/users/AlexKai1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-12-19T09:04:39Z", "updated_at": "2019-12-20T16:46:59Z", "closed_at": "2019-12-20T16:46:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6.8]\r\n - Spark version [2.3.3]\r\n - TensorFlow version [1.14.0]\r\n - TensorFlowOnSpark version [1.4.3]\r\n - Cluster version [yarn 2.7]\r\n\r\n**Describe the bug:**\r\nWhen I set a max_steps which step has be triand, It will be **Feeding partition** and has no progress. what should i do??\r\n\r\n**Logs:**\r\n2019-12-19 16:51:12,694 INFO (MainThread-8681) **Skipping training since max_steps has already saved.**\r\n2019-12-19 16:51:12,716 INFO (MainThread-8721) mgr.state='running'\r\n2019-12-19 16:51:12,717 INFO (MainThread-8721) **Feeding partition** <itertools.chain object at 0x7fc2be3a3668> into input queue <multiprocessing.queues.JoinableQueue object at 0x7fc2be3a36a0>\r\n[Stage 1:>                                                         (0 + 4) / 20]\r\n...\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/482", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/482/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/482/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/482/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/482", "id": 532410256, "node_id": "MDU6SXNzdWU1MzI0MTAyNTY=", "number": 482, "title": "mnist_spark.py': [Errno 2] No such file or directory", "user": {"login": "Boes-man", "id": 26314583, "node_id": "MDQ6VXNlcjI2MzE0NTgz", "avatar_url": "https://avatars0.githubusercontent.com/u/26314583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Boes-man", "html_url": "https://github.com/Boes-man", "followers_url": "https://api.github.com/users/Boes-man/followers", "following_url": "https://api.github.com/users/Boes-man/following{/other_user}", "gists_url": "https://api.github.com/users/Boes-man/gists{/gist_id}", "starred_url": "https://api.github.com/users/Boes-man/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Boes-man/subscriptions", "organizations_url": "https://api.github.com/users/Boes-man/orgs", "repos_url": "https://api.github.com/users/Boes-man/repos", "events_url": "https://api.github.com/users/Boes-man/events{/privacy}", "received_events_url": "https://api.github.com/users/Boes-man/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-12-04T02:40:32Z", "updated_at": "2020-02-19T17:08:44Z", "closed_at": "2020-02-19T17:08:44Z", "author_association": "NONE", "active_lock_reason": null, "body": "spark-submit \\\r\n  --verbose \\\r\n  $(pwd)/TensorFlowOnSpark/examples/mnist/mnist_data_setup.py \\\r\n  --output /user/hdfs/jupyter/mnist_kerberos/csv\r\n\r\nErrors with:\r\n\r\npython: can't open file '/TensorFlowOnSpark/examples/mnist/spark/mnist_spark.py': [Errno 2] No such file or directory\r\n\r\nThanks\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/480", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/480/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/480/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/480/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/480", "id": 531483732, "node_id": "MDU6SXNzdWU1MzE0ODM3MzI=", "number": 480, "title": "under input_spark mode, some stage's feeding of rdd can success, however all feedings in last stage all are time out ", "user": {"login": "iterator0139", "id": 31855195, "node_id": "MDQ6VXNlcjMxODU1MTk1", "avatar_url": "https://avatars3.githubusercontent.com/u/31855195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iterator0139", "html_url": "https://github.com/iterator0139", "followers_url": "https://api.github.com/users/iterator0139/followers", "following_url": "https://api.github.com/users/iterator0139/following{/other_user}", "gists_url": "https://api.github.com/users/iterator0139/gists{/gist_id}", "starred_url": "https://api.github.com/users/iterator0139/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iterator0139/subscriptions", "organizations_url": "https://api.github.com/users/iterator0139/orgs", "repos_url": "https://api.github.com/users/iterator0139/repos", "events_url": "https://api.github.com/users/iterator0139/events{/privacy}", "received_events_url": "https://api.github.com/users/iterator0139/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-12-02T19:21:09Z", "updated_at": "2020-01-14T00:43:15Z", "closed_at": "2020-01-14T00:43:14Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [2.4.0]\r\n - TensorFlow version [1.14.0]\r\n - TensorFlowOnSpark version [1.4.4]\r\n - Cluster version [Hadoop 2.8]\r\n\r\n**Describe the bug:**\r\nunder input_spark mode, some stage's feeding of rdd can success, however, all feedings in the last stage all are time out.\r\n\r\nI'm not sure whether it's caused by the epoch of the data feeding and the termination condition. I have tried to fit the epoch to the steps in training, but still didn't work\r\n\r\n![image](https://user-images.githubusercontent.com/31855195/69987981-a4e73d00-157b-11ea-83fb-c469f8b70521.png)\r\n![image](https://user-images.githubusercontent.com/31855195/69988052-d06a2780-157b-11ea-91a2-4f638d1a5b25.png)\r\n \r\n\r\n**Logs:**\r\n```\r\nLogType:stdout\r\nLog Upload Time:Mon Dec 02 19:01:04 +0000 2019\r\nLogLength:26926\r\nLog Contents:\r\n2019-12-02 19:00:13 INFO  CoarseGrainedExecutorBackend:2566 - Started daemon with process name: 19335@student33-x2\r\n2019-12-02 19:00:13 INFO  SignalUtils:54 - Registered signal handler for TERM\r\n2019-12-02 19:00:13 INFO  SignalUtils:54 - Registered signal handler for HUP\r\n2019-12-02 19:00:13 INFO  SignalUtils:54 - Registered signal handler for INT\r\n2019-12-02 19:00:13 INFO  SecurityManager:54 - Changing view acls to: hduser\r\n2019-12-02 19:00:13 INFO  SecurityManager:54 - Changing modify acls to: hduser\r\n2019-12-02 19:00:13 INFO  SecurityManager:54 - Changing view acls groups to: \r\n2019-12-02 19:00:13 INFO  SecurityManager:54 - Changing modify acls groups to: \r\n2019-12-02 19:00:13 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()\r\n2019-12-02 19:00:14 INFO  TransportClientFactory:267 - Successfully created connection to student33-x1/10.42.0.143:39741 after 100 ms (0 ms spent in bootstraps)\r\n2019-12-02 19:00:14 INFO  SecurityManager:54 - Changing view acls to: hduser\r\n2019-12-02 19:00:14 INFO  SecurityManager:54 - Changing modify acls to: hduser\r\n2019-12-02 19:00:14 INFO  SecurityManager:54 - Changing view acls groups to: \r\n2019-12-02 19:00:14 INFO  SecurityManager:54 - Changing modify acls groups to: \r\n2019-12-02 19:00:14 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()\r\n2019-12-02 19:00:14 INFO  TransportClientFactory:267 - Successfully created connection to student33-x1/10.42.0.143:39741 after 1 ms (0 ms spent in bootstraps)\r\n2019-12-02 19:00:14 INFO  DiskBlockManager:54 - Created local directory at /var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/blockmgr-dc5851ac-8f1a-4ad7-8ad7-26380d0d70a7\r\n2019-12-02 19:00:14 INFO  MemoryStore:54 - MemoryStore started with capacity 5.2 GB\r\n2019-12-02 19:00:14 INFO  CoarseGrainedExecutorBackend:54 - Connecting to driver: spark://CoarseGrainedScheduler@student33-x1:39741\r\n2019-12-02 19:00:14 INFO  CoarseGrainedExecutorBackend:54 - Successfully registered with driver\r\n2019-12-02 19:00:14 INFO  Executor:54 - Starting executor ID 5 on host student33-x2\r\n2019-12-02 19:00:14 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43857.\r\n2019-12-02 19:00:14 INFO  NettyBlockTransferService:54 - Server created on student33-x2:43857\r\n2019-12-02 19:00:14 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n2019-12-02 19:00:14 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(5, student33-x2, 43857, None)\r\n2019-12-02 19:00:14 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(5, student33-x2, 43857, None)\r\n2019-12-02 19:00:14 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(5, student33-x2, 43857, None)\r\n2019-12-02 19:00:18 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 5\r\n2019-12-02 19:00:18 INFO  Executor:54 - Running task 5.0 in stage 2.0 (TID 5)\r\n2019-12-02 19:00:18 INFO  TorrentBroadcast:54 - Started reading broadcast variable 6\r\n2019-12-02 19:00:18 INFO  TransportClientFactory:267 - Successfully created connection to gpu11-x2/10.42.2.91:35201 after 1 ms (0 ms spent in bootstraps)\r\n2019-12-02 19:00:18 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.1 KB, free 5.2 GB)\r\n2019-12-02 19:00:18 INFO  TorrentBroadcast:54 - Reading broadcast variable 6 took 212 ms\r\n2019-12-02 19:00:19 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 10.6 KB, free 5.2 GB)\r\n2019-12-02 19:00:19 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/apple/part-r-00005:0+3080437\r\n2019-12-02 19:00:19 INFO  TorrentBroadcast:54 - Started reading broadcast variable 0\r\n2019-12-02 19:00:19 INFO  TransportClientFactory:267 - Successfully created connection to student31-x2/10.42.1.41:39345 after 4 ms (0 ms spent in bootstraps)\r\n2019-12-02 19:00:19 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 21.0 KB, free 5.2 GB)\r\n2019-12-02 19:00:19 INFO  TorrentBroadcast:54 - Reading broadcast variable 0 took 20 ms\r\n2019-12-02 19:00:19 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 323.9 KB, free 5.2 GB)\r\n2019-12-02 19:00:21 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/orange/part-r-00005:0+3080437\r\n2019-12-02 19:00:21 INFO  TorrentBroadcast:54 - Started reading broadcast variable 3\r\n2019-12-02 19:00:21 INFO  TransportClientFactory:267 - Successfully created connection to student33-x1/10.42.0.143:36119 after 17 ms (0 ms spent in bootstraps)\r\n2019-12-02 19:00:21 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 21.0 KB, free 5.2 GB)\r\n2019-12-02 19:00:21 INFO  TorrentBroadcast:54 - Reading broadcast variable 3 took 53 ms\r\n2019-12-02 19:00:21 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 323.9 KB, free 5.2 GB)\r\n2019-12-02 19:00:25 INFO  PythonRunner:54 - Times: total = 4072, boot = 653, init = 3382, finish = 37\r\n2019-12-02 19:00:25 INFO  PythonRunner:54 - Times: total = 3868, boot = 28, init = 3805, finish = 35\r\n2019-12-02 19:00:25 INFO  MemoryStore:54 - Block taskresult_5 stored as bytes in memory (estimated size 5.9 MB, free 5.2 GB)\r\n2019-12-02 19:00:25 INFO  Executor:54 - Finished task 5.0 in stage 2.0 (TID 5). 6193644 bytes result sent via BlockManager)\r\n2019-12-02 19:00:30 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 26\r\n2019-12-02 19:00:30 INFO  Executor:54 - Running task 4.0 in stage 4.0 (TID 26)\r\n2019-12-02 19:00:30 INFO  TorrentBroadcast:54 - Started reading broadcast variable 8\r\n2019-12-02 19:00:30 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KB, free 5.2 GB)\r\n2019-12-02 19:00:30 INFO  TorrentBroadcast:54 - Reading broadcast variable 8 took 28 ms\r\n2019-12-02 19:00:30 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 16.7 KB, free 5.2 GB)\r\n2019-12-02 19:00:33 INFO  PythonRunner:54 - Times: total = 2468, boot = -6265, init = 6327, finish = 2406\r\n2019-12-02 19:00:33 INFO  Executor:54 - Finished task 4.0 in stage 4.0 (TID 26). 1376 bytes result sent to driver\r\n2019-12-02 19:00:33 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 36\r\n2019-12-02 19:00:33 INFO  Executor:54 - Running task 5.0 in stage 5.0 (TID 36)\r\n2019-12-02 19:00:33 INFO  TorrentBroadcast:54 - Started reading broadcast variable 9\r\n2019-12-02 19:00:33 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 10.4 KB, free 5.2 GB)\r\n2019-12-02 19:00:33 INFO  TorrentBroadcast:54 - Reading broadcast variable 9 took 41 ms\r\n2019-12-02 19:00:33 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 20.2 KB, free 5.2 GB)\r\n2019-12-02 19:00:33 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/apple/part-r-00005:0+3080437\r\n2019-12-02 19:00:33 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/orange/part-r-00005:0+3080437\r\n2019-12-02 19:00:36 INFO  PythonRunner:54 - Times: total = 53, boot = -8284, init = 8296, finish = 41\r\n2019-12-02 19:00:36 INFO  PythonRunner:54 - Times: total = 3306, boot = 27, init = 3242, finish = 37\r\n2019-12-02 19:00:41 ERROR Executor:91 - Exception in task 5.0 in stage 5.0 (TID 36)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 372, in main\r\n    process()\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 367, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 417, in _train\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n2019-12-02 19:00:42 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 42\r\n2019-12-02 19:00:42 INFO  Executor:54 - Running task 15.0 in stage 5.0 (TID 42)\r\n2019-12-02 19:00:42 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/apple/part-r-00005:0+3080437\r\n2019-12-02 19:00:42 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/orange/part-r-00005:0+3080437\r\n2019-12-02 19:00:42 INFO  PythonRunner:54 - Times: total = 189, boot = -8521, init = 8595, finish = 115\r\n2019-12-02 19:00:42 INFO  PythonRunner:54 - Times: total = 397, boot = -5306, init = 5391, finish = 312\r\n2019-12-02 19:00:47 ERROR Executor:91 - Exception in task 15.0 in stage 5.0 (TID 42)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 372, in main\r\n    process()\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 367, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 417, in _train\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n2019-12-02 19:00:47 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 50\r\n2019-12-02 19:00:47 INFO  Executor:54 - Running task 25.0 in stage 5.0 (TID 50)\r\n2019-12-02 19:00:47 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/apple/part-r-00005:0+3080437\r\n2019-12-02 19:00:47 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/orange/part-r-00005:0+3080437\r\n2019-12-02 19:00:47 INFO  PythonRunner:54 - Times: total = 96, boot = -5382, init = 5428, finish = 50\r\n2019-12-02 19:00:47 INFO  PythonRunner:54 - Times: total = 142, boot = -5219, init = 5228, finish = 133\r\n2019-12-02 19:00:53 ERROR Executor:91 - Exception in task 25.0 in stage 5.0 (TID 50)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 372, in main\r\n    process()\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 367, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 417, in _train\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n2019-12-02 19:00:53 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 57\r\n2019-12-02 19:00:53 INFO  Executor:54 - Running task 5.2 in stage 5.0 (TID 57)\r\n2019-12-02 19:00:53 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/apple/part-r-00005:0+3080437\r\n2019-12-02 19:00:53 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/orange/part-r-00005:0+3080437\r\n2019-12-02 19:00:53 INFO  PythonRunner:54 - Times: total = 114, boot = -5301, init = 5362, finish = 53\r\n2019-12-02 19:00:53 INFO  PythonRunner:54 - Times: total = 140, boot = -5269, init = 5289, finish = 120\r\n2019-12-02 19:00:58 ERROR Executor:91 - Exception in task 5.2 in stage 5.0 (TID 57)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 372, in main\r\n    process()\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000006/pyspark.zip/pyspark/worker.py\", line 367, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0178/container_1573746617850_0178_01_000001/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 417, in _train\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n2019-12-02 19:00:58 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 64\r\n2019-12-02 19:00:58 INFO  Executor:54 - Running task 15.2 in stage 5.0 (TID 64)\r\n2019-12-02 19:00:58 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/apple/part-r-00005:0+3080437\r\n2019-12-02 19:00:58 INFO  NewHadoopRDD:54 - Input split: hdfs://gpu11:9000/user/hduser/apple2orange-new/tfrecord/orange/part-r-00005:0+3080437\r\n2019-12-02 19:00:58 INFO  PythonRunner:54 - Times: total = 88, boot = -5215, init = 5233, finish = 70\r\n2019-12-02 19:00:58 INFO  PythonRunner:54 - Times: total = 182, boot = -5237, init = 5259, finish = 160\r\n2019-12-02 19:01:03 INFO  Executor:54 - Executor is trying to kill task 15.2 in stage 5.0 (TID 64), reason: Stage cancelled\r\n2019-12-02 19:01:03 INFO  CoarseGrainedExecutorBackend:54 - Driver commanded a shutdown\r\n2019-12-02 19:01:03 INFO  Executor:54 - Executor killed task 15.2 in stage 5.0 (TID 64), reason: Stage cancelled\r\n2019-12-02 19:01:03 INFO  MemoryStore:54 - MemoryStore cleared\r\n2019-12-02 19:01:03 INFO  BlockManager:54 - BlockManager stopped\r\n2019-12-02 19:01:03 INFO  ShutdownHookManager:54 - Shutdown hook called\r\nEnd of LogType:stdout\r\n```\r\n\r\n\r\n**Spark Submit Command Line:**\r\n```\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue default \\\r\n--num-executors 8 \\\r\n--executor-memory 10G \\\r\n--py-files TensorFlowOnSpark/tensorflowonspark/tfspark.zip,singnode/discriminator.py,singnode/generator.py,singnode/model.py,singnode/ops.py,singnode/utils.py,singnode/export_graph.py,singnode/train.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/${USER}/Python.zip#Python \\\r\n--jars hdfs:///user/${USER}/tensorflow-hadoop-1.10.0.jar \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nsingnode/train_spark.py \\\r\n--imagesX apple2orange-new/tfrecord/apple \\\r\n--imagesY apple2orange-new/tfrecord/orange \r\n```\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/479", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/479/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/479/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/479/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/479", "id": 530571464, "node_id": "MDU6SXNzdWU1MzA1NzE0NjQ=", "number": 479, "title": "report pickle error", "user": {"login": "iterator0139", "id": 31855195, "node_id": "MDQ6VXNlcjMxODU1MTk1", "avatar_url": "https://avatars3.githubusercontent.com/u/31855195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iterator0139", "html_url": "https://github.com/iterator0139", "followers_url": "https://api.github.com/users/iterator0139/followers", "following_url": "https://api.github.com/users/iterator0139/following{/other_user}", "gists_url": "https://api.github.com/users/iterator0139/gists{/gist_id}", "starred_url": "https://api.github.com/users/iterator0139/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iterator0139/subscriptions", "organizations_url": "https://api.github.com/users/iterator0139/orgs", "repos_url": "https://api.github.com/users/iterator0139/repos", "events_url": "https://api.github.com/users/iterator0139/events{/privacy}", "received_events_url": "https://api.github.com/users/iterator0139/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-11-30T12:11:40Z", "updated_at": "2019-12-02T21:02:05Z", "closed_at": "2019-12-02T21:02:05Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [2.7]\r\n - Spark version [2.4.0]\r\n - TensorFlow version [1.14.0]\r\n - TensorFlowOnSpark version [1.4.4]\r\n - Cluster version [Hadoop 2.8]\r\n\r\nWhen I run a TensorFlow application it just gave such error, don't know where has some wrong with the code.\r\n\r\n**Logs:**\r\n```\r\n2019-11-30 12:04:26 INFO  DAGScheduler:54 - Job 1 finished: take at SerDeUtil.scala:239, took 1.349455 s\r\n('args:', Namespace(cluster_size=4, epochs=1, rdma=False, readers=1, tensorboard=False))\r\n2019-11-30 12:04:26,465 INFO (MainThread-5927) Reserving TFSparkNodes \r\n2019-11-30 12:04:26,465 INFO (MainThread-5927) cluster_template: {'ps': [0], 'worker': [1, 2, 3]}\r\n2019-11-30 12:04:26,467 INFO (MainThread-5927) listening for reservations at ('10.42.0.143', 34117)\r\n2019-11-30 12:04:26,467 INFO (MainThread-5927) Starting TensorFlow on executors\r\n2019-11-30 12:04:26,471 INFO (MainThread-5927) Waiting for TFSparkNodes to start\r\n2019-11-30 12:04:26,471 INFO (MainThread-5927) waiting for 4 reservations\r\nTraceback (most recent call last):\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/serializers.py\", line 587, in dumps\r\n    return cloudpickle.dumps(obj, 2)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 863, in dumps\r\n    cp.dump(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 260, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 224, in dump\r\n    self.save(obj)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 568, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\r\n    self._batch_appends(iter(obj))\r\n  File \"/usr/lib/python2.7/pickle.py\", line 639, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\r\n    self._batch_appends(iter(obj))\r\n  File \"/usr/lib/python2.7/pickle.py\", line 639, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\r\n    self._batch_appends(iter(obj))\r\n  File \"/usr/lib/python2.7/pickle.py\", line 639, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\r\n    self._batch_appends(iter(obj))\r\n  File \"/usr/lib/python2.7/pickle.py\", line 642, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\r\n    self._batch_appends(iter(obj))\r\n  File \"/usr/lib/python2.7/pickle.py\", line 642, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 606, in save_list\r\n    self._batch_appends(iter(obj))\r\n  File \"/usr/lib/python2.7/pickle.py\", line 639, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 400, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/container_1573746617850_0119_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 286, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python2.7/pickle.py\", line 655, in save_dict\r\n    self._batch_setitems(obj.iteritems())\r\n  File \"/usr/lib/python2.7/pickle.py\", line 687, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python2.7/pickle.py\", line 306, in save\r\n    rv = reduce(self.proto)\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/flags/_flagvalues.py\", line 640, in __getstate__\r\n    raise TypeError(\"can't pickle FlagValues\")\r\nTypeError: can't pickle FlagValues\r\n2019-11-30 12:04:26,491 ERROR (Thread-3-5927) Exception in TF background thread\r\n2019-11-30 12:04:27,473 INFO (MainThread-5927) waiting for 4 reservations\r\n2019-11-30 12:04:27 INFO  AbstractConnector:318 - Stopped Spark@3d21963{HTTP/1.1,[http/1.1]}{0.0.0.0:0}\r\n2019-11-30 12:04:27 INFO  SparkUI:54 - Stopped Spark web UI at http://student33-x1:36745\r\n2019-11-30 12:04:27 INFO  YarnAllocator:54 - Driver requested a total number of 0 executor(s).\r\n2019-11-30 12:04:27 INFO  YarnClusterSchedulerBackend:54 - Shutting down all executors\r\n2019-11-30 12:04:27 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down\r\n2019-11-30 12:04:27 INFO  SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices\r\n(serviceOption=None,\r\n services=List(),\r\n started=false)\r\n2019-11-30 12:04:27 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\r\n2019-11-30 12:04:27 INFO  MemoryStore:54 - MemoryStore cleared\r\n2019-11-30 12:04:27 INFO  BlockManager:54 - BlockManager stopped\r\n2019-11-30 12:04:27 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\r\n2019-11-30 12:04:27 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\r\n2019-11-30 12:04:27 INFO  SparkContext:54 - Successfully stopped SparkContext\r\n2019-11-30 12:04:27 ERROR ApplicationMaster:70 - User application exited with status 1\r\n2019-11-30 12:04:27 INFO  ApplicationMaster:54 - Final app status: FAILED, exitCode: 1, (reason: User application exited with status 1)\r\n2019-11-30 12:04:27 INFO  ApplicationMaster:54 - Unregistering ApplicationMaster with FAILED (diag message: User application exited with status 1)\r\n2019-11-30 12:04:27 INFO  AMRMClientImpl:382 - Waiting for application to be successfully unregistered.\r\n2019-11-30 12:04:27 INFO  ApplicationMaster:54 - Deleting staging directory hdfs://gpu11:9000/user/hduser/.sparkStaging/application_1573746617850_0119\r\n2019-11-30 12:04:27 INFO  ShutdownHookManager:54 - Shutdown hook called\r\n2019-11-30 12:04:27 INFO  ShutdownHookManager:54 - Deleting directory /var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/spark-f1361f4b-1a19-4082-8093-180453ce5e49/pyspark-44418498-c590-43a7-8da5-0263154d6ca3\r\n2019-11-30 12:04:27 INFO  ShutdownHookManager:54 - Deleting directory /var/hadoop/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1573746617850_0119/spark-f1361f4b-1a19-4082-8093-180453ce5e49\r\nEnd of LogType:stdout\r\n```\r\n\r\n\r\n**Spark Submit Command Line:**\r\n```\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue default \\\r\n--num-executors 4 \\\r\n--executor-memory 4G \\\r\n--py-files TensorFlowOnSpark/tensorflowonspark/tfspark.zip,singnode/discriminator.py,singnode/generator.py,singnode/model.py,singnode/ops.py,singnode/utils.py,singnode/export_graph.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/${USER}/Python.zip#Python \\\r\n--jars hdfs:///user/${USER}/tensorflow-hadoop-1.10.0.jar \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\n--conf spark.executorEnv.CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath --glob) \\\r\nsingnode/train.py\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/478", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/478/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/478/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/478/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/478", "id": 529741246, "node_id": "MDU6SXNzdWU1Mjk3NDEyNDY=", "number": 478, "title": "stuck in waiting for reservations while run mnist example", "user": {"login": "siyu1992", "id": 45645303, "node_id": "MDQ6VXNlcjQ1NjQ1MzAz", "avatar_url": "https://avatars3.githubusercontent.com/u/45645303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/siyu1992", "html_url": "https://github.com/siyu1992", "followers_url": "https://api.github.com/users/siyu1992/followers", "following_url": "https://api.github.com/users/siyu1992/following{/other_user}", "gists_url": "https://api.github.com/users/siyu1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/siyu1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/siyu1992/subscriptions", "organizations_url": "https://api.github.com/users/siyu1992/orgs", "repos_url": "https://api.github.com/users/siyu1992/repos", "events_url": "https://api.github.com/users/siyu1992/events{/privacy}", "received_events_url": "https://api.github.com/users/siyu1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2019-11-28T07:33:52Z", "updated_at": "2020-01-08T03:12:42Z", "closed_at": "2020-01-08T03:12:42Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [2.3.2]\r\n - TensorFlow version [1.14.0]\r\n - TensorFlowOnSpark version [2.0.0]\r\n - Cluster version [Client, Hadoop 3.1]\r\n\r\n**Describe the bug:**\r\nWhen I run mnist example, it stuck in waiting for reservations , I'm using client mode , I'm trying to debug TFCluster.py , and I find num_master=0 , but spark seems fine when I run other tasks , am I missing something? ty\r\n\r\n**Logs:**\r\n2019-11-28 07:08:11,108 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n2019-11-28 07:08:13,536 INFO spark.SparkContext: Running Spark version 2.3.2.3.1.0.0-78\r\n2019-11-28 07:08:13,554 INFO spark.SparkContext: Submitted application: mnist_spark\r\n2019-11-28 07:08:13,639 INFO spark.SecurityManager: Changing view acls to: root,hdfs\r\n2019-11-28 07:08:13,640 INFO spark.SecurityManager: Changing modify acls to: root,hdfs\r\n2019-11-28 07:08:13,640 INFO spark.SecurityManager: Changing view acls groups to: \r\n2019-11-28 07:08:13,640 INFO spark.SecurityManager: Changing modify acls groups to: \r\n2019-11-28 07:08:13,640 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, hdfs); groups with view permissions: Set(); users  with modify permissions: Set(root, hdfs); groups with modify permissions: Set()\r\n2019-11-28 07:08:13,862 INFO util.Utils: Successfully started service 'sparkDriver' on port 40001.\r\n2019-11-28 07:08:13,883 INFO spark.SparkEnv: Registering MapOutputTracker\r\n2019-11-28 07:08:13,899 INFO spark.SparkEnv: Registering BlockManagerMaster\r\n2019-11-28 07:08:13,902 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n2019-11-28 07:08:13,902 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n2019-11-28 07:08:13,912 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-58cd004a-b51c-431a-a9d7-1fb36dfa67b5\r\n2019-11-28 07:08:13,929 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\r\n2019-11-28 07:08:13,978 INFO spark.SparkEnv: Registering OutputCommitCoordinator\r\n2019-11-28 07:08:14,059 INFO util.log: Logging initialized @4070ms\r\n2019-11-28 07:08:14,118 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n2019-11-28 07:08:14,138 INFO server.Server: Started @4151ms\r\n2019-11-28 07:08:14,158 INFO server.AbstractConnector: Started ServerConnector@6671b572{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n2019-11-28 07:08:14,158 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\r\n2019-11-28 07:08:14,179 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16b5866e{/jobs,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,180 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ec3beec{/jobs/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,180 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@506c8bd5{/jobs/job,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,182 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65080553{/jobs/job/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,183 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6dff3c0{/stages,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,183 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3259e2ed{/stages/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,184 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61a5ffff{/stages/stage,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,185 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40a92566{/stages/stage/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,186 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ebd708b{/stages/pool,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,187 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5829573d{/stages/pool/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,187 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@326c95fc{/storage,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,188 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3be704c2{/storage/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,189 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e3b121d{/storage/rdd,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,189 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c392658{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,190 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1aac9956{/environment,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,191 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@196e4fd5{/environment/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,191 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fc31d1f{/executors,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,192 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a4df359{/executors/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,193 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6455af6f{/executors/threadDump,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,193 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f2e44d7{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,199 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8029bc8{/static,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,200 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fc35766{/,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,201 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@500351d8{/api,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,202 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34a4958e{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,202 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@520a2838{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:14,204 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.16.53.47:4040\r\n2019-11-28 07:08:14,948 INFO client.RMProxy: Connecting to ResourceManager at /172.16.53.47:8050\r\n2019-11-28 07:08:15,152 INFO yarn.Client: Requesting a new application from cluster with 3 NodeManagers\r\n2019-11-28 07:08:15,201 INFO conf.Configuration: resource-types.xml not found\r\n2019-11-28 07:08:15,202 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n2019-11-28 07:08:15,217 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2048 MB per container)\r\n2019-11-28 07:08:15,217 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\r\n2019-11-28 07:08:15,218 INFO yarn.Client: Setting up container launch context for our AM\r\n2019-11-28 07:08:15,221 INFO yarn.Client: Setting up the launch environment for our AM container\r\n2019-11-28 07:08:15,227 INFO yarn.Client: Preparing resources for our AM container\r\n2019-11-28 07:08:16,454 INFO yarn.Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz\r\n2019-11-28 07:08:16,457 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-yarn-archive.tar.gz\r\n2019-11-28 07:08:16,515 INFO yarn.Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz\r\n2019-11-28 07:08:16,516 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://172.16.53.47:8020/hdp/apps/3.1.0.0-78/spark2/spark2-hdp-hive-archive.tar.gz\r\n2019-11-28 07:08:16,525 INFO yarn.Client: Uploading resource file:/usr/hdp/3.1.0.0-78/spark2/python/lib/pyspark.zip -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1569326398662_0445/pyspark.zip\r\n2019-11-28 07:08:16,741 INFO yarn.Client: Uploading resource file:/usr/hdp/3.1.0.0-78/spark2/python/lib/py4j-0.10.7-src.zip -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1569326398662_0445/py4j-0.10.7-src.zip\r\n2019-11-28 07:08:16,766 INFO yarn.Client: Uploading resource file:/root/test_csy/TensorFlowOnSpark/tfspark.zip -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1569326398662_0445/tfspark.zip\r\n2019-11-28 07:08:16,786 INFO yarn.Client: Uploading resource file:/root/test_csy/TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1569326398662_0445/mnist_dist.py\r\n2019-11-28 07:08:16,941 INFO yarn.Client: Uploading resource file:/tmp/spark-9891a60f-40cd-4bb4-baa2-6782ccea81c0/__spark_conf__35748400337901907.zip -> hdfs://172.16.53.47:8020/user/hdfs/.sparkStaging/application_1569326398662_0445/__spark_conf__.zip\r\n2019-11-28 07:08:16,981 INFO spark.SecurityManager: Changing view acls to: root,hdfs\r\n2019-11-28 07:08:16,981 INFO spark.SecurityManager: Changing modify acls to: root,hdfs\r\n2019-11-28 07:08:16,981 INFO spark.SecurityManager: Changing view acls groups to: \r\n2019-11-28 07:08:16,981 INFO spark.SecurityManager: Changing modify acls groups to: \r\n2019-11-28 07:08:16,981 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, hdfs); groups with view permissions: Set(); users  with modify permissions: Set(root, hdfs); groups with modify permissions: Set()\r\n2019-11-28 07:08:17,007 INFO yarn.Client: Submitting application application_1569326398662_0445 to ResourceManager\r\n2019-11-28 07:08:17,243 INFO impl.YarnClientImpl: Submitted application application_1569326398662_0445\r\n2019-11-28 07:08:17,246 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1569326398662_0445 and attemptId None\r\n2019-11-28 07:08:18,251 INFO yarn.Client: Application report for application_1569326398662_0445 (state: ACCEPTED)\r\n2019-11-28 07:08:18,253 INFO yarn.Client: \r\n         client token: N/A\r\n         diagnostics: AM container is launched, waiting for AM container to Register with RM\r\n         ApplicationMaster host: N/A\r\n         ApplicationMaster RPC port: -1\r\n         queue: default\r\n         start time: 1574924897020\r\n         final status: UNDEFINED\r\n         tracking URL: http://hdfs.master1:8088/proxy/application_1569326398662_0445/\r\n         user: hdfs\r\n2019-11-28 07:08:19,255 INFO yarn.Client: Application report for application_1569326398662_0445 (state: ACCEPTED)\r\n2019-11-28 07:08:19,731 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> hdfs.master1, PROXY_URI_BASES -> http://hdfs.master1:8088/proxy/application_1569326398662_0445), /proxy/application_1569326398662_0445\r\n2019-11-28 07:08:19,732 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\r\n2019-11-28 07:08:20,178 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\r\n2019-11-28 07:08:20,257 INFO yarn.Client: Application report for application_1569326398662_0445 (state: RUNNING)\r\n2019-11-28 07:08:20,257 INFO yarn.Client: \r\n         client token: N/A\r\n         diagnostics: N/A\r\n         ApplicationMaster host: 172.16.53.51\r\n         ApplicationMaster RPC port: 0\r\n         queue: default\r\n         start time: 1574924897020\r\n         final status: UNDEFINED\r\n         tracking URL: http://hdfs.master1:8088/proxy/application_1569326398662_0445/\r\n         user: hdfs\r\n2019-11-28 07:08:20,258 INFO cluster.YarnClientSchedulerBackend: Application application_1569326398662_0445 has started running.\r\n2019-11-28 07:08:20,265 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40002.\r\n2019-11-28 07:08:20,266 INFO netty.NettyBlockTransferService: Server created on 172.16.53.47:40002\r\n2019-11-28 07:08:20,267 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n2019-11-28 07:08:20,295 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.16.53.47, 40002, None)\r\n2019-11-28 07:08:20,298 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.16.53.47:40002 with 366.3 MB RAM, BlockManagerId(driver, 172.16.53.47, 40002, None)\r\n2019-11-28 07:08:20,302 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.16.53.47, 40002, None)\r\n2019-11-28 07:08:20,302 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.16.53.47, 40002, None)\r\n2019-11-28 07:08:20,458 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\r\n2019-11-28 07:08:20,459 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b622c1c{/metrics/json,null,AVAILABLE,@Spark}\r\n2019-11-28 07:08:22,721 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.53.51:48356) with ID 1\r\n2019-11-28 07:08:22,790 INFO storage.BlockManagerMasterEndpoint: Registering block manager hdfs.slave2:40002 with 366.3 MB RAM, BlockManagerId(1, hdfs.slave2, 40002, None)\r\n2019-11-28 07:08:24,107 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.53.50:58558) with ID 3\r\n2019-11-28 07:08:24,255 INFO storage.BlockManagerMasterEndpoint: Registering block manager hdfs.slave1:40002 with 366.3 MB RAM, BlockManagerId(3, hdfs.slave1, 40002, None)\r\n2019-11-28 07:08:24,369 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.53.50:58560) with ID 2\r\n2019-11-28 07:08:24,377 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\r\n2019-11-28 07:08:24,623 INFO storage.BlockManagerMasterEndpoint: Registering block manager hdfs.slave1:40003 with 366.3 MB RAM, BlockManagerId(2, hdfs.slave1, 40003, None)\r\nargs: Namespace(batch_size=5, cluster_size=3, epochs=1, format='csv', images='/examples/mnist/csv/train/images', labels='/examples/mnist/csv/train/labels', mode='inference', model='mnist_model', output='predictions', rdma=False, readers=1, steps=100, tensorboard=False)\r\n2019-11-28T15:08:24.656271 ===== Start\r\n2019-11-28 07:08:24,975 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 288.0 KB, free 366.0 MB)\r\n2019-11-28 07:08:25,021 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.7 KB, free 366.0 MB)\r\n2019-11-28 07:08:25,023 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.16.53.47:40002 (size: 26.7 KB, free: 366.3 MB)\r\n2019-11-28 07:08:25,026 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n2019-11-28 07:08:25,084 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 288.0 KB, free 365.7 MB)\r\n2019-11-28 07:08:25,097 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 26.7 KB, free 365.7 MB)\r\n2019-11-28 07:08:25,098 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.16.53.47:40002 (size: 26.7 KB, free: 366.2 MB)\r\n2019-11-28 07:08:25,099 INFO spark.SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\r\nzipping images and labels\r\n2019-11-28 07:08:25,153 INFO mapred.FileInputFormat: Total input files to process : 10\r\n2019-11-28 07:08:25,175 INFO mapred.FileInputFormat: Total input files to process : 10\r\n2019-11-28 15:08:25,224 INFO (MainThread-16087) Reserving TFSparkNodes \r\n2019-11-28 15:08:25,224 INFO (MainThread-16087) cluster_template: {'ps': [0], 'worker': [1, 2]}\r\n2019-11-28 15:08:25,228 INFO (MainThread-16087) listening for reservations at ('172.17.0.2', 34077)\r\n2019-11-28 15:08:25,231 INFO (MainThread-16087) Starting TensorFlow on executors\r\n2019-11-28 15:08:25,240 INFO (MainThread-16087) Waiting for TFSparkNodes to start\r\n2019-11-28 15:08:25,241 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 07:08:25,314 INFO spark.SparkContext: Starting job: foreachPartition at /root/test_csy/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py:330\r\n2019-11-28 07:08:25,330 INFO scheduler.DAGScheduler: Got job 0 (foreachPartition at /root/test_csy/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py:330) with 3 output partitions\r\n2019-11-28 07:08:25,331 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /root/test_csy/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py:330)\r\n2019-11-28 07:08:25,331 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n2019-11-28 07:08:25,332 INFO scheduler.DAGScheduler: Missing parents: List()\r\n2019-11-28 07:08:25,339 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[8] at foreachPartition at /root/test_csy/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py:330), which has no missing parents\r\n2019-11-28 07:08:25,356 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.4 KB, free 365.7 MB)\r\n2019-11-28 07:08:25,366 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.1 KB, free 365.7 MB)\r\n2019-11-28 07:08:25,367 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.16.53.47:40002 (size: 11.1 KB, free: 366.2 MB)\r\n2019-11-28 07:08:25,370 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039\r\n2019-11-28 07:08:25,387 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (PythonRDD[8] at foreachPartition at /root/test_csy/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py:330) (first 15 tasks are for partitions Vector(0, 1, 2))\r\n2019-11-28 07:08:25,389 INFO cluster.YarnScheduler: Adding task set 0.0 with 3 tasks\r\n2019-11-28 15:08:26,242 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:27,242 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:28,243 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:29,244 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:30,245 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:31,246 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:32,247 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:33,248 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:34,249 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:35,250 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:36,251 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:37,252 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:38,253 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:39,255 INFO (MainThread-16087) waiting for 3 reservations\r\n2019-11-28 15:08:40,256 INFO (MainThread-16087) waiting for 3 reservations\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode client \\\r\n--queue default \\\r\n--num-executors 3 \\\r\n--executor-memory 1G \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.cores.max=4 \\\r\n--conf spark.task.cpus=3 \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nTensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images /examples/mnist/csv/train/images \\\r\n--labels /examples/mnist/csv/train/labels \\\r\n--mode train \\\r\n--model mnist_model \\\r\n--steps 100 \\\r\n--batch_size 5 \\\r\n--mode inference\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/477", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/477/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/477/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/477/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/477", "id": 527910128, "node_id": "MDU6SXNzdWU1Mjc5MTAxMjg=", "number": 477, "title": "error when using Embedding", "user": {"login": "xiaomajia700", "id": 23690437, "node_id": "MDQ6VXNlcjIzNjkwNDM3", "avatar_url": "https://avatars3.githubusercontent.com/u/23690437?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiaomajia700", "html_url": "https://github.com/xiaomajia700", "followers_url": "https://api.github.com/users/xiaomajia700/followers", "following_url": "https://api.github.com/users/xiaomajia700/following{/other_user}", "gists_url": "https://api.github.com/users/xiaomajia700/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiaomajia700/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiaomajia700/subscriptions", "organizations_url": "https://api.github.com/users/xiaomajia700/orgs", "repos_url": "https://api.github.com/users/xiaomajia700/repos", "events_url": "https://api.github.com/users/xiaomajia700/events{/privacy}", "received_events_url": "https://api.github.com/users/xiaomajia700/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-11-25T07:48:30Z", "updated_at": "2020-01-14T00:44:15Z", "closed_at": "2020-01-14T00:44:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.7\r\n - Spark version 2.4.0-cdh6.1.0\r\n - TensorFlow version 2.0.0\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version CDH6.1.0\r\n\r\n**Describe the bug:**\r\nI got an error when using tf.keras.layers.Embedding, remove the embedding layer it works fine. What is the right way to use embedding layer?\r\n\r\n**Code:**\r\n\r\n    from pyspark.sql import SparkSession\r\n\r\n    import argparse\r\n    import json\r\n\r\n\r\n    BATCH_SIZE = 10\r\n    LEARNING_RATE = 0.01\r\n    EMBEDDING_DIM = 32\r\n\r\n\r\n    def main_fun(args, ctx):\r\n        import numpy as np\r\n        import tensorflow as tf\r\n        from tensorflowonspark import TFNode\r\n\r\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n        \r\n        def build_and_compile_model():\r\n            col = \"v_type\"\r\n            input_layer = tf.keras.layers.Input(shape=(2, 1), name=col, dtype=tf.int32)\r\n            layer = input_layer\r\n            layer = tf.keras.layers.Embedding(2, EMBEDDING_DIM, name=\"embed_\" + col)(layer)\r\n            layer = tf.keras.layers.Flatten(name=\"flat_\" + col)(layer)\r\n            score = tf.keras.layers.Dense(1, activation='relu')(layer)\r\n            model = tf.keras.Model(inputs=input_layer, outputs=score)\r\n            model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MSE)\r\n            return model\r\n\r\n        tf_feed = TFNode.DataFeed(ctx.mgr, False)\r\n        \r\n        def rdd_generator():\r\n            while not tf_feed.should_stop():\r\n                batch = tf_feed.next_batch(1)\r\n                if len(batch) > 0:\r\n                    inputs = np.array([1, 1]).reshape((2, 1))\r\n                    outputs = np.array([1.0]).reshape((1, ))\r\n                    yield (inputs, outputs)\r\n                else:\r\n                    return\r\n\r\n        output_types = (tf.int32, tf.float32)\r\n        output_shapes = (tf.TensorShape([2, 1]), tf.TensorShape([1]))\r\n\r\n        ds = tf.data.Dataset.from_generator(rdd_generator, output_types, output_shapes)\r\n        ds = ds.batch(args.batch_size)\r\n\r\n        with strategy.scope():\r\n            multi_worker_model = build_and_compile_model()\r\n\r\n        multi_worker_model.summary()\r\n\r\n        steps_per_epoch = args.num_records / args.batch_size\r\n        steps_per_epoch_per_worker = steps_per_epoch / ctx.num_workers\r\n        max_steps_per_worker = int(steps_per_epoch_per_worker * 0.9)\r\n\r\n        multi_worker_model.fit(x=ds, epochs=args.epochs, steps_per_epoch=max_steps_per_worker)\r\n\r\n        # terminating feed tells spark to skip processing further partitions\r\n        tf_feed.terminate()\r\n\r\n\r\n    if __name__ == '__main__':\r\n        from tensorflowonspark.pipeline import TFEstimator, TFModel\r\n\r\n        spark = SparkSession.builder.enableHiveSupport().getOrCreate()\r\n        sc = spark.sparkContext\r\n\r\n        executors = sc._conf.get(\"spark.executor.instances\")\r\n        num_executors = int(executors) if executors is not None else 1\r\n\r\n        parser = argparse.ArgumentParser()\r\n        parser.add_argument(\"--batch_size\", help=\"number of records per batch\", type=int, default=BATCH_SIZE)\r\n        parser.add_argument(\"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=num_executors)\r\n        parser.add_argument(\"--epochs\", help=\"number of epochs\", type=int, default=3)\r\n        parser.add_argument(\"--model_dir\", help=\"(ignore) path to save checkpoint\", default=\"tmp_model\")\r\n        parser.add_argument(\"--export_dir\", help=\"(ignore) path to export saved_model\", default=\"tmp_export\")\r\n\r\n        args = parser.parse_args()\r\n        print(\"args:\", args)\r\n\r\n        args.num_records = 100\r\n        df = spark.createDataFrame([(x,) for x in range(args.num_records)], [\"a\"])\r\n\r\n        estimator = TFEstimator(main_fun, args) \\\r\n                .setInputMapping({\"a\": \"a\"}) \\\r\n                .setModelDir(args.model_dir) \\\r\n                .setExportDir(args.export_dir) \\\r\n                .setClusterSize(args.cluster_size) \\\r\n                .setTensorboard(False) \\\r\n                .setEpochs(args.epochs) \\\r\n                .setBatchSize(args.batch_size) \\\r\n                .setGraceSecs(60)\r\n        model = estimator.fit(df)\r\n\r\n**Spark Logs:**\r\n\r\n    19/11/25 10:35:41 INFO spark.SparkContext: Running Spark version 2.4.0-cdh6.1.0\r\n    19/11/25 10:35:41 INFO spark.SparkContext: Submitted application: spark_test.py\r\n    19/11/25 10:35:41 INFO spark.SecurityManager: Changing view acls to: gzitv\r\n    19/11/25 10:35:41 INFO spark.SecurityManager: Changing modify acls to: gzitv\r\n    19/11/25 10:35:41 INFO spark.SecurityManager: Changing view acls groups to: \r\n    19/11/25 10:35:41 INFO spark.SecurityManager: Changing modify acls groups to: \r\n    19/11/25 10:35:41 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gzitv); groups with view permissions: Set(); users  with modify permissions: Set(gzitv); groups with modify permissions: Set()\r\n    19/11/25 10:35:42 INFO util.Utils: Successfully started service 'sparkDriver' on port 60248.\r\n    19/11/25 10:35:42 INFO spark.SparkEnv: Registering MapOutputTracker\r\n    19/11/25 10:35:42 INFO spark.SparkEnv: Registering BlockManagerMaster\r\n    19/11/25 10:35:42 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n    19/11/25 10:35:42 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n    19/11/25 10:35:42 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-a370d02a-9918-4df2-8793-02e6a8fcf028\r\n    19/11/25 10:35:42 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\r\n    19/11/25 10:35:42 INFO spark.SparkEnv: Registering OutputCommitCoordinator\r\n    19/11/25 10:35:42 INFO util.log: Logging initialized @7105ms\r\n    19/11/25 10:35:42 INFO server.Server: jetty-9.3.z-SNAPSHOT\r\n    19/11/25 10:35:42 INFO server.Server: Started @7222ms\r\n    19/11/25 10:35:42 INFO server.AbstractConnector: Started ServerConnector@49d5937e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n    19/11/25 10:35:42 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7309813{/jobs,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e4e37bf{/jobs/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4fb72cd{/jobs/job,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bbe820d{/jobs/job/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f2bd0a{/stages,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b40de43{/stages/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4108816d{/stages/stage,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62ad58ec{/stages/stage/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@672bbe3d{/stages/pool,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ef83f6{/stages/pool/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9ff8a89{/storage,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19dc962{/storage/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bfd553a{/storage/rdd,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40f69180{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3528e0d5{/environment,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a91f9cb{/environment/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b143326{/executors,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f1ad955{/executors/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@dc6b02b{/executors/threadDump,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58cc5ff9{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72ebcac5{/static,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36da411e{/,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b3a845e{/api,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@43392ceb{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@47556815{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:42 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://node-04:4040\r\n    19/11/25 10:35:43 INFO client.RMProxy: Connecting to ResourceManager at node-03/192.168.200.4:8032\r\n    19/11/25 10:35:43 INFO yarn.Client: Requesting a new application from cluster with 9 NodeManagers\r\n    19/11/25 10:35:43 INFO conf.Configuration: resource-types.xml not found\r\n    19/11/25 10:35:43 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\r\n    19/11/25 10:35:43 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (18432 MB per container)\r\n    19/11/25 10:35:43 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\r\n    19/11/25 10:35:43 INFO yarn.Client: Setting up container launch context for our AM\r\n    19/11/25 10:35:43 INFO yarn.Client: Setting up the launch environment for our AM container\r\n    19/11/25 10:35:43 INFO yarn.Client: Preparing resources for our AM container\r\n    19/11/25 10:35:44 INFO yarn.Client: Uploading resource file:/home/gzitv/ccw/recommend/tensorflow-hadoop-1.0-SNAPSHOT.jar -> hdfs://node-03:8020/user/gzitv/.sparkStaging/application_1574297549802_0182/tensorflow-hadoop-1.0-SNAPSHOT.jar\r\n    19/11/25 10:35:44 INFO yarn.Client: Uploading resource file:/tmp/spark-80626013-5ed5-47a7-8987-5b6152988869/__spark_conf__4528364305729155661.zip -> hdfs://node-03:8020/user/gzitv/.sparkStaging/application_1574297549802_0182/__spark_conf__.zip\r\n    19/11/25 10:35:44 INFO spark.SecurityManager: Changing view acls to: gzitv\r\n    19/11/25 10:35:44 INFO spark.SecurityManager: Changing modify acls to: gzitv\r\n    19/11/25 10:35:44 INFO spark.SecurityManager: Changing view acls groups to: \r\n    19/11/25 10:35:44 INFO spark.SecurityManager: Changing modify acls groups to: \r\n    19/11/25 10:35:44 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(gzitv); groups with view permissions: Set(); users  with modify permissions: Set(gzitv); groups with modify permissions: Set()\r\n    19/11/25 10:35:44 INFO conf.HiveConf: Found configuration file file:/etc/hive/conf.cloudera.hive/hive-site.xml\r\n    19/11/25 10:35:44 INFO yarn.Client: Submitting application application_1574297549802_0182 to ResourceManager\r\n    19/11/25 10:35:44 INFO impl.YarnClientImpl: Submitted application application_1574297549802_0182\r\n    19/11/25 10:35:44 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1574297549802_0182 and attemptId None\r\n    19/11/25 10:35:45 INFO yarn.Client: Application report for application_1574297549802_0182 (state: ACCEPTED)\r\n    19/11/25 10:35:45 INFO yarn.Client: \r\n            client token: N/A\r\n            diagnostics: AM container is launched, waiting for AM container to Register with RM\r\n            ApplicationMaster host: N/A\r\n            ApplicationMaster RPC port: -1\r\n            queue: root.users.gzitv\r\n            start time: 1574649344858\r\n            final status: UNDEFINED\r\n            tracking URL: http://node-03:8088/proxy/application_1574297549802_0182/\r\n            user: gzitv\r\n    19/11/25 10:35:46 INFO yarn.Client: Application report for application_1574297549802_0182 (state: ACCEPTED)\r\n    19/11/25 10:35:47 INFO yarn.Client: Application report for application_1574297549802_0182 (state: ACCEPTED)\r\n    19/11/25 10:35:48 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> node-03, PROXY_URI_BASES -> http://node-03:8088/proxy/application_1574297549802_0182), /proxy/application_1574297549802_0182\r\n    19/11/25 10:35:48 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\r\n    19/11/25 10:35:48 INFO yarn.Client: Application report for application_1574297549802_0182 (state: RUNNING)\r\n    19/11/25 10:35:48 INFO yarn.Client: \r\n            client token: N/A\r\n            diagnostics: N/A\r\n            ApplicationMaster host: 192.168.200.4\r\n            ApplicationMaster RPC port: -1\r\n            queue: root.users.gzitv\r\n            start time: 1574649344858\r\n            final status: UNDEFINED\r\n            tracking URL: http://node-03:8088/proxy/application_1574297549802_0182/\r\n            user: gzitv\r\n    19/11/25 10:35:48 INFO cluster.YarnClientSchedulerBackend: Application application_1574297549802_0182 has started running.\r\n    19/11/25 10:35:48 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58711.\r\n    19/11/25 10:35:48 INFO netty.NettyBlockTransferService: Server created on node-04:58711\r\n    19/11/25 10:35:48 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n    19/11/25 10:35:49 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, node-04, 58711, None)\r\n    19/11/25 10:35:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager node-04:58711 with 366.3 MB RAM, BlockManagerId(driver, node-04, 58711, None)\r\n    19/11/25 10:35:49 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, node-04, 58711, None)\r\n    19/11/25 10:35:49 INFO storage.BlockManager: external shuffle service port = 7337\r\n    19/11/25 10:35:49 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, node-04, 58711, None)\r\n    19/11/25 10:35:49 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\r\n    19/11/25 10:35:49 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\r\n    19/11/25 10:35:49 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@619c4e31{/metrics/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:49 INFO scheduler.EventLoggingListener: Logging events to hdfs://node-03:8020/user/spark/applicationHistory/application_1574297549802_0182\r\n    19/11/25 10:35:49 WARN lineage.LineageWriter: Lineage directory /var/log/spark/lineage doesn't exist or is not writable. Lineage for this application will be disabled.\r\n    19/11/25 10:35:49 INFO util.Utils: Extension com.cloudera.spark.lineage.NavigatorAppListener not being initialized.\r\n    19/11/25 10:35:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.200.7:50598) with ID 3\r\n    19/11/25 10:35:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.200.12:50698) with ID 2\r\n    19/11/25 10:35:52 INFO storage.BlockManagerMasterEndpoint: Registering block manager node-06:36587 with 5.2 GB RAM, BlockManagerId(3, node-06, 36587, None)\r\n    19/11/25 10:35:52 INFO storage.BlockManagerMasterEndpoint: Registering block manager node-11:58531 with 5.2 GB RAM, BlockManagerId(2, node-11, 58531, None)\r\n    19/11/25 10:35:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.200.5:44178) with ID 1\r\n    19/11/25 10:35:52 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\r\n    19/11/25 10:35:52 INFO storage.BlockManagerMasterEndpoint: Registering block manager node-04:54800 with 5.2 GB RAM, BlockManagerId(1, node-04, 54800, None)\r\n    19/11/25 10:35:53 INFO internal.SharedState: loading hive config file: file:/etc/hive/conf.cloudera.hive/hive-site.xml\r\n    19/11/25 10:35:53 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').\r\n    19/11/25 10:35:53 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.\r\n    19/11/25 10:35:53 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\r\n    19/11/25 10:35:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b76358d{/SQL,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:53 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\r\n    19/11/25 10:35:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b71a21a{/SQL/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:53 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\r\n    19/11/25 10:35:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ed4f367{/SQL/execution,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:53 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\r\n    19/11/25 10:35:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51a7f70c{/SQL/execution/json,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:53 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\r\n    19/11/25 10:35:53 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@43e6fc32{/static/sql,null,AVAILABLE,@Spark}\r\n    19/11/25 10:35:54 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\r\n    19/11/25 10:35:54 WARN lineage.LineageWriter: Lineage directory /var/log/spark/lineage doesn't exist or is not writable. Lineage for this application will be disabled.\r\n    19/11/25 10:35:54 INFO util.Utils: Extension com.cloudera.spark.lineage.NavigatorQueryListener not being initialized.\r\n    args: Namespace(batch_size=10, cluster_size=3, epochs=3, export_dir='tmp_export', model_dir='tmp_model')\r\n    2019-11-25 10:35:56,438 INFO (MainThread-1506) ===== 1. train args: Namespace(batch_size=10, cluster_size=3, epochs=3, export_dir='tmp_export', model_dir='tmp_model', num_records=100)\r\n    2019-11-25 10:35:56,438 INFO (MainThread-1506) ===== 2. train params: {Param(parent='TFEstimator_aa93e7f12f01', name='input_mapping', doc='Mapping of input DataFrame column to input tensor'): {'a': 'a'}, Param(parent='TFEstimator_aa93e7f12f01', name='model_dir', doc='Path to save/load model checkpoints'): 'tmp_model', Param(parent='TFEstimator_aa93e7f12f01', name='export_dir', doc='Directory to export saved_model'): 'tmp_export', Param(parent='TFEstimator_aa93e7f12f01', name='cluster_size', doc='Number of nodes in the cluster'): 3, Param(parent='TFEstimator_aa93e7f12f01', name='tensorboard', doc='Launch tensorboard process'): False, Param(parent='TFEstimator_aa93e7f12f01', name='epochs', doc='Number of epochs to train'): 3, Param(parent='TFEstimator_aa93e7f12f01', name='batch_size', doc='Number of records per batch'): 10, Param(parent='TFEstimator_aa93e7f12f01', name='grace_secs', doc='Number of seconds to wait after feeding data (for final tasks like exporting a saved_model)'): 60}\r\n    2019-11-25 10:35:56,439 INFO (MainThread-1506) ===== 3. train args + params: Namespace(batch_size=10, cluster_size=3, driver_ps_nodes=False, epochs=3, export_dir='tmp_export', grace_secs=60, input_mapping={'a': 'a'}, master_node='chief', model_dir='tmp_model', num_ps=0, num_records=100, protocol='grpc', readers=1, steps=1000, tensorboard=False, tfrecord_dir=None)\r\n    2019-11-25 10:35:56,440 INFO (MainThread-1506) Reserving TFSparkNodes \r\n    2019-11-25 10:35:56,440 INFO (MainThread-1506) cluster_template: {'chief': [0], 'worker': [1, 2]}\r\n    2019-11-25 10:35:56,444 INFO (MainThread-1506) listening for reservations at ('192.168.200.5', 55855)\r\n    2019-11-25 10:35:56,445 INFO (MainThread-1506) Starting TensorFlow on executors\r\n    2019-11-25 10:35:56,451 INFO (MainThread-1506) Waiting for TFSparkNodes to start\r\n    2019-11-25 10:35:56,451 INFO (MainThread-1506) waiting for 3 reservations\r\n    19/11/25 10:35:56 INFO spark.SparkContext: Starting job: foreachPartition at /opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320\r\n    19/11/25 10:35:56 INFO scheduler.DAGScheduler: Got job 0 (foreachPartition at /opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320) with 3 output partitions\r\n    19/11/25 10:35:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320)\r\n    19/11/25 10:35:56 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n    19/11/25 10:35:56 INFO scheduler.DAGScheduler: Missing parents: List()\r\n    19/11/25 10:35:56 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[6] at foreachPartition at /opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320), which has no missing parents\r\n    19/11/25 10:35:56 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.1 KB, free 366.3 MB)\r\n    19/11/25 10:35:57 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.5 KB, free 366.3 MB)\r\n    19/11/25 10:35:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on node-04:58711 (size: 13.5 KB, free: 366.3 MB)\r\n    19/11/25 10:35:57 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1164\r\n    19/11/25 10:35:57 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (PythonRDD[6] at foreachPartition at /opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320) (first 15 tasks are for partitions Vector(0, 1, 2))\r\n    19/11/25 10:35:57 INFO cluster.YarnScheduler: Adding task set 0.0 with 3 tasks\r\n    19/11/25 10:35:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, node-06, executor 3, partition 0, PROCESS_LOCAL, 7736 bytes)\r\n    19/11/25 10:35:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, node-04, executor 1, partition 1, PROCESS_LOCAL, 7736 bytes)\r\n    19/11/25 10:35:57 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, node-11, executor 2, partition 2, PROCESS_LOCAL, 7736 bytes)\r\n    2019-11-25 10:35:57,451 INFO (MainThread-1506) waiting for 3 reservations\r\n    19/11/25 10:35:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on node-06:36587 (size: 13.5 KB, free: 5.2 GB)\r\n    19/11/25 10:35:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on node-11:58531 (size: 13.5 KB, free: 5.2 GB)\r\n    19/11/25 10:35:57 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on node-04:54800 (size: 13.5 KB, free: 5.2 GB)\r\n    2019-11-25 10:35:58,453 INFO (MainThread-1506) waiting for 3 reservations\r\n    2019-11-25 10:35:59,454 INFO (MainThread-1506) waiting for 3 reservations\r\n    2019-11-25 10:36:00,455 INFO (MainThread-1506) waiting for 3 reservations\r\n    2019-11-25 10:36:01,457 INFO (MainThread-1506) waiting for 3 reservations\r\n    2019-11-25 10:36:02,458 INFO (MainThread-1506) all reservations completed\r\n    2019-11-25 10:36:02,458 INFO (MainThread-1506) All TFSparkNodes started\r\n    2019-11-25 10:36:02,459 INFO (MainThread-1506) {'executor_id': 0, 'host': '192.168.200.7', 'job_name': 'chief', 'task_index': 0, 'port': 46465, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-mt48obxw/listener-q9fbj0re', 'authkey': b'\\xc8\\xc8YMBUH\\x0c\\xb5\\xe3&\\xae\\xbe\\xf6,<'}\r\n    2019-11-25 10:36:02,459 INFO (MainThread-1506) {'executor_id': 2, 'host': '192.168.200.12', 'job_name': 'worker', 'task_index': 1, 'port': 45418, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-5smf0kkb/listener-jolq8ogz', 'authkey': b'\\xa4\\x0f\\x84\\xea\\x19(L\\xf7\\x94\\x84\\x1d\\xe7\\x1d^\\x8d\\x18'}\r\n    2019-11-25 10:36:02,459 INFO (MainThread-1506) {'executor_id': 1, 'host': '192.168.200.5', 'job_name': 'worker', 'task_index': 0, 'port': 45948, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-ja9tmgvi/listener-29sqsi8n', 'authkey': b'\\xaa\\x9e\\x9c$z\\xac@o\\xaf\\x11\\xdf\\x8esR\\xb7 '}\r\n    2019-11-25 10:36:02,922 INFO (MainThread-1506) Feeding training data\r\n    19/11/25 10:36:03 INFO spark.SparkContext: Starting job: collect at PythonRDD.scala:166\r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: Got job 1 (collect at PythonRDD.scala:166) with 9 output partitions\r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at PythonRDD.scala:166)\r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: Missing parents: List()\r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[11] at RDD at PythonRDD.scala:53), which has no missing parents\r\n    19/11/25 10:36:03 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KB, free 366.3 MB)\r\n    19/11/25 10:36:03 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.6 KB, free 366.2 MB)\r\n    19/11/25 10:36:03 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on node-04:58711 (size: 9.6 KB, free: 366.3 MB)\r\n    19/11/25 10:36:03 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1164\r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: Submitting 9 missing tasks from ResultStage 1 (PythonRDD[11] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))\r\n    19/11/25 10:36:03 INFO cluster.YarnScheduler: Adding task set 1.0 with 9 tasks\r\n    19/11/25 10:36:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3, node-04, executor 1, partition 0, PROCESS_LOCAL, 8021 bytes)\r\n    19/11/25 10:36:03 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6156 ms on node-04 (executor 1) (1/3)\r\n    19/11/25 10:36:03 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 53374\r\n    19/11/25 10:36:03 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on node-04:54800 (size: 9.6 KB, free: 5.2 GB)\r\n    19/11/25 10:36:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 4, node-06, executor 3, partition 1, PROCESS_LOCAL, 8021 bytes)\r\n    19/11/25 10:36:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 5, node-11, executor 2, partition 2, PROCESS_LOCAL, 8035 bytes)\r\n    19/11/25 10:36:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6465 ms on node-06 (executor 3) (2/3)\r\n    19/11/25 10:36:03 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6415 ms on node-11 (executor 2) (3/3)\r\n    19/11/25 10:36:03 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: ResultStage 0 (foreachPartition at /opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320) finished in 6.942 s\r\n    19/11/25 10:36:03 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on node-06:36587 (size: 9.6 KB, free: 5.2 GB)\r\n    19/11/25 10:36:03 INFO scheduler.DAGScheduler: Job 0 finished: foreachPartition at /opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320, took 7.018314 s\r\n    19/11/25 10:36:03 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on node-11:58531 (size: 9.6 KB, free: 5.2 GB)\r\n    19/11/25 10:36:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 6, node-06, executor 3, partition 3, PROCESS_LOCAL, 8021 bytes)\r\n    19/11/25 10:36:07 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 1.0 (TID 4, node-06, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\r\n        process()\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\r\n        serializer.dump_stream(func(split_index, iterator), outfile)\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 414, in _train\r\n        raise Exception(\"Exception in worker:\\n\" + e_str)\r\n    Exception: Exception in worker:\r\n    Traceback (most recent call last):\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 330, in wrapper_fn_background\r\n        wrapper_fn(args, context)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 324, in wrapper_fn\r\n        fn(args, context)\r\n    File \"/home/gzitv/ccw/recommend/spark_test.py\", line 72, in main_fun\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n        use_multiprocessing=use_multiprocessing)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 789, in fit\r\n        *args, **kwargs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n        mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n        task_id, session_config, rpc_layer)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n        return worker_fn(strategy)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n        return method(model, **kwargs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n        total_epochs=epochs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n        batch_outs = execution_function(iterator)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n        distributed_function(input_fn))\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n        result = self._call(*args, **kwds)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n        return self._stateless_fn(*args, **kwds)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n        return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n        self.captured_inputs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n        ctx, args, cancellation_manager=cancellation_manager)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n        ctx=ctx)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n        six.raise_from(core._status_to_exception(e.code, message), None)\r\n    File \"<string>\", line 3, in raise_from\r\n    tensorflow.python.framework.errors_impl.InternalError:  RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n            [[node allreduce_1/CollectiveReduce (defined at opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_720]\r\n\r\n    Function call stack:\r\n    distributed_function\r\n\r\n\r\n\r\n            at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n            at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n            at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n            at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n            at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n            at scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n            at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n            at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n            at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n            at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n            at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n            at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n            at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n            at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n            at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n            at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n            at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n            at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n            at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2113)\r\n            at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2113)\r\n            at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n            at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n            at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\r\n            at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n            at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\r\n            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n            at java.lang.Thread.run(Thread.java:748)\r\n\r\n**Executor Logs:**\r\n\r\n    19/11/25 10:35:50 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 89573@node-06\r\n    19/11/25 10:35:50 INFO util.SignalUtils: Registered signal handler for TERM\r\n    19/11/25 10:35:50 INFO util.SignalUtils: Registered signal handler for HUP\r\n    19/11/25 10:35:50 INFO util.SignalUtils: Registered signal handler for INT\r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing view acls to: yarn,gzitv\r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing modify acls to: yarn,gzitv\r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing view acls groups to: \r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing modify acls groups to: \r\n    19/11/25 10:35:51 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, gzitv); groups with view permissions: Set(); users  with modify permissions: Set(yarn, gzitv); groups with modify permissions: Set()\r\n    19/11/25 10:35:51 INFO client.TransportClientFactory: Successfully created connection to node-04/192.168.200.5:60248 after 114 ms (0 ms spent in bootstraps)\r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing view acls to: yarn,gzitv\r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing modify acls to: yarn,gzitv\r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing view acls groups to: \r\n    19/11/25 10:35:51 INFO spark.SecurityManager: Changing modify acls groups to: \r\n    19/11/25 10:35:51 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, gzitv); groups with view permissions: Set(); users  with modify permissions: Set(yarn, gzitv); groups with modify permissions: Set()\r\n    19/11/25 10:35:51 INFO client.TransportClientFactory: Successfully created connection to node-04/192.168.200.5:60248 after 1 ms (0 ms spent in bootstraps)\r\n    19/11/25 10:35:52 INFO storage.DiskBlockManager: Created local directory at /home/data/yarn/nm/usercache/gzitv/appcache/application_1574297549802_0182/blockmgr-e46e4653-8476-4645-8994-d1d6887de7ef\r\n    19/11/25 10:35:52 INFO memory.MemoryStore: MemoryStore started with capacity 5.2 GB\r\n    19/11/25 10:35:52 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@node-04:60248\r\n    19/11/25 10:35:52 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\r\n    19/11/25 10:35:52 INFO executor.Executor: Starting executor ID 3 on host node-06\r\n    19/11/25 10:35:52 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36587.\r\n    19/11/25 10:35:52 INFO netty.NettyBlockTransferService: Server created on node-06:36587\r\n    19/11/25 10:35:52 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n    19/11/25 10:35:52 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, node-06, 36587, None)\r\n    19/11/25 10:35:52 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, node-06, 36587, None)\r\n    19/11/25 10:35:52 INFO storage.BlockManager: external shuffle service port = 7337\r\n    19/11/25 10:35:52 INFO storage.BlockManager: Registering executor with local external shuffle service.\r\n    19/11/25 10:35:52 INFO client.TransportClientFactory: Successfully created connection to node-06/192.168.200.7:7337 after 1 ms (0 ms spent in bootstraps)\r\n    19/11/25 10:35:52 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(3, node-06, 36587, None)\r\n    19/11/25 10:35:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\r\n    19/11/25 10:35:57 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\r\n    19/11/25 10:35:57 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\r\n    19/11/25 10:35:57 INFO client.TransportClientFactory: Successfully created connection to node-04/192.168.200.5:58711 after 2 ms (0 ms spent in bootstraps)\r\n    19/11/25 10:35:57 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.5 KB, free 5.2 GB)\r\n    19/11/25 10:35:57 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 171 ms\r\n    19/11/25 10:35:57 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.1 KB, free 5.2 GB)\r\n    2019-11-25 10:36:01,491 INFO (MainThread-89693) connected to server at ('192.168.200.5', 55855)\r\n    2019-11-25 10:36:01,492 INFO (MainThread-89693) TFSparkNode.reserve: {'executor_id': 0, 'host': '192.168.200.7', 'job_name': 'chief', 'task_index': 0, 'port': 46465, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-mt48obxw/listener-q9fbj0re', 'authkey': b'\\xc8\\xc8YMBUH\\x0c\\xb5\\xe3&\\xae\\xbe\\xf6,<'}\r\n    2019-11-25 10:36:03,496 INFO (MainThread-89693) node: {'executor_id': 0, 'host': '192.168.200.7', 'job_name': 'chief', 'task_index': 0, 'port': 46465, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-mt48obxw/listener-q9fbj0re', 'authkey': b'\\xc8\\xc8YMBUH\\x0c\\xb5\\xe3&\\xae\\xbe\\xf6,<'}\r\n    2019-11-25 10:36:03,496 INFO (MainThread-89693) node: {'executor_id': 1, 'host': '192.168.200.5', 'job_name': 'worker', 'task_index': 0, 'port': 45948, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-ja9tmgvi/listener-29sqsi8n', 'authkey': b'\\xaa\\x9e\\x9c$z\\xac@o\\xaf\\x11\\xdf\\x8esR\\xb7 '}\r\n    2019-11-25 10:36:03,496 INFO (MainThread-89693) node: {'executor_id': 2, 'host': '192.168.200.12', 'job_name': 'worker', 'task_index': 1, 'port': 45418, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-5smf0kkb/listener-jolq8ogz', 'authkey': b'\\xa4\\x0f\\x84\\xea\\x19(L\\xf7\\x94\\x84\\x1d\\xe7\\x1d^\\x8d\\x18'}\r\n    2019-11-25 10:36:03,497 INFO (MainThread-89693) export TF_CONFIG: {\"cluster\": {\"chief\": [\"192.168.200.7:46465\"], \"worker\": [\"192.168.200.5:45948\", \"192.168.200.12:45418\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}, \"environment\": \"cloud\"}\r\n    2019-11-25 10:36:03,497 INFO (MainThread-89693) Starting TensorFlow chief:0 as chief on cluster node 0 on background process\r\n    19/11/25 10:36:03 INFO python.PythonRunner: Times: total = 5611, boot = 607, init = 2961, finish = 2043\r\n    2019-11-25 10:36:03.531691: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2194735000 Hz\r\n    2019-11-25 10:36:03.532858: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f01e0004ae0 executing computations on platform Host. Devices:\r\n    2019-11-25 10:36:03.532895: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n    2019-11-25 10:36:03.537483: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job chief -> {0 -> localhost:46465}\r\n    2019-11-25 10:36:03.537518: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.200.5:45948, 1 -> 192.168.200.12:45418}\r\n    2019-11-25 10:36:03.539686: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:46465\r\n    19/11/25 10:36:03 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1462 bytes result sent to driver\r\n    2019-11-25 10:36:03,554 INFO (MainThread-89806) Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:XLA_CPU:0']\r\n    19/11/25 10:36:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4\r\n    19/11/25 10:36:03 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 4)\r\n    2019-11-25 10:36:03,574 INFO (MainThread-89806) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['192.168.200.7:46465'], 'worker': ['192.168.200.5:45948', '192.168.200.12:45418']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n    19/11/25 10:36:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\r\n    19/11/25 10:36:03 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.6 KB, free 5.2 GB)\r\n    19/11/25 10:36:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 18 ms\r\n    19/11/25 10:36:03 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.0 KB, free 5.2 GB)\r\n    Model: \"model\"\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    v_type (InputLayer)          [(None, 2, 1)]            0         \r\n    _________________________________________________________________\r\n    embed_v_type (Embedding)     (None, 2, 1, 32)          64        \r\n    _________________________________________________________________\r\n    flat_v_type (Flatten)        (None, 64)                0         \r\n    _________________________________________________________________\r\n    dense (Dense)                (None, 1)                 65        \r\n    =================================================================\r\n    Total params: 129\r\n    Trainable params: 129\r\n    Non-trainable params: 0\r\n    _________________________________________________________________\r\n    2019-11-25 10:36:04,075 INFO (MainThread-89806) Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {'chief': ['192.168.200.7:46465'], 'worker': ['192.168.200.5:45948', '192.168.200.12:45418']}, task_type = 'chief', task_id = 0, environment = 'cloud', rpc_layer = 'grpc'\r\n    2019-11-25 10:36:04,075 WARNING (MainThread-89806) `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\n    2019-11-25 10:36:04,075 WARNING (MainThread-89806) `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n    2019-11-25 10:36:04,077 INFO (MainThread-89806) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['192.168.200.7:46465'], 'worker': ['192.168.200.5:45948', '192.168.200.12:45418']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n    2019-11-25 10:36:04,078 INFO (MainThread-89806) Multi-worker CollectiveAllReduceStrategy with cluster_spec = {'chief': ['192.168.200.7:46465'], 'worker': ['192.168.200.5:45948', '192.168.200.12:45418']}, task_type = 'chief', task_id = 0, num_workers = 3, local_devices = ('/job:chief/task:0',), communication = CollectiveCommunication.AUTO\r\n    2019-11-25 10:36:04,078 WARNING (MainThread-89806) ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n    2019-11-25 10:36:04.091124: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:400] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n    Train for 3 steps\r\n    Epoch 1/3\r\n    19/11/25 10:36:04 INFO codegen.CodeGenerator: Code generated in 265.403394 ms\r\n    19/11/25 10:36:04 INFO python.PythonRunner: Times: total = 51, boot = 6, init = 45, finish = 0\r\n    2019-11-25 10:36:04,452 INFO (MainThread-90072) Connected to TFSparkNode.mgr on 192.168.200.7, executor=0, state='running'\r\n    2019-11-25 10:36:04,462 INFO (MainThread-90072) mgr.state='running'\r\n    2019-11-25 10:36:04,463 INFO (MainThread-90072) Feeding partition <itertools.chain object at 0x7f01cce3aa20> into input queue <multiprocessing.queues.JoinableQueue object at 0x7f01cc4b59e8>\r\n    2019-11-25 10:36:04,480 INFO (MainThread-89806) Collective batch_all_reduce: 2 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:04,483 INFO (MainThread-89806) Collective batch_all_reduce for IndexedSlices: 1 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:05,529 INFO (MainThread-89806) Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:05,533 INFO (MainThread-89806) Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:06,041 INFO (MainThread-89806) Collective batch_all_reduce: 2 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:06,044 INFO (MainThread-89806) Collective batch_all_reduce for IndexedSlices: 1 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:07,087 INFO (MainThread-89806) Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:07,092 INFO (MainThread-89806) Collective batch_all_reduce: 1 all-reduces, num_workers = 3\r\n    2019-11-25 10:36:07.261999: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.262071: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.262090: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.262100: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.262335: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.262350: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.262401: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n        [[{{node allreduce_1/CollectiveReduce}}]]\r\n    2019-11-25 10:36:07.268305: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268357: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268450: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268554: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingReduce with Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268597: E tensorflow/core/common_runtime/ring_alg.cc:279] Aborting RingGather with Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268605: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268630: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268753: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:234 : Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n    2019-11-25 10:36:07.268783: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:125 : Internal: [_Derived_]RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n\r\n    1/3 [=========>....................] - ETA: 6s19/11/25 10:36:07 ERROR executor.Executor: Exception in task 1.0 in stage 1.0 (TID 4)\r\n    org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\r\n        process()\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\r\n        serializer.dump_stream(func(split_index, iterator), outfile)\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n    File \"/opt/cloudera/parcels/CDH-6.1.0-1.cdh6.1.0.p0.770702/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 414, in _train\r\n        raise Exception(\"Exception in worker:\\n\" + e_str)\r\n    Exception: Exception in worker:\r\n    Traceback (most recent call last):\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 330, in wrapper_fn_background\r\n        wrapper_fn(args, context)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 324, in wrapper_fn\r\n        fn(args, context)\r\n    File \"/home/gzitv/ccw/recommend/spark_test.py\", line 72, in main_fun\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 728, in fit\r\n        use_multiprocessing=use_multiprocessing)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 789, in fit\r\n        *args, **kwargs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n        mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n        task_id, session_config, rpc_layer)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n        return worker_fn(strategy)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n        return method(model, **kwargs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n        total_epochs=epochs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n        batch_outs = execution_function(iterator)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n        distributed_function(input_fn))\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n        result = self._call(*args, **kwds)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n        return self._stateless_fn(*args, **kwds)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n        return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n        self.captured_inputs)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n        ctx, args, cancellation_manager=cancellation_manager)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n        ctx=ctx)\r\n    File \"/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n        six.raise_from(core._status_to_exception(e.code, message), None)\r\n    File \"<string>\", line 3, in raise_from\r\n    tensorflow.python.framework.errors_impl.InternalError:  RecvBufResponse returned 16 bytes where to_tensor expected 32\r\n        [[node allreduce_1/CollectiveReduce (defined at opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_720]\r\n\r\n    Function call stack:\r\n    distributed_function\r\n\r\n\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2113)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2113)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n    19/11/25 10:36:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6\r\n    19/11/25 10:36:07 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 6)\r\n    19/11/25 10:36:07 INFO python.PythonRunner: Times: total = 43, boot = -3745, init = 3787, finish = 1\r\n    2019-11-25 10:36:07,631 INFO (MainThread-90216) Connected to TFSparkNode.mgr on 192.168.200.7, executor=0, state='running'\r\n    2019-11-25 10:36:07,641 INFO (MainThread-90216) mgr.state='running'\r\n    2019-11-25 10:36:07,641 INFO (MainThread-90216) Feeding partition <itertools.chain object at 0x7f01cce3aa20> into input queue <multiprocessing.queues.JoinableQueue object at 0x7f01cc4b59e8>\r\n    19/11/25 10:36:21 INFO executor.Executor: Executor is trying to kill task 3.0 in stage 1.0 (TID 6), reason: Stage cancelled\r\n    19/11/25 10:36:21 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown\r\n    19/11/25 10:36:21 INFO memory.MemoryStore: MemoryStore cleared\r\n    19/11/25 10:36:21 INFO storage.BlockManager: BlockManager stopped\r\n    19/11/25 10:36:21 INFO util.ShutdownHookManager: Shutdown hook called\r\n    19/11/25 10:36:21 INFO executor.Executor: Executor killed task 3.0 in stage 1.0 (TID 6), reason: Stage cancelled\r\n\r\n**Spark Submit Command Line:**\r\n\r\n    spark-submit --master yarn --conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" --jars tensorflow-hadoop-1.0-SNAPSHOT.jar --num-executors 3 --executor-memory 10g --conf spark.dynamicAllocation.enabled=false spark_test.py --epochs 3\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/476", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/476/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/476/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/476/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/476", "id": 524858633, "node_id": "MDU6SXNzdWU1MjQ4NTg2MzM=", "number": 476, "title": "Standalone mode on multiple machine k8s cluster without using YARN", "user": {"login": "wanfengkai", "id": 16138109, "node_id": "MDQ6VXNlcjE2MTM4MTA5", "avatar_url": "https://avatars3.githubusercontent.com/u/16138109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wanfengkai", "html_url": "https://github.com/wanfengkai", "followers_url": "https://api.github.com/users/wanfengkai/followers", "following_url": "https://api.github.com/users/wanfengkai/following{/other_user}", "gists_url": "https://api.github.com/users/wanfengkai/gists{/gist_id}", "starred_url": "https://api.github.com/users/wanfengkai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wanfengkai/subscriptions", "organizations_url": "https://api.github.com/users/wanfengkai/orgs", "repos_url": "https://api.github.com/users/wanfengkai/repos", "events_url": "https://api.github.com/users/wanfengkai/events{/privacy}", "received_events_url": "https://api.github.com/users/wanfengkai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-19T08:46:01Z", "updated_at": "2019-12-25T14:20:56Z", "closed_at": "2019-12-25T14:20:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6,3.7]\r\n - Spark version [2.4.4 ,2.4.3]\r\n - TensorFlow version [2.0.0, 2.0.0]\r\n - TensorFlowOnSpark version [latest]\r\n - Cluster version [Standalone on one single Macbook local machine, Standalone on one k8s cluster]\r\n\r\n**Describe the bug:**\r\nI can use tensorflowonspark to train on one local machine, when I try to run it on my cluster which is a k8s cluster with one spark master and 4 worker node it report timeout error after really training for a while.  Please check the stderr log of one executor.\r\n\r\n**Logs:**\r\n\r\nINFO (MainThread-100550) Feeding partition <itertools.chain object at 0x7f65c2267710> into input queue <multiprocessing.queues.JoinableQueue object at 0x7f65ae180668>\r\n4887/9537 [==============>...............] - ETA: 12:22 - loss: 72.3734 - mse: 72.3733\r\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", li\r\n    process()\r\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", li\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\",\r\n    return func(split, prev_func(split, iterator))\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\",\r\n    return func(split, prev_func(split, iterator))\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\",\r\n    return func(split, prev_func(split, iterator))\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\",\r\n    return f(iterator)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyspark/rdd.py\",\r\n    r = f(it)\r\n  File \"/opt/conda/lib/python3.7/site-packages/tensorflowonspark\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIt\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIt\r\n        at org.apache.spark.InterruptibleIterator.hasNext(Interr\r\n        at scala.collection.Iterator$class.foreach(Iterator.scal\r\n        at org.apache.spark.InterruptibleIterator.foreach(Interr\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(Ar\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(Ar\r\n        at scala.collection.TraversableOnce$class.to(Traversable\r\n        at org.apache.spark.InterruptibleIterator.to(Interruptib\r\n        at scala.collection.TraversableOnce$class.toBuffer(Trave\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(Inter\r\n        at scala.collection.TraversableOnce$class.toArray(Traver\r\n        at org.apache.spark.InterruptibleIterator.toArray(Interr\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultT\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n        at org.apache.spark.executor.Executor$TaskRunner$$anonfu\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Exe\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Thr\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Th\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n**Spark Submit Command Line:**\r\nI submitted my job in jupyter notebook.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/475", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/475/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/475/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/475/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/475", "id": 520961692, "node_id": "MDU6SXNzdWU1MjA5NjE2OTI=", "number": 475, "title": "Not able to run the example on Standalone spark cluster using InputMode.Tensorflow", "user": {"login": "Ansh1234", "id": 6416630, "node_id": "MDQ6VXNlcjY0MTY2MzA=", "avatar_url": "https://avatars2.githubusercontent.com/u/6416630?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ansh1234", "html_url": "https://github.com/Ansh1234", "followers_url": "https://api.github.com/users/Ansh1234/followers", "following_url": "https://api.github.com/users/Ansh1234/following{/other_user}", "gists_url": "https://api.github.com/users/Ansh1234/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ansh1234/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ansh1234/subscriptions", "organizations_url": "https://api.github.com/users/Ansh1234/orgs", "repos_url": "https://api.github.com/users/Ansh1234/repos", "events_url": "https://api.github.com/users/Ansh1234/events{/privacy}", "received_events_url": "https://api.github.com/users/Ansh1234/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-11-11T13:25:14Z", "updated_at": "2020-01-14T00:43:33Z", "closed_at": "2020-01-14T00:43:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.7.4\r\n - Spark version 2.4.4\r\n - TensorFlow version 2.0.0\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nI am following this  [https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/mnist/keras](code)  to run mnist program. When I run the code via InputMode.SPARK, it works fine but when I run the code via InputMode.Tensorflow, I get the following logs.\r\n\r\n**Logs:**\r\n19/11/11 18:54:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nargs: Namespace(batch_size=64, buffer_size=10000, cluster_size=2, epochs=3, export_dir='/Users/anshul/TensorFlowOnSpark/mnist_export', model_dir='/Users/anshul/TensorFlowOnSpark/mnist_model', steps_per_epoch=469, tensorboard=False)\r\nBefore calling run..\r\n2019-11-11 18:54:44,652 INFO (MainThread-14042) Reserving TFSparkNodes \r\n2019-11-11 18:54:44,652 INFO (MainThread-14042) cluster_template: {'chief': [0], 'worker': [1]}\r\n2019-11-11 18:54:44,673 INFO (MainThread-14042) listening for reservations at ('172.16.17.2', 50156)\r\n2019-11-11 18:54:44,675 INFO (MainThread-14042) Starting TensorFlow on executors\r\n2019-11-11 18:54:44,939 INFO (MainThread-14042) Waiting for TFSparkNodes to start\r\n2019-11-11 18:54:44,939 INFO (MainThread-14042) waiting for 2 reservations\r\n2019-11-11 18:54:45,940 INFO (MainThread-14042) waiting for 2 reservations\r\n2019-11-11 18:54:46,944 INFO (MainThread-14042) waiting for 2 reservations\r\n2019-11-11 18:54:47,948 INFO (MainThread-14042) waiting for 2 reservations\r\n2019-11-11 18:54:48,950 INFO (MainThread-14042) waiting for 2 reservations\r\n2019-11-11 18:54:49,955 INFO (MainThread-14042) all reservations completed\r\n2019-11-11 18:54:49,956 INFO (MainThread-14042) All TFSparkNodes started\r\n2019-11-11 18:54:49,956 INFO (MainThread-14042) {'executor_id': 1, 'host': '172.16.17.2', 'job_name': 'worker', 'task_index': 0, 'port': 50186, 'tb_pid': 0, 'tb_port': 0, 'addr': '/var/folders/ds/zfjxc8_92_n86swjt178vhh40000gp/T/pymp-8jyg_uo8/listener-rt3sag9d', 'authkey': b'\\xd5cU\\xde\\xe6\\xb9@\\xd3\\xa8\\xa5\\xc9\\xaf\\xc4v\\x0f\\xf2'}\r\n2019-11-11 18:54:49,956 INFO (MainThread-14042) {'executor_id': 0, 'host': '172.16.17.2', 'job_name': 'chief', 'task_index': 0, 'port': 50187, 'tb_pid': 0, 'tb_port': 0, 'addr': '/var/folders/ds/zfjxc8_92_n86swjt178vhh40000gp/T/pymp-_8gsash4/listener-96kxj507', 'authkey': b'\\x89\\xf7\\xac\\xa7i\\x83G\\x86\\x99K\\x0b\\xba\\xb3\\x9b\\xef\\x1d'}\r\nCalling shutdown 100\r\n2019-11-11 18:54:49,956 INFO (MainThread-14042) Stopping TensorFlow nodes\r\n\r\n**Spark Submit Command Line:**\r\n/Users/anshul/spark/bin/spark-submit  --master $MASTER --conf spark.cores.max=2 --conf spark.task.cpus=1 $TFoS_HOME/examples/mnist/keras/mnist_tf.py --cluster_size 2 --model_dir $TFoS_HOME/mnist_model --export_dir $TFoS_HOME/mnist_export\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/474", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/474/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/474/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/474/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/474", "id": 517511783, "node_id": "MDU6SXNzdWU1MTc1MTE3ODM=", "number": 474, "title": "Low GPU utilization when distributed training", "user": {"login": "pl-lee", "id": 51313587, "node_id": "MDQ6VXNlcjUxMzEzNTg3", "avatar_url": "https://avatars1.githubusercontent.com/u/51313587?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pl-lee", "html_url": "https://github.com/pl-lee", "followers_url": "https://api.github.com/users/pl-lee/followers", "following_url": "https://api.github.com/users/pl-lee/following{/other_user}", "gists_url": "https://api.github.com/users/pl-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/pl-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pl-lee/subscriptions", "organizations_url": "https://api.github.com/users/pl-lee/orgs", "repos_url": "https://api.github.com/users/pl-lee/repos", "events_url": "https://api.github.com/users/pl-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/pl-lee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-11-05T02:25:46Z", "updated_at": "2019-11-25T01:56:46Z", "closed_at": "2019-11-25T01:56:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "The GPU Utilization of one computer is almost 90% before distributed training on RippleNet. And the GPU Utilization of each worker node is about 10% after distributed training. I used InputMode.TENSORFLOW mode and TFRecord files as input. Could someone tell me why so low GPU utilization when distributed training? Or are there some tricks to improve GPU utilization when distributed training? Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/470", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/470/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/470/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/470/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/470", "id": 511785960, "node_id": "MDU6SXNzdWU1MTE3ODU5NjA=", "number": 470, "title": "TensorFlowOnSpark Documentation", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-24T08:22:18Z", "updated_at": "2019-10-25T17:23:45Z", "closed_at": "2019-10-25T17:23:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi leewyang, i'm an italian student. I'm doing an thesis on TensorFlowOnSpark. I wanted to thank you for all the support you gave me. I wanted to ask you if you have links or papers that explain how TensorFlowOnSpark works. \r\n\r\nTensorFlowOnSpark how do it distributes the tensorflow neural networks in a spark cluster? Can you give me more details about it? \r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/469", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/469/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/469/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/469/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/469", "id": 508994408, "node_id": "MDU6SXNzdWU1MDg5OTQ0MDg=", "number": 469, "title": "resnet in cluster  No module named 'official'", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-18T10:29:01Z", "updated_at": "2019-10-19T11:09:52Z", "closed_at": "2019-10-19T11:09:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.7.2\r\n - Spark version 2.3\r\n - TensorFlow version 2.0.0\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version Hadoop 2.7.3\r\n\r\n**Describe the bug:**\r\nin this guide: https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/resnet\r\nif i don't run this: export PYTHONPATH=/home/cloudbreak/models/models-2.0\r\nI have this error immediately: No module named 'official'\r\n\r\nif a run this: export PYTHONPATH=/home/cloudbreak/models/models-2.0\r\nthe \"No module named 'official'\" error I have after these messages:\r\n2019-10-18 09:49:42,784 INFO (MainThread-65476) waiting for 3 reservations\r\n2019-10-18 09:49:43,784 INFO (MainThread-65476) waiting for 3 reservations\r\n...........\r\n\r\ni installed models-2.0 in every worker.\r\n\r\nhow do i do this \"The dependencies from the tensorflow/models to be available on the executors, either installed locally or bundled with the Spark application\"?\r\n\r\n**Logs:**\r\n19/10/18 09:49:36 INFO YarnClientSchedulerBackend: Application application_1571391726032_0001 has started running.\r\n19/10/18 09:49:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32770.\r\n19/10/18 09:49:36 INFO NettyBlockTransferService: Server created on luca95-m0.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:32770\r\n19/10/18 09:49:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n19/10/18 09:49:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, luca95-m0.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, 32770, None)\r\n19/10/18 09:49:36 INFO BlockManagerMasterEndpoint: Registering block manager luca95-m0.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:32770 with 366.3 MB RAM, BlockManagerId(driver, luca95-m0.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, 32770, None)\r\n19/10/18 09:49:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, luca95-m0.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, 32770, None)\r\n19/10/18 09:49:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, luca95-m0.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, 32770, None)\r\n19/10/18 09:49:37 INFO EventLoggingListener: Logging events to hdfs:/spark2-history/application_1571391726032_0001\r\n19/10/18 09:49:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.4:51009) with ID 2\r\n19/10/18 09:49:39 INFO BlockManagerMasterEndpoint: Registering block manager luca95-w2.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:40490 with 2004.6 MB RAM, BlockManagerId(2, luca95-w2.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, 40490, None)\r\n19/10/18 09:49:40 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.6:51003) with ID 1\r\n19/10/18 09:49:40 INFO BlockManagerMasterEndpoint: Registering block manager luca95-w3.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:32768 with 2004.6 MB RAM, BlockManagerId(1, luca95-w3.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, 32768, None)\r\n19/10/18 09:49:41 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.0.5:52056) with ID 3\r\n19/10/18 09:49:41 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\r\n2019-10-18 09:49:41,615 INFO (MainThread-65476) Reserving TFSparkNodes\r\n2019-10-18 09:49:41,615 INFO (MainThread-65476) cluster_template: {'ps': [0], 'worker': [1, 2]}\r\n2019-10-18 09:49:41,617 INFO (MainThread-65476) listening for reservations at ('10.0.0.7', 38379)\r\n2019-10-18 09:49:41,617 INFO (MainThread-65476) Starting TensorFlow on executors\r\n19/10/18 09:49:41 INFO BlockManagerMasterEndpoint: Registering block manager luca95-w1.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:38738 with 2004.6 MB RAM, BlockManagerId(3, luca95-w1.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, 38738, None)\r\n2019-10-18 09:49:41,783 INFO (MainThread-65476) Waiting for TFSparkNodes to start\r\n2019-10-18 09:49:41,783 INFO (MainThread-65476) waiting for 3 reservations\r\n19/10/18 09:49:41 INFO SparkContext: Starting job: foreachPartition at /home/cloudbreak/Python/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320\r\n19/10/18 09:49:41 INFO DAGScheduler: Got job 0 (foreachPartition at /home/cloudbreak/Python/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320) with 3 output partitions\r\n19/10/18 09:49:41 INFO DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /home/cloudbreak/Python/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320)\r\n19/10/18 09:49:41 INFO DAGScheduler: Parents of final stage: List()\r\n19/10/18 09:49:41 INFO DAGScheduler: Missing parents: List()\r\n19/10/18 09:49:41 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at foreachPartition at /home/cloudbreak/Python/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320), which has no missing parents\r\n19/10/18 09:49:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.6 KB, free 366.3 MB)\r\n19/10/18 09:49:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 10.5 KB, free 366.3 MB)\r\n19/10/18 09:49:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on luca95-m0.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:32770 (size: 10.5 KB, free: 366.3 MB)\r\n19/10/18 09:49:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1039\r\n19/10/18 09:49:42 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 0 (PythonRDD[1] at foreachPartition at /home/cloudbreak/Python/lib/python3.7/site-packages/tensorflowonspark/TFCluster.py:320) (first 15 tasks are for partitions Vector(0, 1, 2))\r\n19/10/18 09:49:42 INFO YarnScheduler: Adding task set 0.0 with 3 tasks\r\n19/10/18 09:49:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, luca95-w2.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 2, partition 0, PROCESS_LOCAL, 7850 bytes)\r\n19/10/18 09:49:42 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, luca95-w1.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 3, partition 1, PROCESS_LOCAL, 7850 bytes)\r\n19/10/18 09:49:42 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, luca95-w3.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 1, partition 2, PROCESS_LOCAL, 7850 bytes)\r\n19/10/18 09:49:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on luca95-w3.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:32768 (size: 10.5 KB, free: 2004.6 MB)\r\n19/10/18 09:49:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on luca95-w2.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:40490 (size: 10.5 KB, free: 2004.6 MB)\r\n19/10/18 09:49:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on luca95-w1.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net:38738 (size: 10.5 KB, free: 2004.6 MB)\r\n2019-10-18 09:49:42,784 INFO (MainThread-65476) waiting for 3 reservations\r\n2019-10-18 09:49:43,784 INFO (MainThread-65476) waiting for 3 reservations\r\n19/10/18 09:49:44 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, luca95-w2.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoopfs/fs1/yarn/nodemanager/usercache/cloudbreak/appcache/application_1571391726032_0001/container_e13_1571391726032_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 216, in main\r\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\r\n  File \"/hadoopfs/fs1/yarn/nodemanager/usercache/cloudbreak/appcache/application_1571391726032_0001/container_e13_1571391726032_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 58, in read_command\r\n    command = serializer._read_with_length(file)\r\n  File \"/hadoopfs/fs1/yarn/nodemanager/usercache/cloudbreak/appcache/application_1571391726032_0001/container_e13_1571391726032_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 170, in _read_with_length\r\n    return self.loads(obj)\r\n  File \"/hadoopfs/fs1/yarn/nodemanager/usercache/cloudbreak/appcache/application_1571391726032_0001/container_e13_1571391726032_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 559, in loads\r\n    return pickle.loads(obj, encoding=encoding)\r\n  File \"/hadoopfs/fs1/yarn/nodemanager/usercache/cloudbreak/appcache/application_1571391726032_0001/container_e13_1571391726032_0001_01_000003/__pyfiles__/resnet_cifar_dist.py\", line 25, in <module>\r\n    from official.utils.flags import core as flags_core\r\nModuleNotFoundError: No module named 'official'\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\n\r\n**Spark Submit Command Line:**\r\n\r\nexport PYTHON_ROOT=./Python\r\nexport LD_LIBRARY_PATH=${PATH}\r\nexport PYSPARK_PYTHON=${PYTHON_ROOT}/bin/python3\r\nexport SPARK_YARN_USER_ENV=\"PYSPARK_PYTHON=Python/bin/python3\"\r\nexport PATH=${PYTHON_ROOT}/bin/:$PATH\r\nexport QUEUE=default\r\nexport PYTHONPATH=/home/cloudbreak/models/models-2.0\r\n\r\nspark-submit \\\r\n--master yarn \\\r\n--deploy-mode client \\\r\n--queue ${QUEUE} \\\r\n--num-executors 3 \\\r\n--executor-memory 4G \\\r\n--py-files TensorFlowOnSpark/examples/resnet/resnet_cifar_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/cloudbreak/Python.zip#Python \\\r\nTensorFlowOnSpark/examples/resnet/resnet_cifar_spark.py \\\r\n--epochs 1 \\\r\n--data_dir /home/cloudbreak/cifar10_data/cifar10_data/cifar-10-batches-bin \\\r\n--num_gpus=0 \\\r\n--ds=multi_worker_mirrored \\\r\n--train_epochs 1\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/468", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/468/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/468/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/468/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/468", "id": 507143301, "node_id": "MDU6SXNzdWU1MDcxNDMzMDE=", "number": 468, "title": "unknown queue: gpu", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-10-15T10:07:53Z", "updated_at": "2019-10-16T14:48:32Z", "closed_at": "2019-10-16T14:48:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.7.4\r\n - Spark version 2.3\r\n - TensorFlow version 2.0.0\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version Hadoop 2.6.5\r\n\r\n**Describe the bug:**\r\n\r\nto inform you, i can't update hadoop in major version.\r\n\r\ni read this issue: https://github.com/yahoo/TensorFlowOnSpark/issues/76\r\nThen, i followed this guide: https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_yarn-resource-management/content/configuring_node_labels.html\r\n\r\nI added a few Node Labels (in point 4)\r\n\r\nThen, how can i configure the GPU nodes with a node label?\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/467", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/467/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/467/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/467/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/467", "id": 507051179, "node_id": "MDU6SXNzdWU1MDcwNTExNzk=", "number": 467, "title": "Could not run examples\\wide_deep", "user": {"login": "samuelpreethaml", "id": 36969455, "node_id": "MDQ6VXNlcjM2OTY5NDU1", "avatar_url": "https://avatars0.githubusercontent.com/u/36969455?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuelpreethaml", "html_url": "https://github.com/samuelpreethaml", "followers_url": "https://api.github.com/users/samuelpreethaml/followers", "following_url": "https://api.github.com/users/samuelpreethaml/following{/other_user}", "gists_url": "https://api.github.com/users/samuelpreethaml/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuelpreethaml/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuelpreethaml/subscriptions", "organizations_url": "https://api.github.com/users/samuelpreethaml/orgs", "repos_url": "https://api.github.com/users/samuelpreethaml/repos", "events_url": "https://api.github.com/users/samuelpreethaml/events{/privacy}", "received_events_url": "https://api.github.com/users/samuelpreethaml/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-15T07:09:34Z", "updated_at": "2019-11-27T23:18:25Z", "closed_at": "2019-11-27T23:18:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "After the recent update, I could not run this example and Im getting the below error.\r\nIm running the code in Colab and getting the below error\r\n\r\n`!${SPARK_HOME}/bin/spark-submit \\\r\n--master spark://423fe09a2473:7077 \\\r\n--py-files train_dataset.py,wide_deep_run_loop.py \\\r\n--conf spark.cores.max=3 \\\r\n--conf spark.task.cpus=1 \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n--conf spark.task.maxFailures=1 \\\r\n--conf spark.stage.maxConsecutiveAttempts=1 \\\r\ntrain_main.py \\\r\n--cluster_size 3`\r\n\r\n\r\n`19/10/15 06:40:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\n19/10/15 06:40:54 INFO SparkContext: Running Spark version 2.4.4\r\n19/10/15 06:40:55 INFO SparkContext: Submitted application: wide_deep\r\n19/10/15 06:40:55 INFO SecurityManager: Changing view acls to: root\r\n19/10/15 06:40:55 INFO SecurityManager: Changing modify acls to: root\r\n19/10/15 06:40:55 INFO SecurityManager: Changing view acls groups to: \r\n19/10/15 06:40:55 INFO SecurityManager: Changing modify acls groups to: \r\n19/10/15 06:40:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\r\n19/10/15 06:40:55 INFO Utils: Successfully started service 'sparkDriver' on port 41063.\r\n19/10/15 06:40:55 INFO SparkEnv: Registering MapOutputTracker\r\n19/10/15 06:40:55 INFO SparkEnv: Registering BlockManagerMaster\r\n19/10/15 06:40:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n19/10/15 06:40:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n19/10/15 06:40:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b897403c-30f7-42d7-92e3-4b1dde7d1625\r\n19/10/15 06:40:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\r\n19/10/15 06:40:55 INFO SparkEnv: Registering OutputCommitCoordinator\r\n19/10/15 06:40:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.\r\n19/10/15 06:40:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://423fe09a2473:4040\r\n19/10/15 06:40:55 INFO SparkContext: Added file file:///root/recommender/TensorFlowOnSpark/examples/train_dataset.py at spark://423fe09a2473:41063/files/train_dataset.py with timestamp 1571121655795\r\n19/10/15 06:40:55 INFO Utils: Copying /root/recommender/TensorFlowOnSpark/examples/train_dataset.py to /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc/userFiles-a0679be5-5e29-4d4c-8641-c3ff3dd3a741/train_dataset.py\r\n19/10/15 06:40:55 INFO SparkContext: Added file file:///root/recommender/TensorFlowOnSpark/examples/wide_deep_run_loop.py at spark://423fe09a2473:41063/files/wide_deep_run_loop.py with timestamp 1571121655811\r\n19/10/15 06:40:55 INFO Utils: Copying /root/recommender/TensorFlowOnSpark/examples/wide_deep_run_loop.py to /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc/userFiles-a0679be5-5e29-4d4c-8641-c3ff3dd3a741/wide_deep_run_loop.py\r\n19/10/15 06:40:55 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://423fe09a2473:7077...\r\n19/10/15 06:40:56 INFO TransportClientFactory: Successfully created connection to 423fe09a2473/172.28.0.2:7077 after 43 ms (0 ms spent in bootstraps)\r\n19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20191015064056-0003\r\n19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191015064056-0003/0 on worker-20191015052724-172.28.0.2-43191 (172.28.0.2:43191) with 1 core(s)\r\n19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20191015064056-0003/0 on hostPort 172.28.0.2:43191 with 1 core(s), 1024.0 MB RAM\r\n19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191015064056-0003/1 on worker-20191015052719-172.28.0.2-43969 (172.28.0.2:43969) with 1 core(s)\r\n19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20191015064056-0003/1 on hostPort 172.28.0.2:43969 with 1 core(s), 1024.0 MB RAM\r\n19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20191015064056-0003/2 on worker-20191015052722-172.28.0.2-36691 (172.28.0.2:36691) with 1 core(s)\r\n19/10/15 06:40:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20191015064056-0003/2 on hostPort 172.28.0.2:36691 with 1 core(s), 1024.0 MB RAM\r\n19/10/15 06:40:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35399.\r\n19/10/15 06:40:56 INFO NettyBlockTransferService: Server created on 423fe09a2473:35399\r\n19/10/15 06:40:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191015064056-0003/0 is now RUNNING\r\n19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191015064056-0003/1 is now RUNNING\r\n19/10/15 06:40:56 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20191015064056-0003/2 is now RUNNING\r\n19/10/15 06:40:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 423fe09a2473, 35399, None)\r\n19/10/15 06:40:56 INFO BlockManagerMasterEndpoint: Registering block manager 423fe09a2473:35399 with 366.3 MB RAM, BlockManagerId(driver, 423fe09a2473, 35399, None)\r\n19/10/15 06:40:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 423fe09a2473, 35399, None)\r\n19/10/15 06:40:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 423fe09a2473, 35399, None)\r\n19/10/15 06:40:56 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\r\nspark args: Namespace(cluster_size=3, num_ps=1)\r\ntf args: ['/root/recommender/TensorFlowOnSpark/examples/train_main.py']\r\n===== num_executors=3, num_workers=2, num_ps=1\r\n2019-10-15 06:40:56,773 INFO (MainThread-3730) Reserving TFSparkNodes \r\n2019-10-15 06:40:56,773 INFO (MainThread-3730) cluster_template: {'ps': [0], 'master': [1], 'worker': [2]}\r\n2019-10-15 06:40:56,775 INFO (MainThread-3730) listening for reservations at ('172.28.0.2', 45731)\r\n2019-10-15 06:40:56,784 INFO (MainThread-3730) Starting TensorFlow on executors\r\n2019-10-15 06:40:57,634 INFO (MainThread-3730) Waiting for TFSparkNodes to start\r\n2019-10-15 06:40:57,634 INFO (MainThread-3730) waiting for 3 reservations\r\nTraceback (most recent call last):\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 590, in dumps\r\n    return cloudpickle.dumps(obj, 2)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 863, in dumps\r\n    cp.dump(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 260, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 409, in dump\r\n    self.save(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 751, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 808, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 808, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 406, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 781, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 805, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 400, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 549, in save_function_tuple\r\n    save(state)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 400, in save_function\r\n    self.save_function_tuple(obj)\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 526, in save_function_tuple\r\n    itertools.chain(f_globals.values(), closure_values or ()),\r\n  File \"/content/spark-2.4.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 431, in _save_subimports\r\n    for name, module in sys.modules.items():\r\nRuntimeError: dictionary changed size during iteration\r\n2019-10-15 06:40:57,683 ERROR (Thread-3-3730) Exception in TF background thread\r\n2019-10-15 06:40:58,635 INFO (MainThread-3730) waiting for 3 reservations\r\n19/10/15 06:40:58 INFO SparkUI: Stopped Spark web UI at http://423fe09a2473:4040\r\n19/10/15 06:40:58 INFO StandaloneSchedulerBackend: Shutting down all executors\r\n19/10/15 06:40:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\r\n19/10/15 06:40:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\r\n19/10/15 06:40:58 INFO MemoryStore: MemoryStore cleared\r\n19/10/15 06:40:58 INFO BlockManager: BlockManager stopped\r\n19/10/15 06:40:58 INFO BlockManagerMaster: BlockManagerMaster stopped\r\n19/10/15 06:40:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\r\n19/10/15 06:40:58 INFO SparkContext: Successfully stopped SparkContext\r\n19/10/15 06:40:59 INFO ShutdownHookManager: Shutdown hook called\r\n19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-e3b8f5ed-ff45-4a36-9764-8cc3128b023a\r\n19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc/pyspark-3fb32a38-e47e-4068-92c9-81643819b25c\r\n19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-723c7567-9e2a-43df-9577-c9339a96e7dc\r\n19/10/15 06:40:59 INFO ShutdownHookManager: Deleting directory /tmp/localPyFiles-804b7a62-6d29-4206-9e6b-1025255d55f9`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/464", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/464/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/464/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/464/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/464", "id": 505780923, "node_id": "MDU6SXNzdWU1MDU3ODA5MjM=", "number": 464, "title": "resnet cifar No module named 'official'", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2019-10-11T10:51:45Z", "updated_at": "2019-10-18T07:35:56Z", "closed_at": "2019-10-18T07:35:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.3\r\n - TensorFlow version 2.0.0\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nthis guide: https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/resnet\r\n\r\ni have this error: **ModuleNotFoundError: No module named 'official'**\r\n\r\nthe models's path is: /home/luca/models\r\nthe tensorflowonspark's path is: export TFoS_HOME=/home/luca/TensorFlowOnSpark\r\n\r\ni tried this and i had the same error:\r\nexport PYTHONPATH=${PYTHONPATH}:/home/luca/models\r\nexport PYTHONPATH=/home/luca/models\r\n\r\nI have also installed this: pip3 install --user -r official/requirements.txt\r\n\r\n**Logs:**\r\n2019-10-11 12:30:01,739 INFO (MainThread-125432) waiting for 3 reservations\r\n19/10/11 12:30:01 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 192.168.163.186, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 364, in main\r\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\r\n    command = serializer._read_with_length(file)\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\r\n    return self.loads(obj)\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 580, in loads\r\n    return pickle.loads(obj, encoding=encoding)\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/work/app-20191011122954-0009/0/resnet_cifar_dist.py\", line 25, in <module>\r\n    from official.utils.flags import core as flags_core\r\n**ModuleNotFoundError: No module named 'official'**\r\n\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--py-files ${TFoS_HOME}/examples/resnet/resnet_cifar_dist.py \\\r\n${TFoS_HOME}/examples/resnet/resnet_cifar_spark.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--epochs 1 \\\r\n--data_dir /home/luca/cifar10/cifar-10-batches-bin \\\r\n--num_gpus=0 \\\r\n--ds=multi_worker_mirrored \\\r\n--train_epochs 1", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/463", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/463/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/463/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/463/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/463", "id": 503948564, "node_id": "MDU6SXNzdWU1MDM5NDg1NjQ=", "number": 463, "title": "yarn mnist_tf.py not work ", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-08T10:01:10Z", "updated_at": "2019-10-11T10:16:57Z", "closed_at": "2019-10-11T10:16:57Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6.8\r\n - Spark version 2.3\r\n - TensorFlow version 2.0.0\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version Hadoop 2.8\r\n\r\n**pip list:**\r\nabsl-py                  0.8.0\r\nastor                    0.8.0\r\nattrs                    19.2.0\r\ncertifi                  2019.9.11\r\nchardet                  3.0.4\r\ndill                     0.3.1.1\r\nfuture                   0.17.1\r\ngast                     0.2.2\r\ngoogle-pasta             0.1.7\r\ngoogleapis-common-protos 1.6.0\r\ngrpcio                   1.24.1\r\nh5py                     2.10.0\r\nidna                     2.8\r\nKeras-Applications       1.0.8\r\nKeras-Preprocessing      1.1.0\r\nMarkdown                 3.1.1\r\nnumpy                    1.17.2\r\nopt-einsum               3.1.0\r\npip                      19.2.3\r\npromise                  2.2.1\r\nprotobuf                 3.10.0\r\npsutil                   5.6.3\r\nrequests                 2.22.0\r\nsetuptools               40.6.2\r\nsix                      1.12.0\r\ntensorboard              2.0.0\r\ntensorflow               2.0.0\r\ntensorflow-datasets      1.2.0\r\ntensorflow-estimator     2.0.0\r\ntensorflow-metadata      0.15.0\r\ntensorflowonspark        2.0.0\r\ntermcolor                1.1.0\r\ntqdm                     4.36.1\r\nurllib3                  1.25.6\r\nWerkzeug                 0.16.0\r\nwheel                    0.33.6\r\nwrapt                    1.11.2\r\n\r\n\r\n**Describe the bug:**\r\nthis is the file: https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_tf.py\r\n\r\nWhen i start mnist_tf.py in standalone it works but in cluster yarn not works.\r\n\r\nthis is my principal log:\r\n    from _bz2 import BZ2Compressor, BZ2Decompressor\r\nModuleNotFoundError: No module named '_bz2'\r\n\r\n\r\n**Logs:**\r\n19/10/08 09:57:03 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 4, luca95-w1.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 3, partition 0, PROCESS_LOCAL, 7850 bytes)\r\n19/10/08 09:57:04 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, luca95-w3.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"/home/cloudbreak/Python3/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 365, in _mapfn\r\n  File \"/home/cloudbreak/Python3/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 324, in wrapper_fn\r\n  File \"/home/cloudbreak/mnist_tf.py\", line 7, in main_fun\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/Python3/lib/python3.6/site-packages/tensorflow_datasets/__init__.py\", line 46, in <module>\r\n    from tensorflow_datasets.core import tf_compat\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/Python3/lib/python3.6/site-packages/tensorflow_datasets/core/__init__.py\", line 23, in <module>\r\n    from tensorflow_datasets.core.dataset_builder import BeamBasedBuilder  # pylint:disable=g-import-not-at-top\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/Python3/lib/python3.6/site-packages/tensorflow_datasets/core/dataset_builder.py\", line 35, in <module>\r\n    from tensorflow_datasets.core import download\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/Python3/lib/python3.6/site-packages/tensorflow_datasets/core/download/__init__.py\", line 19, in <module>\r\n    from tensorflow_datasets.core.download.download_manager import DownloadConfig\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/Python3/lib/python3.6/site-packages/tensorflow_datasets/core/download/download_manager.py\", line 35, in <module>\r\n    from tensorflow_datasets.core.download import extractor\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/Python3/lib/python3.6/site-packages/tensorflow_datasets/core/download/extractor.py\", line 40, in <module>\r\n    import bz2  # pylint:disable=g-import-not-at-top\r\n  File \"/hadoopfs/fs2/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000003/Python3/lib/python3.6/bz2.py\", line 23, in <module>\r\n    **from _bz2 import BZ2Compressor, BZ2Decompressor\r\nModuleNotFoundError: No module named '_bz2'**\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n19/10/08 09:57:09 INFO DAGScheduler: ResultStage 0 (foreachPartition at /home/cloudbreak/Python3/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:320) failed in 11.121 s due to Job aborted due to stage failure: Task 2 in stage 0.0 failed 4 times, most recent failure: Lost task 2.3 in stage 0.0 (TID 9, luca95-w1.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000004/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0018/container_e02_1570519936439_0018_01_000004/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"/home/cloudbreak/Python3/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 270, in _mapfn\r\nException: Duplicate worker/task in cluster_info\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n**Spark Submit Command Line:**\r\nspark-submit \\\r\n--master yarn \\\r\n--deploy-mode client \\\r\n--queue ${QUEUE} \\\r\n--num-executors 3 \\\r\n--executor-memory 4G \\\r\n--executor-cores 2 \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/cloudbreak/Python.zip#Python \\\r\n--jars hdfs:///user/cloudbreak/tensorflow-hadoop-1.10.0.jar \\\r\n/home/cloudbreak/mnist_tf.py \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/462", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/462/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/462/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/462/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/462", "id": 503945066, "node_id": "MDU6SXNzdWU1MDM5NDUwNjY=", "number": 462, "title": "yarn mnist_mlp_estimator.py not work", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-10-08T09:54:12Z", "updated_at": "2019-10-09T10:53:43Z", "closed_at": "2019-10-09T10:53:43Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 2.7,\r\n - Spark version 2.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark version 1.4.4\r\n - Cluster version Hadoop 2.8\r\n\r\n\r\n**my pip list:**\r\nabsl-py              0.8.0\r\nastor                0.8.0\r\nbackports.weakref    1.0.post1\r\nenum34               1.1.6\r\nfuncsigs             1.0.2\r\nfutures              3.3.0\r\ngast                 0.3.2\r\ngoogle-pasta         0.1.7\r\ngrpcio               1.24.1\r\nh5py                 2.10.0\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\nMarkdown             3.1.1\r\nmock                 3.0.5\r\nnumpy                1.16.5\r\npip                  19.2.3\r\nprotobuf             3.10.0\r\nsetuptools           41.4.0\r\nsix                  1.12.0\r\ntensorboard          1.14.0\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntensorflowonspark    1.4.4\r\ntermcolor            1.1.0\r\nWerkzeug             0.16.0\r\nwheel                0.33.6\r\nwrapt                1.11.2\r\n\r\n\r\n**Describe the bug:**\r\nWhen i start mnist_mlp_estimator.py in standalone it works but in cluster yarn not works.\r\n\r\nthis is my principal log:\r\n/tmp/tmpSxaE3v/keras/keras_model.ckpt: Not found: /tmp/tmpSxaE3v/keras; No such file or directory\r\n         [[node checkpoint_initializer (defined at home/cloudbreak/mnist_mlp_estimator.py:106) ]]\r\n\r\nthe code is the same as the old mnist_mlp_estimator.py code that does not exist now.\r\n\r\n**Logs:**\r\n2019-10-08 09:43:49,979 INFO (MainThread-93465) Stopping TensorFlow nodes\r\n19/10/08 09:43:54 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, luca95-w1.tgcxxqymd3fejekkvr1k4kd3dg.fx.internal.cloudapp.net, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"/home/cloudbreak/Python/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 364, in _mapfn\r\n  File \"/home/cloudbreak/Python/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 323, in wrapper_fn\r\n  File \"/home/cloudbreak/mnist_mlp_estimator.py\", line 106, in main_fun\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 640, in run\r\n    getattr(self, task_to_run)()\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 677, in run_master\r\n    self._start_distributed_training(saving_listeners=saving_listeners)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 796, in _start_distributed_training\r\n    saving_listeners=saving_listeners)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\r\n    saving_listeners)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1480, in _train_with_estimator_spec\r\n    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 584, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 871, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 647, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 296, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: From /job:ps/replica:0/task:0:\r\nUnsuccessful TensorSliceReader constructor: Failed to get matching files on **/tmp/tmpSxaE3v/keras/keras_model.ckpt: Not found: /tmp/tmpSxaE3v/keras; No such file or directory\r\n         [[node checkpoint_initializer (defined at home/cloudbreak/mnist_mlp_estimator.py:106) ]]**\r\n\r\nOriginal stack trace for u'checkpoint_initializer':\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/pyspark.zip/pyspark/daemon.py\", line 180, in <module>\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/pyspark.zip/pyspark/daemon.py\", line 157, in manager\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/pyspark.zip/pyspark/daemon.py\", line 61, in worker\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"home/cloudbreak/Python/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 364, in _mapfn\r\n  File \"home/cloudbreak/Python/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 323, in wrapper_fn\r\n  File \"home/cloudbreak/mnist_mlp_estimator.py\", line 106, in main_fun\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 640, in run\r\n    getattr(self, task_to_run)()\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 677, in run_master\r\n    self._start_distributed_training(saving_listeners=saving_listeners)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py\", line 796, in _start_distributed_training\r\n    saving_listeners=saving_listeners)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\r\n    saving_listeners)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1365, in _train_with_estimator_spec\r\n    warm_starting_util.warm_start(*self._warm_start_settings)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/warm_starting_util.py\", line 476, in warm_start\r\n    checkpoint_utils.init_from_checkpoint(ckpt_to_initialize_from, vocabless_vars)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 291, in init_from_checkpoint\r\n    init_from_checkpoint_fn)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1684, in merge_call\r\n    return self._merge_call(merge_fn, args, kwargs)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1691, in _merge_call\r\n    return merge_fn(self._strategy, *args, **kwargs)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 286, in <lambda>\r\n    ckpt_dir_or_file, assignment_map)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 334, in _init_from_checkpoint\r\n    _set_variable_or_list_initializer(var, ckpt_file, tensor_name_in_ckpt)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 458, in _set_variable_or_list_initializer\r\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 412, in _set_checkpoint_initializer\r\n    ckpt_file, [tensor_name], [slice_spec], [base_type], name=name)[0]\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1696, in restore_v2\r\n    name=name)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"hadoopfs/fs3/yarn/nodemanager/usercache/cloudbreak/appcache/application_1570519936439_0017/container_e02_1570519936439_0017_01_000004/Python/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n        at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\n\r\n\r\nexport SPARK_MAJOR_VERSION=2\r\nexport PYTHON_ROOT=./Python\r\nexport LD_LIBRARY_PATH=${PATH}\r\nexport PYSPARK_PYTHON=${PYTHON_ROOT}/bin/python\r\nexport SPARK_YARN_USER_ENV=\"PYSPARK_PYTHON=Python/bin/python\"\r\nexport PATH=${PYTHON_ROOT}/bin/:$PATH\r\nexport QUEUE=default\r\n\r\n**Spark Submit Command Line:**\r\nspark-submit \\\r\n--master yarn \\\r\n--deploy-mode client \\\r\n--queue ${QUEUE} \\\r\n--num-executors 3 \\\r\n--executor-memory 4G \\\r\n--executor-cores 2 \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/cloudbreak/Python.zip#Python \\\r\n--jars hdfs:///user/cloudbreak/tensorflow-hadoop-1.10.0.jar \\\r\n/home/cloudbreak/mnist_mlp_estimator.py \r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/460", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/460/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/460/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/460/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/460", "id": 502538830, "node_id": "MDU6SXNzdWU1MDI1Mzg4MzA=", "number": 460, "title": "yarn cluster Cannot run program \"./Python/bin/python\"", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-10-04T09:46:55Z", "updated_at": "2019-10-07T15:54:49Z", "closed_at": "2019-10-07T15:54:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 2.7\r\n - Spark version 2.3.1\r\n - TensorFlow version 1.14\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version yarn, Hadoop 2.8\r\n\r\n**Describe the bug:**\r\nin this guide: https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN\r\n\r\nwhen i run the command below i have this error in log.\r\nbut in this directory the file is there.\r\n\r\n**Logs:**\r\nCaused by: java.io.IOException: Cannot run program \"./Python/bin/python\": error=2, No such file or directory\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue ${QUEUE} \\\r\n--num-executors 4 \\\r\n--executor-memory 2G \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/cloudbreak/Python.zip#Python \\\r\nTensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images mnist/csv/train/images \\\r\n--labels mnist/csv/train/labels \\\r\n--mode train \\\r\n--model mnist_model\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/459", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/459/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/459/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/459/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/459", "id": 502162367, "node_id": "MDU6SXNzdWU1MDIxNjIzNjc=", "number": 459, "title": "error in new example mnist_tf.py", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-10-03T16:14:21Z", "updated_at": "2019-10-08T07:18:22Z", "closed_at": "2019-10-08T07:18:21Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.3.1\r\n - TensorFlow version 2.0.0\r\n - TensorFlowOnSpark version 2.0.0\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nin this example: https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/mnist/keras\r\nI have this error in the logs and the job freezes\r\n\r\n**Logs:**\r\n2019-10-03 18:08:39,942 INFO (MainThread-23953) waiting for 3 reservations\r\n2019-10-03 18:08:40,943 INFO (MainThread-23953) waiting for 3 reservations\r\n2019-10-03 18:08:41,945 INFO (MainThread-23953) all reservations completed\r\n2019-10-03 18:08:41,945 INFO (MainThread-23953) All TFSparkNodes started\r\n2019-10-03 18:08:41,945 INFO (MainThread-23953) {'executor_id': 0, 'host': '192.168.163.174', 'job_name': 'chief', 'task_index': 0, 'port': 46195, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-r7i19oxo/listener-yl_0xmfk', 'authkey': b'H\\xb6\\xf8s\\x82\\x03L/\\x85\\x96\\x11\\xa9S\\xe6\\x85\\x82'}\r\n2019-10-03 18:08:41,945 INFO (MainThread-23953) {'executor_id': 2, 'host': '192.168.163.174', 'job_name': 'worker', 'task_index': 1, 'port': 34549, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-udp865_z/listener-xot9cc_x', 'authkey': b'\\xe0\\xa2+\\x82\\xe3!A\\x8b\\xa8d\\xd0k\\xdc1\\xa62'}\r\n2019-10-03 18:08:41,945 INFO (MainThread-23953) {'executor_id': 1, 'host': '192.168.163.174', 'job_name': 'worker', 'task_index': 0, 'port': 40825, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-vtxyboh5/listener-z3aetien', 'authkey': b'\\xc3\\x89~\\x1c{\\xf2B\\xd0\\x90T\\xf1x \\xa0\\xccW'}\r\n2019-10-03 18:08:41,945 INFO (MainThread-23953) Stopping TensorFlow nodes\r\n19/10/03 18:09:22 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, 192.168.163.174, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 365, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 324, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/luca/TensorFlowOnSpark/examples/mnist/keras/mnist_tf.py\", line 61, in main_fun\r\n    multi_worker_model.fit(x=train_datasets, epochs=args.epochs, steps_per_epoch=args.steps_per_epoch, callbacks=callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 631, in fit\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 625, in _worker_fn\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 419, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 311, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 969, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 1020, in _save_model\r\n    self._maybe_remove_file(file_handle, filepath)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 1048, in _maybe_remove_file\r\n    os.remove(filepath)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp356umm3s.'\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/10/03 18:09:22 INFO TaskSetManager: Starting task 2.1 in stage 0.0 (TID 3, 192.168.163.174, executor 0, partition 2, PROCESS_LOCAL, 7856 bytes)\r\n19/10/03 18:09:22 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, 192.168.163.174, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  [Previous line repeated 1 more time]\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 365, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py\", line 324, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/luca/TensorFlowOnSpark/examples/mnist/keras/mnist_tf.py\", line 61, in main_fun\r\n    multi_worker_model.fit(x=train_datasets, epochs=args.epochs, steps_per_epoch=args.steps_per_epoch, callbacks=callbacks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 631, in fit\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 625, in _worker_fn\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 143, in fit_distributed\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 419, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 311, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 969, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 1020, in _save_model\r\n    self._maybe_remove_file(file_handle, filepath)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\", line 1048, in _maybe_remove_file\r\n    os.remove(filepath)\r\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpjf5cythu.'\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/10/03 18:09:22 INFO TaskSetManager: Starting task 1.1 in stage 0.0 (TID 4, 192.168.163.174, executor 2, partition 1, PROCESS_LOCAL, 7856 bytes)\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n${TFoS_HOME}/examples/mnist/keras/mnist_tf.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--model_dir ${TFoS_HOME}/mnist_model \\\r\n--export_dir ${TFoS_HOME}/mnist_export\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/456", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/456/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/456/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/456/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/456", "id": 497470536, "node_id": "MDU6SXNzdWU0OTc0NzA1MzY=", "number": 456, "title": "The mnist distributed train example raise Exception: Timeout while feeding partition", "user": {"login": "linjiaqin", "id": 17450813, "node_id": "MDQ6VXNlcjE3NDUwODEz", "avatar_url": "https://avatars3.githubusercontent.com/u/17450813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linjiaqin", "html_url": "https://github.com/linjiaqin", "followers_url": "https://api.github.com/users/linjiaqin/followers", "following_url": "https://api.github.com/users/linjiaqin/following{/other_user}", "gists_url": "https://api.github.com/users/linjiaqin/gists{/gist_id}", "starred_url": "https://api.github.com/users/linjiaqin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linjiaqin/subscriptions", "organizations_url": "https://api.github.com/users/linjiaqin/orgs", "repos_url": "https://api.github.com/users/linjiaqin/repos", "events_url": "https://api.github.com/users/linjiaqin/events{/privacy}", "received_events_url": "https://api.github.com/users/linjiaqin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2019-09-24T05:49:55Z", "updated_at": "2020-07-14T19:59:31Z", "closed_at": "2019-12-03T22:51:36Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [ 3.6]\r\n - Spark version [2.4.3]\r\n - TensorFlow version [1.14]\r\n - TensorFlowOnSpark version [1.4.3]\r\n - Cluster version [hadoop 3.1.4]\r\n\r\n**Describe the bug:**\r\nwhen I run the mnist train example in spark standalone, it seized up and stop. It occurs   \r\n\r\n**Logs:**\r\n2019-09-24 13:31:11,285 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\r\nargs: Namespace(batch_size=100, cluster_size=2, epochs=1, format='csv', images='examples/mnist/csv/train/images', labels='examples/mnist/csv/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=1000, tensorboard=False)\r\n2019-09-24T13:31:11.331185 ===== Start\r\n2019-09-24 13:31:11,560 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 242.1 KB, free 2.5 GB)\r\n2019-09-24 13:31:11,634 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.5 KB, free 2.5 GB)\r\n2019-09-24 13:31:11,636 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on cu11:38440 (size: 23.5 KB, free: 2.5 GB)\r\n2019-09-24 13:31:11,642 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n2019-09-24 13:31:11,710 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 242.2 KB, free 2.5 GB)\r\n2019-09-24 13:31:11,723 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 23.5 KB, free 2.5 GB)\r\n2019-09-24 13:31:11,726 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on cu11:38440 (size: 23.5 KB, free: 2.5 GB)\r\n2019-09-24 13:31:11,727 INFO spark.SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\r\nzipping images and labels\r\n2019-09-24 13:31:11,770 INFO mapred.FileInputFormat: Total input paths to process : 10\r\n2019-09-24 13:31:11,788 INFO mapred.FileInputFormat: Total input paths to process : 10\r\n2019-09-24 13:31:11,820 INFO (MainThread-15780) Reserving TFSparkNodes \r\n2019-09-24 13:31:11,821 INFO (MainThread-15780) cluster_template: {'ps': [0], 'worker': [1]}\r\n2019-09-24 13:31:11,822 INFO (MainThread-15780) listening for reservations at ('192.168.0.11', 46274)\r\n2019-09-24 13:31:11,823 INFO (MainThread-15780) Starting TensorFlow on executors\r\n2019-09-24 13:31:11,833 INFO (MainThread-15780) Waiting for TFSparkNodes to start\r\n2019-09-24 13:31:11,833 INFO (MainThread-15780) waiting for 2 reservations\r\n2019-09-24 13:31:11,903 INFO spark.SparkContext: Starting job: foreachPartition at /software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:321\r\n2019-09-24 13:31:11,922 INFO scheduler.DAGScheduler: Got job 0 (foreachPartition at /software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:321) with 2 output partitions\r\n2019-09-24 13:31:11,923 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:321)\r\n2019-09-24 13:31:11,923 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n2019-09-24 13:31:11,925 INFO scheduler.DAGScheduler: Missing parents: List()\r\n2019-09-24 13:31:11,930 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[8] at foreachPartition at /software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:321), which has no missing parents\r\n2019-09-24 13:31:11,953 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 16.3 KB, free 2.5 GB)\r\n2019-09-24 13:31:11,956 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.5 KB, free 2.5 GB)\r\n2019-09-24 13:31:11,957 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on cu11:38440 (size: 11.5 KB, free: 2.5 GB)\r\n2019-09-24 13:31:11,958 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161\r\n2019-09-24 13:31:11,971 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[8] at foreachPartition at /software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:321) (first 15 tasks are for partitions Vector(0, 1))\r\n2019-09-24 13:31:11,972 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\r\n2019-09-24 13:31:12,834 INFO (MainThread-15780) waiting for 2 reservations\r\n2019-09-24 13:31:12,869 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.8:45566) with ID 0\r\n2019-09-24 13:31:12,891 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.0.8, executor 0, partition 0, PROCESS_LOCAL, 7856 bytes)\r\n2019-09-24 13:31:13,042 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.0.7:42286) with ID 1\r\n2019-09-24 13:31:13,045 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 192.168.0.7, executor 1, partition 1, PROCESS_LOCAL, 7856 bytes)\r\n2019-09-24 13:31:13,056 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.8:35705 with 366.3 MB RAM, BlockManagerId(0, 192.168.0.8, 35705, None)\r\n2019-09-24 13:31:13,197 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.0.7:45057 with 366.3 MB RAM, BlockManagerId(1, 192.168.0.7, 45057, None)\r\n2019-09-24 13:31:13,341 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.8:35705 (size: 11.5 KB, free: 366.3 MB)\r\n2019-09-24 13:31:13,475 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.0.7:45057 (size: 11.5 KB, free: 366.3 MB)\r\n2019-09-24 13:31:13,836 INFO (MainThread-15780) waiting for 2 reservations\r\n2019-09-24 13:31:14,837 INFO (MainThread-15780) waiting for 2 reservations\r\n2019-09-24 13:31:15,838 INFO (MainThread-15780) waiting for 2 reservations\r\n2019-09-24 13:31:16,840 INFO (MainThread-15780) waiting for 2 reservations\r\n2019-09-24 13:31:17,841 INFO (MainThread-15780) all reservations completed\r\n2019-09-24 13:31:17,841 INFO (MainThread-15780) All TFSparkNodes started\r\n2019-09-24 13:31:17,841 INFO (MainThread-15780) {'executor_id': 0, 'host': '192.168.0.8', 'job_name': 'ps', 'task_index': 0, 'port': 44715, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.0.8', 46408), 'authkey': b'o\\x1d\\xd6\\xbeT\\xa6O\\x15\\x82\\x8f\\x87\\x0f\\x8e\\x91\\xce\\x96'}\r\n2019-09-24 13:31:17,841 INFO (MainThread-15780) {'executor_id': 1, 'host': '192.168.0.7', 'job_name': 'worker', 'task_index': 0, 'port': 33719, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-9xkwcw7q/listener-suknqjyv', 'authkey': b'\\xc0r@\\xd7Y\\x9fHT\\x89 \\xc6~N\\xdeNj'}\r\n2019-09-24 13:31:17,841 INFO (MainThread-15780) Feeding training data\r\n2019-09-24 13:31:17,902 INFO spark.SparkContext: Starting job: collect at PythonRDD.scala:166\r\n2019-09-24 13:31:17,904 INFO scheduler.DAGScheduler: Got job 1 (collect at PythonRDD.scala:166) with 10 output partitions\r\n2019-09-24 13:31:17,904 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at PythonRDD.scala:166)\r\n2019-09-24 13:31:17,904 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n2019-09-24 13:31:17,904 INFO scheduler.DAGScheduler: Missing parents: List()\r\n2019-09-24 13:31:17,905 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[10] at RDD at PythonRDD.scala:53), which has no missing parents\r\n2019-09-24 13:31:17,917 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.6 KB, free 2.5 GB)\r\n2019-09-24 13:31:17,920 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.0 KB, free 2.5 GB)\r\n2019-09-24 13:31:17,921 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on cu11:38440 (size: 9.0 KB, free: 2.5 GB)\r\n2019-09-24 13:31:17,922 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161\r\n2019-09-24 13:31:17,923 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 1 (PythonRDD[10] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\r\n2019-09-24 13:31:17,923 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 10 tasks\r\n2019-09-24 13:31:18,240 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 192.168.0.7, executor 1, partition 0, ANY, 8535 bytes)\r\n2019-09-24 13:31:18,245 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5198 ms on 192.168.0.7 (executor 1) (1/2)\r\n2019-09-24 13:31:18,253 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 54429\r\n2019-09-24 13:31:18,321 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.0.7:45057 (size: 9.0 KB, free: 366.3 MB)\r\n2019-09-24 13:31:18,392 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.7:45057 (size: 23.5 KB, free: 366.3 MB)\r\n2019-09-24 13:31:19,452 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.0.7:45057 (size: 23.5 KB, free: 366.2 MB)\r\n2019-09-24 13:41:21,586 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 192.168.0.7, executor 1, partition 1, ANY, 8535 bytes)\r\n2019-09-24 13:41:21,606 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2, 192.168.0.7, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/local/spark-2.4.3/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/local/spark-2.4.3/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/local/spark-2.4.3/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/local/spark-2.4.3/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/local/spark-2.4.3/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/usr/local/spark-2.4.3/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/usr/local/spark-2.4.3/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n\r\n\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master spark://cu11:7077 \\\r\n--py-files ${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--images examples/mnist/csv/train/images \\\r\n--labels examples/mnist/csv/train/labels \\\r\n--format csv \\\r\n--mode train \\\r\n--model mnist_model\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/455", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/455/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/455/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/455/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/455", "id": 497441975, "node_id": "MDU6SXNzdWU0OTc0NDE5NzU=", "number": 455, "title": "how to schedule worker node to use gpu?", "user": {"login": "pl-lee", "id": 51313587, "node_id": "MDQ6VXNlcjUxMzEzNTg3", "avatar_url": "https://avatars1.githubusercontent.com/u/51313587?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pl-lee", "html_url": "https://github.com/pl-lee", "followers_url": "https://api.github.com/users/pl-lee/followers", "following_url": "https://api.github.com/users/pl-lee/following{/other_user}", "gists_url": "https://api.github.com/users/pl-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/pl-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pl-lee/subscriptions", "organizations_url": "https://api.github.com/users/pl-lee/orgs", "repos_url": "https://api.github.com/users/pl-lee/repos", "events_url": "https://api.github.com/users/pl-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/pl-lee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-24T04:05:06Z", "updated_at": "2019-09-29T02:40:26Z", "closed_at": "2019-09-29T02:40:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "Dear all:\r\nThere is a cluster of two nodes. And each node has a gpu, eight cpu cores and 32G memory.The question is how to achieve allocation of spark worker that one ps node using cpu and the other two worker node using gpu. Anyone could tell me? Many Thanks!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/454", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/454/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/454/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/454/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/454", "id": 496972380, "node_id": "MDU6SXNzdWU0OTY5NzIzODA=", "number": 454, "title": "The mnist training example  couldn't work on yarn", "user": {"login": "linjiaqin", "id": 17450813, "node_id": "MDQ6VXNlcjE3NDUwODEz", "avatar_url": "https://avatars3.githubusercontent.com/u/17450813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linjiaqin", "html_url": "https://github.com/linjiaqin", "followers_url": "https://api.github.com/users/linjiaqin/followers", "following_url": "https://api.github.com/users/linjiaqin/following{/other_user}", "gists_url": "https://api.github.com/users/linjiaqin/gists{/gist_id}", "starred_url": "https://api.github.com/users/linjiaqin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linjiaqin/subscriptions", "organizations_url": "https://api.github.com/users/linjiaqin/orgs", "repos_url": "https://api.github.com/users/linjiaqin/repos", "events_url": "https://api.github.com/users/linjiaqin/events{/privacy}", "received_events_url": "https://api.github.com/users/linjiaqin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-09-23T08:57:58Z", "updated_at": "2019-12-03T22:52:00Z", "closed_at": "2019-12-03T22:52:00Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [e.g.3.6]\r\n - Spark version [e.g. 2.4.3]\r\n - TensorFlow version [e.g. 1.14]\r\n - TensorFlowOnSpark version [e.g. 1.1, 1.3.2]\r\n - Cluster version [e.g. yarn, Hadoop 3.1.2]\r\n\r\n**Describe the bug:**\r\nwhen i submit the example with spark_submit on yarn, it seems it can read file from hdfs\r\n\r\n**Logs:**\r\nthe yarn logs:\r\n(unable to get root cause for java.lang.NoClassDefFoundError)\r\n(unable to get stack trace for java.lang.NoClassDefFoundError)\r\nhdfsBuilderConnect(forceNewInstance=0, nn=cu11:9000, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:\r\n(unable to get root cause for java.lang.NoClassDefFoundError)\r\n(unable to get stack trace for java.lang.NoClassDefFoundError)\r\nhdfsBuilderConnect(forceNewInstance=0, nn=cu11:9000, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:\r\n(unable to get root cause for java.lang.NoClassDefFoundError)\r\n(unable to get stack trace for java.lang.NoClassDefFoundError)\r\nhdfsBuilderConnect(forceNewInstance=0, nn=cu11:9000, port=0, kerbTicketCachePath=(NULL), userName=(NULL)) error:\r\n\r\nIt continue to logs below:\r\n2019-09-24 14:01:39,144 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:40,152 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:41,154 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:42,155 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:43,157 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:44,158 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:45,161 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:46,163 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:47,165 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:48,167 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n2019-09-24 14:01:49,168 INFO yarn.Client: Application report for application_1569303309318_0006 (state: ACCEPTED)\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue ${QUEUE} \\\r\n--num-executors 2 \\\r\n--executor-memory 4G \\\r\n--py-files TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nTensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images mnist/csv/train/images \\\r\n--labels mnist/csv/train/labels \\\r\n--mode train \\\r\n--model mnist_model\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/453", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/453/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/453/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/453/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/453", "id": 496938466, "node_id": "MDU6SXNzdWU0OTY5Mzg0NjY=", "number": 453, "title": "can't import other dependency python files with spark submit command ", "user": {"login": "linjiaqin", "id": 17450813, "node_id": "MDQ6VXNlcjE3NDUwODEz", "avatar_url": "https://avatars3.githubusercontent.com/u/17450813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linjiaqin", "html_url": "https://github.com/linjiaqin", "followers_url": "https://api.github.com/users/linjiaqin/followers", "following_url": "https://api.github.com/users/linjiaqin/following{/other_user}", "gists_url": "https://api.github.com/users/linjiaqin/gists{/gist_id}", "starred_url": "https://api.github.com/users/linjiaqin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linjiaqin/subscriptions", "organizations_url": "https://api.github.com/users/linjiaqin/orgs", "repos_url": "https://api.github.com/users/linjiaqin/repos", "events_url": "https://api.github.com/users/linjiaqin/events{/privacy}", "received_events_url": "https://api.github.com/users/linjiaqin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-23T07:37:19Z", "updated_at": "2019-10-04T16:40:59Z", "closed_at": "2019-10-04T16:40:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [2.4.3]\r\n - TensorFlow version [1.14.0]\r\n - TensorFlowOnSpark version []\r\n - Cluster version [yarn\uff0c3.1.2]\r\n\r\n**Describe the bug:**\r\nWhen I run the yarn distruted training example, it can't import the mnist_dict. But  It was included in the spark submit command with the parameter \"--py-files\".\r\n\r\n**Logs:**\r\nrecent failure: Lost task 1.3 in stage 0.0 (TID 4, cu09, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/local/hadoop-3.1.2/tmp/nm-local-dir/usercache/hadoop3/appcache/application_1567925102183_0034/container_1567925102183_0034_01_000002/pyspark.zip/pyspark/worker.py\", line 364, in main\r\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\r\n  File \"/usr/local/hadoop-3.1.2/tmp/nm-local-dir/usercache/hadoop3/appcache/application_1567925102183_0034/container_1567925102183_0034_01_000002/pyspark.zip/pyspark/worker.py\", line 69, in read_command\r\n    command = serializer._read_with_length(file)\r\n  File \"/usr/local/hadoop-3.1.2/tmp/nm-local-dir/usercache/hadoop3/appcache/application_1567925102183_0034/container_1567925102183_0034_01_000002/pyspark.zip/pyspark/serializers.py\", line 172, in _read_with_length\r\n    return self.loads(obj)\r\n  File \"/usr/local/hadoop-3.1.2/tmp/nm-local-dir/usercache/hadoop3/appcache/application_1567925102183_0034/container_1567925102183_0034_01_000002/pyspark.zip/pyspark/serializers.py\", line 580, in loads\r\n    return pickle.loads(obj, encoding=encoding)\r\nModuleNotFoundError: No module named 'mnist_dist'\r\n\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue ${QUEUE} \\\r\n--num-executors 2 \\\r\n--executor-memory 4G \\\r\n--py-files TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nTensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images mnist/csv/train/images \\\r\n--labels mnist/csv/train/labels \\\r\n--mode train \\\r\n--model mnist_model\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/452", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/452/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/452/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/452/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/452", "id": 496876608, "node_id": "MDU6SXNzdWU0OTY4NzY2MDg=", "number": 452, "title": "when I run the \"Convert the MNIST zip files into HDFS files\" example provided by GetStarted_YARN. it occured errors.", "user": {"login": "linjiaqin", "id": 17450813, "node_id": "MDQ6VXNlcjE3NDUwODEz", "avatar_url": "https://avatars3.githubusercontent.com/u/17450813?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linjiaqin", "html_url": "https://github.com/linjiaqin", "followers_url": "https://api.github.com/users/linjiaqin/followers", "following_url": "https://api.github.com/users/linjiaqin/following{/other_user}", "gists_url": "https://api.github.com/users/linjiaqin/gists{/gist_id}", "starred_url": "https://api.github.com/users/linjiaqin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linjiaqin/subscriptions", "organizations_url": "https://api.github.com/users/linjiaqin/orgs", "repos_url": "https://api.github.com/users/linjiaqin/repos", "events_url": "https://api.github.com/users/linjiaqin/events{/privacy}", "received_events_url": "https://api.github.com/users/linjiaqin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-23T03:16:22Z", "updated_at": "2019-09-24T16:42:49Z", "closed_at": "2019-09-24T16:42:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [ 3.6]\r\n - Spark version [ 2.4.3]\r\n - TensorFlow version [e.g. 1.14.0]\r\n - TensorFlowOnSpark version [e.g. 1.1, 1.3.2]\r\n - Cluster version [Hadoop 3.1.2, yarn]\r\n\r\n**Describe the bug:**\r\nwhen I run the \"Convert the MNIST zip files into HDFS files\" example provided by GetStarted_YARN. it occured errors.\r\n\r\n**Logs:**\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting mnist/train-images-idx3-ubyte.gz\r\nTraceback (most recent call last):\r\n  File \"mnist_data_setup.py\", line 143, in <module>\r\n    writeMNIST(sc, \"mnist/train-images-idx3-ubyte.gz\", \"mnist/train-labels-idx1-ubyte.gz\", args.output + \"/train\", args.format, args.num_partitions)\r\n  File \"mnist_data_setup.py\", line 48, in writeMNIST\r\n    images = numpy.array(mnist.extract_images(f))\r\n  File \"/software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 62, in extract_images\r\n    magic = _read32(bytestream)\r\n  File \"/software/hadoop3/anaconda3/envs/zoo/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\", line 43, in _read32\r\n    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\r\nIndexError: index 0 is out of bounds for axis 0 with size 0\r\n\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue ${QUEUE} \\\r\n--num-executors 2 \\\r\n--executor-memory 4G \\\r\n--archives mnist/mnist.zip#mnist \\\r\nTensorFlowOnSpark/examples/mnist/mnist_data_setup.py \\\r\n--output mnist/csv \\\r\n--format csv", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/451", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/451/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/451/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/451/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/451", "id": 494546583, "node_id": "MDU6SXNzdWU0OTQ1NDY1ODM=", "number": 451, "title": "mnist_mlp_estimator.py different speed in tensorflow mode and spark mode", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-17T10:46:19Z", "updated_at": "2019-09-18T07:13:28Z", "closed_at": "2019-09-18T07:13:28Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n- Python version 3.6\r\n- Spark version 2.4.3\r\n- TensorFlow version 1.14.0\r\n- TensorFlowOnSpark version 1.4.3\r\n- Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nin this example: https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_mlp_estimator.py\r\n\r\nwhy starting in spark mode is slower (about 10 minutes) and starting in tensorflow mode is faster (about 1,2 minutes)?\r\n\r\n\r\ncommand tensorflow mode: \r\n\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/keras/mnist_mlp_estimator.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--input_mode tf \\\r\n--model_dir ${TFoS_HOME}/mnist_model \\\r\n--epochs 5 \\\r\n--tensorboard\r\n\r\ncommand spark mode:\r\n\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/keras/mnist_mlp_estimator.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--input_mode spark \\\r\n--images ${TFoS_HOME}/mnist/csv/train/images \\\r\n--labels ${TFoS_HOME}/mnist/csv/train/labels \\\r\n--epochs 5 \\\r\n--model_dir ${TFoS_HOME}/mnist_model \\\r\n--tensorboard", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/450", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/450/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/450/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/450/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/450", "id": 493915475, "node_id": "MDU6SXNzdWU0OTM5MTU0NzU=", "number": 450, "title": "insert two rdd in cluster.train", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-16T08:37:46Z", "updated_at": "2019-09-18T07:10:58Z", "closed_at": "2019-09-18T07:10:58Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.4.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark version 1.4.3\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nin this example: https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_mlp_estimator.py\r\n\r\nIn spark mode, in cluster.train(dataRDD, args.epochs), it's possible send two dataRDD?\r\n\r\nfor example i create another rdd:\r\n\r\nimages_test = sc.textFile('/home/luca/TensorFlowOnSpark/examples/mnist/csv/test/images').map(lambda ln: [float(x) for x in ln.split(',')])\r\nlabels_test = sc.textFile('/home/luca/TensorFlowOnSpark/examples/mnist/csv/test/labels').map(lambda ln: [float(x) for x in ln.split(',')])\r\ndataRDD_test = images_test.zip(labels_test)\r\n\r\nhow to insert, in cluster.train, dataRDD and dataRDD_test? it's possible?\r\nand after, how to distinguish the two rdd in main_fun?\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/449", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/449/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/449/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/449/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/449", "id": 492625376, "node_id": "MDU6SXNzdWU0OTI2MjUzNzY=", "number": 449, "title": "Run distributed MNIST training (using InputMode.SPARK) fails with : Timeout while feeding partition", "user": {"login": "nihavend", "id": 5983818, "node_id": "MDQ6VXNlcjU5ODM4MTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/5983818?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nihavend", "html_url": "https://github.com/nihavend", "followers_url": "https://api.github.com/users/nihavend/followers", "following_url": "https://api.github.com/users/nihavend/following{/other_user}", "gists_url": "https://api.github.com/users/nihavend/gists{/gist_id}", "starred_url": "https://api.github.com/users/nihavend/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nihavend/subscriptions", "organizations_url": "https://api.github.com/users/nihavend/orgs", "repos_url": "https://api.github.com/users/nihavend/repos", "events_url": "https://api.github.com/users/nihavend/events{/privacy}", "received_events_url": "https://api.github.com/users/nihavend/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-09-12T07:07:28Z", "updated_at": "2019-09-24T17:08:53Z", "closed_at": "2019-09-24T17:08:53Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [2.7.5]\r\n - Spark version [2.3.1.3.0.1.0-187]\r\n - TensorFlow version [1.12.0]\r\n - TensorFlowOnSpark version [1.4.3]\r\n - Cluster version [Ambari HDP-3.0.1.0]\r\n\r\n**Describe the bug:**\r\nAfter running Convert the MNIST zip files into HDFS files successfully, I tried on Run distributed MNIST training (using InputMode.SPARK) but failed.\r\n\r\n**Logs:**\r\n\r\n      /tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 417, in _train\r\n     Exception: Timeout while feeding partition at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n**Spark Submit Command Line:**\r\n\r\n#for CPU mode:\r\n#export QUEUE=default\r\n#remove references to $LIB_CUDA\r\n#hadoop fs -rm -r mnist_model\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue ${QUEUE} \\\r\n--num-executors 4 \\\r\n--executor-memory 27G \\\r\n--py-files /bigdata/github/TensorFlowOnSpark/tfspark.zip,/bigdata/github/TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\n/bigdata/github/TensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images mnist/csv/train/images \\\r\n--labels mnist/csv/train/labels \\\r\n--mode train \\\r\n--model mnist_mode\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/448", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/448/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/448/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/448/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/448", "id": 492155003, "node_id": "MDU6SXNzdWU0OTIxNTUwMDM=", "number": 448, "title": "save a file in main_fun", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-09-11T10:35:56Z", "updated_at": "2019-09-16T08:06:31Z", "closed_at": "2019-09-16T08:06:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.4.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark version 1.4.3\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nin this example: https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_mlp_estimator.py\r\n\r\nhow can i save a string in a text file, in main_fun function? \r\n\r\nbecause to do a test i add this code in main_fun:\r\n\r\ntext_file = open(\"Output.txt\", \"w\")\r\ntext_file.write(\"my example\")\r\ntext_file.close()\r\n\r\nbut this file is not created.\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/446", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/446/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/446/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/446/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/446", "id": 488471655, "node_id": "MDU6SXNzdWU0ODg0NzE2NTU=", "number": 446, "title": "where is the result of the evaluate when i run MNIST MLP using InputMode.SPARK", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-09-03T08:46:00Z", "updated_at": "2019-09-04T07:19:46Z", "closed_at": "2019-09-04T07:19:12Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.4.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark version 1.4.3\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nI'm following this guide: https://github.com/yahoo/TensorFlowOnSpark/tree/master/examples/mnist/keras\r\n\r\nwhere is the result of the prediction?\r\n\r\n**Logs:**\r\nIf applicable, add logs to help explain your problem.  Note: errors may not be fully described in the driver/console logs.  Make sure to check the executor logs for possible root causes.\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/keras/mnist_mlp_estimator.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--input_mode spark \\\r\n--images ${TFoS_HOME}/mnist/csv/train/images \\\r\n--labels ${TFoS_HOME}/mnist/csv/train/labels \\\r\n--epochs 5 \\\r\n--model_dir ${TFoS_HOME}/mnist_model \\\r\n--tensorboard\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/445", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/445/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/445/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/445/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/445", "id": 486879414, "node_id": "MDU6SXNzdWU0ODY4Nzk0MTQ=", "number": 445, "title": "waiting for 2 reservations", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-29T10:38:12Z", "updated_at": "2019-09-03T07:37:35Z", "closed_at": "2019-09-03T07:36:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.4.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark version 1.4.3\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nwhen I start the code send me this message (2019-08-29 03:26:14,649 INFO (MainThread-34844) waiting for 2 reservations) until the timeout.\r\n\r\nI started the cluster using this site: https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_standalone\r\n\r\nI was writing an example to try this guide: https://github.com/yahoo/TensorFlowOnSpark/wiki/Conversion-Guide\r\n\r\ni'm using jupyter notebook.\r\nthis is my sparkcontext:\r\n\r\nSparkContext\r\nSpark UI\r\nVersion\r\n    v2.4.3\r\nMaster\r\n    spark://ubuntu:7077\r\nAppName\r\n    PySparkShell\r\n\r\n\r\nThis my code:\r\n`\r\ndef main_fun(argv, ctx):\r\n\r\n    import tensorflow as tf\r\n    mnist = tf.keras.datasets.mnist\r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n    model = tf.keras.models.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dropout(0.2),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    model.fit(x_train, y_train, epochs=5)\r\n\r\n    model.evaluate(x_test, y_test)\r\n\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.conf import SparkConf\r\nfrom tensorflowonspark import TFCluster\r\nimport argparse\r\n\r\nexecutors = sc._conf.get(\"spark.executor.instances\")\r\nnum_executors=2\r\nparser = argparse.ArgumentParser()\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--cluster_size\", help=\"number of nodes in the cluster (for Spark Standalone)\", type=int, default=num_executors)\r\nparser.add_argument(\"--num_ps\", help=\"number of parameter servers\", type=int, default=1)\r\nparser.add_argument(\"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\r\nargs, unknown = parser.parse_known_args()\r\ncluster = TFCluster.run(sc, main_fun, args, args.cluster_size, args.num_ps, args.tensorboard, TFCluster.InputMode.TENSORFLOW)\r\n`\r\n\r\n\r\n**Logs:**\r\n2019-08-29 03:26:11,613 INFO (MainThread-34844) Reserving TFSparkNodes \r\n2019-08-29 03:26:11,615 INFO (MainThread-34844) cluster_template: {'ps': [0], 'worker': [1]}\r\n2019-08-29 03:26:11,618 INFO (MainThread-34844) listening for reservations at ('192.168.163.128', 35149)\r\n2019-08-29 03:26:11,622 INFO (MainThread-34844) Starting TensorFlow on executors\r\n2019-08-29 03:26:11,635 INFO (MainThread-34844) Waiting for TFSparkNodes to start\r\n2019-08-29 03:26:11,637 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:12,643 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:13,646 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:14,649 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:15,652 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:16,654 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:17,656 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:18,658 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:19,661 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:20,666 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:21,670 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:22,673 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:23,676 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:24,679 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:25,682 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:26,685 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:27,689 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:28,692 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:29,696 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:30,699 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:31,701 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:32,704 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:33,707 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:34,710 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:35,712 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:36,715 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:37,718 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:38,722 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:39,724 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:40,727 INFO (MainThread-34844) waiting for 2 reservations\r\n2019-08-29 03:26:41,730 INFO (MainThread-34844) waiting for 2 reservations\r\n\r\n\r\n**Spark Submit Command Line:**\r\n# Launch Spark Standalone cluster\r\nexport MASTER=spark://$(hostname):7077\r\nexport SPARK_WORKER_INSTANCES=2\r\nexport CORES_PER_WORKER=1 \r\nexport TOTAL_CORES=$((${CORES_PER_WORKER}*${SPARK_WORKER_INSTANCES})) \r\n${SPARK_HOME}/sbin/start-master.sh; ${SPARK_HOME}/sbin/start-slave.sh -c $CORES_PER_WORKER -m 3G ${MASTER}\r\n\r\n# Launch Jupyter notebook on Spark master node.\r\npushd ${TFoS_HOME}/examples/mnist\r\nPYSPARK_DRIVER_PYTHON=\"jupyter\" \\\r\nPYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" \\\r\npyspark  --master ${MASTER} \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--py-files ${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\"", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/443", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/443/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/443/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/443/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/443", "id": 486253466, "node_id": "MDU6SXNzdWU0ODYyNTM0NjY=", "number": 443, "title": "Convert the MNIST zip files using Spark", "user": {"login": "lucapas", "id": 27735311, "node_id": "MDQ6VXNlcjI3NzM1MzEx", "avatar_url": "https://avatars0.githubusercontent.com/u/27735311?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucapas", "html_url": "https://github.com/lucapas", "followers_url": "https://api.github.com/users/lucapas/followers", "following_url": "https://api.github.com/users/lucapas/following{/other_user}", "gists_url": "https://api.github.com/users/lucapas/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucapas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucapas/subscriptions", "organizations_url": "https://api.github.com/users/lucapas/orgs", "repos_url": "https://api.github.com/users/lucapas/repos", "events_url": "https://api.github.com/users/lucapas/events{/privacy}", "received_events_url": "https://api.github.com/users/lucapas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-08-28T09:11:17Z", "updated_at": "2019-08-28T16:08:17Z", "closed_at": "2019-08-28T16:08:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6.8\r\n - Spark version 2.4.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark version 1.4.3\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nI\u2019m following the guide: https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_standalone\r\n\r\nWhen i run: \r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n${TFoS_HOME}/examples/mnist/mnist_data_setup.py \\\r\n--output examples/mnist/csv \\\r\n--format csv\r\n\r\nI have this error in the logs\r\n\r\n**Logs:**\r\n19/08/28 01:48:32 ERROR SparkHadoopWriter: Aborting job job_20190828014751_0004.\r\njava.io.IOException: Failed to rename DeprecatedRawLocalFileStatus{path=file:/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000009/part-00009; isDirectory=false; length=10449019; replication=1; blocksize=33554432; modification_time=1566982111000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/part-00009\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:415)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:428)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:362)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:166)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:94)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000009/part-00009]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000009/.part-00009.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000005/.part-00005.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000005/part-00005]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000003/part-00003]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000003/.part-00003.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000008/.part-00008.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000008/part-00008]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000004/part-00004]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000004/.part-00004.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000007/part-00007]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000007/.part-00007.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000002/part-00002]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000002/.part-00002.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000006/part-00006]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000006/.part-00006.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000000/.part-00000.crc]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000000/part-00000]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000001/part-00001]: it still exists.\r\n19/08/28 01:48:32 WARN FileUtil: Failed to delete file or dir [/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000001/.part-00001.crc]: it still exists.\r\nTraceback (most recent call last):\r\n  File \"/home/luca/TensorFlowOnSpark/examples/mnist/mnist_data_setup.py\", line 146, in <module>\r\n    writeMNIST(sc, \"mnist/train-images-idx3-ubyte.gz\", \"mnist/train-labels-idx1-ubyte.gz\", args.output + \"/train\", args.format, args.num_partitions)\r\n  File \"/home/luca/TensorFlowOnSpark/examples/mnist/mnist_data_setup.py\", line 72, in writeMNIST\r\n    imageRDD.map(toCSV).saveAsTextFile(output_images)\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 1570, in saveAsTextFile\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\r\n  File \"/home/luca/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o29.saveAsTextFile.\r\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Failed to rename DeprecatedRawLocalFileStatus{path=file:/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/_temporary/0/task_20190828014751_0004_m_000009/part-00009; isDirectory=false; length=10449019; replication=1; blocksize=33554432; modification_time=1566982111000; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false} to file:/home/luca/TensorFlowOnSpark/examples/mnist/csv/train/images/part-00009\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:415)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:428)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:362)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:166)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:94)\r\n\t... 41 more\r\n\r\n19/08/28 01:48:32 INFO SparkContext: Invoking stop() from shutdown hook\r\n19/08/28 01:48:32 INFO SparkUI: Stopped Spark web UI at http://192.168.88.203:4040\r\n19/08/28 01:48:32 INFO StandaloneSchedulerBackend: Shutting down all executors\r\n19/08/28 01:48:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\r\n19/08/28 01:48:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\r\n19/08/28 01:48:32 INFO MemoryStore: MemoryStore cleared\r\n19/08/28 01:48:32 INFO BlockManager: BlockManager stopped\r\n19/08/28 01:48:32 INFO BlockManagerMaster: BlockManagerMaster stopped\r\n19/08/28 01:48:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\r\n19/08/28 01:48:32 INFO SparkContext: Successfully stopped SparkContext\r\n19/08/28 01:48:32 INFO ShutdownHookManager: Shutdown hook called\r\n19/08/28 01:48:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-f2b983dd-78f7-45f1-9bbd-a65a79b9c0e0\r\n19/08/28 01:48:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-a8ee056a-2e65-4050-9bfb-f5e08377963a/pyspark-f95fa051-b376-4545-8c84-196b2020da6e\r\n19/08/28 01:48:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-a8ee056a-2e65-4050-9bfb-f5e08377963a\r\n\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit --master ${MASTER} ${TFoS_HOME}/examples/mnist/mnist_data_setup.py --output examples/mnist/csv --format csv\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/442", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/442/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/442/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/442/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/442", "id": 485379682, "node_id": "MDU6SXNzdWU0ODUzNzk2ODI=", "number": 442, "title": "TensorflowOnSpark running on a Standalone Multiple Host cluster fails TFCluster.py:323] Exception in TF background thread", "user": {"login": "cleuton", "id": 984136, "node_id": "MDQ6VXNlcjk4NDEzNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/984136?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cleuton", "html_url": "https://github.com/cleuton", "followers_url": "https://api.github.com/users/cleuton/followers", "following_url": "https://api.github.com/users/cleuton/following{/other_user}", "gists_url": "https://api.github.com/users/cleuton/gists{/gist_id}", "starred_url": "https://api.github.com/users/cleuton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cleuton/subscriptions", "organizations_url": "https://api.github.com/users/cleuton/orgs", "repos_url": "https://api.github.com/users/cleuton/repos", "events_url": "https://api.github.com/users/cleuton/events{/privacy}", "received_events_url": "https://api.github.com/users/cleuton/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-08-26T18:35:38Z", "updated_at": "2019-08-27T20:37:31Z", "closed_at": "2019-08-27T20:36:22Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nI am trying to run TFoS on a Spark Standalone Cluster, with multiple hosts. I am getting this error message: \r\nE0826 18:24:18.203910 140397927008000 TFCluster.py:323] Exception in TF background thread\r\n\r\nMy spark-submit command is: \r\n$SPARK_HOME/bin/spark-submit \\\r\n --master $MASTER \\\r\n --deploy-mode client \\\r\n --verbose \\\r\n --num-executors 2 \\\r\n ./iris.py \r\n\r\nI first started using a single host, but when I migrated to multiple hosts (1 master 2 slaves) I got this error. ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/440", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/440/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/440/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/440/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/440", "id": 480447550, "node_id": "MDU6SXNzdWU0ODA0NDc1NTA=", "number": 440, "title": "Is it possible migrate a CNN tensorflow application to tensorflowonspark?", "user": {"login": "rafaelmarconiramos", "id": 18166522, "node_id": "MDQ6VXNlcjE4MTY2NTIy", "avatar_url": "https://avatars1.githubusercontent.com/u/18166522?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rafaelmarconiramos", "html_url": "https://github.com/rafaelmarconiramos", "followers_url": "https://api.github.com/users/rafaelmarconiramos/followers", "following_url": "https://api.github.com/users/rafaelmarconiramos/following{/other_user}", "gists_url": "https://api.github.com/users/rafaelmarconiramos/gists{/gist_id}", "starred_url": "https://api.github.com/users/rafaelmarconiramos/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rafaelmarconiramos/subscriptions", "organizations_url": "https://api.github.com/users/rafaelmarconiramos/orgs", "repos_url": "https://api.github.com/users/rafaelmarconiramos/repos", "events_url": "https://api.github.com/users/rafaelmarconiramos/events{/privacy}", "received_events_url": "https://api.github.com/users/rafaelmarconiramos/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-08-14T02:10:52Z", "updated_at": "2019-10-14T17:48:45Z", "closed_at": "2019-10-14T17:48:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\n\r\nI don\u00b4t know if this git is the correct local to send my doubts, but I did not find another place to do. I need to know if it is possible to migrate a tesorflow CNN to tensorflowonspark.  The example with CNN uses known case, but in my case, I have specific architectures and images.\r\n\r\nThe guide to convert is not clear for me how the distribution will work at the spark cluster. Does someone know a howto to write tensorflowonspark from scratch?\r\n\r\nI\u00b4m working with Python 3, Spark Cluster without hadoop and tensorflow without GPU.\r\n\r\nThanks\r\nRafael\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/439", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/439/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/439/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/439/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/439", "id": 477948527, "node_id": "MDU6SXNzdWU0Nzc5NDg1Mjc=", "number": 439, "title": "can't save the model by using keras example", "user": {"login": "kshina76", "id": 53253817, "node_id": "MDQ6VXNlcjUzMjUzODE3", "avatar_url": "https://avatars2.githubusercontent.com/u/53253817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshina76", "html_url": "https://github.com/kshina76", "followers_url": "https://api.github.com/users/kshina76/followers", "following_url": "https://api.github.com/users/kshina76/following{/other_user}", "gists_url": "https://api.github.com/users/kshina76/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshina76/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshina76/subscriptions", "organizations_url": "https://api.github.com/users/kshina76/orgs", "repos_url": "https://api.github.com/users/kshina76/repos", "events_url": "https://api.github.com/users/kshina76/events{/privacy}", "received_events_url": "https://api.github.com/users/kshina76/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2019-08-07T14:01:39Z", "updated_at": "2019-10-14T17:49:46Z", "closed_at": "2019-10-14T17:49:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python 3.6.9\r\n - Spark version 2.4.3\r\n - open jdk11\r\n - TensorFlow version 1.14.0\r\n\r\nI tried to execute sample program of stand alone by keras at (https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/README.md)\r\nI tried to execute \"Run MNIST MLP using InputMode.SPARK\" on README.md\r\nbut I could not save the model.\r\nDo you know why?\r\n\r\n ${SPARK_HOME}/bin/spark-submit \\\r\n> --master ${MASTER} \\\r\n> --conf spark.cores.max=${TOTAL_CORES} \\\r\n> --conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n> --conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n> ${TFoS_HOME}/examples/mnist/keras/mnist_mlp_estimator.py \\\r\n> --cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n> --input_mode spark \\\r\n> --images ${TFoS_HOME}/mnist/csv/train/images \\\r\n> --labels ${TFoS_HOME}/mnist/csv/train/labels \\\r\n> --epochs 5 \\\r\n> --model_dir ${TFoS_HOME}/mnist_model \\\r\n> --tensorboard\r\n19/08/07 22:36:59 WARN Utils: Your hostname, shina resolves to a loopback address: 127.0.1.1; using 192.168.1.37 instead (on interface enp0s25)\r\n19/08/07 22:36:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-2.4.3-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.3.jar) to method java.nio.Bits.unaligned()\r\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\n19/08/07 22:37:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/home/shina/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0807 22:37:01.925125 139902452205376 deprecation_wrapper.py:119] From /opt/TensorFlowOnSpark/examples/mnist/keras/mnist_mlp_estimator.py:11: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\r\n\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\n19/08/07 22:37:02 INFO SparkContext: Running Spark version 2.4.3\r\n19/08/07 22:37:02 INFO SparkContext: Submitted application: mnist_mlp\r\n19/08/07 22:37:02 INFO SecurityManager: Changing view acls to: root\r\n19/08/07 22:37:02 INFO SecurityManager: Changing modify acls to: root\r\n19/08/07 22:37:02 INFO SecurityManager: Changing view acls groups to: \r\n19/08/07 22:37:02 INFO SecurityManager: Changing modify acls groups to: \r\n19/08/07 22:37:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\r\n19/08/07 22:37:02 INFO Utils: Successfully started service 'sparkDriver' on port 37115.\r\n19/08/07 22:37:02 INFO SparkEnv: Registering MapOutputTracker\r\n19/08/07 22:37:02 INFO SparkEnv: Registering BlockManagerMaster\r\n19/08/07 22:37:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n19/08/07 22:37:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n19/08/07 22:37:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-75e07f01-bcc4-4558-ab6a-05d107110af8\r\n19/08/07 22:37:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MB\r\n19/08/07 22:37:02 INFO SparkEnv: Registering OutputCommitCoordinator\r\n19/08/07 22:37:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.\r\n19/08/07 22:37:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.37:4040\r\n19/08/07 22:37:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://shina:7077...\r\n19/08/07 22:37:03 INFO TransportClientFactory: Successfully created connection to shina/127.0.1.1:7077 after 39 ms (0 ms spent in bootstraps)\r\n19/08/07 22:37:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20190807223703-0006\r\n19/08/07 22:37:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41019.\r\n19/08/07 22:37:03 INFO NettyBlockTransferService: Server created on 192.168.1.37:41019\r\n19/08/07 22:37:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n19/08/07 22:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190807223703-0006/0 on worker-20190807211209-192.168.1.37-46649 (192.168.1.37:46649) with 1 core(s)\r\n19/08/07 22:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20190807223703-0006/0 on hostPort 192.168.1.37:46649 with 1 core(s), 1024.0 MB RAM\r\n19/08/07 22:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190807223703-0006/1 on worker-20190807211206-192.168.1.37-37383 (192.168.1.37:37383) with 1 core(s)\r\n19/08/07 22:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20190807223703-0006/1 on hostPort 192.168.1.37:37383 with 1 core(s), 1024.0 MB RAM\r\n19/08/07 22:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190807223703-0006/2 on worker-20190807211203-192.168.1.37-38143 (192.168.1.37:38143) with 1 core(s)\r\n19/08/07 22:37:03 INFO StandaloneSchedulerBackend: Granted executor ID app-20190807223703-0006/2 on hostPort 192.168.1.37:38143 with 1 core(s), 1024.0 MB RAM\r\n19/08/07 22:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190807223703-0006/0 is now RUNNING\r\n19/08/07 22:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190807223703-0006/1 is now RUNNING\r\n19/08/07 22:37:03 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190807223703-0006/2 is now RUNNING\r\n19/08/07 22:37:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.37, 41019, None)\r\n19/08/07 22:37:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.37:41019 with 434.4 MB RAM, BlockManagerId(driver, 192.168.1.37, 41019, None)\r\n19/08/07 22:37:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.37, 41019, None)\r\n19/08/07 22:37:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.37, 41019, None)\r\n19/08/07 22:37:04 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\r\nargs: Namespace(batch_size=100, cluster_size=3, epochs=5, images='/opt/TensorFlowOnSpark/mnist/csv/train/images', input_mode='spark', labels='/opt/TensorFlowOnSpark/mnist/csv/train/labels', model_dir='/opt/TensorFlowOnSpark/mnist_model', num_ps=1, output='predictions', steps=2000, tensorboard=True)\r\n19/08/07 22:37:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 118.5 KB, free 434.3 MB)\r\n19/08/07 22:37:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 434.3 MB)\r\n19/08/07 22:37:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.37:41019 (size: 22.9 KB, free: 434.4 MB)\r\n19/08/07 22:37:07 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n19/08/07 22:37:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 118.5 KB, free 434.1 MB)\r\n19/08/07 22:37:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.9 KB, free 434.1 MB)\r\n19/08/07 22:37:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.37:41019 (size: 22.9 KB, free: 434.4 MB)\r\n19/08/07 22:37:07 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\r\n19/08/07 22:37:08 INFO FileInputFormat: Total input paths to process : 10\r\n19/08/07 22:37:08 INFO FileInputFormat: Total input paths to process : 10\r\nE0807 22:37:09.127454 139902375241472 TFCluster.py:323] Exception in TF background thread\r\n19/08/07 22:37:09 INFO SparkUI: Stopped Spark web UI at http://192.168.1.37:4040\r\n19/08/07 22:37:09 INFO StandaloneSchedulerBackend: Shutting down all executors\r\n19/08/07 22:37:09 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\r\n19/08/07 22:37:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\r\n19/08/07 22:37:10 INFO MemoryStore: MemoryStore cleared\r\n19/08/07 22:37:10 INFO BlockManager: BlockManager stopped\r\n19/08/07 22:37:10 INFO BlockManagerMaster: BlockManagerMaster stopped\r\n19/08/07 22:37:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\r\n19/08/07 22:37:10 INFO SparkContext: Successfully stopped SparkContext\r\n19/08/07 22:37:10 INFO ShutdownHookManager: Shutdown hook called\r\n19/08/07 22:37:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab9e4057-b5f5-4341-8388-a0ef1a36b78b/pyspark-114cda21-28ee-40b8-92a7-dc060d9fe0fb\r\n19/08/07 22:37:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab9e4057-b5f5-4341-8388-a0ef1a36b78b\r\n19/08/07 22:37:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-cff099b2-da85-4444-817a-9abdd209625c\r\n\r\nI think the following error is issue.\r\nE0807 22:37:09.127454 139902375241472 TFCluster.py:323] Exception in TF background thread", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/438", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/438/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/438/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/438/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/438", "id": 470279237, "node_id": "MDU6SXNzdWU0NzAyNzkyMzc=", "number": 438, "title": "HDP with Spark2.2", "user": {"login": "Ashishyelkar", "id": 41521599, "node_id": "MDQ6VXNlcjQxNTIxNTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/41521599?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ashishyelkar", "html_url": "https://github.com/Ashishyelkar", "followers_url": "https://api.github.com/users/Ashishyelkar/followers", "following_url": "https://api.github.com/users/Ashishyelkar/following{/other_user}", "gists_url": "https://api.github.com/users/Ashishyelkar/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ashishyelkar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ashishyelkar/subscriptions", "organizations_url": "https://api.github.com/users/Ashishyelkar/orgs", "repos_url": "https://api.github.com/users/Ashishyelkar/repos", "events_url": "https://api.github.com/users/Ashishyelkar/events{/privacy}", "received_events_url": "https://api.github.com/users/Ashishyelkar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-07-19T11:33:18Z", "updated_at": "2019-08-27T18:24:24Z", "closed_at": "2019-08-27T18:24:24Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [2.2]\r\n - TensorFlow version [e.g. 1.10.1]\r\n - TensorFlowOnSpark version [e.g. 1.4.3]\r\n - Cluster version [HDP]\r\n\r\n**Describe the Issue:**\r\nWhile running in YARN cluster mode using spark, we are facing error as per attached.\r\n\r\n\r\n\r\n**Logs:**\r\nIf applicable, add logs to help explain your problem.  Note: errors may not be fully described in the driver/console logs.  Make sure to check the executor logs for possible root causes.\r\n\r\n**Spark Submit Command Line:**\r\nIf applicable, add your spark-submit command line.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/437", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/437/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/437/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/437/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/437", "id": 466083689, "node_id": "MDU6SXNzdWU0NjYwODM2ODk=", "number": 437, "title": "Question about using gpu resources.", "user": {"login": "zengxy", "id": 11961641, "node_id": "MDQ6VXNlcjExOTYxNjQx", "avatar_url": "https://avatars1.githubusercontent.com/u/11961641?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zengxy", "html_url": "https://github.com/zengxy", "followers_url": "https://api.github.com/users/zengxy/followers", "following_url": "https://api.github.com/users/zengxy/following{/other_user}", "gists_url": "https://api.github.com/users/zengxy/gists{/gist_id}", "starred_url": "https://api.github.com/users/zengxy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zengxy/subscriptions", "organizations_url": "https://api.github.com/users/zengxy/orgs", "repos_url": "https://api.github.com/users/zengxy/repos", "events_url": "https://api.github.com/users/zengxy/events{/privacy}", "received_events_url": "https://api.github.com/users/zengxy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-07-10T03:49:24Z", "updated_at": "2019-07-11T15:50:46Z", "closed_at": "2019-07-11T15:50:46Z", "author_association": "NONE", "active_lock_reason": null, "body": "It seems that tf process is running on spark executors (correct me if not! ). But spark won't apply for gpu resource currently. So I want to know how it access to the gpu resource?\r\n\r\nIn some issues like https://github.com/yahoo/TensorFlowOnSpark/issues/185, you explian `we use Memory as a proxy for GPU`. But I don't understand what `proxy` here actually mean?\r\n\r\nHoping for reply!\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/436", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/436/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/436/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/436/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/436", "id": 464841241, "node_id": "MDU6SXNzdWU0NjQ4NDEyNDE=", "number": 436, "title": "standalone cluster of multi-node cannot save model to hdfs and read model from hdfs", "user": {"login": "HannanKan", "id": 24515494, "node_id": "MDQ6VXNlcjI0NTE1NDk0", "avatar_url": "https://avatars1.githubusercontent.com/u/24515494?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HannanKan", "html_url": "https://github.com/HannanKan", "followers_url": "https://api.github.com/users/HannanKan/followers", "following_url": "https://api.github.com/users/HannanKan/following{/other_user}", "gists_url": "https://api.github.com/users/HannanKan/gists{/gist_id}", "starred_url": "https://api.github.com/users/HannanKan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HannanKan/subscriptions", "organizations_url": "https://api.github.com/users/HannanKan/orgs", "repos_url": "https://api.github.com/users/HannanKan/repos", "events_url": "https://api.github.com/users/HannanKan/events{/privacy}", "received_events_url": "https://api.github.com/users/HannanKan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-07-06T10:32:53Z", "updated_at": "2019-07-07T06:33:35Z", "closed_at": "2019-07-07T06:33:30Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 2.7.12\r\n - Spark version 2.4.3\r\n - TensorFlow version 1.14.0\r\n - TensorFlowOnSpark version 1.4.3\r\n - Cluster version Standalone, \r\n\r\n*Describe the bug:**\r\nWhen running TensorFlowOnSpark on a Spark Standalone cluster (two host with hdfs) of mnist,\r\nI cannot use hdfs path as model path. Otherwise, the program will hange. If use local path as th e model path, the program will run successfully.  Other paths can use hdfs path. I have read issue: [issue](https://github.com/yahoo/TensorFlowOnSpark/issues/341). But my cluster is standalone.\r\nBTW,  when I run pyspark --conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_HDFS\r\n`f=tf.gfile.GFile(\"hdfs://namenode_ip:port/foo.txt\",\"w\")\r\nf.write(\"hello\")` \r\nwill raise error\r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/tfos/.local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 106, in write\r\n    self._prewrite_check()\r\n  File \"/home/tfos/.local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 92, in _prewrite_check\r\n    compat.as_bytes(self.__name), compat.as_bytes(self.__mode))\r\ntensorflow.python.framework.errors_impl.NotFoundError: libhdfs.so: cannot open shared object file: No such file or directory`\r\n\r\nspark commandline\r\n\r\n`${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--py-files ${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--images hdfs://10.141.221.222:9000/default/tfos/examples/mnist/csv/test/images \\\r\n--labels hdfs://10.141.221.222:9000/default/tfos/examples/mnist/csv/test/labels \\\r\n--mode inference \\\r\n--format csv \\\r\n--model hdfs://10.141.221.222:9000/default/tfos/examples/mnist_model \\\r\n--output predictions` \r\nwill hang and give exception\r\n`Timeout while feeding partition`\r\n\r\n`${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--py-files ${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--images hdfs://10.141.221.222:9000/default/tfos/examples/mnist/csv/test/images \\\r\n--labels hdfs://10.141.221.222:9000/default/tfos/examples/mnist/csv/test/labels \\\r\n--mode inference \\\r\n--format csv \\\r\n--model mnist_model \\\r\n--output predictions`\r\nwill run well and save model to local path.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/434", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/434/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/434/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/434/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/434", "id": 461838340, "node_id": "MDU6SXNzdWU0NjE4MzgzNDA=", "number": 434, "title": "Could current version feed the validation data? ", "user": {"login": "pl-lee", "id": 51313587, "node_id": "MDQ6VXNlcjUxMzEzNTg3", "avatar_url": "https://avatars1.githubusercontent.com/u/51313587?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pl-lee", "html_url": "https://github.com/pl-lee", "followers_url": "https://api.github.com/users/pl-lee/followers", "following_url": "https://api.github.com/users/pl-lee/following{/other_user}", "gists_url": "https://api.github.com/users/pl-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/pl-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pl-lee/subscriptions", "organizations_url": "https://api.github.com/users/pl-lee/orgs", "repos_url": "https://api.github.com/users/pl-lee/repos", "events_url": "https://api.github.com/users/pl-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/pl-lee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-06-28T03:29:56Z", "updated_at": "2019-07-01T16:01:35Z", "closed_at": "2019-07-01T16:01:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/432", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/432/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/432/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/432/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/432", "id": 460819872, "node_id": "MDU6SXNzdWU0NjA4MTk4NzI=", "number": 432, "title": "No executor id error", "user": {"login": "neveryoungzz", "id": 29646981, "node_id": "MDQ6VXNlcjI5NjQ2OTgx", "avatar_url": "https://avatars0.githubusercontent.com/u/29646981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neveryoungzz", "html_url": "https://github.com/neveryoungzz", "followers_url": "https://api.github.com/users/neveryoungzz/followers", "following_url": "https://api.github.com/users/neveryoungzz/following{/other_user}", "gists_url": "https://api.github.com/users/neveryoungzz/gists{/gist_id}", "starred_url": "https://api.github.com/users/neveryoungzz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neveryoungzz/subscriptions", "organizations_url": "https://api.github.com/users/neveryoungzz/orgs", "repos_url": "https://api.github.com/users/neveryoungzz/repos", "events_url": "https://api.github.com/users/neveryoungzz/events{/privacy}", "received_events_url": "https://api.github.com/users/neveryoungzz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-06-26T08:00:47Z", "updated_at": "2020-07-15T14:54:56Z", "closed_at": "2019-08-01T23:33:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [e.g. 2.7, 3.6]\r\n - Spark version [e.g. 2.1, 2.3.1]\r\n - TensorFlow version [e.g. 1.5, 1.9.0]\r\n - TensorFlowOnSpark version [e.g. 1.1, 1.3.2]\r\n - Cluster version [e.g. Standalone, Hadoop 2.8, CDH5]\r\n\r\n**Describe the bug:**\r\nA clear and concise description of what the bug is.\r\n\r\n**Logs:**\r\nIf applicable, add logs to help explain your problem.  Note: errors may not be fully described in the driver/console logs.  Make sure to check the executor logs for possible root causes.\r\n\r\n**Spark Submit Command Line:**\r\nIf applicable, add your spark-submit command line.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/430", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/430/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/430/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/430/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/430", "id": 456771353, "node_id": "MDU6SXNzdWU0NTY3NzEzNTM=", "number": 430, "title": "timeout while feeding partition when run on CDH with the hadoop3.x, ", "user": {"login": "yhlhs", "id": 30208741, "node_id": "MDQ6VXNlcjMwMjA4NzQx", "avatar_url": "https://avatars0.githubusercontent.com/u/30208741?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yhlhs", "html_url": "https://github.com/yhlhs", "followers_url": "https://api.github.com/users/yhlhs/followers", "following_url": "https://api.github.com/users/yhlhs/following{/other_user}", "gists_url": "https://api.github.com/users/yhlhs/gists{/gist_id}", "starred_url": "https://api.github.com/users/yhlhs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yhlhs/subscriptions", "organizations_url": "https://api.github.com/users/yhlhs/orgs", "repos_url": "https://api.github.com/users/yhlhs/repos", "events_url": "https://api.github.com/users/yhlhs/events{/privacy}", "received_events_url": "https://api.github.com/users/yhlhs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-06-17T06:53:26Z", "updated_at": "2019-08-07T03:46:04Z", "closed_at": "2019-06-18T09:43:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [ 3.6]\r\n - Spark version [ 2.4.0]\r\n - TensorFlow version [1.13.1]\r\n - TensorFlowOnSpark version [1.4.3]\r\n - Cluster version [Hadoop 3.x, CDH6]\r\n\r\n**Describe the bug:**\r\ntimeout while feeding partition and the spark.executorEnv.LD_LIBRARY_PATH has been set\r\n\r\n**Logs:**\r\nworker logs:19-06-17 06:50:05,645 INFO (MainThread-47066) connected to server at ('192.168.200.150', 43921)\r\n2019-06-17 06:50:05,646 INFO (MainThread-47066) TFSparkNode.reserve: {'executor_id': 1, 'host': '192.168.200.151', 'job_name': 'worker', 'task_index': 0, 'port': 33687, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-0315ghlw/listener-rahzekdw', 'authkey': b'M\\xa6yT\\xd8\\xbbOC\\xa5\\xef\\x11\\xd40\\xd7\\x80\\xd3'}\r\n2019-06-17 06:50:07,649 INFO (MainThread-47066) node: {'executor_id': 0, 'host': '192.168.200.151', 'job_name': 'ps', 'task_index': 0, 'port': 44263, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.200.151', 34225), 'authkey': b'\\xa7\\xb5\\x189\\xf4\\xc3C\\xc5\\xa2\\xf8\\xdc\\x0b\\\\\\x99\\x08\\xf0'}\r\n2019-06-17 06:50:07,650 INFO (MainThread-47066) node: {'executor_id': 1, 'host': '192.168.200.151', 'job_name': 'worker', 'task_index': 0, 'port': 33687, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-0315ghlw/listener-rahzekdw', 'authkey': b'M\\xa6yT\\xd8\\xbbOC\\xa5\\xef\\x11\\xd40\\xd7\\x80\\xd3'}\r\n2019-06-17 06:50:07,650 INFO (MainThread-47066) Starting TensorFlow worker:0 as worker on cluster node 1 on background process\r\n2019-06-17 06:50:07,737 INFO (MainThread-47556) 1: ======== worker:0 ========\r\n2019-06-17 06:50:07,742 INFO (MainThread-47556) 1: Cluster spec: {'ps': ['192.168.200.151:44263'], 'worker': ['192.168.200.151:33687']}\r\n2019-06-17 06:50:07,742 INFO (MainThread-47556) 1: Using CPU\r\n2019-06-17 06:50:07.762270: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-17 06:50:07.835094: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095074999 Hz\r\n2019-06-17 06:50:07.835332: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1019450 executing computations on platform Host. Devices:\r\n2019-06-17 06:50:07.835371: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-17 06:50:07.837422: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job ps -> {0 -> 192.168.200.151:44263}\r\n2019-06-17 06:50:07.862844: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:33687}\r\n2019-06-17 06:50:07.863903: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:33687\r\n2019-06-17 06:50:08,009 WARNING (MainThread-47556) From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-06-17 06:50:08,187 WARNING (MainThread-47556) From /yarn/nm/usercache/admin/appcache/application_1560245048868_0368/container_1560245048868_0368_01_000004/__pyfiles__/tf_air.py:296: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\r\n2019-06-17 06:50:08,193 WARNING (MainThread-47556) From /yarn/nm/usercache/admin/appcache/application_1560245048868_0368/container_1560245048868_0368_01_000004/__pyfiles__/tf_air.py:298: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\r\n2019-06-17 06:50:08,344 WARNING (MainThread-47556) From /yarn/nm/usercache/admin/appcache/application_1560245048868_0368/container_1560245048868_0368_01_000004/__pyfiles__/tf_air.py:306: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `keras.layers.RNN(cell)`, which is equivalent to this API\r\n2019-06-17 06:50:10,181 INFO (MainThread-47556) tensorflow model path: hdfs://192.168.200.152:8020/user/yuhongliang/zhuzhou/air_model1\r\n2019-06-17 06:50:10,930 INFO (MainThread-47556) Create CheckpointSaverHook.\r\n2019-06-17 06:50:12,471 INFO (MainThread-47069) Connected to TFSparkNode.mgr on 192.168.200.151, executor=1, state='running'\r\n2019-06-17 06:50:12,526 INFO (MainThread-47069) mgr.state='running'\r\n2019-06-17 06:50:12,526 INFO (MainThread-47069) Feeding partition <itertools.chain object at 0x7fa3a7d2a0b8> into input queue <multiprocessing.queues.JoinableQueue object at 0x7fa3a5b55630>\r\n\r\n**Spark Submit Command Line:**\r\nIf applicable, add your spark-submit command line.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/429", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/429/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/429/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/429/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/429", "id": 455799009, "node_id": "MDU6SXNzdWU0NTU3OTkwMDk=", "number": 429, "title": "How to choose the num-executors according to the cluster-size", "user": {"login": "vamsinimmala1992", "id": 25427325, "node_id": "MDQ6VXNlcjI1NDI3MzI1", "avatar_url": "https://avatars2.githubusercontent.com/u/25427325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vamsinimmala1992", "html_url": "https://github.com/vamsinimmala1992", "followers_url": "https://api.github.com/users/vamsinimmala1992/followers", "following_url": "https://api.github.com/users/vamsinimmala1992/following{/other_user}", "gists_url": "https://api.github.com/users/vamsinimmala1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/vamsinimmala1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vamsinimmala1992/subscriptions", "organizations_url": "https://api.github.com/users/vamsinimmala1992/orgs", "repos_url": "https://api.github.com/users/vamsinimmala1992/repos", "events_url": "https://api.github.com/users/vamsinimmala1992/events{/privacy}", "received_events_url": "https://api.github.com/users/vamsinimmala1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-06-13T15:11:17Z", "updated_at": "2019-07-03T17:50:11Z", "closed_at": "2019-07-03T17:50:11Z", "author_association": "NONE", "active_lock_reason": null, "body": "Let's suppose I have a dataset of 1.2 million (1282143) records and as per TFoS the steps and steps_for_epoch are calucalted by the following formulae\r\n\r\n\t\tsteps = epochs * num_records_per_epoch / batch_size\r\n\t\tsteps_per_epoch = steps / num_workers\r\n\t\t\r\n\t\t\r\n==> epochs = 20\r\n==> steps = 20 * 1282143 / 100 ==> steps = 256428.6\r\n==> steps_for_epoch = 256428 / num_workers (num_workers PICKED AS PER num-executors)\r\n\r\n\r\nspark-submit \r\n--driver-memory 64G \r\n--executor-memory 24G \r\n--num-executors _______ ( <---I want to maximize this value, so I can perform the job quicker)\r\n--executor-cores 1 \r\nkeras_model.py\r\n--cluster-size _______ (Is it need to be same as num-executors always, if not how to choose the cluster-size)\r\n\r\n\r\nand Let's assume I have 16 nodes and 25 cores each ==> 400 cores altogether\r\n\r\nso now how to calculate the total executors that I can supply to the job for model training. could you help me crack that.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/428", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/428/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/428/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/428/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/428", "id": 455000483, "node_id": "MDU6SXNzdWU0NTUwMDA0ODM=", "number": 428, "title": "job failed with the error \"timeout while feeding partition\" when run with CDH", "user": {"login": "yhlhs", "id": 30208741, "node_id": "MDQ6VXNlcjMwMjA4NzQx", "avatar_url": "https://avatars0.githubusercontent.com/u/30208741?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yhlhs", "html_url": "https://github.com/yhlhs", "followers_url": "https://api.github.com/users/yhlhs/followers", "following_url": "https://api.github.com/users/yhlhs/following{/other_user}", "gists_url": "https://api.github.com/users/yhlhs/gists{/gist_id}", "starred_url": "https://api.github.com/users/yhlhs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yhlhs/subscriptions", "organizations_url": "https://api.github.com/users/yhlhs/orgs", "repos_url": "https://api.github.com/users/yhlhs/repos", "events_url": "https://api.github.com/users/yhlhs/events{/privacy}", "received_events_url": "https://api.github.com/users/yhlhs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-06-12T03:57:01Z", "updated_at": "2019-06-12T03:58:01Z", "closed_at": "2019-06-12T03:58:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [e.g. 2.7, 3.6]\r\n - Spark version [e.g. 2.1, 2.3.1]\r\n - TensorFlow version [e.g. 1.5, 1.9.0]\r\n - TensorFlowOnSpark version [e.g. 1.1, 1.3.2]\r\n - Cluster version [e.g. Standalone, Hadoop 2.8, CDH5]\r\n\r\n**Describe the bug:**\r\nA clear and concise description of what the bug is.\r\n\r\n**Logs:**\r\nIf applicable, add logs to help explain your problem.  Note: errors may not be fully described in the driver/console logs.  Make sure to check the executor logs for possible root causes.\r\n\r\n**Spark Submit Command Line:**\r\nIf applicable, add your spark-submit command line.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/427", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/427/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/427/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/427/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/427", "id": 450780295, "node_id": "MDU6SXNzdWU0NTA3ODAyOTU=", "number": 427, "title": "mnist example hangs", "user": {"login": "HannanKan", "id": 24515494, "node_id": "MDQ6VXNlcjI0NTE1NDk0", "avatar_url": "https://avatars1.githubusercontent.com/u/24515494?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HannanKan", "html_url": "https://github.com/HannanKan", "followers_url": "https://api.github.com/users/HannanKan/followers", "following_url": "https://api.github.com/users/HannanKan/following{/other_user}", "gists_url": "https://api.github.com/users/HannanKan/gists{/gist_id}", "starred_url": "https://api.github.com/users/HannanKan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HannanKan/subscriptions", "organizations_url": "https://api.github.com/users/HannanKan/orgs", "repos_url": "https://api.github.com/users/HannanKan/repos", "events_url": "https://api.github.com/users/HannanKan/events{/privacy}", "received_events_url": "https://api.github.com/users/HannanKan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2019-05-31T12:42:28Z", "updated_at": "2019-07-24T01:45:48Z", "closed_at": "2019-07-16T17:06:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "I got the same problem as #349. I have read all related issue and FAQs\uff08set LIB_HDFS LIB_JVM \uff09 but unfortunately, I still suck. \r\n**Environment:**\r\n - Python version 2.7.12\r\n - Spark version 2.4.1\r\n - TensorFlow version 1.13.1\r\n - TensorFlowOnSpark version 1.4.3\r\n - Cluster version standalone cluster\r\n\r\n**Describe the bug:**\r\nAfter configuring the cluster, I run mnist example but sucked at \"INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58645\". After timeout, it gave following log(a little bit long)\r\n```\r\n19/05/31 15:18:59 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2, 10.141.221.223, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/home/ghn/.local/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/05/31 15:29:02 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 1.0 (TID 4, 10.141.221.223, executor 0, partition 0, ANY, 8537 bytes)\r\n19/05/31 15:29:02 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 1.0 (TID 3, 10.141.221.223, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/home/ghn/.local/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/05/31 15:39:05 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 1.0 (TID 5, 10.141.221.223, executor 0, partition 1, ANY, 8537 bytes)\r\n19/05/31 15:39:05 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 1.0 (TID 4, 10.141.221.223, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/home/ghn/.local/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/05/31 15:49:08 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 1.0 (TID 6, 10.141.221.223, executor 0, partition 0, ANY, 8537 bytes)\r\n19/05/31 15:49:08 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 1.0 (TID 5, 10.141.221.223, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/home/ghn/.local/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/05/31 15:59:11 INFO scheduler.TaskSetManager: Starting task 1.2 in stage 1.0 (TID 7, 10.141.221.223, executor 0, partition 1, ANY, 8537 bytes)\r\n19/05/31 15:59:11 WARN scheduler.TaskSetManager: Lost task 0.2 in stage 1.0 (TID 6, 10.141.221.223, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/home/ghn/.local/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/05/31 16:09:14 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 1.0 (TID 8, 10.141.221.223, executor 0, partition 0, ANY, 8537 bytes)\r\n19/05/31 16:09:14 WARN scheduler.TaskSetManager: Lost task 1.2 in stage 1.0 (TID 7, 10.141.221.223, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2499, in pipeline_func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 352, in func\r\n  File \"/local/spark-2.4.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 801, in func\r\n  File \"/home/ghn/.local/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n**Logs:**\r\n```\r\nTop of Log\r\nSpark Executor Command: \"/local/jdk1.8/bin/java\" \"-cp\" \"/local/spark-2.4.1-bin-hadoop2.7/conf/:/local/spark-2.4.1-bin-hadoop2.7/jars/*:/local/hadoop-2.7.1/etc/hadoop/\" \"-Xmx1024M\" \"-Dspark.driver.port=34771\" \"org.apache.spark.executor.CoarseGrainedExecutorBackend\" \"--driver-url\" \"spark://CoarseGrainedScheduler@hadoop222:34771\" \"--executor-id\" \"1\" \"--hostname\" \"10.141.221.222\" \"--cores\" \"1\" \"--app-id\" \"app-20190531150850-0054\" \"--worker-url\" \"spark://Worker@10.141.221.222:37957\"\r\n========================================\r\n\r\n19/05/31 15:08:51 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 19542@hadoop222\r\n19/05/31 15:08:51 INFO util.SignalUtils: Registered signal handler for TERM\r\n19/05/31 15:08:51 INFO util.SignalUtils: Registered signal handler for HUP\r\n19/05/31 15:08:51 INFO util.SignalUtils: Registered signal handler for INT\r\n19/05/31 15:08:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n19/05/31 15:08:51 INFO spark.SecurityManager: Changing view acls to: ghn\r\n19/05/31 15:08:51 INFO spark.SecurityManager: Changing modify acls to: ghn\r\n19/05/31 15:08:51 INFO spark.SecurityManager: Changing view acls groups to: \r\n19/05/31 15:08:51 INFO spark.SecurityManager: Changing modify acls groups to: \r\n19/05/31 15:08:51 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ghn); groups with view permissions: Set(); users  with modify permissions: Set(ghn); groups with modify permissions: Set()\r\n19/05/31 15:08:52 INFO client.TransportClientFactory: Successfully created connection to hadoop222/10.141.221.222:34771 after 78 ms (0 ms spent in bootstraps)\r\n19/05/31 15:08:52 INFO spark.SecurityManager: Changing view acls to: ghn\r\n19/05/31 15:08:52 INFO spark.SecurityManager: Changing modify acls to: ghn\r\n19/05/31 15:08:52 INFO spark.SecurityManager: Changing view acls groups to: \r\n19/05/31 15:08:52 INFO spark.SecurityManager: Changing modify acls groups to: \r\n19/05/31 15:08:52 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ghn); groups with view permissions: Set(); users  with modify permissions: Set(ghn); groups with modify permissions: Set()\r\n19/05/31 15:08:52 INFO client.TransportClientFactory: Successfully created connection to hadoop222/10.141.221.222:34771 after 1 ms (0 ms spent in bootstraps)\r\n19/05/31 15:08:52 INFO storage.DiskBlockManager: Created local directory at /tmp/spark-2276f4d2-5eba-4e5b-ac21-5c7e5952fcad/executor-3ece4c79-9a4a-4a88-9cfa-bd101e7f9aa9/blockmgr-3399348d-98ba-4022-aa1c-e55925267063\r\n19/05/31 15:08:52 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\r\n19/05/31 15:08:52 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@hadoop222:34771\r\n19/05/31 15:08:52 INFO worker.WorkerWatcher: Connecting to worker spark://Worker@10.141.221.222:37957\r\n19/05/31 15:08:52 INFO client.TransportClientFactory: Successfully created connection to /10.141.221.222:37957 after 2 ms (0 ms spent in bootstraps)\r\n19/05/31 15:08:52 INFO worker.WorkerWatcher: Successfully connected to spark://Worker@10.141.221.222:37957\r\n19/05/31 15:08:52 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\r\n19/05/31 15:08:52 INFO executor.Executor: Starting executor ID 1 on host 10.141.221.222\r\n19/05/31 15:08:52 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42153.\r\n19/05/31 15:08:52 INFO netty.NettyBlockTransferService: Server created on 10.141.221.222:42153\r\n19/05/31 15:08:52 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n19/05/31 15:08:52 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, 10.141.221.222, 42153, None)\r\n19/05/31 15:08:52 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, 10.141.221.222, 42153, None)\r\n19/05/31 15:08:52 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, 10.141.221.222, 42153, None)\r\n19/05/31 15:08:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\r\n19/05/31 15:08:52 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\r\n19/05/31 15:08:52 INFO executor.Executor: Fetching spark://hadoop222:34771/files/mnist_dist.py with timestamp 1559286530739\r\n19/05/31 15:08:52 INFO client.TransportClientFactory: Successfully created connection to hadoop222/10.141.221.222:34771 after 1 ms (0 ms spent in bootstraps)\r\n19/05/31 15:08:52 INFO util.Utils: Fetching spark://hadoop222:34771/files/mnist_dist.py to /tmp/spark-2276f4d2-5eba-4e5b-ac21-5c7e5952fcad/executor-3ece4c79-9a4a-4a88-9cfa-bd101e7f9aa9/spark-f6cea8b7-46c1-4a8e-bc15-9a9bd6968207/fetchFileTemp9187758738095291929.tmp\r\n19/05/31 15:08:52 INFO util.Utils: Copying /tmp/spark-2276f4d2-5eba-4e5b-ac21-5c7e5952fcad/executor-3ece4c79-9a4a-4a88-9cfa-bd101e7f9aa9/spark-f6cea8b7-46c1-4a8e-bc15-9a9bd6968207/2542979141559286530739_cache to /local/spark-2.4.1-bin-hadoop2.7/work/app-20190531150850-0054/1/./mnist_dist.py\r\n19/05/31 15:08:52 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\r\n19/05/31 15:08:52 INFO client.TransportClientFactory: Successfully created connection to hadoop222/10.141.221.222:33769 after 1 ms (0 ms spent in bootstraps)\r\n19/05/31 15:08:53 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 12.4 KB, free 366.3 MB)\r\n19/05/31 15:08:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 134 ms\r\n19/05/31 15:08:53 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 26.0 KB, free 366.3 MB)\r\n2019-05-31 15:08:54,232 INFO (MainThread-19713) connected to server at ('10.141.221.222', 35631)\r\n2019-05-31 15:08:54,234 INFO (MainThread-19713) TFSparkNode.reserve: {'port': 36833, 'authkey': '\\x9d\\xe76\\x8dpkMD\\xbe}\\xfe\\xf00\\x99\\xe0b', 'executor_id': 0, 'addr': ('10.141.221.222', 44841), 'tb_port': 0, 'task_index': 0, 'host': '10.141.221.222', 'tb_pid': 0, 'job_name': 'ps'}\r\n2019-05-31 15:08:55,238 INFO (MainThread-19713) node: {'executor_id': 0, 'addr': ('10.141.221.222', 44841), 'task_index': 0, 'port': 36833, 'authkey': '\\x9d\\xe76\\x8dpkMD\\xbe}\\xfe\\xf00\\x99\\xe0b', 'host': '10.141.221.222', 'job_name': 'ps', 'tb_pid': 0, 'tb_port': 0}\r\n2019-05-31 15:08:55,238 INFO (MainThread-19713) node: {'executor_id': 1, 'addr': '/tmp/pymp-4PhZsB/listener-jIlI2K', 'task_index': 0, 'port': 36745, 'authkey': '\\x99\\x977\\xe0\\x99UL\\xe2\\xa9\\xdf\\xfa\\xe4\\xc8\\xd9\\x02|', 'host': '10.141.221.223', 'job_name': 'worker', 'tb_pid': 0, 'tb_port': 0}\r\n2019-05-31 15:08:55,238 INFO (MainThread-19713) Starting TensorFlow ps:0 as ps on cluster node 0 on background process\r\n2019-05-31 15:08:55,376 INFO (MainThread-19727) 0: ======== ps:0 ========\r\n2019-05-31 15:08:55,376 INFO (MainThread-19727) 0: Cluster spec: {'ps': ['10.141.221.222:36833'], 'worker': ['10.141.221.223:36745']}\r\n2019-05-31 15:08:55,377 INFO (MainThread-19727) 0: Using CPU\r\n2019-05-31 15:08:55.378921: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-05-31 15:08:55.432072: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-05-31 15:08:55.435059: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x36e1d50 executing computations on platform Host. Devices:\r\n2019-05-31 15:08:55.435106: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-31 15:08:55.438044: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job ps -> {0 -> localhost:36833}\r\n2019-05-31 15:08:55.438093: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> 10.141.221.223:36745}\r\n2019-05-31 15:08:55.443546: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:36833\r\n```\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--py-files ${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n--conf spark.executorENV.LD_LIBRARY_PATH=$LIB_HDFS:$LIB_JVM \\\r\n--conf spark.executorEnv.CLASSPATH=$(hadoop classpath --glob) \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--images examples/mnist/csv/train/images \\\r\n--labels examples/mnist/csv/train/labels \\\r\n--format csv \\\r\n--mode train \\\r\n--model mnist_model\r\n\r\nI\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/426", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/426/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/426/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/426/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/426", "id": 450738829, "node_id": "MDU6SXNzdWU0NTA3Mzg4Mjk=", "number": 426, "title": "internal: Container killed by YARN for exceeding memory limits. 116.8 GB of 60 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.", "user": {"login": "biswasingh", "id": 39695651, "node_id": "MDQ6VXNlcjM5Njk1NjUx", "avatar_url": "https://avatars0.githubusercontent.com/u/39695651?v=4", "gravatar_id": "", "url": "https://api.github.com/users/biswasingh", "html_url": "https://github.com/biswasingh", "followers_url": "https://api.github.com/users/biswasingh/followers", "following_url": "https://api.github.com/users/biswasingh/following{/other_user}", "gists_url": "https://api.github.com/users/biswasingh/gists{/gist_id}", "starred_url": "https://api.github.com/users/biswasingh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/biswasingh/subscriptions", "organizations_url": "https://api.github.com/users/biswasingh/orgs", "repos_url": "https://api.github.com/users/biswasingh/repos", "events_url": "https://api.github.com/users/biswasingh/events{/privacy}", "received_events_url": "https://api.github.com/users/biswasingh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-05-31T10:52:14Z", "updated_at": "2020-06-24T15:36:34Z", "closed_at": "2019-07-16T17:15:06Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 2.7\r\n - Spark version 2.3.1\r\n - TensorFlow version 1.13.0]\r\n - TensorFlowOnSpark version 1.3.2\r\n - Cluster version [e.g. Standalone, Hadoop 2.8, CDH5]\r\n\r\n**Describe the bug:**\r\ngs: Namespace(batch_size=100, cluster_size=2, epochs=1, format='csv', images='hdfs://ec2-54-84-34-119.compute-1.amazonaws.com:9000/user/ubuntu/mnist/csv/train/images', labels='hdfs://ec2-54-84-34-119.compute-1.amazonaws.com:9000/user/ubuntu/mnist/csv/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=1000, tensorboard=False)\r\n2019-05-31T10:46:37.631270 ===== Start\r\nzipping images and labels\r\n2019-05-31 10:46:38,228 INFO (MainThread-23006) Reserving TFSparkNodes \r\n2019-05-31 10:46:38,228 INFO (MainThread-23006) cluster_template: {'ps': [0], 'worker': [1]}\r\n2019-05-31 10:46:38,230 INFO (MainThread-23006) listening for reservations at ('172.16.7.36', 39469)\r\n2019-05-31 10:46:38,230 INFO (MainThread-23006) Starting TensorFlow on executors\r\n2019-05-31 10:46:38,237 INFO (MainThread-23006) Waiting for TFSparkNodes to start\r\n2019-05-31 10:46:38,238 INFO (MainThread-23006) waiting for 2 reservations\r\n2019-05-31 10:46:39,239 INFO (MainThread-23006) waiting for 2 reservations\r\n2019-05-31 10:46:40,240 INFO (MainThread-23006) waiting for 2 reservations\r\n2019-05-31 10:46:41,242 INFO (MainThread-23006) waiting for 2 reservations\r\n2019-05-31 10:46:42,243 INFO (MainThread-23006) waiting for 2 reservations\r\n2019-05-31 10:46:43,244 INFO (MainThread-23006) waiting for 2 reservations\r\n2019-05-31 10:46:44,246 INFO (MainThread-23006) waiting for 2 reservations\r\n2019-05-31 10:46:45,247 INFO (MainThread-23006) all reservations completed\r\n2019-05-31 10:46:45,247 INFO (MainThread-23006) All TFSparkNodes started\r\n2019-05-31 10:46:45,247 INFO (MainThread-23006) {'executor_id': 1, 'addr': '/tmp/pymp-h7C3zs/listener-24snEb', 'task_index': 0, 'job_name': 'worker', 'authkey': '\\xd7\\xd2G\\x19N\\xb3H\\xdd\\xa2)Q\\x99q\\xdd\\xbfd', 'host': '172.16.7.225', 'port': 43995, 'tb_pid': 0, 'tb_port': 0}\r\n2019-05-31 10:46:45,247 INFO (MainThread-23006) {'executor_id': 0, 'addr': ('172.16.7.225', 46080), 'task_index': 0, 'job_name': 'ps', 'authkey': 'fj]\\xffU\\tL\\x95\\xb3\\x03\\r\\xc1r\\xc3\\x13\\x92', 'host': '172.16.7.225', 'port': 41702, 'tb_pid': 0, 'tb_port': 0}\r\n2019-05-31 10:46:45,247 INFO (MainThread-23006) Feeding training data\r\n19/05/31 10:46:58 ERROR YarnScheduler: Lost executor 1 on ip-172-16-7-225.ec2.internal: Container killed by YARN for exceeding memory limits. 98.8 GB of 60 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.\r\n19/05/31 10:46:58 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, ip-172-16-7-225.ec2.internal, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 98.8 GB of 60 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.\r\n19/05/31 10:46:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container killed by YARN for exceeding memory limits. 98.8 GB of 60 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.\r\n19/05/31 10:46:58 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container killed by YARN for exceeding memory limits. 116.4 GB of 60 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.\r\n19/05/31 10:46:58 ERROR YarnScheduler: Lost executor 2 on ip-172-16-7-225.ec2.internal: Container killed by YARN for exceeding memory limits. 116.4 GB of 60 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.\r\n19/05/31 10:46:58 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2, ip-172-16-7-225.ec2.internal, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 116.4 GB of 60 GB virtual memory used. Consider boosting spark.yarn.executor.memoryOverhead.\r\n19/05/31 10:47:10 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 4, ip-172-16-7-225.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"/home/ubuntu/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 384, in _train\r\n  File \"./tfspark.zip/tensorflowonspark/util.py\", line 74, in read_executor_id\r\n    with open(\"executor_id\", \"r\") as f:\r\nIOError: [Errno 2] No such file or directory: 'executor_id'\r\n\r\nat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\nat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\nat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\nat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\nat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\nat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\nat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\nat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\nat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\nat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\nat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\nat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\nat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\nat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\nat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\nat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\nat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\nat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\nat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\nat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)\r\n\r\n19/05/31 10:47:11 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/TensorFlowOnSpark/examples/mnist/spark/mnist_spark.py\", line 66, in <module>\r\n    cluster.train(dataRDD, args.epochs)\r\n  File \"/home/ubuntu/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py\", line 92, in train\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in foreachPartition\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1056, in count\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1047, in sum\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 921, in fold\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 824, in collect\r\n  File \"/home/ubuntu/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\r\n  File \"/home/ubuntu/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 8, ip-172-16-7-225.ec2.internal, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"/home/ubuntu/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 384, in _train\r\n  File \"./tfspark.zip/tensorflowonspark/util.py\", line 74, in read_executor_id\r\n    with open(\"executor_id\", \"r\") as f:\r\nIOError: [Errno 2] No such file or directory: 'executor_id'\r\n\r\nat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\nat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\nat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\nat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\nat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\nat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\nat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\nat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\nat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\nat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\nat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\nat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\nat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\nat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\nat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\nat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\nat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\nat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\nat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\nat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\nat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\r\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\r\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\r\nat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\nat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\nat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\r\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\nat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\nat scala.Option.foreach(Option.scala:257)\r\nat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\nat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\r\nat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\r\nat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\r\nat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\nat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\nat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\r\nat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\r\nat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\r\nat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\r\nat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\r\nat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\nat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\nat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\nat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\r\nat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\r\nat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nat py4j.Gateway.invoke(Gateway.java:282)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\nat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"/home/ubuntu/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 384, in _train\r\n  File \"./tfspark.zip/tensorflowonspark/util.py\", line 74, in read_executor_id\r\n    with open(\"executor_id\", \"r\") as f:\r\nIOError: [Errno 2] No such file or directory: 'executor_id'\r\n\r\nat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\nat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\nat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\nat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\nat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\nat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\nat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\nat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\nat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\nat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\nat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\nat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\nat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\nat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\nat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\nat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\nat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\nat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\nat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\r\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\nat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n... 1 more\r\n\r\n19/05/31 10:47:11 WARN TaskSetManager: Lost task 1.2 in stage 1.0 (TID 9, ip-172-16-7-225.ec2.internal, executor 4): TaskKilled (Stage cancelled)\r\n\r\n**Logs:**\r\n1. Install Cuda 10.0:\r\nsudo apt-get install gnupg-curl\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1604_10.0.130-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install NVIDIA driver\r\n# Issue with driver install requires creating /usr/lib/nvidia\r\nsudo mkdir /usr/lib/nvidia\r\nsudo apt-get install --no-install-recommends nvidia-410\r\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-0 \\\r\n    libcudnn7=7.4.1.5-1+cuda10.0  \\\r\n    libcudnn7-dev=7.4.1.5-1+cuda10.0\r\n\r\nRef: https://www.tensorflow.org/install/gpu\r\n\r\n2. Steps in all instances:\r\nsudo pip install numpy \r\nsudo pip install tensorflow-gpu tensorflow on spark\r\nsudo apt-get install zip\r\n\r\n**Spark Submit Command Line:**\r\n\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode client \\\r\n--queue ${QUEUE} \\\r\n--num-executors 2 \\\r\n--executor-memory 10G \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.memory.storageFraction=0.2 \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.driver.memoryOverhead=1G \\\r\n--conf spark.yarn.executor.memoryOverhead=2G \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_CUDA:$LIB_JVM:$LIB_HDFS:/usr/local/cuda-10.0/extras/CUPTI/lib64 \\\r\n--driver-library-path=$LIB_CUDA \\\r\nTensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images hdfs://ec2-54-84-34-119.compute-1.amazonaws.com:9000/user/ubuntu/mnist/csv/train/images \\\r\n--labels hdfs://ec2-54-84-34-119.compute-1.amazonaws.com:9000/user/ubuntu/mnist/csv/train/labels \\\r\n--mode train \\\r\n--model mnist_model\r\n\r\n\r\n\r\nCan you please help me\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/425", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/425/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/425/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/425/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/425", "id": 447837575, "node_id": "MDU6SXNzdWU0NDc4Mzc1NzU=", "number": 425, "title": "ParameterServerStrategy stuck in \"Feeding partition\"", "user": {"login": "markromedia", "id": 806529, "node_id": "MDQ6VXNlcjgwNjUyOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/806529?v=4", "gravatar_id": "", "url": "https://api.github.com/users/markromedia", "html_url": "https://github.com/markromedia", "followers_url": "https://api.github.com/users/markromedia/followers", "following_url": "https://api.github.com/users/markromedia/following{/other_user}", "gists_url": "https://api.github.com/users/markromedia/gists{/gist_id}", "starred_url": "https://api.github.com/users/markromedia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/markromedia/subscriptions", "organizations_url": "https://api.github.com/users/markromedia/orgs", "repos_url": "https://api.github.com/users/markromedia/repos", "events_url": "https://api.github.com/users/markromedia/events{/privacy}", "received_events_url": "https://api.github.com/users/markromedia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2019-05-23T19:35:17Z", "updated_at": "2019-06-03T19:31:32Z", "closed_at": "2019-06-03T19:31:32Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [2.7]\r\n - Spark version [2.3.2]\r\n - TensorFlow version [1.13.1]\r\n - TensorFlowOnSpark version [1.4.3]\r\n - Cluster version [Hadoop 3.x]\r\n\r\n**Describe the bug:**\r\nUsing the examples provided I have been attempting to port one of our existing spark-ml jobs to Tfos. These jobs all run in a dedicated Yarn cluster(cpu-only).\r\n\r\nAs part of my POC, I am attempting to create a 2-worker, 1-ps, 1-master Tensor-cluster which trains a simple Keras model(converted to tf estimator), using a distributed strategy of ParameterServerStrategy.\r\n\r\nWhen I start this up, the cluster \"establishes\" itself, but I see the master gets stuck trying to feed the queue. I have run the same example in non-distributed mode, and it worked fine(multiple instances doing the same thing). \r\n\r\nThanks in advance for any help.\r\n\r\nHere is the relevant snippet of code running\r\n\r\n` model = Sequential()\r\n        model.add(Dense(64, input_dim=num_features, activation='sigmoid'))\r\n        model.add(Dropout(0.2))\r\n        model.add(Dense(64, activation='sigmoid'))\r\n        model.add(Dropout(0.2))\r\n        model.add(Dense(1, activation='sigmoid'))\r\n        model.compile(loss='binary_crossentropy',\r\n                      optimizer= tf.train.AdamOptimizer(),\r\n                      metrics=['accuracy'])\r\n        model.summary()\r\n\r\n        distribution_strategy = tf.contrib.distribute.ParameterServerStrategy()\r\n        config = tf.estimator.RunConfig(\r\n            train_distribute=distribution_strategy, eval_distribute=distribution_strategy)\r\n        estimator = tf.keras.estimator.model_to_estimator(model, model_dir=model_dir, config=config)\r\n\r\n\t\tdef generate_rdd_data(tf_feed):\r\n\t\t    while not tf_feed.should_stop():\r\n\t\t        batch = tf_feed.next_batch(1)\r\n\t\t        if len(batch) > 0:\r\n\t\t            record = batch[0]\r\n\t\t            features = numpy.array(record[0]).astype(numpy.float32)\r\n\t\t            label = numpy.array([record[1]]).astype(numpy.float32)\r\n\r\n\t\t            yield (features, label)\r\n\t\t        else:\r\n\t\t            return\r\n\r\n        def train_input_fn():\r\n            ds = tf.data.Dataset.from_generator(generator,\r\n                                                (tf.float32, tf.float32),\r\n                                                (tf.TensorShape([num_features]), tf.TensorShape([1])))\r\n            ds = ds.batch(args.batch_size)\r\n            return ds\r\n\r\n        # add a hook to terminate the RDD data feed when the session ends\r\n        hooks = [StopFeedHook(tf_feed)]\r\n\r\n        # train model\r\n        estimator.train(input_fn=train_input_fn, max_steps=steps_per_epoch, `hooks=hooks)`\r\n\r\n\r\n**Logs:**\r\nMaster Logs:\r\n`2019-05-23 18:49:54,434 INFO (MainThread-53285) 1: ======== master:0 ========\r\n2019-05-23 18:49:54,434 INFO (MainThread-53285) 1: Cluster spec: {'worker': ['10.90.28.232:34493', '10.90.28.252:38762'], 'ps': ['10.90.28.222:45450'], 'master': ['10.90.28.230:42065']}\r\n2019-05-23 18:49:54,435 INFO (MainThread-53285) 1: Using CPU\r\n19/05/23 18:49:54 INFO TorrentBroadcast: Started reading broadcast variable 110\r\n19/05/23 18:49:54 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 658.9 KB, free 1970.6 MB)\r\n19/05/23 18:49:54 INFO TorrentBroadcast: Reading broadcast variable 110 took 28 ms\r\n19/05/23 18:49:54 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 1434.9 KB, free 1969.2 MB)\r\n2019-05-23 18:49:54.479917: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz\r\n2019-05-23 18:49:54.481187: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x44295a0 executing computations on platform Host. Devices:\r\n2019-05-23 18:49:54.481225: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-23 18:49:54.483937: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job master -> {0 -> localhost:42065}\r\n2019-05-23 18:49:54.483963: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job ps -> {0 -> 10.90.28.222:45450}\r\n2019-05-23 18:49:54.483990: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> 10.90.28.232:34493, 1 -> 10.90.28.252:38762}\r\n2019-05-23 18:49:54.486650: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:42065\r\n19/05/23 18:49:54 INFO BlockManager: Found block rdd_412_3 locally\r\n19/05/23 18:49:54 INFO CodeGenerator: Code generated in 21.755654 ms\r\n19/05/23 18:49:54 INFO Executor: Finished task 3.0 in stage 73.0 (TID 777). 2805 bytes result sent to driver\r\n19/05/23 18:49:54 INFO CoarseGrainedExecutorBackend: Got assigned task 780\r\n19/05/23 18:49:54 INFO Executor: Running task 6.0 in stage 73.0 (TID 780)\r\n19/05/23 18:49:54 INFO BlockManager: Found block rdd_412_6 locally\r\n19/05/23 18:49:55 INFO Executor: Finished task 6.0 in stage 73.0 (TID 780). 2762 bytes result sent to driver\r\n19/05/23 18:49:55 INFO CoarseGrainedExecutorBackend: Got assigned task 782\r\n19/05/23 18:49:55 INFO Executor: Running task 10.0 in stage 73.0 (TID 782)\r\n19/05/23 18:49:55 INFO BlockManager: Found block rdd_412_10 locally\r\n19/05/23 18:49:55 INFO Executor: Finished task 10.0 in stage 73.0 (TID 782). 2762 bytes result sent to driver\r\n19/05/23 18:50:01 INFO CoarseGrainedExecutorBackend: Got assigned task 784\r\n19/05/23 18:50:01 INFO Executor: Running task 1.0 in stage 73.0 (TID 784)\r\n19/05/23 18:50:01 INFO BlockManager: Found block rdd_412_1 remotely\r\n19/05/23 18:50:01 INFO Executor: Finished task 1.0 in stage 73.0 (TID 784). 2762 bytes result sent to driver\r\n19/05/23 18:50:02 INFO CoarseGrainedExecutorBackend: Got assigned task 787\r\n19/05/23 18:50:02 INFO Executor: Running task 0.0 in stage 74.0 (TID 787)\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Updating epoch to 37 and clearing cache\r\n19/05/23 18:50:02 INFO TorrentBroadcast: Started reading broadcast variable 111\r\n19/05/23 18:50:02 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1969.2 MB)\r\n19/05/23 18:50:02 INFO TorrentBroadcast: Reading broadcast variable 111 took 5 ms\r\n19/05/23 18:50:02 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 23.4 KB, free 1969.2 MB)\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 36, fetching them\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@srv-01-11-b09.iad1.trmr.io:33383)\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Got the output locations\r\n19/05/23 18:50:02 INFO ShuffleBlockFetcherIterator: Getting 12 non-empty blocks out of 12 blocks\r\n19/05/23 18:50:02 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms\r\n2019-05-23 18:50:02,187 INFO (MainThread-52742) Connected to TFSparkNode.mgr on 10.90.28.230, executor=1, state='running'\r\n2019-05-23 18:50:02,194 INFO (MainThread-52742) mgr.state='running'\r\n2019-05-23 18:50:02,194 INFO (MainThread-52742) Feeding partition <itertools.chain object at 0x7f97ea46c2d0> into input queue <multiprocessing.queues.JoinableQueue object at 0x7f97dae9d2d0>\r\n19/05/23 19:00:03 ERROR Executor: Exception in task 0.0 in stage 74.0 (TID 787)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/hadoop/yarn/local/usercache/pipeline/appcache/application_1557769783296_1143/container_e21_1557769783296_1143_01_000005/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/hadoop/yarn/local/usercache/pipeline/appcache/application_1557769783296_1143/container_e21_1557769783296_1143_01_000005/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/usr/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 420, in _train\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition`\r\n\r\nWorker logs:\r\n019-05-23 18:49:53,977 INFO (MainThread-118610) Starting TensorFlow worker:0 as worker on cluster node 2 on background process\r\n19/05/23 18:49:53 INFO PythonRunner: Times: total = 7808, boot = -36392, init = 43165, finish = 1035\r\n19/05/23 18:49:53 INFO Executor: Finished task 2.0 in stage 72.0 (TID 773). 1418 bytes result sent to driver\r\n2019-05-23 18:49:53,985 INFO (MainThread-121583) 2: ======== worker:0 ========\r\n2019-05-23 18:49:53,986 INFO (MainThread-121583) 2: Cluster spec: {'worker': ['10.90.28.232:34493', '10.90.28.252:38762'], 'ps': ['10.90.28.222:45450'], 'master': ['10.90.28.230:42065']}\r\n2019-05-23 18:49:53,986 INFO (MainThread-121583) 2: Using CPU\r\n19/05/23 18:49:53 INFO CoarseGrainedExecutorBackend: Got assigned task 775\r\n19/05/23 18:49:53 INFO Executor: Running task 0.0 in stage 73.0 (TID 775)\r\n19/05/23 18:49:53 INFO TorrentBroadcast: Started reading broadcast variable 110\r\n19/05/23 18:49:54 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 658.9 KB, free 1970.6 MB)\r\n19/05/23 18:49:54 INFO TorrentBroadcast: Reading broadcast variable 110 took 50 ms\r\n19/05/23 18:49:54 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 1434.9 KB, free 1969.2 MB)\r\n19/05/23 18:49:54 INFO BlockManager: Found block rdd_412_0 locally\r\n2019-05-23 18:49:54.084395: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2599985000 Hz\r\n2019-05-23 18:49:54.085463: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4554d90 executing computations on platform Host. Devices:\r\n2019-05-23 18:49:54.085503: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n19/05/23 18:49:54 INFO CodeGenerator: Code generated in 24.400731 ms\r\n2019-05-23 18:49:54.095435: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job master -> {0 -> 10.90.28.230:42065}\r\n2019-05-23 18:49:54.095469: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job ps -> {0 -> 10.90.28.222:45450}\r\n2019-05-23 18:49:54.095481: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:34493, 1 -> 10.90.28.252:38762}\r\n2019-05-23 18:49:54.097460: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:34493\r\n2019-05-23 18:49:54,175 WARNING (MainThread-121583) From /usr/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-05-23 18:49:54,238 WARNING (MainThread-121583) From /usr/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n19/05/23 18:49:54 INFO Executor: Finished task 0.0 in stage 73.0 (TID 775). 2805 bytes result sent to driver\r\n19/05/23 18:49:54 INFO CoarseGrainedExecutorBackend: Got assigned task 776\r\n19/05/23 18:49:54 INFO Executor: Running task 4.0 in stage 73.0 (TID 776)\r\n19/05/23 18:49:54 INFO BlockManager: Found block rdd_412_4 locally\r\n19/05/23 18:49:54 INFO Executor: Finished task 4.0 in stage 73.0 (TID 776). 2762 bytes result sent to driver\r\n19/05/23 18:49:54 INFO CoarseGrainedExecutorBackend: Got assigned task 778\r\n19/05/23 18:49:54 INFO Executor: Running task 8.0 in stage 73.0 (TID 778)\r\n19/05/23 18:49:54 INFO BlockManager: Found block rdd_412_8 locally\r\n19/05/23 18:49:54 INFO Executor: Finished task 8.0 in stage 73.0 (TID 778). 2805 bytes result sent to driver\r\n_________________________________________________________________\r\nTotal params: 15,425\r\nTrainable params: 15,425\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nnum_features: 174\r\nnum_records: 240000\r\nbatch_size: 1953\r\nepochs: 3\r\nsteps_per_epoch: 128\r\n\r\nWARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n2019-05-23 18:49:57,229 INFO (MainThread-121583) ParameterServerStrategy with compute_devices = ('/replica:0/task:0/device:CPU:0',), variable_device = '/device:CPU:0'\r\n2019-05-23 18:49:57,229 INFO (MainThread-121583) TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'10.90.28.222:45450'], u'worker': [u'10.90.28.232:34493', u'10.90.28.252:38762'], u'master': [u'10.90.28.230:42065']}, u'task': {u'index': 0, u'type': u'worker'}}\r\n2019-05-23 18:49:57,229 INFO (MainThread-121583) Initializing RunConfig with distribution strategies.\r\n2019-05-23 18:49:57,230 INFO (MainThread-121583) Not using Distribute Coordinator.\r\n2019-05-23 18:49:57,230 INFO (MainThread-121583) Using the Keras model provided.\r\n2019-05-23 18:49:57,754 WARNING (MainThread-121583) From /usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n2019-05-23 18:49:58,405 INFO (MainThread-121583) Using config: {'_save_checkpoints_secs': 600, '_session_config': device_filters: \"/job:ps\"\r\ndevice_filters: \"/job:worker/task:0\"\r\nallow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_keep_checkpoint_max': 5, '_task_type': u'worker', '_train_distribute': <tensorflow.contrib.distribute.python.parameter_server_strategy.ParameterServerStrategy object at 0x7ff44bbdf1d0>, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff44bbdf050>, '_model_dir': '/tmp/model-20190523184945', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 3, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': <tensorflow.contrib.distribute.python.parameter_server_strategy.ParameterServerStrategy object at 0x7ff44bbdf1d0>, '_global_id_in_cluster': 1, '_master': u'grpc://10.90.28.232:34493', '_distribute_coordinator_mode': None}\r\n2019-05-23 18:49:58,413 WARNING (MainThread-121583) From /usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, use\r\n    tf.py_function, which takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    \r\n2019-05-23 18:49:58,545 INFO (Thread-1-121583) Calling model_fn.\r\n2019-05-23 18:49:59,510 INFO (Thread-1-121583) Done calling model_fn.\r\n2019-05-23 18:49:59,554 INFO (MainThread-121583) Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/model-20190523184945/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\n2019-05-23 18:49:59,554 INFO (MainThread-121583) Warm-starting from: ('/tmp/model-20190523184945/keras/keras_model.ckpt',)\r\n2019-05-23 18:49:59,555 INFO (MainThread-121583) Warm-starting variable: dense_2/kernel; prev_var_name: Unchanged\r\n2019-05-23 18:49:59,555 INFO (MainThread-121583) Warm-starting variable: dense/bias; prev_var_name: Unchanged\r\n2019-05-23 18:49:59,555 INFO (MainThread-121583) Warm-starting variable: dense_2/bias; prev_var_name: Unchanged\r\n2019-05-23 18:49:59,555 INFO (MainThread-121583) Warm-starting variable: dense_1/kernel; prev_var_name: Unchanged\r\n2019-05-23 18:49:59,555 INFO (MainThread-121583) Warm-starting variable: dense_1/bias; prev_var_name: Unchanged\r\n2019-05-23 18:49:59,555 INFO (MainThread-121583) Warm-starting variable: dense/kernel; prev_var_name: Unchanged\r\n2019-05-23 18:49:59,586 INFO (MainThread-121583) Create CheckpointSaverHook.\r\n2019-05-23 18:49:59,856 INFO (MainThread-121583) Graph was finalized.\r\n2019-05-23 18:49:59.897664: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session c7382e2c43bf9a42 with config: device_filters: \"/job:ps\" device_filters: \"/job:worker/task:0\" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } isolate_session_state: true\r\n2019-05-23 18:50:00,001 INFO (MainThread-121583) Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, training/TFOptimizer/beta1_power, training/TFOptimizer/beta2_power, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, dense_1/kernel/Adam, dense_1/kernel/Adam_1, dense_1/bias/Adam, dense_1/bias/Adam_1, dense_2/kernel/Adam, dense_2/kernel/Adam_1, dense_2/bias/Adam, dense_2/bias/Adam_1, ready: None\r\n19/05/23 18:50:01 INFO CoarseGrainedExecutorBackend: Got assigned task 785\r\n19/05/23 18:50:01 INFO Executor: Running task 5.0 in stage 73.0 (TID 785)\r\n19/05/23 18:50:01 INFO BlockManager: Found block rdd_412_5 remotely\r\n19/05/23 18:50:01 INFO Executor: Finished task 5.0 in stage 73.0 (TID 785). 2762 bytes result sent to driver\r\n19/05/23 18:50:02 INFO CoarseGrainedExecutorBackend: Got assigned task 789\r\n19/05/23 18:50:02 INFO Executor: Running task 2.0 in stage 74.0 (TID 789)\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Updating epoch to 37 and clearing cache\r\n19/05/23 18:50:02 INFO TorrentBroadcast: Started reading broadcast variable 111\r\n19/05/23 18:50:02 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1969.2 MB)\r\n19/05/23 18:50:02 INFO TorrentBroadcast: Reading broadcast variable 111 took 5 ms\r\n19/05/23 18:50:02 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 23.4 KB, free 1969.2 MB)\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 36, fetching them\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@srv-01-11-b09.iad1.trmr.io:33383)\r\n19/05/23 18:50:02 INFO MapOutputTrackerWorker: Got the output locations\r\n19/05/23 18:50:02 INFO ShuffleBlockFetcherIterator: Getting 12 non-empty blocks out of 12 blocks\r\n19/05/23 18:50:02 INFO ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms\r\n2019-05-23 18:50:02,149 INFO (MainThread-118615) Connected to TFSparkNode.mgr on 10.90.28.232, executor=2, state='running'\r\n2019-05-23 18:50:02,160 INFO (MainThread-118615) mgr.state='running'\r\n2019-05-23 18:50:02,160 INFO (MainThread-118615) Feeding partition <itertools.chain object at 0x7ff49de182d0> into input queue <multiprocessing.queues.JoinableQueue object at 0x7ff48e5f22d0>\r\n2019-05-23 18:50:30.041061: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 90084f8fef3f0cd9 with config: device_filters: \"/job:ps\" device_filters: \"/job:worker/task:0\" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } isolate_session_state: true\r\n2019-05-23 18:50:30,098 INFO (MainThread-121583) Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, training/TFOptimizer/beta1_power, training/TFOptimizer/beta2_power, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, dense_1/kernel/Adam, dense_1/kernel/Adam_1, dense_1/bias/Adam, dense_1/bias/Adam_1, dense_2/kernel/Adam, dense_2/kernel/Adam_1, dense_2/bias/Adam, dense_2/bias/Adam_1, ready: None\r\n2019-05-23 18:51:00.121986: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 55f754381ba9f6c4 with config: device_filters: \"/job:ps\" device_filters: \"/job:worker/task:0\" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } isolate_session_state: true\r\n2019-05-23 18:51:00,176 INFO (MainThread-121583) Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, training/TFOptimizer/beta1_power, training/TFOptimizer/beta2_power, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, dense_1/kernel/Adam, dense_1/kernel/Adam_1, dense_1/bias/Adam, dense_1/bias/Adam_1, dense_2/kernel/Adam, dense_2/kernel/Adam_1, dense_2/bias/Adam, dense_2/bias/Adam_1, ready: None\r\n2019-05-23 18:51:30.194164: I tensorflow/core/distributed_runtime/master_session.cc:1192] Start master session 920fab950aa69a50 with config: device_filters: \"/job:ps\" device_filters: \"/job:worker/task:0\" allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } } isolate_session_state: true\r\n2019-05-23 18:51:30,244 INFO (MainThread-121583) Waiting for model to be ready.  Ready_for_local_init_op:  Variables not initialized: global_step, dense/kernel, dense/bias, dense_1/kernel, dense_1/bias, dense_2/kernel, dense_2/bias, training/TFOptimizer/beta1_power, training/TFOptimizer/beta2_power, dense/kernel/Adam, dense/kernel/Adam_1, dense/bias/Adam, dense/bias/Adam_1, dense_1/kernel/Adam, dense_1/kernel/Adam_1, dense_1/bias/Adam, dense_1/bias/Adam_1, dense_2/kernel/Adam, dense_2/kernel/Adam_1, dense_2/bias/Adam, dense_2/bias/Adam_1, ready: None\r\n\r\nPS Logs:\r\n`019-05-23 18:49:55,059 INFO (MainThread-39243) 0: ======== ps:0 ========\r\n2019-05-23 18:49:55,060 INFO (MainThread-39243) 0: Cluster spec: {'worker': ['10.90.28.232:34493', '10.90.28.252:38762'], 'ps': ['10.90.28.222:45450'], 'master': ['10.90.28.230:42065']}\r\n2019-05-23 18:49:55,060 INFO (MainThread-39243) 0: Using CPU\r\n2019-05-23 18:49:55.163831: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200225000 Hz\r\n2019-05-23 18:49:55.167500: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7529890 executing computations on platform Host. Devices:\r\n2019-05-23 18:49:55.167542: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-23 18:49:55.185042: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job master -> {0 -> 10.90.28.230:42065}\r\n2019-05-23 18:49:55.185069: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job ps -> {0 -> localhost:45450}\r\n2019-05-23 18:49:55.185084: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> 10.90.28.232:34493, 1 -> 10.90.28.252:38762}\r\n2019-05-23 18:49:55.193529: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:45450\r\n\r\n**Spark Submit Command Line:**\r\n`--master yarn --queue default --conf spark.sql.autoBrodcastJoinThreshold=-1 --conf spark.yarn.executor.memoryOverhead=1g --conf spark.storage.memoryFraction=0.2 --conf spark.executor.memory\r\n=4g --conf spark.driver.memory=2g --conf spark.executor.instances=4 --conf spark.executor.cores=4 --conf spark.dynamicAllocation.enabled=false --conf spark.yarn.maxAppAttempts=1 --conf spark.task.cpus=4`\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/424", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/424/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/424/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/424/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/424", "id": 446560722, "node_id": "MDU6SXNzdWU0NDY1NjA3MjI=", "number": 424, "title": "Issue with TF-1.13.1", "user": {"login": "juzheng-zhang", "id": 48344295, "node_id": "MDQ6VXNlcjQ4MzQ0Mjk1", "avatar_url": "https://avatars2.githubusercontent.com/u/48344295?v=4", "gravatar_id": "", "url": "https://api.github.com/users/juzheng-zhang", "html_url": "https://github.com/juzheng-zhang", "followers_url": "https://api.github.com/users/juzheng-zhang/followers", "following_url": "https://api.github.com/users/juzheng-zhang/following{/other_user}", "gists_url": "https://api.github.com/users/juzheng-zhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/juzheng-zhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/juzheng-zhang/subscriptions", "organizations_url": "https://api.github.com/users/juzheng-zhang/orgs", "repos_url": "https://api.github.com/users/juzheng-zhang/repos", "events_url": "https://api.github.com/users/juzheng-zhang/events{/privacy}", "received_events_url": "https://api.github.com/users/juzheng-zhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-05-21T11:16:30Z", "updated_at": "2019-07-01T23:37:48Z", "closed_at": "2019-07-01T23:37:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.4.0\r\n - TensorFlow version 1.13.1\r\n - TensorFlowOnSpark version: latest\r\n\r\n**Describe the bug:**\r\nIt seems like an incompatible issue with TensorFlowOnSpark with tf-1.13.1\r\n\r\n**Logs:**\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 3 times, most recent failure: Lost task 0.2 in stage 3.0 (TID 206, 100.113.64.163, executor 1): java.lang.UnsatisfiedLinkError: /tmp/tensorflow_native_libraries-1558435821235-0/libtensorflow_jni.so: Error relocating /tmp/tensorflow_native_libraries-1558435821235-0/libtensorflow_jni.so: __strncpy_chk: symbol not found\r\n\t\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/423", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/423/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/423/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/423/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/423", "id": 443417359, "node_id": "MDU6SXNzdWU0NDM0MTczNTk=", "number": 423, "title": "mnist_spark.jpynb Run Distributed Training returned non-zero exit status 2", "user": {"login": "loik620", "id": 32890409, "node_id": "MDQ6VXNlcjMyODkwNDA5", "avatar_url": "https://avatars0.githubusercontent.com/u/32890409?v=4", "gravatar_id": "", "url": "https://api.github.com/users/loik620", "html_url": "https://github.com/loik620", "followers_url": "https://api.github.com/users/loik620/followers", "following_url": "https://api.github.com/users/loik620/following{/other_user}", "gists_url": "https://api.github.com/users/loik620/gists{/gist_id}", "starred_url": "https://api.github.com/users/loik620/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/loik620/subscriptions", "organizations_url": "https://api.github.com/users/loik620/orgs", "repos_url": "https://api.github.com/users/loik620/repos", "events_url": "https://api.github.com/users/loik620/events{/privacy}", "received_events_url": "https://api.github.com/users/loik620/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-05-13T14:03:47Z", "updated_at": "2019-06-19T21:45:17Z", "closed_at": "2019-06-19T21:45:17Z", "author_association": "NONE", "active_lock_reason": null, "body": " - Python version [ 3.6]\r\n - Spark version [ 2.4.1]\r\n - TensorFlow version [1.13.1]\r\n - TensorFlowOnSpark version [1.3.2]\r\n - Cluster version [Standalone]\r\n\r\n**Describe the bug:**\r\nwhen I run the example mnist_spark.jpynb get some problem,show in the image\r\n[\r\n![Screenshot from 2019-05-13 21-57-02](https://user-images.githubusercontent.com/32890409/57627853-65e79f00-75cb-11e9-9e5a-5a2afde8944b.png)\r\n](url)\r\n\r\nHave any problem in my read HDFS?  thank for help\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/422", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/422/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/422/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/422/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/422", "id": 442533524, "node_id": "MDU6SXNzdWU0NDI1MzM1MjQ=", "number": 422, "title": "stuck in feeding train data", "user": {"login": "downcastboy", "id": 38041637, "node_id": "MDQ6VXNlcjM4MDQxNjM3", "avatar_url": "https://avatars3.githubusercontent.com/u/38041637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/downcastboy", "html_url": "https://github.com/downcastboy", "followers_url": "https://api.github.com/users/downcastboy/followers", "following_url": "https://api.github.com/users/downcastboy/following{/other_user}", "gists_url": "https://api.github.com/users/downcastboy/gists{/gist_id}", "starred_url": "https://api.github.com/users/downcastboy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/downcastboy/subscriptions", "organizations_url": "https://api.github.com/users/downcastboy/orgs", "repos_url": "https://api.github.com/users/downcastboy/repos", "events_url": "https://api.github.com/users/downcastboy/events{/privacy}", "received_events_url": "https://api.github.com/users/downcastboy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 14, "created_at": "2019-05-10T04:29:24Z", "updated_at": "2019-08-07T20:26:33Z", "closed_at": "2019-06-19T21:46:23Z", "author_association": "NONE", "active_lock_reason": null, "body": "I tried many ways, but still didn't work,hope someone can help me,thanks.\r\n\r\nthe spark-submit as follow:\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--executor-cores 1 \\\r\n--num-executors 3 \\\r\n--queue default \\\r\n--py-files ${TFoS_HOME}/tfspark.zip,${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\n--conf spark.executorEnv.HADOOP_HDFS_HOME=$HADOOP_HDFS_HOME \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.executorEnv.CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath --glob) \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--images /examples/mnist/csv/train/images \\\r\n--labels /examples/mnist/csv/train/labels \\\r\n--format csv \\\r\n--steps 10 \\\r\n--mode train \\\r\n--model /mnist_model\r\n\r\nsome of the logs:\r\n\r\n2019-05-10 04:28:00,638 INFO (MainThread-32956) Feeding training data\r\n2019-05-10 04:28:00 INFO  SparkContext:54 - Starting job: collect at PythonRDD.scala:153\r\n2019-05-10 04:28:00 INFO  DAGScheduler:54 - Got job 2 (collect at PythonRDD.scala:153) with 10 output partitions\r\n2019-05-10 04:28:00 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (collect at PythonRDD.scala:153)\r\n2019-05-10 04:28:00 INFO  DAGScheduler:54 - Parents of final stage: List()\r\n2019-05-10 04:28:00 INFO  DAGScheduler:54 - Missing parents: List()\r\n2019-05-10 04:28:00 INFO  DAGScheduler:54 - Submitting ResultStage 2 (PythonRDD[11] at RDD at PythonRDD.scala:48), which has no missing parents\r\n2019-05-10 04:28:00 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 46.1 KB, free 911.6 MB)\r\n2019-05-10 04:28:00 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.9 KB, free 911.6 MB)\r\n2019-05-10 04:28:00 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on hadoop-maste:44195 (size: 11.9 KB, free: 912.2 MB)\r\n2019-05-10 04:28:00 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1039\r\n2019-05-10 04:28:00 INFO  DAGScheduler:54 - Submitting 10 missing tasks from ResultStage 2 (PythonRDD[11] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\r\n2019-05-10 04:28:00 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 10 tasks\r\n2019-05-10 04:28:01 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, 172.16.0.3, executor 0, partition 0, ANY, 8512 bytes)\r\n2019-05-10 04:28:01 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 1670 ms on 172.16.0.3 (executor 0) (1/1)\r\n2019-05-10 04:28:01 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool\r\n2019-05-10 04:28:01 INFO  DAGScheduler:54 - ResultStage 1 (foreachPartition at /root/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py:320) finished in 1.680 s\r\n2019-05-10 04:28:01 INFO  DAGScheduler:54 - Job 1 finished: foreachPartition at /root/TensorFlowOnSpark/tfspark.zip/tensorflowonspark/TFCluster.py:320, took 1.686167 s\r\n2019-05-10 04:28:01 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 172.16.0.3:34491 (size: 11.9 KB, free: 912.2 MB)", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/419", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/419/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/419/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/419/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/419", "id": 438880349, "node_id": "MDU6SXNzdWU0Mzg4ODAzNDk=", "number": 419, "title": "Unsuccessful TensorSliceReader constructor ", "user": {"login": "JCuomo", "id": 44904808, "node_id": "MDQ6VXNlcjQ0OTA0ODA4", "avatar_url": "https://avatars3.githubusercontent.com/u/44904808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCuomo", "html_url": "https://github.com/JCuomo", "followers_url": "https://api.github.com/users/JCuomo/followers", "following_url": "https://api.github.com/users/JCuomo/following{/other_user}", "gists_url": "https://api.github.com/users/JCuomo/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCuomo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCuomo/subscriptions", "organizations_url": "https://api.github.com/users/JCuomo/orgs", "repos_url": "https://api.github.com/users/JCuomo/repos", "events_url": "https://api.github.com/users/JCuomo/events{/privacy}", "received_events_url": "https://api.github.com/users/JCuomo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-04-30T17:00:09Z", "updated_at": "2019-07-16T17:06:52Z", "closed_at": "2019-07-16T17:06:51Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6.8]\r\n - Spark version [2.3.2]\r\n - TensorFlow version [1.13.1]\r\n - TensorFlowOnSpark version [1.4.2]\r\n - Cluster version [Hadoop 3.1.2]\r\n\r\n**Describe the bug:**\r\nI modified the mnist_mlp_estimator.py to use my CNN and my dataset but I'm having the following error.\r\n\r\n**Logs:**\r\n> 2019-04-30 10:31:43 INFO  TaskSetManager:54 - Finished task 1.0 in stage 0.0 (TID 1) in 18641 ms on 129.82.44.130 (executor 1) (2/3)\r\n2019-04-30 10:31:43 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 129.82.44.130:45643 (size: 8.8 KB, free: 366.3 MB)\r\n2019-04-30 10:31:43 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 129.82.44.130:45643 (size: 23.5 KB, free: 366.3 MB)\r\n2019-04-30 10:31:44 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 129.82.44.140:34601 (size: 23.5 KB, free: 366.2 MB)\r\n2019-04-30 10:31:44 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 129.82.44.130:45643 (size: 23.5 KB, free: 366.2 MB)\r\n2019-04-30 10:31:51 INFO  TaskSetManager:54 - Starting task 2.0 in stage 1.0 (TID 5, 129.82.44.130, executor 1, partition 2, ANY, 8508 bytes)\r\n2019-04-30 10:31:51 WARN  TaskSetManager:66 - Lost task 1.0 in stage 1.0 (TID 4, 129.82.44.130, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/s/chopin/a/grad/jcuomo/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n    process()\r\n  File \"/s/chopin/a/grad/jcuomo/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/s/chopin/a/grad/jcuomo/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/s/chopin/a/grad/jcuomo/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/s/chopin/a/grad/jcuomo/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2440, in pipeline_func\r\n  File \"/s/chopin/a/grad/jcuomo/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 350, in func\r\n  File \"/s/chopin/a/grad/jcuomo/spark-2.3.2-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 799, in func\r\n  File \"/s/chopin/a/grad/jcuomo/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 416, in _train\r\n    raise Exception(\"exception in worker:\\n\" + e_str)\r\nException: exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /tmp/tmpohomk_bc/keras/keras_model.ckpt: Not found: /tmp/tmpohomk_bc/keras; No such file or directory\r\n\t [[{{node checkpoint_initializer_35}}]]\r\n\r\n\r\n**Spark Submit Command Line:**\r\n`${SPARK_HOME}/bin/spark-submit --master ${MASTER} --conf spark.cores.max=${TOTAL_CORES} --conf spark.task.cpus=${CORES_PER_WORKER} --conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" /s/bach/b/class/cs535/cs535d/code/unet_v1_TFoS/trainTFoS_v4.py`\r\n\r\n**Code:**\r\n\r\n```\r\nimport numpy as np \r\nimport os\r\nimport skimage.io as io\r\nimport skimage.transform as trans\r\nimport numpy as np\r\nimport tensorflow.contrib.keras\r\nfrom tensorflow.contrib.keras import models\r\nfrom tensorflow.contrib.keras import layers \r\nimport tensorflow as tf\r\nimport time\r\nfrom datetime import datetime\r\nfrom tensorflowonspark import TFNode\r\n\r\nimport tensorflow as tf\r\ntry:\r\n    from tensorflow.contrib import keras as keras\r\n    print ('load keras from tensorflow package')\r\nexcept:\r\n    print ('update your tensorflow')\r\nfrom tensorflow.contrib.keras import models\r\nfrom tensorflow.contrib.keras import layers \r\n```\r\n```\r\n\r\nclass UNet():\r\n    def __init__(self):\r\n        print ('build UNet ...')\r\n\r\n    def get_crop_shape(self, target, refer):\r\n        cw = (target.get_shape()[2] - refer.get_shape()[2]).value\r\n        assert (cw >= 0)\r\n        if cw % 2 != 0:\r\n            cw1, cw2 = int(cw/2), int(cw/2) + 1\r\n        else:\r\n            cw1, cw2 = int(cw/2), int(cw/2)\r\n        ch = (target.get_shape()[1] - refer.get_shape()[1]).value\r\n        assert (ch >= 0)\r\n        if ch % 2 != 0:\r\n            ch1, ch2 = int(ch/2), int(ch/2) + 1\r\n        else:\r\n            ch1, ch2 = int(ch/2), int(ch/2)\r\n\r\n        return (ch1, ch2), (cw1, cw2)\r\n\r\n    def create_model(self, img_shape, num_class):\r\n\r\n        concat_axis = 3\r\n        inputs = layers.Input(shape = img_shape)\r\n\r\n        conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_1')(inputs)\r\n        conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\r\n        pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\r\n        conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\r\n        conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\r\n        pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\r\n\r\n        conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\r\n        conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\r\n        pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\r\n\r\n        conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\r\n        conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\r\n        pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\r\n\r\n        conv5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\r\n        conv5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\r\n\r\n        up_conv5 = layers.UpSampling2D(size=(2, 2))(conv5)\r\n        ch, cw = self.get_crop_shape(conv4, up_conv5)\r\n        crop_conv4 = layers.Cropping2D(cropping=(ch,cw))(conv4)\r\n        up6 = layers.concatenate([up_conv5, crop_conv4], axis=concat_axis)\r\n        conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\r\n        conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\r\n\r\n        up_conv6 = layers.UpSampling2D(size=(2, 2))(conv6)\r\n        ch, cw = self.get_crop_shape(conv3, up_conv6)\r\n        crop_conv3 = layers.Cropping2D(cropping=(ch,cw))(conv3)\r\n        up7 = layers.concatenate([up_conv6, crop_conv3], axis=concat_axis) \r\n        conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\r\n        conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\r\n\r\n        up_conv7 = layers.UpSampling2D(size=(2, 2))(conv7)\r\n        ch, cw = self.get_crop_shape(conv2, up_conv7)\r\n        crop_conv2 = layers.Cropping2D(cropping=(ch,cw))(conv2)\r\n        up8 = layers.concatenate([up_conv7, crop_conv2], axis=concat_axis)\r\n        conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\r\n        conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\r\n\r\n        up_conv8 = layers.UpSampling2D(size=(2, 2))(conv8)\r\n        ch, cw = self.get_crop_shape(conv1, up_conv8)\r\n        crop_conv1 = layers.Cropping2D(cropping=(ch,cw))(conv1)\r\n        up9 = layers.concatenate([up_conv8, crop_conv1], axis=concat_axis)\r\n        conv9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\r\n        conv9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\r\n\r\n        ch, cw = self.get_crop_shape(inputs, conv9)\r\n        conv9 = layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(conv9)\r\n        conv10 = layers.Conv2D(1, (1, 1))(conv9)\r\n\r\n        model = models.Model(inputs=inputs, outputs=conv10)\r\n\r\n        return model\r\n\r\n```\r\n\r\n```\r\n\r\ndef main_fun(args, ctx):\r\n    print(\"LOADING IMAGES geneTrainNpy DONE--------------------------------------------------------------------\")\r\n    x_test,y_test = geneTrainNpy(\"/s/bach/b/class/cs535/cs535d/code/unet_v1_TFoS/data/hands/train/aug/\",\"/s/bach/b/class/cs535/cs535d/code/unet_v1_TFoS/data/hands/train/aug/\")\r\n\r\n    img_shape = [256,256]\r\n    print(\"CREATING MODEL--------------------------------------------------------------------\")\r\n    model = UNet().create_model(img_shape=img_shape+[1], num_class=1)\r\n    img = model.input\r\n    pred = model.output\r\n\r\n    opt = tf.train.AdamOptimizer(learning_rate= 1e-4)\r\n    print(\"COMPILING MODEL--------------------------------------------------------------------\")\r\n    model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n    print(\"CONVERTING MODEL TO TF ESTIMATOR--------------------------------------------------------------------\")\r\n    estimator = tf.keras.estimator.model_to_estimator(keras_model=model)\r\n\r\n    tf_feed = TFNode.DataFeed(ctx.mgr)\r\n    \r\n    def rdd_generator():\r\n        print(\"rdd_generator--------------------------------------------------------------------\")\r\n        while not tf_feed.should_stop():\r\n            batch = tf_feed.next_batch(1)\r\n            if len(batch) > 0:\r\n                record = batch[0]\r\n\r\n                image = numpy.array(record[0]).reshape(256,256,1).astype(numpy.float32) / 255.0\r\n                label = numpy.array(record[1]).reshape(256,256,1).astype(numpy.float32) / 255.0\r\n                print(\"SHAPE0\",record[0])\r\n                print(\"SHAPE1\",numpy.array(record[0]))\r\n                print(\"SHAPE2\",numpy.array(record[0]).reshape(256,256))\r\n                print(\"SHAPE3\",image)\r\n                yield (image, label)\r\n            else:\r\n                return\r\n            \r\n    def train_input_fn():\r\n        print(\"train_input_fn--------------------------------------------------------------------\")\r\n        ds = tf.data.Dataset.from_generator(rdd_generator,\r\n                                          (tf.float32, tf.float32),\r\n                                          (tf.TensorShape([256,256, 1]), tf.TensorShape([256,256, 1])))\r\n\r\n        return ds.batch(args.batch_size) \r\n\r\n    hooks = [StopFeedHook(tf_feed)]\r\n    \r\n    print(\"eval_input_fn--------------------------------------------------------------------\")\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"dense_input\": x_test},\r\n        y=y_test,\r\n        num_epochs=1,\r\n        shuffle=False)\r\n    print(\"train_and_evaluate--------------------------------------------------------------------\")\r\n    feature_spec = {'dense_input': tf.placeholder(tf.float32, shape=[None, 256, 256, 1])}\r\n    exporter = tf.estimator.FinalExporter(\"serving\", serving_input_receiver_fn=tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec))\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=args.steps, hooks=hooks)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, exporters=exporter)\r\n    print(\"train_and_evaluate2--------------------------------------------------------------------\")\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\n\r\nclass StopFeedHook(tf.train.SessionRunHook):\r\n    \"\"\"SessionRunHook to terminate InputMode.SPARK RDD feeding if the training loop exits before the entire RDD is consumed.\"\"\"\r\n\r\n    def __init__(self, tf_feed):\r\n        self._tf_feed = tf_feed\r\n\r\n    def end(self, session):\r\n        self._tf_feed.terminate()\r\n        self._tf_feed.next_batch(1)\r\n\r\nif __name__ == '__main__':\r\n    import argparse\r\n    import sys\r\n    from pyspark.context import SparkContext\r\n    from pyspark.conf import SparkConf\r\n    from tensorflowonspark import TFCluster\r\n\r\n    sc = SparkContext(conf=SparkConf().setAppName(\"trainTFoS\"))\r\n    executors = sc._conf.get(\"spark.executor.instances\")\r\n    sc.addFile(\"/s/bach/b/class/cs535/cs535d/code/unet_v1_TFoS/data.py\")\r\n\r\n    from data import *\t\r\n    num_executors = int(executors) if executors is not None else 1\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--batch_size\", help=\"number of records per batch\", type=int, default=100)\r\n    parser.add_argument(\"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=num_executors)\r\n    parser.add_argument(\"--epochs\", help=\"number of epochs of training data\", type=int, default=None)\r\n    parser.add_argument(\"--images\", help=\"HDFS path to MNIST images in parallelized CSV format\")\r\n    parser.add_argument(\"--input_mode\", help=\"input mode (tf|spark)\", default=\"tf\")\r\n    parser.add_argument(\"--labels\", help=\"HDFS path to MNIST labels in parallelized CSV format\")\r\n    parser.add_argument(\"--model_dir\", help=\"directory to write model checkpoints\")\r\n    parser.add_argument(\"--output\", help=\"HDFS path to save test/inference output\", default=\"predictions\")\r\n    parser.add_argument(\"--num_ps\", help=\"number of ps nodes\", type=int, default=1)\r\n    parser.add_argument(\"--steps\", help=\"max number of steps to train\", type=int, default=2000)\r\n    parser.add_argument(\"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\r\n\r\n    sys.argv = ['--batch_size 100', \r\n                '--cluster_size', '3', \r\n                '--epochs', '1', \r\n                '--images', 'HANDS/csv/train/images', \r\n                '--labels', 'HANDS/csv/train/labels', \r\n                '--model_dir', '/s/bach/b/class/cs535/cs535d/', \r\n                '--output', '/s/bach/b/class/cs535/cs535d/', \r\n                '--num_ps', '1', \r\n                '--steps', '2000',\r\n                '--tensorboard']\r\n    \r\n    args = parser.parse_args()\r\n    print(\"args:\", args)\r\n\r\n    images = sc.textFile(args.images).map(lambda ln: [float(x) for x in ln.split(',')])\r\n    print(\"images\",images)\r\n    labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\r\n    print(\"LOADING IMAGES DONE--------------------------------------------------------------------\")\r\n\r\n    def to_simple_rdd(sc, features, labels):\r\n        pairs = [(x, y) for x, y in zip(features, labels)]\r\n        return sc.parallelize(pairs)\r\n\r\n    imgs_train,mask_train = geneTrainNpy(\"/s/bach/b/class/cs535/cs535d/code/unet_v1_TFoS/data/hands/train/aug/\",\"/s/bach/b/class/cs535/cs535d/code/unet_v1_TFoS/data/hands/train/aug/\")\r\n    rdd = to_simple_rdd(sc, imgs_train, mask_train)\r\n\r\n    dataRDD = images.zip(labels)\r\n    print(\"ZIP IMAGES DONE--------------------------------------------------------------------\")\r\n    cluster = TFCluster.run(sc, main_fun, args, args.cluster_size, args.num_ps, args.tensorboard, TFCluster.InputMode.SPARK, log_dir=args.model_dir, master_node='master')\r\n    print(\"RUNNING CLUSTER--------------------------------------------------------------------\")\r\n    cluster.train(dataRDD, args.epochs)\r\n    print(\"TRAIN--------------------------------------------------------------------\")\r\n    cluster.shutdown()\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/418", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/418/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/418/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/418/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/418", "id": 437628221, "node_id": "MDU6SXNzdWU0Mzc2MjgyMjE=", "number": 418, "title": "Exceptions after traing loops may be lost.", "user": {"login": "ZisZ", "id": 578966, "node_id": "MDQ6VXNlcjU3ODk2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/578966?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZisZ", "html_url": "https://github.com/ZisZ", "followers_url": "https://api.github.com/users/ZisZ/followers", "following_url": "https://api.github.com/users/ZisZ/following{/other_user}", "gists_url": "https://api.github.com/users/ZisZ/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZisZ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZisZ/subscriptions", "organizations_url": "https://api.github.com/users/ZisZ/orgs", "repos_url": "https://api.github.com/users/ZisZ/repos", "events_url": "https://api.github.com/users/ZisZ/events{/privacy}", "received_events_url": "https://api.github.com/users/ZisZ/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-04-26T11:26:59Z", "updated_at": "2019-05-10T19:59:48Z", "closed_at": "2019-05-10T19:59:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [e.g. 3.6]\r\n - Spark version [e.g. 2.3.1]\r\n - TensorFlow version [e.g. 1.9.0]\r\n - TensorFlowOnSpark version [e.g. 1.4.3]\r\n - Cluster version [e.g. Hadoop 2.8]\r\n\r\n**Describe the bug:**\r\nI met a similar problem with https://github.com/yahoo/TensorFlowOnSpark/issues/141#issue-265474495. But because that issue is already closed, so I open this one in case of not noticed.\r\n\r\nWhile tunning training process, I found that even if the process \"seems\" correctly finished, the result of code between the end of training loops and the end of my map_fun did not match what I expecting. And then I realized actually exceptions occured but then they were lost.\r\n\r\nThe reason is also the race condition. The TF process are doing something wrong and exceptions are send into error_queue, but the data feeding job already finished and no one will consume error_queue.\r\n\r\nIt is really uncomfortable while debugging. How about check error_queue again before shutdown all queues.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/417", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/417/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/417/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/417/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/417", "id": 437319394, "node_id": "MDU6SXNzdWU0MzczMTkzOTQ=", "number": 417, "title": "[question]no result  in predictions in MNIST prediction", "user": {"login": "RyougiTale", "id": 29205475, "node_id": "MDQ6VXNlcjI5MjA1NDc1", "avatar_url": "https://avatars0.githubusercontent.com/u/29205475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RyougiTale", "html_url": "https://github.com/RyougiTale", "followers_url": "https://api.github.com/users/RyougiTale/followers", "following_url": "https://api.github.com/users/RyougiTale/following{/other_user}", "gists_url": "https://api.github.com/users/RyougiTale/gists{/gist_id}", "starred_url": "https://api.github.com/users/RyougiTale/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RyougiTale/subscriptions", "organizations_url": "https://api.github.com/users/RyougiTale/orgs", "repos_url": "https://api.github.com/users/RyougiTale/repos", "events_url": "https://api.github.com/users/RyougiTale/events{/privacy}", "received_events_url": "https://api.github.com/users/RyougiTale/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-04-25T17:42:24Z", "updated_at": "2019-08-14T00:24:11Z", "closed_at": "2019-04-30T04:26:40Z", "author_association": "NONE", "active_lock_reason": null, "body": "Environment:\r\nPython_version 2.7.5\r\nspark_version spark-2.4.0-bin-hadoop2.7\r\ntensorflow_version 1.13.1\r\nTensorFlowOnSpark_version 1.4.2\r\nCluster_version Standalone\r\n\r\nquestion:\r\nonly _SUCCESS in predictions\r\n\r\nI am a student new to learn spark\r\nI have a cluster consisting of 1 master node and 2 slave nodes,\r\n![image](https://user-images.githubusercontent.com/29205475/56755297-cbf9b700-67c1-11e9-82ba-58d67c7fd67a.png)\r\n\r\nI followed this website\r\nhttps://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_Standalone\r\n\r\n1.after Converting the MNIST zip files using Spark\r\n```\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--conf spark.network.timeout=420000 \\\r\n--master spark://192.168.2.15:7077 \\\r\n/root/TensorFlowOnSpark/examples/mnist/mnist_data_setup.py \\\r\n--output examples/mnist/csv \\\r\n--format csv\r\nin my master node,\r\nls -lR examples/mnist/csv\r\n```\r\n![image](https://user-images.githubusercontent.com/29205475/56755628-87225000-67c2-11e9-9896-c323d23d151e.png)\r\n\r\nin my slave nodes,\r\n![image](https://user-images.githubusercontent.com/29205475/56755820-f39d4f00-67c2-11e9-87d7-623b0943ad5f.png)\r\n![image](https://user-images.githubusercontent.com/29205475/56755894-1b8cb280-67c3-11e9-92e3-464c26ba4b32.png)\r\n\r\n\r\n2.after Runing distributed MNIST training (using feed_dict)\r\n```\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master spark://192.168.2.15:7077 \\\r\n--py-files ${TFoS_HOME}/tfspark.zip,${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=16 \\\r\n--conf spark.task.cpus=8 \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n--conf spark.network.timeout=420000 \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size 2 \\\r\n--images examples/mnist/csv/train/images \\\r\n--labels examples/mnist/csv/train/labels \\\r\n--format csv \\\r\n--mode train \\\r\n--model mnist_model\r\n```\r\n\r\nonly a slave node own the mnist_model\r\n![image](https://user-images.githubusercontent.com/29205475/56756045-7de5b300-67c3-11e9-8d8c-230fdcc5b508.png)\r\n\r\nthen,3,Run distributed MNIST inference (using feed_dict)\r\n```\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master spark://192.168.2.15:7077 \\\r\n--py-files ${TFoS_HOME}/tfspark.zip,${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.cores.max=16 \\\r\n--conf spark.task.cpus=8 \\\r\n--conf spark.network.timeout=420000 \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size 2 \\\r\n--images examples/mnist/csv/test/images \\\r\n--labels examples/mnist/csv/test/labels \\\r\n--mode inference \\\r\n--format csv \\\r\n--model mnist_model \\\r\n--output predictions\r\n```\r\n\r\nonly one error in logs\r\n![image](https://user-images.githubusercontent.com/29205475/56756470-66f39080-67c4-11e9-9c46-2181547ee30f.png)\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/29205475/56756230-e59bfe00-67c3-11e9-9280-0a342963e760.png)\r\nonly a empty file in my master node\r\nwhat's wrong with this?\r\nplease help me", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/416", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/416/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/416/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/416/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/416", "id": 435700562, "node_id": "MDU6SXNzdWU0MzU3MDA1NjI=", "number": 416, "title": "Does TensoreFlowOnSpark support dynamic resource allocation?", "user": {"login": "chen116", "id": 13974230, "node_id": "MDQ6VXNlcjEzOTc0MjMw", "avatar_url": "https://avatars0.githubusercontent.com/u/13974230?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chen116", "html_url": "https://github.com/chen116", "followers_url": "https://api.github.com/users/chen116/followers", "following_url": "https://api.github.com/users/chen116/following{/other_user}", "gists_url": "https://api.github.com/users/chen116/gists{/gist_id}", "starred_url": "https://api.github.com/users/chen116/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chen116/subscriptions", "organizations_url": "https://api.github.com/users/chen116/orgs", "repos_url": "https://api.github.com/users/chen116/repos", "events_url": "https://api.github.com/users/chen116/events{/privacy}", "received_events_url": "https://api.github.com/users/chen116/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-04-22T11:48:47Z", "updated_at": "2019-05-10T19:59:59Z", "closed_at": "2019-05-10T19:59:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am asking because I am thinking about using it in streaming application, with dynamic resource allocation ( adjusting number of executors). Thanks", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/415", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/415/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/415/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/415/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/415", "id": 435679243, "node_id": "MDU6SXNzdWU0MzU2NzkyNDM=", "number": 415, "title": "How to run streaming example", "user": {"login": "chen116", "id": 13974230, "node_id": "MDQ6VXNlcjEzOTc0MjMw", "avatar_url": "https://avatars0.githubusercontent.com/u/13974230?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chen116", "html_url": "https://github.com/chen116", "followers_url": "https://api.github.com/users/chen116/followers", "following_url": "https://api.github.com/users/chen116/following{/other_user}", "gists_url": "https://api.github.com/users/chen116/gists{/gist_id}", "starred_url": "https://api.github.com/users/chen116/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chen116/subscriptions", "organizations_url": "https://api.github.com/users/chen116/orgs", "repos_url": "https://api.github.com/users/chen116/repos", "events_url": "https://api.github.com/users/chen116/events{/privacy}", "received_events_url": "https://api.github.com/users/chen116/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-22T10:21:51Z", "updated_at": "2019-05-03T20:11:38Z", "closed_at": "2019-05-03T20:11:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hi, \r\n\r\nso I am following the yarn setup(https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN), so far is able to run the mnist example without spark streaming. \r\n\r\nBut when I am confused how to run the streaming example, \r\n\r\nto be specific, this is how I try to run the example trying to follow the guide:\r\n\r\n1. I start the training job, dump all the training samples part-0000[0-9] one by one into the stream_data, one thing i notice is, the console output of \"New files at time xxxxx ms\" is always empty  even though I starting dumping training data after the spark job is running. \r\n\r\n2. now I stop the training process, fire up the inference process. well same thing, the console output of \"New files at time xxxxx ms\" is always empty  even though I starting dumping testing data after the spark job is running. And from the output files, its empty under every batch output. \r\n\r\n\r\nAny idea what I am missing? thanks\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/414", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/414/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/414/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/414/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/414", "id": 434664341, "node_id": "MDU6SXNzdWU0MzQ2NjQzNDE=", "number": 414, "title": "Get stuck in \"Feeding inference data\"", "user": {"login": "EliGwz", "id": 15846531, "node_id": "MDQ6VXNlcjE1ODQ2NTMx", "avatar_url": "https://avatars3.githubusercontent.com/u/15846531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EliGwz", "html_url": "https://github.com/EliGwz", "followers_url": "https://api.github.com/users/EliGwz/followers", "following_url": "https://api.github.com/users/EliGwz/following{/other_user}", "gists_url": "https://api.github.com/users/EliGwz/gists{/gist_id}", "starred_url": "https://api.github.com/users/EliGwz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EliGwz/subscriptions", "organizations_url": "https://api.github.com/users/EliGwz/orgs", "repos_url": "https://api.github.com/users/EliGwz/repos", "events_url": "https://api.github.com/users/EliGwz/events{/privacy}", "received_events_url": "https://api.github.com/users/EliGwz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-04-18T09:02:28Z", "updated_at": "2019-05-03T20:10:03Z", "closed_at": "2019-05-03T20:10:02Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.4.0\r\n - TensorFlow version 1.13.1\r\n - TensorFlowOnSpark version 1.4.2\r\n - Cluster version Hadoop 2.7.5\r\n\r\n**Describe the bug:**\r\nI have replaced the original mnist data set with my own data set(500\u00d7500pixels) in the same data format, and I could successfully complete the \"Convert the MNIST zip files into HDFS files\" and \"Run distributed MNIST training (using feed_dict)\" steps. However, it gets stuck in running \"Run distributed MNIST inference (using feed_dict)\" without any error information. \r\n\r\nI only have 20 images in test set so I think it is abnormal to run the inference steps for hours without any prediction results. \r\n\r\n**Logs:**\r\n2019-04-18 07:28:54 INFO  SparkContext:54 - Running Spark version 2.4.0\r\n2019-04-18 07:28:54 INFO  SparkContext:54 - Submitted application: mnist_spark\r\n2019-04-18 07:28:54 INFO  SecurityManager:54 - Changing view acls to: hduser\r\n2019-04-18 07:28:54 INFO  SecurityManager:54 - Changing modify acls to: hduser\r\n2019-04-18 07:28:54 INFO  SecurityManager:54 - Changing view acls groups to: \r\n2019-04-18 07:28:54 INFO  SecurityManager:54 - Changing modify acls groups to: \r\n2019-04-18 07:28:54 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()\r\n2019-04-18 07:28:54 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 43775.\r\n2019-04-18 07:28:54 INFO  SparkEnv:54 - Registering MapOutputTracker\r\n2019-04-18 07:28:54 INFO  SparkEnv:54 - Registering BlockManagerMaster\r\n2019-04-18 07:28:54 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n2019-04-18 07:28:54 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\r\n2019-04-18 07:28:54 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-5ae1ccc8-9a61-4096-9341-e442f2da6aa3\r\n2019-04-18 07:28:54 INFO  MemoryStore:54 - MemoryStore started with capacity 2004.6 MB\r\n2019-04-18 07:28:54 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\r\n2019-04-18 07:28:54 INFO  log:192 - Logging initialized @4224ms\r\n2019-04-18 07:28:54 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\r\n2019-04-18 07:28:54 INFO  Server:419 - Started @4269ms\r\n2019-04-18 07:28:54 INFO  AbstractConnector:278 - Started ServerConnector@658d068f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n2019-04-18 07:28:54 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@47c3dca9{/jobs,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7b90d84e{/jobs/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7ddde85d{/jobs/job,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7ca511c{/jobs/job/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7ae5ab03{/stages,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68e83b02{/stages/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@73dbde39{/stages/stage,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@58bf2c8b{/stages/stage/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@75b81bb3{/stages/pool,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@322c0da0{/stages/pool/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7286687a{/storage,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@53fcc9d5{/storage/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@662c359a{/storage/rdd,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34bf75f2{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@753b1ea7{/environment,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@40bcd3b4{/environment/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@473baedc{/executors,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@864432c{/executors/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2587baa1{/executors/threadDump,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@380fcd6{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@19d7ef34{/static,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@40bdfdf1{/,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@64bf5fd5{/api,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@65c01029{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@8920623{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n2019-04-18 07:28:54 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://gpu4:4040\r\n2019-04-18 07:28:54 INFO  RMProxy:98 - Connecting to ResourceManager at gpu4/10.42.2.24:8032\r\n2019-04-18 07:28:55 INFO  Client:54 - Requesting a new application from cluster with 8 NodeManagers\r\n2019-04-18 07:28:55 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (10240 MB per container)\r\n2019-04-18 07:28:55 INFO  Client:54 - Will allocate AM container, with 4505 MB memory including 409 MB overhead\r\n2019-04-18 07:28:55 INFO  Client:54 - Setting up container launch context for our AM\r\n2019-04-18 07:28:55 INFO  Client:54 - Setting up the launch environment for our AM container\r\n2019-04-18 07:28:55 INFO  Client:54 - Preparing resources for our AM container\r\n2019-04-18 07:28:55 INFO  Client:54 - Source and destination file systems are the same. Not copying hdfs://gpu4:9000/spark-archive.zip\r\n2019-04-18 07:28:55 INFO  Client:54 - Source and destination file systems are the same. Not copying hdfs://gpu4:9000/user/hduser/Python.zip#Python\r\n2019-04-18 07:28:55 INFO  Client:54 - Uploading resource file:/opt/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip -> hdfs://gpu4:9000/user/hduser/.sparkStaging/application_1555572010748_0001/pyspark.zip\r\n2019-04-18 07:28:56 INFO  Client:54 - Uploading resource file:/opt/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip -> hdfs://gpu4:9000/user/hduser/.sparkStaging/application_1555572010748_0001/py4j-0.10.7-src.zip\r\n2019-04-18 07:28:56 INFO  Client:54 - Uploading resource file:/home/hduser/TensorFlowOnSpark/tfspark.zip -> hdfs://gpu4:9000/user/hduser/.sparkStaging/application_1555572010748_0001/tfspark.zip\r\n2019-04-18 07:28:56 INFO  Client:54 - Uploading resource file:/home/hduser/TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py -> hdfs://gpu4:9000/user/hduser/.sparkStaging/application_1555572010748_0001/mnist_dist.py\r\n2019-04-18 07:28:56 INFO  Client:54 - Uploading resource file:/tmp/spark-1725e9a8-9be2-4901-b485-68e10d584689/__spark_conf__6686600139045668008.zip -> hdfs://gpu4:9000/user/hduser/.sparkStaging/application_1555572010748_0001/__spark_conf__.zip\r\n2019-04-18 07:28:57 INFO  SecurityManager:54 - Changing view acls to: hduser\r\n2019-04-18 07:28:57 INFO  SecurityManager:54 - Changing modify acls to: hduser\r\n2019-04-18 07:28:57 INFO  SecurityManager:54 - Changing view acls groups to: \r\n2019-04-18 07:28:57 INFO  SecurityManager:54 - Changing modify acls groups to: \r\n2019-04-18 07:28:57 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()\r\n2019-04-18 07:28:57 INFO  Client:54 - Submitting application application_1555572010748_0001 to ResourceManager\r\n2019-04-18 07:28:58 INFO  YarnClientImpl:273 - Submitted application application_1555572010748_0001\r\n2019-04-18 07:28:58 INFO  SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1555572010748_0001 and attemptId None\r\n2019-04-18 07:28:59 INFO  Client:54 - Application report for application_1555572010748_0001 (state: ACCEPTED)\r\n2019-04-18 07:28:59 INFO  Client:54 - \r\n\t client token: N/A\r\n\t diagnostics: N/A\r\n\t ApplicationMaster host: N/A\r\n\t ApplicationMaster RPC port: -1\r\n\t queue: default\r\n\t start time: 1555572538144\r\n\t final status: UNDEFINED\r\n\t tracking URL: http://gpu4:8088/proxy/application_1555572010748_0001/\r\n\t user: hduser\r\n2019-04-18 07:29:00 INFO  Client:54 - Application report for application_1555572010748_0001 (state: ACCEPTED)\r\n2019-04-18 07:29:01 INFO  Client:54 - Application report for application_1555572010748_0001 (state: ACCEPTED)\r\n2019-04-18 07:29:02 INFO  Client:54 - Application report for application_1555572010748_0001 (state: ACCEPTED)\r\n2019-04-18 07:29:03 INFO  Client:54 - Application report for application_1555572010748_0001 (state: ACCEPTED)\r\n2019-04-18 07:29:04 INFO  Client:54 - Application report for application_1555572010748_0001 (state: ACCEPTED)\r\n2019-04-18 07:29:05 INFO  Client:54 - Application report for application_1555572010748_0001 (state: ACCEPTED)\r\n2019-04-18 07:29:06 INFO  YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> gpu4, PROXY_URI_BASES -> http://gpu4:8088/proxy/application_1555572010748_0001), /proxy/application_1555572010748_0001\r\n2019-04-18 07:29:06 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\r\n2019-04-18 07:29:06 INFO  YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\r\n2019-04-18 07:29:06 INFO  Client:54 - Application report for application_1555572010748_0001 (state: RUNNING)\r\n2019-04-18 07:29:06 INFO  Client:54 - \r\n\t client token: N/A\r\n\t diagnostics: N/A\r\n\t ApplicationMaster host: 10.42.1.21\r\n\t ApplicationMaster RPC port: -1\r\n\t queue: default\r\n\t start time: 1555572538144\r\n\t final status: UNDEFINED\r\n\t tracking URL: http://gpu4:8088/proxy/application_1555572010748_0001/\r\n\t user: hduser\r\n2019-04-18 07:29:06 INFO  YarnClientSchedulerBackend:54 - Application application_1555572010748_0001 has started running.\r\n2019-04-18 07:29:06 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34341.\r\n2019-04-18 07:29:06 INFO  NettyBlockTransferService:54 - Server created on gpu4:34341\r\n2019-04-18 07:29:06 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n2019-04-18 07:29:06 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, gpu4, 34341, None)\r\n2019-04-18 07:29:06 INFO  BlockManagerMasterEndpoint:54 - Registering block manager gpu4:34341 with 2004.6 MB RAM, BlockManagerId(driver, gpu4, 34341, None)\r\n2019-04-18 07:29:06 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, gpu4, 34341, None)\r\n2019-04-18 07:29:06 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, gpu4, 34341, None)\r\n2019-04-18 07:29:06 INFO  JettyUtils:54 - Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\r\n2019-04-18 07:29:06 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@618c40ae{/metrics/json,null,AVAILABLE,@Spark}\r\n2019-04-18 07:29:06 INFO  EventLoggingListener:54 - Logging events to hdfs://gpu4:9000/tmp/sparkLog/application_1555572010748_0001\r\n2019-04-18 07:29:12 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.42.1.21:55958) with ID 7\r\n2019-04-18 07:29:13 INFO  BlockManagerMasterEndpoint:54 - Registering block manager student11-x2:43385 with 2004.6 MB RAM, BlockManagerId(7, student11-x2, 43385, None)\r\n2019-04-18 07:29:18 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.42.0.121:42068) with ID 8\r\n2019-04-18 07:29:18 INFO  BlockManagerMasterEndpoint:54 - Registering block manager student11-x1:32809 with 2004.6 MB RAM, BlockManagerId(8, student11-x1, 32809, None)\r\n2019-04-18 07:29:18 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.42.2.84:50564) with ID 4\r\n2019-04-18 07:29:18 INFO  BlockManagerMasterEndpoint:54 - Registering block manager gpu4-x2:40273 with 2004.6 MB RAM, BlockManagerId(4, gpu4-x2, 40273, None)\r\n2019-04-18 07:29:19 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.42.1.20:34908) with ID 1\r\n2019-04-18 07:29:19 INFO  BlockManagerMasterEndpoint:54 - Registering block manager student10-x2:34033 with 2004.6 MB RAM, BlockManagerId(1, student10-x2, 34033, None)\r\n2019-04-18 07:29:20 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.42.0.120:57802) with ID 2\r\n2019-04-18 07:29:20 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.42.0.122:43882) with ID 6\r\n2019-04-18 07:29:20 INFO  BlockManagerMasterEndpoint:54 - Registering block manager student10-x1:43177 with 2004.6 MB RAM, BlockManagerId(2, student10-x1, 43177, None)\r\n2019-04-18 07:29:20 INFO  BlockManagerMasterEndpoint:54 - Registering block manager student12-x1:38655 with 2004.6 MB RAM, BlockManagerId(6, student12-x1, 38655, None)\r\n2019-04-18 07:29:20 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.42.2.54:33518) with ID 3\r\n2019-04-18 07:29:20 INFO  YarnClientSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\r\nargs: Namespace(batch_size=5, cluster_size=8, epochs=1, format='csv', images='mnist/csv_xray_500/test/images', labels='mnist/csv_xray_500/test/labels', mode='inference', model='mnist_model_xray_500', output='predictions_xray_500', rdma=False, readers=1, steps=100, tensorboard=False)\r\n2019-04-18T07:29:20.927119 ===== Start\r\nzipping images and labels\r\n2019-04-18 07:29:22,601 INFO (MainThread-4044) Reserving TFSparkNodes \r\n2019-04-18 07:29:22,601 INFO (MainThread-4044) cluster_template: {'ps': [0], 'worker': [1, 2, 3, 4, 5, 6, 7]}\r\n2019-04-18 07:29:22,603 INFO (MainThread-4044) listening for reservations at ('10.42.2.24', 42455)\r\n2019-04-18 07:29:22,603 INFO (MainThread-4044) Starting TensorFlow on executors\r\n2019-04-18 07:29:22,608 INFO (MainThread-4044) Waiting for TFSparkNodes to start\r\n2019-04-18 07:29:22,608 INFO (MainThread-4044) waiting for 8 reservations\r\n2019-04-18 07:29:23,609 INFO (MainThread-4044) waiting for 8 reservations\r\n2019-04-18 07:29:24,610 INFO (MainThread-4044) waiting for 6 reservations\r\n2019-04-18 07:29:25,612 INFO (MainThread-4044) waiting for 1 reservations\r\n2019-04-18 07:29:26,613 INFO (MainThread-4044) all reservations completed\r\n2019-04-18 07:29:26,613 INFO (MainThread-4044) All TFSparkNodes started\r\n2019-04-18 07:29:26,614 INFO (MainThread-4044) {'executor_id': 3, 'host': '10.42.2.54', 'job_name': 'worker', 'task_index': 2, 'port': 44997, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-yk3iqkla/listener-ktfhtl4g', 'authkey': b'\\xfa$\\x1bX&\\x8eLj\\x94\\x8a\\x19\\xb5T\\xc6n\\x13'}\r\n2019-04-18 07:29:26,614 INFO (MainThread-4044) {'executor_id': 5, 'host': '10.42.0.120', 'job_name': 'worker', 'task_index': 4, 'port': 46223, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-t40r6bux/listener-8qqlpfdf', 'authkey': b'\\xd9\\x98\\xb5g\\xb6-D\\xbf\\xad1i.\\xabx\\x168'}\r\n2019-04-18 07:29:26,614 INFO (MainThread-4044) {'executor_id': 4, 'host': '10.42.1.20', 'job_name': 'worker', 'task_index': 3, 'port': 46107, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-sxbut4iu/listener-2zwfhg0d', 'authkey': b'\\xa9\\xb7\\x83Il\\x14E\\x8a\\x91T\\x05\\x93\\x90WH\\xcc'}\r\n2019-04-18 07:29:26,614 INFO (MainThread-4044) {'executor_id': 2, 'host': '10.42.0.121', 'job_name': 'worker', 'task_index': 1, 'port': 35341, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-8fvjio8h/listener-f2mmhnys', 'authkey': b'\\xaa\\xd7\\xa7\\x01G,Be\\xafi\\xd5\\x00\\x91\\x0f\\\\}'}\r\n2019-04-18 07:29:26,614 INFO (MainThread-4044) {'executor_id': 6, 'host': '10.42.2.84', 'job_name': 'worker', 'task_index': 5, 'port': 36107, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-iafhfy2e/listener-s93rccfm', 'authkey': b'\\r\\xc1\\x85\\x98_\\xc4E\\x06\\x89\\xc6\\x93\\x9f9\\x15\\x85\\xaf'}\r\n2019-04-18 07:29:26,615 INFO (MainThread-4044) {'executor_id': 1, 'host': '10.42.1.21', 'job_name': 'worker', 'task_index': 0, 'port': 33969, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-pjt6zd2b/listener-q8pof10o', 'authkey': b'\\xca\\x04\\xff%\\xf7\\xc3K\\x00\\xba\\x1aGySr\\xf9\\xee'}\r\n2019-04-18 07:29:26,615 INFO (MainThread-4044) {'executor_id': 0, 'host': '10.42.0.122', 'job_name': 'ps', 'task_index': 0, 'port': 37243, 'tb_pid': 0, 'tb_port': 0, 'addr': ('10.42.0.122', 36717), 'authkey': b'\\x9a\\xa7\\xe4\\xea\\x1b\\xbfA\\xd8\\xa4N\\x82\\xf3\\x95K\\xd2b'}\r\n2019-04-18 07:29:26,615 INFO (MainThread-4044) {'executor_id': 7, 'host': '10.42.1.22', 'job_name': 'worker', 'task_index': 6, 'port': 36879, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-jdje9df0/listener-osqdsai3', 'authkey': b'p#\\nn\\x00\\xc5LV\\xben#\\x7fh\\x97I3'}\r\n2019-04-18 07:29:26,615 INFO (MainThread-4044) Feeding inference data\r\n\r\n**Spark Submit Command Line:**\r\nspark-submit \\\r\n--master yarn \\\r\n--deploy-mode client \\\r\n--queue default \\\r\n--num-executors 8 \\\r\n--executor-memory 4g \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/${USER}/Python.zip#Python \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nTensorFlowOnSpark/examples/mnist/spark/mnist_spark.py \\\r\n--images mnist/csv_xray_500/test/images \\\r\n--labels mnist/csv_xray_500/test/labels \\\r\n--steps 100 \\\r\n--batch_size 5 \\\r\n--mode inference \\\r\n--model mnist_model_xray_500 \\\r\n--output predictions_xray_500\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/413", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/413/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/413/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/413/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/413", "id": 434602694, "node_id": "MDU6SXNzdWU0MzQ2MDI2OTQ=", "number": 413, "title": "hyperParameter tuning", "user": {"login": "yhlhs", "id": 30208741, "node_id": "MDQ6VXNlcjMwMjA4NzQx", "avatar_url": "https://avatars0.githubusercontent.com/u/30208741?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yhlhs", "html_url": "https://github.com/yhlhs", "followers_url": "https://api.github.com/users/yhlhs/followers", "following_url": "https://api.github.com/users/yhlhs/following{/other_user}", "gists_url": "https://api.github.com/users/yhlhs/gists{/gist_id}", "starred_url": "https://api.github.com/users/yhlhs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yhlhs/subscriptions", "organizations_url": "https://api.github.com/users/yhlhs/orgs", "repos_url": "https://api.github.com/users/yhlhs/repos", "events_url": "https://api.github.com/users/yhlhs/events{/privacy}", "received_events_url": "https://api.github.com/users/yhlhs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-04-18T05:58:29Z", "updated_at": "2019-07-16T17:07:01Z", "closed_at": "2019-07-16T17:07:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "there is an estimator API, but how to perform hyperparameter tuning with the spark ml pipeline?  ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/410", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/410/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/410/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/410/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/410", "id": 432888377, "node_id": "MDU6SXNzdWU0MzI4ODgzNzc=", "number": 410, "title": "TFSparkNode.py\", line 183, in _mapfn     TFSparkNode.mgr = TFManager.start(authkey, queues)", "user": {"login": "Auroratan", "id": 22725547, "node_id": "MDQ6VXNlcjIyNzI1NTQ3", "avatar_url": "https://avatars0.githubusercontent.com/u/22725547?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Auroratan", "html_url": "https://github.com/Auroratan", "followers_url": "https://api.github.com/users/Auroratan/followers", "following_url": "https://api.github.com/users/Auroratan/following{/other_user}", "gists_url": "https://api.github.com/users/Auroratan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Auroratan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Auroratan/subscriptions", "organizations_url": "https://api.github.com/users/Auroratan/orgs", "repos_url": "https://api.github.com/users/Auroratan/repos", "events_url": "https://api.github.com/users/Auroratan/events{/privacy}", "received_events_url": "https://api.github.com/users/Auroratan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-04-13T19:15:39Z", "updated_at": "2019-05-03T20:11:59Z", "closed_at": "2019-05-03T20:11:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [ 2.3.1]\r\n - TensorFlow version [1.13.1]\r\n - TensorFlowOnSpark version [e.g. 1.4]\r\n - Cluster version [Standalone, Hadoop 2.7]\r\n\r\nCode\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.conf import SparkConf\r\nfrom tensorflowonspark import TFCluster\r\nfrom tensorflowonspark import TFNode\r\nfrom datetime import datetime\r\nimport tensorflow as tf\r\nimport argparse\r\n\r\ndef main_fun(argv, ctx):\r\n    worker_num = ctx.worker_num\r\n    job_name = ctx.job_name\r\n    print(job_nameame)\r\n    task_index = ctx.task_index\r\n    cluster_spec, server = TFNode.start_cluster_server(ctx)\r\n    hello = tf.constant('Hello, TensorFlow!')\r\n    sess = tf.Session()\r\n    print(sess.run(hello))\r\n\r\nimport sys\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\r\n    conf = SparkConf().setAppName(\"test\").setMaster(\"local[2]\")\r\n    sc = SparkContext(conf=conf)\r\n    print(sc)\r\n    executors = sc._conf.get(\"spark.executor.instances\")\r\n    num_executors = int(executors) if executors is not None else 2\r\n    num_ps = 1\r\n    tensorboard = True\r\n    cluster = TFCluster.run(sc, main_fun, sys.argv, num_executors, num_ps, tensorboard, TFCluster.InputMode.TENSORFLOW)\r\n    cluster.shutdown()\r\n    sc.stop()\r\n\r\n**Describe the bug:**\r\nA clear and concise description of what the bug is.\r\n\r\n**Logs:**\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n[Stage 0:>                                                          (0 + 2) / 2]19/04/14 03:09:21 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"D:\\spark-2.2.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 196, in main\r\n  File \"D:\\spark-2.2.3-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 191, in process\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\rdd.py\", line 2410, in pipeline_func\r\n    return func(split, prev_func(split, iterator))\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\rdd.py\", line 2410, in pipeline_func\r\n    return func(split, prev_func(split, iterator))\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\rdd.py\", line 2410, in pipeline_func\r\n    return func(split, prev_func(split, iterator))\r\n  [Previous line repeated 1 more time]\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\rdd.py\", line 333, in func\r\n    return f(iterator)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\pyspark\\rdd.py\", line 781, in func\r\n    r = f(it)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\tensorflowonspark\\TFSparkNode.py\", line 183, in _mapfn\r\n    TFSparkNode.mgr = TFManager.start(authkey, queues)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\tensorflowonspark\\TFManager.py\", line 64, in start\r\n    mgr.start()\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\multiprocessing\\managers.py\", line 513, in start\r\n    self._process.start()\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\multiprocessing\\process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\multiprocessing\\context.py\", line 322, in _Popen\r\n    return Popen(process_obj)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\r\n    reduction.dump(process_obj, to_child)\r\n  File \"D:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\multiprocessing\\reduction.py\", line 60, in dump\r\n    ForkingPickler(file, protocol).dump(obj)\r\nAttributeError: Can't pickle local object 'start.<locals>.<lambda>'\r\n\r\n<SparkContext master=local[2] appName=test>\r\n2019-04-14 03:09:20,658 INFO (MainThread-26884) waiting for 2 reservations\r\n2019-04-14 03:09:21,658 INFO (MainThread-26884) waiting for 2 reservations\r\n2019-04-14 03:09:21,821 ERROR (Thread-9-26884) Exception in TF background thread\r\n2019-04-14 03:09:22,659 INFO (MainThread-26884) waiting for 2 reservations\r\nAn exception has occurred, use %tb to see the full traceback.\r\n\r\nSystemExit: 1\r\n\r\n\r\nD:\\ProgramData\\Anaconda3\\envs\\bigdata\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\r\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\r\n**Spark Submit Command Line:**\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/407", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/407/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/407/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/407/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/407", "id": 427562776, "node_id": "MDU6SXNzdWU0Mjc1NjI3NzY=", "number": 407, "title": "[keras Embedding] Cannot colocate nodes .. Cannot merge devices with incompatible jobs", "user": {"login": "lazybonesboy", "id": 4652808, "node_id": "MDQ6VXNlcjQ2NTI4MDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/4652808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lazybonesboy", "html_url": "https://github.com/lazybonesboy", "followers_url": "https://api.github.com/users/lazybonesboy/followers", "following_url": "https://api.github.com/users/lazybonesboy/following{/other_user}", "gists_url": "https://api.github.com/users/lazybonesboy/gists{/gist_id}", "starred_url": "https://api.github.com/users/lazybonesboy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lazybonesboy/subscriptions", "organizations_url": "https://api.github.com/users/lazybonesboy/orgs", "repos_url": "https://api.github.com/users/lazybonesboy/repos", "events_url": "https://api.github.com/users/lazybonesboy/events{/privacy}", "received_events_url": "https://api.github.com/users/lazybonesboy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2019-04-01T07:58:01Z", "updated_at": "2019-07-16T17:07:14Z", "closed_at": "2019-07-16T17:07:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [2.7]\r\n - Spark version [2.3.1]\r\n - TensorFlow version [1.7.0]\r\n - TensorFlowOnSpark version [1.3.2]\r\n - Cluster version [Standalone hadoop-2.6.0-cdh5.12.0]\r\n\r\n**Describe the bug:**\r\nI modified the two  part of code of  `TensorflowOnSpark/examples/mnist/keras/mnist_mlp.py ` ,include  generate_rdd_data and model part. \r\n```\r\n            user_input = Input(shape=(1,), name='user_id')\r\n            purchase_input = Input(shape=(10,), name='purchase_category_number')  \r\n            user_features = Input(shape=(354,), name='features')\r\n           \r\n            embed_layer1 = Embedding(92589, 64, input_length=10, name='item_embedding1')\r\n            purchase_category_embedding = embed_layer1(purchase_input)\r\n            purchase_category_avg_layer = Flatten()(purchase_category_embedding)\r\n\r\n            concat_layer = Concatenate()([purchase_category_avg_layer, user_features])\r\n\r\n            dense = Dense(512, activation='relu')(concat_layer)\r\n            dense = Dropout(0.2)(dense)\r\n            dense = Dense(512, activation='relu')(dense)\r\n            dense = Dropout(0.2)(dense)\r\n            dense = Dense(10, activation='softmax')(dense)\r\n            \r\n            model = Model(inputs=[user_input, purchase_input, user_features], outputs=[dense])\r\n            model.summary()\r\n```\r\n\r\nWhen Delete the Embedding layer just keeping user_feautrures input,  the train process  is OK.\r\nI don't know why this problem is happened after add Embedding layer. \r\n\r\nAfter Google, I tried `with tf.Session(server.target, config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)) as sess:` , but it's still not OK. \r\n\r\n\r\n**Logs:**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 329, in wrapper_fn_background\r\n    p.start()\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 323, in wrapper_fn\r\n    logging.info(\"Starting TensorFlow {0}:{1} as {2} on cluster node {3} on background process\".format(\r\n  File \"/home/mlp/notebooks/usp-data-mining/user_match_goods/item_embed/user2vec_mlp.py\", line 188, in main_fun\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 2224, in fit_generator\r\n    class_weight=class_weight)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/engine/training.py\", line 1883, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2476, in __call__\r\n    session = get_session()\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 192, in get_session\r\n    [tf.is_variable_initialized(v) for v in candidate_vars])\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1140, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    run_metadata)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'item_embedding1/embeddings' and 'training/RMSprop/gradients/item_embedding1/Gather_grad/Shape: Cannot merge devices with incompatible jobs: '/job:worker/task:0' and '/job:ps/task:0'\r\n\t [[Node: item_embedding1/embeddings = VariableV2[container=\"\", dtype=DT_FLOAT, shape=[92590,64], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\r\n\r\nCaused by op 'item_embedding1/embeddings', defined at:\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib64/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 195, in <module>\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 170, in manager\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 73, in worker\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 234, in main\r\n    process()\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2457, in pipeline_func\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2457, in pipeline_func\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2457, in pipeline_func\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 370, in func\r\n  File \"/home/mlp/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 819, in func\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 342, in _mapfn\r\n    raise Exception(\"exception in ps:\\n\" + e_str)\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 105, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/usr/lib64/python3.6/multiprocessing/context.py\", line 223, in _Popen\r\n    return _default_context.get_context().Process._Popen(process_obj)\r\n  File \"/usr/lib64/python3.6/multiprocessing/context.py\", line 277, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/usr/lib64/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/usr/lib64/python3.6/multiprocessing/popen_fork.py\", line 73, in _launch\r\n    code = process_obj._bootstrap()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib64/python3.6/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 329, in wrapper_fn_background\r\n    p.start()\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 323, in wrapper_fn\r\n    logging.info(\"Starting TensorFlow {0}:{1} as {2} on cluster node {3} on background process\".format(\r\n  File \"/home/mlp/notebooks/usp-data-mining/user_match_goods/item_embed/user2vec_mlp.py\", line 131, in main_fun\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/engine/topology.py\", line 592, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/layers/embeddings.py\", line 105, in build\r\n    dtype=self.dtype)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/engine/topology.py\", line 416, in add_weight\r\n    constraint=constraint)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 395, in variable\r\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\r\n    constraint=constraint)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 365, in _init_from_args\r\n    name=name)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 135, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 1131, in variable_v2\r\n    shared_name=shared_name, name=name)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot colocate nodes 'item_embedding1/embeddings' and 'training/RMSprop/gradients/item_embedding1/Gather_grad/Shape: Cannot merge devices with incompatible jobs: '/job:worker/task:0' and '/job:ps/task:0'\r\n\t [[Node: item_embedding1/embeddings = VariableV2[container=\"\", dtype=DT_FLOAT, shape=[92590,64], shared_name=\"\", _device=\"/job:ps/task:0\"]()]]\r\n```\r\n**Spark Submit Command Line:**\r\nspark-submit --total-executor-cores 10 --executor-cores 1 --executor-memory 8g mlp.py --cluster_size 10 --input_mode spark --epochs 5 --model_dir ''--export_dir '' --tensorboard ", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/406", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/406/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/406/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/406/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/406", "id": 427387155, "node_id": "MDU6SXNzdWU0MjczODcxNTU=", "number": 406, "title": "Convert the MNIST zip files into HDFS files (w/ grid node access)", "user": {"login": "hadipash", "id": 16683750, "node_id": "MDQ6VXNlcjE2NjgzNzUw", "avatar_url": "https://avatars1.githubusercontent.com/u/16683750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hadipash", "html_url": "https://github.com/hadipash", "followers_url": "https://api.github.com/users/hadipash/followers", "following_url": "https://api.github.com/users/hadipash/following{/other_user}", "gists_url": "https://api.github.com/users/hadipash/gists{/gist_id}", "starred_url": "https://api.github.com/users/hadipash/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hadipash/subscriptions", "organizations_url": "https://api.github.com/users/hadipash/orgs", "repos_url": "https://api.github.com/users/hadipash/repos", "events_url": "https://api.github.com/users/hadipash/events{/privacy}", "received_events_url": "https://api.github.com/users/hadipash/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2019-03-31T11:40:10Z", "updated_at": "2019-05-03T20:12:45Z", "closed_at": "2019-05-03T20:12:45Z", "author_association": "NONE", "active_lock_reason": null, "body": "[Here](https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN#convert-the-mnist-zip-files-into-hdfs-files) is given an example of how to convert the MNIST zip files into HDFS files:\r\n`${SPARK_HOME}/bin/spark-submit \\`\r\n`--master yarn \\`\r\n`--deploy-mode cluster \\`\r\n`--queue ${QUEUE} \\`\r\n`--num-executors 4 \\`\r\n`--executor-memory 4G \\`\r\n`--archives hdfs:///user/${USER}/Python.zip#Python,mnist/mnist.zip#mnist \\`\r\n`--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_CUDA \\`\r\n`--driver-library-path=$LIB_CUDA \\`\r\n`TensorFlowOnSpark/examples/mnist/mnist_data_setup.py \\`\r\n`--output mnist/csv \\`\r\n`--format csv`\r\n\r\nAs I understand, this is for running when there is no access to grid nodes (`--archives hdfs:///user/${USER}/Python.zip#Python`). However, if I used [this](https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN#install-python-w-grid-node-access) method to instal TFoS, how should I change the code above to start the program?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/405", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/405/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/405/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/405/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/405", "id": 426743194, "node_id": "MDU6SXNzdWU0MjY3NDMxOTQ=", "number": 405, "title": "Google Cloud Dataproc support", "user": {"login": "gogasca", "id": 30065079, "node_id": "MDQ6VXNlcjMwMDY1MDc5", "avatar_url": "https://avatars3.githubusercontent.com/u/30065079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gogasca", "html_url": "https://github.com/gogasca", "followers_url": "https://api.github.com/users/gogasca/followers", "following_url": "https://api.github.com/users/gogasca/following{/other_user}", "gists_url": "https://api.github.com/users/gogasca/gists{/gist_id}", "starred_url": "https://api.github.com/users/gogasca/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gogasca/subscriptions", "organizations_url": "https://api.github.com/users/gogasca/orgs", "repos_url": "https://api.github.com/users/gogasca/repos", "events_url": "https://api.github.com/users/gogasca/events{/privacy}", "received_events_url": "https://api.github.com/users/gogasca/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-28T22:59:59Z", "updated_at": "2019-05-03T20:14:15Z", "closed_at": "2019-05-03T20:14:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [2.3.2]\r\n - TensorFlow version [ 1.13.1]\r\n - TensorFlowOnSpark version [ 1.3.2]\r\n - Cluster version [Hadoop 2.9.2]\r\n\r\n**Describe the bug:**\r\nI'm trying to experiment with Google Cloud Dataproc and TensorFlowOnSpark\r\nCloud DataProc supports Hadoop 2.9, afaik Hadoop 3.1 supports full GPU isolation.\r\nDoes TensorFlowOnSpark supports GPU isolation?\r\n\r\nhttps://hortonworks.com/blog/gpus-support-in-apache-hadoop-3-1-yarn-hdp-3/\r\n\r\nCloud Dataproc\r\nhttps://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.3\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/404", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/404/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/404/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/404/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/404", "id": 422573363, "node_id": "MDU6SXNzdWU0MjI1NzMzNjM=", "number": 404, "title": "Accuracy lower than non-distributed traning", "user": {"login": "bobzengscut", "id": 32973428, "node_id": "MDQ6VXNlcjMyOTczNDI4", "avatar_url": "https://avatars0.githubusercontent.com/u/32973428?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bobzengscut", "html_url": "https://github.com/bobzengscut", "followers_url": "https://api.github.com/users/bobzengscut/followers", "following_url": "https://api.github.com/users/bobzengscut/following{/other_user}", "gists_url": "https://api.github.com/users/bobzengscut/gists{/gist_id}", "starred_url": "https://api.github.com/users/bobzengscut/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bobzengscut/subscriptions", "organizations_url": "https://api.github.com/users/bobzengscut/orgs", "repos_url": "https://api.github.com/users/bobzengscut/repos", "events_url": "https://api.github.com/users/bobzengscut/events{/privacy}", "received_events_url": "https://api.github.com/users/bobzengscut/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2019-03-19T07:02:27Z", "updated_at": "2019-03-19T10:51:59Z", "closed_at": "2019-03-19T10:51:59Z", "author_association": "NONE", "active_lock_reason": null, "body": "I use TensorFlowOnSpark to train my tensorflow work, but the accuracy is about 1% lower than before.  Is it normal?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/402", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/402/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/402/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/402/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/402", "id": 420191135, "node_id": "MDU6SXNzdWU0MjAxOTExMzU=", "number": 402, "title": "Hyperparameter tuning in Parallel", "user": {"login": "kshitiz8", "id": 2066242, "node_id": "MDQ6VXNlcjIwNjYyNDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2066242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshitiz8", "html_url": "https://github.com/kshitiz8", "followers_url": "https://api.github.com/users/kshitiz8/followers", "following_url": "https://api.github.com/users/kshitiz8/following{/other_user}", "gists_url": "https://api.github.com/users/kshitiz8/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshitiz8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshitiz8/subscriptions", "organizations_url": "https://api.github.com/users/kshitiz8/orgs", "repos_url": "https://api.github.com/users/kshitiz8/repos", "events_url": "https://api.github.com/users/kshitiz8/events{/privacy}", "received_events_url": "https://api.github.com/users/kshitiz8/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2019-03-12T20:29:58Z", "updated_at": "2019-03-14T07:27:38Z", "closed_at": "2019-03-14T07:27:38Z", "author_association": "NONE", "active_lock_reason": null, "body": "What's the recommended way of doing a grid search with TensoflowOnSpark? It seems that the distributed tensorflow allows data parallelism (and model parallelism) but we can train multiple models (with different hyperparameter) only in a serial fashion.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/401", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/401/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/401/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/401/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/401", "id": 419878599, "node_id": "MDU6SXNzdWU0MTk4Nzg1OTk=", "number": 401, "title": "tfos support tensorflow 2.0?", "user": {"login": "asagjj", "id": 10152847, "node_id": "MDQ6VXNlcjEwMTUyODQ3", "avatar_url": "https://avatars2.githubusercontent.com/u/10152847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asagjj", "html_url": "https://github.com/asagjj", "followers_url": "https://api.github.com/users/asagjj/followers", "following_url": "https://api.github.com/users/asagjj/following{/other_user}", "gists_url": "https://api.github.com/users/asagjj/gists{/gist_id}", "starred_url": "https://api.github.com/users/asagjj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asagjj/subscriptions", "organizations_url": "https://api.github.com/users/asagjj/orgs", "repos_url": "https://api.github.com/users/asagjj/repos", "events_url": "https://api.github.com/users/asagjj/events{/privacy}", "received_events_url": "https://api.github.com/users/asagjj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-03-12T09:21:22Z", "updated_at": "2019-10-02T16:40:22Z", "closed_at": "2019-10-02T16:33:31Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [e.g. 2.7, 3.6]\r\n - Spark version [e.g. 2.1, 2.3.1]\r\n - TensorFlow version [e.g. 1.5, 1.9.0]\r\n - TensorFlowOnSpark version [e.g. 1.1, 1.3.2]\r\n - Cluster version [e.g. Standalone, Hadoop 2.8, CDH5]\r\n\r\n**Describe the bug:**\r\nA clear and concise description of what the bug is.\r\n\r\n**Logs:**\r\nIf applicable, add logs to help explain your problem.  Note: errors may not be fully described in the driver/console logs.  Make sure to check the executor logs for possible root causes.\r\n\r\n**Spark Submit Command Line:**\r\nIf applicable, add your spark-submit command line.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/399", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/399/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/399/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/399/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/399", "id": 418149211, "node_id": "MDU6SXNzdWU0MTgxNDkyMTE=", "number": 399, "title": "Similar to issue#349, to run mnist keras sample, cluster stuck while feeding training data, exception \"Timeout while feeding partition\" raised after stuck for 600 seconds", "user": {"login": "witchoco", "id": 8072700, "node_id": "MDQ6VXNlcjgwNzI3MDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/8072700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/witchoco", "html_url": "https://github.com/witchoco", "followers_url": "https://api.github.com/users/witchoco/followers", "following_url": "https://api.github.com/users/witchoco/following{/other_user}", "gists_url": "https://api.github.com/users/witchoco/gists{/gist_id}", "starred_url": "https://api.github.com/users/witchoco/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/witchoco/subscriptions", "organizations_url": "https://api.github.com/users/witchoco/orgs", "repos_url": "https://api.github.com/users/witchoco/repos", "events_url": "https://api.github.com/users/witchoco/events{/privacy}", "received_events_url": "https://api.github.com/users/witchoco/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-03-07T06:25:37Z", "updated_at": "2019-05-10T19:24:36Z", "closed_at": "2019-05-10T19:24:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.5\r\n - Spark version 2.2\r\n - TensorFlow version 1.12 gpu\r\n - TensorFlowOnSpark version 1.4.2\r\n - Cluster version Hadoop 2.7, yarn\r\n\r\n**Describe the bug:**\r\nSimilar to issue#349, to run mnist keras sample, cluster stuck while feeding training data, exception raised \"Timeout while feeding partition\"\r\n\r\n**Logs:**\r\nroot@master:/software/yahoo/tfos# ./train_keras_mnist_csv.sh \r\n19/03/07 05:26:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nDownloading hdfs:///user/root/TensorFlowOnSpark/examples/mnist/keras/mnist_mlp_estimator.py to /tmp/tmp3152022056050101843/user/root/TensorFlowOnSpark/examples/mnist/keras/mnist_mlp_estimator.py.\r\n19/03/07 05:26:14 INFO spark.SparkContext: Running Spark version 2.2.0\r\n19/03/07 05:26:14 INFO spark.SparkContext: Submitted application: mnist_mlp\r\n19/03/07 05:26:14 INFO spark.SecurityManager: Changing view acls to: root\r\n19/03/07 05:26:14 INFO spark.SecurityManager: Changing modify acls to: root\r\n19/03/07 05:26:14 INFO spark.SecurityManager: Changing view acls groups to: \r\n19/03/07 05:26:14 INFO spark.SecurityManager: Changing modify acls groups to: \r\n19/03/07 05:26:14 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\r\n19/03/07 05:26:14 INFO util.Utils: Successfully started service 'sparkDriver' on port 38627.\r\n19/03/07 05:26:14 INFO spark.SparkEnv: Registering MapOutputTracker\r\n19/03/07 05:26:14 INFO spark.SparkEnv: Registering BlockManagerMaster\r\n19/03/07 05:26:14 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n19/03/07 05:26:14 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n19/03/07 05:26:14 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-c871f126-c5ad-4557-8ff6-94f92a42a46d\r\n19/03/07 05:26:14 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\r\n19/03/07 05:26:14 INFO spark.SparkEnv: Registering OutputCommitCoordinator\r\n19/03/07 05:26:14 INFO util.log: Logging initialized @2301ms\r\n19/03/07 05:26:14 INFO server.Server: jetty-9.3.z-SNAPSHOT\r\n19/03/07 05:26:14 INFO server.Server: Started @2341ms\r\n19/03/07 05:26:14 INFO server.AbstractConnector: Started ServerConnector@32855c04{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n19/03/07 05:26:14 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22ee411d{/jobs,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a6ca16a{/jobs/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c553d3f{/jobs/job,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19034f49{/jobs/job/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7955c667{/stages,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5467b101{/stages/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e0d411d{/stages/stage,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15655d20{/stages/stage/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4eaadbdd{/stages/pool,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37a6268c{/stages/pool/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b25ec77{/storage,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ab4e040{/storage/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30eeb67d{/storage/rdd,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10b0ecbe{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4403e5d9{/environment,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73274369{/environment/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76a25bb4{/executors,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77c80c3c{/executors/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1cffb4e2{/executors/threadDump,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7cefc63e{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44169fd6{/static,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26fb722b{/,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@417899ac{/api,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73e30fa2{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7487caa2{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:14 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.44.0.3:4040\r\n19/03/07 05:26:15 INFO client.RMProxy: Connecting to ResourceManager at master/10.44.0.3:8032\r\n19/03/07 05:26:15 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers\r\n19/03/07 05:26:15 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (10000 MB per container)\r\n19/03/07 05:26:15 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\r\n19/03/07 05:26:15 INFO yarn.Client: Setting up container launch context for our AM\r\n19/03/07 05:26:15 INFO yarn.Client: Setting up the launch environment for our AM container\r\n19/03/07 05:26:15 INFO yarn.Client: Preparing resources for our AM container\r\n19/03/07 05:26:15 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\r\n19/03/07 05:26:16 INFO yarn.Client: Uploading resource file:/tmp/spark-a15b9378-0470-4bd1-ba17-b3832a553d18/__spark_libs__1802946680673561919.zip -> hdfs://master:9000/user/root/.sparkStaging/application_1551864161970_0029/__spark_libs__1802946680673561919.zip\r\n19/03/07 05:26:17 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/root/mnist.npz#mnist.npz\r\n19/03/07 05:26:17 INFO yarn.Client: Uploading resource file:/software/apache/spark/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip -> hdfs://master:9000/user/root/.sparkStaging/application_1551864161970_0029/pyspark.zip\r\n19/03/07 05:26:17 INFO yarn.Client: Uploading resource file:/software/apache/spark/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip -> hdfs://master:9000/user/root/.sparkStaging/application_1551864161970_0029/py4j-0.10.4-src.zip\r\n19/03/07 05:26:17 INFO yarn.Client: Uploading resource file:/tmp/spark-a15b9378-0470-4bd1-ba17-b3832a553d18/__spark_conf__5445895715324259030.zip -> hdfs://master:9000/user/root/.sparkStaging/application_1551864161970_0029/__spark_conf__.zip\r\n19/03/07 05:26:17 INFO spark.SecurityManager: Changing view acls to: root\r\n19/03/07 05:26:17 INFO spark.SecurityManager: Changing modify acls to: root\r\n19/03/07 05:26:17 INFO spark.SecurityManager: Changing view acls groups to: \r\n19/03/07 05:26:17 INFO spark.SecurityManager: Changing modify acls groups to: \r\n19/03/07 05:26:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\r\n19/03/07 05:26:17 INFO yarn.Client: Submitting application application_1551864161970_0029 to ResourceManager\r\n19/03/07 05:26:17 INFO impl.YarnClientImpl: Submitted application application_1551864161970_0029\r\n19/03/07 05:26:17 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1551864161970_0029 and attemptId None\r\n19/03/07 05:26:18 INFO yarn.Client: Application report for application_1551864161970_0029 (state: ACCEPTED)\r\n19/03/07 05:26:18 INFO yarn.Client: \r\n\t client token: N/A\r\n\t diagnostics: N/A\r\n\t ApplicationMaster host: N/A\r\n\t ApplicationMaster RPC port: -1\r\n\t queue: default\r\n\t start time: 1551936377356\r\n\t final status: UNDEFINED\r\n\t tracking URL: http://master:8088/proxy/application_1551864161970_0029/\r\n\t user: root\r\n19/03/07 05:26:19 INFO yarn.Client: Application report for application_1551864161970_0029 (state: ACCEPTED)\r\n19/03/07 05:26:19 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\r\n19/03/07 05:26:19 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> slave2, PROXY_URI_BASES -> http://slave2:8088/proxy/application_1551864161970_0029), /proxy/application_1551864161970_0029\r\n19/03/07 05:26:19 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\r\n19/03/07 05:26:20 INFO yarn.Client: Application report for application_1551864161970_0029 (state: RUNNING)\r\n19/03/07 05:26:20 INFO yarn.Client: \r\n\t client token: N/A\r\n\t diagnostics: N/A\r\n\t ApplicationMaster host: 10.44.0.5\r\n\t ApplicationMaster RPC port: 0\r\n\t queue: default\r\n\t start time: 1551936377356\r\n\t final status: UNDEFINED\r\n\t tracking URL: http://master:8088/proxy/application_1551864161970_0029/\r\n\t user: root\r\n19/03/07 05:26:20 INFO cluster.YarnClientSchedulerBackend: Application application_1551864161970_0029 has started running.\r\n19/03/07 05:26:20 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34457.\r\n19/03/07 05:26:20 INFO netty.NettyBlockTransferService: Server created on 10.44.0.3:34457\r\n19/03/07 05:26:20 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n19/03/07 05:26:20 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.44.0.3, 34457, None)\r\n19/03/07 05:26:20 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.44.0.3:34457 with 366.3 MB RAM, BlockManagerId(driver, 10.44.0.3, 34457, None)\r\n19/03/07 05:26:20 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.44.0.3, 34457, None)\r\n19/03/07 05:26:20 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.44.0.3, 34457, None)\r\n19/03/07 05:26:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1624c314{/metrics/json,null,AVAILABLE,@Spark}\r\n19/03/07 05:26:21 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.44.0.4:33898) with ID 1\r\n19/03/07 05:26:21 INFO storage.BlockManagerMasterEndpoint: Registering block manager slave1:37461 with 2004.6 MB RAM, BlockManagerId(1, slave1, 37461, None)\r\n19/03/07 05:26:22 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.44.0.5:37784) with ID 2\r\n19/03/07 05:26:22 INFO storage.BlockManagerMasterEndpoint: Registering block manager slave2:43783 with 2004.6 MB RAM, BlockManagerId(2, slave2, 43783, None)\r\n19/03/07 05:26:22 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\r\nargs: Namespace(batch_size=100, cluster_size=2, epochs=1, images='mnist/csv/train/images', input_mode='spark', labels='mnist/csv/train/labels', mnist_npz='mnist.npz', model_dir='hdfs:///user/root/TensorFlowOnSpark/mnist_model', num_ps=1, output='predictions', steps=2000, tensorboard=True)\r\n19/03/07 05:26:22 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 289.6 KB, free 366.0 MB)\r\n19/03/07 05:26:22 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.6 KB, free 366.0 MB)\r\n19/03/07 05:26:22 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.44.0.3:34457 (size: 23.6 KB, free: 366.3 MB)\r\n19/03/07 05:26:22 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n19/03/07 05:26:22 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 289.6 KB, free 365.7 MB)\r\n19/03/07 05:26:22 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 23.6 KB, free 365.7 MB)\r\n19/03/07 05:26:22 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.44.0.3:34457 (size: 23.6 KB, free: 366.3 MB)\r\n19/03/07 05:26:22 INFO spark.SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\r\n19/03/07 05:26:22 INFO mapred.FileInputFormat: Total input paths to process : 10\r\n19/03/07 05:26:22 INFO mapred.FileInputFormat: Total input paths to process : 10\r\n2019-03-07 05:26:22,967 INFO (MainThread-14378) Reserving TFSparkNodes w/ TensorBoard\r\n2019-03-07 05:26:22,967 INFO (MainThread-14378) cluster_template: {'ps': range(0, 1), 'master': range(1, 2)}\r\n2019-03-07 05:26:22,968 INFO (MainThread-14378) listening for reservations at ('10.44.0.3', 41155)\r\n2019-03-07 05:26:22,968 INFO (MainThread-14378) Starting TensorFlow on executors\r\n2019-03-07 05:26:22,972 INFO (MainThread-14378) Waiting for TFSparkNodes to start\r\n2019-03-07 05:26:22,972 INFO (MainThread-14378) waiting for 2 reservations\r\n19/03/07 05:26:23 INFO spark.SparkContext: Starting job: foreachPartition at /usr/local/lib/python3.5/dist-packages/tensorflowonspark/TFCluster.py:301\r\n19/03/07 05:26:23 INFO scheduler.DAGScheduler: Got job 0 (foreachPartition at /usr/local/lib/python3.5/dist-packages/tensorflowonspark/TFCluster.py:301) with 2 output partitions\r\n19/03/07 05:26:23 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /usr/local/lib/python3.5/dist-packages/tensorflowonspark/TFCluster.py:301)\r\n19/03/07 05:26:23 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n19/03/07 05:26:23 INFO scheduler.DAGScheduler: Missing parents: List()\r\n19/03/07 05:26:23 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[8] at foreachPartition at /usr/local/lib/python3.5/dist-packages/tensorflowonspark/TFCluster.py:301), which has no missing parents\r\n19/03/07 05:26:23 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 21.5 KB, free 365.7 MB)\r\n19/03/07 05:26:23 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.2 KB, free 365.7 MB)\r\n19/03/07 05:26:23 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.44.0.3:34457 (size: 15.2 KB, free: 366.2 MB)\r\n19/03/07 05:26:23 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006\r\n19/03/07 05:26:23 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[8] at foreachPartition at /usr/local/lib/python3.5/dist-packages/tensorflowonspark/TFCluster.py:301) (first 15 tasks are for partitions Vector(0, 1))\r\n19/03/07 05:26:23 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\r\n19/03/07 05:26:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, slave2, executor 2, partition 0, PROCESS_LOCAL, 4822 bytes)\r\n19/03/07 05:26:23 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, slave1, executor 1, partition 1, PROCESS_LOCAL, 4822 bytes)\r\n19/03/07 05:26:23 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on slave2:43783 (size: 15.2 KB, free: 2004.6 MB)\r\n19/03/07 05:26:23 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on slave1:37461 (size: 15.2 KB, free: 2004.6 MB)\r\n2019-03-07 05:26:23,974 INFO (MainThread-14378) waiting for 2 reservations\r\n2019-03-07 05:26:24,974 INFO (MainThread-14378) waiting for 2 reservations\r\n2019-03-07 05:26:25,975 INFO (MainThread-14378) waiting for 2 reservations\r\n2019-03-07 05:26:26,977 INFO (MainThread-14378) waiting for 1 reservations\r\n2019-03-07 05:26:27,978 INFO (MainThread-14378) all reservations completed\r\n2019-03-07 05:26:27,978 INFO (MainThread-14378) All TFSparkNodes started\r\n2019-03-07 05:26:27,978 INFO (MainThread-14378) {'host': '10.44.0.4', 'executor_id': 1, 'addr': '/tmp/pymp-ouv7htjg/listener-nvgywg76', 'tb_pid': 0, 'task_index': 0, 'job_name': 'master', 'authkey': b'1\\x03\\x91\\xff\\x95\\x1eAw\\xbe\\xc6\\x93*\\xfe\\x0b\\xbbz', 'port': 34517, 'tb_port': 0}\r\n2019-03-07 05:26:27,979 INFO (MainThread-14378) {'host': '10.44.0.5', 'executor_id': 0, 'addr': ('10.44.0.5', 34537), 'tb_pid': 0, 'task_index': 0, 'job_name': 'ps', 'authkey': b'\\xd0\\xef@\"\\xf0\\xabI\\xb3\\x8f\\x00\\xdaZ\\xa8\\xeb\\xe0J', 'port': 43975, 'tb_port': 0}\r\n2019-03-07 05:26:27,979 INFO (MainThread-14378) Feeding training data\r\n19/03/07 05:26:28 INFO spark.SparkContext: Starting job: collect at PythonRDD.scala:458\r\n19/03/07 05:26:28 INFO scheduler.DAGScheduler: Got job 1 (collect at PythonRDD.scala:458) with 10 output partitions\r\n19/03/07 05:26:28 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at PythonRDD.scala:458)\r\n19/03/07 05:26:28 INFO scheduler.DAGScheduler: Parents of final stage: List()\r\n19/03/07 05:26:28 INFO scheduler.DAGScheduler: Missing parents: List()\r\n19/03/07 05:26:28 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[10] at RDD at PythonRDD.scala:48), which has no missing parents\r\n19/03/07 05:26:28 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.1 KB, free 365.6 MB)\r\n19/03/07 05:26:28 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.2 KB, free 365.6 MB)\r\n19/03/07 05:26:28 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.44.0.3:34457 (size: 8.2 KB, free: 366.2 MB)\r\n19/03/07 05:26:28 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006\r\n19/03/07 05:26:28 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ResultStage 1 (PythonRDD[10] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\r\n19/03/07 05:26:28 INFO cluster.YarnScheduler: Adding task set 1.0 with 10 tasks\r\n19/03/07 05:26:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, slave1, executor 1, partition 0, NODE_LOCAL, 5481 bytes)\r\n19/03/07 05:26:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 7746 ms on slave1 (executor 1) (1/2)\r\n19/03/07 05:26:30 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on slave1:37461 (size: 8.2 KB, free: 2004.6 MB)\r\n19/03/07 05:26:30 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on slave1:37461 (size: 23.6 KB, free: 2004.6 MB)\r\n19/03/07 05:26:31 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on slave1:37461 (size: 23.6 KB, free: 2004.5 MB)\r\n19/03/07 05:36:32 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, slave1, executor 1, partition 1, NODE_LOCAL, 5481 bytes)\r\n19/03/07 05:36:32 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 2, slave1, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/software/apache/hadoop/hadoop-2.7.7/tmp/nm-local-dir/usercache/root/appcache/application_1551864161970_0029/container_1551864161970_0029_01_000002/pyspark.zip/pyspark/worker.py\", line 177, in main\r\n    process()\r\n  File \"/software/apache/hadoop/hadoop-2.7.7/tmp/nm-local-dir/usercache/root/appcache/application_1551864161970_0029/container_1551864161970_0029_01_000002/pyspark.zip/pyspark/worker.py\", line 172, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/software/apache/spark/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2423, in pipeline_func\r\n  File \"/software/apache/spark/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2423, in pipeline_func\r\n  File \"/software/apache/spark/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 2423, in pipeline_func\r\n  File \"/software/apache/spark/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 346, in func\r\n  File \"/software/apache/spark/spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/rdd.py\", line 794, in func\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflowonspark/TFSparkNode.py\", line 423, in _train\r\n    raise Exception(\"Timeout while feeding partition\")\r\nException: Timeout while feeding partition\r\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\n\r\n\r\n\r\n**Spark Submit Command Line:**\r\n#!/bin/bash\r\n# set environment variables (if not already done)\r\nexport USER=root\r\n\r\nexport LD_LIBRARY_PATH=${PATH}\r\nexport PYSPARK_PYTHON=python\r\nexport SPARK_YARN_USER_ENV=\"PYSPARK_PYTHON=python\"\r\n#export PATH=${PYTHON_ROOT}/bin/:$PATH\r\nexport QUEUE=default\r\n\r\n# set paths to libjvm.so, libhdfs.so, and libcuda*.so\r\nexport LIB_HDFS=$HADOOP_HOME/lib/native                                     # path to libhdfs.so, for TF acccess to HDFS\r\nexport LIB_JVM=$JAVA_HOME/jre/lib/amd64/server                              # path to libjvm.so\r\nexport LIB_CUDA=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64    # for GPUs only\r\n\r\n#### Launch the Spark cluster\r\nexport MASTER=yarn\r\nexport SPARK_WORKER_INSTANCES=2\r\nexport CORES_PER_WORKER=1\r\nexport TOTAL_CORES=$((${CORES_PER_WORKER}*${SPARK_WORKER_INSTANCES}))\r\nexport TFoS_HOME=hdfs:///user/${USER}/TensorFlowOnSpark\r\n\r\n# remove any old artifacts\r\nhadoop fs -rm -r ${TFoS_HOME}/mnist_model\r\n\r\n# train\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--executor-memory 4G \\\r\n--conf spark.cores.max=${TOTAL_CORES} \\\r\n--conf spark.task.cpus=${CORES_PER_WORKER} \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_CUDA:$LIB_JVM:$LIB_HDFS \\\r\n--driver-library-path=$LIB_CUDA \\\r\n--archives hdfs:///user/${USER}/mnist.npz#mnist.npz \\\r\n${TFoS_HOME}/examples/mnist/keras/mnist_mlp_estimator.py \\\r\n--cluster_size ${SPARK_WORKER_INSTANCES} \\\r\n--input_mode spark \\\r\n--images mnist/csv/train/images \\\r\n--labels mnist/csv/train/labels \\\r\n--mnist_npz mnist.npz \\\r\n--epochs 1 \\\r\n--model_dir ${TFoS_HOME}/mnist_model \\\r\n--tensorboard\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/398", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/398/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/398/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/398/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/398", "id": 416890447, "node_id": "MDU6SXNzdWU0MTY4OTA0NDc=", "number": 398, "title": "Couldn't able to save the model file while using Keras example with a local dataset.", "user": {"login": "vamsinimmala1992", "id": 25427325, "node_id": "MDQ6VXNlcjI1NDI3MzI1", "avatar_url": "https://avatars2.githubusercontent.com/u/25427325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vamsinimmala1992", "html_url": "https://github.com/vamsinimmala1992", "followers_url": "https://api.github.com/users/vamsinimmala1992/followers", "following_url": "https://api.github.com/users/vamsinimmala1992/following{/other_user}", "gists_url": "https://api.github.com/users/vamsinimmala1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/vamsinimmala1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vamsinimmala1992/subscriptions", "organizations_url": "https://api.github.com/users/vamsinimmala1992/orgs", "repos_url": "https://api.github.com/users/vamsinimmala1992/repos", "events_url": "https://api.github.com/users/vamsinimmala1992/events{/privacy}", "received_events_url": "https://api.github.com/users/vamsinimmala1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 29, "created_at": "2019-03-04T16:41:56Z", "updated_at": "2019-08-27T18:27:16Z", "closed_at": "2019-08-27T18:27:16Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am using a keras [example](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_mlp.py#L68) with TensorflowOnSpark. I am trying to save the model file to the hdfs location I specify as args. \r\n\r\nWith no exception or errors in the Yarn Log the process have been completed, BUT I DONT SEE ANY MODEL FILE SAVED IN THE HDFS.\r\n\r\nbelow is the code and the log. Please check.\r\n\r\nAlso I am not inputting  any validation data in the fit_generator method. is it mandatory?\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\n\r\ndef main_fun(args, ctx):\r\n\timport numpy\r\n\timport os\r\n\timport tensorflow as tf\r\n\tfrom tensorflow.python import keras\r\n\tfrom tensorflow.python.keras import backend as K\r\n\tfrom tensorflow.python.keras.datasets import mnist\r\n\tfrom tensorflow.python.keras.models import Sequential, load_model, save_model\r\n\tfrom tensorflow.python.keras.layers import Dense, Dropout\r\n\tfrom tensorflow.python.keras.optimizers import RMSprop\r\n\tfrom tensorflow.python.keras.callbacks import LambdaCallback, TensorBoard\r\n\tfrom tensorflow.python.saved_model import builder as saved_model_builder\r\n\tfrom tensorflow.python.saved_model import tag_constants\r\n\tfrom tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\r\n\tfrom tensorflowonspark import TFNode\r\n\t\r\n\tcluster, server = TFNode.start_cluster_server(ctx)\r\n\t\r\n\tif ctx.job_name == \"ps\":\r\n\t\tserver.join()\r\n\telif ctx.job_name == \"worker\":\r\n\t\t\r\n\t\tdef generate_rdd_data(tf_feed, batch_size):\r\n\t\t\tprint(\"generate_rdd_data invoked\")\r\n\t\t\twhile True:\r\n\t\t\t\tbatch = tf_feed.next_batch(batch_size)\r\n\t\t\t\tfeature_vector = []\r\n\t\t\t\tlbls = []\r\n\t\t\t\tfor item in batch:\r\n\t\t\t\t\tfeature_vector.append(item[0])\r\n\t\t\t\t\tlbls.append(item[1])\r\n\t\t\t\tfeatures = numpy.array(feature_vector).astype('float32')\r\n\t\t\t\tlabels = numpy.stack(lbls).astype('float32')\r\n\t\t\t\tyield (features, labels)\r\n\t\t\r\n\t\twith tf.device(tf.train.replica_device_setter(\r\n\t\t  worker_device=\"/job:worker/task:%d\" % ctx.task_index,\r\n\t\t  cluster=cluster)):\r\n\t\t\t\r\n\t\t\tbatch_size = 100\r\n\t\t\tnum_classes = 14\r\n\t\t\t# args.mode == 'spark':\r\n\t\t\tx_train = tf.placeholder(tf.float32, [None, 28047], name=\"x_train\")\r\n\t\t\ty_train = tf.placeholder(tf.float32, [None, 14], name=\"y_train\")\r\n\t\t\t\r\n\t\t\tmodel = Sequential()\r\n\t\t\tmodel.add(Dense(512, activation='relu', input_shape=(28047,)))\r\n\t\t\tmodel.add(Dropout(0.2))\r\n\t\t\tmodel.add(Dense(512, activation='relu'))\r\n\t\t\tmodel.add(Dropout(0.2))\r\n\t\t\tmodel.add(Dense(14, activation='softmax'))\r\n\t\t\t\r\n\t\t\tmodel.summary()\r\n\t\t\t\r\n\t\t\tmodel.compile(loss='categorical_crossentropy',\r\n\t\t\t              optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001),\r\n\t\t\t              metrics=['accuracy'])\r\n\t\t\r\n\t\tsaver = tf.train.Saver()\r\n\t\t\r\n\t\twith tf.Session(server.target) as sess:\r\n\t\t\tK.set_session(sess)\r\n\t\t\t\r\n\t\t\tdef save_checkpoint(epoch, logs=None):\r\n\t\t\t\tif epoch == 1:\r\n\t\t\t\t\ttf.train.write_graph(sess.graph.as_graph_def(), args.model_dir, 'graph.pbtxt')\r\n\t\t\t\tsaver.save(sess, os.path.join(args.model_dir, 'model.ckpt'), global_step=epoch * args.steps_per_epoch)\r\n\t\t\t\r\n\t\t\t#ckpt_callback = LambdaCallback(on_epoch_end=save_checkpoint)\r\n\t\t\t#tb_callback = TensorBoard(log_dir=args.model_dir, histogram_freq=1, write_graph=True, write_images=True)\r\n\t\t\t\r\n\t\t\t# Add callbacks to save model checkpoint and tensorboard events (on worker:0 only)\r\n\t\t\t#callbacks = [ckpt_callback, tb_callback] if ctx.task_index == 0 else None\r\n\t\t\t\r\n\t\t\t# args.input_mode == 'spark':\r\n\t\t\t#  train on data read from a generator which is producing data from a Spark RDD\r\n\t\t\ttf_feed = TFNode.DataFeed(ctx.mgr)\r\n\t\t\tmodel.fit_generator(generator=generate_rdd_data(tf_feed, batch_size),\r\n\t\t\t                    steps_per_epoch=args.steps_per_epoch,\r\n\t\t\t                    epochs=args.epochs,\r\n\t\t\t                    verbose=1,\r\n\t\t\t                    callbacks=None)\r\n\t\t\t\r\n\t\t\tif args.export_dir and ctx.job_name == 'worker' and ctx.task_index == 0:\r\n\t\t\t\t# save a local Keras model, so we can reload it with an inferencing learning_phase\r\n\t\t\t\tsave_model(model, \"tmp_model\")\r\n\t\t\t\t\r\n\t\t\t\t# reload the model\r\n\t\t\t\tK.set_learning_phase(False)\r\n\t\t\t\tnew_model = load_model(\"tmp_model\")\r\n\t\t\t\t\r\n\t\t\t\t# export a saved_model for inferencing\r\n\t\t\t\tbuilder = saved_model_builder.SavedModelBuilder(args.export_dir)\r\n\t\t\t\tsignature = predict_signature_def(inputs={'fetures': new_model.input},\r\n\t\t\t\t                                  outputs={'scores': new_model.output})\r\n\t\t\t\tbuilder.add_meta_graph_and_variables(sess=sess,\r\n\t\t\t\t                                     tags=[tag_constants.SERVING],\r\n\t\t\t\t                                     signature_def_map={'predict': signature},\r\n\t\t\t\t                                     clear_devices=True)\r\n\t\t\t\tbuilder.save()\r\n\t\t\t\r\n\t\t\tif args.input_mode == 'spark':\r\n\t\t\t\ttf_feed.terminate()\r\n\r\n\r\nif __name__ == '__main__':\r\n\timport argparse\r\n\tfrom pyspark.context import SparkContext\r\n\tfrom pyspark.conf import SparkConf\r\n\tfrom tensorflowonspark import TFCluster\r\n\timport keras\r\n\t\r\n\tsc = SparkContext(conf=SparkConf().setAppName(\"PhaseOneModelling\"))\r\n\texecutors = sc._conf.get(\"spark.executor.instances\")\r\n\tnum_executors = int(executors) if executors is not None else 1\r\n\tnum_ps = 1\r\n\t\r\n\tparser = argparse.ArgumentParser()\r\n\tparser.add_argument(\"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=num_executors)\r\n\tparser.add_argument(\"--epochs\", help=\"number of epochs of training data\", type=int, default=20)\r\n\tparser.add_argument(\"--export_dir\", help=\"directory to export saved_model\")\r\n\tparser.add_argument(\"--data\", help=\"HDFS path to data in parallelized CSV format\")\r\n\t# parser.add_argument(\"--input_mode\", help=\"input mode (tf|spark)\", default=\"tf\")\r\n\tparser.add_argument(\"--labels\", help=\"HDFS path to MNIST labels in parallelized CSV format\")\r\n\tparser.add_argument(\"--model_dir\", help=\"directory to write model checkpoints\")\r\n\tparser.add_argument(\"--num_ps\", help=\"number of ps nodes\", type=int, default=1)\r\n\tparser.add_argument(\"--steps_per_epoch\", help=\"number of steps per epoch\", type=int, default=100)\r\n\tparser.add_argument(\"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\r\n\t\r\n\targs = parser.parse_args()\r\n\tprint(\"args:\", args)\r\n\t\r\n\tdata = sc.textFile(args.data)\r\n\tdata = data.map(lambda l: l.encode(\"UTF8\", \"ignore\").split('\\t'))\r\n\t\r\n\tlabels = data.map(lambda x: x[1])\r\n\tdata = data.map(lambda x: x[19:28066])\r\n\t\r\n\theader = data.first()\r\n\tdata = data.filter(lambda line: line != header)\r\n\tlabel_header = labels.first()\r\n\tlabels = labels.filter(lambda line: line != label_header)\r\n\t\r\n\t# convert values to float\r\n\tconvertToFloat = lambda data: [float(str(x)) for x in data]\r\n\tdataset = data.map(convertToFloat)\r\n\tlabels = labels.map(lambda x: float(x))\r\n\tlabels = labels.map(lambda x: keras.utils.to_categorical(x, num_classes=14))\r\n\t\r\n\t# Split the data for train and validation\r\n\t#testRDD, trainRDD = data.randomSplit(weights=[0.001, 0.999], seed=42)\r\n\t#testlabelRDD, trainlabelRDD = labels.randomSplit(weights=[0.001, 0.999], seed=42)\r\n\t\r\n\tdataRDD = dataset.zip(labels)\r\n\t\r\n\t#dataRDD = dataRDD.sample(False, 0.01, 42)\r\n\t#trainRDD = trainRDD.zip(trainlabelRDD)\r\n\t\r\n\tcluster = TFCluster.run(sc, main_fun, args, args.cluster_size, args.num_ps, args.tensorboard,\r\n\t                        TFCluster.InputMode.SPARK, log_dir=args.model_dir)\r\n\tcluster.train(dataRDD, args.epochs)\r\n\r\n\tcluster.shutdown()\r\n\r\n```\r\n\r\n\r\nAnd the log as follows:\r\n\r\n```\r\nUsing TensorFlow backend.\r\n19/03/03 22:48:13 INFO SparkContext: Running Spark version 2.3.0.2.6.5.0-292\r\n19/03/03 22:48:13 INFO SparkContext: Submitted application: PhaseOneModelling\r\n19/03/03 22:48:13 INFO SecurityManager: Changing view acls to: Surya@..com\r\n19/03/03 22:48:13 INFO SecurityManager: Changing modify acls to: Surya@..com\r\n19/03/03 22:48:13 INFO SecurityManager: Changing view acls groups to:\r\n19/03/03 22:48:13 INFO SecurityManager: Changing modify acls groups to:\r\n19/03/03 22:48:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Surya@..com); groups with view permissions: Se                                                t(); users  with modify permissions: Set(Surya@..com); groups with modify permissions: Set()\r\n19/03/03 22:48:13 INFO Utils: Successfully started service 'sparkDriver' on port 44164.\r\n19/03/03 22:48:13 INFO SparkEnv: Registering MapOutputTracker\r\n19/03/03 22:48:13 INFO SparkEnv: Registering BlockManagerMaster\r\n19/03/03 22:48:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n19/03/03 22:48:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n19/03/03 22:48:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-966d6dac-7f6b-4411-91e2-7b4c07185c8d\r\n19/03/03 22:48:13 INFO MemoryStore: MemoryStore started with capacity 153.4 GB\r\n19/03/03 22:48:13 INFO SparkEnv: Registering OutputCommitCoordinator\r\n19/03/03 22:48:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\r\n19/03/03 22:48:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.\r\n19/03/03 22:48:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://instance-2026033.ipa.ba..com:4041\r\n19/03/03 22:48:15 INFO RMProxy: Connecting to ResourceManager at instance-2026030.ipa.ba..com/10.28.26.30:8050\r\n19/03/03 22:48:15 INFO Client: Requesting a new application from cluster with 14 NodeManagers\r\n19/03/03 22:48:15 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (202752 MB per container)\r\n19/03/03 22:48:15 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\r\n19/03/03 22:48:15 INFO Client: Setting up container launch context for our AM\r\n19/03/03 22:48:15 INFO Client: Setting up the launch environment for our AM container\r\n19/03/03 22:48:15 INFO Client: Preparing resources for our AM container\r\n19/03/03 22:48:15 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-438946188_31, ugi=Surya@..com (auth:KERBEROS)]]\r\n19/03/03 22:48:16 INFO KerberosName: Non-simple name Surya@..com after auth_to_local rule RULE:[1:$1@$0](.*@..com)/L\r\n19/03/03 22:48:16 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 6651 for Surya@..com on 10.28.26.29:8020\r\n19/03/03 22:48:19 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://instance-2026029.ipa.ba..com:8020/hdp/apps/2.6.5.0-292/spark2/spark2-hdp-yarn-archive                                                .tar.gz\r\n19/03/03 22:48:19 INFO Client: Source and destination file systems are the same. Not copying hdfs://instance-2026029.ipa.ba..com:8020/hdp/apps/2.6.5.0-292/spark2/spark2-hdp-yarn-archive.ta                                                r.gz\r\n19/03/03 22:48:19 INFO Client: Uploading resource file:/usr/hdp/2.6.5.0-292/spark2/python/lib/pyspark.zip -> hdfs://instance-2026029.ipa.ba..com:8020/user/Surya@..com/.sparkStagin                                                g/application_1551114784635_0177/pyspark.zip\r\n19/03/03 22:48:19 INFO Client: Uploading resource file:/usr/hdp/2.6.5.0-292/spark2/python/lib/py4j-0.10.6-src.zip -> hdfs://instance-2026029.ipa.ba..com:8020/user/Surya@..com/.spa                                                rkStaging/application_1551114784635_0177/py4j-0.10.6-src.zip\r\n19/03/03 22:48:19 INFO Client: Uploading resource file:/tmp/spark-2c81098d-4f91-4a1d-87ea-71ecf1c72204/__spark_conf__626918427791303293.zip -> hdfs://instance-2026029.ipa.ba..com:8020/user                                                /Surya@..com/.sparkStaging/application_1551114784635_0177/__spark_conf__.zip\r\n19/03/03 22:48:19 INFO SecurityManager: Changing view acls to: Surya@..com\r\n19/03/03 22:48:19 INFO SecurityManager: Changing modify acls to: Surya@..com\r\n19/03/03 22:48:19 INFO SecurityManager: Changing view acls groups to:\r\n19/03/03 22:48:19 INFO SecurityManager: Changing modify acls groups to:\r\n19/03/03 22:48:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Surya@..com); groups with view permissions: Se                                                t(); users  with modify permissions: Set(Surya@..com); groups with modify permissions: Set()\r\n19/03/03 22:48:19 INFO Client: Submitting application application_1551114784635_0177 to ResourceManager\r\n19/03/03 22:48:22 INFO YarnClientImpl: Submitted application application_1551114784635_0177\r\n19/03/03 22:48:22 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1551114784635_0177 and attemptId None\r\n19/03/03 22:48:23 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:23 INFO Client:\r\n         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\r\n         diagnostics: AM container is launched, waiting for AM container to Register with RM\r\n         ApplicationMaster host: N/A\r\n         ApplicationMaster RPC port: -1\r\n         queue: production\r\n         start time: 1551671302538\r\n         final status: UNDEFINED\r\n         tracking URL: https://instance-2026030.ipa.ba..com:8090/proxy/application_1551114784635_0177/\r\n         user: Surya@..com\r\n19/03/03 22:48:24 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:25 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:26 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:27 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:28 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:29 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:30 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:31 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> instance-2026030.ipa.ba..com, PROXY_URI_                                                BASES -> https://instance-2026030.ipa.ba..com:8090/proxy/application_1551114784635_0177), /proxy/application_1551114784635_0177\r\n19/03/03 22:48:31 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\r\n19/03/03 22:48:31 INFO Client: Application report for application_1551114784635_0177 (state: ACCEPTED)\r\n19/03/03 22:48:31 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\r\n19/03/03 22:48:32 INFO Client: Application report for application_1551114784635_0177 (state: RUNNING)\r\n19/03/03 22:48:32 INFO Client:\r\n         client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\r\n         diagnostics: N/A\r\n         ApplicationMaster host: 10.28.26.40\r\n         ApplicationMaster RPC port: 0\r\n         queue: production\r\n         start time: 1551671302538\r\n         final status: UNDEFINED\r\n         tracking URL: https://instance-2026030.ipa.ba..com:8090/proxy/application_1551114784635_0177/\r\n         user: Surya@..com\r\n19/03/03 22:48:32 INFO YarnClientSchedulerBackend: Application application_1551114784635_0177 has started running.\r\n19/03/03 22:48:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38256.\r\n19/03/03 22:48:32 INFO NettyBlockTransferService: Server created on instance-2026033.ipa.ba..com:38256\r\n19/03/03 22:48:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n19/03/03 22:48:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, instance-2026033.ipa.ba..com, 38256, None)\r\n19/03/03 22:48:32 INFO BlockManagerMasterEndpoint: Registering block manager instance-2026033.ipa.ba..com:38256 with 153.4 GB RAM, BlockManagerId(driver, instance-2026033.ipa.ba..com, 382                                                56, None)\r\n19/03/03 22:48:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, instance-2026033.ipa.ba..com, 38256, None)\r\n19/03/03 22:48:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, instance-2026033.ipa.ba..com, 38256, None)\r\n19/03/03 22:48:33 INFO EventLoggingListener: Logging events to hdfs:/spark2-history/application_1551114784635_0177\r\n19/03/03 22:48:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.28.26.39:36672) with ID 3\r\n19/03/03 22:48:38 INFO BlockManagerMasterEndpoint: Registering block manager instance-2026039.ipa.ba..com:33711 with 153.4 GB RAM, BlockManagerId(3, instance-2026039.ipa.ba..com, 33711, N                                                one)\r\n19/03/03 22:48:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.28.26.38:49710) with ID 2\r\n19/03/03 22:48:39 INFO BlockManagerMasterEndpoint: Registering block manager instance-2026038.ipa.ba..com:45476 with 153.4 GB RAM, BlockManagerId(2, instance-2026038.ipa.ba..com, 45476, N                                                one)\r\n19/03/03 22:48:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.28.26.45:36844) with ID 1\r\n19/03/03 22:48:39 INFO BlockManagerMasterEndpoint: Registering block manager instance-2026045.ipa.ba..com:45485 with 153.4 GB RAM, BlockManagerId(1, instance-2026045.ipa.ba..com, 45485, N                                                one)\r\n19/03/03 22:48:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.28.26.46:58368) with ID 4\r\n19/03/03 22:48:39 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.28.26.43:54760) with ID 6\r\n19/03/03 22:48:39 INFO BlockManagerMasterEndpoint: Registering block manager instance-2026046.ipa.ba..com:41346 with 153.4 GB RAM, BlockManagerId(4, instance-2026046.ipa.ba..com, 41346, N                                                one)\r\n19/03/03 22:48:40 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\r\n19/03/03 22:48:40 INFO BlockManagerMasterEndpoint: Registering block manager instance-2026043.ipa.ba..com:33242 with 153.4 GB RAM, BlockManagerId(6, instance-2026043.ipa.ba..com, 33242, N                                                one)\r\nargs: Namespace(cluster_size=6, data='/user/imagen.admins/NormalizedAugustData/Wide/ReducedFeatures/wide_august_tf_idf_normalized_with_col_ReducedFeatures.tsv', epochs=5, export_dir='/tmp/m                                                ss/TensorflowOnSpark/export_dir/', labels=None, model_dir='/tmp/mss/TensorflowOnSpark/model_dir', num_ps=1, steps_per_epoch=2622, tensorboard=False)\r\n19/03/03 22:48:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 466.7 KB, free 153.4 GB)\r\n19/03/03 22:48:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KB, free 153.4 GB)\r\n19/03/03 22:48:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on instance-2026033.ipa.ba..com:38256 (size: 34.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:40 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n19/03/03 22:48:40 INFO KerberosName: Non-simple name Surya@..com after auth_to_local rule RULE:[1:$1@$0](.*@..com)/L\r\n19/03/03 22:48:40 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 6652 for Surya@..com on 10.28.26.29:8020\r\n19/03/03 22:48:40 INFO TokenCache: Got dt for hdfs://instance-2026029.ipa.ba..com:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 10.28.26.29:8020, Ident: (HDFS_DELEGATION_TOKEN token 6652 fo                                                r Surya@..com)\r\n19/03/03 22:48:40 INFO FileInputFormat: Total input paths to process : 1\r\n19/03/03 22:48:40 INFO SparkContext: Starting job: runJob at PythonRDD.scala:141\r\n19/03/03 22:48:40 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:141) with 1 output partitions\r\n19/03/03 22:48:40 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:141)\r\n19/03/03 22:48:40 INFO DAGScheduler: Parents of final stage: List()\r\n19/03/03 22:48:40 INFO DAGScheduler: Missing parents: List()\r\n19/03/03 22:48:40 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:48), which has no missing parents\r\n19/03/03 22:48:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 153.4 GB)\r\n19/03/03 22:48:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KB, free 153.4 GB)\r\n19/03/03 22:48:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on instance-2026033.ipa.ba..com:38256 (size: 4.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039\r\n19/03/03 22:48:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0))\r\n19/03/03 22:48:40 INFO YarnScheduler: Adding task set 0.0 with 1 tasks\r\n19/03/03 22:48:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, instance-2026046.ipa.ba..com, executor 4, partition 0, RACK_LOCAL, 8020 bytes)\r\n19/03/03 22:48:41 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.28.26.37:34390) with ID 5\r\n19/03/03 22:48:41 INFO BlockManagerMasterEndpoint: Registering block manager instance-2026037.ipa.ba..com:45022 with 153.4 GB RAM, BlockManagerId(5, instance-2026037.ipa.ba..com, 45022, N                                                one)\r\n19/03/03 22:48:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on instance-2026046.ipa.ba..com:41346 (size: 4.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on instance-2026046.ipa.ba..com:41346 (size: 34.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2887 ms on instance-2026046.ipa.ba..com (executor 4) (1/1)\r\n19/03/03 22:48:43 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool\r\n19/03/03 22:48:43 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:141) finished in 3.100 s\r\n19/03/03 22:48:43 INFO DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:141, took 3.163148 s\r\n19/03/03 22:48:43 INFO SparkContext: Starting job: runJob at PythonRDD.scala:141\r\n19/03/03 22:48:43 INFO DAGScheduler: Got job 1 (runJob at PythonRDD.scala:141) with 1 output partitions\r\n19/03/03 22:48:43 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at PythonRDD.scala:141)\r\n19/03/03 22:48:43 INFO DAGScheduler: Parents of final stage: List()\r\n19/03/03 22:48:43 INFO DAGScheduler: Missing parents: List()\r\n19/03/03 22:48:43 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[3] at RDD at PythonRDD.scala:48), which has no missing parents\r\n19/03/03 22:48:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 6.3 KB, free 153.4 GB)\r\n19/03/03 22:48:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 153.4 GB)\r\n19/03/03 22:48:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on instance-2026033.ipa.ba..com:38256 (size: 4.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039\r\n19/03/03 22:48:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[3] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0))\r\n19/03/03 22:48:43 INFO YarnScheduler: Adding task set 1.0 with 1 tasks\r\n19/03/03 22:48:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, instance-2026037.ipa.ba..com, executor 5, partition 0, NODE_LOCAL, 8020 bytes)\r\n19/03/03 22:48:44 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on instance-2026037.ipa.ba..com:45022 (size: 4.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on instance-2026037.ipa.ba..com:45022 (size: 34.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2121 ms on instance-2026037.ipa.ba..com (executor 5) (1/1)\r\n19/03/03 22:48:45 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool\r\n19/03/03 22:48:45 INFO DAGScheduler: ResultStage 1 (runJob at PythonRDD.scala:141) finished in 2.129 s\r\n19/03/03 22:48:45 INFO DAGScheduler: Job 1 finished: runJob at PythonRDD.scala:141, took 2.132856 s\r\n2019-03-03 22:48:46,218 INFO (MainThread-75262) Reserving TFSparkNodes\r\n2019-03-03 22:48:46,218 INFO (MainThread-75262) cluster_template: {'ps': [0], 'worker': [1, 2, 3, 4, 5]}\r\n2019-03-03 22:48:46,219 INFO (MainThread-75262) listening for reservations at ('10.28.26.33', 42213)\r\n2019-03-03 22:48:46,219 INFO (MainThread-75262) Starting TensorFlow on executors\r\n2019-03-03 22:48:46,224 INFO (MainThread-75262) Waiting for TFSparkNodes to start\r\n2019-03-03 22:48:46,224 INFO (MainThread-75262) waiting for 6 reservations\r\n19/03/03 22:48:46 INFO SparkContext: Starting job: foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:301\r\n19/03/03 22:48:46 INFO DAGScheduler: Got job 2 (foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:301) with 6 output partitions\r\n19/03/03 22:48:46 INFO DAGScheduler: Final stage: ResultStage 2 (foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:301)\r\n19/03/03 22:48:46 INFO DAGScheduler: Parents of final stage: List()\r\n19/03/03 22:48:46 INFO DAGScheduler: Missing parents: List()\r\n19/03/03 22:48:46 INFO DAGScheduler: Submitting ResultStage 2 (PythonRDD[8] at foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:301), which has no missing                                                 parents\r\n19/03/03 22:48:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.4 KB, free 153.4 GB)\r\n19/03/03 22:48:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.5 KB, free 153.4 GB)\r\n19/03/03 22:48:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-2026033.ipa.ba..com:38256 (size: 12.5 KB, free: 153.4 GB)\r\n19/03/03 22:48:46 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1039\r\n19/03/03 22:48:46 INFO DAGScheduler: Submitting 6 missing tasks from ResultStage 2 (PythonRDD[8] at foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:301)                                                 (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\r\n19/03/03 22:48:46 INFO YarnScheduler: Adding task set 2.0 with 6 tasks\r\n19/03/03 22:48:46 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, instance-2026039.ipa.ba..com, executor 3, partition 0, PROCESS_LOCAL, 7869 bytes)\r\n19/03/03 22:48:46 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3, instance-2026046.ipa.ba..com, executor 4, partition 1, PROCESS_LOCAL, 7869 bytes)\r\n19/03/03 22:48:46 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4, instance-2026038.ipa.ba..com, executor 2, partition 2, PROCESS_LOCAL, 7869 bytes)\r\n19/03/03 22:48:46 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5, instance-2026037.ipa.ba..com, executor 5, partition 3, PROCESS_LOCAL, 7869 bytes)\r\n19/03/03 22:48:46 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 6, instance-2026043.ipa.ba..com, executor 6, partition 4, PROCESS_LOCAL, 7869 bytes)\r\n19/03/03 22:48:46 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7, instance-2026045.ipa.ba..com, executor 1, partition 5, PROCESS_LOCAL, 7869 bytes)\r\n19/03/03 22:48:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-2026037.ipa.ba..com:45022 (size: 12.5 KB, free: 153.4 GB)\r\n19/03/03 22:48:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-2026046.ipa.ba..com:41346 (size: 12.5 KB, free: 153.4 GB)\r\n19/03/03 22:48:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-2026039.ipa.ba..com:33711 (size: 12.5 KB, free: 153.4 GB)\r\n19/03/03 22:48:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-2026038.ipa.ba..com:45476 (size: 12.5 KB, free: 153.4 GB)\r\n19/03/03 22:48:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-2026045.ipa.ba..com:45485 (size: 12.5 KB, free: 153.4 GB)\r\n19/03/03 22:48:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on instance-2026043.ipa.ba..com:33242 (size: 12.5 KB, free: 153.4 GB)\r\n2019-03-03 22:48:47,226 INFO (MainThread-75262) waiting for 6 reservations\r\n2019-03-03 22:48:48,226 INFO (MainThread-75262) waiting for 4 reservations\r\n2019-03-03 22:48:49,228 INFO (MainThread-75262) all reservations completed\r\n2019-03-03 22:48:49,228 INFO (MainThread-75262) All TFSparkNodes started\r\n2019-03-03 22:48:49,228 INFO (MainThread-75262) {'executor_id': 3, 'addr': '/tmp/pymp-Nm3GK_/listener-psN9ka', 'task_index': 2, 'job_name': 'worker', 'authkey': '\\x19\\x83\\xec\\x1e\\xacNM\\x16\\                                                x89\\xc0\\xa0,\\x9b\\x87$\\xd3', 'host': '10.28.26.37', 'port': 34616, 'tb_pid': 0, 'tb_port': 0}\r\n2019-03-03 22:48:49,228 INFO (MainThread-75262) {'executor_id': 1, 'addr': '/tmp/pymp-anFXbv/listener-gYfcyl', 'task_index': 0, 'job_name': 'worker', 'authkey': '\\xca\\xfe\\xf8+k\\xbfC\\n\\xbfI\\                                                x19\\xc7=\\xefR\\x17', 'host': '10.28.26.46', 'port': 35676, 'tb_pid': 0, 'tb_port': 0}\r\n2019-03-03 22:48:49,228 INFO (MainThread-75262) {'executor_id': 2, 'addr': '/tmp/pymp-Ws00Ww/listener-J06992', 'task_index': 1, 'job_name': 'worker', 'authkey': '?\\xc8\\xef\\xde\\x98\\xb3EB\\x8f                                                O\\x80\\x89\\xeb\\xff\\x83\\x91', 'host': '10.28.26.38', 'port': 34594, 'tb_pid': 0, 'tb_port': 0}\r\n2019-03-03 22:48:49,228 INFO (MainThread-75262) {'executor_id': 0, 'addr': ('10.28.26.39', 45967), 'task_index': 0, 'job_name': 'ps', 'authkey': '\\xce\\x1b$\\xbeg6@T\\xb0q\\xb8I\\x04\\xc5\\x1a\\r'                                                , 'host': '10.28.26.39', 'port': 39537, 'tb_pid': 0, 'tb_port': 0}\r\n2019-03-03 22:48:49,229 INFO (MainThread-75262) {'executor_id': 4, 'addr': '/tmp/pymp-9cxRDb/listener-Vfnobx', 'task_index': 3, 'job_name': 'worker', 'authkey': '\\xad\\xf6\\x10\\xb6\\x9f\\x13O\\x                                                c1\\xbb\\xb0\\xd6\\x85\\xb3e\\x16\\xee', 'host': '10.28.26.43', 'port': 35727, 'tb_pid': 0, 'tb_port': 0}\r\n2019-03-03 22:48:49,229 INFO (MainThread-75262) {'executor_id': 5, 'addr': '/tmp/pymp-C7cs51/listener-KKXXmn', 'task_index': 4, 'job_name': 'worker', 'authkey': ';\\xbe\"\\xd73\\xcdLD\\xa559Z\\xc                                                b{\\x92\\x92', 'host': '10.28.26.45', 'port': 46460, 'tb_pid': 0, 'tb_port': 0}\r\n2019-03-03 22:48:49,229 INFO (MainThread-75262) Feeding training data\r\n19/03/03 22:48:49 INFO SparkContext: Starting job: collect at PythonRDD.scala:153\r\n19/03/03 22:48:49 INFO DAGScheduler: Got job 3 (collect at PythonRDD.scala:153) with 600 output partitions\r\n19/03/03 22:48:49 INFO DAGScheduler: Final stage: ResultStage 3 (collect at PythonRDD.scala:153)\r\n19/03/03 22:48:49 INFO DAGScheduler: Parents of final stage: List()\r\n19/03/03 22:48:49 INFO DAGScheduler: Missing parents: List()\r\n19/03/03 22:48:49 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[10] at RDD at PythonRDD.scala:48), which has no missing parents\r\n19/03/03 22:48:49 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 389.2 KB, free 153.4 GB)\r\n19/03/03 22:48:49 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 242.0 KB, free 153.4 GB)\r\n19/03/03 22:48:49 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on instance-2026033.ipa.ba..com:38256 (size: 242.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:49 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1039\r\n19/03/03 22:48:49 INFO DAGScheduler: Submitting 600 missing tasks from ResultStage 3 (PythonRDD[10] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5,                                                 6, 7, 8, 9, 10, 11, 12, 13, 14))\r\n19/03/03 22:48:49 INFO YarnScheduler: Adding task set 3.0 with 600 tasks\r\n19/03/03 22:48:49 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 7) in 3490 ms on instance-2026045.ipa.ba..com (executor 1) (1/6)\r\n19/03/03 22:48:49 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8, instance-2026037.ipa.ba..com, executor 5, partition 0, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:48:49 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 3569 ms on instance-2026037.ipa.ba..com (executor 5) (2/6)\r\n19/03/03 22:48:49 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on instance-2026037.ipa.ba..com:45022 (size: 242.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:50 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 3867 ms on instance-2026046.ipa.ba..com (executor 4) (3/6)\r\n19/03/03 22:48:50 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 9, instance-2026038.ipa.ba..com, executor 2, partition 3, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:48:50 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 4240 ms on instance-2026038.ipa.ba..com (executor 2) (4/6)\r\n19/03/03 22:48:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on instance-2026038.ipa.ba..com:45476 (size: 242.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on instance-2026038.ipa.ba..com:45476 (size: 34.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:50 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 10, instance-2026043.ipa.ba..com, executor 6, partition 1, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:48:50 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 6) in 4423 ms on instance-2026043.ipa.ba..com (executor 6) (5/6)\r\n19/03/03 22:48:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on instance-2026043.ipa.ba..com:33242 (size: 242.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on instance-2026043.ipa.ba..com:33242 (size: 34.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:54 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 11, instance-2026046.ipa.ba..com, executor 4, partition 2, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:48:54 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 12, instance-2026045.ipa.ba..com, executor 1, partition 4, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:48:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on instance-2026045.ipa.ba..com:45485 (size: 242.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on instance-2026046.ipa.ba..com:41346 (size: 242.0 KB, free: 153.4 GB)\r\n19/03/03 22:48:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on instance-2026045.ipa.ba..com:45485 (size: 34.0 KB, free: 153.4 GB)\r\n19/03/03 22:49:35 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 13, instance-2026038.ipa.ba..com, executor 2, partition 5, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:49:35 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 9) in 45462 ms on instance-2026038.ipa.ba..com (executor 2) (1/600)\r\n19/03/03 22:49:36 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 14, instance-2026037.ipa.ba..com, executor 5, partition 6, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:49:36 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 46239 ms on instance-2026037.ipa.ba..com (executor 5) (2/600)\r\n19/03/03 22:49:36 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 15, instance-2026043.ipa.ba..com, executor 6, partition 7, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:49:36 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 10) in 45790 ms on instance-2026043.ipa.ba..com (executor 6) (3/600)\r\n19/03/03 22:49:39 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 11) in 44513 ms on instance-2026046.ipa.ba..com (executor 4) (4/600)\r\n19/03/03 22:49:39 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 16, instance-2026046.ipa.ba..com, executor 4, partition 8, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:49:40 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 17, instance-2026045.ipa.ba..com, executor 1, partition 9, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:49:40 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 12) in 45791 ms on instance-2026045.ipa.ba..com (executor 1) (5/600)\r\n19/03/03 22:50:15 INFO TaskSetManager: Starting task 21.0 in stage 3.0 (TID 18, instance-2026038.ipa.ba..com, executor 2, partition 21, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:50:15 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 13) in 39597 ms on instance-2026038.ipa.ba..com (executor 2) (6/600)\r\n19/03/03 22:50:16 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 19, instance-2026043.ipa.ba..com, executor 6, partition 11, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:50:16 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 15) in 39619 ms on instance-2026043.ipa.ba..com (executor 6) (7/600)\r\n19/03/03 22:50:17 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 20, instance-2026037.ipa.ba..com, executor 5, partition 10, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:50:17 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 14) in 41346 ms on instance-2026037.ipa.ba..com (executor 5) (8/600)\r\n19/03/03 22:50:19 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 16) in 39872 ms on instance-2026046.ipa.ba..com (executor 4) (9/600)\r\n19/03/03 22:50:20 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 21, instance-2026046.ipa.ba..com, executor 4, partition 12, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:50:24 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 22, instance-2026045.ipa.ba..com, executor 1, partition 13, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:50:24 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 17) in 43885 ms on instance-2026045.ipa.ba..com (executor 1) (10/600)\r\n19/03/03 22:50:54 INFO TaskSetManager: Starting task 26.0 in stage 3.0 (TID 23, instance-2026038.ipa.ba..com, executor 2, partition 26, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:50:54 INFO TaskSetManager: Finished task 21.0 in stage 3.0 (TID 18) in 39348 ms on instance-2026038.ipa.ba..com (executor 2) (11/600)\r\n19/03/03 22:50:55 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 24, instance-2026043.ipa.ba..com, executor 6, partition 14, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:50:55 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 19) in 39112 ms on instance-2026043.ipa.ba..com (executor 6) (12/600)\r\n19/03/03 22:50:58 INFO TaskSetManager: Starting task 24.0 in stage 3.0 (TID 25, instance-2026037.ipa.ba..com, executor 5, partition 24, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:50:58 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 20) in 41423 ms on instance-2026037.ipa.ba..com (executor 5) (13/600)\r\n19/03/03 22:51:00 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 21) in 39971 ms on instance-2026046.ipa.ba..com (executor 4) (14/600)\r\n19/03/03 22:51:02 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 26, instance-2026046.ipa.ba..com, executor 4, partition 15, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:51:03 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 27, instance-2026045.ipa.ba..com, executor 1, partition 16, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:51:03 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 22) in 39738 ms on instance-2026045.ipa.ba..com (executor 1) (15/600)\r\n19/03/03 22:51:34 INFO TaskSetManager: Starting task 32.0 in stage 3.0 (TID 28, instance-2026043.ipa.ba..com, executor 6, partition 32, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:51:34 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 24) in 39273 ms on instance-2026043.ipa.ba..com (executor 6) (16/600)\r\n19/03/03 22:51:34 INFO TaskSetManager: Starting task 31.0 in stage 3.0 (TID 29, instance-2026038.ipa.ba..com, executor 2, partition 31, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:51:34 INFO TaskSetManager: Finished task 26.0 in stage 3.0 (TID 23) in 39756 ms on instance-2026038.ipa.ba..com (executor 2) (17/600)\r\n19/03/03 22:51:40 INFO TaskSetManager: Starting task 25.0 in stage 3.0 (TID 30, instance-2026037.ipa.ba..com, executor 5, partition 25, NODE_LOCAL, 8440 bytes)\r\n19/03/03 22:51:40 INFO TaskSetManager: Finished task 24.0 in stage 3.0 (TID 25) in 41287 ms on instance-2026037.ipa.ba..com (executor 5) (18/600)\r\n19/03/03 22:51:42 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 26) in 39507 ms on instance-2026046.ipa.ba..com (executor 4) (19/600)\r\n19/03/03 22:51:43 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 31, instance-2026045.ipa.ba..com, executor 1, partition 17, RACK_LOCAL, 8440 bytes)\r\n19/03/03 22:51:43 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 27) in 39474 ms on instance-2026045.ipa.ba..com (executor 1) (20/600)\r\n19/03/04 00:07:54 INFO TaskSetManager: Starting task 541.0 in stage 3.0 (TID 583, instance-2026038.ipa.ba..com, executor 2, partition 541, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:07:54 INFO TaskSetManager: Finished task 533.0 in stage 3.0 (TID 579) in 40271 ms on instance-2026038.ipa.ba..com (executor 2) (571/600)\r\n19/03/04 00:07:57 INFO TaskSetManager: Starting task 543.0 in stage 3.0 (TID 584, instance-2026037.ipa.ba..com, executor 5, partition 543, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:07:57 INFO TaskSetManager: Finished task 532.0 in stage 3.0 (TID 578) in 44489 ms on instance-2026037.ipa.ba..com (executor 5) (572/600)\r\n19/03/04 00:08:12 INFO TaskSetManager: Starting task 544.0 in stage 3.0 (TID 585, instance-2026043.ipa.ba..com, executor 6, partition 544, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:08:12 INFO TaskSetManager: Finished task 534.0 in stage 3.0 (TID 580) in 40645 ms on instance-2026043.ipa.ba..com (executor 6) (573/600)\r\n19/03/04 00:08:17 INFO TaskSetManager: Starting task 545.0 in stage 3.0 (TID 586, instance-2026045.ipa.ba..com, executor 1, partition 545, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:08:17 INFO TaskSetManager: Finished task 535.0 in stage 3.0 (TID 581) in 40536 ms on instance-2026045.ipa.ba..com (executor 1) (574/600)\r\n19/03/04 00:08:31 INFO TaskSetManager: Starting task 548.0 in stage 3.0 (TID 587, instance-2026046.ipa.ba..com, executor 4, partition 548, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:08:31 INFO TaskSetManager: Finished task 540.0 in stage 3.0 (TID 582) in 38886 ms on instance-2026046.ipa.ba..com (executor 4) (575/600)\r\n19/03/04 00:08:36 INFO TaskSetManager: Starting task 551.0 in stage 3.0 (TID 588, instance-2026038.ipa.ba..com, executor 2, partition 551, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:08:36 INFO TaskSetManager: Finished task 541.0 in stage 3.0 (TID 583) in 41700 ms on instance-2026038.ipa.ba..com (executor 2) (576/600)\r\n19/03/04 00:08:41 INFO TaskSetManager: Starting task 554.0 in stage 3.0 (TID 589, instance-2026037.ipa.ba..com, executor 5, partition 554, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:08:41 INFO TaskSetManager: Finished task 543.0 in stage 3.0 (TID 584) in 44824 ms on instance-2026037.ipa.ba..com (executor 5) (577/600)\r\n19/03/04 00:08:54 INFO TaskSetManager: Starting task 555.0 in stage 3.0 (TID 590, instance-2026043.ipa.ba..com, executor 6, partition 555, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:08:54 INFO TaskSetManager: Finished task 544.0 in stage 3.0 (TID 585) in 41618 ms on instance-2026043.ipa.ba..com (executor 6) (578/600)\r\n19/03/04 00:08:58 INFO TaskSetManager: Starting task 558.0 in stage 3.0 (TID 591, instance-2026045.ipa.ba..com, executor 1, partition 558, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:08:58 INFO TaskSetManager: Finished task 545.0 in stage 3.0 (TID 586) in 40567 ms on instance-2026045.ipa.ba..com (executor 1) (579/600)\r\n19/03/04 00:09:10 INFO TaskSetManager: Starting task 559.0 in stage 3.0 (TID 592, instance-2026046.ipa.ba..com, executor 4, partition 559, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:09:10 INFO TaskSetManager: Finished task 548.0 in stage 3.0 (TID 587) in 39338 ms on instance-2026046.ipa.ba..com (executor 4) (580/600)\r\n19/03/04 00:09:17 INFO TaskSetManager: Starting task 562.0 in stage 3.0 (TID 593, instance-2026038.ipa.ba..com, executor 2, partition 562, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:09:17 INFO TaskSetManager: Finished task 551.0 in stage 3.0 (TID 588) in 41027 ms on instance-2026038.ipa.ba..com (executor 2) (581/600)\r\n19/03/04 00:09:27 INFO TaskSetManager: Starting task 563.0 in stage 3.0 (TID 594, instance-2026037.ipa.ba..com, executor 5, partition 563, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:09:27 INFO TaskSetManager: Finished task 554.0 in stage 3.0 (TID 589) in 45716 ms on instance-2026037.ipa.ba..com (executor 5) (582/600)\r\n19/03/04 00:09:35 INFO TaskSetManager: Starting task 565.0 in stage 3.0 (TID 595, instance-2026043.ipa.ba..com, executor 6, partition 565, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:09:35 INFO TaskSetManager: Finished task 555.0 in stage 3.0 (TID 590) in 41138 ms on instance-2026043.ipa.ba..com (executor 6) (583/600)\r\n19/03/04 00:09:40 INFO TaskSetManager: Starting task 568.0 in stage 3.0 (TID 596, instance-2026045.ipa.ba..com, executor 1, partition 568, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:09:40 INFO TaskSetManager: Finished task 558.0 in stage 3.0 (TID 591) in 42608 ms on instance-2026045.ipa.ba..com (executor 1) (584/600)\r\n19/03/04 00:09:50 INFO TaskSetManager: Starting task 574.0 in stage 3.0 (TID 597, instance-2026046.ipa.ba..com, executor 4, partition 574, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:09:50 INFO TaskSetManager: Finished task 559.0 in stage 3.0 (TID 592) in 39112 ms on instance-2026046.ipa.ba..com (executor 4) (585/600)\r\n19/03/04 00:09:58 INFO TaskSetManager: Starting task 578.0 in stage 3.0 (TID 598, instance-2026038.ipa.ba..com, executor 2, partition 578, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:09:58 INFO TaskSetManager: Finished task 562.0 in stage 3.0 (TID 593) in 41681 ms on instance-2026038.ipa.ba..com (executor 2) (586/600)\r\n19/03/04 00:10:08 INFO TaskSetManager: Starting task 579.0 in stage 3.0 (TID 599, instance-2026037.ipa.ba..com, executor 5, partition 579, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:10:08 INFO TaskSetManager: Finished task 563.0 in stage 3.0 (TID 594) in 41401 ms on instance-2026037.ipa.ba..com (executor 5) (587/600)\r\n19/03/04 00:10:16 INFO TaskSetManager: Starting task 582.0 in stage 3.0 (TID 600, instance-2026043.ipa.ba..com, executor 6, partition 582, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:10:16 INFO TaskSetManager: Finished task 565.0 in stage 3.0 (TID 595) in 41009 ms on instance-2026043.ipa.ba..com (executor 6) (588/600)\r\n19/03/04 00:10:22 INFO TaskSetManager: Starting task 583.0 in stage 3.0 (TID 601, instance-2026045.ipa.ba..com, executor 1, partition 583, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:10:22 INFO TaskSetManager: Finished task 568.0 in stage 3.0 (TID 596) in 41692 ms on instance-2026045.ipa.ba..com (executor 1) (589/600)\r\n19/03/04 00:10:29 INFO TaskSetManager: Starting task 584.0 in stage 3.0 (TID 602, instance-2026046.ipa.ba..com, executor 4, partition 584, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:10:29 INFO TaskSetManager: Finished task 574.0 in stage 3.0 (TID 597) in 39804 ms on instance-2026046.ipa.ba..com (executor 4) (590/600)\r\n19/03/04 00:10:41 INFO TaskSetManager: Starting task 588.0 in stage 3.0 (TID 603, instance-2026038.ipa.ba..com, executor 2, partition 588, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:10:41 INFO TaskSetManager: Finished task 578.0 in stage 3.0 (TID 598) in 42417 ms on instance-2026038.ipa.ba..com (executor 2) (591/600)\r\n19/03/04 00:10:50 INFO TaskSetManager: Starting task 590.0 in stage 3.0 (TID 604, instance-2026037.ipa.ba..com, executor 5, partition 590, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:10:50 INFO TaskSetManager: Finished task 579.0 in stage 3.0 (TID 599) in 41431 ms on instance-2026037.ipa.ba..com (executor 5) (592/600)\r\n19/03/04 00:10:59 INFO TaskSetManager: Starting task 591.0 in stage 3.0 (TID 605, instance-2026043.ipa.ba..com, executor 6, partition 591, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:10:59 INFO TaskSetManager: Finished task 582.0 in stage 3.0 (TID 600) in 42926 ms on instance-2026043.ipa.ba..com (executor 6) (593/600)\r\n19/03/04 00:11:04 INFO TaskSetManager: Starting task 595.0 in stage 3.0 (TID 606, instance-2026045.ipa.ba..com, executor 1, partition 595, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:11:04 INFO TaskSetManager: Finished task 583.0 in stage 3.0 (TID 601) in 42264 ms on instance-2026045.ipa.ba..com (executor 1) (594/600)\r\n19/03/04 00:11:10 INFO TaskSetManager: Starting task 597.0 in stage 3.0 (TID 607, instance-2026046.ipa.ba..com, executor 4, partition 597, RACK_LOCAL, 8440 bytes)\r\n19/03/04 00:11:10 INFO TaskSetManager: Finished task 584.0 in stage 3.0 (TID 602) in 40587 ms on instance-2026046.ipa.ba..com (executor 4) (595/600)\r\n19/03/04 00:11:25 INFO TaskSetManager: Finished task 588.0 in stage 3.0 (TID 603) in 44005 ms on instance-2026038.ipa.ba..com (executor 2) (596/600)\r\n19/03/04 00:11:32 INFO TaskSetManager: Finished task 590.0 in stage 3.0 (TID 604) in 42027 ms on instance-2026037.ipa.ba..com (executor 5) (597/600)\r\n19/03/04 00:11:42 INFO TaskSetManager: Finished task 591.0 in stage 3.0 (TID 605) in 43176 ms on instance-2026043.ipa.ba..com (executor 6) (598/600)\r\n19/03/04 00:11:47 INFO TaskSetManager: Finished task 595.0 in stage 3.0 (TID 606) in 43036 ms on instance-2026045.ipa.ba..com (executor 1) (599/600)\r\n19/03/04 00:11:51 INFO TaskSetManager: Finished task 597.0 in stage 3.0 (TID 607) in 41030 ms on instance-2026046.ipa.ba..com (executor 4) (600/600)\r\n19/03/04 00:11:51 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool\r\n19/03/04 00:11:51 INFO DAGScheduler: ResultStage 3 (collect at PythonRDD.scala:153) finished in 4982.168 s\r\n19/03/04 00:11:51 INFO DAGScheduler: Job 3 finished: collect at PythonRDD.scala:153, took 4982.215393 s\r\n2019-03-04 00:11:51,504 INFO (MainThread-75262) Stopping TensorFlow nodes\r\n19/03/04 00:11:51 INFO SparkContext: Starting job: collect at PythonRDD.scala:153\r\n19/03/04 00:11:51 INFO DAGScheduler: Got job 4 (collect at PythonRDD.scala:153) with 5 output partitions\r\n19/03/04 00:11:51 INFO DAGScheduler: Final stage: ResultStage 4 (collect at PythonRDD.scala:153)\r\n19/03/04 00:11:51 INFO DAGScheduler: Parents of final stage: List()\r\n19/03/04 00:11:51 INFO DAGScheduler: Missing parents: List()\r\n19/03/04 00:11:51 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[12] at RDD at PythonRDD.scala:48), which has no missing parents\r\n19/03/04 00:11:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 7.4 KB, free 153.4 GB)\r\n19/03/04 00:11:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.9 KB, free 153.4 GB)\r\n19/03/04 00:11:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on instance-2026033.ipa.ba..com:38256 (size: 4.9 KB, free: 153.4 GB)\r\n19/03/04 00:11:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1039\r\n19/03/04 00:11:51 INFO DAGScheduler: Submitting 5 missing tasks from ResultStage 4 (PythonRDD[12] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\r\n19/03/04 00:11:51 INFO YarnScheduler: Adding task set 4.0 with 5 tasks\r\n19/03/04 00:11:51 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 608, instance-2026045.ipa.ba..com, executor 1, partition 0, PROCESS_LOCAL, 7869 bytes)\r\n19/03/04 00:11:51 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 609, instance-2026037.ipa.ba..com, executor 5, partition 1, PROCESS_LOCAL, 7869 bytes)\r\n19/03/04 00:11:51 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 610, instance-2026043.ipa.ba..com, executor 6, partition 2, PROCESS_LOCAL, 7869 bytes)\r\n19/03/04 00:11:51 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 611, instance-2026046.ipa.ba..com, executor 4, partition 3, PROCESS_LOCAL, 7869 bytes)\r\n19/03/04 00:11:51 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 612, instance-2026038.ipa.ba..com, executor 2, partition 4, PROCESS_LOCAL, 7869 bytes)\r\n19/03/04 00:11:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on instance-2026038.ipa.ba..com:45476 (size: 4.9 KB, free: 153.4 GB)\r\n19/03/04 00:11:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on instance-2026046.ipa.ba..com:41346 (size: 4.9 KB, free: 153.4 GB)\r\n19/03/04 00:11:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on instance-2026043.ipa.ba..com:33242 (size: 4.9 KB, free: 153.4 GB)\r\n19/03/04 00:11:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on instance-2026037.ipa.ba..com:45022 (size: 4.9 KB, free: 153.4 GB)\r\n19/03/04 00:11:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on instance-2026045.ipa.ba..com:45485 (size: 4.9 KB, free: 153.4 GB)\r\n19/03/04 00:11:51 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 612) in 37 ms on instance-2026038.ipa.ba..com (executor 2) (1/5)\r\n19/03/04 00:11:51 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 611) in 40 ms on instance-2026046.ipa.ba..com (executor 4) (2/5)\r\n19/03/04 00:11:51 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 610) in 41 ms on instance-2026043.ipa.ba..com (executor 6) (3/5)\r\n19/03/04 00:11:51 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 609) in 42 ms on instance-2026037.ipa.ba..com (executor 5) (4/5)\r\n19/03/04 00:11:51 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 608) in 44 ms on instance-2026045.ipa.ba..com (executor 1) (5/5)\r\n19/03/04 00:11:51 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool\r\n19/03/04 00:11:51 INFO DAGScheduler: ResultStage 4 (collect at PythonRDD.scala:153) finished in 0.052 s\r\n19/03/04 00:11:51 INFO DAGScheduler: Job 4 finished: collect at PythonRDD.scala:153, took 0.054342 s\r\n2019-03-04 00:11:51,584 INFO (MainThread-75262) Shutting down cluster\r\n19/03/04 00:11:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4988571 ms on instance-2026039.ipa.ba..com (executor 3) (6/6)\r\n19/03/04 00:11:54 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool\r\n19/03/04 00:11:54 INFO DAGScheduler: ResultStage 2 (foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:301) finished in 4988.578 s\r\n19/03/04 00:11:54 INFO DAGScheduler: Job 2 finished: foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:301, took 4988.581628 s\r\n19/03/04 00:11:57 INFO SparkContext: Invoking stop() from shutdown hook\r\n19/03/04 00:11:57 INFO SparkUI: Stopped Spark web UI at http://instance-2026033.ipa.ba..com:4041\r\n19/03/04 00:11:57 INFO YarnClientSchedulerBackend: Interrupting monitor thread\r\n19/03/04 00:11:58 INFO YarnClientSchedulerBackend: Shutting down all executors\r\n19/03/04 00:11:58 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\r\n19/03/04 00:11:58 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices\r\n(serviceOption=None,\r\n services=List(),\r\n started=false)\r\n19/03/04 00:11:58 INFO YarnClientSchedulerBackend: Stopped\r\n19/03/04 00:11:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\r\n19/03/04 00:11:58 INFO MemoryStore: MemoryStore cleared\r\n19/03/04 00:11:58 INFO BlockManager: BlockManager stopped\r\n19/03/04 00:11:58 INFO BlockManagerMaster: BlockManagerMaster stopped\r\n19/03/04 00:11:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\r\n19/03/04 00:11:58 INFO SparkContext: Successfully stopped SparkContext\r\n19/03/04 00:11:58 INFO ShutdownHookManager: Shutdown hook called\r\n19/03/04 00:11:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c81098d-4f91-4a1d-87ea-71ecf1c72204\r\n19/03/04 00:11:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c81098d-4f91-4a1d-87ea-71ecf1c72204/pyspark-6a95ff4e-93cf-4c31-ac1a-002432e73cd1\r\n19/03/04 00:11:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-3d2e5cc9-f5d1-45a4-a387-50197a773614\r\n[Surya@..com@instance-2026033 ~]$\r\n\r\n```\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/397", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/397/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/397/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/397/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/397", "id": 416430184, "node_id": "MDU6SXNzdWU0MTY0MzAxODQ=", "number": 397, "title": "TypeError: unsupported operand type(s) for /: 'Dimension' and 'int'", "user": {"login": "vamsinimmala1992", "id": 25427325, "node_id": "MDQ6VXNlcjI1NDI3MzI1", "avatar_url": "https://avatars2.githubusercontent.com/u/25427325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vamsinimmala1992", "html_url": "https://github.com/vamsinimmala1992", "followers_url": "https://api.github.com/users/vamsinimmala1992/followers", "following_url": "https://api.github.com/users/vamsinimmala1992/following{/other_user}", "gists_url": "https://api.github.com/users/vamsinimmala1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/vamsinimmala1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vamsinimmala1992/subscriptions", "organizations_url": "https://api.github.com/users/vamsinimmala1992/orgs", "repos_url": "https://api.github.com/users/vamsinimmala1992/repos", "events_url": "https://api.github.com/users/vamsinimmala1992/events{/privacy}", "received_events_url": "https://api.github.com/users/vamsinimmala1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2019-03-02T19:09:26Z", "updated_at": "2019-07-03T17:39:15Z", "closed_at": "2019-07-03T17:39:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "First of all thanks for this amazing library. I am playing around with TensorflowOnSpark with a local dataset of dimensions [260000, 28047]. I am just implementing spark mode using the following  mnist [example](https://github.com/yahoo/TensorFlowOnSpark/blob/master/examples/mnist/keras/mnist_mlp.py). Below is the code that I am trying to run. I get the following exception as well, could you tell where I am getting wrong\r\n\r\n```\r\nfrom __future__ import print_function\r\n\r\n\r\ndef main_fun(args, ctx):\r\n\timport numpy\r\n\timport os\r\n\timport tensorflow as tf\r\n\tfrom tensorflow.python import keras\r\n\tfrom tensorflow.python.keras import backend as K\r\n\tfrom tensorflow.python.keras.datasets import mnist\r\n\tfrom tensorflow.python.keras.models import Sequential, load_model, save_model\r\n\tfrom tensorflow.python.keras.layers import Dense, Dropout\r\n\tfrom tensorflow.python.keras.optimizers import RMSprop\r\n\tfrom tensorflow.python.keras.callbacks import LambdaCallback, TensorBoard\r\n\tfrom tensorflow.python.saved_model import builder as saved_model_builder\r\n\tfrom tensorflow.python.saved_model import tag_constants\r\n\tfrom tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\r\n\tfrom tensorflowonspark import TFNode\r\n\t\r\n\tcluster, server = TFNode.start_cluster_server(ctx)\r\n\t\r\n\tif ctx.job_name == \"ps\":\r\n\t\tserver.join()\r\n\telif ctx.job_name == \"worker\":\r\n\t\t\r\n\t\tdef generate_rdd_data(tf_feed, batch_size):\r\n\t\t\tprint(\"generate_rdd_data invoked\")\r\n\t\t\twhile True:\r\n\t\t\t\tbatch = tf_feed.next_batch(batch_size)\r\n\t\t\t\tfeature_vector = []\r\n\t\t\t\tlbls = []\r\n\t\t\t\tfor item in batch:\r\n\t\t\t\t\tfeature_vector.append(item[0])\r\n\t\t\t\t\tlbls.append(item[1])\r\n\t\t\t\tfeatures = numpy.array(feature_vector).astype('float32')\r\n\t\t\t\tlabels = numpy.stack(lbls).astype('float32')\r\n\t\t\t\tyield (features, labels)\r\n\t\t\r\n\t\twith tf.device(tf.train.replica_device_setter(\r\n\t\t  worker_device=\"/job:worker/task:%d\" % ctx.task_index,\r\n\t\t  cluster=cluster)):\r\n\t\t\t\r\n\t\t\tbatch_size = 100\r\n\t\t\tnum_classes = 13\r\n\t\t\t# args.mode == 'spark':\r\n\t\t\tx_train = tf.placeholder(tf.float32, [None, 28047], name=\"x_train\")\r\n\t\t\ty_train = tf.placeholder(tf.float32, [None, 13], name=\"y_train\")\r\n\t\t\t\r\n\t\t\tmodel = Sequential()\r\n\t\t\tmodel.add(Dense(512, activation='relu', input_shape=(28047,)))\r\n\t\t\tmodel.add(Dropout(0.2))\r\n\t\t\tmodel.add(Dense(512, activation='relu'))\r\n\t\t\tmodel.add(Dropout(0.2))\r\n\t\t\tmodel.add(Dense(13, activation='softmax'))\r\n\t\t\t\r\n\t\t\tmodel.summary()\r\n\t\t\t\r\n\t\t\tmodel.compile(loss='categorical_crossentropy',\r\n\t\t\t              optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001),\r\n\t\t\t              metrics=['accuracy'])\r\n\t\t\r\n\t\tsaver = tf.train.Saver()\r\n\t\t\r\n\t\twith tf.Session(server.target) as sess:\r\n\t\t\tK.set_session(sess)\r\n\t\t\t\r\n\t\t\tdef save_checkpoint(epoch, logs=None):\r\n\t\t\t\tif epoch == 1:\r\n\t\t\t\t\ttf.train.write_graph(sess.graph.as_graph_def(), args.model_dir, 'graph.pbtxt')\r\n\t\t\t\tsaver.save(sess, os.path.join(args.model_dir, 'model.ckpt'), global_step=epoch * args.steps_per_epoch)\r\n\t\t\t\r\n\t\t\tckpt_callback = LambdaCallback(on_epoch_end=save_checkpoint)\r\n\t\t\ttb_callback = TensorBoard(log_dir=args.model_dir, histogram_freq=1, write_graph=True, write_images=True)\r\n\t\t\t\r\n\t\t\t# Add callbacks to save model checkpoint and tensorboard events (on worker:0 only)\r\n\t\t\tcallbacks = [ckpt_callback, tb_callback] if ctx.task_index == 0 else None\r\n\t\t\t\r\n\t\t\t# args.input_mode == 'spark':\r\n\t\t\t#  train on data read from a generator which is producing data from a Spark RDD\r\n\t\t\ttf_feed = TFNode.DataFeed(ctx.mgr)\r\n\t\t\tmodel.fit_generator(generator=generate_rdd_data(tf_feed, batch_size),\r\n\t\t\t                    steps_per_epoch=args.steps_per_epoch,\r\n\t\t\t                    epochs=args.epochs,\r\n\t\t\t                    verbose=1,\r\n\t\t\t                    validation_data=(x_train, y_train),\r\n\t\t\t                    callbacks=callbacks)\r\n\t\t\t\r\n\t\t\tif args.export_dir and ctx.job_name == 'worker' and ctx.task_index == 0:\r\n\t\t\t\t# save a local Keras model, so we can reload it with an inferencing learning_phase\r\n\t\t\t\tsave_model(model, \"tmp_model\")\r\n\t\t\t\t\r\n\t\t\t\t# reload the model\r\n\t\t\t\tK.set_learning_phase(False)\r\n\t\t\t\tnew_model = load_model(\"tmp_model\")\r\n\t\t\t\t\r\n\t\t\t\t# export a saved_model for inferencing\r\n\t\t\t\tbuilder = saved_model_builder.SavedModelBuilder(args.export_dir)\r\n\t\t\t\tsignature = predict_signature_def(inputs={'fetures': new_model.input},\r\n\t\t\t\t                                  outputs={'scores': new_model.output})\r\n\t\t\t\tbuilder.add_meta_graph_and_variables(sess=sess,\r\n\t\t\t\t                                     tags=[tag_constants.SERVING],\r\n\t\t\t\t                                     signature_def_map={'predict': signature},\r\n\t\t\t\t                                     clear_devices=True)\r\n\t\t\t\tbuilder.save()\r\n\t\t\t\r\n\t\t\tif args.input_mode == 'spark':\r\n\t\t\t\ttf_feed.terminate()\r\n\r\n\r\nif __name__ == '__main__':\r\n\timport argparse\r\n\tfrom pyspark.context import SparkContext\r\n\tfrom pyspark.conf import SparkConf\r\n\tfrom tensorflowonspark import TFCluster\r\n\timport keras\r\n\t\r\n\tsc = SparkContext(conf=SparkConf().setAppName(\"PhaseOneModelling\"))\r\n\texecutors = sc._conf.get(\"spark.executor.instances\")\r\n\tnum_executors = int(executors) if executors is not None else 1\r\n\tnum_ps = 1\r\n\t\r\n\tparser = argparse.ArgumentParser()\r\n\tparser.add_argument(\"--cluster_size\", help=\"number of nodes in the cluster\", type=int, default=num_executors)\r\n\tparser.add_argument(\"--epochs\", help=\"number of epochs of training data\", type=int, default=20)\r\n\tparser.add_argument(\"--export_dir\", help=\"directory to export saved_model\")\r\n\tparser.add_argument(\"--data\", help=\"HDFS path to data in parallelized CSV format\")\r\n\t# parser.add_argument(\"--input_mode\", help=\"input mode (tf|spark)\", default=\"tf\")\r\n\tparser.add_argument(\"--labels\", help=\"HDFS path to MNIST labels in parallelized CSV format\")\r\n\tparser.add_argument(\"--model_dir\", help=\"directory to write model checkpoints\")\r\n\tparser.add_argument(\"--num_ps\", help=\"number of ps nodes\", type=int, default=1)\r\n\tparser.add_argument(\"--steps_per_epoch\", help=\"number of steps per epoch\", type=int, default=300)\r\n\tparser.add_argument(\"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\r\n\t\r\n\targs = parser.parse_args()\r\n\tprint(\"args:\", args)\r\n\t\r\n\tdata = sc.textFile(args.data)\r\n\tdata = data.map(lambda l: l.encode(\"UTF8\", \"ignore\").split('\\t'))\r\n\t\r\n\tlabels = data.map(lambda x: x[1])\r\n\tdata = data.map(lambda x: x[19:28066])\r\n\t\r\n\theader = data.first()\r\n\tdata = data.filter(lambda line: line != header)\r\n\tlabel_header = labels.first()\r\n\tlabels = labels.filter(lambda line: line != label_header)\r\n\t\r\n\t# convert values to float\r\n\tconvertToFloat = lambda data: [float(str(x)) for x in data]\r\n\tdataset = data.map(convertToFloat)\r\n\tlabels = labels.map(lambda x: float(x))\r\n\tlabels = labels.map(lambda x: keras.utils.to_categorical(x, num_classes=13))\r\n\t\r\n\t# Split the data for train and validation\r\n\t#testRDD, trainRDD = data.randomSplit(weights=[0.001, 0.999], seed=42)\r\n\t#testlabelRDD, trainlabelRDD = labels.randomSplit(weights=[0.001, 0.999], seed=42)\r\n\t\r\n\tdataRDD = dataset.zip(labels)\r\n\t\r\n\tcluster = TFCluster.run(sc, main_fun, args, args.cluster_size, args.num_ps, args.tensorboard,\r\n\t                        TFCluster.InputMode.SPARK, log_dir=args.model_dir)\r\n\tcluster.train(dataRDDRDD, args.epochs)\r\n\r\n\tcluster.shutdown()\r\n\r\n```\r\n\r\n\r\nAnd the Traceback as follows:\r\n\r\n```\r\n  File \"/hadoop1/yarn/local/usercache/851519@ba.ad.ssa.gov/appcache/application_1551114784635_0164/container_e51_1551114784635_0164_01_000006/pyspark.zip/pyspark/worker.py\", line 229, in main\r\n    process()\r\n  File \"/hadoop1/yarn/local/usercache/851519@ba.ad.ssa.gov/appcache/application_1551114784635_0164/container_e51_1551114784635_0164_01_000006/pyspark.zip/pyspark/worker.py\", line 224, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/hdp/2.6.5.0-292/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/2.6.5.0-292/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/2.6.5.0-292/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\r\n  File \"/usr/hdp/2.6.5.0-292/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 362, in func\r\n  File \"/usr/hdp/2.6.5.0-292/spark2/python/lib/pyspark.zip/pyspark/rdd.py\", line 809, in func\r\n  File \"/usr/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 416, in _train\r\n    raise Exception(\"exception in worker:\\n\" + e_str)\r\nException: exception in worker:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 329, in wrapper_fn_background\r\n    wrapper_fn(args, context)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflowonspark/TFSparkNode.py\", line 323, in wrapper_fn\r\n    fn(args, context)\r\n  File \"/home/ba.ad.ssa.gov/851519/new_keras_model.py\", line 86, in main_fun\r\n    callbacks=callbacks)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 2177, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 137, in fit_generator\r\n    callbacks.on_train_begin()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/keras/callbacks.py\", line 266, in on_train_begin\r\n    callback.on_train_begin(logs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1063, in on_train_begin\r\n    self.validation_data[0].shape[0] / self.batch_size)\r\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'int'\r\n\r\n```\r\n\t\r\nCould you point me where i am getting wrong", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/394", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/394/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/394/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/394/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/394", "id": 411415415, "node_id": "MDU6SXNzdWU0MTE0MTU0MTU=", "number": 394, "title": "Wide-deep example shutting down", "user": {"login": "ksy3395", "id": 22565367, "node_id": "MDQ6VXNlcjIyNTY1MzY3", "avatar_url": "https://avatars0.githubusercontent.com/u/22565367?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ksy3395", "html_url": "https://github.com/ksy3395", "followers_url": "https://api.github.com/users/ksy3395/followers", "following_url": "https://api.github.com/users/ksy3395/following{/other_user}", "gists_url": "https://api.github.com/users/ksy3395/gists{/gist_id}", "starred_url": "https://api.github.com/users/ksy3395/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ksy3395/subscriptions", "organizations_url": "https://api.github.com/users/ksy3395/orgs", "repos_url": "https://api.github.com/users/ksy3395/repos", "events_url": "https://api.github.com/users/ksy3395/events{/privacy}", "received_events_url": "https://api.github.com/users/ksy3395/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2019-02-18T10:51:03Z", "updated_at": "2019-04-05T17:47:02Z", "closed_at": "2019-04-05T17:47:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello,\r\nI am trying to run Wide-deep example with Spark standalone cluster on my Mac and also on a ubuntu machine separately but it keeps shutting down the spark job, so I can't check anything since there is no error or log. I tried mnist stanalone example, but I didn't have any problem on these machines. Is there anything else I can check if there is anything wrong?\r\n\r\nCommand:\r\n${SPARK_HOME}/bin/spark-submit --master spark://192.168.1.9:7077 --py-files census_dataset.py, wide_deep_run_loop.py --conf spark.cores.max=3 --conf spark.task.cpus=1 --conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" --conf spark.task.maxFailures=1 --conf spark.stage.maxConsecutiveAttempts=1 census_main.py --cluster_size 3\r\n\r\n2019-02-18 01:55:26 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2019-02-18 01:55:28 INFO  ShutdownHookManager:54 - Shutdown hook called\r\n2019-02-18 01:55:28 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/98/34ltsy7x4f5btv6bpt6d75xm0000gn/T/spark-7cf40b10-9420-4680-8059-fed7d4ae4930\r\n2019-02-18 01:55:28 INFO  ShutdownHookManager:54 - Deleting directory /private/var/folders/98/34ltsy7x4f5btv6bpt6d75xm0000gn/T/localPyFiles-75adab00-d652-47e4-92c5-b12dcb07ca99\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/391", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/391/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/391/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/391/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/391", "id": 402555198, "node_id": "MDU6SXNzdWU0MDI1NTUxOTg=", "number": 391, "title": "TFSparkNode throws AttributeError on shutdown", "user": {"login": "manuzhang", "id": 1191767, "node_id": "MDQ6VXNlcjExOTE3Njc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1191767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manuzhang", "html_url": "https://github.com/manuzhang", "followers_url": "https://api.github.com/users/manuzhang/followers", "following_url": "https://api.github.com/users/manuzhang/following{/other_user}", "gists_url": "https://api.github.com/users/manuzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/manuzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manuzhang/subscriptions", "organizations_url": "https://api.github.com/users/manuzhang/orgs", "repos_url": "https://api.github.com/users/manuzhang/repos", "events_url": "https://api.github.com/users/manuzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/manuzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2019-01-24T06:01:50Z", "updated_at": "2019-02-11T07:48:48Z", "closed_at": "2019-02-11T07:48:48Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.3.1\r\n - TensorFlow version 1.7.0\r\n - TensorFlowOnSpark version 1.4.2\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nTFSparkNode throws AttributeError on shutdown\r\n\r\n**Logs:**\r\n```\r\n2019-01-24 12:02:02,297 INFO (MainThread-13745) Feeding None into input queue\r\n[2019-01-24 12:02:02.344] [ERROR] [Executor task launch worker for task 2] [org.apache.spark.executor.Executor] >>> [spark-] msg=Exception in task 0.0 in stage 1.0 (TID 2)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 539, in _shutdown\r\nAttributeError: 'AutoProxy[get_queue]' object has no attribute 'put'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 234, in main\r\n    process()\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2457, in pipeline_func\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2457, in pipeline_func\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2457, in pipeline_func\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 370, in func\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 819, in func\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 542, in _shutdown\r\nException: Queue 'input' not found on this node, check for exceptions on other nodes.\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\n**Spark Submit Command Line:**\r\n`spark-submit --py-files mnist_estimator.py mnist_estimator.py`", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/386", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/386/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/386/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/386/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/386", "id": 399677701, "node_id": "MDU6SXNzdWUzOTk2Nzc3MDE=", "number": 386, "title": "ValueError: If \"cluster\" is set in TF_CONFIG, it must have one \"chief\" node.", "user": {"login": "manuzhang", "id": 1191767, "node_id": "MDQ6VXNlcjExOTE3Njc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1191767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manuzhang", "html_url": "https://github.com/manuzhang", "followers_url": "https://api.github.com/users/manuzhang/followers", "following_url": "https://api.github.com/users/manuzhang/following{/other_user}", "gists_url": "https://api.github.com/users/manuzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/manuzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manuzhang/subscriptions", "organizations_url": "https://api.github.com/users/manuzhang/orgs", "repos_url": "https://api.github.com/users/manuzhang/repos", "events_url": "https://api.github.com/users/manuzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/manuzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2019-01-16T07:19:25Z", "updated_at": "2019-01-21T06:31:20Z", "closed_at": "2019-01-21T06:31:20Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.6\r\n - Spark version 2.3.1\r\n - TensorFlow version 1.7.0\r\n - TensorFlowOnSpark version 1.3.2\r\n - Cluster version Standalone\r\n\r\n**Describe the bug:**\r\nThere is no `chief` node in `TF_CONFIG`\r\n\r\n**Logs:**\r\n```\r\n[2019-01-09 17:07:15.496] [ERROR] [Executor task launch worker for task 7] [org.apache.spark.executor.Executor] >>> [spark-] msg=Exception in task 7.0 in stage 0.0 (TID 7)\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 234, in main\r\n    process()\r\n  File \"/home/vipshop/platform/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/home/vipshop/platform/spark/python/pyspark/rdd.py\", line 2457, in pipeline_func\r\n    return func(split, prev_func(split, iterator))\r\n  File \"/home/vipshop/platform/spark/python/pyspark/rdd.py\", line 2457, in pipeline_func\r\n    return func(split, prev_func(split, iterator))\r\n  File \"/home/vipshop/platform/spark/python/pyspark/rdd.py\", line 2457, in pipeline_func\r\n    return func(split, prev_func(split, iterator))\r\n  File \"/home/vipshop/platform/spark/python/pyspark/rdd.py\", line 370, in func\r\n    return f(iterator)\r\n  File \"/home/vipshop/platform/spark/python/pyspark/rdd.py\", line 819, in func\r\n    r = f(it)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 353, in _mapfn\r\n    wrapper_fn(tf_args, ctx)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFSparkNode.py\", line 310, in wrapper_fn\r\n    fn(args, context)\r\n  File \"<ipython-input-2-858eadfad2b6>\", line 51, in train\r\n  File \"<ipython-input-2-858eadfad2b6>\", line 9, in build_estimator\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/estimator/run_config.py\", line 465, in __init__\r\n    self._init_distributed_setting_from_environment_var(tf_config)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/estimator/run_config.py\", line 481, in _init_distributed_setting_from_environment_var\r\n    self._cluster_spec, task_env, TaskType.CHIEF)\r\n  File \"/home/mlp/.local/lib/python3.6/site-packages/tensorflow/python/estimator/run_config.py\", line 154, in _validate_task_type_and_task_id\r\n    chief_task_type)\r\nValueError: If \"cluster\" is set in TF_CONFIG, it must have one \"chief\" node.\r\n```", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/383", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/383/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/383/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/383/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/383", "id": 395173834, "node_id": "MDU6SXNzdWUzOTUxNzM4MzQ=", "number": 383, "title": "TensorFlowOnSpark with Hadoop 3.1.0 and issues in waiting for reservations", "user": {"login": "yang3808282", "id": 40430624, "node_id": "MDQ6VXNlcjQwNDMwNjI0", "avatar_url": "https://avatars1.githubusercontent.com/u/40430624?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yang3808282", "html_url": "https://github.com/yang3808282", "followers_url": "https://api.github.com/users/yang3808282/followers", "following_url": "https://api.github.com/users/yang3808282/following{/other_user}", "gists_url": "https://api.github.com/users/yang3808282/gists{/gist_id}", "starred_url": "https://api.github.com/users/yang3808282/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yang3808282/subscriptions", "organizations_url": "https://api.github.com/users/yang3808282/orgs", "repos_url": "https://api.github.com/users/yang3808282/repos", "events_url": "https://api.github.com/users/yang3808282/events{/privacy}", "received_events_url": "https://api.github.com/users/yang3808282/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2019-01-02T08:09:10Z", "updated_at": "2019-08-16T23:17:19Z", "closed_at": "2019-01-10T00:49:01Z", "author_association": "NONE", "active_lock_reason": null, "body": "I meet a problem about one week,nobody replay me........................................", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/382", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/382/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/382/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/382/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/382", "id": 394752680, "node_id": "MDU6SXNzdWUzOTQ3NTI2ODA=", "number": 382, "title": "\u6211\u60f3\u7528\u5728hadoop3.1.0\u4e0a\u9762", "user": {"login": "yang3808282", "id": 40430624, "node_id": "MDQ6VXNlcjQwNDMwNjI0", "avatar_url": "https://avatars1.githubusercontent.com/u/40430624?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yang3808282", "html_url": "https://github.com/yang3808282", "followers_url": "https://api.github.com/users/yang3808282/followers", "following_url": "https://api.github.com/users/yang3808282/following{/other_user}", "gists_url": "https://api.github.com/users/yang3808282/gists{/gist_id}", "starred_url": "https://api.github.com/users/yang3808282/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yang3808282/subscriptions", "organizations_url": "https://api.github.com/users/yang3808282/orgs", "repos_url": "https://api.github.com/users/yang3808282/repos", "events_url": "https://api.github.com/users/yang3808282/events{/privacy}", "received_events_url": "https://api.github.com/users/yang3808282/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-12-29T02:30:08Z", "updated_at": "2019-03-01T18:30:02Z", "closed_at": "2019-01-10T00:49:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "\u770bpom\u6587\u4ef6\u652f\u6301\u7684hadoop\u7248\u672c\u662f2.7\u7248\u672c\uff0c\u6211\u4e4b\u524d\u5728hadoop2.7\u7248\u672c\u8dd1\u6210\u529f\u4e86\uff0c\u73b0\u5728\u60f3\u7528gpu\uff0c\u6240\u4ee5\u60f3\u8dd1\u5728hadoop3.1.0\u7248\u672c\u4e0a\u9762\uff0c\u6c42\u6307\u5bfc\u8be5\u5982\u4f55\u53bb\u505a\uff0c\u4fee\u6539\u6e90\u7801\u600e\u4e48\u4fee\u6539\uff0c\u6216\u8005\u6709\u6ca1\u6709\u5f00\u6e90\u7684branch\u5171\u4eab\u4e00\u4e0b\u8c22\u8c22", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/381", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/381/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/381/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/381/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/381", "id": 393205278, "node_id": "MDU6SXNzdWUzOTMyMDUyNzg=", "number": 381, "title": "Exception while downloading the data and second exception reading the data from HDFS stored as CSV with TFOS keras example", "user": {"login": "SuryaVamsi1992", "id": 38590523, "node_id": "MDQ6VXNlcjM4NTkwNTIz", "avatar_url": "https://avatars2.githubusercontent.com/u/38590523?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SuryaVamsi1992", "html_url": "https://github.com/SuryaVamsi1992", "followers_url": "https://api.github.com/users/SuryaVamsi1992/followers", "following_url": "https://api.github.com/users/SuryaVamsi1992/following{/other_user}", "gists_url": "https://api.github.com/users/SuryaVamsi1992/gists{/gist_id}", "starred_url": "https://api.github.com/users/SuryaVamsi1992/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SuryaVamsi1992/subscriptions", "organizations_url": "https://api.github.com/users/SuryaVamsi1992/orgs", "repos_url": "https://api.github.com/users/SuryaVamsi1992/repos", "events_url": "https://api.github.com/users/SuryaVamsi1992/events{/privacy}", "received_events_url": "https://api.github.com/users/SuryaVamsi1992/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2018-12-20T20:45:35Z", "updated_at": "2019-01-24T22:20:56Z", "closed_at": "2019-01-24T22:20:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "I am trying to test TensorflowOnspark on my apache cluster. I have some network issues on my end.\r\nSo I even tried downloading mnist.npz file and stored in .keras/datasets/ folder and \r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nthis line break the total execution.\r\n\r\nsample traceback:\r\n\r\n\r\n18/12/20 15:32:32 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 23, cl-rhp-2026044.ipa.ba.ssa.gov, executor 6, partition 0, PROCESS_LOCAL, 7869 bytes)\r\n18/12/20 15:32:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on cl-rhp-2026044.ipa.ba.ssa.gov:42173 (size: 5.9 KB, free: 143.8 GB)\r\n18/12/20 15:32:32 INFO TaskSetManager: Finished task 0.1 in stage 1.0 (TID 23) in 175 ms on cl-rhp-2026044.ipa.ba.ssa.gov (executor 6) (7/7)\r\n18/12/20 15:32:32 INFO DAGScheduler: ResultStage 1 (foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:174) finished in 0.659 s\r\n18/12/20 15:32:32 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool\r\n18/12/20 15:32:32 INFO DAGScheduler: Job 1 finished: foreachPartition at /usr/lib/python2.7/site-packages/tensorflowonspark/TFCluster.py:174, took 0.664504 s\r\n2018-12-20 15:32:32,997 ERROR (MainThread-73825) Exiting Spark application with error status.\r\n18/12/20 15:32:33 INFO SparkUI: Stopped Spark web UI at ........................\r\n18/12/20 15:32:33 INFO YarnClientSchedulerBackend: Interrupting monitor thread\r\n18/12/20 15:32:33 INFO YarnClientSchedulerBackend: Shutting down all executors\r\n18/12/20 15:32:33 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\r\n18/12/20 15:32:33 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices\r\n(serviceOption=None,\r\n services=List(),\r\n started=false)\r\n18/12/20 15:32:33 INFO YarnClientSchedulerBackend: Stopped\r\n18/12/20 15:32:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\r\n18/12/20 15:32:33 INFO MemoryStore: MemoryStore cleared\r\n18/12/20 15:32:33 INFO BlockManager: BlockManager stopped\r\n18/12/20 15:32:33 INFO BlockManagerMaster: BlockManagerMaster stopped\r\n18/12/20 15:32:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\r\n18/12/20 15:32:33 INFO SparkContext: Successfully stopped SparkContext\r\n18/12/20 15:32:34 INFO ShutdownHookManager: Shutdown hook called\r\n18/12/20 15:32:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca0304c4-5be4-4f26-a379-af1015276025/pyspark-733248c5-a3e8-49f0-a172-f617b1241454\r\n18/12/20 15:32:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f16eab9-134d-4dba-9ad0-1e9da541b31a\r\n18/12/20 15:32:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca0304c4-5be4-4f26-a379-af1015276025\r\n[851519@ ~]$ Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [Errno 101] Network is unreachable\r\n-bash: Exception:: command not found\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/380", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/380/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/380/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/380/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/380", "id": 392920603, "node_id": "MDU6SXNzdWUzOTI5MjA2MDM=", "number": 380, "title": "where to find training outputs?", "user": {"login": "canocgithub", "id": 46017621, "node_id": "MDQ6VXNlcjQ2MDE3NjIx", "avatar_url": "https://avatars3.githubusercontent.com/u/46017621?v=4", "gravatar_id": "", "url": "https://api.github.com/users/canocgithub", "html_url": "https://github.com/canocgithub", "followers_url": "https://api.github.com/users/canocgithub/followers", "following_url": "https://api.github.com/users/canocgithub/following{/other_user}", "gists_url": "https://api.github.com/users/canocgithub/gists{/gist_id}", "starred_url": "https://api.github.com/users/canocgithub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/canocgithub/subscriptions", "organizations_url": "https://api.github.com/users/canocgithub/orgs", "repos_url": "https://api.github.com/users/canocgithub/repos", "events_url": "https://api.github.com/users/canocgithub/events{/privacy}", "received_events_url": "https://api.github.com/users/canocgithub/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-12-20T07:20:26Z", "updated_at": "2019-01-07T18:45:56Z", "closed_at": "2019-01-07T18:45:56Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Can't find any useful output of TensorFlowOnSpark/examples/mnist/tf/mnist_dist.py.**\r\ne.g.  156/157\r\n     if (step % 100 == 0):\r\n            print(\"{} step: {} accuracy: {}\".format(datetime.now().isoformat(), step, sess.run(accuracy)))\r\n\r\n**Spark Submit Command Line:**\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--queue default \\\r\n--num-executors 2 \\\r\n--executor-memory 4g \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/tf/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.yarn.executor.memoryOverhead=1g \\\r\n--archives hdfs:///user/${USER}/Python.zip#Python \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nTensorFlowOnSpark/examples/mnist/tf/mnist_spark.py \\\r\n--images_labels mnist/tfr/train \\\r\n--format tfr \\\r\n--mode train \\\r\n--model mnist_model", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/376", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/376/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/376/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/376/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/376", "id": 382934205, "node_id": "MDU6SXNzdWUzODI5MzQyMDU=", "number": 376, "title": "can I change tr_loss to mse loss in FieldAwaredFactorizationMachine.scala file 306 line for solve regereesion problem ?", "user": {"login": "risk2", "id": 37725244, "node_id": "MDQ6VXNlcjM3NzI1MjQ0", "avatar_url": "https://avatars3.githubusercontent.com/u/37725244?v=4", "gravatar_id": "", "url": "https://api.github.com/users/risk2", "html_url": "https://github.com/risk2", "followers_url": "https://api.github.com/users/risk2/followers", "following_url": "https://api.github.com/users/risk2/following{/other_user}", "gists_url": "https://api.github.com/users/risk2/gists{/gist_id}", "starred_url": "https://api.github.com/users/risk2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/risk2/subscriptions", "organizations_url": "https://api.github.com/users/risk2/orgs", "repos_url": "https://api.github.com/users/risk2/repos", "events_url": "https://api.github.com/users/risk2/events{/privacy}", "received_events_url": "https://api.github.com/users/risk2/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-21T02:57:49Z", "updated_at": "2018-11-27T23:55:39Z", "closed_at": "2018-11-27T23:55:39Z", "author_association": "NONE", "active_lock_reason": null, "body": "1, can I change tr_loss to mse loss in FieldAwaredFactorizationMachine.scala file 306 line for solve regereesion problem ?\r\n2\uff0cwhat does kappa mean in FieldAwaredFactorizationMachine.scala file 307 line \uff1f", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/375", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/375/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/375/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/375/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/375", "id": 381858615, "node_id": "MDU6SXNzdWUzODE4NTg2MTU=", "number": 375, "title": "tensorflow.python.framework.errors_impl.PermissionDeniedError: /TensorFlowOnSpark; Permission denied", "user": {"login": "Lihengwannafly", "id": 24928240, "node_id": "MDQ6VXNlcjI0OTI4MjQw", "avatar_url": "https://avatars3.githubusercontent.com/u/24928240?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lihengwannafly", "html_url": "https://github.com/Lihengwannafly", "followers_url": "https://api.github.com/users/Lihengwannafly/followers", "following_url": "https://api.github.com/users/Lihengwannafly/following{/other_user}", "gists_url": "https://api.github.com/users/Lihengwannafly/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lihengwannafly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lihengwannafly/subscriptions", "organizations_url": "https://api.github.com/users/Lihengwannafly/orgs", "repos_url": "https://api.github.com/users/Lihengwannafly/repos", "events_url": "https://api.github.com/users/Lihengwannafly/events{/privacy}", "received_events_url": "https://api.github.com/users/Lihengwannafly/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-17T12:26:47Z", "updated_at": "2019-01-07T20:12:35Z", "closed_at": "2019-01-07T20:12:35Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [2.3.1]\r\n - TensorFlow version [e.g. 1.10.0]\r\n - TensorFlowOnSpark version [1.3.1]\r\n - Cluster version [ HDP2.6.2]\r\n\r\nDescribe the bug:\r\nthe url is: https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_YARN\r\nI run Convert the MNIST zip files into HDFS files from GetStarted_YARN is ok.\r\nWhen I run distributed MNIST training (using feed_dict) from GetStarted_YARN, it gets hdfs permission denied error:\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: /TensorFlowOnSpark; Permission denied\r\n\r\nEnvironment\uff1a\r\nexport PYTHON_ROOT=/opt/rh/rh-python36/root\r\nexport LD_LIBRARY_PATH=/usr/hdp/2.6.2.14-5/hadoop/lib/native\r\nexport PYSPARK_PYTHON=${PYTHON_ROOT}/bin/python3\r\nexport PYSPARK_DRIVER_PYTHON=${PYTHON_ROOT}/bin/python3\r\nexport SPARK_YARN_USER_ENV=\"PYSPARK_PYTHON=/opt/rh/rh-python36/root/bin/python3\"  \r\nexport PATH=${PYTHON_ROOT}/bin/:/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin \r\nexport HADOOP_CONF_DIR=/usr/hdp/2.6.2.14-5/hadoop/etc/hadoop \r\nexport YARN_CONF_DIR=/usr/hdp/2.6.2.14-5/hadoop-yarn/etc/hadoop \r\nexport QUEUE=default\r\nexport LIB_HDFS=/usr/hdp/2.6.2.14-5/usr/lib              # path to libhdfs.so, for TF acccess to HDFS \r\nexport LIB_JVM=/usr/java/jdk1.8.0_191-amd64/jre/lib/amd64/server/\r\n\r\nSpark Submit Command Line:\r\nspark-submit \\\r\n--master yarn \\\r\n--num-executors 2 \\\r\n--executor-memory 8G \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nTensorFlowOnSpark/examples/mnist/keras/mnist_mlp.py \\\r\n--input_mode spark \\\r\n--images /TensorFlowOnSpark/mnist/csv/train/images \\\r\n--labels /TensorFlowOnSpark/mnist/csv/train/labels \\\r\n--epochs 1 \\\r\n--mode train \\\r\n--model  /TensorFlowOnSpark/model\r\n\r\nHDFS:\r\n/TensorFlowOnSpark\uff1a\r\ndrwxrwxrwx | root | hdfs | 0 B | 2018/11/17 \u4e0b\u53488:06:27 | 0 | 0 B | TensorFlowOnSpark\r\n\r\n/TensorFlowOnSpark/model:\r\ndrwxrwxrwx | root | hdfs | 0 B | 2018/11/17 \u4e0b\u53487:20:40 | 0 | 0 B | model\r\n\r\n\r\n\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/371", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/371/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/371/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/371/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/371", "id": 380084610, "node_id": "MDU6SXNzdWUzODAwODQ2MTA=", "number": 371, "title": "pyspark", "user": {"login": "jinyuhang", "id": 7940792, "node_id": "MDQ6VXNlcjc5NDA3OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7940792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jinyuhang", "html_url": "https://github.com/jinyuhang", "followers_url": "https://api.github.com/users/jinyuhang/followers", "following_url": "https://api.github.com/users/jinyuhang/following{/other_user}", "gists_url": "https://api.github.com/users/jinyuhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/jinyuhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jinyuhang/subscriptions", "organizations_url": "https://api.github.com/users/jinyuhang/orgs", "repos_url": "https://api.github.com/users/jinyuhang/repos", "events_url": "https://api.github.com/users/jinyuhang/events{/privacy}", "received_events_url": "https://api.github.com/users/jinyuhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-13T06:56:32Z", "updated_at": "2019-01-07T20:12:26Z", "closed_at": "2019-01-07T20:12:26Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [ 2.7,]\r\n - Spark version [2.1,]\r\n - TensorFlow version [1.11]\r\n - TensorFlowOnSpark version [1.2]\r\n - Cluster version [Hadoop 2.7 ]\r\n\r\n**Describe the bug:**\r\nSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1542026212173_0007_02_000012 on host: slave. Exit status: -1000. Diagnostics: java.io.IOException: Resource file:/opt/big_data/spark/python/lib/pyspark.zip changed on\r\n\r\n**Logs:**\r\n\r\n\r\n**Spark Submit Command Line:**\r\nIf applicable, add your spark-submit command line.\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/370", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/370/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/370/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/370/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/370", "id": 379930840, "node_id": "MDU6SXNzdWUzNzk5MzA4NDA=", "number": 370, "title": "ValueError: Parent directory of mnist_model/model.ckpt doesn't exist, can't save.", "user": {"login": "roshanp", "id": 1636750, "node_id": "MDQ6VXNlcjE2MzY3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1636750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/roshanp", "html_url": "https://github.com/roshanp", "followers_url": "https://api.github.com/users/roshanp/followers", "following_url": "https://api.github.com/users/roshanp/following{/other_user}", "gists_url": "https://api.github.com/users/roshanp/gists{/gist_id}", "starred_url": "https://api.github.com/users/roshanp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/roshanp/subscriptions", "organizations_url": "https://api.github.com/users/roshanp/orgs", "repos_url": "https://api.github.com/users/roshanp/repos", "events_url": "https://api.github.com/users/roshanp/events{/privacy}", "received_events_url": "https://api.github.com/users/roshanp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-12T20:01:06Z", "updated_at": "2018-11-14T00:25:13Z", "closed_at": "2018-11-14T00:25:13Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version 3.5.5\r\n - Spark version 2.1\r\n - TensorFlow version 1.10\r\n - TensorFlowOnSpark version 1.3.4\r\n - Cluster version Yarn Hadoop 2.7.3\r\n\r\n**Describe the bug:**\r\nSeems like the keras version is not writing to HDFS, but I can't be sure. Any ideas?\r\n\r\n**Logs:**\r\nFrom the spark logs I get `ValueError: Parent directory of mnist_model/model.ckpt doesn't exist, can't save.`\r\n\r\n**Spark Submit Command Line:**\r\n```\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master yarn \\\r\n--deploy-mode cluster \\\r\n--num-executors 2 \\\r\n--executor-memory 4G \\\r\n--py-files TensorFlowOnSpark/tfspark.zip,TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.dynamicAllocation.enabled=false \\\r\n--conf spark.yarn.maxAppAttempts=1 \\\r\n--archives hdfs:///user/admin/py-3.5.5-ml.zip#Python \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS \\\r\nTensorFlowOnSpark/examples/mnist/keras/mnist_mlp.py \\\r\n--input_mode spark \\\r\n--images mnist/csv/train/images \\\r\n--labels mnist/csv/train/labels \\\r\n--mode train \\\r\n--epochs 5 \\\r\n--model_dir mnist_model \\\r\n--export_dir mnist_export\r\n```\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/369", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/369/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/369/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/369/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/369", "id": 379601832, "node_id": "MDU6SXNzdWUzNzk2MDE4MzI=", "number": 369, "title": "How to use HDFS file path in estimator example", "user": {"login": "kyauaa", "id": 32992638, "node_id": "MDQ6VXNlcjMyOTkyNjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/32992638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kyauaa", "html_url": "https://github.com/kyauaa", "followers_url": "https://api.github.com/users/kyauaa/followers", "following_url": "https://api.github.com/users/kyauaa/following{/other_user}", "gists_url": "https://api.github.com/users/kyauaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/kyauaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kyauaa/subscriptions", "organizations_url": "https://api.github.com/users/kyauaa/orgs", "repos_url": "https://api.github.com/users/kyauaa/repos", "events_url": "https://api.github.com/users/kyauaa/events{/privacy}", "received_events_url": "https://api.github.com/users/kyauaa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-12T02:57:42Z", "updated_at": "2018-11-14T06:20:55Z", "closed_at": "2018-11-14T06:20:55Z", "author_association": "NONE", "active_lock_reason": null, "body": "Hello, I am studying estimator example.\r\nTensorFlowOnSpark/examples/mnist/estimator\r\nI noticed that in this example, it use local file system instead of hdfs. How can I modify to force it use hdfs so that I can run this program in distributed spark cluster?\r\nThank you!", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/366", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/366/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/366/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/366/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/366", "id": 378984381, "node_id": "MDU6SXNzdWUzNzg5ODQzODE=", "number": 366, "title": "How to change it to TENSORFLOW inputmode?", "user": {"login": "kyauaa", "id": 32992638, "node_id": "MDQ6VXNlcjMyOTkyNjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/32992638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kyauaa", "html_url": "https://github.com/kyauaa", "followers_url": "https://api.github.com/users/kyauaa/followers", "following_url": "https://api.github.com/users/kyauaa/following{/other_user}", "gists_url": "https://api.github.com/users/kyauaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/kyauaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kyauaa/subscriptions", "organizations_url": "https://api.github.com/users/kyauaa/orgs", "repos_url": "https://api.github.com/users/kyauaa/repos", "events_url": "https://api.github.com/users/kyauaa/events{/privacy}", "received_events_url": "https://api.github.com/users/kyauaa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-09T01:17:29Z", "updated_at": "2018-11-12T01:42:18Z", "closed_at": "2018-11-12T01:42:18Z", "author_association": "NONE", "active_lock_reason": null, "body": "When running [MNIST ](https://github.com/yahoo/TensorFlowOnSpark/wiki/GetStarted_standalone)training in spark standalone. However I found that its input mode is SPARK, which means the computation speed is limited by single core. Do I have way to change it to run as TENSORFLOW mode, to speed up in distributed architecture? Thank you.", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/365", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/365/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/365/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/365/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/365", "id": 378715306, "node_id": "MDU6SXNzdWUzNzg3MTUzMDY=", "number": 365, "title": "Inference stage, the job stop  and don't move", "user": {"login": "lazybonesboy", "id": 4652808, "node_id": "MDQ6VXNlcjQ2NTI4MDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/4652808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lazybonesboy", "html_url": "https://github.com/lazybonesboy", "followers_url": "https://api.github.com/users/lazybonesboy/followers", "following_url": "https://api.github.com/users/lazybonesboy/following{/other_user}", "gists_url": "https://api.github.com/users/lazybonesboy/gists{/gist_id}", "starred_url": "https://api.github.com/users/lazybonesboy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lazybonesboy/subscriptions", "organizations_url": "https://api.github.com/users/lazybonesboy/orgs", "repos_url": "https://api.github.com/users/lazybonesboy/repos", "events_url": "https://api.github.com/users/lazybonesboy/events{/privacy}", "received_events_url": "https://api.github.com/users/lazybonesboy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-11-08T12:43:57Z", "updated_at": "2019-01-03T03:56:10Z", "closed_at": "2019-01-03T03:56:10Z", "author_association": "NONE", "active_lock_reason": null, "body": "**Environment:**\r\n - Python version [3.6]\r\n - Spark version [2.3.1]\r\n - TensorFlow version [1.4]\r\n - TensorFlowOnSpark version [1.3.2]\r\n - Cluster version [e.g. Standalone, Hadoop ]\r\n\r\n**Describe the bug:**\r\nspark executor can't print log and don't move there. \r\nI read same code ,  find don't run in that code:\r\nTFSparkNode.py inference method\r\nline: 481\r\n`    logging.info(\"Processed {0} items in partition\".format(count))\r\n\r\n    # read result queue\r\n    results = []\r\n    queue_out = mgr.get_queue('output')\r\n    while count > 0:\r\n      result = queue_out.get(block=True)\r\n      results.append(result)\r\n      count -= 1\r\n      queue_out.task_done()\r\n\r\n    logging.info(\"Finished processing partition\")\r\n    return results`\r\n\r\n\r\n**Logs:**\r\n**stderr log**: \r\n2018-11-08 20:16:34.045250: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 695861fe9938393a with config: \r\ngenerate_rdd_data invoked\r\n2018-11-08 20:16:35,754 INFO (Thread-1-22467) next_batch() got EndPartition\r\n\r\n1/1 [==============================] - 0s\r\n2018-11-08 20:16:36,682 INFO (MainThread-22606) Processed 120 items in partition\r\n\r\n\r\n**print log**\r\n[2018-11-08 20:16:32.097] [INFO] [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] >>> [spark-] msg=Block broadcast_7 stored as values in memory (estimated size 18.5 KB, free 1600.8 MB)[2018-11-08 20:16:32.104] [INFO] [dag-scheduler-event-loop] [org.apache.spark.storage.memory.MemoryStore] >>> [spark-] msg=Block broadcast_7_piece0 stored as bytes in memory (estimated size 11.0 KB, free 1600.8 MB)[2018-11-08 20:16:32.105] [INFO] [dispatcher-event-loop-14] [org.apache.spark.storage.BlockManagerInfo] >>> [spark-] msg=Added broadcast_7_piece0 in memory on 10.240.5.151:37814 (size: 11.0 KB, free: 1601.6 MB)[2018-11-08 20:16:32.106] [INFO] [dag-scheduler-event-loop] [org.apache.spark.SparkContext] >>> [spark-] msg=Created broadcast 7 from broadcast at DAGScheduler.scala:1039[2018-11-08 20:16:32.110] [INFO] [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] >>> [spark-] msg=Submitting 1 missing tasks from ResultStage 6 (PythonRDD[24] at RDD at PythonRDD.scala:49) (first 15 tasks are for partitions Vector(0))[2018-11-08 20:16:32.110] [INFO] [dag-scheduler-event-loop] [org.apache.spark.scheduler.TaskSchedulerImpl] >>> [spark-] msg=Adding task set 6.0 with 1 tasks[2018-11-08 20:16:32.111] [INFO] [dag-scheduler-event-loop] [org.apache.spark.scheduler.FairSchedulableBuilder] >>> [spark-] msg=Added task set TaskSet_6.0 tasks to pool default[2018-11-08 20:16:32.111] [INFO] [dispatcher-event-loop-21] [org.apache.spark.scheduler.TaskSetManager] >>> [spark-] msg=Starting task 0.0 in stage 6.0 (TID 27, 10.216.14.216, executor 0, partition 0, NODE_LOCAL, 7759 bytes)[2018-11-08 20:16:32.122] [INFO] [dispatcher-event-loop-30] [org.apache.spark.storage.BlockManagerInfo] >>> [spark-] msg=Added broadcast_7_piece0 in memory on 10.216.14.216:41626 (size: 11.0 KB, free: 2.1 GB)[2018-11-08 20:16:32.149] [INFO] [dispatcher-event-loop-26] [org.apache.spark.MapOutputTrackerMasterEndpoint] >>> [spark-] msg=Asked to send map output locations for shuffle 0 to 10.216.14.216:49102[2018-11-08 20:16:32.549] [INFO] [task-result-getter-2] [org.apache.spark.scheduler.TaskSetManager] >>> [spark-] msg=Finished task 1.0 in stage 4.0 (TID 13) in 11235 ms on 10.216.14.217 (executor 1) (5/5)[2018-11-08 20:16:32.549] [INFO] [task-result-getter-2] [org.apache.spark.scheduler.TaskSchedulerImpl] >>> [spark-] msg=Removed TaskSet 4.0, whose tasks have all completed, from pool default[2018-11-08 20:16:32.550] [INFO] [dag-scheduler-event-loop] [org.apache.spark.scheduler.DAGScheduler] >>> [spark-] msg=ResultStage 4 (foreachPartition at /home/mlp/.local/lib/python3.6/site-packages/tensorflowonspark/TFCluster.py:282) finished in 11.263 s\r\n\r\n_and then don't move_ \r\n\r\n\r\n**Spark Submit Command Line:**\r\nspark-submit \\\r\n--total-executor-cores 5 \\\r\n--executor-cores 1 \\\r\n--executor-memory 4g \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=$LIB_JVM:$LIB_HDFS\r\nmnist_data_setup.py \\\r\n--batch_size 100 \\\r\n--train_model \"inference\" \\\r\n--moder_dir /nn_v1/model_dir \\\r\n--hdfs /nn_v1/model_dir/output1 \\\r\n--cluster_size 5 \\\r\n--steps 1\\\r\n--rdma True \\\r\n--input_mode spark   > log/tfos.log 2>&1 &\r\n\r\n** partal code **\r\n`cluster = TFCluster.run(sc, map_function, args, args.cluster_size, 0, args.tensorboard, TFCluster.InputMode.SPARK)\r\n        labelRDD = cluster.inference(train_rdd)\r\nlabelRDD.saveAsTextFile(args.hdfs)`\r\n\r\n\r\n\r\n`        elif args.train_model == 'inference':\r\n            with tf.Session(server.target) as sess:\r\n                saver = tf.train.Saver()\r\n                backend.set_session(sess)\r\n                tf_feed = TFNode.DataFeed(ctx.mgr)\r\n                \r\n                ckpt = tf.train.get_checkpoint_state(args.output)\r\n                saver.restore(sess, ckpt.all_model_checkpoint_paths[-1])\r\n                output_model = Model(inputs=model.get_layer('features').input,\r\n                                     outputs=model.get_layer('output').output)\r\n                \r\n                predictions = output_model.predict_generator(generator = generate_rdd_data(tf_feed, args.batch_size),\r\n                        steps= args.steps,\r\n                        max_queue_size=10,\r\n                        workers=1,\r\n                        use_multiprocessing=False,\r\n                        verbose=0)\r\n                \r\n                for result in predictions:\r\n                    tf_feed.batch_results([result])`\r\n\r\n", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/364", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/364/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/364/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/364/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/364", "id": 378707269, "node_id": "MDU6SXNzdWUzNzg3MDcyNjk=", "number": 364, "title": "signature_def_utils import error", "user": {"login": "Roamoin", "id": 20835943, "node_id": "MDQ6VXNlcjIwODM1OTQz", "avatar_url": "https://avatars3.githubusercontent.com/u/20835943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Roamoin", "html_url": "https://github.com/Roamoin", "followers_url": "https://api.github.com/users/Roamoin/followers", "following_url": "https://api.github.com/users/Roamoin/following{/other_user}", "gists_url": "https://api.github.com/users/Roamoin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Roamoin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Roamoin/subscriptions", "organizations_url": "https://api.github.com/users/Roamoin/orgs", "repos_url": "https://api.github.com/users/Roamoin/repos", "events_url": "https://api.github.com/users/Roamoin/events{/privacy}", "received_events_url": "https://api.github.com/users/Roamoin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-08T12:19:51Z", "updated_at": "2018-11-09T19:59:16Z", "closed_at": "2018-11-09T19:59:15Z", "author_association": "NONE", "active_lock_reason": null, "body": "tf.__version__ == '1.12.0'\r\nin tensorflowonspark/pipeline.py\r\ncan not work \r\nfrom tensorflow.contrib.saved_model.python.saved_model import signature_def_utils\r\nit should be \r\nfrom tensorflow.python.saved_model import signature_def_utils", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/363", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/363/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/363/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/363/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/363", "id": 378598620, "node_id": "MDU6SXNzdWUzNzg1OTg2MjA=", "number": 363, "title": "Do TensorflowOnSpark support Python3.6?", "user": {"login": "kyauaa", "id": 32992638, "node_id": "MDQ6VXNlcjMyOTkyNjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/32992638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kyauaa", "html_url": "https://github.com/kyauaa", "followers_url": "https://api.github.com/users/kyauaa/followers", "following_url": "https://api.github.com/users/kyauaa/following{/other_user}", "gists_url": "https://api.github.com/users/kyauaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/kyauaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kyauaa/subscriptions", "organizations_url": "https://api.github.com/users/kyauaa/orgs", "repos_url": "https://api.github.com/users/kyauaa/repos", "events_url": "https://api.github.com/users/kyauaa/events{/privacy}", "received_events_url": "https://api.github.com/users/kyauaa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-08T06:41:29Z", "updated_at": "2018-11-09T21:23:52Z", "closed_at": "2018-11-09T21:23:52Z", "author_association": "NONE", "active_lock_reason": null, "body": "Is it compatible that I use python3.6 to run my tensorlfow applicaiton?", "performed_via_github_app": null, "score": 1.0}, {"url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/362", "repository_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark", "labels_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/362/labels{/name}", "comments_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/362/comments", "events_url": "https://api.github.com/repos/yahoo/TensorFlowOnSpark/issues/362/events", "html_url": "https://github.com/yahoo/TensorFlowOnSpark/issues/362", "id": 378191263, "node_id": "MDU6SXNzdWUzNzgxOTEyNjM=", "number": 362, "title": "Get stuck at \"Added broadcast_1_piece0 in memory\" when runing Spark standalone cluster", "user": {"login": "kyauaa", "id": 32992638, "node_id": "MDQ6VXNlcjMyOTkyNjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/32992638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kyauaa", "html_url": "https://github.com/kyauaa", "followers_url": "https://api.github.com/users/kyauaa/followers", "following_url": "https://api.github.com/users/kyauaa/following{/other_user}", "gists_url": "https://api.github.com/users/kyauaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/kyauaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kyauaa/subscriptions", "organizations_url": "https://api.github.com/users/kyauaa/orgs", "repos_url": "https://api.github.com/users/kyauaa/repos", "events_url": "https://api.github.com/users/kyauaa/events{/privacy}", "received_events_url": "https://api.github.com/users/kyauaa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-07T08:43:48Z", "updated_at": "2018-11-12T01:42:33Z", "closed_at": "2018-11-12T01:42:33Z", "author_association": "NONE", "active_lock_reason": null, "body": "My command:\r\n\r\n${SPARK_HOME}/bin/spark-submit \\\r\n--master ${MASTER} \\\r\n--py-files ${TFoS_HOME}/examples/mnist/spark/mnist_dist.py \\\r\n--conf spark.executorEnv.LD_LIBRARY_PATH=/usr/hdp/3.0.1.0-187/usr/lib \\\r\n--conf spark.cores.max=2 \\\r\n--conf spark.task.cpus=1 \\\r\n--num-executors 2 \\\r\n--executor-cores 1 \\\r\n--executor-memory 3g \\\r\n--conf spark.executorEnv.JAVA_HOME=\"$JAVA_HOME\" \\\r\n${TFoS_HOME}/examples/mnist/spark/mnist_spark.py \\\r\n--cluster_size 2 \\\r\n--images examples/mnist/csv/train/images \\\r\n--labels examples/mnist/csv/train/labels \\\r\n--format csv \\\r\n--mode train \\\r\n--model mnist_model\r\n\r\nIt always stuck at \r\n`\r\nMy Output: INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.3.223.190:46490 (size: 30.0 KB, free: 1458.5 MB)`\r\n\r\nWhat should I do to fix it?\r\n\r\n\r\n18/11/07 16:35:56 INFO SparkContext: Running Spark version 2.3.1.3.0.1.0-187\r\n18/11/07 16:35:56 INFO SparkContext: Submitted application: mnist_spark\r\n18/11/07 16:35:56 INFO SecurityManager: Changing view acls to: root,hdfs\r\n18/11/07 16:35:56 INFO SecurityManager: Changing modify acls to: root,hdfs\r\n18/11/07 16:35:56 INFO SecurityManager: Changing view acls groups to:\r\n18/11/07 16:35:56 INFO SecurityManager: Changing modify acls groups to:\r\n18/11/07 16:35:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root, hdfs); groups with view permissions: Set(); users  with modify permissions: Set(root, hdfs); groups with modify permissions: Set()\r\n18/11/07 16:35:56 INFO Utils: Successfully started service 'sparkDriver' on port 46810.\r\n18/11/07 16:35:57 INFO SparkEnv: Registering MapOutputTracker\r\n18/11/07 16:35:57 INFO SparkEnv: Registering BlockManagerMaster\r\n18/11/07 16:35:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n18/11/07 16:35:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n18/11/07 16:35:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-18463cd3-b3a2-4e27-b738-2d898a38cc49\r\n18/11/07 16:35:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\r\n18/11/07 16:35:57 INFO SparkEnv: Registering OutputCommitCoordinator\r\n18/11/07 16:35:57 INFO log: Logging initialized @3669ms\r\n18/11/07 16:35:57 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-06T01:11:56+08:00, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\r\n18/11/07 16:35:57 INFO Server: Started @3760ms\r\n18/11/07 16:35:57 INFO AbstractConnector: Started ServerConnector@2cb8119{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\r\n18/11/07 16:35:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@32a6bda2{/jobs,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ccc7183{/jobs/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30ec65f3{/jobs/job,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4effe6da{/jobs/job/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1f0d6ef{/stages,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5260f8df{/stages/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5a8a4629{/stages/stage,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3941fde2{/stages/stage/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@31f25fed{/stages/pool,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4feabd55{/stages/pool/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3193bd9d{/storage,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@78be8a0{/storage/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@24a5d28b{/storage/rdd,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3ebfc35f{/storage/rdd/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12ecee40{/environment,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4a7aca55{/environment/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4041781b{/executors,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@24ee77{/executors/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@75cf8edd{/executors/threadDump,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@697bda53{/executors/threadDump/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@22b1f6c7{/static,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7b45d96c{/,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1381af9e{/api,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1121f1bd{/jobs/job/kill,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6dee9150{/stages/stage/kill,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://hadoop1.spark:4040\r\n18/11/07 16:35:57 INFO SparkContext: Added file file:///root/TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py at spark://hadoop1.spark:46810/files/mnist_dist.py with timestamp 1541579757368\r\n18/11/07 16:35:57 INFO Utils: Copying /root/TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py to /tmp/spark-ee49773c-9911-47ad-8416-67f976dfeff6/userFiles-50726fd1-88d6-4c60-a4c1-144434128ffc/mnist_dist.py\r\n18/11/07 16:35:57 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://hadoop1:7077...\r\n18/11/07 16:35:57 INFO TransportClientFactory: Successfully created connection to hadoop1/10.3.223.190:7077 after 61 ms (0 ms spent in bootstraps)\r\n18/11/07 16:35:57 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20181107163557-0001\r\n18/11/07 16:35:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20181107163557-0001/0 on worker-20181107163338-10.3.223.190-45331 (10.3.223.190:45331) with 1 core(s)\r\n18/11/07 16:35:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20181107163557-0001/0 on hostPort 10.3.223.190:45331 with 1 core(s), 3.0 GB RAM\r\n18/11/07 16:35:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20181107163557-0001/1 on worker-20181107163342-10.3.223.190-37592 (10.3.223.190:37592) with 1 core(s)\r\n18/11/07 16:35:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20181107163557-0001/1 on hostPort 10.3.223.190:37592 with 1 core(s), 3.0 GB RAM\r\n18/11/07 16:35:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20181107163557-0001/0 is now RUNNING\r\n18/11/07 16:35:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20181107163557-0001/1 is now RUNNING\r\n18/11/07 16:35:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41180.\r\n18/11/07 16:35:57 INFO NettyBlockTransferService: Server created on hadoop1.spark:41180\r\n18/11/07 16:35:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n18/11/07 16:35:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, hadoop1.spark, 41180, None)\r\n18/11/07 16:35:58 INFO BlockManagerMasterEndpoint: Registering block manager hadoop1.spark:41180 with 366.3 MB RAM, BlockManagerId(driver, hadoop1.spark, 41180, None)\r\n18/11/07 16:35:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, hadoop1.spark, 41180, None)\r\n18/11/07 16:35:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, hadoop1.spark, 41180, None)\r\n18/11/07 16:35:58 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3919e4d3{/metrics/json,null,AVAILABLE,@Spark}\r\n18/11/07 16:35:59 INFO EventLoggingListener: Logging events to hdfs:/spark2-history/app-20181107163557-0001\r\n18/11/07 16:35:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\r\nargs: Namespace(batch_size=100, cluster_size=2, epochs=1, export_dir='mnist_export', format='csv', images='examples/mnist/csv/train/images', labels='examples/mnist/csv/train/labels', mode='train', model='mnist_model', output='predictions', rdma=False, readers=1, steps=1000, tensorboard=False)\r\n2018-11-07T16:35:59.990840 ===== Start\r\nzipping images and labels\r\n18/11/07 16:36:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 353.1 KB, free 366.0 MB)\r\n18/11/07 16:36:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.0 KB, free 365.9 MB)\r\n18/11/07 16:36:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop1.spark:41180 (size: 30.0 KB, free: 366.3 MB)\r\n18/11/07 16:36:00 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\r\n18/11/07 16:36:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 353.2 KB, free 365.6 MB)\r\n18/11/07 16:36:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 30.0 KB, free 365.6 MB)\r\n18/11/07 16:36:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop1.spark:41180 (size: 30.0 KB, free: 366.2 MB)\r\n18/11/07 16:36:00 INFO SparkContext: Created broadcast 1 from textFile at NativeMethodAccessorImpl.java:0\r\n18/11/07 16:36:00 INFO FileInputFormat: Total input files to process : 10\r\n18/11/07 16:36:00 INFO FileInputFormat: Total input files to process : 10\r\n2018-11-07 16:36:01,143 INFO (MainThread-18940) Reserving TFSparkNodes\r\n2018-11-07 16:36:01,144 INFO (MainThread-18940) cluster_template: {'ps': [0], 'worker': [1]}\r\n2018-11-07 16:36:01,152 INFO (MainThread-18940) listening for reservations at ('10.3.223.190', 41217)\r\n2018-11-07 16:36:01,164 INFO (MainThread-18940) Starting TensorFlow on executors\r\n2018-11-07 16:36:01,189 INFO (MainThread-18940) Waiting for TFSparkNodes to start\r\n2018-11-07 16:36:01,189 INFO (MainThread-18940) waiting for 2 reservations\r\n18/11/07 16:36:01 INFO SparkContext: Starting job: foreachPartition at /usr/local/lib/python2.7/dist-packages/tensorflowonspark/TFCluster.py:284\r\n18/11/07 16:36:01 INFO DAGScheduler: Got job 0 (foreachPartition at /usr/local/lib/python2.7/dist-packages/tensorflowonspark/TFCluster.py:284) with 2 output partitions\r\n18/11/07 16:36:01 INFO DAGScheduler: Final stage: ResultStage 0 (foreachPartition at /usr/local/lib/python2.7/dist-packages/tensorflowonspark/TFCluster.py:284)\r\n18/11/07 16:36:01 INFO DAGScheduler: Parents of final stage: List()\r\n18/11/07 16:36:01 INFO DAGScheduler: Missing parents: List()\r\n18/11/07 16:36:01 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[8] at foreachPartition at /usr/local/lib/python2.7/dist-packages/tensorflowonspark/TFCluster.py:284), which has no missing parents\r\n18/11/07 16:36:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.6 KB, free 365.5 MB)\r\n18/11/07 16:36:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 9.7 KB, free 365.5 MB)\r\n18/11/07 16:36:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop1.spark:41180 (size: 9.7 KB, free: 366.2 MB)\r\n18/11/07 16:36:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039\r\n18/11/07 16:36:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[8] at foreachPartition at /usr/local/lib/python2.7/dist-packages/tensorflowonspark/TFCluster.py:284) (first 15 tasks are for partitions Vector(0, 1))\r\n18/11/07 16:36:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\r\n18/11/07 16:36:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.3.223.190:41602) with ID 1\r\n18/11/07 16:36:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.3.223.190, executor 1, partition 0, PROCESS_LOCAL, 7862 bytes)\r\n2018-11-07 16:36:02,190 INFO (MainThread-18940) waiting for 2 reservations\r\n18/11/07 16:36:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.3.223.190:41604) with ID 0\r\n18/11/07 16:36:02 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.3.223.190, executor 0, partition 1, PROCESS_LOCAL, 7862 bytes)\r\n18/11/07 16:36:02 INFO BlockManagerMasterEndpoint: Registering block manager 10.3.223.190:40424 with 1458.6 MB RAM, BlockManagerId(1, 10.3.223.190, 40424, None)\r\n18/11/07 16:36:02 INFO BlockManagerMasterEndpoint: Registering block manager 10.3.223.190:46490 with 1458.6 MB RAM, BlockManagerId(0, 10.3.223.190, 46490, None)\r\n18/11/07 16:36:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.3.223.190:40424 (size: 9.7 KB, free: 1458.6 MB)\r\n18/11/07 16:36:03 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.3.223.190:46490 (size: 9.7 KB, free: 1458.6 MB)\r\n2018-11-07 16:36:03,191 INFO (MainThread-18940) waiting for 2 reservations\r\n2018-11-07 16:36:04,192 INFO (MainThread-18940) waiting for 2 reservations\r\n2018-11-07 16:36:05,193 INFO (MainThread-18940) all reservations completed\r\n2018-11-07 16:36:05,194 INFO (MainThread-18940) All TFSparkNodes started\r\n2018-11-07 16:36:05,194 INFO (MainThread-18940) {'executor_id': 0, 'addr': ('10.3.223.190', 40912), 'task_index': 0, 'job_name': 'ps', 'authkey': '6?H\\xe1h\\xd7K\\xf4\\xbe\\xafJ\\xcd\\xaa\\xdb\\xfa.', 'host': '10.3.223.190', 'port': 44865, 'tb_pid': 0, 'tb_port': 0}\r\n2018-11-07 16:36:05,194 INFO (MainThread-18940) {'executor_id': 1, 'addr': '/tmp/pymp-pW1Xmz/listener-bgJPts', 'task_index': 0, 'job_name': 'worker', 'authkey': '9\\xd6\\x8fz,pL\\xac\\x95\\x8b\\r$\\xe3\\xdc\\r\\x8f', 'host': '10.3.223.190', 'port': 44525, 'tb_pid': 0, 'tb_port': 0}\r\n2018-11-07 16:36:05,194 INFO (MainThread-18940) Feeding training data\r\n18/11/07 16:36:05 INFO SparkContext: Starting job: collect at PythonRDD.scala:162\r\n18/11/07 16:36:05 INFO DAGScheduler: Got job 1 (collect at PythonRDD.scala:162) with 10 output partitions\r\n18/11/07 16:36:05 INFO DAGScheduler: Final stage: ResultStage 1 (collect at PythonRDD.scala:162)\r\n18/11/07 16:36:05 INFO DAGScheduler: Parents of final stage: List()\r\n18/11/07 16:36:05 INFO DAGScheduler: Missing parents: List()\r\n18/11/07 16:36:05 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[10] at RDD at PythonRDD.scala:49), which has no missing parents\r\n18/11/07 16:36:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 14.4 KB, free 365.5 MB)\r\n18/11/07 16:36:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.9 KB, free 365.5 MB)\r\n18/11/07 16:36:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop1.spark:41180 (size: 7.9 KB, free: 366.2 MB)\r\n18/11/07 16:36:05 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1039\r\n18/11/07 16:36:05 INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 1 (PythonRDD[10] at RDD at PythonRDD.scala:49) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\r\n18/11/07 16:36:05 INFO TaskSchedulerImpl: Adding task set 1.0 with 10 tasks\r\n18/11/07 16:36:05 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 10.3.223.190, executor 0, partition 0, ANY, 8534 bytes)\r\n18/11/07 16:36:05 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3294 ms on 10.3.223.190 (executor 0) (1/2)\r\n18/11/07 16:36:05 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 54716\r\n18/11/07 16:36:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.3.223.190:46490 (size: 7.9 KB, free: 1458.6 MB)\r\n18/11/07 16:36:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.3.223.190:46490 (size: 30.0 KB, free: 1458.6 MB)\r\n18/11/07 16:36:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.3.223.190:46490 (size: 30.0 KB, free: 1458.5 MB)", "performed_via_github_app": null, "score": 1.0}]}